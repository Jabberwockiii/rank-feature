[
  {
    "title": "InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model",
    "link": "https://arxiv.org/pdf/2401.16420.pdf",
    "upvote": "52",
    "text": "InternLM-XComposer2: Mastering Free-form Text-Image\nComposition and Comprehension in Vision-Language Large Models\nXiaoyi Dong\u22171,2, Pan Zhang\u22171, Yuhang Zang\u22171, Yuhang Cao1,2, Bin Wang1, Linke Ouyang1, Xilin Wei1,\nSongyang Zhang1, Haodong Duan1, Maosong Cao1, Wenwei Zhang1, Yining Li1, Hang Yan1, Yang Gao1,\nXinyue Zhang1, Wei Li1, Jingwen Li1, Kai Chen1, Conghui He3, Xingcheng Zhang3,\nYu Qiao1, Dahua Lin1,2, Jiaqi Wang1,B\n1Shanghai Artificial Intelligence Laboratory, 2The Chinese University of Hong Kong, 3SenseTime Group\ninternlm@pjlab.org.cn\nAnswer\u00a0the\u00a0two\u00a0\nquestions\u00a0in\u00a0the\u00a0\nfollowing\u00a0figure.\u00a0Provide\u00a0\nintermediate\u00a0steps.\nFree\u2010form\u00a0Text\u2010Image\u00a0Composition\nFree\u2010form Comprehension\nChinese\u00a0New\u00a0Year,\u00a0also\u00a0known\u00a0as\u00a0the\u00a0Spring\u00a0Festival,\u00a0is\u00a0a\u00a0major\u00a0traditional\u00a0\nfestival\u00a0in\u00a0China.\u00a0It\u00a0is\u00a0celebrated\u00a0on\u00a0the\u00a0first\u00a0day\u00a0of\u00a0the\u00a0lunar\u00a0calendar\u00a0and\u00a0\nusually\u00a0falls\u00a0between\u00a0late\u00a0January\u00a0and\u00a0mid\u2010February.\u00a0This\u00a0year,\u00a0it\u00a0will\u00a0be\u00a0on\u00a0\nFebruary\u00a01st.\nThe\u00a0Spring\u00a0Festival\u00a0has\u00a0a\u00a0history\u00a0of\u00a0more\u00a0than\u00a04,000\u00a0years,\u00a0and\u00a0during\u00a0this\u00a0\ntime,\u00a0many\u00a0customs\u00a0have\u00a0emerged\u00a0and\u00a0been\u00a0passed\u00a0down\u00a0through\u00a0\ngenerations.\u00a0These\u00a0customs\u00a0include\u00a0pasting\u00a0couplets,\u00a0setting\u00a0off\u00a0firecrackers,\u00a0\npaying\u00a0respects\u00a0to\u00a0ancestors,\u00a0eating\u00a0dumplings,\u00a0etc.\nIn addition to firecrackers, other noise\u2010making devices such as dragon dancers and\nlion dancers are also used to create a festive atmosphere. These performances are\noften accompanied by drums, cymbals, and other musical instruments.\nWhile setting off firecrackers is still widely practiced today, some cities have banned\nthe use of certain types of firecrackers due to safety concerns. In these areas,\npeople may opt for electronic replacements or simply enjoy the sounds of drums\nand cymbals instead.\nPaying Respects to Ancestors\nThe Spring Festival is also a time to honor one's ancestors. Families typically gather\ntogether to perform ancestral rituals, including burning incense, offering sacrifices,\nand praying for blessings.\nMany families will also visit their ancestors' gravesites to clean them up and pay\ntheir respects. Some will even stay overnight at the cemetery, bringing bedding and\nfood with them.\nThe act of honoring one's ancestors is a reminder of one's roots and history. It also\nhelps to foster a sense of unity and belonging among family members.\nEating Dumplings\nDumplings are a popular food item during the Spring Festival. They are believed to\nsymbolize wealth and prosperity, as they resemble ancient Chinese silver ingots.\nPlease\u00a0write\u00a0an\u00a0article\u00a0based\u00a0on\u00a0the\u00a0given\u00a0requirements\u00a0with\u00a0the\u00a0title\u00a0'Introduction\u00a0to\u00a0Chinese\u00a0New\u00a0Year\u00a0Customs,'\u00a0using\u00a0the\u00a0\nprovided\u00a0materials\uff1a\n1.\u00a0Chinese\u00a0New\u00a0Year\u00a0customs\u00a0include\u00a0pasting\u00a0couplets,\u00a0setting\u00a0off\u00a0firecrackers,\u00a0and\u00a0paying\u00a0respects\u00a0to\u00a0ancestors.\n2.\u00a0New\u00a0Year\u00a0delicacies\u00a0include\u00a0eating\u00a0dumplings\u00a0and\u00a0glutinous\u00a0rice\u00a0balls,\u00a0expressing\u00a0the\u00a0symbolic\u00a0meaning\u00a0of\u00a0family\u00a0reunion.\nIntroduction\u00a0to\u00a0Chinese\u00a0New\u00a0Year\u00a0Customs\nIn\u00a0this\u00a0article,\u00a0we\u00a0will\u00a0take\u00a0a\u00a0closer\u00a0look\u00a0at\u00a0these\u00a0customs\u00a0and\u00a0their\u00a0\nsignificance,\u00a0so\u00a0that\u00a0you\u00a0can\u00a0better\u00a0understand\u00a0the\u00a0rich\u00a0cultural\u00a0heritage\u00a0of\u00a0\nthe\u00a0Chinese\u00a0people.\nPasting\u00a0Couplets\nCouplets\u00a0are\u00a0poems\u00a0with\u00a0two\u00a0lines\u00a0each,\u00a0usually\u00a0written\u00a0by\u00a0famous\u00a0poets\u00a0or\u00a0\nartists.\u00a0They\u00a0are\u00a0commonly\u00a0used\u00a0to\u00a0express\u00a0blessings,\u00a0wishes,\u00a0and\u00a0good\u00a0\nfortune.\u00a0During\u00a0the\u00a0Spring\u00a0Festival,\u00a0people\u00a0often\u00a0paste\u00a0couplets\u00a0on\u00a0doors\u00a0and\u00a0\nwindows,\u00a0hoping\u00a0to\u00a0bring\u00a0luck\u00a0and\u00a0happiness\u00a0to\u00a0their\u00a0homes.\nTraditionally,\u00a0red\u00a0paper\u00a0was\u00a0used\u00a0for\u00a0writing\u00a0couplets\u00a0because\u00a0red\u00a0is\u00a0believed\u00a0\nto\u00a0ward\u00a0off\u00a0evil\u00a0spirits.\u00a0Today,\u00a0however,\u00a0there\u00a0are\u00a0also\u00a0many\u00a0different\u00a0colors of\u00a0\npaper\u00a0available,\u00a0and\u00a0some\u00a0families\u00a0may\u00a0choose\u00a0to\u00a0use\u00a0them\u00a0based\u00a0on\u00a0\npersonal\u00a0preferences.\nPasting\u00a0couplets\u00a0is\u00a0not\u00a0only\u00a0a\u00a0form\u00a0of\u00a0decoration\u00a0but\u00a0also\u00a0a\u00a0way\u00a0to\u00a0display\u00a0\none's\u00a0family's\u00a0knowledge\u00a0and\u00a0literary\u00a0skills.\u00a0Therefore,\u00a0it\u00a0is\u00a0customary\u00a0to\u00a0write\u00a0\nyour\u00a0own\u00a0couplets\u00a0if\u00a0possible,\u00a0rather\u00a0than\u00a0buying\u00a0pre\u2010written\u00a0ones\u00a0from\u00a0\nstores.\nHere\u00a0is\u00a0an\u00a0example\u00a0of\u00a0a\u00a0popular\u00a0Spring\u00a0Festival\u00a0couplet:\n\u4e0a\u8054: \u4e00\u5e06\u98ce\u987a\n\u4e0b\u8054: \u4e07\u4e8b\u5982\u610f\n\u6a2a\u6279: \u8d22\u6e90\u5e7f\u8fdb\nThis translates to \"smooth sailing ahead,\" \"everything goes smoothly,\" and\n\"prosperous business.\" The horizontal character represents the theme of the\nentire couplet.\nEating dumplings is also associated with the legend of Nian, a monster that would\nattack villages during the Spring Festival. To defeat Nian, villagers would prepare a\nlarge feast, including dumplings filled with meat and vegetables. When Nian\napproached the village, the villagers made loud noises by banging pots and pans,\nwhich scared him away.\nThese are just a few examples of the many customs and traditions associated with\nthe Spring Festival. While some of these practices may seem strange or unfamiliar\nto outsiders, they play an important role in preserving Chinese culture and identity.\nSetting Off Firecrackers\nFirecrackers were originally created to scare away evil spirits and monsters. They\nproduce loud noises and bright lights, which are thought to drive away bad luck\nand bring good luck. Therefore, setting off firecrackers is considered an important\nritual during the Spring Festival.\nGPT-4V\nQwenVL-Plus\nGemini-Pro\nOurs\nPrevious open-source SOTA\nMathVista\nAI2D\nMMMU\nMME\nMMBench\nMMBench-CN\nSEEDImage\nLLaVAW\nQBenchT\nMM-Vet\nHallB\nChartVQA\n42.0\n57.6\n51.2\n2242.7\n79.6\n77.6\n75.9\n81.8\n72.5\n78.7\n60.3\n72.6\nQwen-VL-Chat\nCogVLM-17B\nLLaVA-1.5\nOurs\nInternLM-XC\nMathVista\nMMMU\nMMEPer\nMMBench\nMMBench-CN\nSEEDImage\nLLaVAW\nQBenchT\nMM-Vet\nHallB\n42.0\n57.6\n51.2\n530.7\n79.6\n77.6\n75.9\n81.8\n72.5\n1712.0\n60.3\nMMECog\nMonkey\nTo\u00a0find\u00a0the\u00a0length\u00a0of\u00a0the\u00a0edge,\u00a0we\u00a0can\u00a0use\u00a0the\u00a0Pythagorean\u00a0theorem,\u00a0\nwhich\u00a0states\u00a0that\u00a0in\u00a0a\u00a0right\u00a0triangle,\u00a0the\u00a0square\u00a0of\u00a0the\u00a0length\u00a0of\u00a0the\u00a0\nhypotenuse\u00a0(the\u00a0side\u00a0opposite\u00a0the\u00a0right\u00a0angle)\u00a0is\u00a0equal\u00a0to\u00a0the\u00a0sum\u00a0of\u00a0the\u00a0\nsquares\u00a0of\u00a0the\u00a0lengths\u00a0of\u00a0the\u00a0other\u00a0two\u00a0sides.\u00a0In\u00a0this\u00a0case,\u00a0we\u00a0have\u00a0a\u00a0right\u00a0\ntriangle\u00a0with\u00a0one\u00a0side\u00a0of\u00a0length\u00a04\u00a0and\u00a0the\u00a0hypotenuse\u00a0being\u00a0the\u00a0edge\u00a0we\u00a0\nwant\u00a0to\u00a0find\u00a0the\u00a0length\u00a0of.\nLet's\u00a0denote\u00a0the\u00a0length\u00a0of\u00a0the\u00a0edge\u00a0we\u00a0want\u00a0to\u00a0find\u00a0as\u00a0x.\u00a0Then,\u00a0according\u00a0\nto\u00a0the\u00a0Pythagorean\u00a0theorem,\u00a0we\u00a0have:\nx^2\u00a0=\u00a04^2\u00a0+\u00a08^2\nx^2\u00a0=\u00a016\u00a0+\u00a064\nx^2\u00a0=\u00a080\nTo\u00a0find\u00a0the\u00a0length\u00a0of\u00a0the\u00a0edge,\u00a0we\u00a0take\u00a0the\u00a0square\u00a0root\u00a0of\u00a0both\u00a0sides\u00a0of\u00a0the\u00a0\nequation:\nx\u00a0=\u00a0\u221a80\nx\u00a0\u2248\u00a08.944\nSo,\u00a0the\u00a0length\u00a0of\u00a0the\u00a0edge\u00a0is\u00a0approximately\u00a08.944.\nNow,\u00a0to\u00a0find\u00a0the\u00a0angle\u00a0at\u00a0the\u00a0vertex\u00a0of\u00a0the\u00a0triangle,\u00a0we\u00a0can\u00a0use\u00a0the\u00a0tangent\u00a0\nfunction.\u00a0The\u00a0tangent\u00a0of\u00a0an\u00a0angle\u00a0in\u00a0a\u00a0right\u00a0triangle\u00a0is\u00a0equal\u00a0to\u00a0the\u00a0length\u00a0\nof\u00a0the\u00a0side\u00a0opposite\u00a0the\u00a0angle\u00a0divided\u00a0by\u00a0the\u00a0length\u00a0of\u00a0the\u00a0side\u00a0adjacent\u00a0to\u00a0\nthe\u00a0angle.\u00a0In\u00a0this\u00a0case,\u00a0the\u00a0side\u00a0opposite\u00a0the\u00a0angle\u00a0is\u00a04\u00a0and\u00a0the\u00a0side\u00a0\nadjacent\u00a0to\u00a0the\u00a0angle\u00a0is\u00a08.\ntan(angle)\u00a0=\u00a0opposite\u00a0/\u00a0adjacent\ntan(angle)\u00a0=\u00a04\u00a0/\u00a08\ntan(angle)\u00a0=\u00a00.5\nTo\u00a0find\u00a0the\u00a0angle,\u00a0we\u00a0can\u00a0take\u00a0the\u00a0inverse\u00a0tangent\u00a0(arctangent)\u00a0of\u00a0both\u00a0\nsides\u00a0of\u00a0the\u00a0equation:\nangle\u00a0=\u00a0arctan(0.5)\nangle\u00a0\u2248\u00a026.57\u00a0degrees\nSo,\u00a0the\u00a0angle\u00a0at\u00a0the\u00a0vertex\u00a0of\u00a0the\u00a0triangle\u00a0is\u00a0approximately\u00a026.57\u00a0degrees.\nTell\u00a0me\u00a0what\u00a0will\u00a0\nbe\u00a0printed\u00a0by\u00a0\nthis\u00a0program.\nThe\u00a0program\u00a0will\u00a0print\u00a0a\u00a0dictionary\u00a0with\u00a0three\u00a0key\u2010value\u00a0pairs:\u00a0\n'woodman':98,\u00a0'Alan':89,\u00a0and\u00a0'Bobo':56.\nI\u00a0wish\u00a0to\u00a0delete\u00a0Bobo\u00a0from\u00a0dict.\nTo\u00a0delete\u00a0\"Bobo\"\u00a0from\u00a0the\u00a0dictionary,\u00a0you\u00a0can\u00a0use\u00a0the\u00a0following\u00a0code:\nAfter executing this code, the dictionary will be updated without the\nkey \"Bobo\".\ndel\u00a0dict[\u2018Bob\u2019]\nFigure 1. Overview of free-form text-image composition and comprehension of InternLM-XComposer2. Our model based on InternLM2-\n7B [77] not only significantly outperforms existing multimodal models but also matches or even surpasses GPT-4V [58] and Gemini\nPro [76] in certain assessments. (Please zoom-in to see the details.)\nAbstract\nWe introduce InternLM-XComposer2, a cutting-edge\nvision-language model excelling in free-form text-image\ncomposition and comprehension.\nThis model goes be-\nyond conventional vision-language understanding, adeptly\ncrafting interleaved text-image content from diverse in-\nputs like outlines, detailed textual specifications, and ref-\nerence images, enabling highly customizable content cre-\nation.\nInternLM-XComposer2 proposes a Partial LoRA\n(PLoRA) approach that applies additional LoRA parame-\n* indicates equal contribution.\nters exclusively to image tokens to preserve the integrity\nof pre-trained language knowledge, striking a balance be-\ntween precise vision understanding and text composition\nwith literary talent. Experimental results demonstrate the\nsuperiority of InternLM-XComposer2 based on InternLM2-\n7B in producing high-quality long-text multi-modal content\nand its exceptional vision-language understanding perfor-\nmance across various benchmarks, where it not only sig-\nnificantly outperforms existing multimodal models but also\nmatches or even surpasses GPT-4V and Gemini Pro in cer-\ntain assessments. This highlights its remarkable proficiency\nin the realm of multimodal understanding. The InternLM-\n1\narXiv:2401.16420v1  [cs.CV]  29 Jan 2024\nXComposer2 model series with 7B parameters are pub-\nlicly available at https://github.com/InternLM/\nInternLM-XComposer.\n1. Introduction\nIn recent years, there has been a remarkable evolution in\nthe field of large language models (LLMs) [8, 16, 17, 57,\n58, 68, 79].\nForemost among these, models like Chat-\nGPT [57] have completely altered human interaction with\ntechnology. Concurrently, a variety of open-source LLMs,\nsuch as Llama [78], Mistra [37], InternLM [77], QWen [65],\nGLM [25], and Baichuan [7], have empowered the cus-\ntomization of LLMs. Building on these open-source foun-\ndations, the community has seen substantial progress in\nmultimodal large language models (MLLMs) [6, 21, 29,\n48, 49, 82, 95, 100].\nThese MLLMs are adept at in-\nterpreting images and engaging in text-image dialogues,\nshowcasing impressive multimodal understanding. Unlike\ntraditional MLLMs, a recent innovation, i.e., InternLM-\nXComposer [95], has focused on using MLLMs for text-\nimage composition and comprehension, marking a novel di-\nrection in MLLM research. However, this pioneering work\nis currently limited to generating text-image articles based\non titles alone, lacking the sophistication to meet more com-\nplex composition requirements. Furthermore, while achiev-\ning leading performance at its inception, this model still\npossesses significant potential for enhancement in detailed\nperception and complex reasoning capabilities to advance\nits vision-language comprehension performance.\nThis observation motivates the development of more ad-\nvanced vision-language models capable of practical and po-\ntent text-image composition and comprehension. In this pa-\nper, we introduce InternLM-XComposer2, a cutting-edge\nmodel excelling in free-form text-image composition and\ncomprehension, built based on InternLM2 [77]. InternLM-\nXComposer2 represents a significant advancement over its\npredecessor, InternLM-XComposer [95], in both text-image\ncomposition and comprehension. InternLM-XComposer2\nis adept at producing high-quality, integrated text-image ar-\nticles from a variety of free-form inputs, such as detailed\nspecifications, structured outlines, and reference images,\nserving to a wide range of application contexts. In the realm\nof multimodal understanding, it demonstrates exceptional\ncapabilities in detailed perception, logical reasoning, and\nextensive knowledge integration. Its performance signifi-\ncantly surpasses that of existing open-source MLLMs, and\nit stands on par with, or even exceeds, advanced models like\nGPT-4V [58] and Gemini Pro [76] in various benchmarks.\nThe appealing capabilities of InternLM-XComposer2\nare primarily due to two critical design elements.\n(1)\nPartial LoRA: The Partial LoRA (P-LoRA) design har-\nmonizes its abilities in composition and comprehension.\nThis involves feeding forward image tokens with additional\nLoRA [33] (Low-Rank Adaptation) parameters, while lan-\nguage tokens retain the original architecture. This selec-\ntive enhancement ensures robust performance in both visual\nand textual domains. (2) High-quality and Diverse Data\nFoundataion: The quality and diversity of the training data\nare pivotal. Our dataset for free-form text-image composi-\ntion excels in: adhering to complex instructions, customiza-\ntion with text and image for tailored content, high-quality\nand stylistically diverse writing, and versatile text editing\nincluding condensing, expanding, and revising.\nFor ex-\nceptional vision-language comprehension capabilities, we\ngather a wide range of high-quality pretraining and super-\nvised fine-tuning multimodal data. This collection spans\nvarious aspects and types, such as captions, general QA,\nscientific QA, chat-based QA, mathematical QA, concept\nknowledge, conversation, and text-image composition.\nInternLM-XComposer2 surpasses existing benchmarks\nin both composition and comprehension. In the creation\nbenchmark of OpenCompass [18] for evaluating the creativ-\nity of LLMs, InternLM-XComposer2 showcases outstand-\ning performance. To demostrate our multimodal comphren-\nsion capility, we compare our InternLM-XComposer2 on\na list of benchmarks with both open-source MLLMs and\nclosed-source APIs, e.g., GPT4V [58], Gemini Pro [76],\nand Qwen-VL Plus [19].\nWe report results in Math-\nVista [52], MMMU [91], AI2D [40], MME [27], MM-\nBench [51], MMBench-Chinese [51], SEED-Bench (Im-\nage) [41], LLaVA-Bench (In-the-Wild) [49], QBench [85],\nMM-Vet\n[90], HallusionBench [31], ChartQA [56], and\nPOPE [45]. InternLM-XComposer2 based on InternLM2-\n7B significantly exceeds the performance of existing open-\nsource models by an impressive margin. Remarkably, it\ndemonstrates superior performance to GPT4V [58], Gem-\nini Pro [76] across six benchmarks.\n2. Related Works\nLarge Language Models (LLMs).\nRecent LLM archi-\ntectures have marked a transition from encoder-decoder\nframeworks (e.g., BERT [22], T5 [68]) to an emphasis on\ndecoder-only models employed with autoregressive train-\ning techniques for next-token prediction (e.g., GPT [67]).\nThe following works (e.g., GPT3 [8], InstructGPT [60],\nChatGPT [57], PaLM [17]) have seen the integration of\nadvanced techniques such as instruction-tuning and Rein-\nforcement Learning from Human Feedback (RLHF). Cou-\npled with expansive parameter sizes and extensive train-\ning data, these LLM models have achieved substantial per-\nformance enhancements across a diverse range of Nat-\nural Language Processing (NLP) tasks.\nOther notable\nLLMs encompass a range of developments, such as the\nOPT [96], LLaMA series [78, 79], e.g., Mistral [37, 38],\nInternLM [77], GLM series [25, 93], Qwen series [6, 65],\n2\nBaichuan [7], Skywork [84] and Falcon [61] have made sig-\nnificant contributions to the field.\nMultimodal Large Language Models (MLLMs). Vision-\nlanguage models (VLMs), exemplified by CLIP [66] and\nits subsequent works [26, 36, 43, 44, 50, 75, 94], align im-\nage and text features in a unified embedding space. This\nalignment is achieved through contrastive learning objec-\ntives applied to extensive image-text pair datasets. VLMs\nachieve strong zero-shot and few-shot performance, show-\ncasing significant generalization abilities across a range of\ndownstream tasks.\nBenefiting from existing large language models and\nVLMs as the visual encoder, recent Multimodal Large Lan-\nguage Models (MLLMs) [12, 14, 15, 24, 28, 58] achieve\nvisual perception, understanding and reasoning abilities,\nshow superb performance in diverse vision-language tasks.\nA series of studies [2, 5, 9, 10, 20, 21, 42, 46, 49, 62,\n64, 80, 86, 92, 97, 98, 100] have explored further im-\nprove the MLLM in different dimensions, such as instruc-\ntion tuning [11, 49, 98], efficient fine-tuning [33], high-\nresolution image inputs [6, 82, 83], hallucination mitiga-\ntion [34, 87, 99], image generation [23, 30, 74, 89], 3D un-\nderstanding [63] and image-text comprehension and com-\nposition [95].\nTo enable highly customizable content creation, our\nmodel is designed for free-form text-image composition and\ncomprehension based on MLLMs. We use Intern-LM2 as\nthe LLM and CLIP ViT-Large as the visual encoder and\npropose a new partial LoRA to align the text-image modal-\nities. Given flexible and multi-modal user inputs such as\nspecifications, outlines, and reference images, our model\nis capable of generating high-quality interleaved text-image\nwritten content.\n3. Method\n3.1. Model Architecture\nOur proposed model, InternLM-XComposer2, incorporates\na vision encoder and a Language Learning Model (LLM).\nThese two components are interconnected via an innovative\nPartial LoRA module. Given a set of images and text, the\nLLM utilizes the output from the vision encoder as visual\ntokens and the tokenized text as language tokens. These\ntokens are then concatenated to form the input sequence.\nVision Encoder. The vision encoder in our model is de-\nsigned to extract high-level visual features from raw im-\nages. It is pretrained in an image-language contrastive man-\nner(CLIP). Our findings indicate that, when used in con-\njunction with our Partial LoRA module, a lightweight vi-\nsion model performs effectively. For the sake of efficiency,\nwe have opted to use the OpenAI ViT-Large model.\nLarge Language Model. We employ the recently intro-\nduced InternLM-2 as our Large Language Model (LLM).\n\ud835\udc4a\ud835\udc34 \u2208 \u211d\ud835\udc36\ud835\udc5f \u00d7\ud835\udc36\ud835\udc56\ud835\udc5b\n\ud835\udc4a\ud835\udc35 \u2208 \u211d\ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc61 \u00d7\ud835\udc36\ud835\udc5f\nPretrained \nWeights\n\ud835\udc4a0 \u2208 \u211d\ud835\udc36\ud835\udc5c\ud835\udc62\ud835\udc61 \u00d7\ud835\udc36\ud835\udc56\ud835\udc5b\nFigure 2. The illustration of the Partial-LoRA. The blue tokens\nrepresent the visual tokens and the gray tokens are the language\ntokens. Our Partial-LoRA is only applied to the visual tokens.\nThis model boasts exceptional multi-lingual capabilities\nand has demonstrated impressive results in benchmarks. In\npractical applications, we utilize the InternLM2-7B-Chat-\nSFT variant as our LLM.\nPartial Low-Rank Adaptation.\nIn the realm of multi-\nmodal Language Learning Models (LLMs), one insuffi-\nciently explored area is the effective alignment of different\nmodalities. A desired alignment should potentially enrich\nthe LLM with new modality-specific knowledge, while si-\nmultaneously preserving its inherent capabilities. Current\nmethods predominantly adopt one of two approaches: they\neither treat the visual token and language token equally or\nas entirely distinct entities. We contend that the first ap-\nproach overlooks the inherent property distinctions between\nmodalities, while the second approach results in a substan-\ntial alignment cost.\nIn our pursuit of effective modality alignment, we intro-\nduce Partial LoRA, a versatile plug-in module designed to\nalign knowledge from a new modality to the LLM. As illus-\ntrated in Figure X, Partial LoRA draws inspiration from the\noriginal LoRA and incorporates a low-rank adaptation that\nis exclusively applied to the new modality portion of the in-\nput tokens. In our specific configuration, Partial LoRA is\napplied to all visual tokens.\nFormally, for each linear layer L0 in the LLM blocks,\nwe denote its weight matrix W0 \u2208 R(Cout\u00d7Cin) and bias\nB0 \u2208 RCout, where Cin and Cout are the input and output\ndimension. Its corresponding Parital LoRA contains two\nlow-rank matrix WA \u2208 RCr\u00d7Cin and WB \u2208 RCout\u00d7Cr.\nWith a given input x = [xv, xt], we have the output feature\n\u02c6x by:\n\u02c6xt = W0xt + B0\n\u02c6xv = W0xv + WBWAxv + B0\n\u02c6x = [\u02c6xv, \u02c6xt]\n3\nTask\nDataset\nGeneral Semantic Alignment\nShareGPT4V-PT [11], COCO [13], Nocaps [1], TextCaps [73], LAION400M [69], SBU [59], CC 3M [72]\nWorld Knowledge Alignment\nConcept Data [95]\nVision Capability Enhancement\nWanJuan [32], Flicker[88], MMC-Instruction[47]\nTable 1. Datasets used for Pre-Training. The data are collected from diverse sources for the three objectives.\nwhere xv and xt are the visual tokens and language tokens\nof the input sequence respectively.\n3.2. Pre-Training\nDuring the pre-training phase, the LLM remains constant\nwhile both the vision encoder and Partial LoRA are fine-\ntuned to align the visual tokens with the LLM. The pre-\ntraining data is meticulously curated with three objectives\nin mind: 1) general semantic alignment, 2) world knowl-\nedge alignment, 3) vision capability enhancement.\nGeneral Semantic Alignment. The objective of general se-\nmantic alignment is to equip the MLLM with the fundamen-\ntal ability to comprehend image content. For instance, the\nMLLM should be able to recognize that a picture of Einstein\nrepresents \u2018a human\u2019. We utilize image caption data from a\nvariety of sources for this purpose, including high-quality,\ndetailed captions from ShareGPT4V-PT, as well as concise\nand precise captions from COCO, NoCaps, TextCaps, etc.\nDuring the pre-training phase, we employ a simple instruc-\ntion: Describe this image briefly/in detail.\nWorld Knowledge Alignment. World knowledge repre-\nsents an advanced capability of the MLLM. For instance,\nthe MLLM should be able to identify the man in the figure\nmentioned above as \u2018Albert Einstein\u2019 and further talk some-\nthing about him. To align the world knowledge depicted in\nthe image with the knowledge already acquired by the LLM,\nwe have constructed a concept dataset. This dataset is care-\nfully filtered from the concept data utilized in InternLM-\nXComposer [95]. Given that the text in the concept data\nonly partially describes the content in the image and their\nrelationship is complex to model, we employ a more broad\ninstruction: Tell me something about this image.\nVision Capability Enhancement.\nFinally, an advanced\nMLLM necessitates certain vision-specific capabilities,\nsuch as Optical Character Recognition (OCR), object lo-\ncalization (grounding), and the understanding of structured\nimages (e.g., charts, tables). To achieve this, we have com-\npiled relevant datasets, as outlined in Table.1, and have im-\nplemented corresponding instructions for training.\nThanks to the design of Partial LoRA, the LLM is able\nto adapt to visual tokens while maintaining its original lan-\nguage processing capabilities. The fixed LLM also enables\nus to directly use in-context learning performance as a mea-\nsure of pre-training quality.\nIn our implementation, we employ the OpenAI CLIP\nViT-L-14-336 as the vision encoder. We increase its resolu-\nTask\nDataset\nMulti-task training\nCaption\nShareGPT4V [11], COCO [13],Nocaps [1]\nGeneral QA\nVQAv2 [4], GQA [35], OK-VQA [55]\nScience QA\nAI2D [40], SQA [54]\nChart QA\nDVQA [39], ChartQA [56]\nMath QA\nMathQA [3], Geometry3K[53]\nWorld Knowledge QA\nA-OKVQA [70], ,KVQA [71]\nConversation\nLLaVA-150k [49], LVIS-Instruct4V [81]\nInstruction tuning\nFree-from Composiiton\nIn-house data (Refer to Sec.3.4)\nConversation\nLLaVA-150k [49], LVIS-Instruct4V [81]\nShareGPT-en&zh [16], InternLM-Chat[77]\nTable 2. Datasets used for Supervised Fine-Tuning. We collect\ndata from diverse sources to empower the model with different\ncapabilities.\ntion to 490\u00d7490 for improved performance. For the Partial\nLoRA, we set a rank of 256 for all the linear layers in the\nLLM decoder block. Our training process involves a batch\nsize of 4906 and spans across 2 epochs. The learning rate is\ninitially set to increase to 2\u00d710\u22124 within the first 1% of the\ntraining steps. Following this, it decreases to 0 according to\na cosine decay strategy. To preserve the pre-existing knowl-\nedge of the vision encoder, we apply a layer-wise learning\nrate (LLDR) decay strategy and the decay factor is set to\n0.90.\n3.3. Supervised Fine-tuning\nThe pre-training phase aligns the visual feature with the\nlanguage, enabling the Language Learning Model (LLM)\nto comprehend the content of the images. However, it still\nlacks the ability to effectively utilize the image information.\nTo overcome this limitation, we introduce a range of vision-\nlanguage tasks that the model engages in during the subse-\nquent Supervised Fine-Tuning Stage. This stage comprises\ntwo sequential steps: Multi-task Training and Free-form\nText-Image Composition. During this stage, we jointly fine-\ntune the vision encoder, LLM, and Partial LoRA.\nMulti-task Training. As delineated in Table 2, the multi-\ntask training dataset is assembled from various sources,\naiming to equip the model with a broad spectrum of capabil-\nities. Each task is structured as a conversational interaction,\nand the instructions are augmented with GPT-4 to enhance\ndiversity. Concurrently, to maintain the original language\ncapability, we also incorporate the supervised fine-tuning\n4\nMethod\nMathVista\nAI2D\nMMMU\nMME\nMMB\nMMBCN\nSEEDI\nLLaVAW\nQBenchT\nMM-Vet\nHallB\nChartVQA\nOpen-Source\nSPH-MOE\nMonkey\nYi-VL\nWeMM\nL-Int2\nL-Int2\nSPH-2\nCogVLM\nInt-XC\nCogVLM\nMonkey\nCogAgent\nPrevious SOTA\n8x7B\n10B\n34B\n6B\n20B\n20B\n17B\n17B\n8B\n30B\n10B\n18B\n42.3\n72.6\n45.9\n2066.6\n75.1\n73.7\n74.8\n73.9\n64.4\n56.8\n58.4\n68.4\nClosed-source API\nGPT-4V\n49.9\n78.2\n56.8\n1926.5\n77.0\n74.4\n69.1\n93.1\n74.1\n67.7\n65.8\n78.5\nGemini-Pro\n45.2\n73.9\n47.9\n1933.3\n73.6\n74.3\n70.7\n79.9\n70.6\n64.3\n63.9\n74.1\nQwenVL-Plus\n43.3\n75.9\n46.5\n2183.3\n67.0\n70.7\n72.7\n73.7\n68.9\n55.7\n56.4\n78.1\nOurs\n57.6\n78.7\n42.0\n2242.7\n79.6\n77.6\n75.9\n81.8\n72.5\n51.2\n60.3\n72.6\nTable 3. Comparison with closed-source APIs and previous open-source SOTAs. Our InternLM-XComposer2 gets SOTA results in\n6 of the 12 benchmarks with only 7B parameters, showing competitive results with current closed-source APIs and previous open-source\nSOTA MLLMs. The best results are bold and the second-best results are underlined.\ndata from InternLM2, which constitutes a fixed 10% of the\ntotal Supervised Fine-Tuning (SFT) data.\nFree-form Text-Image Composition. To further enhance\nthe model\u2019s ability to follow instructions and compose\nfree-form image-text content, we employ data from both\npure-text conversation corpora and vision-language con-\nversations, as outlined in Table 2.\nThe dataset for free-\nform image-text composition is constructed following the\nmethodology detailed in Section 3.4.\nIn our approach, we jointly train all the components with\na batch size of 2048 over 3000 steps. Data from multiple\nsources are sampled in a weighted manner, with the weights\nbased on the number of data from each source. The maxi-\nmum learning rate is set to 5 \u00d7 10\u22125, and each component\nhas its own unique learning strategy. For the vision encoder,\nwe set the Layer-wise Learning Rate Decay (LLDR) to 0.9,\nwhich aligns with the pretraining strategy. For the LLM, we\nemploy a fixed learning rate scale factor of 0.2. This slows\ndown the update of the LLM, achieving a balance between\npreserving its original capabilities and aligning it with vi-\nsion knowledge.\n3.4. Free-form Text-Image Composition\nFree-form text-image composition refers to the combination\nof textual content and visual elements in a flexible and unre-\nstrictive manner. Our model generates interleaved text and\nimages, specifically customized to align with the text re-\nquirements provided by users, which may include elements\nsuch as a title, outline, and writing material, and optionally,\nany visual requirements like image resources.\nTo facilitate free-form text-image composition, we col-\nlect a wide range of high-quality and diverse in-house data\nacross four key dimensions. These dimensions encompass:\nVaried Writing Styles. Our data spans a multitude of writ-\ning styles, from academic papers to social media posts and\npoems, ensuring a rich and diverse collection of text and\nimage contents.\nFlexible Text Editing. Our dataset includes extensive ex-\namples of text editing, encompassing a wide spectrum of\nmodifications such as shortening, expanding, and rewriting.\nComplex Instruction Adherence.\nWe also capture in-\nstances of adhering to complex instructions to create con-\ntent that caters to diverse demands like titles and outlines,\nencompassing both text and image-based compositions.\nCustomization with Materials. Our collection extends to\nmaterials used for personalized content creation, covering\nboth text and images, enabling customizable and unique\ncontent creation experiences.\nThe distribution of data across the four dimensions is ap-\nproximately equal, with a ratio of approximately 1:1:1:1.\nOur method follows previous work [95] to identify suit-\nable positions for image insertion after generating the text\ncontent.\nA notable distinction in our approach is that\nwhen users provide their own image materials, these im-\nage materials are used for insertion instead of relying on\nretrieved images [95]. We also observe that having a high-\nresolution image input is not essential for text-image com-\nposition. Therefore, following the pre-training phase, we\nopt to down-sample the image input resolution to 224x224\nduring the SFT stage of free-form text-image composition.\n4. Experiments\nIn this section, we validate the benchmark performance\nof our InternLM-XComposer2 after the supervised fine-\ntuning.\n4.1. MLLM Benchmark results.\nIn Table.3 and Table.4,\nwe compare our InternLM-\nXComposer2 on a list of benchmarks with both SOTA\nopen-source MLLMs and closed-source APIs.\nHere we\nreport results in MathVista[52], MMMU[91], AI2D[40],\nMME\nPerception\n(MMEP )\n[27],\nMME\nCognition\n(MMEC)[27],\nMMBench\n(MMB)\n[51],\nMMBench-\nChinese\n(MMBCN)\n[51],\nSEED-Bench\nImage\nPart\n(SEEDI)[41],\nLLaVA-Bench\nIn-the-Wild\n(LLaVAW )\n[49],\nQBench-Testset\n(QBenchT )[85],\nMM-Vet\n[90],\nHallusionBench (HallB)[31], ChartQA[56], POPE[45].\nComparison with Closed-Source APIs.\nAs shown in\n5\nMethod\nLLM\nMathVista\nMMMU\nMMEP\nMMEC\nMMB\nMMBCN\nSEEDI\nLLaVAW\nQBenchT\nMM-Vet\nHallB\nBLIP-2\nFLAN-T5\n-\n35.7\n1,293.8\n290.0\n-\n-\n46.4\n38.1\n-\n22.4\n-\nInstructBLIP\nVicuna-7B\n25.3\n30.6\n-\n-\n36.0\n23.7\n53.4\n60.9\n55.9\n26.2\n53.6\nIDEFICS-80B\nLLaMA-65B\n26.2\n24.0\n-\n-\n54.5\n38.1\n52.0\n56.9\n-\n39.7\n46.1\nQwen-VL-Chat\nQwen-7B\n33.8\n35.9\n1,487.5\n360.7\n60.6\n56.7\n58.2\n67.7\n61.7\n47.3\n56.4\nLLaVA\nVicuna-7B\n23.7\n32.3\n807.0\n247.9\n34.1\n14.1\n25.5\n63.0\n54.7\n26.7\n44.1\nLLaVA-1.5\nVicuna-13B\n26.1\n36.4\n1,531.3\n295.4\n67.7\n63.6\n68.2\n70.7\n61.4\n35.4\n46.7\nShareGPT4V\nVicuna-7B\n25.8\n36.6\n1,567.4\n376.4\n68.8\n62.2\n69.7\n72.6\n-\n37.6\n49.8\nCogVLM-17B\nVicuna-7B\n34.7\n37.3\n-\n-\n65.8\n55.9\n68.8\n73.9\n-\n54.5\n55.1\nLLaVA-XTuner\nInernLM2-20B\n24.6\n39.4\n-\n-\n75.1\n73.7\n70.2\n63.7\n-\n37.2\n47.7\nMonkey-10B\nQwen-7B\n34.8\n40.7\n1,522.4\n401.4\n72.4\n67.5\n68.9\n33.5\n-\n33.0\n58.4\nInternLM-XC\nInernLM-7B\n29.5\n35.6\n1,528.4\n391.1\n74.4\n72.4\n66.1\n53.8\n64.4\n35.2\n57.0\nOurs\nInernLM2-7B\n57.6\n42.0\n1,712.0\n530.7\n79.6\n77.6\n75.9\n81.8\n72.5\n51.2\n60.3\nTable 4. Comparison with open-source SOTA methods. InternLM-XComposer2 outperforms competitors in 10 out of 11 benchmarks.\nThe best results are bold and the second-best results are underlined.\nMethod\nLLM\nPOPE\nHallusionBench*\nClosed-source API\nGPT-4V\n-\n-\n65.8\nGemini-Pro\n-\n-\n63.9\nQwenVL-Plus\n-\n-\n56.4\nOpen-source MLLMs\nInstructBLIP\nVicuna-7B\n78.9\n53.6\nIDEFICS-80B\nLLaMA-65B\n-\n46.1\nQwen-VL-Chat\nQwen-7B\n-\n56.4\nLLaVA\nVicuna-7B\n80.2\n44.1\nLLaVA-1.5\nVicuna-13B\n85.9\n46.7\nInternLM-XC\nInernLM-7B\n-\n57.0\nOurs\nInernLM2-7B\n87.7\n60.3\nTable 5.\nHallucination Evaluation on POPE and Hallu-\nsionBench.\nIntenrLM-XComposer2 outperforms open-source\nMLLMs and performs on par with closed-source APIs. * We skip\nthe non-visual questions, following the setting in VLMEvalKit[18]\nTable.3, InternLM-XComposer2 demonstrates competitive-\nness with Closed-Source APIs across numerous bench-\nmarks. For instance, our model achieves a score of 57.6%\non MathVista and 78.9 on AI2D, outperforming these APIs\nby a significant margin. Meanwhile, despite having only\n7B parameters, our model attains a slightly worse score of\n43.0% on the challenging college-level benchmark MMMU.\nThe strong performance can be attributed to the superb\nknowledge acquired by the new InternLM2 LLM and the\nefficient PLoRA training strategy, which enabled us to align\nthe LLM with image features while preserving its language\ncapability.\nComparison with Open-Source Models. We also conduct\na comprehensive comparison with open-source MLLMs\nunder a similar model scale.\nAs shown in Table.4, our\nmodel significantly outperforms existing open-source mod-\nels, achieving state-of-the-art results across all bench-\nmarks. Notably, InternLM-XComposer2 is the first model\nto achieve a score exceeding 1700 on the MME-Perception\nbenchmark. Furthermore, it attained an accuracy of nearly\n80% on the MMBench.\nHallucination Evaluation. Visual hallucination serves as a\nMethod\nw/o Ref\nw Ref\nAvg. C\nR\nUDF LC\nAvg. C\nR\nUDF LC\nGPT-4\n6.32 5.22 5.98 7.17 7.47 5.98 5.30 5.55 6.51 7.08\nQWen-72b-Chat 5.70 4.78 5.16 6.37 7.13 5.31 4.94 4.72 5.71 6.50\nYi-34b-Chat\n6.03 4.91 5.68 6.79 7.35 5.71 5.03 5.22 6.18 6.87\nOurs\n6.24 5.11 6.12 7.03 7.45 5.90 5.21 5.76 6.27 6.93\nTable 6. Comparison on CreationBench [18]. We report the\nresults with and without the GPT-4 referenced answer. We re-\nport the average score and other metrics including Creativity(C),\nRichness(R), User Demand Fulfillment (UDF), and Logical Co-\nherence(LC).\ncrucial metric in the evaluation of an MLLM. In this report,\nwe present the results obtained on both POPE and Hallu-\nsionBench. As indicated in Table.5, our model achieves an\naverage F1-score of 87.7 across the three tracks of POPE,\nsetting a new state-of-the-art (SOTA) benchmark. In the\ncase of HallusionBench, our model surpasses the accuracy\nof all open-source models, establishing itself as the new\nSOTA. Furthermore, it outperforms the closed-source API,\nQwenVL-Plus.\n4.2. CreationBench Results\nWe use the CreationBench benchmark from OpenCom-\npass [18] to assess the writing ability of our InternLM-\nXComposer2. As shown in Table 6, the results indicate\nthat our approach not only excels in overall creativity but\nalso significantly improves upon key metrics over previous\nopen-source LLMs. When compared without the GPT-4\nreferenced answer, our method scored an impressive 6.24\noverall. Even when evaluated with the GPT-4 reference, our\nmethod maintained strong performance, achieving scores\nthat underscore its ability to generate responses with high\nlevels of creativity and logical structure, critical for user en-\ngagement and satisfaction in conversational AI applications.\n4.3. Qualitative results.\nPlease refer to the Appendix for our qualitative results of the\nfree-form image-text compositions and multimodal conver-\nsations.\n6\n5. Conclusion\nIn this paper, we present InternLM-XComposer2, which\ndemonstrates its exceptional capabilities in the field of\nvision-language understanding and free-form text-image\ncomposition.\nOur proposed innovative Partial LoRA\n(PLoRA) approach, which applies additional LoRA pa-\nrameters exclusively to image tokens, has proven effective\nin preserving the integrity of pre-trained language knowl-\nedge while striking a balance between precise vision un-\nderstanding and text composition with literary talent. Our\nmodel\u2019s performance across various benchmarks not only\nsignificantly outperforms existing multimodal models but\nalso matches or even surpasses GPT-4V and Gemini Pro\nin certain assessments, underscoring its remarkable profi-\nciency in the realm of multimodal understanding.\nThis\nresearch opens up new possibilities for highly customiz-\nable content creation and paves the way for future advance-\nments in the MLLM field. The potential applications of\nInternLM-XComposer2 are vast and exciting, promising a\nfuture where AI can understand and generate high-quality\nlong-text multi-modal content with ease and precision.\nReferences\n[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh,\nStefan Lee, and Peter Anderson.\nNocaps: Novel object\ncaptioning at scale. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 8948\u20138957,\n2019. 4\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur\nMensch, Katie Millican, Malcolm Reynolds, Roman Ring,\nEliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,\nSina Samangooei, Marianne Monteiro, Jacob Menick, Se-\nbastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-\nhand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karen Simonyan.\nFlamingo: a visual language model for few-shot learning,\n2022. 3\n[3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-\nKedziorski,\nYejin\nChoi,\nand\nHannaneh\nHajishirzi.\nMathqa: Towards interpretable math word problem solv-\ning with operation-based formalisms.\narXiv preprint\narXiv:1905.13319, 2019. 4\n[4] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret\nMitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi\nParikh. Vqa: Visual question answering. In International\nConference on Computer Vision (ICCV), 2015. 4\n[5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel,\nYusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan\nBitton, Samir Gadre, Shiori Sagawa, Jenia Jitsev, Simon\nKornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Worts-\nman, and Ludwig Schmidt.\nOpenflamingo:\nAn open-\nsource framework for training large autoregressive vision-\nlanguage models. arXiv.org, 2023. 3\n[6] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan\nTan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model\nwith versatile abilities. arXiv.org, 2023. 2, 3\n[7] Baichuan. Baichuan 2: Open large-scale language models.\narXiv.org, 2023. 2, 3\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. Advances in Neu-\nral Information Processing Systems (NeurIPS), 33:1877\u2013\n1901, 2020. 2\n[9] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu\nLiu,\nPengchuan Zhang,\nRaghuraman Krishnamoorthi,\nVikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny.\nMinigpt-v2: large language model as a unified interface\nfor vision-language multi-task learning.\narXiv preprint\narXiv:2310.09478, 2023. 3\n[10] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,\nFeng Zhu, and Rui Zhao. Shikra: Unleashing multimodal\nllm\u2019s referential dialogue magic. arXiv.org, 2023. 3\n[11] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui\nHe, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v:\nImproving large multi-modal models with better captions.\narXiv preprint arXiv:2311.12793, 2023. 3, 4\n7\n[12] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,\nSoravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz,\nSebastian Goodman, Xiao Wang, Yi Tay, Siamak Shak-\neri, Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael\nTschannen, Arsha Nagrani, Hexiang Hu, Mandar Joshi,\nBo Pang, Ceslee Montgomery, Paulina Pietrzyk, Marvin\nRitter, AJ Piergiovanni, Matthias Minderer, Filip Pavetic,\nAustin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas\nBeyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,\nYang Li, Daniel Keysers, Anurag Arnab, Yuanzhong Xu,\nKeran Rong, Alexander Kolesnikov, Mojtaba Seyedhos-\nseini, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali-x: On scaling up a multilingual vision\nand language model, 2023. 3\n[13] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna\nVedantam, Saurabh Gupta, Piotr Dollar, and C. Lawrence\nZitnick. Microsoft coco captions: Data collection and eval-\nuation server, 2015. 4\n[14] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov,\nJialin Wu,\nPaul Voigtlaender,\nBasil Mustafa,\nSebas-\ntian Goodman, Ibrahim Alabdulmohsin, Piotr Padlewski,\nDaniel Salz, Xi Xiong, Daniel Vlasic, Filip Pavetic, Keran\nRong, Tianli Yu, Daniel Keysers, Xiaohua Zhai, and Radu\nSoricut.\nPali-3 vision language models: Smaller, faster,\nstronger, 2023. 3\n[15] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergio-\nvanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman,\nAdam Grycner, Basil Mustafa, Lucas Beyer, Alexander\nKolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Has-\nsan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal,\nJames Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini,\nChao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas\nSteiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. Pali: A jointly-scaled multilingual language-\nimage model, 2023. 3\n[16] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-\nhao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang,\nYonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and\nEric P. Xing. Vicuna: An open-source chatbot impressing\ngpt-4 with 90%* chatgpt quality, March 2023. 2, 4\n[17] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebastian\nGehrmann, et al. Palm: Scaling language modeling with\npathways. arXiv.org, 2022. 2\n[18] OpenCompass Contributors.\nOpencompass:\nA univer-\nsal evaluation platform for foundation models. https:\n//github.com/open- compass/opencompass,\n2023. 2, 6\n[19] QWen Contributors.\nQwen-vl-plus.\nhttps : / /\nhuggingface . co / spaces / Qwen / Qwen - VL -\nPlus, year=2023. 2\n[20] XTuner Contributors.\nXtuner: A toolkit for efficiently\nfine-tuning llm. https://github.com/InternLM/\nxtuner, 2023. 3\n[21] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi.\nInstructblip: Towards general-\npurpose vision-language models with instruction tuning,\n2023. 2, 3\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv.org, 2018. 2\n[23] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng\nGe, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou,\nHaoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng\nMa, and Li Yi. Dreamllm: Synergistic multimodal com-\nprehension and creation. arXiv preprint arXiv:2309.11499,\n2023. 3\n[24] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch,\nAakanksha Chowdhery,\nBrian Ichter,\nAyzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong\nHuang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-\nworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman,\nMarc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,\nand Pete Florence. Palm-e: An embodied multimodal lan-\nguage model. In arXiv preprint arXiv:2303.03378, 2023.\n3\n[25] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong\nQiu, Zhilin Yang, and Jie Tang. Glm: General language\nmodel pretraining with autoregressive blank infilling.\nIn\nProceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers),\npages 320\u2013335, 2022. 2\n[26] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,\nXinggang Wang, Tiejun Huang, Xinlong Wang, and Yue\nCao. Eva: Exploring the limits of masked visual represen-\ntation learning at scale. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), pages 19358\u201319369, 2023. 3\n[27] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin,\nMengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jin-\nrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Ron-\ngrong Ji. Mme: A comprehensive evaluation benchmark\nfor multimodal large language models.\narXiv preprint\narXiv:2306.13394, 2023. 2, 5\n[28] Chaoyou Fu, Renrui Zhang, Zihan Wang, Yubo Huang,\nZhengye Zhang, Longtian Qiu, Gaoxiang Ye, Yunhang\nShen, Mengdan Zhang, Peixian Chen, Sirui Zhao, Shao-\nhui Lin, Deqiang Jiang, Di Yin, Peng Gao, Ke Li, Hong-\nsheng Li, and Xing Sun. A challenger to gpt-4v? early\nexplorations of gemini in visual expertise. arXiv preprint\narXiv:2312.12436, 2023. 3\n[29] Peng Gao, Jiaming Han, Renrui Zhang, Ziyi Lin, Shijie\nGeng, Aojun Zhou, W. Zhang, Pan Lu, Conghui He, Xi-\nangyu Yue, Hongsheng Li, and Yu Jiao Qiao.\nLlama-\nadapter v2: Parameter-efficient visual instruction model.\nArXiv, abs/2304.15010, 2023. 2\n[30] Yuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying\nShan. Planting a seed of vision in large language model. 3\n[31] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian,\nZongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen,\nFurong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi\nZhou. Hallusionbench: An advanced diagnostic suite for\nentangled language hallucination & visual illusion in large\nvision-language models, 2023. 2, 5\n[32] Conghui He, Zhenjiang Jin, Chaoxi Xu, Jiantao Qiu, Bin\nWang, Wei Li, Hang Yan, Jiaqi Wang, and Da Lin. Wan-\n8\njuan: A comprehensive multimodal dataset for advancing\nenglish and chinese large models. ArXiv, abs/2308.10755,\n2023. 4\n[33] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. LoRA: Low-rank adaptation of large language mod-\nels. In International Conference on Learning Representa-\ntions, 2022. 2, 3\n[34] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Con-\nghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and\nNenghai Yu.\nOpera: Alleviating hallucination in multi-\nmodal large language models via over-trust penalty and\nretrospection-allocation. arXiv preprint arXiv:2311.17911,\n2023. 3\n[35] Drew A Hudson and Christopher D Manning. Gqa: A new\ndataset for real-world visual reasoning and compositional\nquestion answering. Conference on Computer Vision and\nPattern Recognition (CVPR), 2019. 4\n[36] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li,\nand Tom Duerig.\nScaling up visual and vision-language\nrepresentation learning with noisy text supervision. In Pro-\nceedings of the International Conference on Machine learn-\ning (ICML), pages 4904\u20134916. PMLR, 2021. 3\n[37] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch,\nChris Bamford, Devendra Singh Chaplot, Diego de las\nCasas, Florian Bressand, Gianna Lengyel, Guillaume Lam-\nple, Lucile Saulnier, L\u00b4elio Renard Lavaud, Marie-Anne\nLachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril,\nThomas Wang, Timoth\u00b4ee Lacroix, and William El Sayed.\nMistral 7b, 2023. 2\n[38] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux,\nArthur Mensch, Blanche Savary, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de las Casas, Emma Bou Hanna,\nFlorian Bressand, et al. Mixtral of experts. arXiv preprint\narXiv:2401.04088, 2024. 2\n[39] Kushal Kafle, Brian Price, Scott Cohen, and Christopher\nKanan. Dvqa: Understanding data visualizations via ques-\ntion answering. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 5648\u20135656,\n2018. 4\n[40] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon\nSeo, Hannaneh Hajishirzi, and Ali Farhadi. A diagram is\nworth a dozen images. In Computer Vision\u2013ECCV 2016:\n14th European Conference, Amsterdam, The Netherlands,\nOctober 11\u201314, 2016, Proceedings, Part IV 14, pages 235\u2013\n251. Springer, 2016. 2, 4, 5\n[41] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yix-\niao Ge, and Ying Shan. Seed-bench: Benchmarking multi-\nmodal llms with generative comprehension, 2023. 2, 5\n[42] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. Otter: A multi-modal model\nwith in-context instruction tuning. arXiv.org, 2023. 3\n[43] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for uni-\nfied vision-language understanding and generation. In Pro-\nceedings of the International Conference on Machine learn-\ning (ICML), pages 12888\u201312900. PMLR, 2022. 3\n[44] Liunian Harold Li*, Pengchuan Zhang*, Haotian Zhang*,\nJianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang,\nLu Yuan, Lei Zhang, Jenq-Neng Hwang, Kai-Wei Chang,\nand Jianfeng Gao. Grounded language-image pre-training.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 3\n[45] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin\nZhao, and Ji-Rong Wen. Evaluating object hallucination in\nlarge vision-language models, 2023. 2, 5\n[46] Zhang Li, Biao Yang, Qiang Liu, Zhiyin Ma, Shuo Zhang,\nJingxu Yang, Yabo Sun, Yuliang Liu, and Xiang Bai.\nMonkey: Image resolution and text label are important\nthings for large multi-modal models.\narXiv preprint\narXiv:2311.06607, 2023. 3\n[47] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,\nKaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong\nYu.\nMmc:\nAdvancing multimodal chart understand-\ning with large-scale instruction tuning.\narXiv preprint\narXiv:2311.10774, 2023. 4\n[48] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.\nImproved baselines with visual instruction tuning. arXiv\npreprint arXiv:2310.03744, 2023. 2\n[49] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. Visual instruction tuning. arXiv.org, 2023. 2, 3, 4, 5\n[50] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao\nZhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection. arXiv.org, 2023.\n3\n[51] Yuan Liu,\nHaodong Duan,\nYuanhan Zhang,\nBo Li,\nSongyang Zhnag, Wangbo Zhao, Yike Yuan, Jiaqi Wang,\nConghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mm-\nbench: Is your multi-modal model an all-around player?\narXiv:2307.06281, 2023. 2, 5\n[52] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chun-\nyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang,\nMichel Galley, and Jianfeng Gao. Mathvista: Evaluating\nmathematical reasoning of foundation models in visual con-\ntexts. In International Conference on Learning Represen-\ntations (ICLR), 2024. 2, 5\n[53] Pan Lu, Ran Gong, Shibiao Jiang, Liang Qiu, Siyuan\nHuang, Xiaodan Liang, and Song-Chun Zhu. Inter-gps: In-\nterpretable geometry problem solving with formal language\nand symbolic reasoning. In The 59th Annual Meeting of the\nAssociation for Computational Linguistics (ACL), 2021. 4\n[54] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark,\nand Ashwin Kalyan.\nLearn to explain: Multimodal rea-\nsoning via thought chains for science question answer-\ning. Advances in Neural Information Processing Systems,\n35:2507\u20132521, 2022. 4\n[55] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and\nRoozbeh Mottaghi. Ok-vqa: A visual question answering\nbenchmark requiring external knowledge. In Proceedings\nof the IEEE/cvf conference on computer vision and pattern\nrecognition, pages 3195\u20133204, 2019. 4\n[56] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty,\nand Enamul Hoque. Chartqa: A benchmark for question\nanswering about charts with visual and logical reasoning.\narXiv preprint arXiv:2203.10244, 2022. 2, 4, 5\n9\n[57] OpenAI.\nChatgpt.\nhttps://openai.com/blog/\nchatgpt, 2022. 2\n[58] OpenAI. Gpt-4 technical report, 2023. 1, 2, 3\n[59] Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\nIm2text:\nDescribing images using 1 million captioned\nphotographs.\nIn Neural Information Processing Systems\n(NIPS), 2011. 4\n[60] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sand-\nhini Agarwal, Katarina Slama, Alex Ray, et al. Training\nlanguage models to follow instructions with human feed-\nback. Advances in Neural Information Processing Systems\n(NeurIPS), 35:27730\u201327744, 2022. 2\n[61] Guilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza Alobei-\ndli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Lau-\nnay. The refinedweb dataset for falcon llm: Outperforming\ncurated corpora with web data, and web data only, 2023. 3\n[62] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan\nHuang, Shuming Ma, and Furu Wei. Kosmos-2: Grounding\nmultimodal large language models to the world. arXiv.org,\n2023. 3\n[63] Zhangyang Qi, Ye Fang, Zeyi Sun, Xiaoyang Wu, Tong\nWu, Jiaqi Wang, Dahua Lin, and Hengshuang Zhao.\nGpt4point: A unified framework for point-language under-\nstanding and generation, 2023. 3\n[64] Zhangyang Qi, Ye Fang, Mengchen Zhang, Zeyi Sun, Tong\nWu, Ziwei Liu, Dahua Lin, Jiaqi Wang, and Hengshuang\nZhao. Gemini vs gpt-4v: A preliminary comparison and\ncombination of vision-language models through qualitative\ncases, 2023. 3\n[65] Qwen. Introducing qwen-7b: Open foundation and human-\naligned models (of the state-of-the-arts), 2023. 2\n[66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In Proceedings of the International Conference on\nMachine learning (ICML), pages 8748\u20138763. PMLR, 2021.\n3\n[67] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. Improving language understanding by gen-\nerative pre-training. 2018. 2\n[68] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li,\nand Peter J Liu. Exploring the limits of transfer learning\nwith a unified text-to-text transformer. Journal of Machine\nLearning Research (JMLR), 21(1):5485\u20135551, 2020. 2\n[69] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki.\nLaion-\n400m: Open dataset of clip-filtered 400 million image-text\npairs. arXiv preprint arXiv:2111.02114, 2021. 4\n[70] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark,\nKenneth Marino, and Roozbeh Mottaghi.\nA-okvqa: A\nbenchmark for visual question answering using world\nknowledge. In European Conference on Computer Vision,\npages 146\u2013162. Springer, 2022. 4\n[71] Sanket Shah,\nAnand Mishra,\nNaganand Yadati,\nand\nPartha Pratim Talukdar.\nKvqa: Knowledge-aware visual\nquestion answering. In Proceedings of the AAAI conference\non artificial intelligence, 2019. 4\n[72] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu\nSoricut. Conceptual captions: A cleaned, hypernymed, im-\nage alt-text dataset for automatic image captioning. In Pro-\nceedings of the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages\n2556\u20132565, 2018. 4\n[73] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. Textcaps: a dataset for image caption-\ning with reading comprehension.\nIn Computer Vision\u2013\nECCV 2020: 16th European Conference, Glasgow, UK, Au-\ngust 23\u201328, 2020, Proceedings, Part II 16, pages 742\u2013758.\nSpringer, 2020. 4\n[74] Quan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong\nZhang, Yueze Wang, Hongcheng Gao, Jingjing Liu, Tiejun\nHuang, and Xinlong Wang. Generative pretraining in mul-\ntimodality. Jul 2023. 3\n[75] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang,\nShu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang.\nAlpha-CLIP: A clip model focusing on wherever you want.\narXiv preprint arXiv:2312.03818, 2023. 3\n[76] Gemini Team. Gemini: A family of highly capable multi-\nmodal models, 2023. 1, 2\n[77] InternLM Team. Internlm: A multilingual language model\nwith progressively enhanced capabilities.\nhttps://\ngithub.com/InternLM/InternLM, 2023. 1, 2, 4\n[78] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Bap-\ntiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar,\net al. Llama: Open and efficient foundation language mod-\nels. arXiv.org, 2023. 2\n[79] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models,\n2023. 2\n[80] Bin Wang, Fan Wu, Xiao Han, Jiahui Peng, Huaping\nZhong, Pan Zhang, Xiaoyi Dong, Weijia Li, Wei Li, Jiaqi\nWang, et al. Vigc: Visual instruction generation and correc-\ntion. arXiv.org, 2023. 3\n[81] Junke Wang, Lingchen Meng, Zejia Weng, Bo He, Zuxuan\nWu, and Yu-Gang Jiang. To see is to believe: Prompting\ngpt-4v for better visual instruction tuning. arXiv preprint\narXiv:2311.07574, 2023. 4\n[82] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji\nQi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan\nSong, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming\nDing, and Jie Tang. Cogvlm: Visual expert for pretrained\nlanguage models, 2023. 2, 3\n[83] Haoran Wei, Lingyu Kong, Jinyue Chen, Liang Zhao,\nZheng Ge, Jinrong Yang, Jianjian Sun, Chunrui Han, and\nXiangyu Zhang.\nVary:\nScaling up the vision vocab-\nulary for large vision-language models.\narXiv preprint\narXiv:2312.06109, 2023. 3\n[84] Tianwen Wei, Liang Zhao, Lichang Zhang, Bo Zhu, Lijie\nWang, Haihua Yang, Biye Li, Cheng Cheng, Weiwei L\u00a8u,\nRui Hu, et al. Skywork: A more open bilingual foundation\n10\nmodel. arXiv preprint arXiv:2310.19341, 2023. 3\n[85] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen,\nLiang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong\nYan, Guangtao Zhai, et al.\nQ-bench: A benchmark for\ngeneral-purpose foundation models on low-level vision.\narXiv preprint arXiv:2309.14181, 2023. 2, 5\n[86] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv.org, 2023.\n3\n[87] Shukang Yin, Chaoyou Fu, Sirui Zhao, Tong Xu, Hao\nWang, Dianbo Sui, Yunhang Shen, Ke Li, Xing Sun,\nand Enhong Chen.\nWoodpecker: Hallucination correc-\ntion for multimodal large language models. arXiv preprint\narXiv:2310.16045, 2023. 3\n[88] Peter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. From image descriptions to visual denotations:\nNew similarity metrics for semantic inference over event\ndescriptions. Transactions of the Association for Computa-\ntional Linguistics, 2:67\u201378, 2014. 4\n[89] Lili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin\nMuller, Olga Golovneva, Tianlu Wang, Arun Babu, Binh\nTang, Brian Karrer, Shelly Sheynin, Candace Ross, Adam\nPolyak, Russell Howes, Vasu Sharma, Puxin Xu, Hov-\nhannes Tamoyan, Oron Ashual, Uriel Singer, Shang-Wen\nLi, Susan Zhang, Gargi Ghosh, Yaniv Taigman, Maryam\nFazel-Zarandi, Asli Celikyilmaz, Luke Zettlemoyer, and\nArmen Aghajanyan.\nScaling autoregressive multi-modal\nmodels: Pretraining and instruction tuning. 3\n[90] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang,\nKevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang.\nMm-vet:\nEvaluating large multimodal models for inte-\ngrated capabilities. arXiv preprint arXiv:2308.02490, 2023.\n2, 5\n[91] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng,\nRuoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang,\nWeiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin\nYuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu\nYang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and\nWenhu Chen. Mmmu: A massive multi-discipline multi-\nmodal understanding and reasoning benchmark for expert\nagi. arXiv preprint arXiv:2311.16502, 2023. 2, 5\n[92] Yuhang Zang, Wei Li, Jun Han, Kaiyang Zhou, and\nChen Change Loy.\nContextual object detection with\nmultimodal large language models.\narXiv preprint\narXiv:2305.18279, 2023. 3\n[93] Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu\nLai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng,\nXiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong\nZhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao\nDong, and Jie Tang. GLM-130b: An open bilingual pre-\ntrained model. In The Eleventh International Conference\non Learning Representations (ICLR), 2023. 2\n[94] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan,\nJenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying\nlocalization and vision-language understanding. Advances\nin Neural Information Processing Systems (NeurIPS),\n35:36067\u201336080, 2022. 3\n[95] Pan Zhang, Xiaoyi Dong Bin Wang, Yuhang Cao, Chao Xu,\nLinke Ouyang, Zhiyuan Zhao, Shuangrui Ding, Songyang\nZhang, Haodong Duan, Hang Yan, et al.\nInternlm-\nxcomposer: A vision-language large model for advanced\ntext-image comprehension and composition. arXiv preprint\narXiv:2309.15112, 2023. 2, 3, 4, 5\n[96] Susan Zhang,\nStephen Roller,\nNaman Goyal,\nMikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, et al. OPT: Open\npre-trained transformer language models.\narXiv preprint\narXiv:2205.01068, 2022. 2\n[97] Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma, Kaikai\nAn, Liang Chen, Zixuan Liu, Sheng Wang, Wenjuan Han,\nand Baobao Chang. Mmicl: Empowering vision-language\nmodel with multi-modal in-context learning.\narXiv.org,\n2023. 3\n[98] Zhiyuan Zhao, Linke Ouyang, Bin Wang, Siyuan Huang,\nPan Zhang, Xiaoyi Dong, Jiaqi Wang, and Conghui He.\nMllm-dataengine:\nAn iterative refinement approach for\nmllm. arXiv.org, 2023. 3\n[99] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Ji-\naqi Wang, and Conghui He. Beyond hallucinations: En-\nhancing lvlms through hallucination-aware direct prefer-\nence optimization. arXiv preprint arXiv:2311.16839, 2023.\n3\n[100] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny.\nMinigpt-4:\nEnhancing vision-\nlanguage understanding with advanced large language\nmodels. arXiv.org, 2023. 2, 3\n11\nFrench Pastries: A Sweet Indulgence\nFrench pastries are something that everyone should indulge\nin at least once in their lifetime. The delicate flavors and\nbeautiful presentation of these pastries make them the\nperfect treat for any occasion. From croissants to macarons,\n\u00e9clairs to tarts, French pastries come in a variety of shapes\nand sizes. These sweet delights are not only delicious but\nalso incredibly addictive. It's hard to resist just one pastry\nwhen there are so many to choose from! Whether you're\nlooking for a quick breakfast on the go or a decadent\ndessert after dinner, French pastries are sure to satisfy your\ncravings. So why not treat yourself to some French pastries\ntoday? You won't regret it!\nCroissants\nThe croissant is a crescent-shaped French pastry that has\nbecome one of the most well-known French pastries in the\nworld. This buttery, flaky pastry is created by laminating\ndough with layers of butter; this process creates beautiful\nlayers and flakiness when the croissant bakes. In France,\ncroissants are typically served for breakfast or as a snack.\nCroissants can be eaten plain or topped with sugar, jam,\nNutella, or other spreads. While croissants may seem like\nan indulgent treat, they're actually relatively easy to make\nat home. It's important to note that there are some\nvariations on how to make croissants, including sweeter\nversions\nsuch\nas\nchocolate\ncroissants\nand\nalmond\ncroissants.\nTips\u00a0for\u00a0Visiting\u00a0Shanghai\nMacarons\nMacarons are small, colorful cookies that are made with\nalmond flour and filled with a variety of fillings such as\nganache, buttercream, or jam. The delicate texture of these\npastries make them perfect for pairing with coffee or tea.\nFrench Macarons are traditionally associated with Parisian\ncafes, where they can be found in an array of flavors\nranging from classic to experimental.\n\u00c9clairs\nWhat are Eclairs? \u00c8clairs (French pronunciation: (ekl\u025b\u0281))\nare a type of pastry that originated in France and is still\nwidely consumed there. They are long, thin pastries that\nare filled with cream or pastry cream. The exterior has a\ncrispy coating while the interior is soft. There are many\nways to make eclairs, but one thing remains constant\u2014\nthey have an unforgettable taste!\nTarts\nTarts are open-faced pies that are typically filled with fruit\nor custard. They can be either sweet or savory, but the\nsweet versions are definitely more popular in France.\nThere are many types of tarts in French cuisine: apple tart\n(tarte Tatin), almond tart (tarte amandine), chocolate tart\n(tarte au chocolat), cheese tart (tarte aux fromages) and\nmany others.\nPain au Chocolat\nPain au chocolat is a French pastry that consists of a\ncrescent-shaped roll filled with chocolate. It's similar to a\ncroissant but has a rich chocolate filling that adds an extra\nlayer of decadence. The texture of the chocolate can be\nanything from soft and creamy to hard and crunchy\ndepending on personal preference. The buttery, flaky crust\nmakes it a deliciously indulgent treat for any time of day.\nIn conclusion, French pastries are a sweet indulgence that\nshould not be missed. Whether you are looking for\nsomething light like a croissant or something more\nsubstantial like a tart, there is sure to be a French pastry\nthat will satisfy your cravings. So why not head to your\nlocal bakery and indulge in some French pastry goodness?\nPlease write a blog based on the title: French Pastries: A Sweet Indulgence\nFigure 3. Interleaved composition generated by InternLM-XComposer2.\n12\nTips\u00a0for\u00a0Visiting\u00a0Shanghai\n\u6839\u636e\u4ee5\u4e0b\u6807\u9898\uff1a\u201c\u4e2d\u56fd\u6c34\u58a8\u753b\uff1a\u6d41\u52a8\u7684\u8bd7\u610f\u4e0e\u4e1c\u65b9\u7f8e\u5b66\u201d\uff0c\u521b\u4f5c\u957f\u6587\u7ae0\uff0c\u5b57\u6570\u4e0d\u5c11\u4e8e800\u5b57\u3002\u8bf7\u7ed3\u5408\u4ee5\n\u4e0b\u6587\u672c\u7d20\u6750\uff1a\n\u201c\u6c34\u58a8\u753b\u662f\u7531\u6c34\u548c\u58a8\u8c03\u914d\u6210\u4e0d\u540c\u6df1\u6d45\u7684\u58a8\u8272\u6240\u753b\u51fa\u7684\u753b\uff0c\u662f\u7ed8\u753b\u7684\u4e00\u79cd\u5f62\u5f0f\uff0c\u66f4\u591a\u65f6\u5019\uff0c\u6c34\u58a8\u753b\u88ab\u89c6\u4e3a\n\u4e2d\u56fd\u4f20\u7edf\u7ed8\u753b\uff0c\u4e5f\u5c31\u662f\u56fd\u753b\u7684\u4ee3\u8868\u3002\u4e5f\u79f0\u56fd\u753b\uff0c\u4e2d\u56fd\u753b\u3002\u58a8\u6c34\u753b\u662f\u4e2d\u56fd\u4f20\u7edf\u753b\u4e4b\u4e00\u3002\u58a8\u6c34\u662f\u56fd\u753b\u7684\u8d77\u6e90\uff0c\n\u4ee5\u7b14\u58a8\u8fd0\u7528\u7684\u6280\u6cd5\u57fa\u7840\u753b\u6210\u58a8\u6c34\u753b\u3002\u7ebf\u6761\u4e2d\u950b\u7b14\uff0c\u4fa7\u950b\u7b14\uff0c\u987a\u950b\u548c\u9006\u950b\uff0c\u70b9\u67d3\uff0c\u64e6\uff0c\u7834\u58a8\uff0c\u62e8\u58a8\u7684\u6280\u6cd5\u3002\n\u58a8\u4e8e\u6c34\u7684\u53d8\u5316\u5206\u4e3a\u4e94\u8272\u3002\u753b\u6210\u4f5c\u54c1\uff0c\u9898\u6b3e\uff0c\u76d6\u7ae0\u3002\u5c31\u662f\u5b8c\u6574\u7684\u58a8\u6c34\u753b\u4f5c\u54c1\u3002\n\u57fa\u672c\u7684\u6c34\u58a8\u753b\uff0c\u4ec5\u6709\u6c34\u4e0e\u58a8\uff0c\u9ed1\u4e0e\u767d\u8272\uff0c\u4f46\u8fdb\u9636\u7684\u6c34\u58a8\u753b\uff0c\u4e5f\u6709\u5de5\u7b14\u82b1\u9e1f\u753b\uff0c\u8272\u5f69\u7f24\u7eb7\u3002\u540e\u8005\u6709\u65f6\u4e5f\n\u79f0\u4e3a\u5f69\u58a8\u753b\u3002\u5728\u4e2d\u56fd\u753b\u4e2d\uff0c\u4ee5\u4e2d\u56fd\u753b\u7279\u6709\u7684\u6750\u6599\u4e4b\u4e00\uff0c\u58a8\u4e3a\u4e3b\u8981\u539f\u6599\u52a0\u4ee5\u6e05\u6c34\u7684\u591a\u5c11\u5f15\u4e3a\u6d53\u58a8\u3001\u6de1\u58a8\u3001\u5e72\n\u58a8\u3001\u6e7f\u58a8\u3001\u7126\u58a8\u7b49\uff0c\u753b\u51fa\u4e0d\u540c\u6d53\u6de1\uff08\u9ed1\u3001\u767d\u3001\u7070\uff09\u5c42\u6b21\u3002\u522b\u6709\u4e00\u756a\u97f5\u5473\u79f0\u4e3a\u201c\u58a8\u97f5\u201d\u3002\u800c\u5f62\u6210\u6c34\u58a8\u4e3a\u4e3b\u7684\n\u4e00\u79cd\u7ed8\u753b\u5f62\u5f0f\u3002\u201d\n\u4e2d\u56fd\u6c34\u58a8\u753b\uff1a\u6d41\u52a8\u7684\u8bd7\u610f\u4e0e\u4e1c\u65b9\u7f8e\u5b66\n\u4e2d\u56fd\u7684\u4f20\u7edf\u7ed8\u753b\uff0c\u7279\u522b\u662f\u6c34\u58a8\u753b\uff0c\u88ab\u8a89\u4e3a\u201c\u58a8\u97f5\u4e4b\u821e\u201d\uff0c\u4ee5\n\u7b14\u58a8\u6325\u6d12\u3001\u7ebf\u6761\u4ea4\u7ec7\u7684\u5f62\u5f0f\uff0c\u5c55\u73b0\u4e86\u4e2d\u56fd\u7279\u6709\u7684\u5ba1\u7f8e\u60c5\u8da3\u548c\u6587\u5316\n\u5185\u6db5\u3002\u6c34\u58a8\u753b\u662f\u4e00\u79cd\u72ec\u7279\u7684\u827a\u672f\u5f62\u5f0f\uff0c\u5b83\u4e0d\u4ec5\u4f53\u73b0\u4e86\u4e2d\u56fd\u4f20\u7edf\u6587\n\u5316\u7684\u7cbe\u9ad3\uff0c\u4e5f\u5f70\u663e\u4e86\u4e1c\u65b9\u7684\u7f8e\u5b66\u89c2\u5ff5\u548c\u54f2\u5b66\u601d\u60f3\u3002\u5728\u8fd9\u7bc7\u6587\u7ae0\u4e2d\uff0c\n\u6211\u4eec\u5c06\u6df1\u5165\u63a2\u8ba8\u6c34\u58a8\u753b\u7684\u827a\u672f\u7279\u70b9\uff0c\u53ca\u5176\u5728\u4e2d\u56fd\u6587\u5316\u4e2d\u7684\u91cd\u8981\u5730\n\u4f4d\u3002\n\u8d77\u6e90\u4e0e\u53d1\u5c55\n\u6c34\u58a8\u753b\u7684\u5386\u53f2\u53ef\u4ee5\u8ffd\u6eaf\u5230\u4e2d\u56fd\u53e4\u4ee3\u7684\u6218\u56fd\u65f6\u671f\uff08\u516c\u5143\u524d475\n\u5e74\u2014\u516c\u5143\u524d221\u5e74\uff09\u3002\u5f53\u65f6\u7684\u7ed8\u753b\u4e3b\u8981\u662f\u7528\u6bdb\u7b14\u5728\u7ee2\u6216\u7eb8\u4e0a\u7ed8\u5236\uff0c\n\u800c\u989c\u6599\u5219\u662f\u4ece\u77ff\u7269\u3001\u690d\u7269\u548c\u52a8\u7269\u4e2d\u63d0\u53d6\u3002\u968f\u7740\u65f6\u4ee3\u7684\u53d1\u5c55\uff0c\u7ed8\u753b\n\u6280\u6cd5\u4e0d\u65ad\u4e30\u5bcc\uff0c\u5176\u4e2d\u6700\u91cd\u8981\u7684\u8d21\u732e\u4e4b\u4e00\u5c31\u662f\u58a8\u7684\u53d1\u73b0\u548c\u4f7f\u7528\u3002\u636e\n\u4f20\uff0c\u79e6\u59cb\u7687\u7edf\u4e00\u516d\u56fd\u540e\uff0c\u4e0b\u4ee4\u70e7\u6bc1\u4e66\u7c4d\uff0c\u4f46\u4e00\u4f4d\u4e66\u751f\u85cf\u8d77\u4e86\u4e00\u672c\n\u73cd\u8d35\u7684\u53e4\u4e66\u3002\u8fd9\u672c\u53e4\u4e66\u540e\u6765\u88ab\u65e0\u610f\u4e2d\u4e22\u5f03\uff0c\u843d\u5165\u6c34\u4e2d\u88ab\u6ce1\u6e7f\u3002\u5f53\n\u4e66\u751f\u635e\u8d77\u4e66\u9875\u65f6\uff0c\u610f\u5916\u5730\u53d1\u73b0\u4e0a\u9762\u7684\u6587\u5b57\u5e76\u672a\u88ab\u6c34\u7834\u574f\uff0c\u53cd\u800c\u66f4\n\u52a0\u6e05\u6670\u4e86\u3002\u8fd9\u4e2a\u6545\u4e8b\u544a\u8bc9\u6211\u4eec\uff0c\u6c34\u4e0e\u58a8\u7684\u642d\u914d\u662f\u521b\u9020\u5947\u8ff9\u7684\u5173\u952e\u3002\n\u4ece\u6b64\u4ee5\u540e\uff0c\u4e2d\u56fd\u4eba\u5f00\u59cb\u5c06\u6c34\u548c\u58a8\u7ed3\u5408\u8d77\u6765\u521b\u4f5c\u51fa\u5177\u6709\u72ec\u7279\u97f5\u5473\u7684\n\u753b\u4f5c\u3002\n\u6838\u5fc3\u5143\u7d20\n\u6c34\u58a8\u753b\u7684\u6838\u5fc3\u5143\u7d20\u5305\u62ec\u7b14\u6cd5\u3001\u58a8\u8272\u548c\u6784\u56fe\u3002\u9996\u5148\uff0c\u7b14\u6cd5\u5728\u6c34\n\u58a8\u753b\u4e2d\u81f3\u5173\u91cd\u8981\u3002\u4e0d\u540c\u7684\u7b14\u89e6\u548c\u7b14\u529b\u80fd\u591f\u4ea7\u751f\u4e30\u5bcc\u7684\u89c6\u89c9\u6548\u679c\uff0c\n\u5982\u7c97\u72b7\u8c6a\u653e\u7684\u76b4\u64e6\uff0c\u7ec6\u817b\u67d4\u7f8e\u7684\u70b9\u67d3\u7b49\u3002\u5176\u6b21\uff0c\u58a8\u8272\u7684\u8fd0\u7528\u4e5f\u662f\n\u6c34\u58a8\u753b\u7684\u7cbe\u9ad3\u6240\u5728\u3002\u901a\u8fc7\u8c03\u8282\u6c34\u7684\u591a\u5c11\uff0c\u53ef\u4ee5\u4ea7\u751f\u6d53\u6de1\u5e72\u6e7f\u7684\u53d8\n\u5316\uff0c\u521b\u9020\u51fa\u6df1\u9083\u795e\u79d8\u7684\u6c1b\u56f4\u3002\u6700\u540e\uff0c\u6784\u56fe\u5219\u51b3\u5b9a\u4e86\u753b\u9762\u7684\u6574\u4f53\u5e03\n\u5c40\u548c\u610f\u5883\u8868\u8fbe\u3002\u5408\u7406\u7684\u6784\u56fe\u80fd\u591f\u5f15\u5bfc\u89c2\u8005\u7684\u89c6\u7ebf\u6d41\u52a8\uff0c\u589e\u5f3a\u753b\u9762\n\u7684\u827a\u672f\u611f\u67d3\u529b\u3002\n\u6280\u6cd5\u4e0e\u8868\u73b0\u529b\n\u6c34\u58a8\u753b\u7684\u6280\u6cd5\u975e\u5e38\u591a\u6837\u5316\uff0c\u5e38\u89c1\u7684\u6709\u6cfc\u58a8\u3001\u7834\u58a8\u3001\u79ef\u58a8\u3001\u5bbf\n\u58a8\u3001\u67af\u58a8\u7b49\u3002\u8fd9\u4e9b\u6280\u6cd5\u5404\u6709\u7279\u8272\uff0c\u80fd\u591f\u4ea7\u751f\u4e0d\u540c\u7684\u6548\u679c\u3002\u4f8b\u5982\uff0c\n\u6cfc\u58a8\u9002\u7528\u4e8e\u8868\u73b0\u5c71\u5ce6\u8d77\u4f0f\u7684\u5927\u6c14\u78c5\u7934\uff1b\u7834\u58a8\u5219\u9002\u5408\u63cf\u7ed8\u6811\u6728\u679d\u53f6\n\u7684\u7e41\u8302\u7eb7\u6742\uff1b\u79ef\u58a8\u5219\u591a\u7528\u4e8e\u8425\u9020\u539a\u91cd\u6df1\u9083\u7684\u5c71\u6c34\u6c14\u6c1b\u3002\u6b64\u5916\uff0c\u6c34\n\u58a8\u753b\u8fd8\u5e38\u5e38\u7ed3\u5408\u8bd7\u8bcd\u6b4c\u8d4b\uff0c\u5f62\u6210\u4e00\u79cd\u8bd7\u60c5\u753b\u610f\u7684\u5883\u754c\u3002\u8fd9\u79cd\u878d\u5408\n\u4e0d\u4ec5\u63d0\u5347\u4e86\u753b\u4f5c\u7684\u610f\u5883\uff0c\u4e5f\u8ba9\u6b23\u8d4f\u8005\u80fd\u591f\u66f4\u597d\u5730\u9886\u7565\u5230\u5176\u4e2d\u7684\u6587\n\u5316\u5185\u6db5\u3002\n\u4ef7\u503c\u4e0e\u610f\u4e49\n\u6c34\u58a8\u753b\u4f5c\u4e3a\u4e2d\u56fd\u4f20\u7edf\u6587\u5316\u7684\u7470\u5b9d\uff0c\u4e0d\u4ec5\u662f\u4e2d\u56fd\u827a\u672f\u7684\u4ee3\u8868\uff0c\n\u66f4\u662f\u4e16\u754c\u6587\u5316\u827a\u672f\u5b9d\u5e93\u4e2d\u7684\u4e00\u9897\u7480\u74a8\u660e\u73e0\u3002\u5b83\u7684\u4ef7\u503c\u4e0d\u4ec5\u4ec5\u5728\u4e8e\n\u5176\u72ec\u7279\u7684\u827a\u672f\u98ce\u683c\u548c\u6280\u672f\u624b\u6bb5\uff0c\u66f4\u5728\u4e8e\u5176\u6240\u627f\u8f7d\u7684\u6df1\u539a\u6587\u5316\u5e95\u8574\n\u548c\u7cbe\u795e\u5185\u6db5\u3002\u901a\u8fc7\u6b23\u8d4f\u6c34\u58a8\u753b\uff0c\u4eba\u4eec\u4e0d\u4ec5\u53ef\u4ee5\u611f\u53d7\u5230\u827a\u672f\u5bb6\u5bf9\u81ea\n\u7136\u7684\u611f\u609f\u548c\u5bf9\u751f\u547d\u7684\u601d\u8003\uff0c\u4e5f\u80fd\u591f\u9886\u609f\u5230\u4e2d\u56fd\u54f2\u5b66\u7684\u667a\u6167\u548c\u9053\u5fb7\n\u51c6\u5219\u3002\u56e0\u6b64\uff0c\u4fdd\u62a4\u548c\u4f20\u627f\u6c34\u58a8\u753b\u8fd9\u4e00\u5b9d\u8d35\u7684\u6587\u5316\u9057\u4ea7\u663e\u5f97\u5c24\u4e3a\u91cd\n\u8981\u3002\n\u7ed3\u8bed\n\u7efc\u4e0a\u6240\u8ff0\uff0c\u6c34\u58a8\u753b\u4f5c\u4e3a\u4e00\u95e8\u53e4\u8001\u7684\u827a\u672f\u5f62\u5f0f\uff0c\u4e0d\u4ec5\u5177\u6709\u72ec\u7279\n\u7684\u5ba1\u7f8e\u4ef7\u503c\uff0c\u66f4\u662f\u4e2d\u56fd\u4f20\u7edf\u6587\u5316\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\u3002\u5b83\u7684\u9b45\u529b\u4e0d\u4ec5\n\u4f53\u73b0\u5728\u6280\u827a\u7684\u9ad8\u8d85\u4e0a\uff0c\u66f4\u5728\u4e8e\u5176\u80cc\u540e\u6240\u8574\u542b\u7684\u6df1\u523b\u601d\u60f3\u548c\u6587\u5316\u5185\n\u6db5\u3002\u8ba9\u6211\u4eec\u5171\u540c\u52aa\u529b\uff0c\u8ba9\u8fd9\u95e8\u53e4\u8001\u800c\u53c8\u9c9c\u6d3b\u7684\u827a\u672f\u5f62\u5f0f\u5f97\u4ee5\u4f20\u627f\n\u5e76\u53d1\u626c\u5149\u5927\u3002\nFigure 4. Interleaved composition generated by InternLM-XComposer2.\n13\nWhen the sun sets, the sky turns to dark\nAnd the tears that you shed, leave a mark\nTips\u00a0for\u00a0Visiting\u00a0Shanghai\nBut if you look up, you'll see the stars\nTwinkling in the night, like precious pearls\nThe tears that you shed, can blind your sight\nAnd you may miss the beauty that's in sight\nBut if you look up, you'll see the stars\nPlease write a English poetry based on \nRabindranath Tagore's classic poem: \"If \nyou shed tears when you miss the sun, you \nalso miss the stars.\", no less than 10 \nsentences.\nShining bright, like a beacon from afar\nSo don't be sad when the sun is gone\nAnd don't shed tears for what's done\nInstead, look up and see the stars\nThat light up the night, like shining stars\nThe tears that you shed, can be replaced\nBy the beauty of the stars, that you can embrace\nSo don't miss the sun, but look up to the stars\nAnd find the beauty that's waiting, near and far.\nPlease write a English poetry based on \nRabindranath Tagore's classic poem: \"If \nyou shed tears when you miss the sun, you \nalso miss the stars.\", no less than 10 \nsentences.\nThe mountains rise majestically,\nTheir peaks shrouded in mist and cloud,\nThe valleys below stretch far and wide,\nA tapestry of green and gold.\nThe rivers flow with gentle grace,\nTheir currents carving through the earth,\nCarrying life and beauty to all,\nA symphony of nature's worth.\nThe forests stand in solemn grandeur,\nTheir leaves rustling in the breeze,\nA living testament to time,\nA place of peace and tranquility.\nSo let us not weep for what is lost,\nBut cherish what remains,\nFor life is fleeting, but the world,\nIs a place of endless wonders and charms.\nFigure 5. Interleaved composition generated by InternLM-XComposer2.\n14\nTips\u00a0for\u00a0Visiting\u00a0Shanghai\n\u8bf7\u6839\u636e\u7ed9\u5b9a\u6807\u9898\uff1a\u201c\u718a\u732b\uff1a\u81ea\u7136\u754c\u7684\u6e29\u67d4\u4f7f\u8005\u201d\uff0c\u5199\u4e00\u7bc7\u957f\u6587\u7ae0\u3002\u9996\u5148\uff0c\u8be6\u7ec6\u4ecb\u7ecd\u718a\u732b\u7684\u57fa\u672c\u7279\u5f81\uff0c\u5982\n\u5916\u8c8c\uff0c\u4e60\u6027\u7b49\u3002\u7136\u540e\uff0c\u63a2\u8ba8\u718a\u732b\u7684\u81ea\u7136\u6816\u606f\u5730\u53ca\u5176\u9762\u4e34\u7684\u73af\u5883\u6311\u6218\uff0c\u5305\u62ec\u6816\u606f\u5730\u4e27\u5931\u548c\u6c14\u5019\u53d8\u5316\u7b49\u95ee\u9898\u3002\n\u4e4b\u540e\uff0c\u8be6\u7ec6\u8ba8\u8bba\u718a\u732b\u7684\u4fdd\u62a4\u5de5\u4f5c\u73b0\u72b6\uff0c\u5305\u62ec\u5efa\u7acb\u81ea\u7136\u4fdd\u62a4\u533a\uff0c\u5927\u718a\u732b\u7684\u4eba\u5de5\u7e41\u80b2\u7b49\u3002\u6700\u540e\uff0c\u5206\u6790\u718a\u732b\u5728\u4e2d\n\u56fd\u548c\u4e16\u754c\u8303\u56f4\u5185\u7684\u6587\u5316\u5f71\u54cd\u3002\u5b57\u6570\u4e0d\u5c11\u4e8e800\u5b57\u3002\n\u718a\u732b\uff1a\u81ea\u7136\u754c\u7684\u6e29\u67d4\u4f7f\u8005\n\u5927\u718a\u732b\uff0c\u718a\u79d1\u54fa\u4e73\u52a8\u7269\uff0c\u4f53\u80a5\u5934\u5927\u3001\u5706\u8138\u5706\u8033\uff0c\u5e7c\u5e74\u4f53\u8272\u4e3a\n\u7070\u767d\u8272\uff0c\u56e0\u5176\u72ec\u7acb\u7279\u5f81\u548c\u61a8\u6001\u53ef\u63ac\u7684\u6837\u5b50\u88ab\u4eba\u4eec\u8a89\u4e3a\u201c\u6d3b\u5316\u77f3\u201d\n\u548c\u201c\u4e2d\u56fd\u56fd\u5b9d\u201d\u3002\n\u4ece\u5916\u8868\u6765\u770b\uff0c\u5927\u718a\u732b\u4f3c\u4e4e\u662f\u81ea\u7136\u754c\u6700\u201c\u6e29\u67d4\u201d\u7684\u751f\u7269\uff1a\u9ed1\u767d\n\u76f8\u95f4\u7684\u82b1\u7eb9\u8ba9\u5b83\u7684\u5916\u5f62\u770b\u8d77\u6765\u66f4\u50cf\u662f\u4e00\u79cd\u5361\u901a\u5f62\u8c61\uff1b\u5706\u6eda\u6eda\u7684\u8eab\n\u8eaf\u548c\u5927\u5927\u7684\u5934\u90e8\u8ba9\u4eba\u5fcd\u4e0d\u4f4f\u60f3\u8981\u62b1\u4e00\u62b1\uff0c\u5373\u4f7f\u4e0d\u719f\u6089\u7684\u4eba\u4e5f\u4f1a\u56e0\n\u4e3a\u5176\u61a8\u6001\u53ef\u63ac\u7684\u5916\u8868\u800c\u5fc3\u751f\u597d\u611f\u2026\u2026\n\u4f46\u5c31\u662f\u8fd9\u6837\u4e00\u79cd\u6e29\u987a\u7684\u751f\u7269\uff0c\u5374\u4e5f\u6b63\u9762\u4e34\u7740\u79cd\u79cd\u5371\u673a\u2014\u2014\u6816\n\u606f\u5730\u4e27\u5931\u3001\u6c14\u5019\u53d8\u5316\u7b49\u56e0\u7d20\u90fd\u5bf9\u5176\u751f\u5b58\u6784\u6210\u5a01\u80c1\u3002\n\u718a\u732b\u6240\u9762\u4e34\u7684\u73af\u5883\u6311\u6218\n\u5728\u8fdc\u53e4\u65f6\u671f\uff0c\u4e2d\u56fd\u7684\u5357\u65b9\u5730\u533a\u5c31\u751f\u6d3b\u7740\u539f\u59cb\u5927\u718a\u732b\uff08\u4e5f\u53eb\u59cb\n\u718a\u732b\uff09\u3002\u5927\u7ea6\u5728\u8ddd\u4eca800\u4e07\u5e74\u524d\uff0c\u5168\u7403\u8fdb\u5165\u51b0\u671f\u65f6\u4ee3\uff0c\u5730\u7403\u5f00\u59cb\n\u53d8\u51b7\uff0c\u690d\u88ab\u9010\u6e10\u5411\u4f4e\u7eac\u5ea6\u65b9\u5411\u79fb\u52a8\uff0c\u8fd9\u4f7f\u5f97\u539f\u672c\u751f\u6d3b\u5728\u9ad8\u5bd2\u73af\u5883\n\u4e2d\u7684\u5927\u718a\u732b\u5931\u53bb\u4e86\u9002\u5b9c\u7684\u751f\u6d3b\u73af\u5883\uff0c\u56e0\u6b64\u5b83\u4eec\u88ab\u8feb\u8fc1\u79fb\u5230\u6e29\u6696\u7684\n\u5357\u65b9\u5730\u533a\u3002\u968f\u7740\u51b0\u5ddd\u8fdb\u9000\u7684\u53d8\u5316\uff0c\u5927\u718a\u732b\u4e5f\u5728\u4e0d\u65ad\u5730\u5bfb\u627e\u9002\u5408\u81ea\n\u5df1\u7684\u751f\u5b58\u73af\u5883\uff0c\u5b83\u4eec\u7684\u8db3\u8ff9\u904d\u5e03\u5168\u56fd\u5404\u5730\u3002\n\u718a\u732b\u7684\u4fdd\u62a4\u73b0\u72b6\u5982\u4f55\uff1f\n\u9488\u5bf9\u4e0a\u8ff0\u95ee\u9898\uff0c\u653f\u5e9c\u548c\u4f01\u4e1a\u76f8\u7ee7\u91c7\u53d6\u4e86\u4e00\u7cfb\u5217\u63aa\u65bd\u4fdd\u62a4\u5927\u718a\n\u732b\u53ca\u5176\u6816\u606f\u5730\u3002\u4f8b\u5982\uff0c\u5efa\u7acb\u81ea\u7136\u4fdd\u62a4\u533a\u548c\u68ee\u6797\u516c\u56ed\uff0c\u9650\u5236\u4eba\u7c7b\u6d3b\n\u52a8\u8303\u56f4\uff0c\u6539\u5584\u5927\u718a\u732b\u7684\u98df\u7269\u6765\u6e90\u7b49\u3002\u540c\u65f6\uff0c\u8fd8\u901a\u8fc7\u79d1\u5b66\u7ba1\u7406\u3001\u5408\n\u7406\u5f00\u53d1\u5229\u7528\u7b49\u65b9\u5f0f\u63a8\u52a8\u4e86\u5f53\u5730\u7ecf\u6d4e\u53d1\u5c55\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u7531\u4e8e\u5386\u53f2\u539f\n\u56e0\u548c\u81ea\u7136\u73af\u5883\u7684\u6076\u5316\u7b49\u539f\u56e0\uff0c\u91ce\u751f\u5927\u718a\u732b\u4ecd\u5904\u4e8e\u6fd2\u5371\u72b6\u6001\u3002\u6839\u636e\n\u56fd\u9645\u81ea\u7136\u4fdd\u62a4\u8054\u76df\u6700\u65b0\u53d1\u5e03\u7684\u7ea2\u8272\u540d\u5f55\uff0c\u76ee\u524d\u5168\u7403\u73b0\u5b58\u7684\u5927\u718a\u732b\n\u7ea6\u67091864\u53ea\uff0c\u5176\u4e2d\u7ea690%\u4ee5\u4e0a\u4e3a\u4e2d\u56fd\u5883\u5185\u7684\u5927\u718a\u732b\u3002\n\u9664\u4e86\u5bf9\u91ce\u5916\u73af\u5883\u8fdb\u884c\u4fdd\u62a4\u5916\uff0c\u4eba\u5de5\u7e41\u80b2\u4e5f\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u624b\u6bb5\u3002\n\u901a\u8fc7\u5b9e\u65bd\u7e41\u6b96\u8ba1\u5212\u548c\u6551\u52a9\u9879\u76ee\uff0c\u5e2e\u52a9\u66f4\u591a\u7684\u5927\u718a\u732b\u5b58\u6d3b\u4e0b\u6765\u3002\u76ee\n\u524d\u56fd\u5185\u5df2\u6709\u591a\u4e2a\u52a8\u7269\u56ed\u3001\u79d1\u7814\u673a\u6784\u5f00\u5c55\u4e86\u5927\u718a\u732b\u7684\u4eba\u5de5\u9972\u517b\u7814\u7a76\n\u5de5\u4f5c\uff0c\u8bb8\u591a\u96cc\u6027\u4e2a\u4f53\u5df2\u7ecf\u4ea7\u4e0b\u5e7c\u5d3d\u3002\u867d\u7136\u5b58\u5728\u4e00\u5b9a\u7684\u6280\u672f\u96be\u5ea6\u548c\n\u98ce\u9669\uff0c\u4f46\u662f\u968f\u7740\u7ecf\u9a8c\u7684\u79ef\u7d2f\u548c\u6280\u672f\u8fdb\u6b65\uff0c\u672a\u6765\u6709\u671b\u5b9e\u73b0\u66f4\u591a\u7684\u5927\n\u718a\u732b\u7e41\u884d\u540e\u4ee3\u3002\n\u718a\u732b\u7684\u6587\u5316\u5f71\u54cd\u529b\u5982\u4f55\u4f53\u73b0\uff1f\n\u6700\u540e\u6211\u4eec\u6765\u804a\u804a\u5927\u718a\u732b\u5728\u56fd\u9645\u4e0a\u7684\u6587\u5316\u5f71\u54cd\u529b\u3002\u8fd1\u5e74\u6765\uff0c\u968f\n\u7740\u5a92\u4f53\u62a5\u9053\u548c\u65c5\u6e38\u4e1a\u7684\u53d1\u5c55\uff0c\u8d8a\u6765\u8d8a\u591a\u7684\u4eba\u4e86\u89e3\u5230\u4e86\u8fd9\u4e2a\u53ef\u7231\u53c8\n\u73cd\u8d35\u7684\u7269\u79cd\u3002\u8bb8\u591a\u4eba\u751a\u81f3\u4e0d\u8fdc\u4e07\u91cc\u524d\u5f80\u56db\u5ddd\u5367\u9f99\u3001\u96c5\u5b89\u7b49\u5730\u7684\u81ea\n\u7136\u4fdd\u62a4\u533a\u89c2\u8d4f\u91ce\u751f\u7684\u718a\u732b\u3002\u800c\u5728\u5f71\u89c6\u4f5c\u54c1\u4e2d\uff0c\u300a\u529f\u592b\u718a\u732b\u300b\u7cfb\u5217\n\u7535\u5f71\u66f4\u662f\u5c06\u5927\u718a\u732b\u7684\u5f62\u8c61\u63a8\u5411\u4e86\u4e00\u4e2a\u5168\u65b0\u7684\u9ad8\u5ea6\u3002\u6b64\u5916\uff0c\u5404\u79cd\u4e0e\n\u5927\u718a\u732b\u76f8\u5173\u7684\u884d\u751f\u54c1\u4e5f\u8d8a\u6765\u8d8a\u53d7\u5230\u6d88\u8d39\u8005\u7684\u9752\u7750\uff0c\u6bd4\u5982\u73a9\u5177\u3001\u6587\n\u5177\u3001\u98df\u54c1\u7b49\u7b49\u3002\u53ef\u4ee5\u8bf4\uff0c\u5927\u718a\u732b\u5df2\u7ecf\u6210\u4e3a\u4e2d\u5916\u4eba\u6c11\u53cb\u597d\u4ea4\u5f80\u548c\u6587\n\u5316\u4ea4\u6d41\u7684\u6865\u6881\u4e4b\u4e00\u3002\n\u603b\u800c\u8a00\u4e4b\uff0c\u4fdd\u62a4\u597d\u5927\u718a\u732b\u8fd9\u4e00\u73cd\u8d35\u7269\u79cd\u4e0d\u4ec5\u5173\u4e4e\u5230\u6211\u56fd\u7684\u751f\n\u6001\u6587\u660e\u5efa\u8bbe\uff0c\u540c\u65f6\u4e5f\u5177\u6709\u5e7f\u6cdb\u7684\u793e\u4f1a\u610f\u4e49\u548c\u4ef7\u503c\u3002\u5e0c\u671b\u4eca\u540e\u80fd\u591f\n\u6709\u66f4\u591a\u7684\u529b\u91cf\u52a0\u5165\u5230\u8fd9\u9879\u4f1f\u5927\u7684\u4e8b\u4e1a\u4e2d\u6765\uff0c\u5171\u540c\u52aa\u529b\uff0c\u4e3a\u7ef4\u62a4\u4eba\n\u7c7b\u5171\u540c\u7684\u5bb6\u56ed\u8d21\u732e\u4e00\u4efd\u529b\u91cf\uff01\n\u718a\u732b\u7684\u81ea\u7136\u6816\u606f\u5730\u5728\u54ea\uff1f\n\u9996\u5148\u6765\u4e86\u89e3\u4e00\u4e0b\u4ec0\u4e48\u662f\u5927\u718a\u732b\u7684\u81ea\u7136\u6816\u606f\u5730\u3002\u4e00\u822c\u6765\u8bf4\uff0c\u5927\n\u718a\u732b\u751f\u6d3b\u5728\u6d77\u62d41000~3000\u7c73\u7684\u9ad8\u5c71\u6df1\u8c37\u4e2d\uff0c\u4ee5\u7af9\u5b50\u4e3a\u4e3b\u98df\uff0c\u559c\n\u9634\u6e7f\uff0c\u8010\u5bd2\u51b7\u7684\u751f\u6001\u73af\u5883\u662f\u5176\u751f\u5b58\u7684\u57fa\u7840\u6761\u4ef6\u3002\u5728\u6211\u56fd\u897f\u5357\u5730\u533a\n\u7684\u56db\u5ddd\u76c6\u5730\u3001\u9655\u897f\u5357\u90e8\u53ca\u7518\u8083\u5357\u90e8\u7684\u5cb7\u5c71\u3001\u909b\u5d03\u5c71\u3001\u5927\u5c0f\u51c9\u5c71\u3001\n\u79e6\u5cad\u7b49\u5730\u662f\u5927\u718a\u732b\u7684\u5178\u578b\u6816\u606f\u5730\u3002\u6b64\u5916\uff0c\u5728\u6211\u56fd\u7684\u6e56\u5317\u795e\u519c\u67b6\u3001\n\u6e56\u5357\u5f20\u5bb6\u754c\u4ee5\u53ca\u6cb3\u5357\u4f0f\u725b\u5c71\u7b49\u5730\u7684\u4e9a\u70ed\u5e26\u5c71\u5730\u4e5f\u6709\u53d1\u73b0\u3002\n\u968f\u7740\u7b2c\u56db\u7eaa\u51b0\u5ddd\u671f\u7684\u7ed3\u675f\uff0c\u6c14\u6e29\u5f00\u59cb\u56de\u5347\uff0c\u5317\u65b9\u68ee\u6797\u9762\u79ef\u4e0d\n\u65ad\u6269\u5927\uff0c\u8fd9\u5c31\u4e3a\u5927\u718a\u732b\u63d0\u4f9b\u4e86\u66f4\u52a0\u5e7f\u9614\u7684\u751f\u5b58\u7a7a\u95f4\u3002\u4e0d\u8fc7\u4e0e\u6b64\u540c\n\u65f6\uff0c\u4eba\u7c7b\u7684\u6d3b\u52a8\u4e5f\u5f00\u59cb\u5f71\u54cd\u5230\u5927\u718a\u732b\u7684\u751f\u5b58\u73af\u5883\u3002\u5927\u89c4\u6a21\u7684\u91c7\u4f10\n\u68ee\u6797\u5bfc\u81f4\u6816\u606f\u5730\u9762\u79ef\u5927\u5e45\u7f29\u5c0f\uff0c\u6c34\u571f\u6d41\u5931\u52a0\u5267\uff0c\u8fd9\u4e9b\u90fd\u76f4\u63a5\u6216\u95f4\n\u63a5\u5730\u5f71\u54cd\u5230\u91ce\u751f\u5927\u718a\u732b\u7684\u6570\u91cf\u3002\n\u6b64\u5916\uff0c\u6c14\u5019\u53d8\u6696\u4e5f\u662f\u5927\u718a\u732b\u76ee\u524d\u6240\u9762\u4e34\u7684\u91cd\u8981\u95ee\u9898\u4e4b\u4e00\u3002\u636e\n\u7814\u7a76\uff0c\u81ea20\u4e16\u7eaa70\u5e74\u4ee3\u4ee5\u6765\uff0c\u6211\u56fd\u4e1c\u90e8\u6cbf\u6d77\u5730\u533a\u51fa\u73b0\u4e86\u6301\u7eed\u589e\u6e29\n\u73b0\u8c61\uff0c\u8fd9\u79cd\u8d8b\u52bf\u4ecd\u5728\u7ee7\u7eed\u53d1\u5c55\u3002\u9ad8\u6e29\u4f1a\u5bfc\u81f4\u7af9\u6797\u751f\u957f\u7f13\u6162\uff0c\u4ece\u800c\n\u964d\u4f4e\u4e86\u7af9\u5b50\u4f5c\u4e3a\u5927\u718a\u732b\u4e3b\u8981\u98df\u7269\u7684\u4f9b\u5e94\u91cf\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u590f\u5b63\u964d\u96e8\n\u91cf\u589e\u591a\u8fd8\u4f1a\u9020\u6210\u6ce5\u77f3\u6d41\u3001\u6ed1\u5761\u7b49\u707e\u5bb3\uff0c\u8fdb\u4e00\u6b65\u7834\u574f\u5927\u718a\u732b\u7684\u6816\u606f\n\u5730\u3002\u8fd9\u4e9b\u56e0\u7d20\u5171\u540c\u4f5c\u7528\uff0c\u4f7f\u5f97\u5927\u718a\u732b\u7684\u79cd\u7fa4\u6570\u91cf\u65e5\u76ca\u51cf\u5c11\u3002\nFigure 6. Interleaved composition generated by InternLM-XComposer2.\n15\nTips\u00a0for\u00a0Visiting\u00a0Shanghai\n\u6839\u636e\u4ee5\u4e0b\u6807\u9898\uff1a\u201c\u82ad\u857e\u821e\uff1a\u4f18\u96c5\u4e0e\u529b\u91cf\u7684\u821e\u8e48\u827a\u672f\u201d\uff0c\u521b\u4f5c\u957f\u6587\u7ae0\uff0c\u5168\u9762\u63a2\u8ba8\u82ad\u857e\u821e\u8fd9\u4e00\u4f18\u96c5\u800c\u5bcc\u6709\u529b\u91cf\n\u7684\u821e\u8e48\u827a\u672f\u5f62\u5f0f\uff0c\u5b57\u6570\u4e0d\u5c11\u4e8e800\u5b57\u3002\u4e3b\u8981\u5185\u5bb9\u5982\u4e0b\uff1a1\u3001\u4ecb\u7ecd\u82ad\u857e\u821e\u7684\u5386\u53f2\u8d77\u6e90\u548c\u5728\u4e16\u754c\u821e\u8e48\u827a\u672f\u4e2d\u7684\u5730\n\u4f4d\u30022\u3001\u63a2\u8ba8\u82ad\u857e\u821e\u4ece\u6587\u827a\u590d\u5174\u65f6\u671f\u5230\u73b0\u4ee3\u7684\u53d1\u5c55\u8fc7\u7a0b\uff0c\u5206\u6790\u4e0d\u540c\u65f6\u671f\u548c\u56fd\u5bb6\u7684\u82ad\u857e\u821e\u98ce\u683c\u53ca\u5176\u5f71\u54cd\u30023\u3001\n\u63cf\u8ff0\u82ad\u857e\u821e\u7684\u6280\u672f\u548c\u52a8\u4f5c\u8bed\u8a00\uff0c\u4ee5\u53ca\u5176\u5728\u8868\u73b0\u529b\u548c\u60c5\u611f\u4f20\u8fbe\u4e0a\u7684\u7279\u70b9\u30024\u3001\u4ecb\u7ecd\u4e00\u4e9b\u7ecf\u5178\u7684\u82ad\u857e\u821e\u5267\u76ee\u548c\u8457\n\u540d\u821e\u8005\uff0c\u5206\u6790\u5b83\u4eec\u5bf9\u82ad\u857e\u821e\u827a\u672f\u7684\u8d21\u732e\u30025\u3001\u6700\u540e\u603b\u7ed3\u82ad\u857e\u821e\u4f5c\u4e3a\u827a\u672f\u5f62\u5f0f\u7684\u7f8e\u5b66\u548c\u6587\u5316\u4ef7\u503c\uff0c\u5e76\u5bf9\u5176\u672a\u6765\n\u7684\u53d1\u5c55\u8d8b\u52bf\u8fdb\u884c\u5c55\u671b\u3002\n\u82ad\u857e\u821e\uff1a\u4f18\u96c5\u4e0e\u529b\u91cf\u7684\u821e\u8e48\u827a\u672f\n\u821e\u8e48\uff0c\u662f\u4e00\u79cd\u4f18\u96c5\u7684\u827a\u672f\u3002\u82ad\u857e\u821e\uff0c\u66f4\u662f\u5176\u4e2d\u6700\u4f18\u96c5\u7684\u4e00\u79cd\n\u827a\u672f\u5f62\u5f0f\u3002\n\u8d77\u6e90\u4e0e\u53d1\u5c55\n\u5728\u8fdc\u53e4\u65f6\u671f\uff0c\u4e2d\u56fd\u7684\u5357\u65b9\u5730\u533a\u5c31\u751f\u6d3b\u7740\u539f\u59cb\u5927\u718a\u732b\uff08\u4e5f\u53eb\u59cb\n\u718a\u732b\uff09\u3002\u5927\u7ea6\u5728\u8ddd\u4eca800\u4e07\u5e74\u524d\uff0c\u5168\u7403\u8fdb\u5165\u51b0\u671f\u65f6\u4ee3\uff0c\u5730\u7403\u5f00\u59cb\n\u53d8\u51b7\uff0c\u690d\u88ab\u9010\u6e10\u5411\u4f4e\u7eac\u5ea6\u65b9\u5411\u79fb\u52a8\uff0c\u8fd9\u4f7f\u5f97\u539f\u672c\u751f\u6d3b\u5728\u9ad8\u5bd2\u73af\u5883\n\u4e2d\u7684\u5927\u718a\u732b\u5931\u53bb\u4e86\u9002\u5b9c\u7684\u751f\u6d3b\u73af\u5883\uff0c\u56e0\u6b64\u5b83\u4eec\u88ab\u8feb\u8fc1\u79fb\u5230\u6e29\u6696\u7684\n\u5357\u65b9\u5730\u533a\u3002\u968f\u7740\u51b0\u5ddd\u8fdb\u9000\u7684\u53d8\u5316\uff0c\u5927\u718a\u732b\u4e5f\u5728\u4e0d\u65ad\u5730\u5bfb\u627e\u9002\u5408\u81ea\n\u5df1\u7684\u751f\u5b58\u73af\u5883\uff0c\u5b83\u4eec\u7684\u8db3\u8ff9\u904d\u5e03\u5168\u56fd\u5404\u5730\u3002\n\u603b\u4e4b\uff0c\u82ad\u857e\u821e\u662f\u4e00\u95e8\u5386\u53f2\u60a0\u4e45\u3001\u6280\u672f\u7cbe\u6e5b\u3001\u5185\u6db5\u4e30\u5bcc\u7684\u821e\u8e48\n\u827a\u672f\u5f62\u5f0f\u3002\u5b83\u4e0d\u4ec5\u5c55\u793a\u4e86\u4eba\u7c7b\u7684\u8eab\u4f53\u4e4b\u7f8e\uff0c\u66f4\u8574\u542b\u7740\u6df1\u523b\u7684\u6587\u5316\n\u5185\u6db5\u548c\u54f2\u5b66\u601d\u8003\u3002\n\u603b\u4e4b\uff0c\u82ad\u857e\u821e\u65e2\u6ce8\u91cd\u6280\u672f\u53c8\u5f3a\u8c03\u60c5\u611f\uff0c\u4e8c\u8005\u7f3a\u4e00\u4e0d\u53ef\u3002\u53ea\u6709\n\u5c06\u6280\u672f\u548c\u60c5\u611f\u5b8c\u7f8e\u5730\u878d\u5408\u5728\u4e00\u8d77\uff0c\u624d\u80fd\u521b\u9020\u51fa\u771f\u6b63\u4f18\u79c0\u7684\u82ad\u857e\u821e\n\u4f5c\u54c1\u3002\n\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\uff0c\u82ad\u857e\u821e\u9010\u6e10\u6f14\u53d8\u6210\u4e86\u4e00\u79cd\u9ad8\u96c5\u7684\u5bab\u5ef7\u5a31\u4e50\n\u6d3b\u52a8\u3002\u5728\u6cd5\u56fd\u56fd\u738b\u8def\u6613\u5341\u56db\u7684\u63a8\u5d07\u4e0b\uff0c\u82ad\u857e\u821e\u6210\u4e3a\u4e86\u6b27\u6d32\u738b\u5ba4\u8d35\n\u65cf\u4eec\u6700\u4e3a\u559c\u7231\u7684\u8868\u6f14\u827a\u672f\u4e4b\u4e00\u3002\u800c\u5230\u4e86\u5341\u4e5d\u4e16\u7eaa\uff0c\u82ad\u857e\u821e\u53c8\u8fce\u6765\n\u4e86\u4e00\u4e2a\u65b0\u7684\u9ad8\u5cf0\u671f\u3002\u8fd9\u4e2a\u65f6\u671f\u51fa\u73b0\u4e86\u8bb8\u591a\u8457\u540d\u7684\u82ad\u857e\u821e\u5927\u5e08\uff0c\u5982\n\u9a6c \u91cc \u00b7 \u5fb7 \u62c9 \u4f69 \u8bfa \u666e \uff08 Marie Taglioni \uff09 \u3001 \u8d39 \u5fb7 \u5229 \u514b \u00b7 \u5854 \u56fe\n\uff08Friedrich Tarantin\uff09\u7b49\u3002\u4ed6\u4eec\u521b\u4f5c\u4e86\u8bb8\u591a\u7ecf\u5178\u7684\u4f5c\u54c1\uff0c\u63a8\u52a8\n\u4e86\u82ad\u857e\u821e\u827a\u672f\u7684\u53d1\u5c55\u3002\n\u9664\u4e86\u8fd9\u4e9b\u91cd\u8981\u7684\u5386\u53f2\u4eba\u7269\u5916\uff0c\u8fd8\u6709\u8bb8\u591a\u6770\u51fa\u7684\u82ad\u857e\u821e\u56e2\u4e3a\u82ad\n\u857e\u821e\u827a\u672f\u7684\u7e41\u8363\u505a\u51fa\u4e86\u5de8\u5927\u7684\u8d21\u732e\u3002\u4f8b\u5982\uff0c\u82f1\u56fd\u7687\u5bb6\u82ad\u857e\u821e\u56e2\u3001\n\u5df4\u9ece\u6b4c\u5267\u9662\u82ad\u857e\u821e\u56e2\u3001\u7f8e\u56fd\u82ad\u857e\u821e\u5267\u9662\u7b49\u90fd\u662f\u4eab\u8a89\u4e16\u754c\u7684\u82ad\u857e\u821e\n\u56e2\u4f53\u3002\n\u82ad\u857e\u821e\u7684\u6280\u5de7\u8981\u6c42\u975e\u5e38\u9ad8\uff0c\u52a8\u4f5c\u9700\u8981\u975e\u5e38\u7cbe\u51c6\u3001\u6d41\u7545\uff0c\u800c\u4e14\n\u529b\u5ea6\u8981\u638c\u63e1\u5f97\u6070\u5230\u597d\u5904\u3002\u6b64\u5916\uff0c\u82ad\u857e\u821e\u7684\u52a8\u4f5c\u8bed\u8a00\u4e5f\u975e\u5e38\u4e30\u5bcc\uff0c\n\u80fd\u591f\u8868\u8fbe\u51fa\u5f88\u591a\u590d\u6742\u7684\u60c5\u611f\u548c\u610f\u4e49\u3002\u6b63\u56e0\u4e3a\u5982\u6b64\uff0c\u82ad\u857e\u821e\u88ab\u8ba4\u4e3a\n\u662f\u6700\u5177\u8868\u73b0\u529b\u7684\u821e\u8e48\u4e4b\u4e00\u3002\n\u82ad\u857e\u821e\u7684\u5386\u53f2\u53ef\u4ee5\u8ffd\u6eaf\u5230\u5341\u4e94\u4e16\u7eaa\uff0c\u5f53\u65f6\u4eba\u4eec\u5728\u5e86\u795d\u80dc\u5229\u65f6\n\u4f1a \u8df3 \u821e \uff0c \u8fd9 \u79cd \u821e \u8e48 \u88ab \u79f0 \u4e3a \u201c \u5361 \u65af \u8482 \u5229 \u4e9a \u821e \u201d \uff08 Castilian\ndance\uff09\u3002\u968f\u7740\u65f6\u4ee3\u7684\u53d1\u5c55\uff0c\u4eba\u4eec\u5f00\u59cb\u5c06\u97f3\u4e50\u4e0e\u821e\u8e48\u7ed3\u5408\u8d77\u6765\uff0c\n\u4e8e\u662f\u4fbf\u8bde\u751f\u4e86\u6700\u65e9\u7684\u82ad\u857e\u821e\u5267\u300a\u5e15\u51e1\u300b\uff08Pavane\uff09\u3002\u4ece\u90a3\u65f6\u8d77\uff0c\n\u82ad\u857e\u821e\u5c31\u5f00\u59cb\u4e86\u5b83\u7684\u8f89\u714c\u5386\u53f2\u3002\n\u6280\u672f\u4e0e\u60c5\u611f\n\u82ad\u857e\u821e\u7684\u6280\u672f\u975e\u5e38\u91cd\u8981\uff0c\u56e0\u4e3a\u53ea\u6709\u638c\u63e1\u4e86\u6b63\u786e\u7684\u6280\u5de7\u624d\u80fd\u8ba9\n\u821e\u8e48\u770b\u8d77\u6765\u66f4\u52a0\u4f18\u7f8e\u3001\u6d41\u7545\u3002\u5728\u82ad\u857e\u821e\u4e2d\uff0c\u6f14\u5458\u9700\u8981\u7528\u811a\u5c16\u6765\u652f\n\u6491\u8eab\u4f53\u7684\u91cd\u91cf\uff0c\u8fd9\u9700\u8981\u6781\u9ad8\u7684\u5e73\u8861\u80fd\u529b\u548c\u67d4\u97e7\u6027\u3002\u6b64\u5916\uff0c\u82ad\u857e\u821e\n\u8fd8\u6d89\u53ca\u5230\u8df3\u8dc3\u3001\u65cb\u8f6c\u7b49\u5404\u79cd\u9ad8\u96be\u5ea6\u52a8\u4f5c\uff0c\u90fd\u9700\u8981\u5f3a\u5927\u7684\u529b\u91cf\u548c\u8010\n\u529b\u6765\u652f\u6491\u3002\u56e0\u6b64\uff0c\u8981\u60f3\u6210\u4e3a\u4e00\u540d\u4f18\u79c0\u7684\u82ad\u857e\u821e\u6f14\u5458\uff0c\u5fc5\u987b\u7ecf\u8fc7\u957f\n\u65f6\u95f4\u7684\u8bad\u7ec3\u548c\u78e8\u783a\u3002\n\u867d\u7136\u82ad\u857e\u821e\u7684\u6280\u672f\u5341\u5206\u91cd\u8981\uff0c\u4f46\u82ad\u857e\u821e\u7684\u7075\u9b42\u5374\u662f\u60c5\u611f\u3002\u901a\n\u8fc7\u821e\u8e48\u52a8\u4f5c\u7684\u4f20\u9012\uff0c\u82ad\u857e\u821e\u80fd\u591f\u8868\u8fbe\u51fa\u5404\u79cd\u590d\u6742\u7684\u60c5\u611f\u548c\u601d\u60f3\u3002\n\u6bd4\u5982\uff0c\u5728\u53e4\u5178\u82ad\u857e\u821e\u5267\u4e2d\uff0c\u5e38\u5e38\u4f1a\u51fa\u73b0\u60b2\u6b22\u79bb\u5408\u7684\u60c5\u8282\uff0c\u800c\u8fd9\u4e9b\n\u60c5\u8282\u5f80\u5f80\u662f\u901a\u8fc7\u7ec6\u817b\u7684\u821e\u8e48\u52a8\u4f5c\u6765\u5448\u73b0\u7684\u3002\u5728\u8fd9\u4e9b\u4f5c\u54c1\u4e2d\uff0c\u6f14\u5458\n\u9700\u8981\u901a\u8fc7\u8868\u60c5\u3001\u624b\u52bf\u7b49\u591a\u79cd\u65b9\u5f0f\u6765\u4f20\u8fbe\u89d2\u8272\u7684\u5185\u5fc3\u4e16\u754c\uff0c\u4ece\u800c\u4f7f\n\u89c2\u4f17\u66f4\u597d\u5730\u7406\u89e3\u5267\u60c5\u548c\u89d2\u8272\u6027\u683c\u3002\u9664\u6b64\u4e4b\u5916\uff0c\u73b0\u4ee3\u82ad\u857e\u821e\u4e5f\u8d8a\u6765\n\u8d8a\u6ce8\u91cd\u5bf9\u793e\u4f1a\u73b0\u5b9e\u95ee\u9898\u7684\u5173\u6ce8\u548c\u63a2\u8ba8\uff0c\u7528\u821e\u8e48\u6765\u547c\u5401\u4eba\u4eec\u5bf9\u67d0\u4e9b\n\u95ee\u9898\u7684\u5173\u6ce8\u548c\u53cd\u601d\u3002\n\u7ecf\u5178\u5267\u76ee\u4e0e\u8457\u540d\u821e\u8005\n\u8bf4\u5230\u82ad\u857e\u821e\u7684\u7ecf\u5178\u5267\u76ee\uff0c\u6700\u8457\u540d\u7684\u5f53\u5c5e\u300a\u5929\u9e45\u6e56\u300b\u3001\u300a\u80e1\u6843\n\u5939\u5b50\u300b\u3001\u300a\u5409\u8d5b\u5c14\u300b\u7b49\u3002\u8fd9\u4e9b\u5267\u76ee\u4e0d\u4ec5\u5177\u6709\u6781\u9ad8\u7684\u827a\u672f\u4ef7\u503c\uff0c\u800c\n\u4e14\u6df1\u53d7\u89c2\u4f17\u559c\u7231\u3002\u9664\u4e86\u8fd9\u4e9b\u4f20\u7edf\u5267\u76ee\u5916\uff0c\u73b0\u4ee3\u82ad\u857e\u821e\u4e5f\u6709\u5f88\u591a\u4f18\n\u79c0\u4e4b\u4f5c\uff0c\u6bd4\u5982\u7f8e\u56fd\u7684\u300a\u96f7\u7c73\u7ea6\u7684\u68a6\u300b\u3001\u82f1\u56fd\u7684\u300a\u7761\u7f8e\u4eba\u300b\u7b49\u7b49\u3002\n\u5f53\u7136\uff0c\u9664\u4e86\u8fd9\u4e9b\u4f18\u79c0\u7684\u4f5c\u54c1\u5916\uff0c\u8fd8\u6709\u8bb8\u591a\u8457\u540d\u7684\u82ad\u857e\u821e\u6f14\u5458\n\u4e5f\u7ed9\u4eba\u4eec\u7559\u4e0b\u4e86\u6df1\u523b\u7684\u5370\u8c61\u3002\u50cf\u7f8e\u56fd\u7684\u739b\u4e3d\u5b89\u5a1c\u00b7\u963f\u7c73\u8389\u4e9a\n\uff08 Mariana Amalia \uff09 \u3001 \u82f1 \u56fd \u7684 \u739b \u838e \u00b7 \u683c \u96f7 \u5384 \u59c6 \uff08 Martha\nGraham\uff09\u3001\u4fc4\u7f57\u65af\u7684\u5a1c\u5854\u838e\u00b7\u6885\u5c14\u79d1\u5a03\uff08Natalia Mekhonova\uff09\n\u7b49\u7b49\u3002\u5979\u4eec\u4e0d\u4ec5\u662f\u82ad\u857e\u821e\u754c\u7684\u7fd8\u695a\uff0c\u66f4\u662f\u5f71\u54cd\u4e86\u6574\u4e2a\u821e\u8e48\u754c\u7684\u4eba\n\u7269\u3002\u5979\u4eec\u7684\u821e\u8e48\u98ce\u683c\u5404\u5f02\uff0c\u6709\u7684\u70ed\u60c5\u5954\u653e\uff0c\u6709\u7684\u67d4\u7f8e\u5a49\u7ea6\uff0c\u4f46\u65e0\n\u4e00\u4f8b\u5916\u90fd\u7ed9\u89c2\u4f17\u7559\u4e0b\u4e86\u6df1\u523b\u7684\u5370\u8c61\u3002\n\u7ed3\u8bed\n\u82ad\u857e\u821e\u4f5c\u4e3a\u4e00\u95e8\u827a\u672f\u5f62\u5f0f\uff0c\u5b83\u4e0d\u4ec5\u4ec5\u4ee3\u8868\u7740\u4f18\u96c5\u548c\u7f8e\u4e3d\uff0c\u66f4\n\u8574\u542b\u7740\u6df1\u523b\u7684\u7f8e\u5b66\u548c\u6587\u5316\u4ef7\u503c\u3002\u5b83\u4e0d\u4ec5\u8ba9\u6211\u4eec\u6b23\u8d4f\u5230\u4eba\u7c7b\u8eab\u4f53\u7684\n\u6781\u81f4\u4e4b\u7f8e\uff0c\u4e5f\u8ba9\u6211\u4eec\u9886\u7565\u5230\u4eba\u7c7b\u667a\u6167\u7684\u65e0\u9650\u53ef\u80fd\u3002\u76f8\u4fe1\u5728\u672a\u6765\uff0c\n\u82ad\u857e\u821e\u5c06\u7ee7\u7eed\u53d1\u626c\u5149\u5927\uff0c\u4e3a\u4eba\u7c7b\u6587\u660e\u505a\u51fa\u66f4\u5927\u7684\u8d21\u732e\u3002\nFigure 7. Interleaved composition generated by InternLM-XComposer2.\n16\n*\u4e94\u3001\u7279\u2f8a\u7f8e\u2edd**\n\u6566\u714c\u7684\u7279\u2f8a\u7f8e\u2edd\u4e5f\u2fae\u5e38\u4e30\u5bcc\uff0c\u5176\u4e2d\u6700\u6709\u540d\u7684\u5f53\u5c5e\u9a74\u2f81\u2ee9\u2faf\u4e86\u3002\u9a74\u2f81\u2ee9\u2faf\u662f\u2f00\u9053\u4ee5\u9a74\u2f81\u4e3a\u4e3b\u8981\u2edd\u6750\u7684\u2faf\u2edd\uff0c\u5473\u9053\u9c9c\u2fb9\u53ef\u2f1d\uff0c\u6df1\u53d7\u5f53\u5730\u2f08\u548c\u6e38\u5ba2\n\u7684\u559c\u7231\u3002\u6b64\u5916\uff0c\u8fd8\u6709\u2f7a\u2f81\u6ce1\u998d\u3001\u80e1\u2f7a\u7116\u997c\u3001\u70e4\u5168\u2f7a\u7b49\u7279\u2f8a\u7f8e\u2edd\uff0c\u90fd\u662f\u4e0d\u5bb9\u9519\u8fc7\u7684\u7f8e\u5473\u4f73\u80b4\u3002\n    \u4f59\u95fb\u54c8\u5c14\u6ee8\u4e4b\u51ac\uff0c\u6781\u5bd2\u800c\u7f8e\u4e3d\u3002\u6545\u4e8e\u5c81\u672b\u4e4b\n\u9645\uff0c\u643a\u53cb\u524d\u5f80\u4e00\u63a2\u7a76\u7adf\u3002\n    \u521d\u81f3\uff0c\u89c1\u5e02\u5edb\u4e4b\u76db\uff0c\u95fe\u960e\u4e4b\u4eba\u58f0\u55a7\u54d7\u3002\u65e2\u5165\n\u591c\uff0c\u706f\u706b\u8f89\u714c\uff0c\u5982\u661f\u6597\u4e4b\u5728\u5929\u3002\u9042\u5bfb\u5f97\u4e00\u820d\u5bbf\u7109\u3002\n\u7a97\u5916\u671b\u4e4b\uff0c\u7d22\u83f2\u4e9a\u5927\u6559\u5802\u4e4b\u5029\u5f71\u4f9d\u7a00\u53ef\u89c1\uff0c\u7f8e\u4e0d\u80dc\n\u6536\u3002\n    \u6b21\u65e5\u6668\uff0c\u5929\u5fae\u660e\uff0c\u5fc3\u9a70\u7d22\u83f2\u4e9a\u3002\u53ca\u95e8\uff0c\u5dcd\u5ce8\u58ee\n\u89c2\uff0c\u53f9\u4e3a\u89c2\u6b62\u3002\u5165\u5176\u4e2d\uff0c\u5e84\u4e25\u8083\u7a46\u4e4b\u6c14\u6251\u9762\u800c\u6765\uff0c\n\u656c\u800c\u8fdc\u4e4b\u3002\n    \u7f8e\u98df\u8005\u4e5f\uff0c\u82e5\u7ea2\u80a0\u3001\u70e4\u51b7\u9762\u7b49\u8bf8\u7269\u7686\u5c5e\u6b64\u5217\u3002\n\u4f59\u5c1d\u5556\u4e4b\uff0c\u5176\u5473\u751a\u7f8e\uff0c\u8272\u9c9c\u4e14\u9999\uff0c\u771f\u4e0d\u98df\u5219\u998b\u77e3\uff01\n    \u5348\u6e38\u51b0\u96ea\u5927\u4e16\u754c\u3002\u6b64\u4e43\u4ee5\u51b0\u96ea\u4e3a\u4e3b\u9898\u7684\u6e38\u4e50\n\u573a\uff0c\u5176\u666f\u81f4\u7531\u51b0\u96d5\u5236\u4f5c\u800c\u6210\u3002\u5efa\u7b51\u3001\u4eba\u7269\u3001\u52a8\u7269\u65e0\n\u6240\u4e0d\u5305\uff0c\u4ece\u98de\u9f99\u5230\u51e4\u51f0\uff0c\u4ece\u72ee\u5b50\u5230\u4ead\u53f0\u697c\u9601\uff0c\u65e0\u4e00\n\u4e0d\u7cbe\u7f8e\u7edd\u4f26\uff0c\u5f62\u6001\u5404\u5f02\u3002\u6f2b\u6b65\u5176\u4e2d\uff0c\u5982\u5165\u7ae5\u8bdd\u4e16\n\u754c\uff0c\u8ba9\u4eba\u6d41\u8fde\u5fd8\u8fd4\u3002\n    \u591c\uff0c\u72ec\u6b65\u4e8e\u4e2d\u592e\u5927\u8857\u4e4b\u4e0a\u3002\u4e24\u8fb9\u5546\u94fa\u6797\u7acb\uff0c\u9713\n\u8679\u95ea\u70c1\uff0c\u7e41\u534e\u55a7\u56a3\u4e4b\u58f0\u4e0d\u7edd\u4e8e\u8033\u3002\u8857\u5934\u827a\u4eba\u5f39\u5531\u3001\n\u6742\u800d\u8005\u6bd4\u80a9\u63a5\u8e35\uff0c\u7686\u805a\u7cbe\u4f1a\u795e\u5730\u89c2\u6469\u6b23\u8d4f\u3002\u5176\u4e50\u878d\n\u878d\uff0c\u70ed\u95f9\u975e\u51e1\u3002\u4e0d\u7981\u8d5e\u53f9\uff1a\u54c8\u5c14\u6ee8\u771f\u4e43\u4e00\u5ea7\u5145\u6ee1\u6d3b\n\u529b\u4e0e\u9b45\u529b\u7684\u57ce\u5e02\uff01\n    \u81f3\uff0c\u5929\u5df2\u660f\u9ed1\u3002\u4f59\u501a\u7a97\u8fdc\u773a\uff0c\u661f\u5982\u94bb\u77f3\u9576\u5d4c\u4e8e\n\u9ed1\u5e03\u4e4b\u4e0a\u3002\u6d6e\u60f3\u8054\u7fe9\uff1a\u6216\u6f2b\u6b65\u8857\u5934\uff0c\u611f\u5317\u56fd\u98ce\u5149\u4e4b\n\u58ee\u4e3d\uff1b\u6216\u6e38\u4e2d\u592e\u5927\u8857\uff0c\u89c2\u6b27\u5f0f\u5efa\u7b51\u4e4b\u72ec\u7279\uff1b\u53c8\u6216\u767b\n\u4e9a\u5e03\u529b\u6ed1\u96ea\u573a\uff0c\u5c1d\u96ea\u5730\u6ed1\u9053\u4e4b\u523a\u6fc0\u3002\u7136\u6700\u4ee4\u4f59\u96be\u4ee5\n\u5fd8\u6000\u8005\uff0c\u4e43\u54c8\u5c14\u6ee8\u4eba\u6c11\u70ed\u60c5\u597d\u5ba2\u4e4b\u60c5\u8c0a\u4e5f\u3002\u6b64\u884c\u867d\n\u5df2\u7ed3\u675f\uff0c\u800c\u6b64\u6bb5\u7f8e\u597d\u56de\u5fc6\uff0c\u5fc5\u954c\u523b\u4e8e\u5fc3\u95f4\u77e3\uff01\n\u2763\u3010\u7b2c\u4e00\u7ad9\u3011\u51b0\u96ea\u5927\u4e16\u754c\n\ud83d\udcb0\u4eba\u5747\uff1a\u00a5200-300\uff0c\u63a8\u8350\u6307\u6570 \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\n\u2aef\u7968\u4ef7\u504f\u8d35\uff0c\u4f46\u4f53\u9a8c\u611f\u5f88\u68d2\uff5e\u4e00\u5b9a\u8981\u53bb\u6253\u5361\u54e6\uff01\n\ud83c\udf78\u5403\uff1a\u51b0\u96ea\u5927\u4e16\u754c\u9644\u8fd1\u6709\u5f88\u591a\u7f8e\u98df\u5c0f\u5403\uff0c\u6bd4\u5982\u70e4\u51b7\n\u9762\u3001\u9999\u80a0\u7b49\u3002\u5efa\u8bae\u5148\u4e70\u597d\u7968\uff0c\u7136\u540e\u6162\u6162\u54c1\u5c1d\u7f8e\u98df\u54e6~\n\ud83d\udeb6 \u2640\u901b\uff1a\u4e2d\u592e\u5927\u8857\u662f\u4e00\u6761\u975e\u5e38\u6709\u7279\u8272\u7684\u5546\u4e1a\u8857\uff0c\u4e24\u4fa7\u7684\n\u5efa\u7b51\u975e\u5e38\u6709\u7279\u8272\uff0c\u901b\u901b\u4e70\u4e70\u62cd\u62cd\u62cd\u90fd\u5f88\u4e0d\u9519~\n\u2763\u3010\u7b2c\u4e8c\u7ad9\u3011\u5723\u7d22\u83f2\u4e9a\u5927\u6559\u5802\n\ud83d\udcb0\u514d\u8d39\u53c2\u89c2\uff0c\u63a8\u8350\u6307\u6570 \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\n\u2aef\u8fd9\u5ea7\u6559\u5802\u662f\u54c8\u5c14\u6ee8\u7684\u6807\u5fd7\u6027\u5efa\u7b51\u4e4b\u4e00\uff0c\u975e\u5e38\u503c\u5f97\u4e00\n\u6e38\u54e6~\n\ud83c\udf78\u5403\uff1a\u5728\u5723\u7d22\u83f2\u4e9a\u5927\u6559\u5802\u9644\u8fd1\u6709\u5f88\u591a\u5c0f\u5403\u5e97\u548c\u9910\u5385\uff0c\n\u53ef\u4ee5\u5c1d\u8bd5\u4e00\u4e0b\u5f53\u5730\u7684\u7279\u8272\u7f8e\u98df\u54e6~\n\ud83d\udeb6 \u2640\u901b\uff1a\u8fd9\u91cc\u6709\u5f88\u591a\u5c0f\u5e97\u548c\u5c0f\u5df7\u5b50\uff0c\u53ef\u4ee5\u53d1\u73b0\u5f88\u591a\u6709\u8da3\n\u7684\u5c0f\u73a9\u610f\u513f\u54e6~\n\u2763\u3010\u7b2c\u4e09\u7ad9\u3011\u4e2d\u592e\u5927\u8857\n\ud83d\udcb0\u95e8\u7968\uff1a\u514d\u8d39\uff0c\u63a8\u8350\u6307\u6570 \ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\ud83c\udf1f\n\u2aef\u8fd9\u6761\u8857\u9053\u975e\u5e38\u6709\u7279\u8272\uff0c\u4e24\u65c1\u90fd\u662f\u6b27\u5f0f\u5efa\u7b51\uff0c\u975e\u5e38\u9002\n\u5408\u62cd\u7167\u54e6~\n\ud83c\udf78\u5403\uff1a\u8fd9\u91cc\u6709\u5404\u79cd\u7f8e\u98df\uff0c\u6bd4\u5982\u70e4\u51b7\u9762\u3001\u70b8\u9e21\u6392\u3001\u94c1\u677f\n\u9c7f\u9c7c\u7b49\u7b49\uff0c\u8fd8\u53ef\u4ee5\u5c1d\u8bd5\u4e00\u4e9b\u5f53\u5730\u7684\u5c0f\u5403\u54e6~\n\ud83d\udeb6 \u2640\u901b\uff1a\u4e2d\u592e\u5927\u8857\u662f\u4e00\u4e2a\u8d2d\u7269\u5929\u5802\uff0c\u6709\u5f88\u591a\u5546\u573a\u548c\u5e97\n\u94fa\uff0c\u53ef\u4ee5\u627e\u5230\u5f88\u591a\u559c\u6b22\u7684\u4e1c\u897f\u54e6~\n#\u54c8\u5c14\u6ee8\u65c5\u6e38 #\u51b0\u96ea\u5927\u4e16\u754c #\u5723\u7d22\u83f2\u4e9a\u5927\u6559\u5802 #\u4e2d\u592e\u5927\n\u8857 #\u54c8\u5c14\u6ee8\u5fc5\u73a9\u666f\u70b9 #\u54c8\u5c14\u6ee8\u65c5\u884c\u653b\u7565 #\u5357\u65b9\u5c0f\u571f\u8c46\n\u5386\u9669\u8bb0\n    \u6700\u8fd1\u5bd2\u51ac\u4e4b\u9645\uff0c\u5728\u4e92\u8054\u7f51\u4e0a\u7a81\u7136\u522e\u8d77\u4e86\u53bb\n\u54c8\u5c14\u6ee8\u65c5\u6e38\u7684\u70ed\u6f6e\u3002\u65e0\u6570\u5357\u65b9\u6e38\u5ba2\u4e89\u5148\u6050\u540e\u7684\n\u524d\u5f80\u4e2d\u56fd\u7684\u6781\u5317\u65b9-\u54c8\u5c14\u6ee8\u6e38\u73a9\u3002\u54c8\u5c14\u6ee8\u672c\u5730\u7684\n\u5c45\u6c11\u4eec\u4e5f\u8e0a\u8dc3\u53c2\u4e0e\u62db\u5f85\u6e38\u5ba2\u7684\u6d3b\u52a8\u5f53\u4e2d\uff0c\u70ed\u60c5\n\u5730\u62db\u5f85\u8fd9\u4e9b\u6765\u81ea\u5357\u65b9\u7684\u201c\u5c0f\u571f\u8c46\u201d\u4eec\u3002\u8bf7\u4f60\u4ee5\n\u7b2c\u4e00\u4eba\u79f0\uff0c\u7528\u6587\u8a00\u6587\u7684\u5f62\u5f0f\u8bb0\u5f55\u4e00\u4e2a\u5357\u65b9\u4eba\u5728\n\u51ac\u5b63\u524d\u5f80\u54c8\u5c14\u6ee8\u6e38\u73a9\u7684\u6e38\u8bb0\u3002\u8be5\u6587\u8a00\u6587\u6e38\u8bb0\u53ef\n\u80fd\u5305\u542b\uff1a\u5165\u4f4f\u9152\u5e97\uff0c\u53c2\u89c2\u7d22\u83f2\u4e9a\u5927\u6559\u5802\uff0c\u5403\u5f53\n\u5730\u7684\u7279\u8272\u7f8e\u98df\uff08\u7ea2\u80a0\uff0c\u70e4\u51b7\u9762\uff09\uff0c\u6e38\u73a9\u51b0\u96ea\u5927\n\u4e16\u754c\uff08\u51b0\u96ea\u6253\u9020\u7684\u6e38\u4e50\u573a\uff09\u7b49\u5185\u5bb9\u3002\u6587\u8a00\u6587\u5f62\n\u5f0f\u53ef\u4ee5\u53c2\u8003\u300a\u9189\u7fc1\u4ead\u8bb0\u300b\n\ud83e\uddf8\u54c8\u5c14\u6ee8\u65c5\u6e38|\u5357\u65b9\u5c0f\u571f\u8c46\u5386\u9669\u8bb0\ud83e\udde3\n    \u4ee5\u5c0f\u7ea2\u4e66\u535a\u4e3b\u7684\u53e3\u543b\uff0c\u5199\u4e00\u7bc7\u6587\u7ae0\uff0c\u6bcf\u4e00\u53e5\n\u8bdd\u542b\u6709\u9002\u5f53\u7684emoji\u8868\u60c5\u3002\u62bd\u53d63-6\u4e2aseo\u5173\u952e\u8bcd\uff0c\n\u751f\u6210#\u6807\u7b7e\u5e76\u653e\u5728\u6587\u7ae0\u6700\u540e\u3002 \u8981\u6c42\u4ee5\u201c\ud83e\uddf8\u54c8\u5c14\u6ee8\n\u65c5\u6e38|\u5357\u65b9\u5c0f\u571f\u8c46\u5386\u9669\u8bb0\ud83e\udde3\u201d\u4e3a\u6807\u9898\u5199\u4e00\u7bc7\u5c0f\u7ea2\n\u4e66\u5206\u4eab\uff0c\u4ecb\u7ecd\u5305\u62ec\u5165\u4f4f\u9152\u5e97\uff0c\u5403\u7f8e\u98df\uff0c\u901b\u666f\u70b9\u7b49\n\u5185\u5bb9\u3002\u9700\u8981\u5305\u62ec\u4ee5\u4e0b\u5185\u5bb9\uff1a\u51b0\u96ea\u5927\u4e16\u754c\uff0c\u5723\u7d22\u83f2\n\u4e9a\u5927\u6559\u5802\uff0c\u4e2d\u592e\u5927\u8857\u4ee5\u53ca\u70e4\u51b7\u9762\uff0c\u9999\u80a0\u3002\nFigure 8. Interleaved composition generated by InternLM-XComposer2.\n17\nIn summer, people will be attracted by the clear water\nswimming pool, which makes them want to dive into it.\nHowever, many people don't know how to swim or are\nafraid of the water. In fact, there is no need to be afraid.\nToday I would like to share some tips on how to learn\nswimming.\nTips\u00a0for\u00a0Visiting\u00a0Shanghai\nOne thing that most novice swimmers may not be aware of\nis the selection of swimming equipment. When they go to\nthe swimming pool, they may see other people swimming\nin different costumes and wearing different accessories.\nWhich one should you choose? The answer depends on\nyour swimming ability level.\nThe correct swimming posture is: keeping your head\nslightly above the water surface, keeping your chin down,\nextending your arms forward and placing them behind\nyour ears, rotating your shoulders backwards, keeping your\nchest high, extending your legs straightly, bending your\nknees slightly, keeping your feet pointed, and putting your\ntoes together.\nAt the beginning of swimming, we should adopt floating\nand kicking method, that is, lying flat on the water surface\nwith both hands extended forward, alternating between\nupper and lower kicks, allowing water to enter the mouth\nand nose, taking deep breaths when breathing. With\nrepeated practice, the frequency of alternating between\nupper and lower kicks will become faster and more\naccurate.\nSwimming techniques include front crawl, back crawl,\nbreaststroke, side stroke, butterfly kick, and frog kick.\nEach technique has its own advantages and disadvantages,\nand\nmastering\neach\ntechnique\nrequires\ndifferent\nmovements.\nThe\nswimming\nskills\nthat\nwe\nusually\nencounter in life include front crawl and back crawl, which\nare commonly known as \"freestyle\". We just need to learn\nthese two skills in order to enjoy swimming.\nFront crawl:\n1. Head position\nPlease write an blog based on the title: how to learn swim. \nThe first part should cover the swimming equipments.\nThe second part should discuss warm-up exercises before swimming.\nThe last part tells us how to swim, including different swimming techniques.\nFor those who can swim well but do not wear goggles, they\nwill be able to quickly find objects underwater; for\nbeginners, the choice of goggles is very important, because\nthey help us avoid hitting things when swimming, prevent\nwater from entering our eyes, reduce eye fatigue caused by\nwater resistance, and protect the eyes. Goggles suitable for\nbeginners\nhave\ngood\nsealing\nperformance\nand\nare\nrelatively comfortable to wear.\nWhen we go swimming, it's important to take a bath first to\nclean ourselves, so that we can get used to the temperature\nof the water. Before swimming, we also need to warm up\nto improve our body's flexibility, reduce the risk of injury\nduring swimming exercise, and ensure better performance\nafter swimming. This process is often ignored by many\nnovice swimmers, but if you ignore this step, you will\neasily hurt yourself while swimming.\nHead movement determines the whole stroke rhythm and\ndirection of freestyle swimming. Freestyle swimming\nconsists of two strokes per arm. The head and neck should\nalways be in line with the center of gravity, turning\nsideways to breathe evenly, and turning to the opposite\nside at the end of each stroke. The head should move\nupwards naturally when turning.\nFigure 9. Interleaved composition generated by InternLM-XComposer2.\n18\n2. Arm movement\nTips\u00a0for\u00a0Visiting\u00a0Shanghai\nArm movement is divided into two stages, namely the\npush-forward stage and the pull-back stage. The push-\nforward stage refers to the time when the forearm enters\nthe water, moves downward, and extends forward until the\nfingertips pass the hips. It is important to extend the\nforearm fully during this stage. At the same time, the\nelbows should be lowered, and the hand should stretch out\nas far as possible.\n3. Leg movement\nYellowstone National Park is a must-see destination for\nany traveler. The park's natural beauty and geothermal\nwonders are truly awe-inspiring. From the stunning Old\nFaithful geyser to the breathtaking Grand Canyon of\nYellowstone, there is no shortage of amazing sights to see\nin this national treasure.\nOne of the most popular attractions in Yellowstone is the\nOld Faithful geyser. This iconic geyser erupts every 90\nminutes, shooting water up to 180 feet into the air. It's a\ntruly spectacular sight to behold, and visitors can watch\nfrom boardwalks that circle the geyser basin.\nAnother highlight of Yellowstone is the Grand Canyon of\nYellowstone. This massive canyon is over 20 miles long\nand up to 2,000 feet deep. The canyon is home to several\nwaterfalls, including the impressive Lower Falls, which\ndrops 308 feet into the canyon below.\nLeg movement mainly includes the kick and the foot\nmovement. First, flex the knee and lift the leg to 45\ndegrees, then stretch the legs slowly, and finally use the\nheel to start the kick, and make sure that the whole body is\ncompletely horizontal before finishing the kick. In addition,\nthe calf muscle is also very important for propulsion, so\nkeep flexing your calf muscles throughout the swimming\nprocess.\n4. Breathing method\nWhen swimming, turn your head sideways to breathe in\nnatural rhythm, and look at the side about 60cm away from\nyour face. You can also exhale while swimming in shallow\nwater to control the air flow rate of the mouth and nose.\nBack crawl:\nThe main difference between back crawl and front crawl is\nthat the legs are kicked together instead of alternately. The\nback crawl is basically similar to the front crawl, except\nthat\nthe\narms\nmove\nbackward\ninstead\nof\nforward.\nTherefore, only one person uses his hands to carry the\nwhole weight of the body, and the other person is\nsupported by the palms and the back of the hands to move\nforward. As for the breathing method, you can follow the\nexample of others to breathe naturally.\nIn addition to these two main attractions, Yellowstone is\nalso\nhome\nto\nnumerous\nother\ngeothermal\nfeatures,\nincluding hot springs, mud pots, and fumaroles. These\nunique landscapes are unlike anything you'll find anywhere\nelse in the world.\nOverall,\nYellowstone\nNational\nPark\nis\na\nmust-visit\ndestination for anyone looking to experience the natural\nbeauty and geothermal wonders of the American West.\nWhether you're interested in hiking, wildlife watching, or\nsimply taking in the stunning scenery, Yellowstone has\nsomething for everyone.\nGenerate a travel diary focusing on a trip \nto Yellowstone National Park and \nrecommend Yellowstone National Park in \nEnglish. \nFigure 10. Interleaved composition generated by InternLM-XComposer2.\n19\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\u89c2\u3002\u2f7d\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\u51fa\u9635\u9635\u6e05\u8106\u7684\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\n\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\n\u8bf7\u544a\u8bc9\u6211\u56fe\u4e2d\u4ee3\u7801\u7684\u529f\u80fd\u662f\u4ec0\u4e48\uff1f\n\u8fd9\u6bb5\u4ee3\u7801\u662f Python \u8bed\u8a00\u7684\u4e00\u4e2a\u51fd\u6570\uff0c\u540d\u4e3a bubble_sort\u3002\n\u5b83\u4f7f\u7528\u4e86\u5192\u6ce1\u6392\u5e8f\u7b97\u6cd5\u6765\u5bf9\u7ed9\u5b9a\u7684\u5217\u8868\u8fdb\u884c\u6392\u5e8f\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u4e0d\u65ad\u4ea4\u6362\u76f8\u90bb\u7684\u5143\u7d20\uff0c\u5c06\u8f83\u5927\u7684\u6570\u9010\u6e10\u201c\u6d6e\u201d\u5230\u5217\n\u8868\u7684\u672b\u7aef\uff0c\u6700\u7ec8\u8fbe\u5230\u6392\u5e8f\u7684\u76ee\u7684\u3002\n\u4e0b\u9762\u662f\u51fd\u6570 bubble_sort \u7684\u4ee3\u7801\uff1a\ndef bubble_sort(arr):\nn = len(arr)\n#\u904d\u5386\u6240\u6709\u53ef\u80fd\u7684\u5143\u7d20\u5bf9\nfor i in range(n):\n#\u6700\u540e n - i - 1 \u4e2a\u5143\u7d20\u5df2\u6392\u597d\u5e8f\uff0c\u4e0d\u9700\u8981\u518d\u6bd4\u8f83\nfor j in range(0, n-i-1):\nif arr[j] > arr[j+1] :\narr[j], arr[j+1] = arr[j+1], arr[j]\nreturn arr\n# \u793a\u4f8b\nmy_list = [64, 34, 25, 12, 22, 11, 90]\nbubble_sort(my_list)\n#\u6253\u5370\u6392\u5e8f\u540e\u7684\u6570\u5b57\u5217\u8868\nprint(\", \".join(map(str, my_list)))\n\u5728\u8fd9\u4e2a\u793a\u4f8b\u4e2d\uff0cmy_list \u662f\u4e00\u4e2a\u9700\u8981\u6392\u5e8f\u7684\u6570\u5b57\u5217\u8868\u3002\n\u51fd\u6570 bubble_sort \u63a5\u6536\u4e00\u4e2a\u5217\u8868\u4f5c\u4e3a\u53c2\u6570\uff0c\u8fd4\u56de\u6392\u5e8f\u540e\u7684\u5217\u8868\u3002\n\u5728 main \u51fd\u6570\u4e2d\uff0cmy_list \u7684\u6570\u5b57\u5217\u8868\u88ab\u4f20\u9012\u7ed9\u5192\u6ce1\u6392\u5e8f\u51fd\u6570\u5e76\u6392\u5e8f\u540e\uff0c\u6253\u5370\u6392\u5e8f\u540e\u7684\u5217\u8868\u3002\n\u5f88\u597d\uff0c\u73b0\u5728\u80fd\u5e2e\u6211\u5c06\u4e0a\u9762\u4ee3\u7801\u6539\u6210\u5feb\u901f\u6392\u5e8f\u5417\uff1f\n\u5f53\u7136\u53ef\u4ee5\uff0c\u4ee5\u4e0b\u662f\u4f7f\u7528\u5feb\u901f\u6392\u5e8f\u7b97\u6cd5\u7684 Python \u4ee3\u7801\u793a\u4f8b\uff1a\ndef quick_sort(arr):\nif len(arr) <= 1:\nreturn arr\npivot = arr[len(arr)//2]\nleft = [x for x in arr if x < pivot]\nmiddle = [x for x in arr if x == pivot]\nright = [x for x in arr if x > pivot]\nreturn quick_sort(left) + middle + quick_sort(right)\n# \u793a\u4f8b\nmy_list = [64, 34, 25, 12, 22, 11, 90]\nsorted_list = quick_sort(my_list)\n#\u6253\u5370\u6392\u5e8f\u540e\u7684\u6570\u5b57\u5217\u8868\nprint(\", \".join(map(str, sorted_list)))\nFigure 11. Chat with InternLM-XComposer2.\n20\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\n\u89c2\u3002\u2f7d\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\n\u51fa\u9635\u9635\u6e05\u8106\u7684\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\n\u8bf7\u63cf\u8ff0\u8fd9\u5f20\u56fe\u7247\u3002\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\u89c2\u3002\u2f7d\n\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\u51fa\u9635\u9635\u6e05\u8106\u7684\n\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\n\u8fd9\u9053\u9898\u7684\u6570\u5b66\u7b54\u6848\u662f\u591a\u5c11\n\u6211\u770b\u5230\u4e86\u8fd9\u6837\u4e00\u5e45\u7f8e\u4e3d\u7684\u56fe\u50cf\u3002\u753b\u9762\u4e2d\uff0c\n\u4e00\u53ea\u4eba\u7684\u624b\u638c\u4f38\u4e86\u51fa\u6765\uff0c\u624b\u5fc3\u91cc\u6709\u4e00\u6761\n\u873f\u8712\u7684\u516c\u8def\u3002\u8fd9\u6761\u9053\u8def\u5728\u7eff\u8349\u5982\u8335\u7684\u5c71\n\u8c37\u4e2d\u5ef6\u4f38\uff0c\u4e00\u8def\u901a\u5f80\u8fdc\u65b9\u3002\u4e91\u96fe\u73af\u7ed5\u7740\n\u7fa4\u5c71\uff0c\u521b\u9020\u51fa\u4e00\u4e2a\u68a6\u5e7b\u822c\u7684\u73af\u5883\u3002\n\u8fd9\u5f20\u56fe\u771f\u5b9e\u5b58\u5728\u5417\uff1f\n\u6211\u5206\u6790\u8fd9\u5f20\u56fe\u50cf\uff0c\u53d1\u73b0\u5176\u4e2d\u7684\u624b\u638c\u548c\u9053\n\u8def\u662f\u4eba\u5de5\u6dfb\u52a0\u7684\u5408\u6210\u5185\u5bb9\uff0c\u800c\u5c71\u8c37\u548c\u7fa4\n\u5c71\u5e94\u8be5\u662f\u73b0\u5b9e\u751f\u6d3b\u4e2d\u5b58\u5728\u7684\u73af\u5883\u3002\u6574\u4e2a\n\u573a\u666f\u662f\u5408\u6210\u7684\uff0c\u5e76\u4e0d\u771f\u5b9e\u5b58\u5728\u3002\n\u8fd9\u9053\u6570\u5b66\u9898\u7684\u7b54\u6848\u662fA\u3001-23\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\n\u89c2\u3002\u2f7d\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\n\u51fa\u9635\u9635\u6e05\u8106\u7684\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\n\u63cf\u8ff0\u8fd9\u5f20\u56fe\u7247\u3002\n\u8fd9\u662f\u4e00\u5f20\u751f\u6001\u7cfb\u7edf\u5faa\u73af\u56fe\uff0c\u5b83\u8be6\u7ec6\u63cf\n\u7ed8\u4e86\u4e0d\u540c\u751f\u7269\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002\u751f\u6001\n\u73af\u5883\u4e2d\uff0c\u5154\u5b50\u901a\u8fc7\u98df\u7528\u690d\u7269\u8301\u58ee\u6210\u957f\u3002\n\u4f5c\u4e3a\u5154\u5b50\u7684\u5929\u654c\uff0c\u72d0\u72f8\u548c\u9e70\u5219\u901a\u8fc7\u98df\n\u7528\u5154\u5b50\u6765\u4f5c\u4e3a\u98df\u54c1\u94fe\u7684\u4e00\u90e8\u5206\u3002\u6709\u6bd2\n\u86c7\u548c\u9e70\u8fd9\u4e24\u79cd\u5927\u578b\u6355\u98df\u8005\u90fd\u6355\u98df\u8001\u9f20\uff0c\n\u5e76\u4e14\u5b83\u4eec\u81ea\u5df1\u4e5f\u5728\u81ea\u7136\u754c\u4e2d\u5bfb\u6c42\u751f\u5b58\u3002\n\u9e70\u6355\u98df\u86c7\uff0c\u6bd2\u86c7\u6355\u98df\u86d9\u3002\u690d\u7269\u548c\u5fae\u751f\n\u7269\u5728\u571f\u58e4\u4e2d\u5206\u89e3\u7caa\u4fbf\uff0c\u8fd9\u4e9b\u5206\u89e3\u7269\u5faa\n\u73af\u56de\u571f\u58e4\u4e2d\uff0c\u4f9b\u8349\u98df\u52a8\u7269\u6d88\u8017\u3002\u5728\u8fd9\n\u4e2a\u5faa\u73af\u56fe\u4e2d\uff0c\u690d\u7269\u662f\u751f\u4ea7\u8005\uff0c\u5154\u5b50\u3001\n\u6bd2\u86c7\u3001\u8001\u9f20\u548c\u72d0\u72f8\u662f\u6d88\u8d39\u8005\uff0c\u800c\u9e70\u662f\n\u98df\u7269\u94fe\u4e2d\u6700\u76f4\u63a5\u7684\u6d88\u8d39\u8005\u3002\u751f\u6001\u7cfb\u7edf\n\u4fdd\u6301\u751f\u6001\u5e73\u8861\u5e76\u7ef4\u6301\u751f\u547d\u4f53\u7684\u6301\u7eed\u5b58\n\u5728\u662f\u81f3\u5173\u91cd\u8981\u7684\u3002\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\u89c2\u3002\u2f7d\n\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\u51fa\u9635\u9635\u6e05\u8106\u7684\n\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\n\u6839\u636e\u4e0a\u9762\u7684\u4eba\u7269\u5173\u7cfb\u56fe\u56de\u7b54\uff1a\u7941\u540c\u4f1f\u662f\n\u8c01\u7684\u5b66\u751f\uff1f\n\u7941\u540c\u4f1f\u662f\u9ad8\u80b2\u826f\u7684\u5b66\u751f\u3002\n\u4faf\u4eae\u5e73\u662f\u8c01\u7684\u5b66\u751f\uff1f\n\u4faf\u4eae\u5e73\u662f\u9ad8\u80b2\u826f\u7684\u5b66\u751f\u3002\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\u89c2\u3002\u2f7d\n\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\u51fa\u9635\u9635\u6e05\u8106\u7684\n\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\n\u8fd9\u5f20\u56fe\u8ba9\u4f60\u60f3\u5230\u4e86\u54ea\u9996\u53e4\u8bd7\uff1f\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\u89c2\u3002\u2f7d\u2f49\u2f5b\n\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\u51fa\u9635\u9635\u6e05\u8106\u7684\u54cd\u58f0\uff0c\u72b9\u5982\n\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\n\u8fd9\u8ba9\u6211\u60f3\u8d77\u4e86\u67f3\u5b97\u5143\u7684\u300a\u6c5f\u96ea\u300b\uff1a\u4f17\u9e1f\n\u98de\u7edd\uff0c\u4e07\u7c41\u4ff1\u5bc2\uff1b\u552f\u72ec\u6709\u8001\u7fc1\u72ec\u9493\u5bd2\u6c5f\n\u96ea\u3002\u5728\u8fd9\u5e45\u753b\u4e2d\u4e5f\u6709\u4e00\u4eba\u5728\u72ec\u9493\u4e8e\u5bd2\u6c5f\n\u96ea\uff0c\u753b\u9762\u5145\u6ee1\u4e86\u5b64\u5bc2\u548c\u5b81\u9759\u7684\u611f\u89c9\u3002\nFigure 12. Chat with InternLM-XComposer2.\n21\n*\u4e94\u3001\u7279\u2f8a\u7f8e\u2edd**\n\u6566\u714c\u7684\u7279\u2f8a\u7f8e\u2edd\u4e5f\u2fae\u5e38\u4e30\u5bcc\uff0c\u5176\u4e2d\u6700\u6709\u540d\u7684\u5f53\u5c5e\u9a74\u2f81\u2ee9\u2faf\u4e86\u3002\u9a74\u2f81\u2ee9\u2faf\u662f\u2f00\u9053\u4ee5\u9a74\u2f81\u4e3a\u4e3b\u8981\u2edd\u6750\u7684\u2faf\u2edd\uff0c\u5473\u9053\u9c9c\u2fb9\u53ef\u2f1d\uff0c\u6df1\u53d7\u5f53\u5730\u2f08\u548c\u6e38\u5ba2\n\u7684\u559c\u7231\u3002\u6b64\u5916\uff0c\u8fd8\u6709\u2f7a\u2f81\u6ce1\u998d\u3001\u80e1\u2f7a\u7116\u997c\u3001\u70e4\u5168\u2f7a\u7b49\u7279\u2f8a\u7f8e\u2edd\uff0c\u90fd\u662f\u4e0d\u5bb9\u9519\u8fc7\u7684\u7f8e\u5473\u4f73\u80b4\u3002\n*\u4e94\u3001\u7279\u2f8a\u7f8e\u2edd**\n\u6566\u714c\u7684\u7279\u2f8a\u7f8e\u2edd\u4e5f\u2fae\u5e38\u4e30\u5bcc\uff0c\u5176\u4e2d\u6700\u6709\u540d\u7684\u5f53\u5c5e\u9a74\u2f81\u2ee9\u2faf\u4e86\u3002\u9a74\u2f81\u2ee9\u2faf\u662f\u2f00\u9053\u4ee5\u9a74\u2f81\u4e3a\u4e3b\u8981\u2edd\u6750\u7684\u2faf\u2edd\uff0c\u5473\u9053\u9c9c\u2fb9\u53ef\u2f1d\uff0c\u6df1\u53d7\u5f53\u5730\u2f08\u548c\u6e38\u5ba2\n\u7684\u559c\u7231\u3002\u6b64\u5916\uff0c\u8fd8\u6709\u2f7a\u2f81\u6ce1\u998d\u3001\u80e1\u2f7a\u7116\u997c\u3001\u70e4\u5168\u2f7a\u7b49\u7279\u2f8a\u7f8e\u2edd\uff0c\u90fd\u662f\u4e0d\u5bb9\u9519\u8fc7\u7684\u7f8e\u5473\u4f73\u80b4\u3002\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\u89c2\u3002\u2f7d\n\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\u51fa\u9635\u9635\u6e05\u8106\u7684\n\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\nGiven a circle with a diameter of 10 cm\nand two smaller circles of equal radius,\nfind the area of the shaded region in the\npicture.\nFirst, we need to find the radius of the\nlarger circle. Since the diameter is 10 cm,\nthe radius is half of the diameter, which is:\nradius = diameter / 2 = 10 cm / 2 = 5 cm\nNext, we need to find the areas of the two\nsmaller circles. The radius of each smaller\ncircle is half of the radius of the large\ncircle, which is:\nsmaller radius = radius / 2 = 5 cm / 2 =\n2.5 cm\nSo the area of each smaller circle is:\narea of smaller circle = \u03c0 * (smaller\nradius)\u00b2\narea of smaller circle = \u03c0 * (2.5 cm)\u00b2 \u2248\n19.63 cm\u00b2\nThe sum of the areas of the two smaller\ncircles is:\n2 * area of smaller circle \u2248 39.26 cm\u00b2\nFinally, we can find the area of the\nshaded region by subtracting the sum of\nthe areas of the two smaller circles from\nthe area of the large circle:\nshaded region area = area of large circle -\nsum of smaller circle areas\nshaded region area = \u03c0 * (5 cm)\u00b2 - 39.26\ncm\u00b2 \u2248 78.54 cm\u00b2 - 39.26 cm\u00b2\nshaded region area \u2248 39.28 cm\u00b2\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\u89c2\u3002\n\u2f7d\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\u51fa\u9635\u9635\u6e05\n\u8106\u7684\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\nThe first column of the image contains\ntwo diamond-shaped objects. The first\ndiamond shape is completely filled in\nwith a solid color, while the second one\nalso has an additional dot within it.\nAnalyzing this pattern, it seems that the\ntop row and bottom row contain the same\ntype of figure but with different features -\nthe top one is completely filled (possibly\nwith a different color or texture), and the\nbottom one includes a dot inside it.\nFrom this information, we can infer that\nthe missing image in the second column\nshould be similar to the leftmost image in\nthe second row\u2014a star-shaped object\nwith a solid fill. Since the other three\nimages are already present and the pattern\nsuggests\nrepetition\nof\nfigures\nwith\nvarying properties, the image in the\nsecond column will likely consist of a star\nwith a solid fill.\n{\"class\": \"D\",\n\"DLN\": \"D08954796\",\n\"DOB\": \"01/01/1974\",\n\"Name\": \"Jelani\",\n\"Address\": \"123 Main St, Phoenix, AZ\n85007\",\n\"EXP\": \"03/01/2024\",\n\"ISS\": \"03/01/2016\",\n\"SEX\": \"M\",\n\"HGT\": \"5\u201909\"\",\n\"WGT\": \"185 lb\",\n\"EYES\": \"brown\",\n\"HAIR\": \"brown\",\n\"DONOR\": \"yes\"}\nPlease read the text in this image and return the\ninformation in the following JSON format (note\nxxx is placeholder, if the information is not\navailable\nin\nthe\nimage,\nput\n\"N/A\"\ninstead).\n{\"class\": xxx, \"DLN\": xxx, \"DOB\": xxx, \"Name\":\nxxx, \"Address\": xxx, \"EXP\": xxx, \"ISS\": xxx,\n\"SEX\": xxx, \"HGT\": xxx, \"WGT\": xxx, \"EYES\":\nxxx, \"HAIR\": xxx, \"DONOR\": xxx}\nHere are three sub images arranged in a 2-by-2\nmatrix. First, look at the two images in the\nfirst column, in the top left and bottom left.\nThen, find out the pattern in the first column\nbased on the left two images. Next, use the\nfound pattern and the image in the top right to\ninfer the missing figure. Finally, describe what\nwould the missing image look like?\nFigure 13. Chat with InternLM-XComposer2.\n22\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\u89c2\u3002\u2f7d\n\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\u51fa\u9635\u9635\u6e05\u8106\u7684\n\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\nDescribe this image in detail.\nThe image is a black and white floor plan\nof a house, providing a detailed layout of\nthe rooms and their dimensions. The plan\nis oriented with the entrance on the\nbottom left and the terrace on the top left.\nStarting from the entrance, there is a\nliving room to the right, which is 14'6\" x\n20'5\" in size. Adjacent to the living room\nis the dining area, which is 9'10\" x 13'0\"\nin size. The dining area is connected to\nthe kitchen, which is 7'2\" x 9'2\" in size.\nNext to the kitchen is the second bedroom,\nwhich is 10'10\" x 14'7\" in size. The\nmaster bedroom is located to the right of\nthe second bedroom and is 12'10\" x 17'9\"\nin size. The plan also includes a foyer,\nwhich is 6'0\" x 5'3\" in size, and a\nbathroom, which is 7'2\" x 9'2\" in size.\nThe terrace, which is 21'6\" x 80\" in size,\nis located on the top left side of the plan.\nThe floor plan provides a comprehensive\nview of the house's layout, allowing one\nto understand the spatial arrangement and\ndimensions of each room.\n**\u2f06\u3001\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9**\n\u9e23\u6c99\u2f2d\u2f49\u2f5b\u6cc9\u662f\u2f00\u5904\u2f83\u7136\u5947\u89c2\uff0c\u4f4d\u4e8e\u6566\u714c\u5e02\u2ec4\u5317\u7ea640\u516c\u2fa5\u5904\u7684\u6c99\u6f20\u4e2d\u3002\u8fd9\u2fa5\u5730\u52bf\u5e73\u5766\uff0c\u6c99\u4e18\u8fde\u7ef5\u8d77\u4f0f\uff0c\u5f62\u6210\u4e86\u2f00\u2f5a\u2f34\u88a4\u2f46\u57a0\u7684\u6c99\u6f20\u666f\n\u89c2\u3002\u2f7d\u2f49\u2f5b\u6cc9\u5219\u9759\u9759\u5730\u9576\u5d4c\u5728\u8fd9\u2f5a\u6c99\u6f20\u4e4b\u4e2d\uff0c\u6cc9\u2f54\u6e05\u6f88\u2ec5\u5e95\uff0c\u5f62\u72b6\u5982\u65b0\u2f49\uff0c\u6545\u79f0\u201c\u2f49\u2f5b\u6cc9\u201d\u3002\u6bcf\u5230\u591c\u665a\uff0c\u2f49\u4eae\u5347\u8d77\u65f6\uff0c\u2f49\u2f5b\u6cc9\u5468\u56f4\u4f1a\u53d1\n\u51fa\u9635\u9635\u6e05\u8106\u7684\u54cd\u58f0\uff0c\u72b9\u5982\u5929\u7c41\u4e4b\u2fb3\uff0c\u4ee4\u2f08\u2f3c\u65f7\u795e\u6021\u3002\n| Country | Sales Volume | Revenue |\nProfit |\n|---------|--------------|---------|--------|\n| USA\n| 40.080\n| $15.971.880 |\n$3.086.421\n| China\n| 35.070\n| $15.866.670 |\n$3.032.162\n| Australia| 27.054\n| $14.812.566 |\n$2.888.636\n| India\n| 23.046\n| $10.608.174 |\n$1.853.710\n| South Korea | 16.032\n| $10.494.948 |\n$1.975.844\nPlease read the table in this image and return a \nmarkdown-style reconstructed table in text\nFigure 14. Chat with InternLM-XComposer2.\n23\n"
  },
  {
    "title": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models",
    "link": "https://arxiv.org/pdf/2401.15947.pdf",
    "upvote": "45",
    "text": "MoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nBin Lin 1 Zhenyu Tang 1 Yang Ye 2 Jiaxi Cui 3 Bin Zhu 1 Peng Jin 1 Jinfa Huang 4 Junwu Zhang 1\nMunan Ning 1 5 Li Yuan 1 5\nAbstract\nRecent advances demonstrate that scaling Large\nVision-Language Models (LVLMs) effectively im-\nproves downstream task performances. However,\nexisting scaling methods enable all model pa-\nrameters to be active for each token in the cal-\nculation, which brings massive training and in-\nferring costs. In this work, we propose a sim-\nple yet effective training strategy MoE-Tuning\nfor LVLMs. This strategy innovatively addresses\nthe common issue of performance degradation in\nmulti-modal sparsity learning, consequently con-\nstructing a sparse model with an outrageous num-\nber of parameters but a constant computational\ncost. Furthermore, we present the MoE-LLaVA,\na MoE-based sparse LVLM architecture, which\nuniquely activates only the top-k experts through\nrouters during deployment, keeping the remain-\ning experts inactive. Extensive experiments show\nthe significant performance of MoE-LLaVA in\na variety of visual understanding and object hal-\nlucination benchmarks. Remarkably, with only\napproximately 3B sparsely activated parameters,\nMoE-LLaVA demonstrates performance compa-\nrable to the LLaVA-1.5-7B on various visual\nunderstanding datasets and even surpasses the\nLLaVA-1.5-13B in object hallucination bench-\nmark. Through MoE-LLaVA, we aim to establish\na baseline for sparse LVLMs and provide valuable\ninsights for future research in developing more\nefficient and effective multi-modal learning sys-\ntems. Code is released at https://github.com/PKU-\nYuanGroup/MoE-LLaVA.\n1. Introduction\nLarge\nVision-Language\nModels\n(LVLMs),\nsuch\nas\nLLaVA (Liu et al., 2023c) and MiniGPT-4 (Zhu et al., 2023),\nhave shown promising results by leveraging an image en-\n1Peking University 2Sun Yat-sen University 3FarReel AI Lab\n4University of Rochester 5Peng Cheng Laboratory. Correspon-\ndence to: Li Yuan <yuanli-ece@pku.edu.cn>, Munan Ning <mu-\nnanning@pku.edu.cn>.\nLLaVA-Phi-2.7B\nMobileVLM-2.7B\nLLaMA-VID-7B\nLLaVA-1.5-7B\nmPLUG-Owl2-7B\nOtterHD-8B\nLLaVA-1.5-13B\nLLaMA-VID-13B\nShikra-7B\nChat-UniVi-1.5-7B\nBLIP-2-13B\nLION-12B\nInternVL-Chat-19B\nInternVL-Chat-14B\nMoE-LLaVA-1.8B\u00d74\nHallucination Average Performance\nNumber of Activated Parameters (Billions)\nFigure 1. Comparison between MoE-LLaVA-1.8B\u00d74 and open-\nsource LVLMs on object hallucination benchmark. We report\nthe average performance on the POPE (Li et al., 2023d) benchmark,\nwhich includes three subsets of Adversarial, Random, and Popular.\nThe red dashed line represents the linear fit to the data points of all\nmodels except MoE-LLaVA.\ncoder and several visual projection layers to enhance the\nvisual perception capabilities of the Large Language Models\n(LLMs). Typically, increasing the model size (Zhang et al.,\n2023a; Bai et al., 2023b) and dataset scale (Zhang et al.,\n2023c; Zhao et al., 2023a; Chen et al., 2023d) can improve\nmodel performance. For instance, InternVL (Chen et al.,\n2023e) has extended the image encoder to 6B parameters.\nA series of works (Li et al., 2022; Dai et al., 2023; Liu\net al., 2023b) have expanded the backend of LVLM to 13B\nparameters and achieved state-of-the-art performance on\ndownstream tasks. IDEFICS (Laurenc\u00b8on et al., 2023) even\ntrained an LVLM with 80B parameters. These methods have\ndemonstrated superior performance even in LLMs, which\nare typically pretrained on 34B parameters (SUSTech-IDEA,\n2023; 01-ai, 2023; FlagAI-Open, 2023) or 70B parame-\nters (Touvron et al., 2023a;b; Bai et al., 2023a; DeepSeek-\nAI, 2024; Zhang & Yang, 2023), and models surpassing\n100B parameters are common (Brown et al., 2020; Zeng\net al., 2022; Zhang et al., 2022; Scao et al., 2022; Li et al.,\n2023c; falconry, 2023) .\nIn practical applications, scaling model with high-quality\ntraining data is crucial for improving model perfor-\nmance (Lepikhin et al., 2020). However, training and de-\nploying such large models demand significant computa-\n1\narXiv:2401.15947v3  [cs.CV]  17 Feb 2024\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nCopy\nweight\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nWord Embedding Layer\nVision Encoder & MLP\nDescribe the image?\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nMoE\nLayer\nMoE Layer \u00d7 N\nThe image capture the beauty and\ngrandeur of the statue of liberty \u2026\nExpert 4\nExpert 1\nExpert 2\nExpert 3\nV1\nT1\nV1\nVn\nT1\nTn\nAttention\nConvert LLM to LVLM\nVE\nFFN\nMLP\nWEL\nTrainable parameter\nNon-trainable\n(b) Stage III\n(a) Stage I and Stage II\nFrozen in Stage I \nTrained in Stage II\nFigure 2. Illustration of MoE-Tuning. The MoE-Tuning consists\nof three stages. In stage I, only the MLP is trained. In stage II,\nall parameters are trained except for the Vision Encoder (VE). In\nstage III, FFNs are used to initialize the experts in MoE, and only\nthe MoE layers are trained. For each MoE layer, only two experts\nare activated for each token, while the other experts remain silent.\ntional costs and efficient implementation on parallel devices,\nwhich can be extremely expensive. This is because each to-\nken requires computations with all model parameters, called\nthe dense model. In contrast, sparse Mixtures of Experts\n(MoE) (Jacobs et al., 1991; Eigen et al., 2013) effectively\nscale model capacity by using fixed activated parameters to\nprocess data, which has thrived in the field of NLP (Fedus\net al., 2022; Zoph et al., 2022; Komatsuzaki et al., 2022).\nRecently, Mistral LLM (Jiang et al., 2023) equipped with the\nMoE layers has gained popularity in LLMs. Mixtral-MoE-\n8\u00d77B (Jiang et al., 2024) achieves performance comparable\nto LLaMA 2-70B with fewer computational resources.\nHowever, directly applying MoE to train sparse LVLMs is\nchallenging. We observe that simultaneously converting\nLLM to LVLM and sparsifying the model leads to signifi-\ncant performance degradation. After multiple attempts, we\nfind that proper initialization is crucial for sparsifying the\nLVLM, Therefore, we introduce a simple yet effective three-\nstage training strategy MoE-Tuning. Specifically, as shown\nin Figure 2, we first train an MLP that adapts visual tokens to\nthe LLM in stage I. Then, we pre-empower the LVLM with\na general multi-modal understanding capability by training\nthe whole LLM\u2019s parameters in stage II. Furthermore, in\nstage III we replicate the FFN as the initialization weights\nfor the experts and only train the MoE layers. Finally, the\nsparse model gradually transitions from a general LVLM\ninitialization to sparse mixture of experts.\nIn this work, we explore a baseline for the LVLM with\nmixture of experts called MoE-LLaVA, which incorporates\nmixture of experts and learnable routers. MoE-LLaVA con-\nsists of multiple sparse paths where each token is dispatched\nto different experts through the router. The activated experts\ncollectively process the tokens, while the inactive paths re-\nmain silent. By iteratively stacking MoE encoder layers,\nMoE-LLaVA provides a sparse path toward a larger and\nmore powerful LVLM.\nAs a result, in Figure 1, our MoE-LLaVA with only 2.2B\nsparse activated parameters outperforms models with simi-\nlar activated parameters and LLaVA-1.5-13B, surpassing it\nby a large margin on the POPE object hallucination bench-\nmark. Additionally, MoE-LLaVA achieves comparable per-\nformance to InternVL-Chat-19B, which has approximately\n8 times the activated parameters. We further scale MoE-\nLLaVA to 3.6B sparse activated parameters, which outper-\nform LLaVA-1.5-7B by 1.9%, 0.4%, 0.9%, 30.7%, and 3.8%\nin ScienceQA, POPE, MMBench, LLaVAW, and MM-Vet,\nrespectively. Extensive experiments validate the rationality\nof our MoE-LLaVA architecture and MoE-Tuning strategy.\nWe summarize our primary contributions as follows:\n\u2022 We explore the MoE-Tuning, a novel three-stage train-\ning strategy for adapting MoE to LVLMs and prevent-\ning the model degradation caused by sparsity.\n\u2022 We propose MoE-LLaVA, a MoE-based sparse LVLM\nframework, which significantly expands the number of\nparameters while maintaining computational costs.\n\u2022 Extensive experiments demonstrate that our MoE-\nLLaVA has excellent multi-modal understanding and\nhallucination mitigation abilities. With only approx-\nimately 3B sparse activated parameters, our method\nachieves comparable performance with SOTA 7B mod-\nels on the visual understanding benchmarks. It is worth\nnoting that MoE-LLaVA outperforms LLaVA-1.5-13B\nby 1.1% on the POPE hallucination benchmark with\n2.2B activated parameters.\n2. Related Work\n2.1. Large Vision-Language Models\nPowerful LLMs (OpenAI, 2023; Touvron et al., 2023a; Wei\net al., 2022; Touvron et al., 2023b; Zheng et al., 2023; Team,\n2023; Sun et al., 2023; Du et al., 2021; Bai et al., 2023a;\nYang et al., 2023; Penedo et al., 2023; Taori et al., 2023)\nwith strong instruction-following and generalization capa-\nbilities have been applied to LVLMs. Early works such as\nBLIP-2 (Li et al., 2023b) and FROMAGe (Koh et al., 2023)\nencoded visual signals into a sequence of visual tokens,\nsuccessfully adapting vision to LLMs through several pro-\njection layers. Subsequently, recent works have focused on\nimproving performance through methods such as expanding\nthe instruction-tuning dataset (Liu et al., 2023a;c; Zhang\net al., 2023c; Zhao et al., 2023a; Chen et al., 2023d), op-\ntimizing training strategies (Bai et al., 2023b; Chen et al.,\n2023b), increasing resolution of image (Liu et al., 2023b;\nBai et al., 2023b; Wang et al., 2023d) enhancing image en-\n2\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nFFN\nAdd & Norm\nSelf-Attention\nAdd & Norm\nMLP\nVision\nEncoder\nRouter\nAdd & Norm\nSelf-Attention\nAdd & Norm\nMLP\nVision\nEncoder\nMoE\nFFN 1\n(b) Stage II\n(c) Stage III\nImage\nInstruction            \nRequest\nResponse\nImage\nInstruction            \nRequest\nResponse\nFFN 3\nFFN E\n\u00b7\u00b7\u00b7\n\u00b7\u00b7\u00b7\nCopy weight \nfor experts\nCopy weight \nfor experts\nCopy weight \nfor experts\nCopy weight \nfor experts\nCopy weight \nfor experts\nCopy weight \nfor experts\nCopy weight \nfor experts\nCopy weight \nfor experts\nFFN\nAdd & Norm\nSelf-Attention\nAdd & Norm\nMLP\nVision\nEncoder\nImage\nCaption\nRequest\nGenerated Text Caption\n(a) Stage I\nCopy weight\nCopy weight\nCopy \nweight\nTrainable parameter\nNon-trainable\nCopy weight\nActivated forward\nNon-activated forward\nWord\nEmbedding\nWord\nEmbedding\nWord\nEmbedding\nFigure 3. Training framework and strategy. MoE-LLaVA adopts a three-stage training strategy. (a) We solely train the MLP to adapt the\nLLM to visual inputs. (b) Training the LLM backend empowers multi-modal understanding capability and MoE layers are not involved.\n(c) In this stage, we replicate the weights of the FFN to initialize each expert.\ncoders (Chen et al., 2023e; Zhang et al., 2023a; Bai et al.,\n2023b), aligning the input (Lin et al., 2023) and projection\nlayers (Cha et al., 2023; Alayrac et al., 2022; Bai et al.,\n2023b; Dai et al., 2023; Ye et al., 2023; Zhao et al., 2023a).\nThese works empowered LVLMs with powerful visual un-\nderstanding capabilities by expanding the visual instruction\nfine-tuning datasets and model scales.\nCurrently, some works have endowed LVLMs with fine-\ngrained image understanding capabilities, such as region un-\nderstanding (Chen et al., 2023c; Zhao et al., 2023b; Liu et al.,\n2023e), multi-region understanding (Wang et al., 2023c;\nPi et al., 2023; Peng et al., 2023), and pixel-wise ground-\ning (Rasheed et al., 2023; Lai et al., 2023). However, the\ncost of scaling up dense visual data and models is chal-\nlenging to bear (Liu et al., 2022; Yin et al., 2023). In this\nwork, we aim to make state-of-the-art LVLMs research more\naccessible by leveraging mixture of experts.\n2.2. Mixture of Experts in Multi-modal Learning\nMixture of Experts (MoE) (Jacobs et al., 1991; Eigen et al.,\n2013) is a hybrid model consisting of multiple sub-models,\nknown as experts, which are integrated together. The key\nconcept of MoE is the use of a router to determine the token\nset that each expert handles, thereby reducing interference\nbetween different types of samples.\nHard Routers. In the hard router mode, each expert is\ntypically pre-defined as a specific pattern. This is because\nmulti-modal data naturally exhibit gaps (Liang et al., 2022),\nmaking it difficult for soft routers to learn the optimal pat-\nterns for assigning tokens to different experts. A series of\nworks (Bao et al., 2022; Long et al., 2023; Satar et al., 2022;\nWang et al., 2022; Shen et al., 2023) naturally decouple ex-\nperts based on modal categories and pre-define each expert\nto handle a specific modality. An important feature of these\nhard-based routers is that they do not require learning the\nrouter. This mode is also widely applied in the task-specific\nMoE (Li et al., 2023e; Zhu et al., 2022; Ma et al., 2023;\nKudugunta et al., 2021).\nSoft Routers. Some works (Shazeer et al., 2017; Lep-\nikhin et al., 2020; Fedus et al., 2022; Zoph et al., 2022;\nKomatsuzaki et al., 2022) in natural language process have\nexplored the MoE based on soft routers. Soft routers en-\nable dynamic allocation of data among different experts,\nallowing each expert to focus on its expertise and achieve\nmodel sparsity. Therefore, our main focus is on leveraging\nsoft routers in the MoE. Small-scale (million-level) models\nbased on soft routers have also been explored in the context\nof multi-modal learning, such as EVE (Chen et al., 2023a)\nand LIMoE (Mustafa et al., 2022), which attempt a fusion\nof data by using soft routers. The work most relevant to ours\nis MoCLE (Gou et al., 2023). However, MoCLE clusters\ndifferent instruction sets and distributes them to different\nexperts, which compromises the flexibility and autonomy of\nthe experts. Differently, MoE-LLaVA relies on knowledge-\nrich routers to distribute tokens to different paths.\n3\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nTable 1. Architecture details of the MoE-LLaVA model. \u201cFFN Factor\u201d represents the number of linear layers in the FFN. \u201c1.6B\u00d74-Top2\u201d\nrepresents a dense foundation model with 1.6B parameters, which is equipped with a total of four experts, two of them being activated.\nName\nExperts Top-k\nMoE\nEmbedding Width Layers\nFFN\nFFN\nHeads Activated\nTotal\nLayers\nFactor\nParam\nParam\nStableLM-1.6B (Team)\n-\n-\n-\n100352\n2560\n32\n10240\n2\n32\n1.6B\n1.6B\nMoE-LLaVA-1.6B\u00d74-Top2\n4\n2\n16\n100352\n2560\n32\n10240\n2\n32\n2.0B\n2.9B\nQwen-1.8B (Bai et al., 2023a)\n-\n-\n-\n151936\n2048\n24\n5504\n3\n16\n1.8B\n1.8B\nMoE-LLaVA-1.8B\u00d74-Top2\n4\n2\n12\n151936\n2048\n24\n5504\n3\n16\n2.2B\n3.1B\nPhi2-2.7B (Microsoft, 2023)\n-\n-\n-\n51200\n2560\n32\n10240\n2\n32\n2.7B\n2.7B\nMoE-LLaVA-2.7B\u00d74-Top2\n4\n2\n16\n51200\n2560\n32\n10240\n2\n32\n3.6B\n5.3B\n3. Method\n3.1. Overview\nAs shown in Figure 3, MoE-LLaVA consists of a vision\nencoder, a visual projection layer (MLP), a word embedding\nlayer, multiple stacked LLM blocks, and MoE blocks. We\nfirst introduce the model architecture of MoE-LLaVA in\nthree stages in Section 3.2. Furthermore, in Section 3.3, we\nexplain how to train MoE-LLaVA. Finally, in Section 3.4,\nwe elaborate on the training objectives of MoE-LLaVA.\n3.2. Architecture of MoE-LLaVA\nAs shown in Table 1, we present the detailed configura-\ntion of MoE-LLaVA and more details can be found in Ap-\npendix A.1. Given a RGB image v \u2208 RH\u00d7W \u00d73, where\nH and W are the origin resolution. The vision encoder\nprocesses input images to obtain a visual token sequence\nZ = [z1, z2, \u00b7 \u00b7 \u00b7 , zP ] \u2208 RP \u00d7C, where P = H\u00d7W\n142\nrepre-\nsents the sequence length of visual tokens. A visual pro-\njection layer f is used to map Z \u2208 RP \u00d7C to V \u2208 RP \u00d7D,\nwhere D represents the hidden size of LLM. Similarly, the\ntext undergoes a word embedding layer g and is projected to\nobtain the sequence tokens T = [t1, t2, \u00b7 \u00b7 \u00b7 , tN] \u2208 RN\u00d7D,\nwhere N represents the sequence length of text tokens.\nSubsequently, we concatenate the visual tokens and text\ntokens together and feed them into a large language model.\nInstead, we solely train the visual projection layer. The large\nlanguage model consists of stacked multi-head self-attention\n(MSA) and feed-forward neural networks (FFN). Layer\nnormalization (LN) and residual connections are applied\nwithin each block (Wang et al., 2019; Baevski & Auli, 2018).\nTherefore, we formulate as:\nx0 = [v1, v2, \u00b7 \u00b7 \u00b7 , vP , \u00b7 \u00b7 \u00b7 , t1, t2, \u00b7 \u00b7 \u00b7 , tN],\n(1)\nx\u2032\n\u2113 = MSA(LN(x\u2113\u22121)) + x\u2113\u22121, \u2113 = 1 . . . L,\n(2)\nx\u2113 = MoE(LN(x\u2032\n\u2113)) + x\u2032\n\u2113, \u2113 = 1 . . . L,\n(3)\nY = LN(xL).\n(4)\nMoE Forward. Typically, a MoE layer consists of mul-\ntiple FFNs.\nAs an initialization step, we replicate the\nFFNs from stage II to form an ensemble of experts E =\n[e1, e2, \u00b7 \u00b7 \u00b7 , eE]. The router is a linear layer that predicts\nthe probability of each token being assigned to each expert.\nWe formulate as:\nP(x)i =\nef(x)i\nPE\nj ef(x)j ,\n(5)\nwhere the router produces weight logits f(x) = W \u00b7 x,\nwhich are normalized by the softmax function. The W \u2208\nRD\u00d7E represents the lightweight training parameters and E\nrepresents the number of experts. Therefore, each token is\nprocessed by the top-k experts with the highest probabilities,\nand the weighted sum is calculated based on the softmax\nresults of the probabilities:\nMoE(x) =\nk\nX\ni=1\nP(x)i \u00b7 E(x)i.\n(6)\n3.3. MoE-Tuning\nStage I: In this stage, our objective is to adapt the image\ntokens to LLM, allowing the LLM to comprehend the in-\nstances in the images. To achieve this, we employ an MLP to\nproject the image tokens into the input domain of the LLM,\ntreating the image patches as pseudo-text tokens. During\nthis stage, the LLM is trained to describe the images. MoE\nlayers are not applied to the LLM during this stage.\nStage II: Tuning with multi-modal instruction data is a key\ntechnique to enhance the capabilities and controllability of\nlarge models (Zhang et al., 2023b). In this stage, LLM is\nadjusted to become an LVLM with multi-modal understand-\ning. We use more complex instructions, including tasks\nsuch as image logical reasoning and text recognition, which\nrequire the model to have a stronger multi-modal under-\nstanding. Typically, for dense models, the LVLM training is\nconsidered complete at this stage. However, we encounter\nchallenges in simultaneously transforming the LLM into an\nLVLM and sparsifying the LVLM. Therefore, MoE-LLaVA\nutilizes the weights from the second stage as initialization\nfor the third stage to alleviate the learning difficulty of the\nsparse model.\n4\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nStage III: As an initialization, we replicate the FFN multi-\nple times to initialize the experts. When image tokens and\ntext tokens are fed into the MoE layers, the router calculates\nthe matching weights between each token and the experts.\nEach token is then processed by the top-k experts, and the\noutputs are aggregated by weighted summation based on the\nrouter\u2019s weights. When the top-k experts are activated, the\nremaining experts remain silent. This modeling approach\nforms the MoE-LLaVA with infinitely possible sparse path-\nways, offering a wide range of capabilities.\n3.4. Training Objectives\nThe Ltotal consists of auto-regressive loss Lregressive and auxil-\niary loss Laux, and auxiliary loss are scaled by the balancing\ncoefficient \u03b1:\nLtotal = Lregressive + \u03b1 \u00b7 Laux.\n(7)\nAuto-Regressive Loss. We optimize the output of LLM\nthrough a generative loss in an auto-regressive manner.\nGiven an image and text, MoE-LLaVA generates the output\nsequence Y = [y1, y2, \u00b7 \u00b7 \u00b7 , yK] \u2208 RK\u00d7D by progressively\ngenerating each element, where K = P + D represents the\nlength of the output sequence. The formula is:\nLregressive = \u2212\nN\nX\ni=1\nlog p\u03b8\n\u0010\nY[P +i] | V, T [:i\u22121]\u0011\n,\n(8)\nwhere \u03b8 is a trainable parameter and we only calculate the\nloss for the generated text.\nAuxiliary Loss. Due to the presence of multiple experts,\nit is necessary to impose load balancing constraints on the\nMoE layer. We incorporate differentiable load balancing\nloss (Fedus et al., 2022) into each MoE layer to encourage\nexperts to handle tokens in a balanced manner as follows:\nLaux = E \u00b7\nE\nX\ni=1\nFi \u00b7 Gi,\n(9)\nwhere F represents the fraction of tokens processed by each\nexpert Ei, and G represents the average routing probability\nof Ei, which can be expressed by the following formulas:\nF = 1\nK\nE\nX\ni=1\n1{argmax P(x) = i},\n(10)\nG = 1\nK\nK\nX\ni=1\nP(x)i.\n(11)\n4. Experiments\n4.1. Experimental Setup\nModel Settings. Following LLaVA 1.5 (Liu et al., 2023b),\nwe utilize CLIP-Large (Radford et al., 2021) as the vision en-\nTable 2. Composition of the data groups. For MIMIC-IT, and\nSViT datasets, we only use the LA split, and core split, respectively.\nData group\nUsage\nSource\n#Sample\nLLaVA-PT\nStage I\nLLaVA 1.5-558k\n558k\nHybird-FT\nStage II\nSViT-157k, LVIS-220k\n964k\nLRV-331k, MIMIC-IT-256k\nLLaVA-FT\nStage III\nLLaVA 1.5-mix-665k\n665k\ncoder, and the MLP consists of two linear layers with GELU\nactivation function (Hendrycks & Gimpel, 2016) between\nthem. Unless otherwise specified, MoE-LLaVA employs an\nalternating replacement of FFN with MoE layers, meaning\nthat the number of MoE layers is half of the total number\nof layers. The value of balancing coefficient \u03b1 is 0.01. We\nprovide additional training details in Appendix A.2.\nData Details. As shown in Table 2, we reorganize the\ncurrently available data for the three-stage training. For\nthe first stage of pretraining, we use the pretrained data\nof LLaVA 1.5-558k (Liu et al., 2023b). For the second\nstage, we collect datasets from MIMIC-IT (Li et al., 2023a),\nLRV (Liu et al., 2023a), SViT (Zhao et al., 2023a) and\nLVIS (Wang et al., 2023b) to provide a robust initialization\nfor MoE-LLaVA. For the third stage, we utilize the same\ndata pipeline as LLaVA-mix-665k (Liu et al., 2023b).\n4.2. Image Understanding Evaluation\nZero-shot Image Question Answering. As shown in Ta-\nble 3, since MoE-LLaVA is a sparse model equipped with\na soft router based on LVLM, we categorize the previous\nmodels as dense models. We evaluate the performance of\nMoE-LLaVA on five image question-answering benchmarks\nand report the number of activated parameters. Compared\nto the state-of-the-art method LLaVA 1.5, MoE-LLaVA\ndemonstrates powerful image understanding capabilities\nand performs very close to LLaVA-1.5 on five benchmarks.\nSpecifically, MoE-LLaVA-Phi-2.7B\u00d74 surpasses LLaVA-\n1.5-7B by 2.7% on SQAI using 3.6B sparse activated param-\neters. Notably, MoE-LLaVA-StableLM-1.6B\u00d74 achieves\ncomprehensive superiority over IDEFICS-80B with only\n2.0B activated parameters. Furthermore, we observe the re-\ncent small-scale vision-language model, LLaVA-Phi. MoE-\nLLaVA-Phi-2.7B\u00d74 outperforms LLaVA-Phi by more than\n6.2% on VQAv2, highlighting the strong comprehension\nabilities of MoE-LLaVA in natural vision.\nEvaluation under Benchmark Toolkits. To comprehen-\nsively evaluate the multi-modal understanding capabilities\nof MoE-LLaVA, we evaluate its performance on four bench-\nmark toolkits. These benchmark toolkits typically involve\nopen-ended answers, serving as tools to verify a model\u2019s\nability to engage in natural language questioning. In Ta-\n5\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nTable 3. Comparison among different LVLMs on image understanding benchmarks. \u201cRes.\u201d, \u201cAct.\u201d, \u201cL\u201d, \u201cV\u201d, \u201cS\u201d, \u201cQ\u201d, \u201cP\u201d, \u201cM\u201d\nand \u201cI\u201d respectively represent the input image resolution, activated parameters, LLaMA (Touvron et al., 2023a), Vicuna (Chiang et al.,\n2023), StableLM (Team), Qwen (Bai et al., 2023a), Phi-2 (Microsoft, 2023) MobileLLaMA (Chu et al., 2023) and IDEFICS (Laurenc\u00b8on\net al., 2023). Evaluation Benchmarks include VQA-v2 (Goyal et al., 2017); GQA (Hudson & Manning, 2019); VisWiz (Gurari et al.,\n2018); SQAI: ScienceQA-IMG (Lu et al., 2022); VQAT: TextVQA (Singh et al., 2019); POPE (Li et al., 2023d); MME (Fu et al., 2023);\nMMB: MMBench (Liu et al., 2023d); LLaVAW: LLaVA-Bench (in-the-Wild) (Liu et al., 2023c); MM-Vet (Yu et al., 2023). \u2217 donates that\nthere is some overlap in the training data. \u2020 donates that the model is trained with an image resolution of 384. The best results and second\nbest results are indicated by boldface and underline, respectively.\nMethods\nLLM\nAct. Res.\nImage Question Answering\nBenchmark Toolkit\nVQAv2 GQA VisWiz SQAI VQAT POPE MME MMB LLaVAW MM-Vet\nDense Model\nI-80B (Laurenc\u00b8on et al., 2023)\nL-65B\n65B\n224\n60.0\n45.2\n36.0\n-\n30.9\n-\n-\n54.5\n-\n-\nLLaVA-1.5 (Liu et al., 2023b)\nV-13B\n13B\n336\n80.0\u2217\n63.3\u2217\n53.6\n71.6\n61.3\n85.9\n1531.3\n67.7\n70.7\n35.4\nQwen-VL (Bai et al., 2023b)\nQ-7B\n6.7B 448\n78.8\u2217\n59.3\u2217\n35.2\n67.1\n63.8\n-\n-\n38.2\n-\n-\nLLaVA-1.5 (Liu et al., 2023b)\nV-7B\n6.7B 336\n78.5\u2217\n62.0\u2217\n50.0\n66.8\n58.2\n85.9\n1510.7\n64.3\n63.4\n30.5\nTinyGPT-V (Yuan et al., 2023) P-2.7B 2.7B 448\n-\n33.6\u2217\n33.4\n-\n-\n-\n-\n-\n-\n-\nMobileVLM (Chu et al., 2023) M-2.7B 2.7B 336\n-\n59.0\u2217\n-\n61.0\n47.5\n84.9\n1288.9\n59.6\n-\n-\nLLaVA-Phi (Zhu et al., 2024)\nP-2.7B 2.7B 336\n71.4\u2217\n-\n35.9\n68.4\n48.6\n85.0\n1335.1\n59.8\n-\n28.9\nSparse Model\nMoE-LLaVA-1.6B\u00d74-Top2\nS-1.6B 2.0B 336\n76.7\u2217\n60.3\u2217\n36.2\n62.6\n50.1\n85.7\n1318.2\n60.2\n86.8\n26.9\nMoE-LLaVA-1.8B\u00d74-Top2\nQ-1.8B 2.2B 336\n76.2\u2217\n61.5\u2217\n32.6\n63.1\n48.0\n87.0\n1291.6\n59.7\n88.7\n25.3\nMoE-LLaVA-2.7B\u00d74-Top2\nP-2.7B 3.6B 336\n77.6\u2217\n61.4\u2217\n43.9\n68.5\n51.4\n86.3\n1423.0\n65.2\n94.1\n34.3\nMoE-LLaVA-1.6B\u00d74-Top2\u2020\nS-1.6B 2.0B 384\n78.6\u2217\n61.5\u2217\n40.5\n63.9\n54.3\n85.9\n1335.7\n63.3\n90.3\n32.3\nMoE-LLaVA-2.7B\u00d74-Top2\u2020\nP-2.7B 3.6B 384\n79.9\u2217\n62.6\u2217\n43.7\n70.3\n57.0\n85.7\n1431.3\n68.0\n97.3\n35.9\nTable 4. Zero-shot object hallucination evaluation results. \u201cYes\u201d indicates the proportion of positive responses to the given question.\nMethods\nLLM\nActivated\nAdersarial\nPopular\nRandom\nAcc\nF1-Score\nYes\nAcc\nF1-Score\nYes\nAcc\nF1-Score\nYes\nDense Model\nmPLUG-Owl (Ye et al., 2023)\nL-7B\n6.7B\n82.4\n81.6\n45.2\n85.5\n84.3\n42.1\n86.3\n85.3\n42.3\nMM-GPT (Gong et al., 2023)\nL-7B\n6.7B\n50.0\n66.7\n100.0\n50.0\n66.7\n100.0\n50.0\n66.7\n100.0\nLLaVA-1.5 (Liu et al., 2023b)\nV-13B\n13B\n85.5\n84.4\n43.3\n87.4\n86.2\n41.3\n88.0\n87.1\n41.7\nSparse Model\nMoE-LLaVA-1.6B\u00d74-Top2\nS-1.6B\n2.0B\n86.9\n85.7\n41.7\n85.3\n84.2\n43.5\n88.0\n87.1\n41.6\nMoE-LLaVA-1.8B\u00d74-Top2\nQ-1.8B\n2.2B\n86.1\n85.4\n44.9\n88.6\n87.7\n42.5\n88.7\n88.0\n43.0\nMoE-LLaVA-2.7B\u00d74-Top2\nP-2.7B\n3.6B\n85.9\n84.9\n43.2\n87.5\n86.4\n41.8\n88.5\n87.7\n41.8\nMoE-LLaVA-1.6B\u00d74-Top2\u2020\nS-1.6B\n2.0B\n86.9\n85.6\n41.5\n85.7\n84.6\n43.0\n88.4\n87.5\n41.5\nMoE-LLaVA-2.7B\u00d74-Top2\u2020\nP-2.7B\n3.6B\n85.5\n84.2\n41.9\n86.7\n84.4\n41.7\n87.9\n86.9\n40.6\nble 3, MoE-LLaVA-Qwen-1.8B\u00d74 surpasses Qwen-VL-7B\nby 21.5%, on MMBench, despite the latter utilizing higher\nimage resolutions. These results collectively demonstrate\nthat the sparse model MoE-LLaVA achieves comparable\nor even superior performance to dense models with fewer\nactivated parameters.\n4.3. Object Hallucination Evaluation\nWe adopt the evaluation pipeline of POPE (Li et al., 2023d),\na polling-based query method, to evaluate object hallucina-\ntion in MoE-LLaVA. The results are presented in Table 4,\nwhere MoE-LLaVA exhibits the best performance, indicat-\ning that MoE-LLaVA tends to generate objects consistent\nwith the given image. Specifically, MoE-LLaVA-1.8B\u00d74\nsurpasses LLaVA-1.5-13B by 1.0%, 1.5%, and 0.8% in ad-\nversarial sampling, popular sampling, and random sampling,\nrespectively, with 2.2B activated parameters. Additionally,\nwe observe that the yes ratio of MoE-LLaVA remains rela-\ntively balanced, indicating that our sparse model is capable\nof providing accurate feedback based on the given questions.\n4.4. Quantitative Analysis\nRouting Distributions. In Figure 4, we present the ex-\npert loads (leftmost plot) and the modalities preferences of\ndifferent experts (four subplots on the right) through MoE-\nLLaVA-2.7B\u00d74-Top2 on ScienceQA. More visualization\ncan be found in Appendix B.3. To begin with, the expert\nloads in all MoE layers are totally balanced. However, as\n6\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\nFigure 4. Distribution of expert loadings. The discontinuous lines represent a perfectly balanced distribution of tokens among different\nexperts or modalities. The first figure on the left illustrates the workload among experts, while the remaining four figures depict the\npreferences of experts towards different modalities.\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\nFigure 5. Distribution of modalities across different experts.\nInterrupted lines mean a perfectly balanced distribution of tokens.\nthe model gradually becomes sparser, the expert 3 loads for\nlayers 17 to 27 suddenly increase, and they even dominate\nthe workload of almost all tokens. For the shallow layers\n(5-11), experts 2, 3, and 4 mainly collaborate. It is worth\nnoting that expert 1 only works predominantly in the first\nfew layers, and as the model becomes deeper, expert 1 grad-\nually withdraws from the workload. Therefore, the experts\nin MoE-LLaVA have learned a certain pattern that allows\nthem to divide their tasks in a specific manner.\nFurthermore, we show the distribution of modalities across\ndifferent experts in Figure 5. Similarly, experts develop\ntheir own preferences. Additionally, we find that the rout-\ning distributions for text and image are highly similar. For\nexample, when expert 3 is actively working in layers 17-27,\nthe proportions of text and image that MoE-LLaVA pro-\ncesses are similar. Each expert in MoE-LLaVA is capable of\nhandling both text tokens and image tokens simultaneously,\nwhich demonstrates that MoE-LLaVA does not exhibit a\nclear preference for any modality. This serves as evidence\nof its strong interaction in multimodal learning.\nToken Pathways. Furthermore, we examine the behavior of\nexperts at the token level. More visualization can be found\nin Appendix B.4 and Appendix B.5. We track the trajec-\ntories of all tokens on downstream tasks. For all activated\npathways, we employ PCA (Pearson, 1901) to obtain the\ntop-10 pathways, as shown in Figure 6. We found that for\na given unseen text token or image tokens, MoE-LLaVA\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\nFigure 6. Visualization of activated pathways. We highlight the\ntop-10 activated pathways on the text and image. Among them,\nthe colorful paths represent the top-2 paths for text and image,\nrespectively, while the gray paths represent the remaining 8 paths.\nTable 5. Ablation study about different training strategies. \u201cLA\u201d\nand \u201cHb\u201d represent LLaVA-FT and Hybrid-FT in Table 2.\nMoE Stage II Stage III GQA SQAI POPE LLaVAW\n(a)\n\u2714\n-\nLV+Hb\n58.4\n58.1\n81.9\n88.0\n(b)\n\u2714\nHb\nLV\n61.5\n63.1\n87.0\n88.7\n(c)\n\u2717\nLV+Hb\n-\n60.9\n60.2\n86.4\n86.3\n(d)\n\u2717\nHb\nLV\n60.9\n62.5\n86.9\n90.1\nconsistently tends to assign experts 2 and 3 to handle them\nin the deeper layers of the model. Regarding experts 1 and\n4, they tend to handle the tokens during the initialization\nphase. These findings contribute to a better understanding\nof the behavior of sparse models in multi-modal learning.\n4.5. Ablation Study\nIn this section, we first validate the necessity of the three-\nstage training strategy. We then explore the impact of differ-\nent base models and conduct ablation studies on the number\nof experts and active experts, and the MoE structure. We\nprovide additional results in Appendix B.2.\nEffect of Training Strategy. In Table 5, we conduct three\nvariant experiments to demonstrate the rationale behind us-\ning the second-stage instruction tuning as the initialization\nfor the third-stage MoE tuning. When adapting MoE to\n7\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nTable 6. Ablation study about training setting and architecture design decisions. Settings for results in Table 3 and Table 4 are\nhighlighted in blue . We report the training time on 8 V100-32G.\n(a) Tuning the parameters of different subsets.\nSubset\nGQA\nVisWiz\nVQAT\nPOPE\nLLaVAW\nTime\nFFN\n61.5\n32.6\n48.0\n87.0\n88.7\n20h\nAll\n61.3\n31.9\n47.6\n87.0\n88.1\n27h\n(b) The number of experts.\nExperts\nGQA\nSQAI\nVQAT\nPOPE\nLLaVAW\nTime\n1\n60.9\n60.2\n48.3\n86.4\n86.3\n13h\n2\n61.2\n60.8\n47.0\n87.5\n86.5\n14h\n(c) The value of top-k.\nTop-k\nVQAv2\nGQA\nSQAI\nVQAT\nPOPE\nTime\n1\n74.5\n58.4\n58.0\n44.0\n85.7\n19h\n2\n76.2\n61.5\n63.1\n48.0\n88.7\n20h\n(d) The architectures of MoE-LLaVA.\nArchitecture\nVQAv2\nGQA SQAI\nVQAT\nPOPE\nTime\nFirst-Half\n75.9\n61.3\n62.4\n47.0\n86.9\n20h\nSecond-Half\n76.3\n61.2\n62.6\n47.2\n86.9\n20h\nInterval\n76.2\n61.5\n63.1\n48.0\n88.7\n20h\nAll\n74.5\n61.5\n62.1\n47.1\n87.0\n32h\nLVLMs, a straightforward approach is to replace the clas-\nsic LLaVA\u2019s FFN with a MoE layer and train it according\nto the original second-stage script, denoted as variant (a).\nHowever, variant (a) performs the worst, suggesting that\nthe current multi-modal instruction dataset is insufficient to\nsupport both the conversion from LLM to LVLM and the\nconversion from LVLM to a sparse model simultaneously.\nTherefore, we collect more data, referred to as Hybrid-FT,\nand initially convert LLM to LVLM in the second stage. Sub-\nsequently, in the third stage, LVLM is sparsified by using\nthe LLaVA-FT dataset, resulting in variant (b). Additionally,\nwe expand the data of the original LLaVA\u2019s second stage for\nfair comparison, denoted as variant (c). The results indicate\nthat variants (b) outperformed variants (a) and (c). These\nfindings demonstrate that providing a reasonable LVLM\ninitialization allows the model to transition rapidly from\na dense model to a sparse model, validating the principle\nbehind our three-stage training strategy.\nEffect of Tuning the Parameters of Different Subsets.\nIn Table 6a, we examine the performance of fine-tuning\ndifferent parts of the parameters. \u201cFFN\u201d represents fine-\ntuning all FFN layers and MoE layers in the model. \u201cAll\u201d\nindicates fine-tuning all parameters. The results indicate\ntuning the FFN is sufficient to achieve results comparable\nto full-parameter tuning, but it requires only approximately\n75% of the time. Therefore, to enhance generalization and\nreduce training costs, we only fine-tune FFN layers.\nEffect of the Number of Experts. Typically, increasing the\nnumber of experts directly leads to higher performance (Lep-\nikhin et al., 2020; Fedus et al., 2022). In Table 6b, we change\nthe number of experts while keeping the number of activated\nexperts the same, so the number of activated parameters for\nboth models remains the same. More sparse experts outper-\nform the single expert dense model by 1.1% on POPE and\n0.6% on SQAI, respectively. The results demonstrate that\nsparse experts can deliver superior performance.\nEffect of the Number of Activated Experts. To evaluate\nTable 7. Ablation study about the model size of MoE-LLaVA.\nModel\nMoE\nVQAv2\nSQAI\nVQAT\nMMB LLaVAW\nStableLM\n\u2717\n74.5\n62.0\n48.8\n58.2\n83.2\n\u2714\n76.7\n62.6\n50.1\n60.2\n86.8\nQwen\n\u2717\n74.9\n60.2\n48.3\n60.6\n86.3\n\u2714\n76.2\n63.1\n48.0\n59.7\n88.7\nPhi-2\n\u2717\n75.6\n67.8\n50.0\n65.0\n91.3\n\u2714\n77.6\n68.5\n51.4\n65.2\n94.1\nthe effect of the number of activated experts, we compare\nthe performance of using different top-k strategies. With\nthe number of activated experts changing from 1 to 2, it\nbrings a significant improvement with only 1h training time\nincreasing. These results show that activating more experts\ncan improve the MOE-LLaVA ability. To leverage the ad-\nvantages of the MoE scheme, we set the number of activated\nexperts to 2.\nEffect of the Architectures. In Table 6d, we explore four\nvariations of MoE architecture. Specifically, \u201cFirst-Half\u201d\nindicates that MoE layers are applied only to the first half of\nthe model while the second half retains the original dense\narchitecture. \u201cSecond-Half\u201d means that MoE layers are\nplaced in the second half of the model while the first half\nremains dense. \u201cInterval\u201d represents alternating occurrences\nof MoE layers and dense layers. \u201cAll\u201d indicates that all\nlayers are sparse MoE layers. Intuitively, it is expected that\nincorporating all MoE will enhance performance. However,\nusing \u201cAll\u201d does not yield better results and results in longer\ntraining times compared to other architectures. Therefore,\nMoE-LLaVA alternates the insertion of MoE layers.\nEffect of the Model Size. As shown in Table 7, we compare\nthe performance of models with different parameter sizes as\nthe foundation models for MoE-LLaVA. For smaller models\nsuch as Phi2-MoE and Qwen-MoE, the performance with\nMoE surpasses that of dense models. We provide additional\nresults in Appendix B.1.\n8\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n5. Conclusion and Future Directions\nIn this work, we propose the MoE-Tuning to adapting the\nMoE architecture to LVLMs, and construct the MoE-based\nspare model MoE-LLaVA, which can find a sparse pathway\nby simultaneously handling image and text features. Our\nframework demonstrates strong ability of multi-modal un-\nderstanding and rich potential for hallucination inhibition,\nachieving comparable performance of LLaVA-1.5-7B with\nonly 3B activated parameters.\nWhile MoE-LLaVA demonstrates competitive capabilities,\nwe observe some difficulties in training stability, particularly\nwith 16-bit float precision. Furthermore, due to the presence\nof multiple experts specializing in different abilities, MoE-\nLLaVA can easily be expanded to handle additional tasks\nsuch as detection, segmentation, generation, or handling\nmore modalities such as video, depth, and thermal.\n9\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nImpact Statements\nBroader Impacts\nWhile MoE-LLaVA holds great potential and application\nvalue in multi-modal understanding, it may also have some\nnegative social impacts:\n\u2022 Information credibility: MoE-LLaVA can generate re-\nalistic texts, including false information and misleading\ncontent.\n\u2022 Bias and discrimination: The training data for MoE-\nLLaVA often comes from the internet, where various bi-\nases and discriminatory content may exist. If these un-\nequal patterns are learned and amplified by the model,\nthey may be reflected in the generated responses.\n\u2022 Social influence: People may become overly reliant on\nMoE-LLaVA for information and problem-solving, in-\nstead of actively thinking and seeking multiple sources\nof information. This can lead to increased dependency,\nreduced autonomy in thinking, and judgment skills.\nReproducibility\nIn Appendix A.2, we have provided a detailed list of all the\ntraining hyperparameters. We have open-sourced all models\nand codes. Reproducibility can be achieved by using the\ncode provided in the materials.\nCompute\nFor the main results, we conducte experiments on 8 A800-\n80G. For the ablation study, we measure the time on 8\nV100-32G.\nLicenses\nThe majority of this project is released under the Apache\n2.0 license.\n\u2022 The\nservice\nis\na\nresearch\npreview\nintended\nfor\nnon-commercial\nuse\nonly,\nsubject\nto\nthe\nmodel\nLicense\nof\nLLaMA\n(https:\n//github.com/facebookresearch/llama/\nblob/main/MODEL_CARD.md).\n\u2022 Terms of Use of the data generated by Ope-\nnAI\n(https://openai.com/policies/\nterms-of-use).\n\u2022 Privacy\nPractices\nof\nShareGPT\n(https:\n//chrome.google.com/webstore/\ndetail/sharegpt-share-your-chatg/\ndaiacboceoaocpibfodeljbdfacokfjb).\nReferences\n01-ai. Building the next generation of open-source and\nbilingual llms. https://github.com/01-ai/Yi,\n2023.\nAlayrac, J.-B., Donahue, J., Luc, P., Miech, A., Barr, I.,\nHasson, Y., Lenc, K., Mensch, A., Millican, K., Reynolds,\nM., et al. Flamingo: a visual language model for few-shot\nlearning. Advances in Neural Information Processing\nSystems, 35:23716\u201323736, 2022.\nBaevski, A. and Auli, M.\nAdaptive input representa-\ntions for neural language modeling.\narXiv preprint\narXiv:1809.10853, 2018.\nBai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan,\nY., Ge, W., Han, Y., Huang, F., et al. Qwen technical\nreport. arXiv preprint arXiv:2309.16609, 2023a.\nBai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J.,\nZhou, C., and Zhou, J. Qwen-vl: A frontier large vision-\nlanguage model with versatile abilities. arXiv preprint\narXiv:2308.12966, 2023b.\nBao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O. K.,\nAggarwal, K., Som, S., Piao, S., and Wei, F. Vlmo:\nUnified vision-language pre-training with mixture-of-\nmodality-experts. Advances in Neural Information Pro-\ncessing Systems, 35:32897\u201332912, 2022.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,\nDhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,\nAskell, A., et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:\n1877\u20131901, 2020.\nCha, J., Kang, W., Mun, J., and Roh, B. Honeybee: Locality-\nenhanced projector for multimodal llm. arXiv preprint\narXiv:2312.06742, 2023.\nChen, J., Guo, L., Sun, J., Shao, S., Yuan, Z., Lin, L., and\nZhang, D. Eve: Efficient vision-language pre-training\nwith masked prediction and modality-aware moe. arXiv\npreprint arXiv:2308.11971, 2023a.\nChen, J., Zhu, D., Shen, X., Li, X., Liu, Z., Zhang, P., Krish-\nnamoorthi, R., Chandra, V., Xiong, Y., and Elhoseiny, M.\nMinigpt-v2: large language model as a unified interface\nfor vision-language multi-task learning. arXiv preprint\narXiv:2310.09478, 2023b.\nChen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., and\nZhao, R. Shikra: Unleashing multimodal llm\u2019s referential\ndialogue magic. arXiv preprint arXiv:2306.15195, 2023c.\nChen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J.,\nZhao, F., and Lin, D.\nSharegpt4v: Improving large\nmulti-modal models with better captions. arXiv preprint\narXiv:2311.12793, 2023d.\n10\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nChen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing,\nS., Muyan, Z., Zhang, Q., Zhu, X., Lu, L., et al. In-\nternvl: Scaling up vision foundation models and align-\ning for generic visual-linguistic tasks. arXiv preprint\narXiv:2312.14238, 2023e.\nChiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang,\nH., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E.,\net al. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023), 2023.\nChu, X., Qiao, L., Lin, X., Xu, S., Yang, Y., Hu, Y., Wei,\nF., Zhang, X., Zhang, B., Wei, X., et al. Mobilevlm: A\nfast, reproducible and strong vision language assistant for\nmobile devices. arXiv preprint arXiv:2312.16886, 2023.\nDai, W., Li, J., Li, D., Tiong, A. M. H., Zhao, J., Wang,\nW., Li, B., Fung, P., and Hoi, S. Instructblip: Towards\ngeneral-purpose vision-language models with instruction\ntuning, 2023.\nDeepSeek-AI.\nDeepseek llm:\nScaling open-source\nlanguage models with longtermism.\narXiv preprint\narXiv:2401.02954, 2024.\nDu, Z., Qian, Y., Liu, X., Ding, M., Qiu, J., Yang, Z.,\nand Tang, J. Glm: General language model pretrain-\ning with autoregressive blank infilling. arXiv preprint\narXiv:2103.10360, 2021.\nEigen, D., Ranzato, M., and Sutskever, I. Learning fac-\ntored representations in a deep mixture of experts. arXiv\npreprint arXiv:1312.4314, 2013.\nfalconry.\nFalcon-180b.\nhttps://falconllm.tii.\nae/, 2023.\nFedus, W., Zoph, B., and Shazeer, N. Switch transform-\ners: Scaling to trillion parameter models with simple\nand efficient sparsity. The Journal of Machine Learning\nResearch, 23(1):5232\u20135270, 2022.\nFlagAI-Open. Aquila2-34b. https://github.com/\nFlagAI-Open/Aquila2, 2023.\nFu, C., Chen, P., Shen, Y., Qin, Y., Zhang, M., Lin, X., Yang,\nJ., Zheng, X., Li, K., Sun, X., Wu, Y., and Ji, R. Mme:\nA comprehensive evaluation benchmark for multimodal\nlarge language models. arXiv preprint arXiv:2306.13394,\n2023.\nGong, T., Lyu, C., Zhang, S., Wang, Y., Zheng, M., Zhao, Q.,\nLiu, K., Zhang, W., Luo, P., and Chen, K. Multimodal-gpt:\nA vision and language model for dialogue with humans.\narXiv preprint arXiv:2305.04790, 2023.\nGou, Y., Liu, Z., Chen, K., Hong, L., Xu, H., Li, A., Yeung,\nD.-Y., Kwok, J. T., and Zhang, Y. Mixture of cluster-\nconditional lora experts for vision-language instruction\ntuning. arXiv preprint arXiv:2312.12379, 2023.\nGoyal, Y., Khot, T., Summers-Stay, D., Batra, D., and\nParikh, D. Making the v in vqa matter: Elevating the\nrole of image understanding in visual question answer-\ning. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 6904\u20136913, 2017.\nGurari, D., Li, Q., Stangl, A. J., Guo, A., Lin, C., Grauman,\nK., Luo, J., and Bigham, J. P. Vizwiz grand challenge:\nAnswering visual questions from blind people. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, pp. 3608\u20133617, 2018.\nHendrycks, D. and Gimpel, K. Gaussian error linear units\n(gelus). arXiv preprint arXiv:1606.08415, 2016.\nHudson, D. A. and Manning, C. D. Gqa: A new dataset for\nreal-world visual reasoning and compositional question\nanswering. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 6700\u2013\n6709, 2019.\nJacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E.\nAdaptive mixtures of local experts. Neural computation,\n3(1):79\u201387, 1991.\nJiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\nChaplot, D. S., de las Casas, D., Bressand, F., Lengyel,\nG., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-\nA., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix,\nT., and Sayed, W. E. Mistral 7b, 2023.\nJiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary,\nB., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna,\nE. B., Bressand, F., Lengyel, G., Bour, G., Lample, G.,\nLavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P.,\nSubramanian, S., Yang, S., Antoniak, S., Scao, T. L.,\nGervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed,\nW. E. Mixtral of experts, 2024.\nKoh, J. Y., Salakhutdinov, R., and Fried, D. Grounding\nlanguage models to images for multimodal generation.\narXiv preprint arXiv:2301.13823, 2023.\nKomatsuzaki, A., Puigcerver, J., Lee-Thorp, J., Ruiz,\nC. R., Mustafa, B., Ainslie, J., Tay, Y., Dehghani, M.,\nand Houlsby, N. Sparse upcycling: Training mixture-\nof-experts from dense checkpoints.\narXiv preprint\narXiv:2212.05055, 2022.\nKudugunta, S., Huang, Y., Bapna, A., Krikun, M., Lepikhin,\nD., Luong, M.-T., and Firat, O. Beyond distillation: Task-\nlevel mixture-of-experts for efficient inference. arXiv\npreprint arXiv:2110.03742, 2021.\n11\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nLai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., and\nJia, J. Lisa: Reasoning segmentation via large language\nmodel. arXiv preprint arXiv:2308.00692, 2023.\nLaurenc\u00b8on, H., Saulnier, L., Tronchon, L., Bekman, S.,\nSingh, A., Lozhkov, A., Wang, T., Karamcheti, S., Rush,\nA. M., Kiela, D., Cord, M., and Sanh, V. Obelics: An\nopen web-scale filtered dataset of interleaved image-text\ndocuments, 2023.\nLepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y.,\nKrikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling\ngiant models with conditional computation and automatic\nsharding. arXiv preprint arXiv:2006.16668, 2020.\nLi, B., Zhang, Y., Chen, L., Wang, J., Pu, F., Yang, J., Li, C.,\nand Liu, Z. Mimic-it: Multi-modal in-context instruction\ntuning. arXiv preprint arXiv:2306.05425, 2023a.\nLi, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping\nlanguage-image pre-training for unified vision-language\nunderstanding and generation. In International Confer-\nence on Machine Learning, pp. 12888\u201312900. PMLR,\n2022.\nLi, J., Li, D., Savarese, S., and Hoi, S.\nBlip-2: Boot-\nstrapping language-image pre-training with frozen im-\nage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023b.\nLi, X., Yao, Y., Jiang, X., Fang, X., Meng, X., Fan, S., Han,\nP., Li, J., Du, L., Qin, B., et al. Flm-101b: An open llm\nand how to train it with 100 k budget. arXiv preprint\narXiv:2309.03852, 2023c.\nLi, Y., Du, Y., Zhou, K., Wang, J., Zhao, W. X., and Wen,\nJ.-R.\nEvaluating object hallucination in large vision-\nlanguage models.\narXiv preprint arXiv:2305.10355,\n2023d.\nLi, Y., Hui, B., Yin, Z., Yang, M., Huang, F., and Li, Y.\nPace: Unified multi-modal dialogue pre-training with\nprogressive and compositional experts. arXiv preprint\narXiv:2305.14839, 2023e.\nLiang, V. W., Zhang, Y., Kwon, Y., Yeung, S., and Zou,\nJ. Y. Mind the gap: Understanding the modality gap\nin multi-modal contrastive representation learning. Ad-\nvances in Neural Information Processing Systems, 35:\n17612\u201317625, 2022.\nLin, B., Zhu, B., Ye, Y., Ning, M., Jin, P., and Yuan,\nL.\nVideo-llava:\nLearning united visual representa-\ntion by alignment before projection.\narXiv preprint\narXiv:2311.10122, 2023.\nLiu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., and Wang, L.\nAligning large multi-modal model with robust instruction\ntuning. arXiv preprint arXiv:2306.14565, 2023a.\nLiu, H., Li, C., Li, Y., and Lee, Y. J.\nImproved base-\nlines with visual instruction tuning.\narXiv preprint\narXiv:2310.03744, 2023b.\nLiu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction\ntuning. arXiv preprint arXiv:2304.08485, 2023c.\nLiu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W.,\nYuan, Y., Wang, J., He, C., Liu, Z., et al. Mmbench: Is\nyour multi-modal model an all-around player?\narXiv\npreprint arXiv:2307.06281, 2023d.\nLiu, Z., Hu, H., Lin, Y., Yao, Z., Xie, Z., Wei, Y., Ning, J.,\nCao, Y., Zhang, Z., Dong, L., et al. Swin transformer v2:\nScaling up capacity and resolution. In Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pp. 12009\u201312019, 2022.\nLiu, Z., He, Y., Wang, W., Wang, W., Wang, Y., Chen, S.,\nZhang, Q., Lai, Z., Yang, Y., Li, Q., et al. Interngpt:\nSolving vision-centric tasks by interacting with chatgpt\nbeyond language. arXiv preprint arXiv:2305.05662, 3,\n2023e.\nLong, Z., Killick, G., McCreadie, R., and Camarasa, G. A.\nMultiway-adapater: Adapting large-scale multi-modal\nmodels for scalable image-text retrieval. arXiv preprint\narXiv:2309.01516, 2023.\nLu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.-W., Zhu,\nS.-C., Tafjord, O., Clark, P., and Kalyan, A. Learn to\nexplain: Multimodal reasoning via thought chains for\nscience question answering. Advances in Neural Infor-\nmation Processing Systems, 35:2507\u20132521, 2022.\nMa, G., Wu, X., Wang, P., and Hu, S. Cot-mote: Explor-\ning contextual masked auto-encoder pre-training with\nmixture-of-textual-experts for passage retrieval. arXiv\npreprint arXiv:2304.10195, 2023.\nMicrosoft.\nPhi-2:\nThe\nsurprising\npower\nof\nsmall\nlanguage\nmodels.\nhttps://www.\nmicrosoft.com/en-us/research/blog/\nphi-2-the-surprising-power-of-small-language-mode\n2023.\nMustafa, B., Riquelme, C., Puigcerver, J., Jenatton, R., and\nHoulsby, N. Multimodal contrastive learning with limoe:\nthe language-image mixture of experts.\nAdvances in\nNeural Information Processing Systems, 35:9564\u20139576,\n2022.\nOpenAI. Gpt-4 technical report, 2023.\nPearson, K. Liii. on lines and planes of closest fit to systems\nof points in space. The London, Edinburgh, and Dublin\nphilosophical magazine and journal of science, 2(11):\n559\u2013572, 1901.\n12\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nPenedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cap-\npelli, A., Alobeidli, H., Pannier, B., Almazrouei, E., and\nLaunay, J. The refinedweb dataset for falcon llm: out-\nperforming curated corpora with web data, and web data\nonly. arXiv preprint arXiv:2306.01116, 2023.\nPeng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma,\nS., and Wei, F.\nKosmos-2: Grounding multimodal\nlarge language models to the world.\narXiv preprint\narXiv:2306.14824, 2023.\nPi, R., Gao, J., Diao, S., Pan, R., Dong, H., Zhang, J.,\nYao, L., Han, J., Xu, H., and Zhang, L. K. T. Detgpt:\nDetect what you need via reasoning.\narXiv preprint\narXiv:2305.14167, 2023.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pp. 8748\u20138763. PMLR, 2021.\nRasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S.,\nCholakkal, H., Anwer, R. M., Xing, E., Yang, M.-H., and\nKhan, F. S. Glamm: Pixel grounding large multimodal\nmodel. arXiv preprint arXiv:2311.03356, 2023.\nRiquelme, C., Puigcerver, J., Mustafa, B., Neumann, M.,\nJenatton, R., Susano Pinto, A., Keysers, D., and Houlsby,\nN. Scaling vision with sparse mixture of experts. Ad-\nvances in Neural Information Processing Systems, 34:\n8583\u20138595, 2021.\nSatar, B., Zhu, H., Zhang, H., and Lim, J. H. Rome: Role-\naware mixture-of-expert transformer for text-to-video re-\ntrieval. arXiv preprint arXiv:2206.12845, 2022.\nScao, T. L., Fan, A., Akiki, C., Pavlick, E., Ili\u00b4c, S., Hesslow,\nD., Castagn\u00b4e, R., Luccioni, A. S., Yvon, F., Gall\u00b4e, M.,\net al. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100, 2022.\nShazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le,\nQ., Hinton, G., and Dean, J. Outrageously large neural\nnetworks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538, 2017.\nShen, S., Yao, Z., Li, C., Darrell, T., Keutzer, K., and He, Y.\nScaling vision-language models with sparse mixture of\nexperts. arXiv preprint arXiv:2303.07226, 2023.\nSingh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X.,\nBatra, D., Parikh, D., and Rohrbach, M. Towards vqa\nmodels that can read. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npp. 8317\u20138326, 2019.\nSun, T., Zhang, X., He, Z., Li, P., Cheng, Q., Yan, H., Liu,\nX., Shao, Y., Tang, Q., Zhao, X., et al. Moss: Train-\ning conversational language models from synthetic data.\narXiv preprint arXiv:2307.15020, 7, 2023.\nSUSTech-IDEA.\nSus-chat:\nInstruction tuning done\nright.\nhttps://github.com/SUSTech-IDEA/\nSUS-Chat, 2023.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,\nGuestrin, C., Liang, P., and Hashimoto, T. B. Alpaca: A\nstrong, replicable instruction-following model. Stanford\nCenter for Research on Foundation Models. https://crfm.\nstanford. edu/2023/03/13/alpaca. html, 3(6):7, 2023.\nTeam, I. Internlm: A multilingual language model with\nprogressively enhanced capabilities, 2023.\nTeam,\nS.\nA.\nL.\nStable\nlm\n2\n1.6b.\nURL\n[https://huggingface.co/stabilityai/\nstablelm-2-1.6b](https://huggingface.\nco/stabilityai/stablelm-2-1.6b).\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\nM.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\nAzhar, F., et al. Llama: Open and efficient foundation lan-\nguage models. arXiv preprint arXiv:2302.13971, 2023a.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., et al. Llama 2: Open foundation and fine-\ntuned chat models. arXiv preprint arXiv:2307.09288,\n2023b.\nWang, G., Cheng, S., Zhan, X., Li, X., Song, S., and Liu, Y.\nOpenchat: Advancing open-source language models with\nmixed-quality data. arXiv preprint arXiv:2309.11235,\n2023a.\nWang, J., Meng, L., Weng, Z., He, B., Wu, Z., and Jiang,\nY.-G. To see is to believe: Prompting gpt-4v for better vi-\nsual instruction tuning. arXiv preprint arXiv:2311.07574,\n2023b.\nWang, Q., Li, B., Xiao, T., Zhu, J., Li, C., Wong, D. F.,\nand Chao, L. S. Learning deep transformer models for\nmachine translation. arXiv preprint arXiv:1906.01787,\n2019.\nWang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu,\nQ., Aggarwal, K., Mohammed, O. K., Singhal, S., Som,\nS., et al. Image as a foreign language: Beit pretraining\nfor all vision and vision-language tasks. arXiv preprint\narXiv:2208.10442, 2022.\nWang, W., Chen, Z., Chen, X., Wu, J., Zhu, X., Zeng, G.,\nLuo, P., Lu, T., Zhou, J., Qiao, Y., et al. Visionllm: Large\nlanguage model is also an open-ended decoder for vision-\ncentric tasks. arXiv preprint arXiv:2305.11175, 2023c.\n13\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nWang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji,\nJ., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual\nexpert for pretrained language models. arXiv preprint\narXiv:2311.03079, 2023d.\nWei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,\nChi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought\nprompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:\n24824\u201324837, 2022.\nYang, A., Xiao, B., Wang, B., Zhang, B., Bian, C., Yin,\nC., Lv, C., Pan, D., Wang, D., Yan, D., et al. Baichuan\n2: Open large-scale language models. arXiv preprint\narXiv:2309.10305, 2023.\nYe, Q., Xu, H., Xu, G., Ye, J., Yan, M., Zhou, Y., Wang, J.,\nHu, A., Shi, P., Shi, Y., et al. mplug-owl: Modulariza-\ntion empowers large language models with multimodality.\narXiv preprint arXiv:2304.14178, 2023.\nYin, S., Fu, C., Zhao, S., Li, K., Sun, X., Xu, T., and Chen,\nE. A survey on multimodal large language models. arXiv\npreprint arXiv:2306.13549, 2023.\nYu, W., Yang, Z., Li, L., Wang, J., Lin, K., Liu, Z., Wang,\nX., and Wang, L.\nMm-vet: Evaluating large multi-\nmodal models for integrated capabilities. arXiv preprint\narXiv:2308.02490, 2023.\nYuan, Z., Li, Z., and Sun, L. Tinygpt-v: Efficient multi-\nmodal large language model via small backbones. arXiv\npreprint arXiv:2312.16862, 2023.\nZeng, A., Liu, X., Du, Z., Wang, Z., Lai, H., Ding, M.,\nYang, Z., Xu, Y., Zheng, W., Xia, X., et al. Glm-130b:\nAn open bilingual pre-trained model.\narXiv preprint\narXiv:2210.02414, 2022.\nZhang, P., Wang, X. D. B., Cao, Y., Xu, C., Ouyang, L.,\nZhao, Z., Ding, S., Zhang, S., Duan, H., Yan, H., et al.\nInternlm-xcomposer: A vision-language large model for\nadvanced text-image comprehension and composition.\narXiv preprint arXiv:2309.15112, 2023a.\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M.,\nChen, S., Dewan, C., Diab, M., Li, X., Lin, X. V.,\net al. Opt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068, 2022.\nZhang, S., Dong, L., Li, X., Zhang, S., Sun, X., Wang, S.,\nLi, J., Hu, R., Zhang, T., Wu, F., et al. Instruction tuning\nfor large language models: A survey. arXiv preprint\narXiv:2308.10792, 2023b.\nZhang, X. and Yang, Q. Xuanyuan 2.0: A large chinese\nfinancial chat model with hundreds of billions parameters.\nIn Proceedings of the 32nd ACM International Confer-\nence on Information and Knowledge Management, pp.\n4435\u20134439, 2023.\nZhang, Y., Zhang, R., Gu, J., Zhou, Y., Lipka, N., Yang,\nD., and Sun, T. Llavar: Enhanced visual instruction\ntuning for text-rich image understanding. arXiv preprint\narXiv:2306.17107, 2023c.\nZhao, B., Wu, B., and Huang, T.\nSvit: Scaling up vi-\nsual instruction tuning. arXiv preprint arXiv:2307.04087,\n2023a.\nZhao, Y., Lin, Z., Zhou, D., Huang, Z., Feng, J., and Kang,\nB. Bubogpt: Enabling visual grounding in multi-modal\nllms. arXiv preprint arXiv:2307.08581, 2023b.\nZheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z.,\nZhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al. Judging\nllm-as-a-judge with mt-bench and chatbot arena. arXiv\npreprint arXiv:2306.05685, 2023.\nZhu, D., Chen, J., Shen, X., Li, X., and Elhoseiny, M.\nMinigpt-4: Enhancing vision-language understanding\nwith advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\nZhu, J., Zhu, X., Wang, W., Wang, X., Li, H., Wang, X.,\nand Dai, J. Uni-perceiver-moe: Learning sparse gener-\nalist models with conditional moes. Advances in Neural\nInformation Processing Systems, 35:2664\u20132678, 2022.\nZhu, Y., Zhu, M., Liu, N., Ou, Z., Mou, X., and Tang,\nJ. Llava-phi: Efficient multi-modal assistant with small\nlanguage model, 2024.\nZoph, B., Bello, I., Kumar, S., Du, N., Huang, Y., Dean,\nJ., Shazeer, N., and Fedus, W. St-moe: Designing stable\nand transferable sparse expert models. arXiv preprint\narXiv:2202.08906, 2022.\n14\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nAppendix for MoE-LLaVA\nA. Implementation Details\nA.1. More Model Architecture\nIn Table 8, we present additional variants of the MoE-LLaVA. We introduce how the total parameters is calculated. When\nthe number of activated experts is 2, setting Experts = 2 yields the number of activated parameters.\nTotal Parameters =Embedding \u00b7 Width\n+ Layers \u00b7 (4 \u00b7 Width \u00b7 Width + Width \u00b7 FFN \u00b7 FFN Factor + 2 \u00b7 Width)\n+ Width + Width \u00b7 Embedding\n+ MoE Layers \u00b7 (Experts \u2212 1) \u00b7 (Width \u00b7 FFN \u00b7 FFN Factor + 2 \u00b7 Width)\n+ MoE Layers \u00b7 (Width \u00b7 Experts)\n(12)\nTable 8. More architecture details of the MoE-LLaVA model. \u201cFFN Factor\u201c represents the number of linear layers in the FFN. \u201c*\u201d\ndonates the dimension of the hidden states for the keys (k) and values (v) is 1024. \u201c1.6B\u00d74-Top2\u201d represents a dense foundation model\nwith 1.6B parameters, which will be equipped with a total of four experts, with two of them being activated. \u201c\u2020\u201d donates all layers will\nequipped with MoE layer.\nName\nExperts Top-k\nMoE\nEmbedding Width Layers\nFFN\nFFN\nHeads Activated\nTotal\nLayers\nFactor\nParam\nParam\nStableLM-1.6B (Team)\n-\n-\n-\n100352\n2560\n32\n10240\n2\n32\n1.6B\n1.6B\nMoE-LLaVA-1.6B\u00d74-Top2\n4\n2\n16\n100352\n2560\n32\n10240\n2\n32\n2.0B\n2.9B\nMoE-LLaVA-1.6B\u00d74-Top2\u2020\n4\n2\n32\n100352\n2560\n32\n10240\n2\n32\n2.5B\n4.1B\nQwen-1.8B (Bai et al., 2023a)\n-\n-\n-\n151936\n2048\n24\n5504\n3\n16\n1.8B\n1.8B\nMoE-LLaVA-1.8B\u00d74-Top2\n4\n2\n12\n151936\n2048\n24\n5504\n3\n16\n2.2B\n3.1B\nMoE-LLaVA-1.8B\u00d74-Top2\u2020\n4\n2\n24\n151936\n2048\n24\n5504\n3\n16\n2.6B\n4.3B\nPhi2-2.7B (Microsoft, 2023)\n-\n-\n-\n51200\n2560\n32\n10240\n2\n32\n2.7B\n2.7B\nMoE-LLaVA-2.7B\u00d74-Top2\n4\n2\n16\n51200\n2560\n32\n10240\n2\n32\n3.6B\n5.3B\nMoE-LLaVA-2.7B\u00d74-Top2\u2020\n4\n2\n32\n51200\n2560\n32\n10240\n2\n32\n4.5B\n7.8B\nOpenChat-7B (Wang et al., 2023a)\n-\n-\n-\n32000\n4096\u2217\n32\n14336\n3\n32\n6.7B\n6.7B\nMoE-LLaVA-7B\u00d74-Top2\n4\n2\n16\n32000\n4096\u2217\n32\n14336\n3\n32\n9.6B\n15.2B\nMoE-LLaVA-7B\u00d74-Top2\u2020\n4\n2\n32\n32000\n4096\u2217\n32\n14336\n3\n32\n12.4B\n23.7B\nA.2. Training Details\nTable 9. Training hyperparameters.\nConfig\nStage I\nStage II\nStage III\nExperts\n-\n-\n4\nTop-k\n-\n-\n2\nDeepspeed\nZero2\nZero2\nZero2 offload\nData\nLLaVA-PT Hybird-PT\nLLaVA-FT\nImage resolution\n336\u00d7336\nImage encoder\nCLIP-Large/336\nFeature select layer\n-2\nImage projector\n2 Linear layers with GeLU\nEpoch\n1\nLearning rate\n1e-3\n2e-5\n2e-5\nLearning rate schdule\nCosine\nWeight decay\n0.0\nText max length\n2048\nBatch size per GPU\n32\n16\n16\nGPU\n8 \u00d7 A800-80G\nPrecision\nBf16\nAs shown in Table 9, we present the training hyperparameters\nfor all models, which are applicable to Qwen, StableLM, Phi and\nOpenChat. For the training process in all stages, we consistently\ntrain for 1 epoch, as we find that the models overfit when training\nfor 2 epochs. The batch size for the first stage is 256 and 128\nfor the second and third stages. We use an image resolution of\n336x336 for all three stages. Additionally, for smaller models\nlike Qwen-1.8B, it is feasible to train them on 8 V100-32G\nGPUs. However, during the training process, using fp16 may\nsometimes lead to loss becoming NaN. Since our models are\nsmaller than 7B, we can train them in zero2 mode. However, for\nstage 3, deepspeed temporarily does not support training MoE\narchitecture in zero3 mode. Therefore, we choose zero2 offload\nto further reduce the memory requirements and enable running\non 8 A800-80G GPUs. We enable the gradient checkpoint mode\nfor all training stage.\n15\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nB. Additional Results and Visualization\nB.1. Model Scaling\nTable 10. Ablation study about the model size of MoE-LLaVA.\nModel\nMoE\nVQAv2\nSQAI\nVQAT\nMMB\nLLaVAW\nStableLM\n\u2717\n74.5\n62.0\n48.8\n58.2\n83.2\n\u2714\n76.0\n62.6\n47.8\n59.4\n85.9\nQwen\n\u2717\n74.9\n60.2\n48.3\n60.6\n86.3\n\u2714\n76.2\n63.1\n48.0\n59.7\n88.7\nPhi-2\n\u2717\n75.6\n67.8\n50.0\n65.0\n91.3\n\u2714\n77.6\n68.5\n51.4\n65.2\n94.1\nOpenChat\n\u2717\n77.9\n69.0\n54.7\n66.9\n89.7\n\u2714\n78.9\n62.8\n52.5\n65.9\n86.3\nAs shown in Table 10, for models smaller than 7B,\nwe demonstrate a strong scale of law. MoE-LLaVA\nexhibits improved performance as the model size\nincreases, as exemplified by StableLM-1.6B, Qwen-\n1.8B, and Phi-2.7B. But surprisingly, the overall\nperformance of OpenChat-MoE is significantly in-\nferior to dense models. We speculate that this may\nbe due to the insufficient data for current multi-\nmodal instruction tuning to support sparse pattern\nlearning in 10B-level models, which should be ad-\ndressed in future work when scaling up to larger\nMoE-LLaVA models.\nB.2. Training Capacity\nFor MoE layers, we employ the Batch Priority Routing (BPR) strategy (Riquelme et al., 2021). This strategy utilizes the\nrouting results to determine which tokens should be dropped, ensuring a more balanced workload among the experts. During\nthe training process, the BPR strategy dynamically adjusts the routing results for each expert based on their capacity. When\nthe tokens assigned to an expert exceed its predefined capacity, the excess tokens are dropped. We conduct a ablation\nstudy on the hyperparameter capacity, as shown in Table 11. Increasing the capacity consistently improves performance for\ndifferent sizes of MoE-LLaVA.\nTable 11. Ablation study about the capacity of MoE-LLaVA. \u201cRes.\u201d represent the input image resolution. \u2217 donates that there is some\noverlap in the training data.\nMethods\nRes.\nCapacity\nImage Question Answering\nBenchmark Toolkit\nVQAv2\nGQA\nVisWiz\nSQAI\nVQAT\nPOPE\nMMB\nLLaVAW\nMM-Vet\nAvg\nMoE-LLaVA-1.6B\u00d74-Top2\n336\n1.5\n76.7\u2217\n60.3\u2217\n36.2\n62.6\n50.1\n85.7\n60.2\n86.8\n26.9\n60.6\n1.0\n76.0\u2217\n60.4\u2217\n37.2\n62.6\n47.8\n84.3\n59.4\n85.9\n26.1\n59.9\nMoE-LLaVA-2.7B\u00d74-Top2\n336\n1.5\n77.6\u2217\n61.4\u2217\n43.9\n68.5\n51.4\n86.3\n65.2\n94.1\n34.3\n64.7\n1.0\n77.1\u2217\n61.1\u2217\n43.4\n68.7\n50.2\n85.0\n65.5\n93.2\n31.1\n63.9\nMoE-LLaVA-2.7B\u00d74-Top2\n384\n1.5\n79.9\u2217\n62.6\u2217\n43.7\n70.3\n57.0\n85.7\n68.0\n97.3\n35.9\n66.7\n1.0\n79.4\u2217\n62.7\u2217\n42.1\n70.3\n55.7\n85.5\n67.9\n95.1\n33.6\n65.8\nB.3. Routing Distributions\nIn this section, we present the routing distributions of MoE-LLaVA-OpenChat-7B\u00d74-Top2, MoE-LLaVA-Phi-2.7B\u00d74-Top2,\nMoE-LLaVA-Qwen-1.8B\u00d74-Top2, and MoE-LLaVA-StableLM-1.6B\u00d74-Top2 on six benchmarks (ScienceQA-IMG (Lu\net al., 2022), TextVQA (Singh et al., 2019), POPE (Li et al., 2023d), MMBench (Liu et al., 2023d), VisWiz (Gurari et al.,\n2018), MM-Vet (Yu et al., 2023)). These routing distributions are based on the training up to the final checkpoint.\nFor MoE-LLaVA-OpenChat-7B\u00d74-Top2, it is a truly large model compared to our setting. However, as shown in Ap-\npendix B.1, its performance is not as good as expected. We provide the routing distribution of MoE-LLaVA-OpenChat\nafter sparsification in Figure 7. We can observe that even after three stages of training, the routing distributions of MoE-\nLLaVA-OpenChat and MoE-LLaVA-Phi ( Figure 8) differ significantly. MoE-LLaVA-OpenChat exhibits a relatively\nbalanced distribution overall, in terms of both expert loads and expert preferences for different modalities. On the other\nhand, MoE-LLaVA-Phi, along with other smaller models such as MoE-LLaVA-Qwen and MoE-LLaVA-StableLM, show\nsome specific patterns or, in other words, their distributions are more disordered. For example, (1) in Figure 8, MoE-\nLLaVA-Phi exhibits a prominent expert 3 in layers 17-23, which dominates the majority of the workload. (2) In Figure 9,\nMoE-LLaVA-Qwen shows a strong preference for the image modality in expert 1. (3) In Figure Figure 10, experts 2 and 3\nof MoE-LLaVA-StableLM are actively engaged in the middle layers of the model. We believe this is highly likely due to\nthe insufficient amount of current multimodal fine-tuning data (655k in our setting) to enable sparsification for 10B-level\nmodels, even starting from a well-initialized LVLM.\n16\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(a) ScienceQA-IMG\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(b) TextQA\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(c) POPE\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(d) MMBench\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(e) Viswiz\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(f) MM-Vet\nFigure 7. Distribution of expert loadings and expert preferences on MoE-LLaVA-OpenChat-7B\u00d74-Top2.\nIn fact, we should reflect on what behavior is expected for a sparse MoE model. Should it exhibit specific patterns for each\nexpert, like MoE-LLaVA-Phi, or should it have similar behavior among the experts, like MoE-LLaVA-OpenChat? If\nwe consider that in a sparse model, the behavior of each expert should be similar at initialization, as they are initialized from\na shared FFN and the router has not yet learned any inductive biases, then if the routing distribution continues to remain\nbalanced as the network learns, it would be similar to the initialization and may lead to confusion in the model. Therefore,\nwe speculate that the lack of sufficient data may be a reason for the poor performance of MoE-LLaVA-OpenChat.\nHowever, due to the current limitations in data and computational resources, we are unable to further explore this, and we\nhope that future work can make progress in this direction.\nAdditionally, we provide more details in Figure 11, Figure 12, Figure 13, and Figure 14.\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(a) ScienceQA-IMG\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(b) TextQA\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(c) POPE\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(d) MMBench\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(e) Viswiz\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9 11 13 15 17 19 21 23 25 27 29 31\nMoE layer\nExpert 4\nText\nImage\n(f) MM-Vet\nFigure 8. Distribution of expert loadings and expert preferences on MoE-LLaVA-Phi-2.7B\u00d74-Top2.\nB.4. Token Pathways\nIn Figure 11, Figure 12, Figure 13, and Figure 14, we track the paths of each token for MoE-LLaVA-OpenChat-7B\u00d74-Top2,\nMoE-LLaVA-Phi-2.7B\u00d74-Top2, MoE-LLaVA-Qwen-1.8B\u00d74-Top2, and MoE-LLaVA-StableLM-1.6B\u00d74-Top2, respectively.\nIn general, the overall trends of the token paths align with the analysis in Appendix B.3. The paths of MoE-LLaVA-\nOpenChat-7B\u00d74-Top2 appear more disorderly and diverse, which is attributed to a more balanced expert assignment. On the\nother hand, MoE-LLaVA-Phi-2.7B\u00d74-Top2, MoE-LLaVA-Qwen-1.8B\u00d74-Top2, and MoE-LLaVA-StableLM-1.6B\u00d74-Top2\neach exhibit their specific patterns.\n17\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(a) ScienceQA-IMG\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(b) TextQA\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(c) POPE\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(d) MMBench\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(e) Viswiz\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(f) MM-Vet\nFigure 9. Distribution of expert loadings and expert preferences on MoE-LLaVA-Qwen-1.8B\u00d74-Top2.\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(a) ScienceQA-IMG\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(b) TextQA\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(c) POPE\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(d) MMBench\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(e) Viswiz\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\n0%\n25%\n50%\n75%\n100%\nPercentage\nAll experts\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 1\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 2\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 3\nText\nImage\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer\nExpert 4\nText\nImage\n(f) MM-Vet\nFigure 10. Distribution of expert loadings and expert preferences on MoE-LLaVA-StableLM-1.6B\u00d74-Top2.\nB.5. Exhibition Board\nIn Table 12, we present some classic examples using images from LLaVA (Liu et al., 2023c) and LLaVA-1.5 (Liu et al.,\n2023b). We observe that MoE-LLaVA performs comparably to them on these classic images, despite using fewer parameters.\n18\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\nTable 12. Exhibition Board of MoE-LLaVA. MoE-LLaVA demonstrates the ability to detect and answer challenging questions when\nprompted to verify them.\nVisual input example, Tricky Question and Image:\nUser\nIf there are factual errors in the questions, point it out;\nif not, proceed answering the question.\nWhat\u2019s happening in the desert?\nLLaVA-1.5\nThere are no deserts in the image. The image features\na beach with palm trees, a city skyline, and a large\nbody of water.\nMoE-LLaVA\nThere are no deserts in the image. The scene features\na city with a beach and a large body of water, which is\nmore likely to be a coastal city.\nUser\nDo you know who drew this painting?\nMoE-LLaVA\nThe painting is a portrait of the famous artist, Leonardo\nda Vinci.\nUser\nDo you know who drew this painting?\nMoE-LLaVA\nThe painting is a drawing of a dog, and it is not possi-\nble to definitively determine the artist who created it\nbased on the image alone.\nUser\nWhat is the name of the man in the photo?\nMoE-LLaVA\nThe man in the photo is named Elon Musk.\nUser\nWhat is the name of the man in the photo?\nMoE-LLaVA\nThe man in the photo is named Elon Musk.\nVisual input example, Common Question:\nUser\nCan you explain this meme?\nGPT-4 (OpenAI, 2023)\nThis meme is a joke that combines two unrelated\nthings: pictures of the earth from space and chicken\nnuggets. The text of the meme suggests that the image\nbelow is a beautiful picture of the earth from space.\nHowever, the image is actually of chicken nuggets ar-\nranged to vaguely resemble a map of the world. The\nhumor in this meme comes from the unexpected juxta-\nposition of the text and the image. The text sets up an\nexpectation of a majestic image of the earth, but the\nimage is actually something mundane and silly.\nMoE-LLaVA\nThe image features a pan filled with golden-brown\nfried chicken pieces arranged in the shape of a world\nmap. There are numerous chicken pieces spread across\nthe pan, with some pieces located near the center,\nwhile others are positioned towards the edges. The\nchicken pieces are placed in a way that resembles the\ncontinents and countries, creating a visually appealing\nand creative presentation. The pan itself is placed on a\ntable, ready to be served and enjoyed.\nUser\nWhat is unusual about this image?\nMoE-LLaVA\nThe unusual aspect of this image is that a man is stand-\ning on a clothesline that is attached to the back of a\nmoving taxi cab. This is not a typical scene, as clothes-\nlines are usually found in residential areas, patios, or\nlaundry rooms, and not on the back of a vehicle. The\nman is holding a shirt while standing on the clothes-\nline, which adds to the unusualness of the situation. It\nis likely that the man is performing some task or trying\nto dry the shirt in an unconventional manner.\nUser\nDescribe the image and where\u2019s the red arrow?\nMoE-LLaVA\nThe image is a graph or chart with a red arrow pointing\nto the bottom. The graph is color-coded, with pink and\ngreen lines, and it appears to be a line graph. The red\narrow is located at the bottom of the graph, indicating\na specific point or value.\n19\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(a) ScienceQA-IMG\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(b) TextQA\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(c) POPE\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(d) MMBench\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(e) Viswiz\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(f) MM-Vet\nFigure 11. Distribution of modalities across different experts on MoE-LLaVA-OpenChat-7B\u00d74-Top2.\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(a) ScienceQA-IMG\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(b) TextQA\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(c) POPE\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(d) MMBench\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(e) Viswiz\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1 3 5 7 9 1113151719212325272931\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(f) MM-Vet\nFigure 12. Distribution of modalities across different experts on MoE-LLaVA-Phi-2.7B\u00d74-Top2.\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(a) ScienceQA-IMG\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(b) TextQA\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(c) POPE\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(d) MMBench\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(e) Viswiz\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(f) MM-Vet\nFigure 13. Distribution of modalities across different experts on MoE-LLaVA-Qwen-1.8B\u00d74-Top2.\n20\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(a) ScienceQA-IMG\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(b) TextQA\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(c) POPE\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(d) MMBench\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(e) Viswiz\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\n0%\n25%\n50%\n75%\n100%\nPercentage\nText\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n1\n3\n5\n7\n9\n11 13 15 17 19 21 23\nMoE layer idx\nImage\nExpert 1\nExpert 2\nExpert 3\nExpert 4\n(f) MM-Vet\nFigure 14. Distribution of modalities across different experts on MoE-LLaVA-StableLM-1.6B\u00d74-Top2.\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(a) ScienceQA-IMG\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(b) TextQA\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(c) POPE\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(d) MMBench\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(e) Viswiz\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(f) MM-Vet\nFigure 15. Visualization of activated pathways on MoE-LLaVA-OpenChat-7B\u00d74-Top2.\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(a) ScienceQA-IMG\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(b) TextQA\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(c) POPE\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(d) MMBench\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(e) Viswiz\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\n25\n27\n29\n31\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(f) MM-Vet\nFigure 16. Visualization of activated pathways on MoE-LLaVA-Phi-2.7B\u00d74-Top2.\n21\nMoE-LLaVA: Mixture of Experts for Large Vision-Language Models\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(a) ScienceQA-IMG\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(b) TextQA\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(c) POPE\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(d) MMBench\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(e) Viswiz\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(f) MM-Vet\nFigure 17. Visualization of activated pathways on MoE-LLaVA-Qwen-1.8B\u00d74-Top2.\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(a) ScienceQA-IMG\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(b) TextQA\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(c) POPE\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(d) MMBench\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(e) Viswiz\n4\n3\n2\n1\nExpert idx\nText\nTop-1\nTop-2\nOthers\n1\n3\n5\n7\n9\n11\n13\n15\n17\n19\n21\n23\nMoE layer idx\n4\n3\n2\n1\nExpert idx\nImage\nTop-1\nTop-2\nOthers\n(f) MM-Vet\nFigure 18. Visualization of activated pathways on MoE-LLaVA-StableLM-1.6B\u00d74-Top2.\n22\n"
  },
  {
    "title": "Rephrasing the Web: A Recipe for Compute and Data-Efficient Language Modeling",
    "link": "https://arxiv.org/pdf/2401.16380.pdf",
    "upvote": "43",
    "text": "Rephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nRephrasing the Web:\nA Recipe for Compute and Data-Efficient Language Modeling\nPratyush Maini\u2217\u2020\nCarnegie Mellon Univeristy\npratyushmaini@cmu.edu\nSkyler Seto\u2217, He Bai, David Grangier, Yizhe Zhang, Navdeep Jaitly\nApple\n{sseto,hbai22,grangier,yizhe zhang,njaitly}@apple.com\nAbstract\nLarge language models are trained on massive scrapes of the web, which are\noften unstructured, noisy, and poorly phrased. Current scaling laws show\nthat learning from such data requires an abundance of both compute and\ndata, which grows with the size of the model being trained. This is infeasi-\nble both because of the large compute costs and duration associated with\npre-training, and the impending scarcity of high-quality data on the web.\nIn this work, we propose Web Rephrase Augmented Pre-training (WRAP)\nthat uses an off-the-shelf instruction-tuned model prompted to paraphrase\ndocuments on the web in specific styles such as \u201clike Wikipedia\u201d or in\n\u201cquestion-answer format\u201d to jointly pre-train LLMs on real and synthetic\nrephrases. First, we show that using WRAP on the C4 dataset, which is\nnaturally noisy, speeds up pre-training by \u223c 3\u00d7. At the same pre-training\ncompute budget, it improves perplexity by more than 10% on average\nacross different subsets of the Pile, and improves zero-shot question answer\naccuracy across 13 tasks by more than 2%. Second, we investigate the\nimpact of the re-phrasing style on the performance of the model, offering\ninsights into how the composition of the training data can impact the per-\nformance of LLMs in OOD settings. Our gains are attributed to the fact that\nre-phrased synthetic data has higher utility than just real data because it\n(i) incorporates style diversity that closely reflects downstream evaluation\nstyle, and (ii) has higher \u2018quality\u2019 than web-scraped data.\n1\nIntroduction\nLarge language model (LLM) pre-training has been largely democratized and open-sourced,\nallowing various academic labs, and industries to pre-train custom LLMs. Yet, a key differ-\nentiator between these models is the composition and size of the data used to train them.\nData curation strategies are required to filter out scrapes of the web that are unstructured\nand/or poorly phrased (Eisenstein, 2013). While some of these strategies have been made\npublic (Brown et al., 2020; Wenzek et al., 2020; Penedo et al., 2023), most state-of-the-art data\ncuration techniques are unknown to the research community, and only anecdotal evidence\nremains. Research on data curation requires multiple rounds of re-training, making it an\nexpensive endeavour to document techniques that lead to practical improvements. On the\nother hand, scaling laws for language models (such as Chinchilla scaling laws (Hoffmann\net al., 2022)) show that with increasing model sizes, we should also increase both the training\ncompute and data size linearly. This is infeasible because (a) high-quality data is limited\n(Villalobos et al., 2022), and repeating for even a small number of epochs (4 or more) results\nin diminishing returns or overfitting (Muennighoff et al., 2023; Touvron et al., 2023; Xue\net al., 2023); and (b) pre-training for such long durations is prohibitively expensive.\nMeanwhile, the use of synthetic data has gained prominence in the paradigm of aligning\npre-trained LLMs via instruction fine-tuning, RLHF (Ouyang et al., 2022), and instruction\nbacktranslation (Li et al., 2023b). Recently, in the context of pre-training, synthetic data\n\u2217Equal Contribution\n\u2020Work done during internship at Apple\n1\narXiv:2401.16380v1  [cs.CL]  29 Jan 2024\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\n(a)\n0\n100\n200\n300\nTokens seen (in Billions)\n40\n41\n42\n43\n44\n45\n46\n47\n48\nAvg. Zeroshot Accuracy\n~3x Faster\nWRAP-85B (ours)\nC4-85B\nC4-170B\nRW-300B\n(b)\n85\n35\n18\n170 150\n85\n75\nData Pool Size (Billion Tokens)\n0\n5\n10\n15\n20\n25\n30\nAvg. Perplexity on the PILE\nRecipe: \n WRAP \n C4\nModel Size: \n 350M \n 1.3B\nModel Size: \n 350M \n 1.3B\n(c)\nFigure 1: (a) WRAP Recipe: We prompt an off-the-shelf instruction-tuned model to rephrase\narticles on the web, and pre-train an LLM on a mixture of real and synthetic data.\n(b)\nZero-shot performance of GPT 1.3B models trained on combinations of C4 and synthetic\nvariations. Each step corresponds to a batch of 1M samples. (c) Weighted average perplexity\nover 21 sub-domains of the Pile for varying model sizes and amount of pre-training data.\nwas used to generate datasets such as Tiny Stories (Eldan & Li, 2023) and Textbook quality\nsynthetic data (Gunasekar et al., 2023; Li et al., 2023c). These were used to train smaller\nlanguage models (like the Phi model family) that were as performant as larger language\nmodels on certain tasks. However, their data generation process stays largely opaque,\nand prohibitively expensive, requiring prompting a GPT-3.5 model for generating billions\nof tokens. Additionally, such data generation can create a large \u201cknowledge bias\u201d by\nspecifically generating data pertaining to tasks that we want to perform well on. While\nsynthetic data has shown promise, it is unclear if this is because of the higher quality nature\nof synthetic data, or because of strategic topic selection (Maini, 2023).\nIn this work, we propose Web Rephrase Augmented Pre-training (WRAP)\u2014that attempts\nto bridge three important challenges stemming from the ambiguity around data curation\u2014\n(i) what data should you pre-train on? (ii) how can you pre-train with limited data? (iii)\nhow can you pre-train computationally efficiently? In particular, we show that re-phrasing\ndocuments on the web using an off-the-shelf medium size LLM allows models to learn\nmuch more efficiently than learning from raw text on the web, and accounts for performance\ngains on out of distribution datasets that can not be offset with additional web data. Our\nproposed method involves using a pre-trained off-the-shelf LLM to re-phrase documents\nfrom a web corpus into different styles. An overview of our approach is shown in Figure 1a.\nIn our work, we tackle two important challenges faced during synthetic data curation in the\nworks of Gunasekar et al. (2023)\u2014generation cost, and data bias\u2014by rephrasing articles on\nthe web. (i) WRAP allows for using an open source, and much smaller LLM (1.8B/7B v/s\nGPT3.5) to rephrase unstructured and poorly phrased documents in different styles, since it\ndoes not rely on the LLM as a knowledge bank. (ii) Thanks to the information maintaining\nnature of rephrasing, we are able to leverage the natural diversity of the web, rather than\nrelying on an LLM for information which may be prone to factual errors, and/or data biases.\nOur work shows that the \u201cstyle\u201d alone can result in large gains in downstream performance.\nUsing WRAP on the C4, we evaluate model performance on 13 different zero-shot tasks, and\n21 different language modeling domains of the Pile, and find that pre-training LLMs with\nsynthetic data allows us to train equivalent models with 5x lesser data, or 3x lesser compute.\nIn fact, our synthetic data trained models, also outperform the recent TinyLLama models\nthat were trained for 3 trillion tokens (10x data and compute) across several zero-shot Q/A\ntasks. We further observe a reduction in perplexity by \u223c 50% on the Pile, and note that our\n350M parameter model trained on combinations of real and synthetic rephrases on just 15%\nof the entire C4 corpus, outperforms pre-training a 1.3B parameter on the entire C4. Finally,\nwe conduct an analysis on the potential of data leakage, properties of synthetic data styles,\nand how to combine synthetic data for improving WRAP based LLM pre-training.\n2\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\n2\nRelated Work\nNeural Scaling Laws for Language Models\nNeural scaling laws relate the optimal number\nof model parameters and amount of training data for a fixed amount of compute. Hoffmann\net al. (2022) presented the Chinchilla scaling laws for language models demonstrating that\nthere was a linear relationship between the size of the model and the amount of training\ndata needed. Their findings indicated that prior models such as Gopher (Rae et al., 2021)\nare severely undertrained. Recently, models such as Llama (Touvron et al., 2023) are trained\nwith much more data. These scaling laws were drawn for the paradigm of single-epoch\ntraining. Recently, Muennighoff et al. (2023) showed that the marginal utility of repeated\ndata rapidly diminishes when training for more than 4 epochs, and formulated scaling\nlaws under repeated data. Concurrently, Xue et al. (2023) showed that repeating even small\nfractions of the pre-training data can lead to overfitting and reduce model performance.\nDataset Selection\nSelecting high quality data for pre-training LLMs remains an active,\nhigh-impact, yet understudied area of research. For instance, GPT-2 model was pre-trained\non all outbound links from Reddit, a social media platform, which received at least 3\nkarma (Brown et al., 2020). This was used as a heuristic indicator that the document may\nbe interesting, educational, or just funny. Follow-up works have used other heuristics such\nas prioritizing documents that resemble wikipedia (Gururangan et al., 2022). Rae et al.\n(2021) used multiple heuristic filters to remove documents, such as the absence of certain\nstopwords, length of the document, percentage of alphabetic characters, mean word length,\nsymbol-to-word ratio, percentage of lines starting with a bullet point, or ending with an\nellipsis etc. Their work highlights the intricacies of filtering out text data. An alternative\nparadigm for building better datasets for training is to distill high-quality datasets. Xie\net al. (2023) proposed a method, DoReMi, to select the best data mixture for pre-training\nlanguage models by reweighting data from various domains. Concurrently, Abbas et al.\n(2023) showed that de-duplicating pre-training data can improve pre-training efficiency.\nRecently several methods were proposed for automatic filtering of low-quality data for\nfaster fine-tuning of LLMs (Chen et al., 2023; Solaiman & Dennison, 2021; Zhou et al., 2023).\nSimultaneously, in the realm of image-language models such as CLIP (Radford et al., 2021),\nthe Datacomp benchmark (Gadre et al., 2023) and recent entries (Maini et al., 2023; Yu et al.,\n2023) have developed approaches at filtering out low-quality subsets from pre-training\ndatasets like LAION (Schuhmann et al., 2022), or from scrapes of the common crawl.\nData Augmentation and synthetic data\nEldan & Li (2023) showed that a synthetically\ngenerated dataset in the form of stories that toddlers can understand allows training a small\nlanguage model that can generate coherent sentences. Gunasekar et al. (2023) showed that\ntextbook quality (synthetic) data alone helps models achieve state-of-the-art performance on\nreasoning and coding tasks. Similar approaches are used in concurrent work for enhancing\ncoding and mathematical reasoning abilities while finetuning Liu et al. (2023a); Wei et al.\n(2023). Shumailov et al. (2023) show that training on synthetic data can actually be harmful\nfor model performance, especially when we do multiple rounds of pre-training an LLM and\nthen training the next LLM on data generated by the previous one. On the other hand, some\nother works have shown that such a strategy can actually be useful. Li et al. (2023b) and\nK\u00a8oksal et al. (2023) discuss how a model can generate instruction data and then fine-tune\non its own generated data to improve performance. Jung et al. (2023) discuss how such\nrepeated cycles of synthetic data can help train a very small paraphrase and summarization\nmodel that even outperforms GPT-3.\nThe vision and multimodal literatures have also seen a surge of works examining the use\nof synthetic data for training. The works of Bansal & Grover (2023); Trabucco et al. (2023);\nAzizi et al. (2023) have shown that using synthetic data in combination with real data\nachieves state of art model performance both in-distribution and out-of-distribution. Cubuk\net al. (2020) used generative models to generate image augmentations for better domain\ngeneralization. There are also multiple studies on increasing multiplicity of augmentations\nand their value for improving generalization (Choi et al., 2019; Fort et al., 2021; Hoffer et al.,\n2020). However, Alemohammad et al. (2023) showed that generated models trained for\nmore than five cycles of their own generated data can undergo severe mode collapse.\n3\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\n3\nWRAP: Web Rephrase Augmented Pretraining\nGenerating synthetic data using an off-the-shelf language model can be both computation-\nally expensive and operationally challenging. Prior approaches to generating synthetic\ntextbook quality data using LLMs (Gunasekar et al., 2023) required (1) a language model\nthat contains sufficient world knowledge to generate articles worth training on, thereby\nincreasing generation cost; (2) a careful selection of prompts that enable generating high\nquality, and diverse articles that fill any knowledge gaps in the synthetic corpus. This\nchallenge was highlighted in follow-up work of Li et al. (2023c), and has the potential of\ninadvertently creeping in biases in the language models (Maini, 2023), as opposed to those\ntrained on the natural diversity of the web. As a remedy to the challenge of (i) generation\ncost, and (ii) data diversity, we propose WRAP that leverages the natural diversity of\narticles on the web, allowing us to utilize significantly smaller LLMs (than GPT-3.5) to\ngenerate high-quality paraphrases of noisy and unstructured articles on the web.\n3.1\nRephrasing the Web\nIt has been observed in past work that up-weighting high-quality data, such as texts from\nWikipedia, can be useful to improve language modeling. These terms have generally been\nvery loosely defined and there is only anecdotal evidence of the same (Brown et al., 2020;\nWenzek et al., 2020). At the same time, web data is deficient of text in question-answering or\nconversational format, which is a prominent use case of language models. Based on these\ntwo insights, we design the rephrasing styles for our work.\nRephrasing Styles\nIn lieu of the anecdotal evidence above, we attempt rephrasing docu-\nments on the web in four different styles\u2014(i) Easy (text that even a toddler will understand);\n(ii) Medium (in high quality English such as that found on Wikipedia); (iii) Hard (in terse\nand abstruse language); (iv) Q/A (in conversation question-answering format). In or-\nder to operationalize rephrasing in these stylistic variations, we appropriately prompt an\ninstruction-tuned model. The rephrased examples of these four styles and the prompts\ntemplates used in our work are provided in Appendix G.\nGenerating Synthetic Data\nNow, we detail how we utilize an instruction-tuned language\nmodel to rephrase texts from web-crawled datasets such as C4 (Raffel et al., 2020) (which\nwe use for all our experiments). In particular, we use a frozen Mistral-7B instruction-tuned\nmodel (Jiang et al., 2023) (see Ablations in Section 6 for other models). To generate synthetic\ndata in \u201cmedium\u201d style, the Mistral model is prompted using the following instruction:\n\u201cFor the following paragraph give me a paraphrase of the same in high-quality English language\nas in sentences on Wikipedia\u201d. The prompt was created using iterative human feedback by\ncomparing outputs of \u2018medium\u2019 sized LLMs with those of GPT-4. We use the model output\nto create a parallel corpus of \u201chigh-quality\u201d synthetic data corresponding to the original\nnoisy web data. Each example has a maximum of 300 tokens, which was decided based on\nour empirical observation that asking an LLM to rephrase more than 300 tokens, often led\nto loss of information. Discussions on data quality can be found in Section C.\nCombining Real and Synthetic Data\nOur method of re-phrasing web data naturally incor-\nporates the information diversity found on the internet. However, it does not incorporate\nthe noise in real data. While synthetic data may help LLMs pre-train faster, we also want\nthem to be able to understand noisy web text that may be filled with typos and linguistic\nerrors so that the LLMs do not fail in user facing situations. In order to incorporate this style\ndiversity in language modeling, we sample real and synthetic data in a 1:1 ratio.\n3.2\nImplementation Details\nArchitecture\nWe train decoder-only transformer models (Vaswani et al., 2017) at three\ndifferent scales, small, medium and XL. The small-scale (128M parameter) model consists of\n12 layers, 12 attention heads, and a hidden dimension size of 768. The medium-scale (350M\nparameter) model consists of 24 layers, 16 attention heads, and a hidden dimension size\n4\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nAverage\nWiki\nOWT2\nBooks2\nBooks\nArXiv\nStackX\nGithub\nMath\nHNews\nEnron\nPubmed-C\nUbuntu\nPubmed-A\nFreeLaw\nNIH\nUSPTO\nPG-19\nPhil\nYoutube\nOpenSubs\nCC\n0\n20\n40\n60\nPerplexity\nC4-170B\nC4-85B\nC4+QA-85B\nFigure 2: WRAP (C4 + QA-85B) v/s C4: Comparison of perplexity on the Pile for a 1.3B\nLLM trained for 300B tokens shows that WRAP outperforms models trained on 2x real data.\nof 1024. The XL-scale (1.3B parameter) model consists of 24 layers, 16 attention heads, and\na hidden dimension size of 2048. We do not use dropout in either model and a maximum\nsequence length of 1024. The models are trained using NVIDIA\u2019s Megatron-LM repository.\nPre-training\nWe train all our XL models for a total of 300k steps with a batch size of one\nmillion tokens, unless otherwise specified. We use a maximum learning rate of 3e\u22124 for the\n128M, and 350M parameter models, and 2e\u22124 for the 1.3B parameter model. The minimum\nlearning rate is 1e\u22125. We use a weight decay of 0.01, along with a gradient clipping norm of\n1.0. We use cosine learning rate scheduler with a warmup for 1% of the total steps; and the\nAdam optimizer with \u03b21 = 0.9 and \u03b22 = 0.999.\n4\nPerplexity Evaluation\nWe evaluate the perplexity of the pre-trained model on the validation set of multiple out-of-\ndistribution datasets. All models are either trained on the C4 dataset (Raffel et al., 2020), or\na particular stylistic rephrase of the same. All the evaluations are done on 21 sub-domains\nof the Pile (Gao et al., 2020). These subsets are created from the first 10,000 documents from\neach domain of the Pile dataset. We then evaluate the perplexity of the model on these\nsubsets. Additional evaluation details are provided in Appendix D. It is important to note\nthat we evaluate perplexities on the Pile instead of C4. Training on multiple distributions of\ntext (synthetic and real web) does come at a small cost of less than 1 perplexity on the C4\nvalidation set. To understand our choice of evaluation, and why we observe this perplexity\nincrease, we note that training over the C4 corpus corresponds to minimizing the objective\n\u03b8c4 = min\n\u03b8\nEx\u223cDc4 [L(\u03b8; x)] ,\n(1)\nthat attempts to exactly model the C4 web text. In contrast, training over multiple styles\ncorresponds to minimizing the risk over a different distribution,\n\u03b8WRAP = min\n\u03b8\nEx\u223cDc4\u222aDsyn [L(\u03b8; x)] .\n(2)\nSolving for equation 2 does not minimize the risk over C4-only, and hence it is unfair to\ncompare \u03b8c4 and \u03b8WRAP on the C4. For meaningfully comparing models trained on the C4\nand on its synthetic rephrases, we evaluate their generalization capability on 21 different\ndomains of the Pile (Gao et al., 2020). Results for each domain are presented in Figure 2.\n5\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nData Complexity\nIn Figure 1c, we show that models trained for fewer tokens (150B) and\neven smaller 350M models outperform training on the full C4 for 300B tokens indicating\nfaster learning with synthetic rephrases. On some domains such as ArXiv and HackerNews,\nwe observe that training with synthetic data allows reducing the perplexity by nearly 3x\nof the perplexity of models trained on real data alone. This suggests that in many cases it\nmay not be possible to offset the performance advantage of pre-training on synthetic data\nby merely training on more real data. Overall, on an average of multiple subsets of the Pile,\nour models improve perplexity by 50% over models trained on real data alone.\nLearning Speed\nWe observe that even at the first checkpoint (10B tokens) of WRAP train-\ning, the average perplexity of the LLM on the Pile is lower than that achieved by pre-training\non C4 for 15 checkpoints. This suggests a 15x pre-training speed-up. We defer the discussion\non learning speed to \u2018zero-shot\u2019 tasks in order to make more meaningful comparisons.\n5\nZero-shot Tasks\nWe now evaluate our pre-trained language models on various zero-shot question answering\n(QA) benchmarks using the LLM Evaluation Harness1 (Gao et al., 2023).\n5.1\nDatasets\nWe evaluate our models on a total of 13 different zero-shot benchmarks to assess their\nabilities across various natural language tasks like common sense reasoning, language and\nknowledge understanding and mathematical reasoning.\nGeneral Understanding\nThe General Understanding category comprises datasets testing\nbroader cognitive skills and language comprehension. ARC Easy (ARC-E) (Clark et al.,\n2018) is the less challenging counterpart of ARC-C, featuring questions that require basic\nreasoning skills. BoolQ (Clark et al., 2019) includes boolean questions that focus on reading\ncomprehension and general language understanding. Winogrande (Wino.) (ai2, 2019)\nchallenges models with common sense reasoning in language, particularly in pronoun\ndisambiguation. PIQA (Bisk et al., 2020) assesses understanding of physical processes, an\nessential part of practical common sense. HellaSwag (Zellers et al., 2019) tests the ability\nto complete scenarios coherently, demanding both language understanding and common\nsense. TruthfulQA (Lin et al., 2021) is centered on generating truthful, accurate answers,\nthus testing the model\u2019s factual correctness. OpenBookQA (OBQA) (Mihaylov et al., 2018)\nevaluates the understanding of a broad range of facts and concepts. Finally, LogiQA-2 (Liu\net al., 2023b) assesses the model\u2019s capacity to comprehend and apply logical principles.\nSpecialized Knowledge\nIn the Specialized Knowledge category, we include datasets that\ndemand expertise in specific domains. The ARC Challenge (ARC-C) (Clark et al., 2018)\ncontains challenging science exam questions from grades 3 to 9, demanding advanced\nknowledge. SciQ (Johannes Welbl, 2017) provides science exam questions to test mod-\nels\u2019 understanding and reasoning in the scientific domain. PubMedQA (Jin et al., 2019)\nfocuses on biomedical literature, assessing comprehension in medical and health-related\ninformation. MathQA (Amini et al., 2019) tests mathematical problem-solving, requiring\nboth numerical comprehension and reasoning. Lastly, MMLU (Hendrycks et al., 2021) spans\nmultiple domains, from professional to academic, testing the model on specialized subjects.\n5.2\nResults\nWe compare the performance of a model trained on a mixture of real and synthetic data\nwith models trained on various splits of real data. In all our experiments, we use the\nC4 (Raffel et al., 2020) dataset for rephrasing and producing splits of synthetic data. We\nuse the abbreviation \u2018Real Tok.\u2019 to denote the number of tokens of web data available\n1We use git commit - 89618bf8 for consistency across all experiments with a batch size of 32.\n6\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nDataset (Real Tok.)\nARC-E\nBoolQ\nWino.\nPIQA\nHellaSwag\nTruthfulQA\nOBQA\nLogiQA\nAvg\nHalf C4 (85B)\n61.2\n59.1\n57.3\n74.9\n46.5\n34.1\n22.4\n23.5\n47.4\nFull C4 (170B)\n61.6\n54.2\n59.0\n74.9\n46.8\n33.5\n25.0\n23.4\n47.3\nRW (160B)\n61.6\n60.7\n57.5\n74.3\n45.2\n36.8\n21.8\n23.2\n47.6\nRW (320B)\n60.7\n61.1\n57.1\n74.4\n45.6\n36.0\n22.6\n22.5\n47.5\nPythia-Pile (300B)\n60.5\n63.3\n57.5\n70.8\n40.4\n38.9\n22.2\n22.2\n47.0\nTinyLlama (1T)\n60.3\n57.8\n59.1\n73.3\n45.0\n37.6\n21.8\n24.5\n47.4\nSynthetic (85B)\n63.9\n60.0\n58.8\n76.1\n45.2\n44.0\n23.0\n24.1\n49.4\nSynthetic+C4 (85B)\n64.1\n62.2\n58.9\n75.4\n46.2\n40.6\n24.1\n23.9\n49.4\nTable 1: Evaluation of \u223c 1.3B parameter LLMs on \u2018General Understanding Tasks\u2019 on datasets\nfocusing on general reasoning, language understanding, and common sense. Results for\nWRAPare averaged over 3 runs\nDataset (Real Tok.)\nARC-C\nSciQ\nPubMedQA\nMathQA\nMMLU\nAvg\nHalf C4 (85B)\n26.3\n84.5\n57.2\n23.4\n24.2\n43.1\nFull C4 (170B)\n26.8\n85.0\n57.4\n24.3\n23.9\n43.5\nRW (160B)\n27.2\n87.2\n56.2\n24.1\n25.9\n44.1\nRW (320B)\n27.8\n88.0\n57.4\n23.0\n25.4\n44.3\nPythia-Pile (300B)\n26.1\n86.6\n60.6\n25.2\n24.3\n44.6\nTinyLlama (1T)\n27.8\n88.9\n61.4\n24.1\n25.8\n45.6\nSynthetic (85B)\n29.7\n87.0\n60.2\n23.4\n24.6\n45.0\nSynthetic+C4 (85B)\n29.9\n87.6\n61.5\n23.9\n24.8\n45.5\nTable 2: Evaluation of \u223c 1.3B parameter LLMs on \u2018Specialized Knowledge Tasks\u2019 that require\nspecific domain knowledge such as science, medicine, mathematics, and logic. Results for\nWRAPare averaged over 3 runs.\nfor pre-training. In the \u2018Synthetic + Real\u2019 experiments, we augment the same number of\nsynthetic rephrases. We choose \u2018Real Tokens\u2019 as the metric of comparison because we can\npotentially rephrase the same document multiple times, implying that the total corpus size\nis not meaningful, and corpus \u2018knowledge\u2019 is the actual currency of interest.\nBaselines Methods\nWe pre-train LLMs of (i) Half of C4, and the (ii) Full C4 corresponding\nto approximately 85 Billion and 170 Billion real tokens respectively (Raffel et al., 2020).\nWe also pre-train our own models on (iii) 160 Billion and (iv) 320 Billion tokens of the\nRefinedWeb Dataset (Penedo et al., 2023). Additionally, we also compare with the (iv)\nPythia-1.4B model that was trained on the Pile (Gao et al., 2020). This dataset is no longer\npublicly available, hence we utilize a pre-trained model. Finally, we also compare with the\nrecent (v) TinyLlama model (Zhang et al., 2024) that was trained for 3 epochs on 1 Trillion\ntokens of data from SlimPajama (Shen et al., 2023) and StarCoder (Li et al., 2023a).\nGeneral Improvements\nAcross all tasks in Table 1, we observe that models trained on\nsynthetic data combined with the C4 dataset (Synthetic+C4) exhibit an overall higher\naverage performance of 49.4% as compared to those trained solely on the real C4 dataset\nwith a 85B token split, which scored an average of 47.4%. This shows that the inclusion of\nsynthetic data can enhance the general understanding capabilities of NLP models. Moreover,\neven the TinyLLama model trained for 10x compute and data, performs comparably to the\nother models trained on real data. This suggests that the gains from filtering out, or adding\nmore real data are very low. As opposed to this, WRAP shows that pre-training on even\nsmall amounts of synthetic data can contribute to large performance gains.\nSpecialized Knowledge Tasks\nThe key message from the results in Table 2 is that synthetic\ndata can not impart \u2018new knowledge\u2019. It can only help pre-train faster, which was also the\npremise of our work. In particular, we note several key findings:\n1. Pre-training on larger datasets helps improve performance, by presumably exposing\nthe LLM to more \u201cknowledge\u201d. For instance, the Pythia (300B) model achieves an\naverage score of 44.6%, outperforming the smaller C4 (85B) dataset\u2019s score of 43.5%.\n7\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nAverage\nWiki\nOWT2\nBooks2\nBooks\nArXiv\nStackX\nGithub\nMath\nHNews\nEnron\nPubmed-C\nUbuntu\nPubmed-A\nFreeLaw\nNIH\nUSPTO\nPG-19\nPhil\nYoutube\nOpenSubs\nCC\n0\n10\n20\n30\n40\n50\nPerplexity\nC4+QA-35B\nC4+Med-35B\nQA-35B\nMed-35B\nFigure 3: Importance of Real Data: Comparing perplexity on the Pile when pre-training\non C4 with synthetic data vs. synthetic data only. Models are 1.3B parameters trained for a\ntotal of 150B tokens on a real data subset containing 35 Billion tokens of the C4.\nDataset (Real Tok.)\nARC-E\nBoolQ\nWino.\nPIQA\nHellaSwag\nTruthfulQA\nOBQA\nLogiQA\nAvg\nMed+C4-35B\n59.8\n57.0\n55.7\n74.6\n44.5\n36.5\n23.8\n21.5\n46.7\nQA+C4-35B\n62.2\n63.3\n55.7\n74.8\n44.6\n41.4\n22.4\n23.2\n48.4\nMed-35B\n56.6\n59.5\n53.4\n74.0\n41.9\n36.3\n22.2\n22.7\n45.8\nQA-35B\n61.7\n62.0\n53.9\n75.2\n43.4\n43.0\n22.8\n23.4\n48.2\nTable 3: Importance of Real Data: Evaluation of \u223c 1.3B parameter LLMs trained for 150B\ntokens on General Understanding Tasks. Results show that adding real data helps improve\nmodel performance when pre-training on \u2018Medium\u2019 or \u2018Wikipedia-style\u2019 paraphrases.\n2. Despite the advantages of a larger dataset, the improvements saturate. For ex-\nample, RefinedWeb (320B) model outperforms the RefinedWeb (160B) model by\nonly 0.2%. Similarly, the TinyLlama model (1T tokens) performs comparably to the\nWRAP model, which only had 85B tokens of raw web data.\nSpecific Improvements\nWe see maximum improvement in the TruthfulQA dataset, with\nthe Synthetic (85B) model scoring 44.0%, which is significantly higher than any other\nmodel\u2019s performance on this dataset. This is potentially because instruction-tuned LLMs\nalready correct potential misconceptions while rephrasing the text. Interestingly, we notice\nhow adding real data to the synthetic model (Synthetic+C4) reduces the performance on\nTruthfulQA by 4%, down to 40.5%, indicating a potential dilution of the benefits gained\nfrom synthetic data when combined with real data. Other datasets such as HellaSwag, and\nBoolQ, for which C4 trained models do well, continue to show the benefits of incorporating\ncombinations of C4 and synthetic rephrases.\n6\nAnalysis and Ablations\nWe further ask the following Research Questions (RQs) to investigate in a finer granularity\nhow to enhance performance optimally.\n6.1\nData Combination Analysis\nRQ1: How important is it to have real C4 data?\nOur findings in Tables 1\u20132indicate\nthat synthetic data using the QA prompt are sufficient for strong performance on QA\n8\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nDataset (Real Tok.)\nARC-C\nSciQ\nPubMedQA\nMathQA\nMMLU\nAvg\nMed+C4-35B\n27.2\n82.2\n46.2\n23.1\n25.2\n40.8\nQA+C4-35B\n29.0\n85.1\n62.2\n22.5\n26.1\n45.0\nMed-35B\n27.0\n80.0\n59.4\n22.5\n24.7\n42.7\nQA-35B\n27.1\n85.5\n59.2\n22.2\n25.0\n43.8\nTable 4: Importance of Real Data: Evaluation of \u223c 1.3B parameter LLMs on Specialized\nKnowledge Tasks. Results show that adding real data helps improve model performance\nwhen pre-training on \u2018Q/A-style\u2019 paraphrases.\nDataset (Real Tok.)\nARC-C\nSciQ\nPubMedQA\nMathQA\nMMLU\nAvg\nMed+C4-35B\n27.2\n82.2\n46.2\n23.1\n25.2\n40.8\nQA+C4-35B\n29.0\n85.1\n62.2\n22.5\n26.1\n45.0\nCombined-1:1-35B\n28.2\n85.9\n61.2\n23.2\n23.9\n44.5\nCombined-1:2-35B\n29.0\n85.7\n57.4\n23.5\n23.1\n43.7\nTable 5: Combining multiple styles: Evaluation of \u223c 1.3B parameter LLMs trained for 150B\ntokens on \u2018Specialized Knowledge Tasks\u2019. Results suggest that combining rephrasing styles\ndoes not yield performance benefit on zero-shot tasks compared to just Q/A style.\ntasks. However, when evaluated on Pile perplexity, we observe significant degradation\nin perplexity across many sub-domains in Figure 3. This is likely because synthetic data\nis very clean containing few special characters and being highly structured. In contrast\nseveral sub-domains of the Pile such as OWT, and Hackernews have such special tokens.\nOn domains such as Philpapers and Gutenberg, we observe that dropping real C4 text\nfrom the pre-training data, and training on synthetic documents alone drops performance\nsignificantly (increase in perplexity). This is once again attributed to the fact that synthetic\ndata does not contain certain \u2018tags\u2019 and \u2018styles\u2019 that are prevalent in real data scrapes, and\nemphasized how WRAP is a better strategy than pre-training on synthetic data alone. In\nterms of performance on zero-shot tasks, we once again note that the presence of real\ndata helps improve zero-shot performance in Tables 3,4. Since zero-shot tasks contain\nwell-written Q/A pairs, this effect is not as evident as that for perplexities on real data.\nRQ2: Does a combination of multiple synthetic datasets improve performance?\nWe\nmeasure the impact of combining multiple synthetic styles with C4 for training. We consider\ntwo variants: combining in a 1:1 ratio meaning that there are two copies of C4 to match two\nsynthetic styles (medium and QA), and 1:2 ratio which combines only one instance of the\nC4 dataset. For zero-shot QA tasks, our finding in Table 5-6 indicate lower performance\nthan combining only QA and C4 data. Evaluations over the Pile are shown in Figure 4.\nWe notice that both the \u2018Q/A\u2019 and \u2018Wikipedia\u2019 paraphrases help improve performance on\ncertain domains. For example, \u2018Stackexchange\u2019, that has lots of question-answers benefits\nfrom the presence of synthetic data in Q/A style. Overall, we note that there is a small\nimprovement on the average perplexity on the Pile by combining multiple styles.\n6.2\nMethod Ablations\nRQ3: How important is to have a high-quality re-phraser?\nTo answer this, we use data\nfrom four distinct re-phrasing models (T5-base (Raffel et al., 2020), Qwen-1.8B-chat (Bai et al.,\n2023a), Mistral-7B-chat (Jiang et al., 2023), and Vicuna-13B-chat-v1.3 (Chiang et al., 2023))\nand train a 345M model for 30B tokens. We generate data from all models using the same\nprompt. In case of the T5-base model, we finetune the model for 1 epoch on re-phrase pairs\nfrom the Vicuna-13b-chat model. We find that pre-training on data generated by smaller\nre-phrase models like Qwen-1.8B and Mistral-7B achieve lower perplexity than Vicuna 13B\n(Figure 5). At the same time, our fine-tuned T5-base model performs significantly worse\nthan the rest. Even then, all rephrase models reduce perplexity over only real C4 data. It\nremains an open question to test the limits of how small can we train a paraphrase model\nthat can generate high quality synthetic data to further scale the applicability of WRAP.\n9\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nDataset (Real Tok.)\nARC-E\nBoolQ\nWino.\nPIQA\nHellaSwag\nTruthfulQA\nOBQA\nLogiQA\nAvg\nMed+C4-35B\n59.8\n57.0\n55.7\n74.6\n44.5\n36.5\n23.8\n21.5\n46.7\nQA+C4-35B\n62.2\n63.3\n55.7\n74.8\n44.6\n41.4\n22.4\n23.2\n48.4\nCombined-1:1-35B\n60.6\n60.2\n57.7\n73.8\n43.7\n40.2\n22.0\n22.1\n47.5\nCombined-1:2-35B\n61.4\n62.0\n57.0\n74.8\n44.6\n39.5\n23.0\n21.3\n48.0\nTable 6: Combining multiple styles: Evaluation of \u223c 1.3B parameter LLMs trained for 150B\ntokens on General Understanding Tasks. Results suggest that combining rephrasing styles\ndoes not yield performance benefit on zero-shot tasks compared to just Q/A style.\nAverage\nWiki\nOWT2\nBooks2\nBooks\nArXiv\nStackX\nGithub\nMath\nHNews\nEnron\nPubmed-C\nUbuntu\nPubmed-A\nFreeLaw\nNIH\nUSPTO\nPG-19\nPhil\nYoutube\nOpenSubs\nCC\n0\n5\n10\n15\n20\n25\n30\nPerplexity\nC4+QA-35B\nC4+Med-35B\nCombined-1:1-35B\nCombined-1:2-35B\nFigure 4: Combining multiple styles: Perplexity across all domains of the Pile comparing\ncombining multiple styles of synthetic data. Models are 1.3B parameters trained for a total\nof 150B tokens. We see small perplexity improvements from combining multiple styles.\nRQ4: Does synthetic data improve over augmentations?\nAre the gains observed by\npre-training on synthetic data the same as pre-training with augmentations? In order to\ntest this, we consider two popular text augmentation baselines\u2014synonym replacement and\nrandom deletion using the NL-Augmenter library (Dhole et al., 2021). We pre-train a 350M\nparameter model for 15B tokens in order to conduct this set of experiments. The total pool\nsize is only about 1.5B tokens, meaning that the model would have to repeat data around\n10 times during the pre-training phase, unless augmented over. As seen in the perplexity\nanalysis in Figure 6, the models trained on augmented data perform significantly worse\nthan those trained on combinations of real and synthetic data. This suggests that synthetic\ndata enhances the learning process, and is not merely another form of augmentation.\nRQ5: How does the style of synthetic data impact performance on specialized domains?\nWe compare the performance of various models trained on different styles of synthetic\ndata. In particular, we generate four styles of synthetic data (easy, medium, hard, and Q/A)\nand evaluate the performance of training on combinations of each style across Pile subsets.\nThe prompts to generate these synthetic data styles are outlined in Appendix G. Results\ncorresponding to generations from a Vicuna-v1.3 model, and for a 128M model trained for\n3B tokens are summarized in Figure 7. We see that training with combinations of real C4\nand synthetic data matching the style of the domain at evaluation improves performance.\nHowever, we find that no single synthetic data style performs the best across all domains,\nresulting in similar performance across training with combinations of real C4 data and\neach synthetic style variant. While knowing the best synthetic style to pre-train an LLM is\nimpractical, an oracle that selects the best synthetic style across all domains will improve\nperplexity by 16%\u2014indicating the importance of training with diverse data styles for LLM\ngeneralization, even when the underlying knowledge stays the same.\n10\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nAverage\nWiki\nOWT2\nBooks2\nBooks\nArXiv\nStackX\nGithub\nMath\nHNews\nEnron\nPubmed-C\nUbuntu\nPubmed-A\nFreeLaw\nNIH\nUSPTO\nPG-19\nPhil\nYoutube\nOpenSubs\nCC\n0\n20\n40\n60\n80\nPerplexity\nT5\nQwen-1.8B\nMistral-7B\nVicuna-13B\nFigure 5: Importance of High Quality Paraphraser: Perplexity across all the Pile domains\nfor WRAP on data generated by different LLMs. Results show that even small models like\nQwen-1.8B can generate paraphrases of high quality. Though, a low quality rephraser like\nour fine-tuned T5-base model leads to significantly worse language modeling.\nAverage\nWiki\nOWT2\nBooks2\nBooks\nArXiv\nStackX\nGithub\nMath\nHNews\nEnron\nPubmed-C\nUbuntu\nPubmed-A\nFreeLaw\nNIH\nUSPTO\nPG-19\nPhil\nYoutube\nOpenSubs\nCC\n0\n20\n40\n60\n80\n100\nPerplexity\nMed+C4\nDel+C4\nSub+C4\nC4\nFigure 6: Is re-phrasing same as any augmentation? We compare perplexity on the Pile for\ndifferent augmemntation strategies. 350M parameter models are trained for a total of 15B\ntokens. WRAP (Medium + C4) performs significantly better than traditional augmentations.\nRQ6: Is there data leakage from the rephrase model to the trained model?\nWe investigate\nwhether our synthetic data maintains similar semantic meaning while being stylistically\ndifferent from the original C4 data and matching the style of different PILE domains. We\nstart by comparing pairs of examples of synthetic and real data to confirm the performance\ngain is not attributed to knowledge leakage from the rephrase models. We take a subset of\nthe first 1000 samples from each of the datasets.\nWe show the cosine similarity of the sentence embeddings from a pre-trained BERT model\ntrained with SimCSE objective (Gao et al., 2021) for medium and qa prompts in Figure 8(a)\nand (b). When computing similarity, we remove outliers. Figures with distributions use\na gaussian Kernel Density Estimator (KDE) to construct distributions for statistics from\n11\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nAverage\nWiki\nOWT2\nBooks2\nBooks\nArXiv\nStackX\nGithub\nMath\nHNews\nEnron\nPubmed-C\nUbuntu\nPubmed-A\nFreeLaw\nNIH\nUSPTO\nPG-19\nPhil\nYoutube\nOpenSubs\nCC\n0\n25\n50\n75\n100\n125\n150\nPerplexity\nEasy+C4\nMed+C4\nHard+C4\nQA+C4\nCombined+C4\nC4\nFigure 7: Impact of style of synthetic rephrases: Perplexity across all domains of the Pile\ncomparing different styles of synthetic data. We train 128M parameter models for 3B tokens.\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSimilarity\n0\n2\n4\n6\n8\n10\nDensity\nReal1-Para1\nPara1-Para2\nReal1-Real2\n1st Half-2nd Half\n1st Half-Real1\n(a) Cosine similarity Medium synthetic data\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSimilarity\n0\n1\n2\n3\n4\n5\n6\n7\nDensity\nReal1-Para1\nPara1-Para2\nReal1-Real2\n1st Half-2nd Half\n1st Half-Real1\n(b) Cosine similarity QA synthetic data\nFigure 8: Comparison between synthetic and real data from the C4 corpus showing that\nsynthetic data maintains semantic meaning compared with the real C4 data and primarily\nchanges style for (a) medium rephrases of C4, and (b) QA rephrases of C4.\n1000 values. The cosine similarity of real-synthetic pairs is higher than several baselines\nincluding two random real samples from C4, a continuation baseline which computes cosine\nbetween the first half of a sample and the full sample, and cosine similarity between the\nfirst half and second half of the same sample. High similarity indicates that the re-phrases\nmaintain similar meaning to their real counterparts without adding information.\n7\nLimitations and Opportunities\n7.1\nCost Analysis\nShould you generate synthetic data, or just train longer on real data?\nThe applications of WRAP lies in both paradigms\u2014(i) low-resourced data settings such as a\nlanguage model for Finnish language (Luukkonen et al., 2023), and (ii) data-rich settings\nsuch as training on the common crawl. In the former, there is no alternative option of\nnaively gathering more data, and hence, synthetic data is a natural solution that should\noutperform training on in-domain data alone. However, there is a significant interest in\ntraining language models on English, or more broadly, general web data. Is using synthetic\ndata a viable option even in this paradigm?\n12\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nBefore, we dive into the feasibility of pre-training on synthetic data, we should acknowledge\nthe results of Table 1. The TinyLlama model trained for 3 Trillion tokens also underperforms\na model jointly trained on real and synthetic data. In fact, it performs quite comparably to\nthe models that were trained for 300B tokens on just real data as well. This suggests that\nthe ceiling for improvement by training for longer may not be that high (for a model of size\n350M/1.3B parameters; larger models may benefit from training for longer).\nTo analyze this cost trade-off, we compare the cost of generating synthetic data, versus that\nof training a language model on extra data. For our synthetic data generation experiments,\nwe use the vLLM (Kwon et al., 2023) library for fast generation. In particular, we are able to\ngenerate 3M tokens per hour on a single A100 when using the Mistral-7B. Generating 85B\ntokens (as in our work) accounts for about 25K GPU hours.\nIn comparison, on 64 A100s, we achieve a throughput of 0.5M tokens per second. Assuming\ntraining for 300B tokens, would mean 256 GPU days, accounting for about 6k GPU hours to\ntrain a single model. On the contrary, training a 13B model would take about 30K GPU hours.\nAt the scale of training a 13B model, reducing the training cost by 3-10x can incorporate the\ncost overhead of training with synthetic data in a single run.\nWhile the cost of generating high quality data is still relatively high, two important sources\nof improvement impact this cost analysis. First, if we use the Qwen-1.8B model Bai et al.\n(2023b) for rephrasing, we are able to get a 3x higher token throughput. As seen in our\npreliminary results in Fig 5, the model pre-trained on rephrases generated by Qwen model\nperforms comparably to that by the Mistral model. This reduces the cost of generation by 3x.\nMore recent work in speculative decoding (Liu et al., 2023c) and optimized inference (Xia\net al., 2024) suggest that we can leverage another 3-5x improvement in the generation cost.\nHence, indeed, even at the scale of just 1.3B parameter model training, we can already\nimprove upon the cost of pre-training using just real data.\nTwo additional important advantages of synthetic data generation that could not be ac-\ncounted for in the discussion above:\n1. The cost of synthetic data generation is a one-time investment, and we may train\nmany models of varying scales once the data is generated.\n2. Data generation is 100% parallelizable, whereas training requires the availability of\na big cluster with fast inter-node connectivity. This is much more expensive. On the\nother hand, generation can be thought of as a side process that can fill in the empty\nGPUs in any large-scale compute cluster, and runs on single GPU machines.\n7.2\nDiversity of Synthetic Generations\nAnother limitation is enforcing the diversity in the generated data. This diversity comes from\nboth the \u201cstyle\u201d and the \u201cknowledge\u201d contained in the generated data. Recent works (Li\net al., 2023b;c) used a selection of topics, or scenarios to seed the model to generate novel\ntexts. Still, a recent study by Padmakumar et al. (2023) showed that using language models\nfor AI-assisted writing tends to reduce content diversity, particularly when using instruction-\ntuned models. While we used the paradigm of rephrasing specifically to mitigate the issues\npertaining to the diversity of novel content generation, it remains for future work to assess\nthe presence (or lack of) and impact of content diversity in paraphrase models.\n8\nConclusion\nStrong language models are being pre-trained on combinations of real and synthetic data.\nUsing synthetic data enables baking in desirable attributes such as fairness, bias, and style\n(like instruction following) directly into the data, eliminating the need to adjust the training\nalgorithm specifically. This offers an alternative approach to aligning language models to\nhuman values. The recent uptick in interest around synthetic data, especially for instruction-\ntuning language models, is noteworthy, with concurrent researchers also leveraging it for\npre-training. As we transition into this paradigm, understanding the properties of the data\nfed to our models is paramount. This paper aims to be a comprehensive guide on employing\n13\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\ndifferent synthetic style data in LLM pre-training. We delve into its significance from two\nvantage points: (1) In scenarios with scarce high-quality data, synthetic rephrases offer more\nvalue than mere repetition of existing data; (2) Synthetic data can be a boon for generalization\non different text domains, and for generating text in styles that are underrepresented in the\npre-training dataset. As practitioners generate synthetic data for training models, they will\nbe faced with important and expensive design choices\u2014(i) How important is the quality of\nthe synthetic data generator?; (ii) How to balance real and synthetic data? (iii) When does\ntraining on synthetic data reach a point of diminishing returns in terms of epochs? This\nwork takes a first step towards answering these questions.\nConversely, it\u2019s essential to note the inherent limitations, and opportunities with synthetic\ndata. We highlight two limitations: (1) cost of generation is still large and requires strong\nLMs, and (2) enforcing the diversity in the generated data is challenging. In this work,\nwe leverage the natural diversity of the web to generate synthetic \u201cre-phrases\u201d. This\nlimits the model from learning new \u201cknowledge\u201d and enhances the learning process only\nvia the provision of high-quality inputs. Whereas past work required a more intricate\nunderstanding of the blind spots of the model, potentially biasing the knowledge contained\nin the pre-training data distribution. Nonetheless, we demonstrate the potential of synthetic\ndata to improve LLM training efficiency both in compute and data size.\nReferences\nWinogrande: An adversarial winograd schema challenge at scale. 2019.\nAmro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S. Morcos. Semd-\nedup: Data-efficient learning at web-scale through semantic deduplication.\nArXiv,\nabs/2303.09540, 2023. URL https://api.semanticscholar.org/CorpusID:257557221.\nSina Alemohammad, Josue Casco-Rodriguez, Lorenzo Luzi, Ahmed Imtiaz Humayun,\nHossein Babaei, Daniel LeJeune, Ali Siahkoohi, and Richard G Baraniuk. Self-consuming\ngenerative models go mad. arXiv preprint arXiv:2307.01850, 2023.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Han-\nnaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with\noperation-based formalisms.\nIn Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pp. 2357\u20132367, Minneapolis, Minnesota, June\n2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245. URL\nhttps://aclanthology.org/N19-1245.\nShekoofeh Azizi, Simon Kornblith, Chitwan Saharia, Mohammad Norouzi, and David J\nFleet. Synthetic data from diffusion models improves imagenet classification. arXiv\npreprint arXiv:2304.08466, 2023.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,\nYu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu,\nGao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng\nRen, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-\nguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao,\nBowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang,\nZhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen\ntechnical report. arXiv preprint arXiv:2309.16609, 2023a.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin\nGe, Yu Han, Fei Huang, et al. Qwen technical report. arXiv preprint arXiv:2309.16609,\n2023b.\nHritik Bansal and Aditya Grover. Leaving reality to imagination: Robust classification via\ngenerated datasets. arXiv preprint arXiv:2302.02503, 2023.\n14\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning\nabout physical commonsense in natural language. In Thirty-Fourth AAAI Conference on\nArtificial Intelligence, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya\nRamesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam\nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are\nfew-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin\n(eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877\u20131901. Curran\nAssociates, Inc., 2020. URL https://proceedings.neurips.cc/paper files/paper/2020/\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang,\nVijay Srinivasan, Tianyi Zhou, Heng Huang, et al. Alpagasus: Training a better alpaca\nwith fewer data. arXiv preprint arXiv:2307.08701, 2023.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing.\nVicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\nURL https://lmsys.org/blog/2023-03-30-vicuna/.\nDami Choi, Alexandre Passos, Christopher J Shallue, and George E Dahl. Faster neural\nnetwork training with data echoing. arXiv preprint arXiv:1907.05550, 2019.\nChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and\nKristina Toutanova. BoolQ: Exploring the surprising difficulty of natural yes/no ques-\ntions. In Proceedings of the 2019 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-\npers), pp. 2924\u20132936, Minneapolis, Minnesota, June 2019. Association for Computational\nLinguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,\nand Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning\nchallenge. arXiv:1803.05457v1, 2018.\nTogether Computer. Redpajama: an open dataset for training large language models. 2023.\nURL https://github.com/togethercomputer/RedPajama-Data.\nEkin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical\nautomated data augmentation with a reduced search space. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition workshops, pp. 702\u2013703, 2020.\nKaustubh D. Dhole, Varun Gangal, Sebastian Gehrmann, Aadesh Gupta, Zhenhao Li,\nSaad Mahamood, Abinaya Mahendiran, Simon Mille, Ashish Srivastava, Samson Tan,\nTongshuang Wu, Jascha Sohl-Dickstein, Jinho D. Choi, Eduard Hovy, Ondrej Dusek,\nSebastian Ruder, Sajant Anand, Nagender Aneja, Rabin Banjade, Lisa Barthe, Hanna\nBehnke, Ian Berlot-Attwell, Connor Boyle, Caroline Brun, Marco Antonio Sobrevilla\nCabezudo, Samuel Cahyawijaya, Emile Chapuis, Wanxiang Che, Mukund Choudhary,\nChristian Clauss, Pierre Colombo, Filip Cornell, Gautier Dagan, Mayukh Das, Tanay Dixit,\nThomas Dopierre, Paul-Alexis Dray, Suchitra Dubey, Tatiana Ekeinhor, Marco Di Gio-\nvanni, Rishabh Gupta, Rishabh Gupta, Louanes Hamla, Sang Han, Fabrice Harel-Canada,\nAntoine Honore, Ishan Jindal, Przemyslaw K. Joniak, Denis Kleyko, Venelin Kovatchev,\nKalpesh Krishna, Ashutosh Kumar, Stefan Langer, Seungjae Ryan Lee, Corey James Levin-\nson, Hualou Liang, Kaizhao Liang, Zhexiong Liu, Andrey Lukyanenko, Vukosi Marivate,\nGerard de Melo, Simon Meoni, Maxime Meyer, Afnan Mir, Nafise Sadat Moosavi, Niklas\nMuennighoff, Timothy Sum Hon Mun, Kenton Murray, Marcin Namysl, Maria Obedkova,\nPriti Oli, Nivranshu Pasricha, Jan Pfister, Richard Plant, Vinay Prabhu, Vasile Pais, Libo\nQin, Shahab Raji, Pawan Kumar Rajpoot, Vikas Raunak, Roy Rinberg, Nicolas Roberts,\nJuan Diego Rodriguez, Claude Roux, Vasconcellos P. H. S., Ananya B. Sai, Robin M.\n15\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nSchmidt, Thomas Scialom, Tshephisho Sefara, Saqib N. Shamsi, Xudong Shen, Haoyue\nShi, Yiwen Shi, Anna Shvets, Nick Siegel, Damien Sileo, Jamie Simon, Chandan Singh, Ro-\nman Sitelew, Priyank Soni, Taylor Sorensen, William Soto, Aman Srivastava, KV Aditya\nSrivatsa, Tony Sun, Mukund Varma T, A Tabassum, Fiona Anting Tan, Ryan Teehan,\nMo Tiwari, Marie Tolkiehn, Athena Wang, Zijian Wang, Gloria Wang, Zijie J. Wang, Fux-\nuan Wei, Bryan Wilie, Genta Indra Winata, Xinyi Wu, Witold Wydma\u00b4nski, Tianbao Xie,\nUsama Yaseen, M. Yee, Jing Zhang, and Yue Zhang. Nl-augmenter: A framework for\ntask-sensitive natural language augmentation, 2021.\nJacob Eisenstein. What to do about bad language on the internet. In Proceedings of the 2013\nConference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pp. 359\u2013369, Atlanta, Georgia, June 2013. Association for\nComputational Linguistics. URL https://aclanthology.org/N13-1037.\nRonen Eldan and Yuanzhi Li. Tinystories: How small can language models be and still\nspeak coherent english? arXiv preprint arXiv:2305.07759, 2023.\nStanislav Fort, Andrew Brock, Razvan Pascanu, Soham De, and Samuel L Smith. Drawing\nmultiple augmentation samples per image during training efficiently decreases test error.\narXiv preprint arXiv:2105.13343, 2021.\nRichard Futrell, Kyle Mahowald, and Edward Gibson. Large-scale evidence of dependency\nlength minimization in 37 languages. Proceedings of the National Academy of Sciences, 112\n(33):10336\u201310341, 2015.\nSamir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao\nNguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp:\nIn search of the next generation of multimodal datasets. arXiv preprint arXiv:2304.14108,\n2023.\nLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason\nPhang, Horace He, Anish Thite, Noa Nabeshima, et al. The pile: An 800gb dataset of\ndiverse text for language modeling. arXiv preprint arXiv:2101.00027, 2020.\nLeo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles\nFoster, Laurence Golding, Jeffrey Hsu, Alain Le Noac\u2019h, Haonan Li, Kyle McDonell,\nNiklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf,\nAviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang,\nand Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL\nhttps://zenodo.org/records/10256836.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: Simple contrastive learning of sen-\ntence embeddings. In 2021 Conference on Empirical Methods in Natural Language Processing,\nEMNLP 2021, pp. 6894\u20136910. Association for Computational Linguistics (ACL), 2021.\nEdward Gibson et al. The dependency locality theory: A distance-based theory of linguistic\ncomplexity. Image, language, brain, 2000:95\u2013126, 2000.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00b4esar Teodoro Mendes, Allie Del Giorno,\nSivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi,\net al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023.\nSuchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke\nZettlemoyer, and Noah A. Smith. Whose language counts as high quality? measuring\nlanguage ideologies in text data selection. In Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, pp. 2562\u20132580, Abu Dhabi, United Arab Emirates,\nDecember 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.\nemnlp-main.165. URL https://aclanthology.org/2022.emnlp-main.165.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and\nJacob Steinhardt. Measuring massive multitask language understanding. Proceedings of\nthe International Conference on Learning Representations (ICLR), 2021.\n16\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nElad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten Hoefler, and Daniel Soudry.\nAugment your batch: Improving generalization through instance repetition. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8129\u20138138, 2020.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark,\net al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556,\n2022.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh\nChaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile\nSaulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\nQiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. Pubmedqa: A\ndataset for biomedical research question answering. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Processing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP), pp. 2567\u20132577, 2019.\nMatt Gardner Johannes Welbl, Nelson F. Liu. Crowdsourcing multiple choice science\nquestions. 2017.\nJaehun Jung, Peter West, Liwei Jiang, Faeze Brahman, Ximing Lu, Jillian Fisher, Taylor\nSorensen, and Yejin Choi. Impossible distillation: from low-quality model to high-quality\ndataset & model for summarization and paraphrasing. arXiv preprint arXiv:2305.16635,\n2023.\nAbdullatif K\u00a8oksal, Timo Schick, Anna Korhonen, and Hinrich Sch\u00a8utze. Longform: Opti-\nmizing instruction tuning for long text generation with corpus extraction. arXiv preprint\narXiv:2304.08460, 2023.\nWoosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\nJoseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large\nlanguage model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th\nSymposium on Operating Systems Principles, 2023.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Cheng-\nhao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii\nZheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj,\nJoel Lamy-Poirier, Jo\u02dcao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,\nArmel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muh-\ntasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel,\nDmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi\nBhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Ku-\nnakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\nSchlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer\nRobinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy,\nDaniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u02dcnoz Ferrandis, Sean Hughes,\nThomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. Starcoder: may the\nsource be with you! 2023a.\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke Zettlemoyer, Omer Levy, Jason We-\nston, and Mike Lewis. Self-alignment with instruction backtranslation. arXiv preprint\narXiv:2308.06259, 2023b.\nYuanzhi Li, S\u00b4ebastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat\nLee. Textbooks are all you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463,\n2023c.\nStephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic\nhuman falsehoods, 2021.\n17\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nBingbin Liu, Sebastien Bubeck, Ronen Eldan, Janardhan Kulkarni, Yuanzhi Li, Anh Nguyen,\nRachel Ward, and Yi Zhang. Tinygsm: achieving\u00bf 80% on gsm8k with small language\nmodels. arXiv preprint arXiv:2312.09241, 2023a.\nHanmeng Liu, Jian Liu, Leyang Cui, Zhiyang Teng, Nan Duan, Ming Zhou, and Yue\nZhang. Logiqa 2.0 \u2014 an improved dataset for logical reasoning in natural language\nunderstanding. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 1\u201316,\n2023b. doi: 10.1109/TASLP.2023.3293046.\nXiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao\nZhang. Online speculative decoding. arXiv preprint arXiv:2310.07177, 2023c.\nRisto Luukkonen, Ville Komulainen, Jouni Luoma, Anni Eskelinen, Jenna Kanerva, Hanna-\nMari Kupari, Filip Ginter, Veronika Laippala, Niklas Muennighoff, Aleksandra Piktus,\net al. Fingpt: Large generative models for a small language. arXiv preprint arXiv:2311.05640,\n2023.\nPratyush Maini. Phi-1.5 model: A case of comparing apples to oranges?\n2023. URL\nhttps://pratyushmaini.github.io/phi-1 5/.\nPratyush Maini, Sachin Goyal, Zachary C Lipton, J Zico Kolter, and Aditi Raghunathan.\nT-mars: Improving visual representations by circumventing text feature learning. arXiv\npreprint arXiv:2307.03132, 2023.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor\nconduct electricity? a new dataset for open book question answering. In EMNLP, 2018.\nNiklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus,\nNouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained\nlanguage models. arXiv preprint arXiv:2305.16264, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin,\nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language\nmodels to follow instructions with human feedback. Advances in Neural Information\nProcessing Systems, 35:27730\u201327744, 2022.\nMasanori Oya. Three types of average dependency distances of sentences in a multilingual\nparallel corpus. In Proceedings of the 35th Pacific Asia Conference on Language, Information\nand Computation, pp. 652\u2013661, 2021.\nVishakh Padmakumar, Behnam Hedayatnia, Di Jin, Patrick Lange, Seokhwan Kim, Nanyun\nPeng, Yang Liu, and Dilek Hakkani-Tur. Investigating the representation of open do-\nmain dialogue context for transformer models. In Proceedings of the 24th Meeting of the\nSpecial Interest Group on Discourse and Dialogue, pp. 538\u2013547, Prague, Czechia, September\n2023. Association for Computational Linguistics. URL https://aclanthology.org/2023.\nsigdial-1.50.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro\nCappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay.\nThe refinedweb dataset for falcon llm: outperforming curated corpora with web data, and\nweb data only. arXiv preprint arXiv:2306.01116, 2023.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini\nAgarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervision. In International conference\non machine learning, pp. 8748\u20138763. PMLR, 2021.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis\nSong, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling\nlanguage models: Methods, analysis & insights from training gopher. arXiv preprint\narXiv:2112.11446, 2021.\n18\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551,\n2020.\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\nLaion-5b: An open large-scale dataset for training next generation image-text models.\nAdvances in Neural Information Processing Systems, 35:25278\u201325294, 2022.\nZhiqiang Shen, Tianhua Tao, Liqun Ma, Willie Neiswanger, Joel Hestness, Natalia Vassilieva,\nDaria Soboleva, and Eric Xing. Slimpajama-dc: Understanding data combinations for llm\ntraining. arXiv preprint arXiv:2309.10818, 2023.\nIlia Shumailov, Zakhar Shumaylov, Yiren Zhao, Yarin Gal, Nicolas Papernot, and Ross\nAnderson.\nModel dementia: Generated data makes models forget.\narXiv preprint\narXiv:2305.17493, 2023.\nIrene Solaiman and Christy Dennison. Process for adapting language models to society\n(palms) with values-targeted datasets. Advances in Neural Information Processing Systems,\n34:5861\u20135873, 2021.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\nBrandon Trabucco, Kyle Doherty, Max Gurinas, and Ruslan Salakhutdinov. Effective data\naugmentation with diffusion models. arXiv preprint arXiv:2302.07944, 2023.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nPablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and\nAnson Ho. Will we run out of data? an analysis of the limits of scaling datasets in machine\nlearning. arXiv preprint arXiv:2211.04325, 2022.\nYuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Source\ncode is all you need. arXiv preprint arXiv:2312.02120, 2023.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco\nGuzm\u00b4an, Armand Joulin, and Edouard Grave. CCNet: Extracting high quality monolin-\ngual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and\nEvaluation Conference, pp. 4003\u20134012, Marseille, France, May 2020. European Language\nResources Association. ISBN 979-10-95546-34-4. URL https://aclanthology.org/2020.\nlrec-1.494.\nHeming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie\nLi, and Zhifang Sui. Unlocking efficiency in large language model inference: A compre-\nhensive survey of speculative decoding, 2024.\nSang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang,\nQuoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds\nup language model pretraining. arXiv preprint arXiv:2305.10429, 2023.\nFuzhao Xue, Yao Fu, Wangchunshu Zhou, Zangwei Zheng, and Yang You. To repeat or not\nto repeat: Insights from scaling llm under token-crisis. arXiv preprint arXiv:2305.13230,\n2023.\nHaichao Yu, Yu Tian, Sateesh Kumar, Linjie Yang, and Heng Wang. The devil is in the\ndetails: A deep dive into the rabbit hole of data filtering. arXiv preprint arXiv:2309.15954,\n2023.\n19\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can\na machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, 2019.\nPeiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. Tinyllama: An open-source\nsmall language model, 2024.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma,\nAvia Efrat, Ping Yu, Lili Yu, et al. Lima: Less is more for alignment. arXiv preprint\narXiv:2305.11206, 2023.\n20\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nA\nDataset Details\nA.1\nTraining Dataset\nThe primary pretraining corpus in our experiments is Colossal Clean Crawled Corpus (C4),\na curated English text dataset comprising over 170 billion tokens. This corpus is derived\nfrom CommonCrawl, a common practice in the pretraining of LLMs Brown et al. (2020);\nRaffel et al. (2020); Touvron et al. (2023). This data source is also prominently featured\nin openly available LLM pretraining corpora, including The Pile Gao et al. (2020) and\nRedPajama Computer (2023). There are different versions of CommonCrawl data and our\nselection of C4 for pretraining is driven by driven by its size and quality.\nWe also compare with pre-training on the Refined Web corpus Penedo et al. (2023). The\ndataset is also derived from the CommonCrawl, however has a more stringent filtering\nprocess. Our selection of Refined Web is for comparing synthetic rephrases to high quality\nsubsets of web data, which were shown to achieve similar performance compared with\ncurated datasets Penedo et al. (2023). For our experiments we used the first 3050 files and\ntrain for 300B tokens to match training on C4. We aso conduct experiments with the first\n1650 files to account for multiple epochs on the Refined Web dataset.\nA.2\nPile Perplexity Evaluation\nFor the evaluation phase, we employed 20 subsets from the Pile corpus. We excluded the\nEuroparl subset because it contained non-English language. The subsets used are: CC,\nStackExchange, Wikipedia, GitHub, PubMed Abstracts, Openwebtext2, Freelaw, Math, NIH,\nUSPTO, Hackernews, Enron, Books3, PubMed Central, Gutenberg, Arxiv, Bookcorpus2,\nOpensubtitles, Youtubesubtitles, Ubuntu, and Philpapers. We take the first 10000 samples\nfrom each subset and split into documents of maximum length 1024. The reported average\nin all perplexity plots is a weighted average over the perplexity of all domains according to\nthe ratios in Table 7.\nA.2.1\nPile Weighted Average Ratios\nWe report the ratios for samples according to the first 10,000 documents from our Pile\nvalidation set in Table 7. Note that there are some slight variations in the ratios compared\nwith those reported in (Gao et al., 2020), but most ratios are similar.\nA.3\nZero-shot Evaluation Dataset\nWe evaluate our models on a total of 13 different zero-shot benchmarks to assess their\nabilities across various natural language tasks. These benchmarks are categorized into two\nsubsets: Specialized Knowledge and General Understanding.\nSpecialized Knowledge\nThis subset comprises datasets that focus on domain-specific\nknowledge and expertise.\n\u2022 ARC Challenge (ARC-C): This dataset is part of the AI2 Reasoning Challenge\n(ARC) (Clark et al., 2018), containing science exam questions from grades 3 to 9. The\nARC Challenge set includes more difficult questions that necessitate higher-order\nreasoning.\n\u2022 SciQ: A dataset of science exam questions, specifically designed to evaluate the\nability of NLP models in understanding and reasoning within the scientific do-\nmain (Johannes Welbl, 2017).\n\u2022 PubMedQA: This dataset focuses on biomedical literature and is designed to\nevaluate the understanding of medical and healthcare-related information (Jin et al.,\n2019).\n\u2022 MathQA: This dataset challenges models in mathematical problem-solving, requir-\ning both numerical understanding and reasoning skills (Amini et al., 2019).\n21\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nDataset\nValidation Ratio (%)\nPublished Ratio (%)\nArXiv\n10.4\n9.0\nBookCorpus2\n0.8\n0.8\nBooks3\n11.8\n12.1\nPile-CC\n14.0\n18.11\nEnron\n0.1\n0.1\nEuroParl\n1.1\n0.7\nFreeLaw\n5.3\n6.1\nGithub\n10.9\n7.6\nGutenberg\n1.5\n2.2\nHackernews\n0.6\n0.6\nDm Mathematics\n2.0\n1.2\nNIH\n0.2\n0.3\nOpenSubtitles\n1.3\n1.6\nOpenWebText2\n8.2\n10.0\nPhilPapers\n0.7\n0.4\nPubMed Abstracts\n0.7\n3.1\nPubMed Central\n14.9\n14.4\nStackExchange\n5.8\n5.1\nUbuntu\n1.3\n0.9\nUSPTO\n2.7\n3.7\nWikipedia\n3.4\n1.5\nYoutubeSubtitles\n0.6\n0.6\nTable 7: Pile ratios for our evaluation compared with published ratios\n\u2022 MMLU: Multi-domain question answering, MMLU assesses the model\u2019s exper-\ntise over a wide range of specialized subjects, from professional domains to\nacademia (Hendrycks et al., 2021).\nGeneral Understanding\nThis subset contains datasets that test general cognitive skills,\nlanguage understanding, and common sense reasoning.\n\u2022 ARC Easy (ARC-E): The Easy set of the AI2 Reasoning Challenge (Clark et al.,\n2018) features questions from the same source as ARC-C but are considered less\nchallenging and do not require as advanced reasoning skills.\n\u2022 BoolQ: A dataset consisting of boolean (yes/no) questions, focusing on reading\ncomprehension and general understanding of natural language text (Clark et al.,\n2019).\n\u2022 Winogrande (Wino.): This dataset challenges models on common sense reasoning\nin a language context, focusing on pronoun disambiguation tasks (ai2, 2019).\n\u2022 PIQA: Physical Interaction Question Answering tests the understanding of every-\nday physical processes, an aspect of practical common sense (Bisk et al., 2020).\n\u2022 HellaSwag: This dataset evaluates a model\u2019s ability to complete scenarios in a\ncontextually and logically coherent manner, requiring both language understanding\nand common sense reasoning (Zellers et al., 2019).\n\u2022 TruthfulQA: Focused on the generation of truthful, accurate answers, this dataset\nchallenges models on their ability to discern and reproduce factually correct infor-\nmation (Lin et al., 2021).\n\u2022 OpenBookQA (OBQA): OpenBookQA requires understanding a wide array of facts\nand concepts, thereby evaluating the model\u2019s broader knowledge and reasoning\nskills (Mihaylov et al., 2018).\n\u2022 LogiQA-2: This dataset involves logical reasoning, testing the model\u2019s capability to\nunderstand and apply logical constructs and principles (Liu et al., 2023b).\n22\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nEach dataset in these subsets is carefully selected to challenge and evaluate specific aspects\nof natural language processing models, ranging from domain-specific knowledge in science,\nmedicine, and mathematics, to broader skills like common sense reasoning and general\nlanguage understanding.\nB\nFiltering Details for Synthetic Data\nWhen generating synthetic paraphrases using language models, we occasionally encounter\nthe challenge of extraneous introductions in the generated outputs. Such paraphrases might\ncommence with phrases like \u201dHere\u2019s a paraphrase...\u201d, \u201dThe following...\u201d or even contain\nkeywords such as \u201dhigh-quality English\u201d. To mitigate this, we\u2019ve developed a method to\nfilter and refine the synthetic outputs.\nB.1\nMethodology\nThe primary function, remove unwanted part, starts by splitting the input data into indi-\nvidual sentences. If the first sentence contains delimiters such as \u201d\\n\\n\u201d (indicating a\nnew paragraph) or \u201d:\u201d, the function checks the segment preceding the delimiter for the\naforementioned unwanted elements. If these elements are detected, the preceding segment\nis removed. The entire revised content is then reconstructed and returned. In cases where\nno modifications are applicable, but we still have the flagged keywords, we remove the\nparaphrase completely. To achieve this:\n1. Split the input data into individual sentences using the NLTK\u2019s sentence splitter\nfunction.\n2. Examine the first sentence for the presence of delimiters.\n3. If a delimiter is detected, check the preceding segment for unwanted elements.\n4. If unwanted elements are found, discard the preceding segment (before an occur-\nrence of \"\\n\\n\" or \":\").\n5. Modify and return the filtered paragraph.\nBased on manual inspection we found that the error rate (occurrence of sentences with\nunwanted elements) after the modification is less than 0.1%.\nC\nProperties of Synthetic Corpus\nTo understand the properties of synthetic data generated from the rephrase model that lead\nto better pre-training performance, we compare the semantic similarity, syntactic complexity,\nand diversity between synthetic data, C4 data, and data from the Pile. Our primary focus is\nanswering the following questions about synthetic data: (i) Do models trained on synthetic\ndata perform better due to information leakage from the rephrase model? (ii) Does the\nrephrase model accurately capture multiple styles? (iii) What attributes of synthetic data\nmake it high quality? Our investigation helps address what data is beneficial for better\ngeneralization to specific domains, and quantify the importance of data variability and\nquality.\nC.1\nExperimental Setup\nWe take a subset of the first 1000 documents from each of the datasets. For synthetic\ncomparisons with real C4 data, we take pairs of samples, while for Pile subsets, we take\nthe first 1000 samples from the test subset. When computing dataset quality statistics, we\nremove outliers more than two standard deviations in metric value. When the number\nof samples from the Pile subset was fewer than 1000, we split samples. Figures with\ndistributions use a Gaussian Kernel Density Estimator (KDE) to construct distributions for\nstatistics from 1000 values.\n23\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nSimilarity\n0\n1\n2\n3\n4\n5\n6\n7\n8\nDensity\nReal-RealP\nReal-Para\nReal-Real'\nR1-R2\nR1-Real\nRP1-Real\nR1-RealP\nFigure 9: Cosine similarity medium synthetic MRPC rephrases\nMed\nQA\nC4\nStackX\nWiki\nCC\nGithub\nPubmed-A\nOWT2\nFreeLaw\nMath\nNIH\nUSPTO\nHNews\nEnron\nBooks\nPubmed-C\nPG-19\nArXiv\nBooks2\nOpenSubs\nYoutube\nUbuntu\nPhil\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\nReading Level\n(a) Flesch-Kincaid Reading Level\nMed\nQA\nC4\nStackX\nWiki\nCC\nGithub\nPubmed-A\nOWT2\nFreeLaw\nMath\nNIH\nUSPTO\nHNews\nEnron\nBooks\nPubmed-C\nPG-19\nArXiv\nBooks2\nOpenSubs\nYoutube\nUbuntu\nPhil\n0.0\n0.2\n0.4\n0.6\nType Token Ratio\n(b) Type token ratio\nFigure 10: Comparison of readability and diversity (ttr) of synthetic data compared with C4\nand different subsets of the Pile.\nC.2\nSemantic Properties\nIn Section 6, we compared pairs of examples of synthetic and real data to confirm the\nperformance gain is not attributed to knowledge leakage from the rephrase models using\na pre-trained BERT model trained with SimCSE objective (Gao et al., 2021) for medium\nand qa prompts in Figure 8(a) and (b). We additionally compare the similarity of synthetic\nrephrases and actual rephrases using the MRPC corpus in Figure 9(c). We denote this\nadditional comparison by RealP (real paraphrase), while maintaining comparison of splits\nof the sentence: R1 and R2. Synthetic rephrases have similar cosine similarity on average\nand lower spread compared with true rephrases according in the MRPC corpus.\nAs the semantic information is similar between C4 and our synthetic data, we further\ninvestigate stylistic differences in the data. Figure 10(a) shows the Flesch\u2013Kincaid reading\nlevels for different rephrase styles, and the Pile. Our findings indicate that C4 is on the\nlow end of reading level (7-8). In contrast, medium increases the reading level to 10,\nand qa synthetic variants further reduces the reading level to 6. Medium synthetic data\nmatches the reading level of Wikipedia, and other high reading level datasets yielding better\nperformance on these domains. On QA synthetic data, we observe reduced reading level.\nThis is because we observed that sentences are typically split into question and answer\nleading to shorter setnences compared with in the original text and medium style rephrases.\n24\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nMed\nQA\nC4\nStackX\nWiki\nCC\nGithub\nPubmed-A\nOWT2\nFreeLaw\nMath\nNIH\nUSPTO\nHNews\nEnron\nBooks\nPubmed-C\nPG-19\nArXiv\nBooks2\nOpenSubs\nYoutube\nUbuntu\nPhil\n0\n2\n4\n6\nTree Depth\n(a) Tree Depth\nMed\nQA\nC4\nStackX\nWiki\nCC\nGithub\nPubmed-A\nOWT2\nFreeLaw\nMath\nNIH\nUSPTO\nHNews\nEnron\nBooks\nPubmed-C\nPG-19\nArXiv\nBooks2\nOpenSubs\nYoutube\nUbuntu\nPhil\n0\n1\n2\n3\nMDD\n(b) Mean Dependency Distance\nFigure 11: Comparison between synthetic and real data from the C4 corpus showing that\nsynthetic data have higher syntactic complexity indicated by higher average tree depth, and\nhigher mean dependency distance (MDD).\nThis leads to lower metric values for many of the metrics. For type token ratio, we note that\nthe diversity is quite similar between medium and most subsets of the Pile. The QA dataset\nhas particularly low TTR matching ubuntu, github, and math as these are more similar to\nQA format datasets and have heavy repetition of the Question, and Answer format.\nC.3\nSyntactic Properties\nFinally, we compare the mean tree depth (measured by the mean over setences of the\ndepth of the dependency tree), and mean dependency distance (measured as the average\ndependency distance of any pair of words within a sentence) in Figure 11, which have been\nshown to be good measures of syntactic difficulty Futrell et al. (2015); Gibson et al. (2000);\nOya (2021). We find similar trends as for reading level and TTR diversity where mediums\ntyle increase depth, mdd, and syntactic complexity in general. We find again that QA style\nreduces this complexity.\nD\nEvaluation Metrics\nThe metric utilized for evaluation is the macro token level perplexity. Given a batch of encoded\ntexts, the perplexity at the token level was computed as follows:\nGiven the accumulated loss over the entire dataset, denoted as L, and the total number of\ntokens, represented by T, the macro token-level perplexity, denoted as P, is calculated as:\nP = exp\n\u0012\nmin\n\u0012\n20, L\nT\n\u0013\u0013\n(3)\nWhere:\n\u2022 exp is the exponential function.\n\u2022 L is the cumulative loss over all shifted logits and labels in the dataset.\n\u2022 T is the total number of tokens in the dataset.\nThe value of 20 acts as an upper limit to stabilize the metric in cases of high loss values.\nE\nAdditional Results for Smaller Model and Token Sizes\nE.1\nResults for 350M Models Trained for 75B Tokens\nWe train models at smaller scales and demonstrate improvement. In particular we train\na 350M GPT-2-medium architecture for a total of 75B tokens. We show Pile perplexity\naveraged across the 21 domains is much lower than for that of the model trained only on\n25\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nAverage\nWiki\nOWT2\nBooks2\nBooks\nArXiv\nStackX\nGithub\nMath\nHNews\nEnron\nPubmed-C\nUbuntu\nPubmed-A\nFreeLaw\nNIH\nUSPTO\nPG-19\nPhil\nYoutube\nOpenSubs\nCC\n0\n20\n40\n60\n80\nPerplexity\nC4-60B\nC4-15B\nQA+C4-15B\nMed+C4-15B\nFigure 12: Perplexity across all domains of the Pile comparing combining multiple styles of\nsynthetic data. Models are 350M parameters trained for a total of 75B tokens.\nDataset (Real Tok.)\nARC-C\nSciQ\nPubMedQA\nMathQA\nMMLU\nAvg\nC4-15B\n21.2\n77.1\n50.6\n22.2\n23.1\n38.8\nC4-60B\n23.4\n76.2\n46.4\n22.0\n23.0\n38.2\nQA+C4-15B\n24.4\n79.8\n56.0\n21.7\n22.9\n41.0\nMed+C4-15B\n22.7\n74.5\n53.6\n22.0\n23.1\n39.2\nTable 8: Evaluation of 350M parameter LLMs trained for 75B tokens on Specialized Knowl-\nedge Tasks. This table presents the performance on tasks that require specific domain\nknowledge such as science, medicine, mathematics, and logic.\nC4 in Figure 12, and even lower than 1.3B models trained only on C4 in Figure 1c. We\nalso show an increase of 1.5% across general understanding language tasks, and roughly\n3% on specialized knowledge tasks in Tables 8\u20139 when adding QA rephrases. We also\nexperimented with medium rephrases at this smaller scale. Our findings indicate that the\nhigh quality provided by medium rephrases improves performance over only C4, however\nmatching the style as indicated by QA rephrase performance further improves performance.\nE.2\nResults for 1.3B Models Trained for 150B Tokens\nWe additionally train 1.3B GPT-2-XL models at 150B tokens, reducing the number of steps\nby half. We show Pile perplexity averaged across the 20 domains is much lower than for\nthat of the model trained only on C4 in Figure 13, and even lower than 1.3B models trained\nonly on C4 in Figure 1c for twice as long. We also show an increase of 2% across specialized\nknowledge tasks, and roughly 2.5% on general understanding tasks in Tables 10-11 when\nadding QA rephrases. We also experimented with medium rephrases at this smaller scale,\nand report similar findings consistent with other small-scale experiments.\nF\nLLM Leaderboard Few-shot Results\nIn our main experiments in Section 4 we demonstrate that LLMs trained with synthetic\nrephrases are a better backbone for zero-shot question-answering tasks as the model learns\nthe question-answer format and style during pre-training. In this section, we show that\nimprovements from pre-training on synthetic rephrases are still present even in few-shot\n26\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nDataset (Real Tok.)\nARC-E\nBoolQ\nWino.\nPIQA\nHellaSwag\nTruthfulQA\nOBQA\nLogiQA\nAvg\nC4-18B\n50.5\n52.8\n53.0\n69.8\n35.6\n37.8\n18.6\n23.0\n42.6\nC4-75B\n51.4\n53.4\n51.6\n70.3\n36.1\n39.0\n17.4\n22.6\n42.7\nQA+C4-18B\n53.4\n60.7\n52.2\n70.0\n36.3\n40.0\n17.6\n22.3\n44.1\nMed+C4-18B\n50.6\n57.3\n53.6\n70.8\n36.1\n36.9\n18.6\n22.0\n43.2\nTable 9: Evaluation of 350M parameter LLMs trained for 75B tokens on General Understand-\ning Tasks. This table shows the performance across various datasets, focusing on general\nreasoning, language understanding, and common sense comparing training .\nAverage\nWiki\nOWT2\nBooks2\nBooks\nArXiv\nStackX\nGithub\nMath\nHNews\nEnron\nPubmed-C\nUbuntu\nPubmed-A\nFreeLaw\nNIH\nUSPTO\nPG-19\nPhil\nYoutube\nOpenSubs\nCC\n0\n20\n40\n60\nPerplexity\nC4-150B\nC4-35B\nC4+QA-35B\nC4+Med-35B\nFigure 13: Perplexity across all domains of the Pile comparing combining multiple styles of\nsynthetic data. Models are 350M parameters trained for a total of 75B tokens.\nsettings where the model has access to test samples. To study few-shot performance, we\nevaluate on six tasks present in the OpenLLMLeaderboard2:\n1. ARC-Challenge (25 shot)\n2. HellaSwag (10 shot)\n3. MMLU (5 shot)\n4. Truthful-QA (5 shot)\n5. Winogrande (5 shot)\n6. GSM8k (5 shot)\nWe evaluate two models trained for 300B and 350B tokens corresponding to roughly 85B\nand 100B unique C4 tokens respectively. Our findings show substantial improvements on\nthe ARC-challenge benchmark, and Truthful-QA conssitent in the zero-shot settings and\ncomparable performance across other datasets. Our models also perform better than the\npublicly released Falcon-1.3B model trained on the Refined Web dataset, and the Pythia-1.4B\nmodel, which was trained on Pile.\n2https://huggingface.co/spaces/HuggingFaceH4/open llm leaderboard\n27\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nDataset (Real Tok.)\nARC-C\nSciQ\nPubMedQA\nMathQA\nMMLU\nAvg\nC4-35B\n27.0\n83.4\n55.0\n22.5\n24.3\n42.4\nC4-150B\n25.9\n83.8\n55.4\n23.5\n25.4\n42.8\nMed+C4-35B\n27.2\n82.2\n46.2\n23.1\n25.2\n40.8\nQA+C4-35B\n29.0\n85.1\n62.2\n22.5\n26.1\n45.0\nTable 10: Evaluation of \u223c 1.3B parameter LLMs trained for 150B tokens on Specialized\nKnowledge Tasks. This table presents the performance on tasks that require specific domain\nknowledge such as science, medicine, mathematics, and logic.\nDataset (Real Tok.)\nARC-E\nBoolQ\nWino.\nPIQA\nHellaSwag\nTruthfulQA\nOBQA\nLogiQA\nAvg\nC4-35B\n58.6\n55.2\n56.1\n73.9\n44.5\n36.0\n22.2\n22.8\n46.2\nC4-150B\n59.1\n54.4\n56.4\n74.5\n44.9\n34.3\n22.2\n22.1\n46.0\nMed+C4-35B\n59.8\n57.0\n55.7\n74.6\n44.5\n36.5\n23.8\n21.5\n46.7\nQA+C4-35B\n62.2\n63.3\n55.7\n74.8\n44.6\n41.4\n22.4\n23.2\n48.4\nTable 11: Evaluation of \u223c 1.3B parameter LLMs trained for 150B tokens on General Un-\nderstanding Tasks. This table shows the performance across various datasets, focusing on\ngeneral reasoning, language understanding, and common sense comparing training .\nDataset\nARC\nHellaswag\nMMLU\nTruthfulQA\nWinoGrande\nGSM8K\nAvg\nC4\n31.7\n62.1\n26.7\n33.4\n57.9\n0.9\n35.5\nFalcon-RW\n35.1\n63.6\n25.3\n36.0\n62.0\n0.5\n37.1\nPythia-1.4b-Pile\n32.7\n55.0\n25.6\n38.7\n57.3\n0.8\n35.0\nQA+C4-85B (300K)\n36.4\n60.9\n25.5\n40.6\n59.4\n0.4\n37.2\nQA+C4-100B (350K)\n35.5\n60.5\n26.8\n40.6\n61.3\n0.3\n37.5\nTable 12: 1.3B 300K LLM Leaderboard Eval. Evaluation is done on a single seed (1234).\n28\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nG\nRephrase Prompt Templates\nWe detail the prompts given to the Mistral-7B model to generate synthetic versions of the\nC4 dataset in specific styles. Note: there are slight variations in the prompt that were used for\nother frozen LLMs, and no prompt was used for the T5 model.\nEasy Style\nA style designed to generate content understandable by toddlers.\nA chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the questions.\nUSER: For the following paragraph give me a paraphrase of the same using\na very small vocabulary and extremely simple sentences that a toddler will\nunderstand:\nHard Style\nA style designed to generate content comprehensible primarily to scholars using arcane\nlanguage.\nA chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the questions.\nUSER: For the following paragraph give me a paraphrase of the same using very\nterse and abstruse language that only an erudite scholar will understand.\nReplace simple words and phrases with rare and complex ones:\nMedium Style\nA style designed to generate content comparable to standard encyclopedic entries.\nA chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the questions.\nUSER: For the following paragraph give me a diverse paraphrase of the same\nin high quality English language as in sentences on Wikipedia:\nQ/A Style\nA style intended to convert narratives into a conversational format.\nA chat between a curious user and an artificial intelligence assistant.\nThe assistant gives helpful, detailed, and polite answers to the questions.\nUSER: Convert the following paragraph into a conversational format with\nmultiple tags of \"Question:\" followed by \"Answer:\":\n29\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nH\nRephrase Examples\nSamplesfrom the MRPC corpus generated by the Mistral-7B model.\nOriginal\nThe stock rose $2.11, or about 11 percent, to close Friday at $21.51 on\nthe New York Stock Exchange.\nRevenue\nin\nthe\nfirst\nquarter\nof\nthe\nyear\ndropped\n15\npercent\nfrom\nthe same period a year earlier.\nMedium Style\nThe stock experienced an increase of approximately 11 percent, closing at\n$21.51 on the New York Stock Exchange on Friday, with a rise of $2.11.\nDuring the initial three months of the current year,\nthere was a 15\npercent decrease in revenue compared to the corresponding quarter of the\nprevious year.\nQ/A Style\nQuestion: What was the stock\u2019s closing price on Friday?\nAnswer: $21.51\nQuestion: How much did the stock rise on Friday? Answer: $2.11 or about\n11 percent.\nQuestion:\nWhat\nwas\nthe\nrevenue\ndrop\nin\nthe\nfirst\nquarter\ncompared\nto the same period last year? Answer: The revenue dropped 15 percent.\n30\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nSamples from the C4 corpus generated by the Mistral-7B model.\nOriginal\nFirst round on stress at work survey.\nAnswering the questionnaire is\nvoluntary and all answers will be saved anonymously. Please fill in this\nquestionnaire only if you have some work experience, part-or full time.\nOtherwise, you will not be able to answer some of the questions! Here is\na the link to all language version.\nNot\nthat\nthere\u2019s\na\nthing\nwrong\nwith\nfrozen\nburgers.\nThe\nkey\nhere\nis the meat seasonings, which are pretty strong and spicy and just GOOD,\nsomething else I think is really necessary in a turkey burger because\nground turkey otherwise can be kind of flavorless.\nYou\u2019ll need ground\nturkey, onion powder, chili powder, salt, pepper, and cayenne pepper for\nthe burgers.\nThen the mayo takes garlic and onion.\nThen we need buns,\nclearly, swiss cheese, lettuce, and onion. I LOVE tomatoes but sometimes\nfind that they get in the way of other flavors, so I left them off of this\nburger. Add them if you\u2019d like to your array of toppings! First, we\u2019ll\nmake the mayo.\nGrate the garlic directly into the mayo, add a pinch of\nsalt, and squeeze in the lemon juice. Stir. Done! I love this. Then, we\u2019ll\nwork on the burgers. Preheat a large skillet to medium-high heat with some\nolive oil, preheat the broiler to high, then add all the spices to the\nground turkey.\nWhether you like your velvet crushed, vibrant or head-to-toe, there\u2019s\nreally no denying the sheer luxe and elegance of this timeless textile.\nNot only is it super stylish, it can actually be so wearable for day-to-day\nwear.\nYes, really!\nThis year it\u2019s all about embracing fun gem-toned\nvelvety pieces. Long gone are the days when velvet was solely associated\nwith dark moody shades of navy and black. Below we\u2019ve rounded up the most\ncovetable velvet pieces on the high street right now. We\u2019re already coming\nup with outfit ideas! Are you completely obsessed or beyond bored of it?\nSave\nup\nto\n$8,086\non\none\nof\n1,258\nChrysler\n200s\nnear\nyou.\nFind\nyour perfect car with Edmunds expert and consumer car reviews, dealer\nreviews, car comparisons and pricing tools. We have 4,850,420. Research\n2015 Chrysler 200 Sedan 4D 200C I4 prices, used values & 200 Sedan 4D 200C\nI4 pricing, specs and more. Many years ago, we wrote about the stalling\nproblem with the 2011 Chrysler 200, and believe it or not, we still receive\nan occasional call regarding the problem.However, a much larger issue has\nmonopolized the phone lines as of late 2015 Chrysler 200 transmission\nproblems leaving drivers with check engine lights, harsh shifting, and the\noccasional loss of power. The 2015 Chrysler 200 can fetch a premium for\nits style and its horsepower--but rear-seat room and handling are better\nbargains elsewhere.\nFind out why the 2015 Chrysler 200 is rated 8.4 by\nThe. Don\u2019t know where to find the perfect rims for your 2015 Chrysler 200\nCARiD.com stores a massive selection of 2015 Chrysler 200 wheels offered\nin myriads of design and finish options, including chrome, black, silver,\nand so much more.\n31\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nMedium Style\nIn the initial stage of the survey on work-related stress, participation is\noptional, and all responses will be kept confidential. It is important to\nnote that only those with work experience, whether part-time or full-time,\nwill be able to accurately respond to certain questions.\nTo complete\nthe questionnaire, please use the following link, which is available in\nmultiple languages.\nFrozen\nburgers\nare\nnot\ninherently\nflawed,\nbut\nthe\nmeat\nseasonings\nare a crucial element that elevates the flavor of the dish. Ground turkey\ncan be quite bland without the right blend of spices, so it is essential\nto add onion powder, chili powder, salt, pepper, and cayenne pepper to the\nmixture. Additionally, garlic and onion should be used in the mayonnaise,\nwhich is necessary for the burger\u2019s topping.\nTo make the mayo, grate\nthe garlic directly into the mixture, add a pinch of salt, and squeeze\nin lemon juice. Mix well. The mayonnaise is a delicious addition to the\nburger topping. Now, let\u2019s work on the burgers. Heat a large skillet over\nmedium-high heat with some olive oil, and preheat the broiler to high. Add\nthe seasonings to the ground turkey and mix well. The burgers should be\ncooked on the skillet for about 5 minutes on each side, then transferred\nto the broiler to cook for an additional 2-3 minutes.\nThe burgers are\nready to be assembled on buns, along with swiss cheese, lettuce, and onion.\nTomatoes can be added as a topping, but they may overpower other flavors.\nIt is up to the individual to decide if they want to include them in their\nburger. In conclusion, frozen burgers can be tasty, but the meat seasonings\nare essential to add depth and flavor to the dish.\nThe mayonnaise and\ntoppings can also be customized to suit individual preferences.\nThe luxurious and elegant nature of velvet cannot be denied, whether it\nis crushed, vibrant, or head-to-toe.\nThis timeless textile is not only\nfashionable but also wearable for daily wear. In recent years, there has\nbeen a shift towards embracing fun gem-toned velvety pieces, breaking away\nfrom the traditional association of velvet with dark moody shades of navy\nand black. Below are some of the most desirable velvet pieces available on\nthe high street today, and we are already envisioning outfits around them.\nAre you completely enamored with velvet or simply beyond bored with it?\nDiscover\nsavings\nup\nto\n$8,086\non\none\nof\n1,258\nChrysler\n200s\nnear\nyou. Get expert and consumer car reviews, dealer reviews, car comparisons,\nand pricing tools from Edmunds.\nWe have 4,850,420 listings for 2015\nChrysler 200 Sedan 4D 200C I4 prices, used values, and more. While we wrote\nabout the stalling problem with the 2011 Chrysler 200, we still receive\noccasional calls regarding the issue. However, a more significant issue\nhas taken over the phone lines recently - 2015 Chrysler 200 transmission\nproblems that leave drivers with check engine lights, harsh shifting, and\noccasional loss of power.\nDespite the premium price for its style and\nhorsepower, rear-seat room and handling are better bargains elsewhere. The\n2015 Chrysler 200 is rated 8.4 by The Car Connection. If you\u2019re looking\nfor the perfect rims for your 2015 Chrysler 200 CARiD.com offers a vast\nselection of 2015 Chrysler 200 wheels in various design and finish options,\nincluding chrome, black, silver, and more\n32\nRephrasing the Web: A Recipe for Compute & Data-Efficient Language Modeling\nQ/A Style\nQuestion:\nIs\nparticipating\nin\nthe\nstress\nat\nwork\nsurvey\nmandatory?\nAnswer:\nNo,\nit\nis\nvoluntary.\nQuestion:\nWill\nall\nanswers\nbe\nsaved\nanonymously? Answer: Yes, they will. Question: Who is eligible to take\nthe questionnaire? Answer: Only individuals with work experience, whether\npart-time or full-time. Question: What happens if someone is not eligible\nto take the questionnaire? Answer: They will not be able to answer some of\nthe questions. Question: Can you provide a link to all language versions\nof the questionnaire? Answer: Yes, here is the link.\nQuestion:\nWhat\nmakes\nthese\nfrozen\nburgers\nso\ngood?\nAnswer:\nThe\nmeat seasonings are strong and spicy, and they give the burgers a good\nflavor. Question: Do you think ground turkey can be flavorless? Answer:\nYes, ground turkey can be flavorless, but adding spices and seasonings\ncan help. Question: What do you need to make these turkey burgers? Answer:\nGround turkey,\nonion powder,\nchili powder,\nsalt,\npepper,\nand cayenne\npepper for the burgers. Then the mayo takes garlic and onion. Buns, swiss\ncheese, lettuce, and onion are also needed. Question: What about toppings?\nAnswer: Add tomatoes if you\u2019d like to your array of toppings. Question:\nHow do you make the mayo?\nAnswer:\nGrate the garlic directly into the\nmayo, add a pinch of salt, and squeeze in the lemon juice. Stir. Done!\nQuestion: How do you work on the burgers? Answer: Preheat a large skillet\nto medium-high heat with some olive oil, preheat the broiler to high, then\nadd all the spices to the ground turkey.\nQuestion:\nWhat\ndo\nyou\nthink\nof\nvelvet?\nAnswer:\nWhether\nyou\nlike\nyour velvet crushed, vibrant or head-to-toe, there\u2019s really no denying\nthe sheer luxe and elegance of this timeless textile. Question: Is velvet\nsuper stylish?\nAnswer: Yes, really!\nThis year it\u2019s all about embracing\nfun gem-toned velvety pieces. Question: Was velvet solely associated with\ndark moody shades of navy and black? Answer: Long gone are the days when\nvelvet was solely associated with dark moody shades of navy and black.\nQuestion: Do you have any covetable velvet pieces on the high street right\nnow? Answer: Below we\u2019ve rounded up the most covetable velvet pieces on\nthe high street right now. Question: Are you completely obsessed or beyond\nbored of it? Answer: We\u2019re already coming up with outfit ideas!\nQuestion:\nHow can I save money on a Chrysler 200?\nAnswer:\nSave up\nto $8,086 on one of 1,258 Chrysler 200s near you.\nFind your perfect\ncar with Edmunds expert and consumer car reviews, dealer reviews, car\ncomparisons and pricing tools. Question: What are the problems with the\n2015 Chrysler 200? Answer: We have 4,850,420. Research 2015 Chrysler 200\nSedan 4D 200C I4 prices, used values & 200 Sedan 4D 200C I4 pricing, specs\nand more.\nMany years ago, we wrote about the stalling problem with the\n2011 Chrysler 200, and believe it or not, we still receive an occasional\ncall regarding the problem. However, a much larger issue has monopolized\nthe phone lines as of late 2015 Chrysler 200 transmission problems leaving\ndrivers with check engine lights, harsh shifting, and the occasional loss\nof power. Question: What are the benefits of buying a 2015 Chrysler 200?\nAnswer: The 2015 Chrysler 200 can fetch a premium for its style and its\nhorsepower--but rear-seat room and handling are better bargains elsewhere.\nQuestion: How is the 2015 Chrysler 200 rated? Answer: It\u2019s rated 8.4 by\nThe. Question: Where can I find the perfect rims for my 2015 Chrysler 200?\nAnswer: CARiD.com stores a massive selection of 2015 Chrysler 200 wheels\noffered in myriads of design and finish options, including chrome, black,\nsilver, and so much more.\n33\n"
  },
  {
    "title": "Motion-I2V: Consistent and Controllable Image-to-Video Generation with Explicit Motion Modeling",
    "link": "https://arxiv.org/pdf/2401.15977.pdf",
    "upvote": "28",
    "text": "Motion-I2V: Consistent and Controllable Image-to-Video Generation with\nExplicit Motion Modeling\nXiaoyu Shi1\u2217\nZhaoyang Huang7\u2217B\nFu-Yun Wang1\u2217\nWeikang Bian1\u2217\nDasong Li1\nYi Zhang3\nManyuan Zhang1\nKa Chun Cheung2\nSimon See2\nHongwei Qin3\nJifeng Dai4\nHongsheng Li1,5,6B\n1Multimedia Laboratory, The Chinese University of Hong Kong\n2NVIDIA AI Technology Center\n3SenseTime Research\n4Tsinghua University\n5Centre for Perceptual and Interactive Intelligence (CPII)\n6Shanghai AI Laboratory\n7Avolution AI\nxiaoyushi@link.cuhk.edu.hk\nInput Frame\nOutput Video\nImage2Video\nInput Frame\nOutput Video\nMotion Drag (+ Brush)\nVideo2Video\nOutput Video\nInput Video\nInput Frame\nOutput Video\nInput Frame\nOutput Video\nFigure 1:\nMotion-I2V can generate consistent image-to-video results with large motion and viewpoint change. It also\nnaturally supports users to more precisely control the motion trajectories and animated region with sparse trajectories (red\ncurved arrow) and motion brush (purple mask). Additionally, the second stage of Motion-I2V also supports zero-shot video-\nto-video translation.\nAbstract\nWe introduce Motion-I2V, a novel framework for consis-\ntent and controllable image-to-video generation (I2V). In\ncontrast to previous methods that directly learn the compli-\ncated image-to-video mapping, Motion-I2V factorizes I2V\ninto two stages with explicit motion modeling. For the first\nstage, we propose a diffusion-based motion field predic-\ntor, which focuses on deducing the trajectories of the ref-\nerence image\u2019s pixels. For the second stage, we propose\nmotion-augmented temporal attention to enhance the lim-\nited 1-D temporal attention in video latent diffusion mod-\n1\narXiv:2401.15977v2  [cs.CV]  31 Jan 2024\nels. This module can effectively propagate reference im-\nage\u2019s feature to synthesized frames with the guidance of\npredicted trajectories from the first stage. Compared with\nexisting methods, Motion-I2V can generate more consis-\ntent videos even at the presence of large motion and view-\npoint variation. By training a sparse trajectory Control-\nNet for the first stage, Motion-I2V can support users to\nprecisely control motion trajectories and motion regions\nwith sparse trajectory and region annotations.\nThis of-\nfers more controllability of the I2V process than solely re-\nlying on textual instructions.\nAdditionally, Motion-I2V\u2019s\nsecond stage naturally supports zero-shot video-to-video\ntranslation.\nBoth qualitative and quantitative compar-\nisons demonstrate the advantages of Motion-I2V over prior\napproaches in consistent and controllable image-to-video\ngeneration.\nPlease see our project page at https://\nxiaoyushi97.github.io/Motion-I2V/.\n1. Introduction\nImage-to-video generation (I2V) targets at animating a\ngiven image to a video clip with natural dynamics, while\npreserving the visual appearance. It has widespread appli-\ncations in fields of film industry, augmented reality, auto-\nmatic advertising and content creation for social media plat-\nforms. Traditional I2V methods, however, focus on specific\ncategories (e.g. human hair [71], fluid [20, 37, 38, 43], por-\ntraits [13, 65, 66, 67, 14]). Consequently, such specializa-\ntion restricts their utility in more diverse, open-domain sce-\nnarios.\nIn recent years, diffusion models [49, 51, 41] trained on\nweb-scale image datasets have made impressive strides in\nproducing high-quality and diverse images. Encouraged by\nthis success, researchers have begun extending these mod-\nels to the realm of I2V, aiming to leverage the strong image\ngenerative priors. These works [80, 6, 72, 64, 77] typically\nequip text-to-image (T2I) models with 1-D temporal atten-\ntion modules to create video base models. However, I2V\npresents more challenges compared to static image genera-\ntion. It requires modeling the complicated spatial-temporal\npriors. The narrow temporal receptive field of 1-D temporal\nattention makes it difficult to ensure temporal consistency\nof the generated videos, especially in the presence of large\nmotion. Another notable shortage of current I2V works is\ntheir limited controllability. These models primarily utilize\nthe reference image and textual instructions as the genera-\ntion conditions, but lack precise and even interactive con-\ntrol of the generated motions. This is in stark contrast to\nthe field of image manipulation, where techniques like drag-\nbased [54, 40, 44] and region-specific [22, 70] controls have\ndemonstrated substantial efficacy.\nTo remedy the aforementioned issues,\nwe present\nMotion-I2V, a framework that factorizes image-to-video\ngeneration into two stages. The first stage focuses on pre-\ndicting the plausible motions, in the form of pixel-wise\ntrajectories. With such explicit motion modeling, the sec-\nond stage is responsible for generating consistent animation\nwith the predicted dynamics from the first stage. Specifi-\ncally, in the first stage, we tune a pre-trained video diffu-\nsion model for motion field prediction. It takes the refer-\nence image and textual instruction as conditions, and pre-\ndicts the trajectories of all pixels in the reference image.\nIn the second stage, we propose a motion-augmented tem-\nporal attention to enhance the video diffusion model. The\nlatent features of the reference image are warped according\nto all pixels\u2019 predicted trajectories and act as guidance via\nadaptively (through cross-attention) injecting into the syn-\nthesized frames at multiple scales. This warping operation\nbrings dynamic temporal receptive field and alleviates the\npressure of learning the complicated spatial-temporal pat-\nterns with only 1-D temporal attention.\nInspired by previous successes of adapting pre-trained\nlarge-scale model [12, 76, 10, 11], we also train a Control-\nNet [75] for motion prediction in the first stage, which takes\nsparse trajectories as the condition and generates plausible\ndense trajectories. This design empowers users to manipu-\nlate object motions with very sparse trajectory annotations.\nOur framework also naturally supports region-specific an-\nimation (named motion brush), enabling users to animate\nselected image areas with custom motion masks. More-\nover, the second stage of Motion-I2V is capable of achiev-\ning video-to-video translation, where the trajectories are ob-\ntained from the source video. Users can transform the first\nframe with existing image-to-image tools and consistently\npropagate the transformed first frame using the second stage\nof Motion-I2V. These characteristics grant users enhanced\ncontrollability over the I2V process.\n2. Related Work\n2.1. Image Animation\nAnimating a single image has attracted a lot of attention\nin the research field. Previous approaches simulate motion\nfor natural dynamics [20, 32, 37, 43, 73, 50, 27], human\nfaces [66, 67, 14] and bodies [69, 67, 55, 29, 3]. Some of\nthe previous methods employ optical flow to model the mo-\ntion and uses warping-based rendering techniques. We get\ninspiration from this line of research and introduce explicit\nmotion modeling into modern generative models. Recent\ndevelopments on image animation are driven by diffusion\nmodels [49, 19, 57]. Mahapatra et al. [38] transplant the es-\ntimated optical flow to artistic paintings with a pre-trained\ntext-to-image diffusion model. Li et al. [33] utilize a diffu-\nsion model to handle the natural oscillating motions. These\nanimation approaches can only synthesize specific types of\ncontent and motion such as time-lapse videos and body ani-\nStage1: Explicit Motion Generation\nStage 2: Motion-Guided I2V\nI2V 3D U-Net\n\u201cTiger in the water\u201d\nCLIP\nFlow 3D U-Net\nTimestep\nMotion Strength\nSpatial Layer\nText Cross Attention Layer\nTemporal Layer\nMotion Augmented Temporal Layer\nMLP\nMLP\n\u201cTiger in the water\u201d\nCLIP\nFigure 2: Overview of Motion-I2V. The first stage of Motion-I2V targets at deducing the motions that can plausibly animate\nthe reference image. It is conditioned on the reference image and text prompt, and predicts the motion field maps between\nthe reference frame and all the future frames. The second stage propagates reference image\u2019s content to synthesize frames. A\nnovel motion-augmented temporal layer enhances 1-D temporal attention with warped features. This operation enlarges the\ntemporal receptive field and alleviates the complexity of directly learning the complicated spatial-temporal patterns.\nmation. To solve this problem, some diffusion-based meth-\nods [72, 77, 64, 80, 6], are proposed to address the chal-\nlenge of open-domain image animation. They capitalize on\nthe strong generative priors of pre-trained diffusion models\nand have achieved unprecedented open-domain animation\nperformance. However, these typically rely on vanilla 1-D\ntemporal attention to learn the complicated image to video\nmapping. We propose to enlarge the receptive fields with\nexplicit motion prediction.\n2.2. Diffusion Models\nDiffusion models (DMs) [19, 57] have recently shown\nmore stable training, better sample quality, and flexibility\nthan VAE [31], GAN [15] and FLow models [7]. DALL-E\n2[47], GLIDE [41] and Imagen [51] employ diffusion mod-\nels for text-to-image generation by conducting the diffusion\nprocess in pixel space, guided by language models [45, 46]\nor classifier-free approaches. Stable diffusion [49] shows\nunprecendented power on text-to-image generation by per-\nforming denoising diffusion on the latent space and sup-\nports many downstream applications [79, 78].\nRecent attention has also been paid to employing diffu-\nsion models [49] for video synthesis.\nNotably, Imagen-\nVideo [18] and Make-A-Video [56] perform denoising dif-\nfusion in video pixel space, while MagicVideo [82] models\nthe video distribution in the latent space. Video-P2P [35]\nand vid2vid-zero [63] propose to edit the video via cross-\nattention map manipulation. Text-to-video zero [30] con-\nstruct latent code to model dynamics to employ stable dif-\nfusion models [49] for video generation. Wang et al. pro-\npose a versatile pipeline for extending the video generation\nlength [62]. VideoComposer [64] adopts textual condition,\nspatial conditions and temporal conditions on the video dif-\nfusion models. Zhang et al. propose a cascaded i2vgen-XL\n[77] to ensure semantically and qualitatively excellent video\ngeneration. Dynamicrafter [72] proposes a dual-stream im-\nage injection mechanism to utilize the motion prior of text-\nto-video diffusion models.\nThese methods [64, 72, 77]\nusually allow the diffusion model to handle motion mod-\neling and video generation simulateously, which leads to\nunrealistic motions and temporal inconsistent visual details.\nOur method decouples the motion modeling and video de-\ntails generation to achieve realistic motions and preserve the\npleasant details.\n2.3. Motion Modeling\nMotion modeling aims to understand and predict the\nmovement of objects. Optical flow is a common approach\nto represent motion, which estimates the displacement field\nbetween two consecutive frames. Early work formulated\noptical flow estimation as an optimization problem that uti-\nlized handcrafted features to maximize the visual similarity\nbetween image pairs [21, 2, 5, 58]. Deep learning-based\nmethods have recently revolutionized the field of optical\nflow estimation. FlowNet [9] was the first to introduce deep\nlearning into end-to-end optical flow estimation, which\ndemonstrated the potential of deep learning in this domain.\nAfter that, well-designed neural network architecture and\nsynthetic datasets promoted the progress of optical flow es-\ntimation [26, 48, 59, 60, 24, 25, 74]. RAFT [61] adopting it-\nerative refinement with correlation volume significantly im-\nproved the performance. FlowFormer [23, 53] successfully\napplied the attention mechanism. Recently, VideoFlow [52]\nexplored the temporal information between multiple frames\nand achieved state-of-the-art accuracy. Point tracking is an-\nother motion modeling method that computes the trajectory\nof the query point throughout the video frames [17, 8, 81].\nContext-PIPs [68] and CoTracker [28] further improve the\naccuracy of point tracking with the context information. In\nrecent work, DOT [39] demonstrated that using sparse point\ntracing to initialize arbitrary distance optical flow estima-\ntion can maintain high accuracy with reasonable computa-\ntional overhead.\n3. Method\nGiven a reference image I0 and a text prompt c, image-\nto-video synthesis (I2V) targets at generating a sequence of\nsubsequent video frames {\u02c6I1, \u02c6I2, . . . , \u02c6IN}. The key objec-\ntive is to ensure that the generated video clip not only ex-\nhibits plausible motion but also faithfully preserves the vi-\nsual appearance of the reference image. By leveraging the\nstrong generative priors of diffusion models, recent meth-\nods have shown promising open-domain I2V generalization\ncapacity. However, existing methods struggle to maintain\ntemporal consistency largely due to the limited 1-D tem-\nporal attention mechanism. Meanwhile, they offer limited\ncontrol over the generation results. In view of these limita-\ntions, we propose Motion-I2V, a novel framework that fac-\ntorizes image-to-video generation into two stages, as shown\nin Fig. 2.\nThe first stage, detailed in Sec.\n3.2, focuses\non predicting plausible motions in the form of pixel-wise\ntrajectories.\nBuilding on the predicted motion field, the\nsecond stage, described in Section 3.3, utilizes our pro-\nposed warpping-augmented temporal attention to synthe-\nsize future frames. We start with introducing the prelimi-\nnary knowledge of the latent diffusion model [49] and video\ndiffusion model in Sec. 3.1.\n3.1. Preliminaries\nLatent diffusion model.\nWe choose Latent Diffusion\nModel [49] (LDM) as the backbone generative model. It\nconducts the denoising process in the latent space of a\nVariational Autoencoder (VAE). During training, the in-\nput image x0 is first encoded into a latent representation\nz0 = E(x0) with the frozen encode E(\u00b7). This latent code\nz0 is then perturbed as:\nzt = \u221a\u03b1tz0 +\n\u221a\n1 \u2212 \u03b1t\u03f5, \u03f5 \u223c N(0, I),\n(1)\nwhere \u03b1t = Qt\ni=1(1 \u2212 \u03b2t) with \u03b2t is the noise strength\ncoefficient at step t, and t is uniformly sampled from the\ntimestep index set {1, . . . , T}.\nThis process can be re-\ngarded as a Markov chain, which incrementally adds Gaus-\nsian noise to the latent code z0. The denoising model \u03f5\u03b8 re-\nceives zt as input and is optimized to learn the latent space\ndistribution with the objective function\nl\u03f5 = ||\u03f5 \u2212 \u03f5\u03b8(zt, t, c)||2\n2,\n(2)\nwhere c represents the condition, and is the user-provided\ntext prompt in our case. In this paper, we choose Stable\nDiffusion 1.5 as the base LDM, where the denoising model\n\u03f5\u03b8 is implemented as a U-Net architecture.\nVideo latent diffusion model.\nWe follow the previous\nworks [16, 4, 72] to expand the image LDM by incor-\nporating temporal modules to create the video latent dif-\nfusion model (VLDM). Specifically, the spatial modules\nfrom the original image LDM are initialized with the pre-\ntrained weights and are frozen during training. This is to\ninherit the generative priors from the image LDM. Tem-\nporal modules li\n\u03d5 comprise of efficient 1-D temporal atten-\ntion for efficiency are inserted after each spatial attention\nblock li\n\u03b8. Given a 5-D video tensor z of shape batch \u00d7\nchannels \u00d7 frames \u00d7 height \u00d7 width, passed through\nthe temporal modules, its spatial dimensions height and\nwidth are reshaped to the batch dimension, yielding 1-\nD (batch \u00d7 height \u00d7 width) feature sequences of length\nframes, and are transformed by the self-attention blocks.\nSuch temporal modules are responsible for capturing the\ntemporal dependencies between features of the same spa-\ntial location across different frames. For clarity and ease\nof discussion, we refer to such a VLDM variant with 1-D\ntemporal attentions as the vanilla VLDM.\n3.2. Motion Prediction with Video Diffusion Models\nThe first stage of our proposed image-to-video gener-\nation framework targets at deducing the motions that can\nplausibly animate the reference image. As the latest large-\nscale diffusion models have been trained on web-scale text-\nimage data, they contain rich knowledge of visual seman-\ntics. Such semantic knowledge can greatly benefit motion\nprediction once the model is trained to associate motion\ndistributions with corresponding objects.\nTherefore, we\nchoose to adapt the pre-trained stable diffusion model for\nvideo motion fields prediction to capitalize on the strong\ngenerative priors.\nMotion fields modeling.\nWe denote the predicting tar-\nget of the first stage, the motion fields that animate the\nreference image, as a sequence of 2D displacement maps\n{f0\u2192i|i = 1, . . . , N}, where each f0\u2192i \u2208 R2\u00d7H\u00d7W is\nthe optical flow between the reference frame and future\nframe at timestep i. With such a motion fields represen-\ntation, for each source pixel p \u2208 I2 of the reference image\nI0, we can easily determine its corresponding coordinate\np\u2032\ni = p + f0\u2192i(p) on the target image Ii at timestep i.\nTraining a motion field predictor.\nTo learn a motion\nfield prediction VLDM, we propose a three-step fine-tuning\nstrategy. Initially, we tune a pre-trained LDM to predict a\nsingle displacement field conditioned on the reference im-\nage and text prompt. Subsequently, we freeze the tuned\nLDM parameters and integrate the vanilla temporal mod-\nules (as described in Sec. 3.1) to create an VLDM for train-\ning. This integration allows the model to learn the video\u2019s\ntemporal motion distribution by jointly denoising the whole\nsequence of the motion fields. After training the temporal\nmodules, we proceed to finetune the entire VLDM model\nto obtain the final motion fields predictor. We use Flow-\nFormer++ [53] and DOT [39] to estimate optical flow and\nmulti-frame trajectories as ground truth during training, re-\nspectively.\nEncoding motion fields and conditonal image. As we\nchoose the latent diffusion model for its computational ef-\nficiency, we encode each flow map f0\u2192i \u2208 R2\u00d7H\u00d7W into\na latent representation z0\u2192i,0 = Eflow(fi) \u2208 R4\u00d7h\u00d7w us-\ning an optical flow VAE encoder, where h = H/8 and w =\nW/8. The optical flow autoencoder mirrors the LDM image\nautoencoder\u2019s structure, except that it receives and outputs\n2-channel optical flow map rather than 3-channel RGB im-\nages. To support image conditioning, we concatenate the la-\ntent code of clean reference image E(I0) \u2208 R4\u00d7h\u00d7w along\nthe channel dimension.\nWe initialize all available LDM\nweights from the SD 1.5 checkpoint, and set weights for the\nnewly added 4 input channels to zero. Additionally, frame\nstride i is embedded using a two-layer MLP and is added\nto the time embeddings, serving as a motion strength con-\ndition.\n3.3. Video Rendering with Predicted Motion\nThe second stage of Motion-I2V targets at propagating\nthe content of the reference image according to the pre-\ndicted motion fields from stage 1 to synthesized frames,\nmaintaining the fidelity and temporal consistency. We pro-\npose a motion-augmented temporal attention to enhance the\nvanilla 1-D temporal attention, guided by the predicted mo-\ntion fields from the first stage. This operation enlarges the\ntemporal receptive field and alleviates the pressure of di-\nrectly predicting the complicated spatial-temporal patterns\nfrom a single image.\nMotion-augmented temporal attention.\nWe enhance\nvanilla VLDM\u2019s 1-D temporal attention with the pro-\nposed motion-augmented temporal attention and keep its\nother modules as is.\nConsider a latent feature z\n\u2208\nR(1+N)\u00d7Cl\u00d7hl\u00d7wl in the l \u2212th temporal layer li\n\u03d5, where cl,\nhl, wl represent the channel dimension, height and width\nof the feature, respectively. We omit the batch dimension\nfor brevity. Here we use z[0] \u2208 R1\u00d7Cl\u00d7hl\u00d7wl to denote\nthe feature map corresponding to the reference frame, and\nz[1 : N] \u2208 RN\u00d7Cl\u00d7hl\u00d7wl for the subsequent frames. With\nthe predicted motion fields {f0\u2192i|i = 1, . . . , N} (assum-\ning resized to align the spatial shape) from the first stage,\nwe forward-warp [42] z[0] according to each of the motion\nfield f0\u2192i as:\nz[i]\u2032 = W(z[0], f0\u2192i).\n(3)\nThese\nwarped\nfeature\nmaps\nz[i]\u2032\nare\ninterleaved\nwith\noriginal\nfeature\nmaps\nalong\nthe\ntemporal\ndi-\nmension\nto\ncreate\naugmented\nfeatures\nzaug\n=\n[z[0], z[1]\u2032, z[1], ..., z[N]\u2032, z[N]] \u2208 R(1+2\u00d7N)\u00d7Cl\u00d7hl\u00d7wl.\nThen z and zaug are reshaped to z\u2032 \u2208 R(hl\u00d7wl)\u00d7(1+N)\u00d7Cl\nand z\u2032\naug \u2208 R(hl\u00d7wl)\u00d7(1+2\u00d7N)\u00d7Cl, respectively. In other\nwords, the spatial dimensions are shifted to batch axis and\nthey are treated as 1-D tokens. The reshaped feature maps\nwill be projected and go through the 1-D temporal attention\nlayer:\nz\u2032\u2032 = Attention(Q, K, V) = Softmax(QKT)V,\n(4)\nwhere Q = W Qz\u2032, K = W Kz\u2032\naug and V = W V z\u2032\naug\nare the three projections. Notably, the warped feature maps\nonly serve as key and value features. Additionally, we add\nsinusoidal position encoding to z and zaug to make the net-\nwork aware of the temporal order the interleaved augmented\nfeature maps. This operation enlarges the receptive fields of\nthe temporal modules guided by the predicted motion fields\nfrom the first stage.\nSelective noising. At each timestep t of the denoising pro-\ncess, we always concatenate the clean reference image\u2019s\nlatent code zref \u2208 R1\u00d74\u00d7h\u00d7w with other noisy latent codes\nz0:N,t \u2208 RN\u00d74\u00d7h\u00d7w along the temporal axis. This guar-\nantees that the reference image\u2019s content is faithfully pre-\nserved in the generation process.\n4. Fine-grained Control of Motion-I2V Gener-\nation\nRelying solely on textual prompt can lead to a lack of\nfine-grained control of the generation results. This limita-\ntion often results in users engaging in multiple rounds of\ntrial and error to achieve their desired creation. In this sec-\ntion, we show that, by virtue of the explicit motion mod-\neling, our Motion-I2V naturally supports fine-grained con-\ntrols over the I2V process.\n4.1. Sparse Trajectory Guided I2V\nWe propose sparse trajectory guided I2V as an extension\nof our Motion-I2V framework. Specifically, given an im-\nage, users can draw one or multiple pixel-wise trajectories\nto precisely specify the desired motion of target pixels. Our\nnetwork is designed to interpret these sparse trajectory in-\nputs and transform them into plausible dense displacement\nfields with generative priors.\nSubsequently, these dense\nmotion fields are utilized as inputs for the second stage of\nMotion-I2V. This strategy effectively enables users to inter-\nactively control the I2V process, as shown in Fig. 4.\nTo achieve this intuitive setting, we train a Control-\nNet for the first stage, as shown in Fig. 3.\nSpecifically,\nwe clone the down-sample and middle blocks of the 3D-\nUnet in the first stage as the ControlNet branch.\nThis\ntrainable ControlNet branch is connected to the frozen\nmain branch with zero-initialized convolution layers fol-\nlowing [75]. The ControlNet additionally takes sparse dis-\nplacement fields fsparse \u2208 RN\u00d72\u00d7H\u00d7W and a binary mask\nConv\nResidual Addition\nFlow 3D U-Net\nFigure 3: Overview of trajectory ControlNet. We train a\nTrajectory ControlNet based on the pre-trained stage 1 of\nMotion-I2V. It takes sparse trajectories and corresponding\nbinary mask as additional conditions, and output dense op-\ntical flow maps.\nm \u2208 {0, 1}H\u00d7W as conditions, where 1 indicates pixels\nwith given motion. We use a shallow 3D Conv network to\nencode the concatenation of fsparse and m into 4-dim fea-\nture maps and add to the noisy latents as residuals. Please\nrefer to Supplementary for training details.\nInput+Drag\nFrame 2\nFrame 9\nFrame 16\nFigure 4: Examples of sparse trajectory guided I2V.\nUsers can precisely control the synthesized motions by\ndrawing one or multiple trajectories (red curved arrow).\n4.2. Region-Specific I2V\nOur framework also naturally supports region-specific\nI2V, where only user-specified regions of the reference im-\nage are animated, as shown in Fig. 5. It also can be used in\ncombination with the sparse trajectory guidance, as shown\nin Fig. 6, for more controllability.\nThis is a natural extension of the aforementioned sparse\ntrajectory guided I2V. Specifically, the input fsparse is set\nto all-zero maps. As for the mask m, user-specified regions\nare set to 0 and other areas as 1. Intuitively, this setting\nrequires the ControlNet to keep the non-specified regions\nstatic while infer plausible motions for the user-specified\nregions.\nInput+Brush\nFrame 2\nFrame 9\nFrame 16\nFigure 5: Examples of region-specific I2V. Users can pre-\ncisely Specify the animated regions by motion brush (purple\nmask). Unmasked regions remains static.\nInput+Drag\nFrame 2\nFrame 9\nFrame 16\nFigure 6: Combination of motion trajectories and mo-\ntion brush. Motion-I2V supports the combined usage of\nmotion brush and trajectory guidance.\n4.3. Zero-Shot Video-to-Video Translation\nOur framework also naturally supports video-to-video\ntranslation (V2V), where a given video is rendered into a\nnew video of another artistic expression specified by the\ntext prompt, as shown in Fig 7.\nSpecifically, users can\nutilize existing image-to-image tools to convert the first\nframe into the target style. Then the displacement fields of\nthe source video can be predicted using off-the-shelf dense\npoint tracker and are used to animate the converted first\nframe with the second stage of our Motion-I2V.\n5. Experiments\n5.1. Experimental Setup\nTraining.\nWe choose Stable Diffusion v1.5 as the base\nLDM model for the first stage and AnimateDiff v2 as the\nbase VLDM for the second stage. All models are trained on\nWebVid-10M [1], a large scale text-video dataset. During\ntraining, we randomly sample 16-frame video clips with a\nstride of 8. We employ the AdamW [36] optimizer with a\nconstant learning rate of 3 \u00d7 10\u22125 for training all models.\nStage 1 is trained with videos of resolution of 320 \u00d7 512\nand stage 2 is 320 \u00d7 320. All experiments are conducted on\n32 NVIDIA A100 GPU. Please refer to supplementary for\nmore training details.\nFrame 1\nFrame 6\nFrame 11\nFrame 16\nFigure 7: Example of video-to-video translation. The sec-\nond stage of Motion-I2V can be used for zero-shot video-to-\nvideo translation. The first frame of source video is trans-\nformed into the target style using existing image-to-image\ntools. Then the transformed image can be animated using\nthe second stage of Motion-I2V guided by the motions from\nthe source video.\nEvaluation. There are a few image-to-video benchmarks,\nbut they are limited to specific domains. For extensive eval-\nuation, we build a test set that covers various categories,\nsuch as human activity, animals, vehicle, natural scenes and\nAI-generated images. It contains 80 images downloaded\nfrom the copyright-free website Pixabay. We use ChatGPT-\n4V to generate prompts for the image content and possi-\nble motion. We use CLIP text-image logits to measure the\nprompt consistency between prompt and generated frames.\nWe calculate the cosine similarity between consecutive gen-\nerated frames in the CLIP embedding space to measure the\ntemporal consistency. We further estimate the optical flows\nbetween the first and subsequent generated frames to show\nthe motion magnitude.\n5.2. Comparison with Other Methods\nFor quantitative evaluation, we compare our method\nwith the open-sourced state-of-the-art methods VideoCom-\nposer [64], I2VGen-XL [77] and DynamiCrafter [72]. De-\ntailed results are shown in Table. 1. Our Motion-I2V outper-\nforms other methods in the prompt-following metric. Addi-\ntionally, Motion-I2V can generate more consistent videos\neven with larger motions.\nFor qualitative comparison,\ndue to limited space,\nwe compare with the quantitatively second best method\nDynamiCrafter, together with two commercial products\nPika1.0 and Gen2. Results are shown in Fig. 8. We ob-\nserve that DynamiCrafter is not sensitive to motion prompts\nand tends to generate videos with small motion. This ob-\nservation is in line with the quantitative results. Pika 1.0\nshares similar limited motions but offers better visual qual-\nity. Gen2 can generate motions as large as those produced\nby Motion-I2V, but it suffers from severe distortion. These\nresults verify that Motion-I2V has superior performance in\ngenerating consistent results even at the presence of large\nmotions.\nMethod\nPrompt Consistency \u2191 Frame Consistency \u2191 Average Displacement\nVideoComposer\n32.62\n0.9393\n67.15\nI2VGen-XL\n33.69\n0.9650\n17.70\nDynamiCrafter\n34.60\n0.9860\n3.31\nOurs\n34.86\n0.9871\n20.06\nTable 1:\nQuantitative comparison. Motion-I2V shows\nbest instruction-following ability and temporal consistency.\nMeanwhile, Motion-I2V generates relatively large motions.\n5.3. Ablation Study\nWe conduct ablation studies to evaluate the effects of\ncritical design choices. We first train a model without stage\n1 (first row of Table. 2) where temporal dependencies are\nsolely learned by the 1-D temporal module. We observe that\nthis model is unstable during inference and easy to gener-\nate crashed results. This is in line with the low consistency\nscores and extremely large motions. Then we add stage 1\nbut utilize the predicted motion fields in a naive way: di-\nrectly adding the warped feature maps z[i]\u2032 to z[i] rather\nthan using attention to adaptively inject the warped feature\nto subsequent frames. As shown in the second row of Ta-\nble. 2, this additional motion information stabilizes the pre-\ndiction, leading to higher consistency score and more vivid\nmotions. By further changing the fusion type to attention\nas Equation 3.3, we obtain the final model that achieves the\nhighest consistency score.\nStage 1? Fusion Type Prompt Consis. \u2191 Frame Consis. \u2191 Average Displacement\n\u2717\n-\n32.95\n0.9505\n66.44\n\u2713\nAddition\n33.99\n0.9542\n48.91\n\u2713\nAttention\n34.86\n0.9871\n20.06\nTable 2: Ablation study. Utilizing the motion fields from\nstage 1 can significantly stabilize the prediction. Addition-\nally, using attention to adaptively inject the warped fea-\ntures into synthesized frames can further increase consis-\ntency and avoid extreme distortions.\n6. Limitations and Conclusions\nWe observe that our method tends to generate videos of\nmedium brightness. This is likely because the noise sched-\nule does not enforce the last timestep to have zero signal-to-\nnoise (SNR) ratio as discussed in [34]. This flawed schedule\nleads to training-test discrepancy and limit the model\u2019s gen-\neralization. We believe using the latest Zero-SNR sched-\nulers can alleviate this issue. To conclude, in this paper we\npropose a novel I2V framework that factorizes the difficult\nimage to video generation task into two stages. In the first\nstage, we train a diffusion-based motion fields predictor that\nfocuses on deducing the plausible motions. It shows great\nDynamiCrafter\nPika 1.0\nA fast driving tank.\nGen2\nOurs\nDynamiCrafter\nPika 1.0\nA swimming white Betta fish.\nGen2\nOurs\nFigure 8: Qualitative comparison. DynamiCrafter and Pika 1.0 tend to generate videos of very small motions. Gen2\ncan generate as large motion as our method, but fails to preserve the identity of the reference image. Our Motion-I2V can\nsynthesize temporally consistent videos in the presence of large motions.\nmotion generative capacity. In the second stage of video\nrendering, we identify that the naive 1-D temporal atten-\ntion limits the temporal modeling capacity. To effectively\nenlarge temporal receptive field, we propose the motion-\nguided temporal attention. We further explore to provide\nmore controls over the I2V generation process by training a\nControlNet for the first stage. We believe controllability of\nI2V will obtain more attention from the community in the\nfuture.\n7. Acknowledgements\nThis project is funded in part by National Key R&D Pro-\ngram of China Project 2022ZD0161100, and in part by Gen-\neral Research Fund of Hong Kong RGC Project 14204021.\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In IEEE International Conference on\nComputer Vision, 2021.\n[2] Michael J Black and Padmanabhan Anandan. A framework\nfor the robust estimation of optical flow. In 1993 (4th) In-\nternational Conference on Computer Vision, pages 231\u2013236.\nIEEE, 1993.\n[3] Andreas Blattmann, Timo Milbich, Michael Dorkenwald,\nand Bj\u00a8orn Ommer. Understanding object dynamics for inter-\nactive image-to-video synthesis. In 2021 IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\npages 5167\u20135177, 2021.\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023.\n[5] Andr\u00b4es Bruhn, Joachim Weickert, and Christoph Schn\u00a8orr.\nLucas/kanade meets horn/schunck: Combining local and\nglobal optic flow methods. International journal of computer\nvision, 61(3):211\u2013231, 2005.\n[6] Zuozhuo Dai, Zhenghao Zhang, Yao Yao, Bingxue Qiu, Siyu\nZhu, Long Qin, and Weizhi Wang. Fine-grained open do-\nmain image animation with motion guidance, 2023.\n[7] Laurent Dinh, David Krueger, and Yoshua Bengio. NICE:\nnon-linear independent components estimation. In Yoshua\nBengio and Yann LeCun, editors, 3rd International Confer-\nence on Learning Representations, ICLR 2015, San Diego,\nCA, USA, May 7-9, 2015, Workshop Track Proceedings,\n2015.\n[8] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adri`a Re-\ncasens, Lucas Smaira, Yusuf Aytar, Jo\u02dcao Carreira, Andrew\nZisserman, and Yi Yang. Tap-vid: A benchmark for track-\ning any point in a video. arXiv preprint arXiv:2211.03726,\n2022.\n[9] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip\nHausser, Caner Hazirbas, Vladimir Golkov, Patrick Van\nDer Smagt, Daniel Cremers, and Thomas Brox. Flownet:\nLearning optical flow with convolutional networks. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2758\u20132766, 2015.\n[10] Rongyao Fang, Peng Gao, Aojun Zhou, Yingjie Cai, Si Liu,\nJifeng Dai, and Hongsheng Li. Feataug-detr: Enriching one-\nto-many matching for detrs with feature augmentation. arXiv\npreprint arXiv:2303.01503, 2023.\n[11] Rongyao Fang, Shilin Yan, Zhaoyang Huang, Jingqiu Zhou,\nHao Tian, Jifeng Dai, and Hongsheng Li. Instructseq: Unify-\ning vision tasks with instruction-conditioned multi-modal se-\nquence generation. arXiv preprint arXiv:2311.18835, 2023.\n[12] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao\nFang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao.\nClip-adapter: Better vision-language models with feature\nadapters. International Journal of Computer Vision, pages\n1\u201315, 2023.\n[13] Jiahao Geng, Tianjia Shao, Youyi Zheng, Yanlin Weng, and\nKun Zhou. Warp-guided gans for single-photo facial anima-\ntion. ACM Trans. Graph., 37(6), dec 2018.\n[14] Jiahao Geng, Tianjia Shao, Youyi Zheng, Yanlin Weng, and\nKun Zhou. Warp-guided gans for single-photo facial anima-\ntion. ACM Trans. Graph., 37(6), dec 2018.\n[15] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Z. Ghahra-\nmani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Wein-\nberger, editors, Advances in Neural Information Processing\nSystems, volume 27. Curran Associates, Inc., 2014.\n[16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023.\n[17] Adam W Harley, Zhaoyuan Fang, and Katerina Fragkiadaki.\nParticle video revisited: Tracking through occlusions using\npoint trajectories. In Computer Vision\u2013ECCV 2022: 17th\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022,\nProceedings, Part XXII, pages 59\u201375. Springer, 2022.\n[18] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey A. Gritsenko, Diederik P. Kingma, Ben\nPoole, Mohammad Norouzi, David J. Fleet, and Tim Sali-\nmans. Imagen video: High definition video generation with\ndiffusion models. ArXiv, abs/2210.02303, 2022.\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. arXiv preprint arxiv:2006.11239,\n2020.\n[20] Aleksander Holynski, Brian L. Curless, Steven M. Seitz, and\nRichard Szeliski. Animating pictures with eulerian motion\nfields. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 5810\u2013\n5819, June 2021.\n[21] Berthold KP Horn and Brian G Schunck. Determining opti-\ncal flow. Artificial intelligence, 17(1-3):185\u2013203, 1981.\n[22] Nisha Huang, Fan Tang, Weiming Dong, Tong-Yee Lee,\nand Changsheng Xu. Region-aware diffusion for zero-shot\ntext-driven image editing. arXiv preprint arXiv:2302.11797,\n2023.\n[23] Zhaoyang Huang, Xiaoyu Shi, Chao Zhang, Qiang Wang,\nKa Chun Cheung, Hongwei Qin, Jifeng Dai, and Hongsheng\nLi. Flowformer: A transformer architecture for optical flow.\narXiv preprint arXiv:2203.16194, 2022.\n[24] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy. Lite-\nflownet: A lightweight convolutional neural network for op-\ntical flow estimation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 8981\u2013\n8989, 2018.\n[25] Tak-Wai Hui, Xiaoou Tang, and Chen Change Loy.\nA\nlightweight optical flow cnn\u2014revisiting data fidelity and reg-\nularization. IEEE transactions on pattern analysis and ma-\nchine intelligence, 43(8):2555\u20132569, 2020.\n[26] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper,\nAlexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu-\ntion of optical flow estimation with deep networks. In Pro-\nceedings of the IEEE conference on computer vision and pat-\ntern recognition, pages 2462\u20132470, 2017.\n[27] Wei-Cih Jhou and Wen-Huang Cheng. Animating still land-\nscape photographs through cloud motion creation.\nIEEE\nTransactions on Multimedia, 18(1):4\u201313, 2016.\n[28] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia\nNeverova, Andrea Vedaldi, and Christian Rupprecht. Co-\ntracker:\nIt is better to track together.\narXiv preprint\narXiv:2307.07635, 2023.\n[29] Johanna Karras, Aleksander Holynski, Ting-Chun Wang,\nand Ira Kemelmacher-Shlizerman.\nDreampose: Fashion\nimage-to-video synthesis via stable diffusion, 2023.\n[30] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023.\n[31] Diederik P. Kingma and Max Welling. Auto-encoding vari-\national bayes. In Yoshua Bengio and Yann LeCun, editors,\n2nd International Conference on Learning Representations,\nICLR 2014, Banff, AB, Canada, April 14-16, 2014, Confer-\nence Track Proceedings, 2014.\n[32] Xingyi Li, Zhiguo Cao, Huiqiang Sun, Jianming Zhang, Ke\nXian, and Guosheng Lin. 3d cinemagraphy from a single im-\nage. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 4595\u2013\n4605, June 2023.\n[33] Zhengqi Li, Richard Tucker, Noah Snavely, and Aleksander\nHolynski. Generative image dynamics, 2023.\n[34] Shanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang.\nCommon diffusion noise schedules and sample steps are\nflawed.\nIn Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision, pages 5404\u20135411,\n2024.\n[35] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya\nJia. Video-p2p: Video editing with cross-attention control,\n2023.\n[36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017.\n[37] Aniruddha Mahapatra and Kuldeep Kulkarni. Controllable\nanimation of fluid elements in still images. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022.\n[38] Aniruddha Mahapatra, Aliaksandr Siarohin, Hsin-Ying Lee,\nSergey Tulyakov, and Jun-Yan Zhu. Text-guided synthesis\nof eulerian cinemagraphs. arXiv preprint arXiv:2307.03190,\n2023.\n[39] Guillaume Le Moing, Jean Ponce, and Cordelia Schmid.\nDense optical tracking: Connecting the dots. arXiv preprint\narXiv:2312.00786, 2023.\n[40] Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and\nJian Zhang. Dragondiffusion: Enabling drag-style manipula-\ntion on diffusion models. arXiv preprint arXiv:2307.02421,\n2023.\n[41] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. GLIDE: towards photorealistic image genera-\ntion and editing with text-guided diffusion models. CoRR,\nabs/2112.10741, 2021.\n[42] Simon Niklaus and Feng Liu. Softmax splatting for video\nframe interpolation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5437\u20135446, 2020.\n[43] Makoto Okabe, Ken ichi Anjyo, Takeo Igarashi, and Hans-\nPeter Seidel. Animating pictures of fluid using video exam-\nples. Computer Graphics Forum, 28, 2009.\n[44] Xingang Pan, Ayush Tewari, Thomas Leimk\u00a8uhler, Lingjie\nLiu, Abhimitra Meka, and Christian Theobalt. Drag your\ngan: Interactive point-based manipulation on the generative\nimage manifold. In ACM SIGGRAPH 2023 Conference Pro-\nceedings, SIGGRAPH \u201923, New York, NY, USA, 2023. As-\nsociation for Computing Machinery.\n[45] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, Gretchen\nKrueger, and Ilya Sutskever.\nLearning transferable visual\nmodels from natural language supervision. In International\nConference on Machine Learning, 2021.\n[46] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,\nSharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and\nPeter J. Liu. Exploring the limits of transfer learning with a\nunified text-to-text transformer. J. Mach. Learn. Res., 21(1),\njan 2020.\n[47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. ArXiv, abs/2204.06125, 2022.\n[48] Anurag Ranjan and Michael J Black. Optical flow estima-\ntion using a spatial pyramid network. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 4161\u20134170, 2017.\n[49] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models, 2021.\n[50] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Sin-\ngan: Learning a generative model from a single natural im-\nage. In Computer Vision (ICCV), IEEE International Con-\nference on, 2019.\n[51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol Ayan,\nTim Salimans, Jonathan Ho, David J. Fleet, and Mohammad\nNorouzi. Photorealistic text-to-image diffusion models with\ndeep language understanding. In Alice H. Oh, Alekh Agar-\nwal, Danielle Belgrave, and Kyunghyun Cho, editors, Ad-\nvances in Neural Information Processing Systems, 2022.\n[52] Xiaoyu Shi, Zhaoyang Huang, Weikang Bian, Dasong Li,\nManyuan Zhang, Ka Chun Cheung, Simon See, Hongwei\nQin, Jifeng Dai, and Hongsheng Li. Videoflow: Exploiting\ntemporal cues for multi-frame optical flow estimation. arXiv\npreprint arXiv:2303.08340, 2023.\n[53] Xiaoyu Shi, Zhaoyang Huang, Dasong Li, Manyuan Zhang,\nKa Chun Cheung, Simon See, Hongwei Qin, Jifeng Dai, and\nHongsheng Li. Flowformer++: Masked cost volume autoen-\ncoding for pretraining optical flow estimation. arXiv preprint\narXiv:2303.01237, 2023.\n[54] Yujun Shi, Chuhui Xue, Jiachun Pan, Wenqing Zhang, Vin-\ncent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffu-\nsion models for interactive point-based image editing. arXiv\npreprint arXiv:2306.14435, 2023.\n[55] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei\nChai, and Sergey Tulyakov. Motion representations for ar-\nticulated animation. In CVPR, 2021.\n[56] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman.\nMake-a-video: Text-to-video generation without text-video\ndata, 2022.\n[57] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In International Conference\non Learning Representations, 2021.\n[58] Deqing Sun, Stefan Roth, and Michael J Black. A quan-\ntitative analysis of current practices in optical flow estima-\ntion and the principles behind them. International Journal of\nComputer Vision, 106(2):115\u2013137, 2014.\n[59] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz.\nPwc-net: Cnns for optical flow using pyramid, warping, and\ncost volume.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 8934\u20138943,\n2018.\n[60] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and\nXiaowei Zhou. Loftr: Detector-free local feature matching\nwith transformers. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n8922\u20138931, 2021.\n[61] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. In European conference on com-\nputer vision, pages 402\u2013419. Springer, 2020.\n[62] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye,\nYu Liu, and Hongsheng Li. Gen-l-video: Multi-text to long\nvideo generation via temporal co-denoising. arXiv preprint\narXiv:2305.18264, 2023.\n[63] Wen Wang, kangyang Xie, Zide Liu, Hao Chen, Yue Cao,\nXinlong Wang, and Chunhua Shen. Zero-shot video editing\nusing off-the-shelf image diffusion models. arXiv preprint\narXiv:2303.17599, 2023.\n[64] Xiang* Wang, Hangjie* Yuan, Shiwei* Zhang, Dayou*\nChen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability. 2023.\n[65] Yaohui WANG, Piotr Bilinski, Francois Bremond, and An-\ntitza Dantcheva.\nImaginator: Conditional spatio-temporal\ngan for video generation. In The IEEE Winter Conference\non Applications of Computer Vision (WACV), March 2020.\n[66] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza\nDantcheva. Imaginator: Conditional spatio-temporal gan for\nvideo generation. In 2020 IEEE Winter Conference on Ap-\nplications of Computer Vision (WACV), pages 1149\u20131158,\n2020.\n[67] Yaohui Wang, Di Yang, Francois Bremond, and Antitza\nDantcheva. Latent image animator: Learning to animate im-\nages via latent space navigation. In International Conference\non Learning Representations, 2022.\n[68] BIAN Weikang, Zhaoyang Huang, Xiaoyu Shi, Yitong\nDong, Yijin Li, and Hongsheng Li. Context-pips: Persistent\nindependent particles demands context features. In Thirty-\nseventh Conference on Neural Information Processing Sys-\ntems, 2023.\n[69] Chung-Yi Weng, Brian Curless, and Ira Kemelmacher-\nShlizerman. Photo wake-up: 3d character animation from\na single photo. 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 5901\u20135910,\n2018.\n[70] Changming Xiao, Qi Yang, Xiaoqiang Xu, Jianwei Zhang,\nFeng Zhou, and Changshui Zhang. Where you edit is what\nyou get: Text-guided image editing with region-based atten-\ntion. Pattern Recognition, 139:109458, 2023.\n[71] Wenpeng Xiao, Wentao Liu, Yitong Wang, Bernard Ghanem,\nand Bing Li. Automatic animation of hair blowing in still\nportrait photos. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 22963\u201322975,\n2023.\n[72] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen,\nWangbo Yu, Hanyuan Liu, Xintao Wang, Tien-Tsin Wong,\nand Ying Shan.\nDynamicrafter: Animating open-domain\nimages with video diffusion priors.\narXiv preprint\narXiv:2310.12190, 2023.\n[73] Wei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo.\nLearning to generate time-lapse videos using multi-stage dy-\nnamic generative adversarial networks. In The IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\nJune 2018.\n[74] Gengshan Yang and Deva Ramanan. Volumetric correspon-\ndence networks for optical flow. Advances in neural infor-\nmation processing systems, 32:794\u2013805, 2019.\n[75] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n[76] Renrui Zhang, Rongyao Fang, Wei Zhang, Peng Gao,\nKunchang Li, Jifeng Dai, Yu Qiao, and Hongsheng Li.\nTip-adapter:\nTraining-free clip-adapter for better vision-\nlanguage modeling. arXiv preprint arXiv:2111.03930, 2021.\n[77] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qing, Xiang Wang, Deli Zhao, and\nJingren Zhou. I2vgen-xl: High-quality image-to-video syn-\nthesis via cascaded diffusion models. 2023.\n[78] Yi Zhang, Dasong Li, Xiaoyu Shi, Dailan He, Kangning\nSong, Xiaogang Wang, Hongwei Qin, and Hongsheng Li.\nKbnet: Kernel basis network for image restoration. arXiv\npreprint arXiv:2303.02881, 2023.\n[79] Yi Zhang, Xiaoyu Shi, Dasong Li, Xiaogang Wang, Jian\nWang, and Hongsheng Li.\nA unified conditional frame-\nwork for diffusion-based image restoration. arXiv preprint\narXiv:2305.20049, 2023.\n[80] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing\nFang, and Kai Chen. Pia: Your personalized image animator\nvia plug-and-play modules in text-to-image models, 2023.\n[81] Yang Zheng, Adam W Harley, Bokui Shen, Gordon Wet-\nzstein, and Leonidas J Guibas. Pointodyssey: A large-scale\nsynthetic dataset for long-term point tracking. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 19855\u201319865, 2023.\n[82] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models, 2023.\n"
  },
  {
    "title": "Media2Face: Co-speech Facial Animation Generation With Multi-Modality Guidance",
    "link": "https://arxiv.org/pdf/2401.15687.pdf",
    "upvote": "18",
    "text": "Media2Face: Co-speech Facial Animation Generation With Multi-Modality\nGuidance\nQingcheng Zhao1,2*\nPengyu Long1,2\u2217\nQixuan Zhang1,2\u2020\nDafei Qin2,3\nHan Liang1\nLongwen Zhang1,2\nYingliang Zhang4\nJingyi Yu1\u2021\nLan Xu1\u2021\n1ShanghaiTech University\n2Deemos Technology\n3University of Hong Kong\n4DGene Digital Technology Co., Ltd.\n{zhaoqch1, longpy, zhangqx1, lianghan, zhanglw2, yujingyi, xulan1}@shanghaitech.edu.cn\nqindafei@connect.hku.hk\nyingliang.zhang@dgene.com\nhttps://sites.google.com/view/media2face\nFigure 1. Given the speech signal and multi-modal conditions (Left), our system generates personalized and stylized co-speech facial\nanimation and head poses (Middle, Right).\nAbstract\nThe synthesis of 3D facial animations from speech has\ngarnered considerable attention.\nDue to the scarcity of\nhigh-quality 4D facial data and well-annotated abundant\nmulti-modality labels, previous methods often suffer from\nlimited realism and a lack of flexible conditioning. We ad-\ndress this challenge through a trilogy. We first introduce\nGeneralized Neural Parametric Facial Asset (GNPFA), an\nefficient variational auto-encoder mapping facial geometry\nand images to a highly generalized expression latent space,\ndecoupling expressions and identities.\nThen, we utilize\nGNPFA to extract high-quality expressions and accurate\nhead poses from a large array of videos. This presents the\nM2F-D dataset, a large, diverse, and scan-level co-speech\n3D facial animation dataset with well-annotated emotional\nand style labels. Finally, we propose Media2Face, a dif-\nfusion model in GNPFA latent space for co-speech facial\nanimation generation, accepting rich multi-modality guid-\nances from audio, text, and image. Extensive experiments\ndemonstrate that our model not only achieves high fidelity\nin facial animation synthesis but also broadens the scope\nof expressiveness and style adaptability in 3D facial anima-\ntion.\n1. Introduction\nAdvancements in generative AI, powered by large lan-\nguage models, have brought to life virtual companion AI\nsystems, reminiscent of the film \u201cHer\u201d. The core of these\nsystems is to provide realistic and immersive experiences,\nespecially sustained emotional connections with users. To\nachieve this goal, it is crucial to generate natural facial an-\nimation consistent with rich speech content, subtle voice\ntones, and complicated underlying emotions.\nIt has been a long journey for our graphics community\nto generate realistic co-speech facial animation. Early de-\narXiv:2401.15687v2  [cs.CV]  30 Jan 2024\nterministic methods [16,18,19] could generate only limited\nanimation variations from audio. Recently, generative mod-\nels, especially diffusion models, found their way to non-\ndeterministic generation ranging from 2D image [26,52,55]\nto 3D human motion [1, 3, 39, 59].\nInspired by them,\ndiffusion-based facial animation generation [2,10,56,57,60]\nhas achieved promising results and hence received substan-\ntial attention.\nWe observe two key factors to explore such diffusion-\nbased generation schemes for facial animations. First, the\ncapability of the diffusion models heavily relies on large-\nscale and high-quality training data.\nHowever, most of\nthe existing methods [20, 56, 70] are trained on small-scale\ndatasets such as VOCASET [14] or BIWI [22].\nThese\ndatasets only cover limited speech states and lack the di-\nversity of emotional variations and character traits. Some\nrecent methods [15,40,46,57,73] attempt to enrich speaking\nstyles with 2D datasets. Yet, they fall short in authentically\nreplicating natural expressions, since the adopted video-\nbased facial trackers often produce sub-optimal expressions\nor neglect head motions. Second, it is crucial to enable\nflexible conditioning and disentangled controls, from di-\nverse modalities like speech, style, or emotion. Some con-\ncurrent methods offer keyframe [60] (3DiFace), implicit\nstyle [46, 57], or emoji-based [15] controls. Yet, faithful\nconditioning from more diverse multi-modalities like text\nand image inputs remains an open challenge.\nIn this paper, we approach the above challenges through\na trilogy. First, we introduce General Neural Parametric Fa-\ncial Asset (GNPFA), a neural representation of fine-grained\nfacial expressions and head poses in latent space. We train\nGNPFA on a wide array of multi-identity 4D facial scanning\ndata, including high-resolution images and artists\u2019 refined\nface geometries, dubbed Range of Motion (RoM) data. As\nsuch, we decouple nuanced facial expressions from iden-\ntity in a latent representation that is generalizable to various\ntalking styles. Then, we utilize GNPFA to extract high-\nquality facial expressions and head poses from a diverse\nrange of videos, including different content, styles, emo-\ntions, and languages. This results in the creation of the Me-\ndia2Face Dataset (M2F-D), a diverse 4D dataset annotated\nwith a variety of emotions and styles, with quality compa-\nrable to face scans.\nFinally, we propose Media2Face, a latent diffusion\nmodel for co-speech facial animation generation using the\nM2F-D dataset.\nIt generates high-quality lip-sync with\nspeech and expresses nuanced human emotions contained\nin text, images, and even music.\nSpecifically, we train\nMedia2Face in the latent space of GNPFA to recover fine-\ngrained facial animations. It takes both audio features ex-\ntracted by Wav2Vec2 [4] and text/image prompts encoded\nby CLIP\n[49] as conditions and generates the sequen-\ntial facial expressions and head poses in a multi-classifier-\nfree guidance manner. We conduct extensive experiments\nand user studies to demonstrate the effectiveness of Me-\ndia2Face. We further showcase various applications, i.e.,\ngenerating vivid and realistic facial animations from diverse\naudio sources like dialogues, music, and speeches, as well\nas various text/image-based conditioning and editing. To\nsummarize, our main contributions include:\n\u2022 We present Media2Face, a diffusion-based generator\nthat integrates diverse media inputs (audio, image, and\ntext) to drive vivid facial animations including head\nposes.\n\u2022 To train Media2Face, We propose GNPFA, a neural\nlatent representation to capture nuanced facial motion\ndetails, enabling the collection of a diverse co-speech\n4D facial animation dataset with annotated expressions\nand styles.\n\u2022 We conduct extensive experiments and user studies\nand demonstrate exciting applications for generating\nfacial animations with multi-modality guidances.\n2. Related Work\n2.1. 3D Facial Animation Representations\nThe quality of generated 3D facial expressions heav-\nily relies on the chosen representation method for 3D fa-\ncial animation.\nOver the years, various representation\nmethods have been proposed in the field [5, 18, 35]. One\nwidely adopted approach is the Facial Action Coding Sys-\ntem (FACS) [18], which defines facial movements as com-\nbinations of muscle activations based on expert knowl-\nedge of human anatomy.\nTraditionally, these activations\nhave been implemented using blendshape deformers [35].\nTo automate the process and encompass a broader range\nof facial expressions, [5] proposes 3D Morphable Models\n(3DMM). These models capture a linear deformation space\ndirectly from face scans, encompassing diverse identities\nand expressions [6, 12, 29]. For a comprehensive survey of\n3DMM-related methods, we refer readers to [17]. Despite\ntheir usefulness, linear methods have limitations in captur-\ning subtle nuances of facial expressions. To address this,\nFLAME [38] introduces pose-dependent corrective blend-\nshapes, as well as articulated jaw, neck, and eyeballs, to en-\nhance the fidelity of facial animation modeling. Further ap-\nproaches have been developed to model facial expressions\nin nonlinear spaces [34,54]. [34] employ a Gaussian mix-\nture model to extend the traditional 3DMM, and [54] in-\ntroduce the use of multi-scale maps to transfer expression\nwrinkles from a high-resolution example face model to a\ntarget model.\nRecent work [7, 11, 37, 48, 63\u201365] leverages deep neu-\nral networks to build the latent expression space from data,\nachieving state-of-the-art performance on facial animation\ntasks.\n[7] incorporates a specialized convolutional oper-\nator designed for 3D meshes, capitalizing on the consis-\ntent graph structure inherent to deformable shapes with un-\nchanging topology. [48] learn interpretable and editable la-\ntent code over high-fidelity facial deformations, extending\nits application to various mesh topologies. These methods,\nhowever, are constrained by the scope of available datasets,\noften necessitating a compromise between enhancement of\nquality and retention of diversity. With the advancements\nin implicit representations, several work [24, 72, 80] have\nspurred efforts to employ Signed Distance Fields (SDFs)\nfor modeling human head geometry, coupled with a forward\ndeformation field to articulate facial expressions. However,\nthey fall short in comparison to our approach, which con-\nstructs a latent space from a head mesh with a fixed topol-\nogy. Our technique not only aligns better with practical\napplications but also ensures compatibility with traditional\ncomputer graphics (CG) pipelines.\n2.2. Conditional Facial Animation Synthesis\nAudio-Driven Facial Animation.\nRecent studies in\naudio-driven 3D facial animation have leveraged procedu-\nral [19,31] and learning-based methods [9,13,20,21,25,28,\n30, 50, 62, 81] to map speech to facial movements. While\nprocedural techniques give artists control over lip shapes,\nthey lack flexibility in capturing individual speech styles\nand require intensive manual tuning [16]. Learning-based\napproaches, using blendshapes [46, 47, 58, 67, 75] or 3D\nmeshes [14, 20, 45], better capture speech nuances but can\nlead to overly smooth lip motion and limited upper face\nexpressiveness. To address the complex speech-to-facial-\nexpression mapping, probabilistic models like Vector Quan-\ntized Variational Autoencoders (VQ-VAE) [66] have been\nintroduced, predicting facial expression distributions from\nspeech [44, 70, 74]. Despite their strengths, these models\noften fail to fully represent the stochastic nature of facial\nexpressions. Diffusion models [26,55], recognized for han-\ndling intricate generative tasks [23,51,52,71], show promise\nin audio-driven facial animation [2,10,56,57,60], facilitat-\ning multi-modal generation. Yet, integrating speech with\nhead movements remains a challenge, often yielding less\nconvincing expressions.\nOur method introduces a novel\nhead motion component within a prompt-guided diffusion\nmodel, improving the cohesiveness and expressiveness of\ngenerated facial animations, thus advancing the state-of-\nthe-art in speech-driven 3D facial animation.\nStyle Control.\nCurrent approaches predominantly uti-\nlize two methods for style modulation: label-based and\ncontextual-based control. Label-based control employs a\nfixed set of categories within the data to construct labels,\nsuch as emotion\n[15, 32], speaker style\n[14, 20], and\nemotional intensity\n[46]. However, the pre-defined cat-\negories constrain the model to capture complex, nuanced\nemotions. Contextual-based control allows for the imita-\ntion of styles from given segments through a one-shot ap-\nproach [57] or learning new speaking styles from given mo-\ntions [61]. Techniques like 3DiFace [60] adapt to the con-\ntext of provided keyframes, while DiffPoseTalk [57] and\nImitator [61] extract or learn style embeddings from given\nfacial animations and style adaptations, respectively. Yet,\nthese methods fall short of quality and explicit controls, of-\nten resulting in animations that lack the depth and complex-\nity of genuine human expressions.\n3. Reshape Facial Animation Data\nRealistic synthesis of 3D facial animations necessitates\n4D dynamic facial performance capture, typically reliant on\nmultiview-camera setups [69]. This requirement signifi-\ncantly limits the diversity and scalability of data acquisition\ndue to the labor-intensive nature of capturing and process-\ning such data.\nTo address these constraints, we propose Generalized\nNeural Parametric Facial Asset (GNPFA), which is in\nessence a Variational Auto Encoder, mapping facial geom-\netry and video footprints to the same latent space. We train\nGNPFA on large-scale 4D facial scanning, including high-\nresolution images and artists\u2019 refined geometries, enabling\nit to produce nuanced facial animation from videos with di-\nverse identities, languages, emotions, and head poses.\n3.1. Expression Latent Space Learning\nTraining data\nWe first capture a dataset with vast multi-\nidentity 4D scanings. We call this the Range of Motion\n(RoM) data. RoM consists of 43652 registered meshes and\n698432 images from 300 identities across different genders,\nages, and ethnicities. In addition, to enhance the robustness\nof expression-identity disentanglement, we create personal-\nized blendshapes for 200 identities under FACS standards\naccording to [36], and generate random artificial expres-\nsions during training for augmentation.\nGeometry VAE\nTo learn an expression latent space dis-\nentangled from identities, we design a geometry VAE con-\nsisting of a geometry encoder Egeo and a geometry generator\nGgeo, where Ggeo is conditioned on a neutral geometry and\nutilizes a UNet architecture. To support traditional blend-\nshape animation, we train two mapping networks M and\nM\u2032, where the former maps the weight of blendshapes, w\nto our latent space, and the latter does the inverse. The for-\nward process is illustrated in Fig. 2.\nGiven the input geometry, GR and its paired neutral ge-\nometry \u00afGR, our geometry encoder encodes it to the ex-\npression latent code via VAE sampling: zR = Egeo(GR).\nThen, the geometry decoder recovers the face geometry:\n\u210a!\"#\n\u2107!\"#\n\u210a!\"#\ne\nNeutral\nGeometry\nBlendShape\nExpression\nNeutral\nGeometry\n\u2107!\"#\nRange of Motion\nFrame\n\u210a!\"#\ne\ne\nNeutral\nGeometry\n\u210a!\"#\nTextures\nDifferentiable\nRendering\nCaptured\nImage\nStage 1\nLearning Motion Space\nStage 2\nLearning Vision Encoders\nPersonalized\nBlendShape\nDataset\nRange of \nMotion\nDataset\n\u2107$#%\"\n\u2107\"&$\nMultiview\nImage\nDataset\nPaired Data\nExpression\nLatent code\nPose\nExpression\nLatent code\nExpression\nLatent code\nExpression\nLatent code\nBlendShape\nWeights\n4D/Expression\n\u2112'\"(#)\n\u2112'\"(#)\n\u2112'\"(#)\n\u2112'\")*\"'\n\u2112'\"(#)\nFigure 2. GNPFA pipeline. (Left:) We train a geometry VAE to learn a latent space of expression and head pose, disentangling expression\nwith identity. (Right:) Two vision encoders are trained to extract expression latent codes and head poses from RGB images, which enables\nus to capture a wide array of 4D data.\n\u02dcGR = Ggeo( \u00afGR, zR). The training objective is simply a\nreconstruction loss:\nLrecon,R = \u2225 \u02dcGR \u2212 GR\u22252\n2.\n(1)\nGiven a randomly sampled blendshape wB and neutral face\n\u00afGB, we obtain the deformed expression GB using person-\nalized blendshapes. Similar to the real data scenario, we\nextract the expression latent code \u02dczB = Mgeo(wB) and\nreconstruct the geometry through the geometry decoder,\n\u02dcGB = Ggeo( \u00afGB, zB).\nWe map the expression back by\n\u02dcwB = M\u2032(\u02dczB). The training objective is defined as:\nLrecon,B = \u2225 \u02dcGB \u2212 GB\u22252\n2 + \u2225\u02dczB \u2212 zB\u22252\n2 + \u2225 \u02dcwB \u2212 wB\u22252\n2. (2)\nWe use coordinate maps [77] to represent the geome-\ntries, which store the 3D position of each vertex on the 2D\ngeometry map in the UV space. This representation can\nbe converted to and from mesh representation using a fixed\ntopology. Besides CG-compatible, it creates a more realis-\ntic and believable animation space than existing parametric\nfacial models, due to its non-linearity and vertex-level gran-\nularity.\n3.2. Image Facial Expression Extraction\nIn addition to the geometry VAE, we train two vision en-\ncoders, Eexp and Epose, to extract unified expression latent\ncode and head pose from RGB images. We freeze the ge-\nometry VAE and train the vision encoders under the super-\nTable 1. 4D datasets comparison. Notice that DiffposeTalk [57]\nis a combination of reconstructed TFHP [57] and HDTF.\nEMOTE [15] is trained on reconstructed MEAD.\nHours\nEmotion\nHead Pose\nLanguage\nVOCASET\n0.5\n%\n%\nEN\nBIWI\n1.7\n!\n!\nEN\nMultilFace\n2.8\n!\n!\nEN\nUUDaMM\n9.6\n%\n%\nEN\nDiffposeTalk\n26.5\n%\n!\nEN\nEMOTE\n25.3\n!\n%\nEN\nM2F-D (Ours)\n60.6\n!\n!\n6\nvision of both real images of RoM data and rendered im-\nages from geometries randomly generated by personalized\nblendshapes.\nSpecifically, given an image IR in the RoM data with cor-\nresponding ground-truth geometry GR and neutral \u00afGR, we\nextract the expression latent code \u02c6zR = Eexp(IR), and head\npose \u02c6pR = Epose(IR). Then, we reconstruct the face geome-\ntry using our pretrained decoder, \u02c6GR = Ggeo( \u00afGR, \u02c6zR). We\nutilize a differentiable renderer R, to get the rendered im-\nage of the face: \u02c6IR = R( \u02c6GR, \u02c6pR). We define the training\nloss, Lexp, as the combination of geometry loss and image\nloss:\nLexp, R = \u2225 \u02c6GR \u2212 GR\u22252\n2 + +\u2225\u02c6IR \u2212 IR\u22252\n2.\n(3)\nSimilarly, with a randomly sampled geometry GB, we can\nextract its head pose \u02c6pB = Epose(R(GB, pB)), and expres-\nsion code \u02c6zB = Eexp(R(GB, pB)). We render the image\nby the same differentiable renderer \u02c6IB = R( \u02c6GB, \u02c6pB). Our\ntraining objective is defined as:\nLexp, B = \u2225 \u02c6GB \u2212 GB\u22252\n2 + \u2225R( \u02c6GB, \u02c6pB) \u2212 R(GB, pB)\u22252\n2.\nAfter training, Eexp and Epose can capture fine-grained ex-\npressions and head poses from in-the-wild videos, repre-\nsented in the expression latent space, and map them to per-\nsonalized expressions by Ggeo. Owing to the rapid inference\nspeed of GNPFA, we can efficiently extract high-quality and\ndiverse expressions and head poses from in-the-wild videos.\n3.3. Latent-based Facial Animation Dataset\nWe leverage a large collection of online video facial data\nwith abundant audio and text labels, and use GNPFA to\nextract exact facial expressions and accurate head poses.\nThis allows us to avoid tedious annotations and thus eas-\nily augment the limited 4D facial animation dataset, which\npresents the Media2Face Dataset (M2F-D).\nWe retrieve both the expression latent codes and head\nposes from MEAD [68], CREMA-D [8], RAVDESS [41],\nHDTF [79] and Acappella [43]. The MEAD dataset com-\nprises talking-face videos of 60 actors and actresses ex-\npressing 8 different emotions at 3 varying intensity lev-\nels. The CREMA-D dataset comprises 7,442 distinct clips\nfeaturing 91 actors who delivered 12 sentences expressing\n6 different emotions at 4 different intensity levels.\nThe\nRAVDESS dataset consists of videos by 24 professional ac-\ntors who vocalize speeches and songs, with a total of 7 and 5\nemotions respectively. The HDTF dataset is a collection of\nhigh-quality videos and the Acappella dataset encompasses\nsolo singing videos.\nTo further increase the diversity of talking languages,\nwe collect a 2-hour dataset from in-the-wild videos which\ncontains 6 different languages: Chinese, French, German,\nJapanese, Russian, and Spanish. To allow for explicit head\npose control under different scenarios, we capture a 1.6-\nhour dataset that consists of 14 speakers performing 14 dif-\nferent head movements including speaking, singing, nod-\nding, shaking head, frowning, winking, etc.\nOur M2F-D dataset has a total duration of over 60 hours\nat 30 fps.\nAs shown in Table 1, and surpasses existing\naudio-visual datasets both in duration and diversity.\n4. Media2Face Methods\nMedia2Face is a transformer-based latent diffusion\nmodel conditioning on multi-modal driving signals. It mod-\nels the joint distribution of sequential head poses and facial\nexpressions, i.e., full facial animation, and thus facilitates\nthe natural synergy of poses and expressions. It also em-\nploys multi-conditioning guidance, enabling highly consis-\ntent co-speech facial animation synthesis with CLIP-guided\nstylization and image-based keyframe editing.\n4.1. Facial Animation Latent Diffusion Models\nAs described in Fig. 3, the expression latent code zi\ne and\nhead pose \u03b8i are first extracted from each video frame i.\nThen, we concatenate them to form a single-frame facial an-\nimation state, denoted by xi = [zi\ne, \u03b8i]. The facial anima-\ntion is thus formed by a sequence of states X1:N = [xi]N\ni=1.\nIn the diffusion model, generation is modeled as a\nMarkov denoising process.\nHere, X1:N\nt\nis obtained\nby adding noise to the ground truth head motion code\nX1:N\n0\nover t steps. Our method models the distribution\np\n\u0010\nX1:N\n0\n|X1:N\nt\n\u0011\nto facilitate a stepwise denoising process.\nSimilar to the approach in MDM [59], we predict X1:N\n0\ndirectly. This prediction method allows us to introduce ad-\nditional regularization terms to improve action consistency\nand smoothness.\nWe employ large-scale pre-trained encoders to incorpo-\nrate multi-modal conditions. The raw speech audio is en-\ncoded by the pre-trained Wav2Vec2 [4] and aligned to the\nlength of the facial animation sequence by linear interpo-\nlation, resulting in audio feature A1:N.\nBesides, a text\nor an image serving as the prompt for talking style is en-\ncoded to CLIP latent code P by the pre-trained CLIP model\n[49]. Our Transformer-based denoiser learns to predict fa-\ncial animation X1:N\n0\nconditioning on the concatenation of\nthese multi-modal embeddings via the common style-aware\ncross-attention layers. At each time step, the denoising pro-\ncess can be formulated as:\n\u02c6X\n1:N\n0\n= G\n\u0010\nX1:N\nt\n, t, A1:N, P\n\u0011\n.\n(4)\nTo enable disentanglement of speech and prompt control,\nduring training, two random masks are introduced for multi-\nconditioning classifier-free guidance\n[27].\nInitially, the\nCLIP latent code P undergoes the first random mask, which\nbrings both stylized and non-stylized co-speech denoisers\nand enables style control disentangled with speech signals.\nThen, this masked code is concatenated with the audio fea-\nture A1:N. A second phase of random masking is applied\nto the final concatenated code, which similarly brings both\nspeech-driven and non-speech-driven denoisers and facili-\ntates adjusting speech content consistency strength.\nTraining\nWe employ the simple loss [26] as the main\nobjective to train our models, which is defined as:\nLsimple = \u2225X1:N\n0\n\u2212 \u02c6\nX\n1:N\n0\n\u22252\n2.\n(5)\nBesides, we introduce a velocity loss\n[14] to enforce\nthe model to produce natural transitions between adjacent\nConcatenate\n  \ud835\udc4b!\n\"~$\nAdd noise\nX num of blocks\n  \ud835\udc4b#%\n\"~$\nMulti-Head\nStyle-Aware \nAttention\n\u2295\nK\nV\nQ\nDiffusion Step t\nRandom\nMask\n\u2295\nMulti-Head\nSelf Attention\nRandom\nMask\n\u2107!\"#\nVideo Frames\nAudio\nImage/Text\nWav2Vec2\nEncoder\nCLIP\nEncoder\nPaired Data\nAudio Features\nCLIP Latent code\nAlignment \nMask\nTraining Stage \nInference Stage \n\u210a!\"#\n  \ud835\udc4b%\n\"~$\n  \ud835\udc4b&\n\"~$\n  \ud835\udc4b&'\"\n\"~$\nDDIM \nSampling\nHead Pose\n....\nExpression Latent code\n...\n...\n....\n....\n\ud835\udc3f\ud835\udc5c\ud835\udc60\ud835\udc60\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\u2295\n\u2107#$%!\nFigure 3. Architecture of Media2Face. Our model takes audio features and CLIP latent code as conditions and denoise the noised\nsequence of expression latent code together with head pose i.e. head motion code. The conditions are randomly masked and subjected to\ncross-attention with the noisy head motion code. At inference, we sample head motion codes by DDIM. We feed the expression latent code\nto the GNPFA decoder to extract the expression geometry, combined with a model template to produce facial animation enhanced by head\npose parameters.\nframes, which is formulated as:\nLvelocity =\n\r\r\r\n\u0010\nX2:N\n0\n\u2212 X1:N\u22121\n0\n\u0011\n\u2212\n\u0010\n\u02c6\nX\n2:N\n0\n\u2212 \u02c6\nX\n1:N\u22121\n0\n\u0011\r\r\r\n2\n2 .\n(6)\nFurthermore, a smooth loss [57] is employed to enforce\nsmoothness and reduce abrupt transitions and discontinu-\nities:\nLsmooth =\n\r\r\r \u02c6\nX\n3:N\n0\n+ \u02c6\nX\n1:N\u22122\n0\n\u2212 \u02c6\nX\n2:N\u22121\n0\n\r\r\r\n2\n2 .\n(7)\nOverall, the denoiser is trained with the following objective:\nL = \u03bbsimpleLsimple + \u03bbvelocityLvelocity + \u03bbsmoothLsmooth, (8)\nwhere \u03bbsimple, \u03bbvelocity, and \u03bbsmooth are hyper-parameters\nserving as loss weights to balance the contributions from\nthese terms.\nInference\nDuring the denoising process, our model com-\nbines two types of guidance, the main speech audio and the\nadditional text/image style guidance, with the classifier-free\nguidance technique [27]:\n\u02c6X\n1:N\n0\n= (1 \u2212 sA \u2212 sP ) \u00b7 G\n\u0010\nX1:N\nt\n, t\n\u0011\n+ sA \u00b7 G\n\u0010\nX1:N\nt\n, t, A1:N\u0011\n+ sP \u00b7 G\n\u0010\nX1:N\nt\n, t, A1:N, P\n\u0011\n,\n(9)\nwhere sA and sP are two strength factors to adjust speech\nand style guidance strength respectively. The last two terms\nprovide both non-stylized and stylized predictions within\nthe same speech inputs, which implies the disentangled\nstyle control beyond the speech content.\nOverlapped batching denoising\nTo reduce the inference\ntime for real-time applications, we employ batching de-\nnoising, a technique akin to the batching denoising step\nintroduced in StreamDiffusion [33], and further extend it\nto overlapped batching denoising, to process exceedingly\nlong audios in a single denoising pass, by segmenting au-\ndio into overlapped windows. The overlapped batching de-\nnoising approach transforms the traditionally multiple, au-\ntoregressive sequence generation tasks into a parallelizable\nendeavor. Within the confines of VRAM capacity, its pro-\ncessing time does not increase linearly with the length of\nthe audio, thereby significantly enhancing the speed of head\nmotion generation.\n4.2. Conditional Facial Animation Editing\nMedia2Face achieves fine-grained control of generation\nthrough keyframe editing and text/image guidance. As il-\nlustrated in Fig.\n4, we use GNPFA and CLIP to extract\nthe conditions from face images and text/image prompts\nand leverage classifier-free guidance to control the diffusion\nprocess.\nGenerated Animation\nKeyframe\nEditing\n\u2107!\"#\nKeyframe\nStyle Editing\nSad\nHappy\nFigure 4. Application show case. We can fine-tune the generated\nfacial animation (Row 2) by 1. extracting key-frame expression\nlatent codes through our expression encoder (Row 3), 2. providing\nper-frame style prompts through CLIP (Row 4, Left: happy, Right:\nSad). The intensity and range of control can be adjusted using\ndiffusion in-betweening techniques.\nKeyframe editing\nWe can modify the keyframes of the\ngenerated animation and smoothly integrate them with the\ncorresponding lip movements using diffusion inpainting\ntechnique [42] in the temporal domain. Fig. 4 illustrates an\nexample of modifying a keyframe retrieved from an image\nusing GNPFA. Similarly, this ability can be generalized as\nsequential composition in [53] to diffuse animations from\ndifferent sources together. Please refer to our supplemen-\ntary video.\nCLIP-guided style editing\nUtilizing an in-betweening\ntechnique in [3,59], our approach enables the application of\ndiverse style controls across different frames within an au-\ndio segment. By assigning distinct style prompts to individ-\nual frames and employing a gradient mask during each dif-\nfusion step, we seamlessly and naturally integrate the sam-\npling results of various prompts. This methodology ensures\na coherent transition of style influences throughout the au-\ndio sequence.\n5. Results\nIn Fig. 8, we showcase multiple audio-driven animations\nas well as style-based generation results based on text and\nimage prompts. Please refer to the supplementary video for\nmore results.\n5.1. Implementation Details\nFor GNPFA, we follow Dreamface [76] to design our\ngeometry VAE, vision encoders share the same architec-\nture as the geometry encoder.\nTraining of the geometry\nVAE and vision encoders take 10 days and 96 hours to\nconverge respectively on Nvidia A6000 GPU, using an Ad-\naBelief optimizer. GNPFA can inference on Nvidia RTX\n3090 GPU at about 500 fps. For Media2Face, we employ\nan eight-layer transformer decoder for the denoiser, utiliz-\ning four attention heads.\nThe feature dimension is 512,\nand the window size is N = 200 at 30 fps. During train-\ning, our models follow a cosine noise schedule with 500\nnoising steps. Media2Face is trained on Nvidia RTX 3090\nGPU for 36 hours using an AdamW optimizer.\nWe set\n\u03bbsmooth = 0.01, \u03bbvelocity = 1, \u03bbsimple = 1. During inference\ntime, we set sA = 2.5, sP = 1.5. We achieved over 300 fps\noffline and 30fps in real-time on Nvidia RTX 3090 GPU.\n5.2. Comparisons\nWe compare Media2Face with several state-of-the-art fa-\ncial animation methods. We separate a two-hour segment\nfrom the M2F-D as the test set and keep it with a simi-\nlar data structure as the training set. For 3D methods, we\ncompared with FaceFormer [20], CodeTalker [70], FaceD-\niffuser [56] and EmoTalk [46] and use their pre-trained\nmodel as the baseline.\nWe unify all results to the same\nFLAME topology for fair comparisons. We also compared\nthe quality of generated head poses with SadTalker [78],\na 2D talking face generation method that incorporates head\nmovements.\n5.2.1\nQuantitative Comparisons\nTo measure lip synchronization, we employ Lip Vertex Er-\nror (LVE) [50], calculating the maximum L2 error across\nall lip vertices for each frame. Upper Face Dynamics Devi-\nation (FDD) [70] measures the diversity of expressions by\ncomparing the standard deviation of each upper face motion\nover time between the synthesized and the ground truth. To\nevaluate the synchronization between audio and generated\nhead pose, we utilize Beat Alignment (BA) [57,78], which\ncomputes the synchronization of head pose beats between\nthe generated and the ground truth. As shown in Table 2,\nour method surpasses existing methods in terms of lip ac-\ncuracy, facial expression stylization, and the synthesis of\nrhythmic head movements.\n5.2.2\nQualitative Comparisons\nWe show our qualitative comparison in Fig. 7. Compared\nwith emotion-blind methods, (FaceFormer, CodeTalker,\nTable 2. Quantitative comparisons and evaluations. Notice that\nthe BA metric is not utilized for FaceFormer, CodeTalker, FaceD-\niffuser, and EmoTalk, as they do not generate head poses. Also,\nmetrics related to vertices are not utilized for SadTalker due to its\ndifferent facial topology.\nMethods\nLVE(mm)\u2193\nFDD(\u00d710\u22125m)\u2193\nBA \u2191\nFaceFormer\n18.19\n21.37\nN/A\nCodeTalker\n16.74\n21.95\nN/A\nFaceDiffuser\n16.33\n22.38\nN/A\nEmoTalk\n14.61\n17.84\nN/A\nSadTalker\n\u2014\n\u2014\n0.219\nOurs w/o CFG\n10.67\n16.69\n0.166\nOurs w/o GNPFA\n14.89\n12.81\n0.198\nOurs w/ 10% data\n10.75\n20.65\n0.170\nOurs w/ 40% data\n10.55\n18.32\n0.208\nOurs w/ 70% data\n10.43\n14.98\n0.221\nOurs\n10.44\n12.21\n0.254\nFaceDiffuser), our method can generate not only more accu-\nrate lip movement but also micro-expressions under neutral\nconditions (eye blink, eyebrow gesture). Compared with\nemotion-aware methods, (EMOTE, EmoTalk), our method\ndemonstrates a more vibrant and natural expression of emo-\ntions and facial details while maintaining lip shape accu-\nracy.\nNotice that our method also generates head poses\nhighly synchronized with the given conditions(raise the\nhead in surprise and lower it in sadness).\n5.3. Ablation study\nWe conduct following ablation experiments to evaluate\nour key components:(1) Ours w/o GNPFA: We train Me-\ndia2Face on linear blendshapes obtained from the GNPFA\nmapping network M\u2032. (2) Ours w/o CFG: We inference\nMedia2Face without classifier-free guidance. As shown in\nTable 2, the removal of GNPFA leads to a significant degra-\ndation in LVE, validating the effectiveness of GNPFA on\nmodeling accurate lip shape. Inference without CFG has\nbad performance on FDD since the model fails to gener-\nate stylized head motions. We also train Media2Face on\n10%, 40%, 70% of M2F-D. As shown in Table 2, model\nperformance on FDD and BA increases during dataset scal-\ning up while that on LVE remains steady. This validates\nour hypothesis that while the model can learn precise lip-\nsync animation on small datasets, it requires learning from\na large amount of rich-conditioned data to generate anima-\ntion with realistic expressions, diverse emotions, and appro-\npriate head movements.\n5.4. User Study\nWe conduct 30 diverse audio samples, including dia-\nlogues, speeches, and songs and invert 100 participants.\nWe ensure fair comparisons by employing the same shader\nand template for all generated geometries. Participants en-\nsure side-by-side animations with other methods, assess-\ning Media2Face with three conditions: with a specific style\nprompt for each audio, with a neutral prompt, and without\nany prompts and head pose animation. Our model demon-\nstrates superior preference ratings: over 90% for general\ncases, 80% without specific style prompts, and 70% in the\nabsence of specific style prompts and head pose, underscor-\ning the effectiveness of head pose generation and our style\nprompt.\n6. Conclusions\nIn this paper, we present Media2Face, pushing the\nboundary of diffusion models for realistic co-speech facial\nanimation synthesis with rich multi-modal conditionings.\nTo enhance the diffusion models with high-quality fa-\ncial animation data, we introduce GNPFA, a facial VAE\nwith a latent neural representation of facial expressions and\nhead poses, pre-trained on a wide array of facial scanning\ndata. GNPFA is then utilized to extract high-quality ex-\npressions and head poses from a mass of accessible facial\nvideos from various resources. It brings M2F-D, a large, di-\nverse, and scan-level 3D facial animation dataset with abun-\ndant speech, emotion, and style annotations, over 60 hours.\nFinally, we train our Media2Face model in GNPFA latent\nspace with M2F-D dataset. Media2Face integrates diverse\nmedia inputs as conditions including audio, text, and image,\nwhich flexibly control facial emotion and style while pre-\nserving high-quality lip-sync with speech. The experimen-\ntal results demonstrate the effectiveness of Media2Face and\nshowcase various related applications, such as reconstruct-\ning dialogue situations and multi-modality conditional edit-\ning. We believe Media2Face is a significant step towards re-\nalizing realistic human-centric AI virtual companions with\nstrong emotional connection and resonance with us humans.\nFigure 5. User study result. Note how our method has demon-\nstrated overwhelming superiority in the singing cases, showcasing\nthe model\u2019s ability to generate rich emotions and rhythmic head\nmovements.\nExpression 1\nExpression 2\nNeutral\nIdentity 1\nIdentity 2\nIdentity 3\nIdentity 4\nFigure 6. Retargeting to various identities. Thanks to GNPFA,\nwe can further generate personalized and nuanced facial mesh,\nwhich can fit various identities across different genders, ages, and\nethnicities. Note the differences in facial details among different\nidentities, notably the different wrinkles.\nJazz\nOurs\nFaceFormer\nCodeTalker\nFaceDiffuser\nClub\nWhat\nNo\nSee\nHum\nOurs\nEMOTE\nEmoTalk\nKind\nReally\nSave\nknowing \nsmile\npleasant \nsurprise\ndejected\nirritated\n[Happy]\n[Surprise]\n[Sad]\n[Anger]\nFigure 7.\nQualitative comparison.\nTop:\nComparing with\nemotion-blind methods, we use a neutral prompt to feed to Me-\ndia2Face. Bottom: Comparing with emotion-aware methods. We\nutilize text prompts for Media2Face and assign corresponding\nemotion labels to EMOTE. Notice that EmoTalk extracts emo-\ntional features from audio that cannot be manually assigned.\nArguing with his\nbeloved, he felt\ndeeply sad and \nunjustly wounded.\nThis is\nwhat I   thought  u\nwant                             me                                to\ndo.\nShe got really \nmad and her face \ntwisted up while \nshe argued with \nher partner.\nI                         do !       \nIt\u2019s                                because                         of                                         you !\nYou\nknow      nothing                            be-\nhind           this        \npower.\nOh?      \n\u2026                                            no\nno\nno.\n\u00c1\nvoir le \nmonde et \nsa\nbeau-\nt\u00e9.\nNever      \ngonna\nmake                       you cry ,  never  gonna\nsay                            goodbye. \nAri\nno,       \nmama  no                       sugata\nmiseru no                                       you.\nFigure 8. Result gallery. We generate vivid dialogue scenes (Row 1,2) through scripted textual descriptions. We synthesize stylized facial\nanimations (Row 3,4) through image prompts, which can be emoji or even more abstract images. We also perform emotional singing in\nFrance, English and Japanese(Row 5-7). For more results, please refer to the supplementary video.\nReferences\n[1] Simon Alexanderson, Rajmund Nagy, Jonas Beskow,\nand Gustav Eje Henter. Listen, denoise, action! audio-\ndriven motion synthesis with diffusion models. ACM\nTransactions on Graphics, 42(4):1\u201320, July 2023. 2\n[2] Shivangi Aneja, Justus Thies, Angela Dai, and\nMatthias Nie\u00dfner. Facetalk: Audio-driven motion dif-\nfusion for neural parametric head models, 2023. 2, 3\n[3] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturedif-\nfuclip: Gesture diffusion model with clip latents. ACM\nTrans. Graph., 2023. 2, 7\n[4] Alexei Baevski, Henry Zhou, Abdelrahman Mo-\nhamed, and Michael Auli. wav2vec 2.0: a framework\nfor self-supervised learning of speech representations.\nIn Proceedings of the 34th International Conference\non Neural Information Processing Systems, NIPS\u201920,\nRed Hook, NY, USA, 2020. Curran Associates Inc. 2,\n5\n[5] Volker Blanz and Thomas Vetter. A morphable model\nfor the synthesis of 3d faces.\nIn Proceedings of\nthe 26th Annual Conference on Computer Graphics\nand Interactive Techniques, SIGGRAPH \u201999, page\n187\u2013194, USA, 1999. ACM Press/Addison-Wesley\nPublishing Co. 2\n[6] James Booth, Epameinondas Antonakos, Stylianos\nPloumpis, George Trigeorgis, Yannis Panagakis, and\nStefanos Zafeiriou.\n3d face morphable models\u201d in-\nthe-wild\u201d. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 48\u2013\n57, 2017. 2\n[7] Giorgos\nBouritsas,\nSergiy\nBokhnyak,\nStylianos\nPloumpis, Stefanos Zafeiriou, and Michael Bronstein.\nNeural 3d morphable models: Spiral convolutional\nnetworks for 3d shape representation learning and\ngeneration. In 2019 IEEE/CVF International Confer-\nence on Computer Vision (ICCV), pages 7212\u20137221,\n2019. 2, 3\n[8] Houwei Cao, David G Cooper, Michael K Keut-\nmann, Ruben C Gur, Ani Nenkova, and Ragini Verma.\nCrema-d: Crowd-sourced emotional multimodal ac-\ntors dataset. IEEE transactions on affective comput-\ning, 5(4):377\u2013390, 2014. 5\n[9] Yong Cao, Wen C. Tien, Petros Faloutsos, and\nFr\u00b4ed\u00b4eric Pighin. Expressive speech-driven facial an-\nimation. ACM Trans. Graph., 24(4):1283\u20131302, oct\n2005. 3\n[10] Peng Chen, Xiaobao Wei, Ming Lu, Yitong Zhu,\nNaiming Yao, Xingyu Xiao, and Hui Chen.\nDif-\nfusiontalker:\nPersonalization and acceleration for\nspeech-driven 3d face diffuser, 2023. 2, 3\n[11] Zhixiang Chen and Tae-Kyun Kim. Learning feature\naggregation for deep 3d morphable models. In Pro-\nceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 13164\u201313173,\n2021. 2\n[12] Byoungwon Choe and Hyeong-Seok Ko. Analysis and\nsynthesis of facial expressions with hand-generated\nmuscle actuation basis.\nIn ACM SIGGRAPH 2006\nCourses, pages 21\u2013es. 2006. 2\n[13] Zhaojie Chu, Kailing Guo, Xiaofen Xing, Yilin Lan,\nBolun Cai, and Xiangmin Xu. Corrtalk: Correlation\nbetween hierarchical speech and facial activity vari-\nances for 3d animation, 2023. 3\n[14] Daniel Cudeiro, Timo Bolkart, Cassidy Laidlaw,\nAnurag Ranjan, and Michael J. Black. Capture, learn-\ning, and synthesis of 3d speaking styles. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2019. 2, 3, 5\n[15] Radek Dan\u02c7e\u02c7cek, Kiran Chhatre, Shashank Tripathi,\nYandong Wen, Michael Black, and Timo Bolkart.\nEmotional speech-driven animation with content-\nemotion disentanglement. In SIGGRAPH Asia 2023\nConference Papers, SA \u201923, New York, NY, USA,\n2023. Association for Computing Machinery. 2, 3, 4\n[16] Pif Edwards, Chris Landreth, Eugene Fiume, and\nKaran Singh. Jali: an animator-centric viseme model\nfor expressive lip synchronization. ACM Transactions\non graphics (TOG), 35(4):1\u201311, 2016. 2, 3\n[17] Bernhard Egger, William A. P. Smith, Ayush Tewari,\nStefanie Wuhrer, Michael Zollhoefer, Thabo Beeler,\nFlorian Bernard, Timo Bolkart, Adam Kortylewski,\nSami Romdhani, Christian Theobalt, Volker Blanz,\nand Thomas Vetter. 3d morphable face models\u2014past,\npresent, and future. ACM Trans. Graph., 39(5), jun\n2020. 2\n[18] Paul Ekman and Wallace V Friesen. Facial action cod-\ning system. Environmental Psychology & Nonverbal\nBehavior, 1978. 2\n[19] Tony Ezzat and Tomaso Poggio.\nMiketalk:\nA\ntalking facial display based on morphing visemes.\nIn Proceedings Computer Animation\u201998 (Cat. No.\n98EX169), pages 96\u2013102. IEEE, 1998. 2, 3\n[20] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping\nWang, and Taku Komura. Faceformer: Speech-driven\n3d facial animation with transformers. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), 2022. 2, 3, 7\n[21] Yingruo Fan, Zhaojiang Lin, Jun Saito, Wenping\nWang, and Taku Komura. Joint audio-text model for\nexpressive speech-driven 3d facial animation.\n5(1),\nmay 2022. 3\n[22] G. Fanelli, J. Gall, H. Romsdorfer, T. Weise, and\nL. Van Gool. A 3-d audio-visual corpus of affective\ncommunication. IEEE Transactions on Multimedia,\n12(6):591 \u2013 598, October 2010. 2\n[23] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H. Bermano, Gal Chechik, and Daniel\nCohen-Or. An image is worth one word: Personal-\nizing text-to-image generation using textual inversion,\n2022. 3\n[24] Simon Giebenhain, Tobias Kirschstein, Markos Geor-\ngopoulos,\nMartin R\u00a8unz,\nLourdes Agapito,\nand\nMatthias Nie\u00dfner.\nLearning neural parametric head\nmodels. In Proc. IEEE Conf. on Computer Vision and\nPattern Recognition (CVPR), 2023. 3\n[25] Kazi Injamamul Haque and Zerrin Yumak.\nFacex-\nhubert: Text-less speech-driven e(x)pressive 3d facial\nanimation synthesis using self-supervised speech rep-\nresentation learning, 2023. 3\n[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising\ndiffusion probabilistic models, 2020. 2, 3, 5\n[27] Jonathan Ho and Tim Salimans. Classifier-free dif-\nfusion guidance.\nIn NeurIPS 2021 Workshop on\nDeep Generative Models and Downstream Applica-\ntions, 2021. 5, 6\n[28] Dong-Yan Huang, Ellensi Chandra, Xiangting Yang,\nYing Zhou, Huaiping Ming, Weisi Lin, Minghui\nDong, and Haizhou Li. Visual speech emotion con-\nversion using deep learning for 3d talking head. In\nProceedings of the Joint Workshop of the 4th Work-\nshop on Affective Social Multimedia Computing and\nFirst Multi-Modal Affective Computing of Large-Scale\nMultimedia Data, ASMMC-MMAC\u201918, page 7\u201313,\nNew York, NY, USA, 2018. Association for Comput-\ning Machinery. 3\n[29] Patrik Huber, Guosheng Hu, Rafael Tena, Pouria Mor-\ntazavian, Willem P Koppen, William J Christmas,\nMatthias R\u00a8atsch, and Josef Kittler.\nA multiresolu-\ntion 3d morphable face model and fitting framework.\nIn International conference on computer vision theory\nand applications, volume 5, pages 79\u201386. SciTePress,\n2016. 2\n[30] Patrik Jonell, Taras Kucherenko, Gustav Eje Henter,\nand Jonas Beskow. Let\u2019s face it: Probabilistic multi-\nmodal interlocutor-aware generation of facial gestures\nin dyadic settings. In Proceedings of the 20th ACM In-\nternational Conference on Intelligent Virtual Agents,\nIVA \u201920, New York, NY, USA, 2020. Association for\nComputing Machinery. 3\n[31] G.A. Kalberer and L. Van Gool. Face animation based\non observed 3d speech dynamics.\nIn Proceedings\nComputer Animation 2001. Fourteenth Conference on\nComputer Animation (Cat. No.01TH8596), pages 20\u2013\n251, 2001. 3\n[32] Tero Karras, Timo Aila, Samuli Laine, Antti Herva,\nand Jaakko Lehtinen. Audio-driven facial animation\nby joint end-to-end learning of pose and emotion.\nACM Trans. Graph., 36(4), jul 2017. 3\n[33] Akio Kodaira,\nChenfeng Xu,\nToshiki Hazama,\nTakanori Yoshimoto, Kohei Ohno, Shogo Mitsuhori,\nSoichi Sugano, Hanying Cho, Zhijian Liu, and Kurt\nKeutzer. Streamdiffusion: A pipeline-level solution\nfor real-time interactive generation. 2023. 6\n[34] Paul Koppen, Zhen-Hua Feng, Josef Kittler, Muham-\nmad Awais, William Christmas, Xiao-Jun Wu, and\nHe-Feng Yin. Gaussian mixture 3d morphable face\nmodel. Pattern Recogn., 74(C):617\u2013628, feb 2018. 2\n[35] John P Lewis, Ken Anjyo, Taehyun Rhee, Mengjie\nZhang, Frederic H Pighin, and Zhigang Deng. Practice\nand theory of blendshape facial models. Eurographics\n(State of the Art Reports), 1(8):2, 2014. 2\n[36] Jiaman Li, Zhengfei Kuang, Yajie Zhao, Mingming\nHe, Karl Bladin, and Hao Li. Dynamic facial asset\nand rig generation from a single scan. ACM Trans.\nGraph., 39(6):215\u20131, 2020. 3\n[37] Ruilong Li, Karl Bladin, Yajie Zhao, Chinmay Chi-\nnara, Owen Ingraham, Pengda Xiang, Xinglei Ren,\nPratusha Prasad, Bipin Kishore, Jun Xing, and Hao\nLi. Learning formation of physically-based face at-\ntributes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\nJune 2020. 2\n[38] Tianye Li, Timo Bolkart, Michael J. Black, Hao Li,\nand Javier Romero. Learning a model of facial shape\nand expression from 4d scans. ACM Trans. Graph.,\n36(6), nov 2017. 2\n[39] Han Liang, Jiacheng Bao, Ruichi Zhang, Sihan Ren,\nYuecheng Xu, Sibei Yang, Xin Chen, Jingyi Yu, and\nLan Xu.\nOmg: Towards open-vocabulary motion\ngeneration via mixture of controllers. arXiv preprint\narXiv:2312.08985, 2023. 2\n[40] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen\nPeng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo\nZheng. Beat: A large-scale semantic and emotional\nmulti-modal dataset for conversational gestures syn-\nthesis. In Shai Avidan, Gabriel Brostow, Moustapha\nCiss\u00b4e, Giovanni Maria Farinella, and Tal Hassner, ed-\nitors, Computer Vision \u2013 ECCV 2022, pages 612\u2013630,\nCham, 2022. Springer Nature Switzerland. 2\n[41] Steven R Livingstone and Frank A Russo. The ryerson\naudio-visual database of emotional speech and song\n(ravdess): A dynamic, multimodal set of facial and\nvocal expressions in north american english. PloS one,\n13(5):e0196391, 2018. 5\n[42] Andreas Lugmayr, Martin Danelljan, Andres Romero,\nFisher Yu, Radu Timofte, and Luc Van Gool.\nRe-\npaint: Inpainting using denoising diffusion probabilis-\ntic models.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 11461\u201311471, June 2022. 7\n[43] Juan F Montesinos, Venkatesh S Kadandale, and Glo-\nria Haro. A cappella: Audio-visual singing voice sep-\naration. In 32nd British Machine Vision Conference,\nBMVC 2021, 2021. 5\n[44] Evonne Ng, Hanbyul Joo, Liwen Hu, Hao Li, Trevor\nDarrell, Angjoo Kanazawa, and Shiry Ginosar. Learn-\ning to listen: Modeling non-deterministic dyadic fa-\ncial motion. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR), pages 20395\u201320405, June 2022. 3\n[45] Ziqiao Peng, Yihao Luo, Yue Shi, Hao Xu, Xiangyu\nZhu, Jun He, Hongyan Liu, and Zhaoxin Fan. Self-\ntalk: A self-supervised commutative training diagram\nto comprehend 3d talking faces, 2023. 3\n[46] Ziqiao Peng, Haoyu Wu, Zhenbo Song, Hao Xu,\nXiangyu Zhu, Jun He, Hongyan Liu, and Zhaoxin\nFan. Emotalk: Speech-driven emotional disentangle-\nment for 3d face animation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vi-\nsion, pages 20687\u201320697, 2023. 2, 3, 7\n[47] Hai Xuan Pham, Yuting Wang, and Vladimir Pavlovic.\nEnd-to-end learning for 3d facial animation from\nspeech.\nIn Proceedings of the 20th ACM Interna-\ntional Conference on Multimodal Interaction, ICMI\n\u201918, page 361\u2013365, New York, NY, USA, 2018. As-\nsociation for Computing Machinery. 3\n[48] Dafei Qin, Jun Saito, Noam Aigerman, Thibault\nGroueix, and Taku Komura. Neural face rigging for\nanimating and retargeting facial meshes in the wild.\narXiv preprint arXiv:2305.08296, 2023. 2, 3\n[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. Learning transferable visual models from natural\nlanguage supervision. In International conference on\nmachine learning, pages 8748\u20138763. PMLR, 2021. 2,\n5\n[50] Alexander Richard, Michael Zollh\u00a8ofer, Yandong Wen,\nFernando de la Torre, and Yaser Sheikh. Meshtalk:\n3d face animation from speech using cross-modality\ndisentanglement. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV),\npages 1173\u20131182, October 2021. 3, 7\n[51] Robin\nRombach,\nAndreas\nBlattmann,\nDominik\nLorenz, Patrick Esser, and Bj\u00a8orn Ommer.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\npages 10684\u201310695, June 2022. 3\n[52] Tim Salimans and Jonathan Ho. Progressive distilla-\ntion for fast sampling of diffusion models, 2022. 2,\n3\n[53] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H\nBermano.\nHuman motion diffusion as a generative\nprior. arXiv preprint arXiv:2303.01418, 2023. 7\n[54] Il-Kyu Shin, A. Cengiz \u00a8Oztireli, Hyeon-Joong Kim,\nThabo Beeler, Markus Gross, and Soo-Mi Choi. Ex-\ntraction and Transfer of Facial Expression Wrinkles\nfor Facial Performance Enhancement. In John Keyser,\nYoung J. Kim, and Peter Wonka, editors, Pacific\nGraphics Short Papers. The Eurographics Associa-\ntion, 2014. 2\n[55] Jascha\nSohl-Dickstein,\nEric\nWeiss,\nNiru\nMah-\neswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics.\nIn\nFrancis Bach and David Blei, editors, Proceedings of\nthe 32nd International Conference on Machine Learn-\ning, volume 37 of Proceedings of Machine Learning\nResearch, pages 2256\u20132265, Lille, France, 07\u201309 Jul\n2015. PMLR. 2, 3\n[56] Stefan Stan, Kazi Injamamul Haque, and Zerrin Yu-\nmak. Facediffuser: Speech-driven 3d facial animation\nsynthesis using diffusion. In ACM SIGGRAPH Con-\nference on Motion, Interaction and Games (MIG \u201923),\nNovember 15\u201317, 2023, Rennes, France, New York,\nNY, USA, 2023. ACM. 2, 3, 7\n[57] Zhiyao Sun, Tian Lv, Sheng Ye, Matthieu Gaetan Lin,\nJenny Sheng, Yu-Hui Wen, Minjing Yu, and Yong jin\nLiu. Diffposetalk: Speech-driven stylistic 3d facial an-\nimation and head pose generation via diffusion mod-\nels, 2023. 2, 3, 4, 6, 7\n[58] Sarah Taylor, Taehwan Kim, Yisong Yue, Moshe\nMahler, James Krahe, Anastasio Garcia Rodriguez,\nJessica Hodgins, and Iain Matthews. A deep learn-\ning approach for generalized speech animation. ACM\nTrans. Graph., 36(4), jul 2017. 3\n[59] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,\nDaniel Cohen-Or, and Amit H Bermano. Human mo-\ntion diffusion model. 2022. 2, 5, 7\n[60] Balamurugan Thambiraja, Sadegh Aliakbarian, Dar-\nren Cosker, and Justus Thies.\n3diface: Diffusion-\nbased speech-driven 3d facial animation and editing,\n2023. 2, 3\n[61] Balamurugan Thambiraja, Ikhsanul Habibie, Sadegh\nAliakbarian, Darren Cosker, Christian Theobalt, and\nJustus Thies. Imitator: Personalized speech-driven 3d\nfacial animation. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision (ICCV),\npages 20621\u201320631, October 2023. 3\n[62] Justus Thies, Mohamed Elgharib, Ayush Tewari,\nChristian Theobalt, and Matthias Nie\u00dfner.\nNeural\nvoice puppetry: Audio-driven facial reenactment. In\nAndrea Vedaldi, Horst Bischof, Thomas Brox, and\nJan-Michael Frahm, editors, Computer Vision \u2013 ECCV\n2020, pages 716\u2013731, Cham, 2020. Springer Interna-\ntional Publishing. 3\n[63] Luan Tran, Feng Liu, and Xiaoming Liu.\nTowards\nhigh-fidelity nonlinear 3d face morphable model. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 1126\u2013\n1135, 2019. 2\n[64] Luan Tran and Xiaoming Liu.\nNonlinear 3d face\nmorphable model. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 7346\u20137355, 2018. 2\n[65] Luan Tran and Xiaoming Liu. On learning 3d face\nmorphable model from in-the-wild images.\nIEEE\ntransactions on pattern analysis and machine intelli-\ngence, 43(1):157\u2013171, 2019. 2\n[66] Aaron Van Den Oord, Oriol Vinyals, et al.\nNeural\ndiscrete representation learning. Advances in neural\ninformation processing systems, 30, 2017. 3\n[67] Monica Villanueva Aylagas, Hector Anadon Leon,\nMattias Teye, and Konrad Tollmar.\nVoice2face:\nAudio-driven facial and tongue rig animations with\ncvaes.\nComputer Graphics Forum, 41(8):255\u2013265,\n2022. 3\n[68] Kaisiyuan Wang, Qianyi Wu, Linsen Song, Zhuoqian\nYang, Wayne Wu, Chen Qian, Ran He, Yu Qiao, and\nChen Change Loy. Mead: A large-scale audio-visual\ndataset for emotional talking-face generation. In An-\ndrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-\nMichael Frahm, editors, Computer Vision \u2013 ECCV\n2020, pages 700\u2013717, Cham, 2020. Springer Interna-\ntional Publishing. 5\n[69] Cheng-hsin Wuu, Ningyuan Zheng, Scott Ardisson,\nRohan Bali, Danielle Belko, Eric Brockmeyer, Lucas\nEvans, Timothy Godisart, Hyowon Ha, Xuhua Huang,\net al. Multiface: A dataset for neural face rendering.\narXiv preprint arXiv:2207.11243, 2022. 3\n[70] Jinbo Xing, Menghan Xia, Yuechen Zhang, Xiaodong\nCun, Jue Wang, and Tien-Tsin Wong.\nCodetalker:\nSpeech-driven 3d facial animation with discrete mo-\ntion prior.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition,\npages 12780\u201312790, 2023. 2, 3, 7\n[71] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong,\nRunsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and\nMing-Hsuan Yang. Diffusion models: A comprehen-\nsive survey of methods and applications. ACM Com-\nput. Surv., 56(4), nov 2023. 3\n[72] Tarun Yenamandra, Ayush Tewari, Florian Bernard,\nHans-Peter Seidel, Mohamed Elgharib, Daniel Cre-\nmers, and Christian Theobalt. i3dmm: Deep implicit\n3d morphable model of human heads. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 12803\u201312813, 2021. 3\n[73] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao,\nYandong Wen, Timo Bolkart, Dacheng Tao, and\nMichael J. Black. Generating holistic 3d human mo-\ntion from speech, 2023. 2\n[74] Hongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao,\nYandong Wen, Timo Bolkart, Dacheng Tao, and\nMichael J. Black. Generating holistic 3d human mo-\ntion from speech. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 469\u2013480, June 2023. 3\n[75] Chenxu Zhang, Saifeng Ni, Zhipeng Fan, Hongbo Li,\nMing Zeng, Madhukar Budagavi, and Xiaohu Guo. 3d\ntalking face with personalized pose dynamics. IEEE\nTransactions on Visualization and Computer Graph-\nics, 29(2):1438\u20131449, 2023. 3\n[76] Longwen Zhang, Qiwei Qiu, Hongyang Lin, Qixuan\nZhang, Cheng Shi, Wei Yang, Ye Shi, Sibei Yang, Lan\nXu, and Jingyi Yu. Dreamface: Progressive genera-\ntion of animatable 3d faces under text guidance. arXiv\npreprint arXiv:2304.03117, 2023. 7\n[77] Longwen Zhang, Chuxiao Zeng, Qixuan Zhang,\nHongyang Lin, Ruixiang Cao, Wei Yang, Lan Xu, and\nJingyi Yu. Video-driven neural physically-based facial\nasset for production. volume 41, pages 1\u201316. ACM\nNew York, NY, USA, 2022. 4\n[78] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong\nZhang, Xi Shen, Yu Guo, Ying Shan, and Fei Wang.\nSadtalker: Learning realistic 3d motion coefficients\nfor stylized audio-driven single image talking face an-\nimation, 2022. 7\n[79] Zhimeng Zhang, Lincheng Li, Yu Ding, and Changjie\nFan.\nFlow-guided one-shot talking face generation\nwith a high-resolution audio-visual dataset. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021. 5\n[80] Mingwu Zheng, Hongyu Yang, Di Huang, and Liming\nChen. Imface: A nonlinear 3d morphable face model\nwith implicit neural representations. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 20343\u201320352, 2022. 3\n[81] Yang Zhou, Zhan Xu, Chris Landreth, Evange-\nlos Kalogerakis, Subhransu Maji, and Karan Singh.\nVisemenet: Audio-driven animator-centric speech an-\nimation. 37(4), jul 2018. 3\n"
  },
  {
    "title": "StableIdentity: Inserting Anybody into Anywhere at First Sight",
    "link": "https://arxiv.org/pdf/2401.15975.pdf",
    "upvote": "16",
    "text": "StableIdentity: Inserting Anybody into Anywhere at First Sight\nQinghe Wang1, Xu Jia1\u2217, Xiaomin Li1, Taiqing Li1, Liqian Ma2, Yunzhi Zhuge1, Huchuan Lu1\n1Dalian University of Technology, 2ZMO AI Inc.\nhttps://qinghew.github.io/StableIdentity\nAbstract\nRecent advances in large pretrained text-to-image\nmodels have shown unprecedented capabilities for\nhigh-quality human-centric generation, however,\ncustomizing face identity is still an intractable\nproblem.\nExisting methods cannot ensure stable\nidentity preservation and flexible editability, even\nwith several images for each subject during train-\ning. In this work, we propose StableIdentity, which\nallows identity-consistent recontextualization with\njust one face image. More specifically, we employ\na face encoder with an identity prior to encode the\ninput face, and then land the face representation\ninto a space with an editable prior, which is con-\nstructed from celeb names. By incorporating iden-\ntity prior and editability prior, the learned identity\ncan be injected anywhere with various contexts. In\naddition, we design a masked two-phase diffusion\nloss to boost the pixel-level perception of the input\nface and maintain the diversity of generation. Ex-\ntensive experiments demonstrate our method out-\nperforms previous customization methods. In addi-\ntion, the learned identity can be flexibly combined\nwith the off-the-shelf modules such as ControlNet.\nNotably, to the best knowledge, we are the first to\ndirectly inject the identity learned from a single im-\nage into video/3D generation without finetuning.\nWe believe that the proposed StableIdentity is an\nimportant step to unify image, video, and 3D cus-\ntomized generation models.\n1\nIntroduction\nWith the boom in diffusion models [29, 28, 47], customized\ngeneration has garnered widespread attention [31, 45, 18].\nThis task aims to inject new subject (e.g., identity) into the\ntext-to-image models and generate images with consistent\nsubjects in various contexts while aligning the input text\nprompt. For example, users can upload their photos to ob-\ntain interesting pictures, such as \u201cwearing a Superman outfit\u201d.\n\u2217 Corresponding authors.\nThe success of customized generation can facilitate many ap-\nplications such as personalized portrait photos [20], virtual\ntry-on [5] and art & design [26].\nHowever, existing customization methods solve this task\nby either finetuning the part/all parameters of the model\nor learning a generic encoder. Parameter finetuning meth-\nods [12, 8, 30] take a long time to search optimal parame-\nters, but often return an inaccurate trivial solution for rep-\nresenting the identity. Especially if only with a single im-\nage, these methods tend to overfit the input, resulting in ed-\nitability degradation. Alternatively, the encoder-based meth-\nods [43, 42] require large-scale datasets for training and strug-\ngle to capture distinctive identity and details. Moreover, the\nidentity learned by current methods is susceptible to be incon-\nsistent with the target identity in various contexts. Therefore,\nthere is an urgent need to propose a new framework to address\nthe enormous challenges (e.g., unstable identity preservation,\npoor editability, inefficiency) faced by this task.\nHere we are particularly interested in customized genera-\ntion for human under one-shot training setting, and how to\nstore identity information into word embeddings, which can\nnaturally integrate with text prompts. We believe prior knowl-\nedge can help for this task. On the one hand, face recognition\ntask [36] has been fully explored and the identity-aware abil-\nity of pretrained models can be exploited. On the other hand,\ntext-to-image models, which are trained on massive internet\ndata, can generate images with celeb names in various con-\ntexts, thus these names contain rich editability prior. Using\nthese priors can alleviate these challenges, and some meth-\nods [6, 45] have made preliminary attempts.\nIn this work, we propose StableIdentity which incorporates\nidentity prior and editability prior into the human-centric cus-\ntomized generation. Specifically, an encoder pretrained on\nface recognition task is introduced to capture identity rep-\nresentation. Celeb names are collected to construct an em-\nbedding space as a prior identity distribution for customized\ngeneration. To encourage the target identity to perform like\nceleb names in pretrained diffusion model, we further land the\nidentity representation into the prior space. Furthermore, to\nlearn more stable identity and fine-grained reconstruction, we\ndesign a masked two-phase diffusion loss, which assigns spe-\ncialized objectives in the early and late phases of denoising\nprocess respectively. Extensive experiments show StableI-\ndentity performs favorably against state-of-the-art methods\narXiv:2401.15975v1  [cs.CV]  29 Jan 2024\nInput Image wearing headphones \nwith red hair\ncoding in front \nof a computer\nwearing a \nSuperman outfit\nin a helmet and vest \nriding a motorcycle\nInput Image\nswimming \nin the pool\nColorful graffiti of \n& ControlNet \n(OpenPose)\nplaying the guitar \nholding a cake\nclimbing a \nmountain\ncooking\nplaying video games\nlaughing on \nthe lawn\n& ControlNet \n(OpenPose)\n& ModelScopeT2V: putting on makeup\n& LucidDreamer: wearing a golden crown\n& LucidDreamer: as oil painting \n& ModelScopeT2V: eating a burger\nFigure 1: Given a single input image, the proposed StableIdentity can generate diverse customized images in various contexts. Notably, we\npresent that the learned identity can be combined with ControlNet [48] and even injected into video (ModelScopeT2V [37]) and 3D (Lucid-\nDreamer [19]) generation.\nand we further analyse our superiority over several baselines\nof the same kind. The proposed method also shows stable\ngeneralization ability, which can directly collaborate with the\noff-the-shelf image/video/3D models as shown in Figure 1.\nOur contributions can be summarized as follows:\n\u2022 We propose StableIdentity, which incorporates identity\nprior and editability prior to enable identity-consistent\nrecontextualization with just one face image.\n\u2022 We design a masked two-phase diffusion loss to perceive\npixel-level details and learn more stable identity for di-\nverse generation.\n\u2022 Extensive experiments show that our method is effec-\ntive and prominent. Remarkably, our method can not\nonly combine with image-level modules, but also unlock\nthe generalization ability that the identity learned from a\nsingle image can achieve identity-consistent customized\nvideo/3D generation without finetuning.\n2\nRelated Work\n2.1\nText-to-Image Diffusion Models\nDiffusion models [15, 34] have exhibited overwhelming suc-\ncess in text-conditioned image generation, deriving numer-\nous classical works [29, 24, 13]. Among them, Stable Dif-\nfusion [29] is widely used for its excellent open-source envi-\nronment. In practice, Stable Diffusion can generate diverse\nand exquisite images from Gaussian noises and text prompts\nwith DDIM sampling [34]. Since the training dataset contains\nlots of celeb photos and corresponding names, Stable Diffu-\nsion can combine celeb names with different text prompts to\ngenerate diverse images. However, ordinary people cannot\nenjoy this \u201cprivilege\u201d directly. Therefore, to democratize Sta-\nble Diffusion to broader users, many studies [6, 45, 4] have\nfocused on the customized generation task.\n2.2\nCustomized Generation\nCurrently, customized generation methods can be mainly di-\nvided into optimization-based and encoder-based methods.\nThe former often require long time to optimize, while the\nlatter need large-scale data and struggle to learn a distinc-\ntive identity. Given 3-5 images of the same subject, Textual\nInversion [12] optimizes a new word embedding to repre-\nsent the target subject. DreamBooth [30] finetunes the en-\ntire model to fit the target subject only. On the other hand,\nELITE [38], InstantBooth [33] and IP-Adapter [43] intro-\nduce identity information into attention layers by learning\nan encoder. FastComposer [40] trains its encoder with the\nwhole U-Net of Stable Diffsuion together to capture identi-\nties. There are also some methods that incorporate an en-\ncoder to assist the optimization-based methods [39], raising\nthe performance ceiling. Celeb-Basis [45] collects 691 celeb\nnames which are editable in Stable Diffusion to build a celeb\nbasis by PCA [25]. The weight of basis is optimized based\n\ud835\udf16\ud835\udf03\nAdaIN\nTokenizer & Embedding Layer\n\u00b7\u00b7\u00b7\nCeleb Embedding Space\nA \nphoto \nof\nMLPs\n\ud835\udc631\n\u2217\n\ud835\udc632\n\u2217\n\ud835\udf41, \ud835\udf48\nA\nphoto\nof\nTom Cruise\nTaylor Swift Will Smith\n\u00b7\u00b7\u00b7\nWord Embeddings\nText2Image\nModel\nData Aug.\n\ud835\udc61 < \ud835\udefc\ud835\udc47\n\ud835\udc670\nAdding Noise\n\ud835\udc61 \u2265 \ud835\udefc\ud835\udc47\n\ud835\udf16\n\u01b8\ud835\udc670\n\u2112\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc52\n\u2112\ud835\udc5f\ud835\udc52\ud835\udc50\nFR-ViT\nFigure 2: Overview of the proposed StableIdentity. Given a single face image, we first employ a FR-ViT encoder and MLPs to capture\nidentity representation, and then land it into our constructed celeb embedding space to better learn identity-consistent editability. In addition,\nwe design a masked two-phase diffusion loss including Lnoise and Lrec for training.\non the output of ArcFace encoder [7], a new identity\u2019s repre-\nsentation can be obtained by weighting the basis. However,\nthe mentioned methods still perform imbalance on identity\npreservation and editability.\nIn comparison, our method exploits identity and editabil-\nity prior to significantly ease the optimization process, and\nlearns more stable identity with the proposed loss. Since Sta-\nble Diffusion is fixed, plug-and-play modules such as Con-\ntrolNet [48] can be employed seamlessly. Furthermore, to\nthe best knowledge, we are the first work to enable the learn\nidentity from a single image injected into video [37] / 3D gen-\neration [19].\n3\nMethod\nGiven a single face image, we aim to represent its iden-\ntity via word embeddings as shown in Figure 2, to im-\nplement identity-consistent recontextualization under various\ntext prompts. To achieve this, we incorporate identity prior\nand editability prior (See Sec. 3.2) and propose a masked two-\nphase diffusion loss (See Sec. 3.3).\n3.1\nPreliminary\nIn this work, we adopt the pretrained Stable Diffusion [29]\nas our text-to-image model (denoted as SD). SD consists of\nthree components: a VAE (E, D) [11], a denoising U-Net\n\u03f5\u03b8 and a CLIP text encoder etext [27]. Benefiting from the\nhigh-quality reconstruction of VAE, the diffusion process of\ninput image x is performed in the latent space z (z = E(x)).\nSpecifically, at random timestep t (t \u2208 [1, 1000)), zt can be\nsampled as a weighted combination z0 and a random noise\n\u03f5 (\u03f5 \u223c N(0, I)):\nzt = \u221a\u00af\u03b1tz0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5,\n(1)\nwhere \u00af\u03b1t is a predefined hyperparameter set. Meanwhile,\ngiven text prompts p, the tokenizer of etext divides and en-\ncodes p into l integer tokens sequentially. Then, the embed-\nding layer in etext obtains a corresponding embedding group\ng = [v1, ..., vl], vi \u2208 Rd which consists of l word embeddings\nby looking up the dictionary. After that, the text transformer\n\u03c4text of etext further represents g to guide model to generate\nimages conforming to the given text prompts p. With latent\nzt, the training process is optimized by:\nLnoise = Ez,g,\u03f5,t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c4text(g))\u22252\n2\n\u0003\n(2)\n3.2\nModel Architecture\nIdentity Prior. Existing methods extract subject information\ncommonly with CLIP image encoder, which is pretrained for\nlearning high-level semantics, lacks detailed identity percep-\ntion. Therefore, we employ a ViT [9] encoder finetuned for\nface recognition task (denote as FR-ViT) to reap ID-aware\nrepresentation I from the input image.\nTo maintain the generalizability and editability of the\nvanilla SD, we fix the FR-ViT encoder and SD. Follow-\ning [6, 45], we only project I into two word embeddings\n[v\u2032\n1, v\u2032\n2] with MLPs:\n[v\u2032\n1, v\u2032\n2] = MLPs(I)\n(3)\nBenefiting the identity prior knowledge, we can inject fa-\ncial features from the input image into diffusion model more\nefficiently without additional feature injection.\nEditability Prior. Since SD is trained on large-scale internet\ndata, using celeb names can generate images with prompt-\nconsistent identity. Therefore, we posit that the celeb names\nconstitute a space with editability prior. We consider 691\nceleb names [45] as sampling points in this space and intend\nto represent this space distribution with the mean and stan-\ndard deviation of their word embeddings. However, in prac-\ntice, the tokenizer decomposes unfamiliar word into multiple\nInput Image (\ud835\udc61 = 0)\n\ud835\udc61 = 300\n\ud835\udc61 = 400\n\ud835\udc61 = 600\n\ud835\udc61 = 700\n\ud835\udc61 = 800\n\ud835\udc61 = 900\n\ud835\udc61 = 500\n\ud835\udc61 = 1000\n\u22c5\u22c5\u22c5\n\ud835\udc670 \u2192 \ud835\udc67\ud835\udc61 \u2192 \u01b8\ud835\udc670\nFigure 3: We present the predicted \u02c6z0 from zt at various timestep t.\n\u02c6z0 at t = {100, 200}, similar to t = 300, are omitted for brevity.\ntokens (e.g., Deschanel \u2192 [561, 31328, 832]), consequently\nthe number of tokens produced by different celeb names may\nnot be equal. To find an editable space with a uniform dimen-\nsion, we select celeb names consisting only of first name and\nlast name, and each word corresponds to only one token (e.g.,\nTom Cruise \u2192 [2435, 6764]). Eventually we obtain 326 celeb\nnames and encode them into the corresponding word embed-\ndings C \u2208 R326\u00d7d.\nTo master the identity-consistent recontextualization abil-\nity like celeb embeddings, we employ AdaIN [10] to incorpo-\nrate the editablity prior and land [v\u2032\n1, v\u2032\n2] into celeb embedding\nspace:\nv\u2217\ni = \u03c3(C)(v\u2032\ni \u2212 \u00b5(v\u2032\ni)\n\u03c3(v\u2032\ni)\n) + \u00b5(C), for i = 1, 2\n(4)\nwhere \u00b5(v\u2032\ni), \u03c3(v\u2032\ni) are scalars. \u00b5(C) \u2208 Rd, \u03c3(C) \u2208 Rd\nare vectors, since each dimension of C has a different dis-\ntribution. With this editablity prior, the learned embeddings\n[v\u2217\n1, v\u2217\n2] are closer to the celeb embedding space than base-\nlines as shown in Figure 7, which improves editablity ele-\ngantly and effectively. In addition, it also constrains the opti-\nmization process within the celeb embedding space and pre-\nvents drifting towards other categories.\n3.3\nModel Training\nTwo-Phase Diffusion Loss. In addition to the architecture\ndesign, we rethink the training objective of diffusion mod-\nels. The vanilla training loss Lnoise excites the denoising\nU-Net \u03f5\u03b8 to predict the noise \u03f5 contained in the input zt at\nany time t, and the introduced \u03f5 is randomly sampled each\ntime. Therefore, such an objective function only implicitly\nand inefficiently learns the identity in the input image.\nDDIM [34] proposes a denoised observation predicted by\na variant of Eq. 1: \u02c6z0 =\nzt\u2212\u221a1\u2212\u00af\u03b1t\u03f5\u03b8\n\u221a\u00af\u03b1t\n. A naive idea is to\nreplace Lnoise with the mean squared error between the pre-\ndicted \u02c6z0 and the real z0 [39]: Lrec = Ez,g,\u03f5,t\n\u0002\n\u2225\u02c6z0 \u2212 z0\u22252\n2\n\u0003\n,\nwhich can explicitly optimize the reconstruction for z0. How-\never, we observe that as timestep increases, predicted \u02c6z0 be-\ncomes more difficult to approximate the true distribution of\nz0 as shown in Figure 3. Therefore, for larger timestep, Lrec\nbecomes less meaningful and even misleads the model to fo-\ncus excessively on pixel-level reconstruction. To this end, we\npropose two-phase diffusion loss divided by timestep \u03b1T:\nLdiffusion =\n\u001aEz,g,\u03f5,t\n\u0002\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, \u03c4text(g))\u22252\n2\n\u0003\nt \u2265 \u03b1T,\nEz,g,\u03f5,t\n\u0002\n\u2225\u02c6z0 \u2212 z0\u22252\n2\n\u0003\nt < \u03b1T. (5)\nEmpirically, the division parameter \u03b1 \u2208 [0.4, 0.6] yields\ngood results that balance identity preservation and diver-\nsity (\u03b1 = 0.6 as default). Using Lnoise at the early phase\nof denoising process that decides the layout of generated im-\nage [21, 40, 23] can allow the learned identity to adapt to\ndiverse layouts, while using Lrec at the late phase can boost\nthe pixel-level perception for input image to learn more stable\nidentity.\nMasked Diffusion Loss. To prevent learning irrelevant back-\nground, we also employ the masked diffusion loss [1, 39].\nSpecifically, we use a pretrained face parsing model [44] to\nobtain the face mask Mf and hair mask Mh of the input im-\nage. The training loss is calculated in the face area and hair\narea respectively:\nL = Mf \u2299 Ldiffusion + \u03b2Mh \u2299 Ldiffusion.\n(6)\nIn our experiments, we set \u03b2 = 0.1 as default.\n4\nExperiments\n4.1\nExperimental Setting\nImplementation Details. Our experiments are based on Sta-\nble Diffusion 2.1-base. The FR-ViT is a ViT-B/16 encoder\nfinetuned for face recognition task. For an input single image,\nwe use color jitter, random shift, random crop and random\nresize as data augmentations. The learning rate and batch\nsize are set to 5e \u2212 5 and 1. The MLPs are trained for 450\nsteps (4 mins). The placeholders v\u2217\n1 v\u2217\n2 of prompts such as \u201cv\u2217\n1\nv\u2217\n2 wearing a spacesuit\u201d, \u201clatte art of v\u2217\n1 v\u2217\n2\u201d are omitted for\nbrevity in this paper. The scale of classifier-free guidance [14]\nis set to 8.5 by default. Our experiments are conducted on a\nsingle A800 GPU.\nDataset. We randomly select 70 non-celeb images from the\nFFHQ [16] and resize to 512\u00d7512 as our test dataset. To per-\nform a comprehensive evaluation, we employ 40 test prompts\nwhich cover actions, decorations, attributes, expressions and\nbackgrounds [18].\nBaselines.\nWe compare the proposed method with base-\nlines including the optimization-based methods: Textual In-\nversion [12], DreamBooth [30], Celeb-Basis [45] and the\nencoder-based methods: ELITE [38], FastComposer [40], IP-\nAdapter [43]. We prioritize using the official model released\nby each method. For Textual Inversion and DreamBooth, we\nuse their Stable Diffusion 2.1 versions for a fair comparison.\nEvaluation Metrics. Following DreamBooth [30], we cal-\nculate CLIP [27] visual similarity (CLIP-I) to evaluate high-\nlevel semantic alignment and text-image similarity (CLIP-\nT) to measure editablity.\nBesides, we calculate the Face\nSimilarity by ArcFace [7] and Face Diversity [18, 39] by\nLPIPS [46] on detected face regions between the generated\nimages and real images of the same ID. However, some\nbaselines may generate completely inconsistent faces under\nvarious text prompts, which incorrectly raise face diversity.\nTherefore, we propose the Trusted Face Diversity by the\nwearing a \nspacesuit\nwearing a \ndoctoral cap\nholding a bottle \nof red wine\nwith white hair, \nwearing glasses\nswimming \nin the pool\nTextual Inversion\nDreamBooth\nELITE\nFastComposer\nIP-Adapter\nCeleb-Basis\nStableIdentity(Ours)\nInput Image\nlatte art of \nFigure 4: We present the qualitative comparisons with six baselines for different identities (including various races) and diverse text\nprompts (covering decoration, action, attribute, background, style). Our method achieves high-quality generation with consistent identity\nand outstanding editability (Zoom-in for the best view). We provide more results in supplementary material.\nproduct of cosine distances from face similarity and face di-\nversity for each pair images, to evaluate whether the gener-\nated faces are both diverse and similar. To measure the qual-\nity of generation, we randomly sample 70 celeb names to\ngenerate images with test prompts as pseudo ground truths\nand calculate Fr\u00b4echet Inception Distance (FID) [22] between\nthe generated images by the competing methods and pseudo\nground truths.\n4.2\nComparison\nQualitative Comparison. As shown in Figure 4, given a\nsingle image as input, we show the generation results with\nvarious prompts. Textual Inversion is optimized only with\nLnoise, which leads to a trivial solution for identity in dif-\nferent contexts. DreamBooth finetunes the whole SD model\nto fit the input face, but still fails to learn similar iden-\ntities (row 1th, 5th) and tends to replicate the foreground\nface (row 2th, 3th). The encoder-based methods ELITE and\nIP-Adapter only learn rough shape and attributes of the in-\nput face, perform mediocrely in both identity preservation\nand editability. FastComposer finetunes a CLIP image en-\ncoder and the whole SD for learning identities, but suffers\nfrom low quality and artifacts (row 4th, 5th, 6th).\nCeleb-\nBasis also fails to learn accurate identity for recontextualiza-\ntion (row 1th, 3th). Notably, when using \u201clatte art of\u201d as text\nprompt, all baselines either produce inconsistent identity or\nfail to get the desired style in row 6th. In comparison, bene-\nfiting from the effectiveness of the proposed method, our re-\nsults shows superiority both in terms of identity preservation\nand editablity.\nQuantitative Comparison. In addition, we also report the\nquantitative comparison in Table 1.\nSome baselines like\nELITE and IP-Adapter learn only facial structure and at-\ntributes, and are susceptible to generate frontal view, resulting\nin better CLIP-I. This metric focuses on high-level seman-\ntic alignment and ignores identity consistency.\nTherefore,\nthese methods obtain worse face similarity (-24.54, -15.39\nthan ours) and trusted face diversity (-9.66, -3.95 than ours).\nTable 1: Quantitative comparisons with baselines. \u2191 indicates higher is better, while \u2193 indicates that lower is better. The best result is shown\nin bold. Our method obtains the best results over the text consistency (i.e., CLIP-T), identity preservation (i.e., Face Similarity), diversity of\ngenerated faces (i.e., Trusted Face Diversity), and generation quality (i.e., FID).\nCLIP-I\u2191(%)\nCLIP-T\u2191(%)\nFace Sim.\u2191(%)\nFace Div.\u2191(%)\nTrusted Div.\u2191(%)\nFID\u2193\nTextual Inversion\n61.30\n28.23\n31.30\n37.03\n10.75\n28.64\nDreamBooth\n67.01\n28.91\n35.80\n36.79\n5.89\n48.55\nELITE\n73.94\n26.43\n12.58\n25.55\n5.35\n84.32\nFastComposer\n72.32\n28.87\n36.91\n28.84\n13.90\n47.98\nIP-Adapter\n85.14\n23.67\n21.73\n25.61\n11.06\n78.95\nCeleb-Basis\n63.69\n27.84\n25.55\n37.85\n13.41\n33.72\nStableIdentity (Ours)\n65.91\n29.03\n37.12\n35.46\n15.01\n24.92\nTable 2: Ablation study. We also present results with various divi-\nsion parameter \u03b1 in the supplementary material.\nCLIP-T\u2191 Face Sim.\u2191 Trusted Div.\u2191 FID\u2193\nCLIP Enc.\n28.03\n35.73\n14.81\n25.66\nw/o AdaIN\n24.81\n47.81\n13.73\n48.73\nw/o Mask\n28.15\n34.98\n14.47\n25.12\nOnly Lnoise\n28.81\n36.55\n14.97\n25.76\nOnly Lrec\n27.35\n30.69\n13.89\n40.54\nOurs\n29.03\n37.12\n15.01\n24.92\nWe also observe that the optimization-based methods Textual\nInversion and DreamBooth fail to learn stable identities for\nrecontextualization and tend to overfit to the input face, lead-\ning to poor trusted face diversity (-4.26, -9.12 than ours). Our\nmethod achieves best performance on vision-language align-\nment (CLIP-T), identity preservation (Face Sim.), identity-\nconsistent diversity (Trusted Div.) and image quality (FID).\n4.3\nAblation Study\nWe conduct a comprehensive ablation study across various\nsettings as shown in Table 2 and Figure 5, 6. We employ\nthe CLIP Image Encoder as a baseline which is commonly\nadopted in encoder-based methods. Following [33, 40], we\nuse the CLS feature of CLIP encoder\u2019s last layer to extract\nidentity information. In col 2 of Figure 5, it can be observed\nthat the CLIP image encoder is mediocre for identity preser-\nvation (-1.39 than ours on Face Sim.). On the other hand, the\nsetting of \u201cw/o AdaIN\u201d cannot explicitly learn editability and\nfails to limit the value range of the learned word embeddings.\nIt tends to generate the frontal faces and fails to align the de-\nsired text prompt (col 3 in Figure 5), therefore it obtains high\nface similarity but poor CLIP-T, Trusted Div., and FID (-4.22,\n-1.28, -23.81 than ours).\nFurthermore, we show the ablation results for the train-\ning loss.\nThe masked diffusion loss has been proven ef-\nfective [1, 39] and it does help focus foreground and pre-\nvent background leakage. The reconstruction of the \u201cOnly\nLnoise\u201d setting is inferior than ours and is prone to undesired\nchanges and artifacts (col 3 in Figure 6), resulting lower iden-\ntity preservation and image quality (i.e., -0.60, -0.84 than ours\non Face Sim., FID). Due to the meaningless Lrec in the early\nphase of denoise process, the \u201cOnly Lrec\u201d setting only learns\nin the snow\nwearing a \nChristmas hat\nCLIP Encoder\nw/o AdaIN\nOurs\nInput Image\nFigure 5: Ablation study for model architecture. We show the results\nof using the CLIP image encoder and removing the AdaIN.\nwearing \nheadphones\nwith red hair\nw/o mask\nOnly \u2112\ud835\udc5b\ud835\udc5c\ud835\udc56\ud835\udc60\ud835\udc52\nOurs\nInput Image\nOnly \u2112\ud835\udc5f\ud835\udc52\ud835\udc50\nFigure 6: Ablation study for training loss. We present the visualiza-\ntion results of various loss settings.\nmediocre identities with artifacts (col 4 in Figure 6) and leads\nto unsatisfactory face similarity, trusted diversity, and FID (-\n6.43, -1.12, -15.62 than ours). In comparison, the proposed\nmasked two-phase diffusion loss shows best results, and the\ndiscussion of the division parameter \u03b1 can be found in sup-\nplementary material.\n5\nDiscussion\n5.1\nDownstream Applications\nPose-controlled Customized Image Generation. Since the\npretrained Stable Diffusion is fixed, SD-based plug-and-play\nmodules can collaborate with our method. ControlNet con-\ntrols the pretrained SD to support additional input conditions\nsuch as keypoints, edge maps, etc. In this paper, we obtain\npose images with human skeletons as condition by Open-\nPose [3], as an example. As shown in the row 2 of Figure 1,\nFigure 7: 2-D visualization of word embeddings using t-SNE with\nCeleb names, Textual Inversion, Celeb-Basis and our method.\nwe demonstrate the integration of StableIdentity and Control-\nNet (SD2.1 version) which achieves simultaneous structure-\ncontrolled and identity-driven generation.\nZero-shot Identity-driven Video/3D Generation.\nOur\nmethod can be considered as introducing new identity for\nthe dictionary of CLIP text encoder. Therefore, we believe\nthat ideally, as long as the SD-based video and 3D generation\nmodels do not finetune the CLIP text encoder, the learned\nidentity can be directly injected into these models.\nModelScopeT2V [37] is a text-to-video generation model\nwhich brings some temporal structures into the U-Net of\nSD2.1 and finetunes the U-Net on large-scale datasets [32,\n2, 41]. We attempt to insert the learned identity into the un-\nchanged CLIP text encoder without finetuning as shown in\nthe row 3 of Figure 1. The generated video shows promising\nidentity preservation and text alignment.\nLucidDreamer [19] is a text-to-3D generation pipeline\nbased on 3D Gaussian Splatting [17] and allows to sample\ndirectly with the pre-trained SD2.1, like us. Therefore, it can\nnaturally collaborate with our method. In a similar way, we\ninsert the learned identity into this pipeline, as shown in the\nrow 4 of Figure 1. The generated results achieve stable iden-\ntity, high fidelity and geometry consistency. The result of\n\u201cwearing a golden crown\u201d exhibits precise geometric struc-\ntures and realistic colors and the \u201cas oil painting\u201d obtains the\ndesired style, a 3D portrait oil painting that does not exist in\nreality.\nOverall, our method can effortlessly enable prompt-\nconsistent identity-driven video/3D generation with the off-\nthe-shelf text-to-video/text-to-3D models. We show more re-\nsults of video/3D in the supplementary material.\n5.2\nWord-Embedding-Based Methods Analysis\nConsidering that Textual Inversion, Celeb-Basis and our\nmethod are all optimized in the word embedding space, we\nfurther analyze 70 embeddings learned by these methods\nfrom different perspectives. To match the dimension of word\nembeddings, Textual Inversion is conducted with 2-word ver-\nsion and Celeb-Basis is implemented with SD2.1 for analysis.\nTo intuitively show the difference between the distributions\nof learned embeddings and celeb embeddings, we use the t-\nTable 3: Comparison with baselines optimized in the word em-\nbedding space on training time, maximum and minimum values of\nlearned embeddings.\nTraining time\nMax\nMin\nCeleb names\n\u2212\n0.0551\n-0.0558\nTextual Inversion\n43mins\n0.7606\n-0.9043\nCeleb-Basis\n8mins\n0.1592\n-0.1499\nStableIdentity (Ours)\n4mins\n0.0557\n-0.0520\nStableIdentity(Ours)\nTextual Inversion\nCeleb-Basis\nTom Cruise\n\ud835\udc631\n\u2217 \ud835\udc632\n\u2217\nInput Image\nFigure 8: Comparison of 3D generation based on LucidDreamer.\nWe show the result of a celeb name \u201cTom Cruise\u201d (prompt) as a\nstandard and the results with the embeddings [v\u2217\n1, v\u2217\n2] learned from\ncompeting methods (Zoom-in for the best view).\nSNE [35] to visualize word embeddings in Figure 7. \u201cCeleb\nnames\u201d denotes the word embeddings corresponding to the\ncollected 326 celeb names. It can be observed that the dis-\ntribution of ours is more compact with fewer outliers and\ncloser to the real distribution of celeb names, achieving the\nbest identity-consistent editability. Besides, we compare the\nmax & min values of the learned embeddings and training\ntime in Table 3. Our method is faster than existing methods\nof the same kind, and the value range is closest to real celeb\nembeddings.\nFurthermore, to examine the generalization ability of these\nmethods, we present 3D generation results with the learned\nidentity embeddings directly using the mentioned 3D gener-\nation pipeline LucidDreamer in Figure 8. And we show a\nstandard result using a celeb name \u201cTom Cruise\u201d as a prompt.\nObviously, our method achieves celeb-like results in every 3D\nview, which further demonstrates stable and strong general-\nization ability of our learned identity.\n6\nConclusion\nIn this paper, we propose StableIdentity, a customized gen-\neration framework which can inject anybody into anywhere.\nThe model architecture that integrates identity and editability\nprior allows the learned identity to master identity-consistent\nrecontextualization ability.\nBesides, the designed masked\ntwo-phase diffusion loss enables the learned identity more\nstable.\nExtensive quantitative and qualitative experiments\ndemonstrate the superiority of the proposed method. Sur-\nprisingly, our method can directly work with the plug-and-\nplay SD-based modules such as ControlNet, and even can in-\nsert the learned identity into off-the-shelf video/3D generated\nmodels without finetuning to produce outstanding effects. We\nhope that our work can contribute to the unification of cus-\ntomization over image, video, and 3D generation tasks.\nReferences\n[1] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel\nCohen-Or, and Dani Lischinski. Break-a-scene: Extract-\ning multiple concepts from a single image. arXiv preprint\narXiv:2305.16311, 2023. 4, 6\n[2] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 1728\u2013\n1738, 2021. 7\n[3] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part affin-\nity fields. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 7291\u20137299,\n2017. 6\n[4] Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding,\nYangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing\nLiu, Kang Du, et al. Photoverse: Tuning-free image cus-\ntomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2309.05793, 2023. 2\n[5] Xi Chen,\nLianghua Huang,\nYu Liu,\nYujun Shen,\nDeli Zhao, and Hengshuang Zhao.\nAnydoor:\nZero-\nshot object-level image customization.\narXiv preprint\narXiv:2307.09481, 2023. 1\n[6] Zhuowei Chen, Shancheng Fang, Wei Liu, Qian He,\nMengqi Huang, Yongdong Zhang, and Zhendong Mao.\nDreamidentity:\nImproved editability for efficient face-\nidentity preserved image generation.\narXiv preprint\narXiv:2307.00300, 2023. 1, 2, 3\n[7] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\nZafeiriou. Arcface: Additive angular margin loss for deep\nface recognition. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n4690\u20134699, 2019. 3, 4\n[8] Ziyi Dong, Pengxu Wei, and Liang Lin.\nDrea-\nmartist: Towards controllable one-shot text-to-image gen-\neration via contrastive prompt-tuning.\narXiv preprint\narXiv:2211.11337, 2022. 1\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al.\nAn image is worth 16x16 words:\nTransformers for image recognition at scale.\narXiv\npreprint arXiv:2010.11929, 2020. 3\n[10] Vincent Dumoulin, Jonathon Shlens, and Manjunath\nKudlur. A learned representation for artistic style. arXiv\npreprint arXiv:1610.07629, 2016. 4\n[11] Patrick Esser, Robin Rombach, and Bjorn Ommer. Tam-\ning transformers for high-resolution image synthesis. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 12873\u201312883, 2021.\n3\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr. An image is worth one word: Personalizing text-to-\nimage generation using textual inversion. arXiv preprint\narXiv:2208.01618, 2022. 1, 2, 4\n[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aber-\nman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt\nimage editing with cross attention control. arXiv preprint\narXiv:2208.01626, 2022. 2\n[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising\ndiffusion probabilistic models. Advances in Neural Infor-\nmation Processing Systems, 33:6840\u20136851, 2020. 2\n[16] Tero Karras, Samuli Laine, and Timo Aila.\nA style-\nbased generator architecture for generative adversarial net-\nworks. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 4401\u2013\n4410, 2019. 4\n[17] Bernhard\nKerbl,\nGeorgios\nKopanas,\nThomas\nLeimk\u00a8uhler, and George Drettakis. 3d gaussian splatting\nfor real-time radiance field rendering. ACM Transactions\non Graphics, 42(4), 2023. 7\n[18] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi,\nMing-Ming Cheng, and Ying Shan.\nPhotomaker: Cus-\ntomizing realistic human photos via stacked id embedding.\narXiv preprint arXiv:2312.04461, 2023. 1, 4\n[19] Yixun Liang, Xin Yang, Jiantao Lin, Haodong Li, Xi-\naogang Xu, and Yingcong Chen.\nLuciddreamer: To-\nwards high-fidelity text-to-3d generation via interval score\nmatching. arXiv preprint arXiv:2311.11284, 2023. 2, 3, 7,\n10\n[20] Yang Liu, Cheng Yu, Lei Shang, Ziheng Wu, Xingjun\nWang,\nYuze Zhao,\nLin Zhu,\nChen Cheng,\nWeitao\nChen, Chao Xu, et al.\nFacechain: A playground for\nidentity-preserving portrait generation.\narXiv preprint\narXiv:2308.14256, 2023. 1\n[21] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng,\nKai Zhu, Ruili Feng, Yu Liu, Deli Zhao, Jingren Zhou, and\nYang Cao. Cones 2: Customizable image synthesis with\nmultiple subjects. arXiv preprint arXiv:2305.19327, 2023.\n4\n[22] Mario Lucic, Karol Kurach, Marcin Michalski, Sylvain\nGelly, and Olivier Bousquet. Are gans created equal? a\nlarge-scale study. arXiv preprint arXiv:1711.10337, 2017.\n5\n[23] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang,\nZhongang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter:\nLearning adapters to dig out more controllable abil-\nity for text-to-image diffusion models.\narXiv preprint\narXiv:2302.08453, 2023. 4\n[24] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\nand Mark Chen.\nGlide: Towards photorealistic image\ngeneration and editing with text-guided diffusion models.\narXiv preprint arXiv:2112.10741, 2021. 2\n[25] Karl Pearson.\nLiii. on lines and planes of closest fit\nto systems of points in space. The London, Edinburgh,\nand Dublin philosophical magazine and journal of sci-\nence, 2(11):559\u2013572, 1901. 2\n[26] Joern Ploennigs and Markus Berger. Ai art in architec-\nture. AI in Civil Engineering, 2(1):8, 2023. 1\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language su-\npervision. In International conference on machine learn-\ning, pages 8748\u20138763. PMLR, 2021. 3, 4\n[28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen.\nHierarchical text-conditional\nimage generation with clip latents.\narXiv preprint\narXiv:2204.06125, 1(2):3, 2022. 1\n[29] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 10684\u201310695, 2022. 1, 2, 3, 10\n[30] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael\nPritch, Michael Rubinstein, and Kfir Aberman. Dream-\nbooth:\nFine tuning text-to-image diffusion models\nfor subject-driven generation.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 22500\u201322510, 2023. 1, 2, 4\n[31] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei,\nTingbo Hou, Yael Pritch, Neal Wadhwa, Michael Rubin-\nstein, and Kfir Aberman. Hyperdreambooth: Hypernet-\nworks for fast personalization of text-to-image models.\narXiv preprint arXiv:2307.06949, 2023. 1\n[32] Christoph Schuhmann, Richard Vencu, Romain Beau-\nmont, Robert Kaczmarczyk, Clayton Mullis, Aarush\nKatta, Theo Coombes, Jenia Jitsev, and Aran Komat-\nsuzaki. Laion-400m: Open dataset of clip-filtered 400 mil-\nlion image-text pairs. arXiv preprint arXiv:2111.02114,\n2021. 7\n[33] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung.\nInstantbooth: Personalized text-to-image generation with-\nout test-time finetuning. arXiv preprint arXiv:2304.03411,\n2023. 2, 6\n[34] Jiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2, 4\n[35] Laurens Van Der Maaten. Accelerating t-sne using tree-\nbased algorithms.\nThe journal of machine learning re-\nsearch, 15(1):3221\u20133245, 2014. 7\n[36] Mei Wang and Weihong Deng. Deep face recognition:\nA survey. Neurocomputing, 429:215\u2013244, 2021. 1\n[37] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya\nZhang,\nXiang Wang,\nand Shiwei Zhang.\nMod-\nelscope text-to-video technical report.\narXiv preprint\narXiv:2308.06571, 2023. 2, 3, 7, 10, 12\n[38] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai,\nLei Zhang, and Wangmeng Zuo. Elite: Encoding visual\nconcepts into textual embeddings for customized text-to-\nimage generation. arXiv preprint arXiv:2302.13848, 2023.\n2, 4\n[39] Zijie Wu, Chaohui Yu, Zhen Zhu, Fan Wang, and Xiang\nBai. Singleinsert: Inserting new concepts from a single\nimage into text-to-image models for flexible editing. arXiv\npreprint arXiv:2310.08094, 2023. 2, 4, 6\n[40] Guangxuan Xiao, Tianwei Yin, William T Freeman,\nFr\u00b4edo Durand, and Song Han.\nFastcomposer: Tuning-\nfree multi-subject image generation with localized atten-\ntion. arXiv preprint arXiv:2305.10431, 2023. 2, 4, 6\n[41] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A\nlarge video description dataset for bridging video and lan-\nguage. In Proceedings of the IEEE conference on com-\nputer vision and pattern recognition, pages 5288\u20135296,\n2016. 7\n[42] Yuxuan Yan, Chi Zhang, Rui Wang, Yichao Zhou, Gege\nZhang, Pei Cheng, Gang Yu, and Bin Fu.\nFacestudio:\nPut your face everywhere in seconds.\narXiv preprint\narXiv:2312.02663, 2023. 1\n[43] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei\nYang.\nIp-adapter:\nText compatible image prompt\nadapter for text-to-image diffusion models. arXiv preprint\narXiv:2308.06721, 2023. 1, 2, 4\n[44] Changqian Yu, Jingbo Wang, Chao Peng, Changxin\nGao, Gang Yu, and Nong Sang. Bisenet: Bilateral seg-\nmentation network for real-time semantic segmentation.\nIn Proceedings of the European conference on computer\nvision (ECCV), pages 325\u2013341, 2018. 4\n[45] Ge Yuan, Xiaodong Cun, Yong Zhang, Maomao Li,\nChenyang Qi, Xintao Wang, Ying Shan, and Huicheng\nZheng. Inserting anybody in diffusion models via celeb\nbasis. arXiv preprint arXiv:2306.00926, 2023. 1, 2, 3, 4,\n10\n[46] Richard Zhang, Phillip Isola, Alexei A Efros, Eli\nShechtman, and Oliver Wang.\nThe unreasonable effec-\ntiveness of deep features as a perceptual metric. In CVPR,\n2018. 4\n[47] Chenshuang\nZhang,\nChaoning\nZhang,\nMengchun\nZhang, and In So Kweon.\nText-to-image diffusion\nmodel in generative ai:\nA survey.\narXiv preprint\narXiv:2303.07909, 2023. 1\n[48] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.\nAdding conditional control to text-to-image diffusion\nmodels. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 3836\u20133847, 2023.\n2, 3, 10\n[49] Yiqun Zhang, Zhenyue Qin, Yang Liu, and Dylan\nCampbell.\nDetecting and restoring non-standard hands\nin stable diffusion generated images.\narXiv preprint\narXiv:2312.04236, 2023. 12\ncoding in front of a \ncomputer\nwearing a \nSuperman outfit\nwearing a purple \nwizard outfit\nplaying the violin\nwith red hair\nInput Image\nFigure 9: More generated results with the proposed StableIdentity for different identities (including various races) under various contexts (cov-\nering decoration, action, attribute).\nA\nMore Visual Results\nMore Customized Results. As shown in Figure 9, it can be\nobserved that StableIdentity can handle different races in vari-\nous contexts. On the other hand, we also show the customized\nresults with diverse artistic styles in Figure 10. Overall, the\ngenerated results have satisfactory identity preservation and\neditability, which demonstrates the effectiveness of the pro-\nposed method.\nStableIdentity & Image/Video/3D Models.\nIn addition,\nas shown in Figure 12, 13, we show more image/video/3D\ncustomized generation results with ControlNet [48], Mod-\nelScopeT2V [37], and LucidDreamer [19]. As introduced\nin Sec. 4.1, StableIdentity can be considered as introducing\nnew identity for the dictionary of CLIP text encoder. There-\nfore, the learned identity can be naturally inserted into various\ncontexts or even into video/3D generated models for identity-\ndriven customized generation.\nDue to the limited perfor-\nmance of 3D generation, following [19], we only generate\nand edit in the head region, which can clearly demonstrates\nwhether the learned identity is successfully inserted or not.\nImpressive experimental results show that our method can\nbe stably injected into image/video/3D generative models to\nachieve identity-consistent recontextualization. Furthermore,\nwe also show more customized generation results using celeb\nphotos as input as shown in 14.\nMore Comparisons. As shown in Figure 15, we further com-\npare with baselines on decoration, action, background, style.\nObviously, we achieve the best results for identity preser-\nvation, editability and image quality. DreamBooth, which\nseems to perform well (row 1,3,4), either overfits to the input\nimage or fails to produce results similar to the target identity.\nB\nImplementation Details\nFiltering Celeb Names.\nAs mentioned in Sec. 2, Celeb-\nBasis [45] collects 691 celeb names which are editable in\nStable Diffusion [29]. We only filter out names consisting\nof only two words and then count the number of the corre-\nsponding tokens. The percentages of 2 words\u2192{2,3,4,5,6}\na sand sculpture \nof\nin Ghibli animation \nstyle\na pencil sketch \nof\nFauvism painting \nof \nInput Image\na Ukiyo-e painting \nof\nFigure 10: Additional customized results with StableIdentity for diverse artistic styles.\nFigure 11: Parameter analysis for the division parameter \u03b1.\ntokens are 56%, 27%, 13%, 3%, and 0.3% respectively. To\nobtain a more accurate prior space, we choose the setting of\n2 words\u21922 tokens, which has more sampling points.\nDivision Parameter \u03b1. As shown in Figure 11, we present\nthe effect of different \u03b1 in [0, 0.1, \u00b7 \u00b7 \u00b7, 1]. Empirically, \u03b1 \u2208\n[0.4, 0.6] shows better identity preservation, editability and\nimage quality. When \u03b1 is larger, meaningless reconstruction\nwill lead to performance degradation.\nC\nLimitations\nAlthough the proposed method achieves outstanding perfor-\nmance for customization generation of new identities and can\ncollaborate with the off-the-shelf image/video/3D models, it\nflowers on the \nhead of \nin space wearing \na spacesuit\nin front of the \nGreat Wall\ndancing on a \nstage\nplaying basketball\n& ControlNet\n(OpenPose)\nas a knight \nin plate\nInput Image\na Greek \nsculpture of \nplaying chess\nholding a bunch \nof roses\n& ControlNet\n(OpenPose)\ncooking\nwearing \nspiderman suit\n& ModelScopeT2V: driving a car\n& ModelScopeT2V: shaving beard\n& ModelScopeT2V: eating an apple\n& ModelScopeT2V: taking a phone call\nwearing a blur \nhoodie\nInput Image\nFigure 12: Pose-controlled customized image generation (StableIdentity & ControlNet) and zero-shot identity-driven customized video gen-\neration (StableIdentity & ModelScopeT2V).\nstill faces some limitations. (1) Since we only act in the word\nembedding space and fix the Stable Diffusion (SD), we in-\nherit not only the excellent performance of SD, but also some\ndrawbacks, such as hand anomalies [49]. (2) Existing text-\nto-video generation models can generate with diverse con-\ntexts, but is still immature for human-centric generation [37].\nIt leads to limited performance for the video customization\ngeneration.\n& LucidDreamer: wearing glasses\n& LucidDreamer: in comic style\n& LucidDreamer: \ud835\udc631\n\u2217 \ud835\udc632\n\u2217\n& LucidDreamer: made out of toy bricks\n& LucidDreamer: \ud835\udc631\n\u2217 \ud835\udc632\n\u2217\n& LucidDreamer: in Fauvism painting style\nInput Image\nInput Image\nInput Image\ndrinking beer\nwearing golden \narmor\nas a doctor\nplaying the guitar\nwearing a purple \nwizard outfit\nplaying the violin\nwearing a \nChristmas hat\nreading \nnewspapers\nsitting in a park\n& LucidDreamer: wearing a crown\n& LucidDreamer: in Ukiyo-e painting style\nInput Image\nwith curly hair\nwearing a red \nsweater\nwearing a \nSuperman outfit\nFigure 13: Zero-shot identity-driven customized 3D generation (StableIdentity & LucidDreamer). As mentioned in Sec. 4.1, we omit the\nplaceholders v\u2217\n1 v\u2217\n2 of prompts such as \u201cv\u2217\n1 v\u2217\n2 wearing glasses\u201d for brevity. Here, we use \u201cv\u2217\n1 v\u2217\n2 as the input prompt to show the 3D\nreconstruction for the learned identities.\nwearing Ironman suit\n& ControlNet: wearing Captain America costume \nwearing spiderman suit\nlatte art of\n& ControlNet: holding a lollipop\nInput Image\nInput Image\nInput Image\nInput Image\n& ModelScopeT2V: drinking water\n& LucidDreamer: made out of toy bricks\n& LucidDreamer: \ud835\udc631\n\u2217 \ud835\udc632\n\u2217\n& ControlNet: in front of Eiffel Tower\n& ControlNet: cooking in the kitchen\nplaying the guitar\n& ModelScopeT2V: wearing headphones and eyeglasses\n& ModelScopeT2V: wiping face\n& LucidDreamer: wearing a hat\n& ModelScopeT2V: with sunglasses driving\n& LucidDreamer: in comic style\nFigure 14: More image/video/3D customized generation results for celeb photos as input.\nTextual Inversion\nDreamBooth\nELITE\nFastComposer\nIP-Adapter\nCeleb-Basis\nStableIdentity(Ours)\nInput Image\nswimming \nin the pool\nholding a bottle \nof red wine\nwearing a \nspacesuit\nin the snow\nplaying \nthe violin\na Pixar \ncharacter of\nFigure 15: More qualitative comparisons for different identities (including various races) with diverse text prompts (covering decoration,\naction, background, style). Our method shows best performance for identity preservation and editability (Zoom-in for the best view).\n"
  },
  {
    "title": "Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception",
    "link": "https://arxiv.org/pdf/2401.16158.pdf",
    "upvote": "15",
    "text": "MOBILE-AGENT: AUTONOMOUS MULTI-MODAL MOBILE\nDEVICE AGENT WITH VISUAL PERCEPTION\nA PREPRINT\nJunyang Wang1\u2217\nHaiyang Xu2\u2020\nJiabo Ye2\nMing Yan2\u2020\nWeizhou Shen2\nJi Zhang2\nFei Huang2\nJitao Sang1\u2020\n{junyangwang, jtsang}@bjtu.edu.cn, {shuofeng.xhy, ym119608}@alibaba-inc.com\n1Beijing Jiaotong University\n2Alibaba Group\nABSTRACT\nMobile device agent based on Multimodal Large Language Models (MLLM) is becoming a popular\napplication. In this paper, we introduce Mobile-Agent, an autonomous multi-modal mobile device\nagent. Mobile-Agent first leverages visual perception tools to accurately identify and locate both\nthe visual and textual elements within the app\u2019s front-end interface. Based on the perceived vision\ncontext, it then autonomously plans and decomposes the complex operation task, and navigates\nthe mobile Apps through operations step by step. Different from previous solutions that rely on\nXML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across\ndiverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for\nsystem-specific customizations. To assess the performance of Mobile-Agent, we introduced Mobile-\nEval, a benchmark for evaluating mobile device operations. Based on Mobile-Eval, we conducted a\ncomprehensive evaluation of Mobile-Agent. The experimental results indicate that Mobile-Agent\nachieved remarkable accuracy and completion rates. Even with challenging instructions, such as\nmulti-app operations, Mobile-Agent can still complete the requirements. Code and model will be\nopen-sourced at https://github.com/X-PLUG/MobileAgent.\nFigure 1: Mobile-Agent is an autonomous agent for operating the mobile device. Based on user instruction, Mobile-\nAgent can plan a series of operations to complete the requirements.\n\u2217Work done during internship at Alibaba Group.\n\u2020Corresponding author\narXiv:2401.16158v1  [cs.CL]  29 Jan 2024\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\n1\nIntroduction\nLLM-based agents Li et al. [2023], Liu et al. [2023a,b,c], Shen et al. [2023], Wu et al. [2023], Yang et al. [2023a], Shen\net al. [2024], Yang et al. [2023b], Hong et al. [2023], Yang et al. [2023c], utilizing a variety of tools, have demonstrated\nstrong capabilities in task planning and reasoning. As Multimodal Large Language Models (MLLM) Liu et al. [2023d],\nZhu et al. [2023], Ye et al. [2023a], Dai et al. [2023], Liu et al. [2023e], Chen et al. [2023], Ye et al. [2023b], Bai et al.\n[2023], Lin et al. [2023] rapidly progress and exhibit remarkably visual comprehension capabilities, the realization of\nMLLM-based agents has become feasible, also sparking the potential for a variety of innovative applications.\nRecently, mobile device agent has emerged as a novel and popular application of MLLM-based agents. The agent\nneeds to operate the mobile device based on the screen and user instructions. This requires the agent to possess both\nvisual perception and semantic understanding capabilities. However, existing MLLMs, including the state-of-the-art\nGPT-4V, still lack sufficient visual perception abilities to serve as an effective agent. Zheng et al. [2024] points out that\nalthough GPT-4V can generate effective operations, it struggles to accurately locate the positions of these operations on\nthe screen. This limitation hinders the ability to operations on mobile device solely through advanced MLLMs.\nTo address this issue, existing works have attempted to assist GPT-4V in localization by leveraging user interface\nlayout files. Yang et al. [2023d] extracted actionable positions on the screen by accessing Android application XML\nfiles. Zheng et al. [2024] used HTML code from web applications to aid in localization. These methods rely on the\naccessibility of underlying files. However, in many scenarios, permissions to access these files may not be available,\nrendering these methods ineffective.\nIn order to eliminate the dependency on the underlying files in existing localization methods, in this work, we propose\nMobile-Agent, an autonomous mobile device agent with visual perception. Mobile-Agent, through the visual perception\nmodule, can accurately locate operations using only screenshots from the mobile device. The visual perception module\nconsists of detection and OCR models, responsible for describing the content of localized screen regions and identifying\ntext within the screen, respectively. Through carefully crafted prompts, we facilitate effective interaction between the\nagent and tools, enabling the automation of mobile device operations. Leveraging the robust contextual capabilities\nof GPT-4V, Mobile-Agent achieves a self-planning capability to plan tasks holistically based on the screenshot, user\ninstruction, and operation history. To enhance the agent\u2019s ability to identify erroneous operations and incomplete\ninstructions, we introduce a self-reflection method. Guided by prompts, the agent continually reflects on invalid and\nincorrect operations, and the agent can halt once the instruction is completed. In order to comprehensively assess\nMobile-Agent\u2019s capabilities, we have introduced Mobile-Eval, a benchmark centered around current mainstream mobile\nApps. Mobile-Eval includes instructions for various difficulty levels. We have conducted an analysis of Mobile-Agent\nbased on Mobile-Eval, showcasing and analyzing some of the cases within it. The experimental results indicate that\nMobile-Agent exhibits remarkable instruction completion rates and operation accuracy. Even in challenging instructions,\nsuch as operating multiple Apps, Mobile-Agent is able to successfully complete the tasks.\nThe contributions summarized are as follows:\n\u2022 We propose Mobile-Agent, an autonomous mobile device agent. Mobile-Agent utilizes visual perception tools\nfor operation localization. It can self-plan each step and complete self-reflection. Mobile-Agent solely relies\non device screenshots without any system code, which is a purely vision-based solution.\n\u2022 We introduce Mobile-Eval, a benchmark designed to assess mobile device agents. This benchmark comprises\n10 commonly used Apps and features instructions with varying three difficulty levels.\n\u2022 We conducted a comprehensive analysis of Mobile-Agent based on Mobile-Eval. We presented typical selected\ncases to analyze the capabilities of it.\n2\nMobile-Agent\nThis section introduces our Mobile-Agent framework. The framework consists of state-of-the-art MLLM GPT-4V, a\ntext detection module for text localization, and an icon detection module for icon localization. We will first explain\nhow to use visual tools to position the instructions generated by GPT-4V to specific locations on the mobile device.\nSubsequently, we will describe the workflow of the Mobile-Agent.\n2.1\nVisual Perception\nGPT-4V Lacks Localization Capability. While GPT-4V can provide correct operations for instruction and screenshot,\nexisting work Zheng et al. [2024] indicates that GPT-4V is unable to effectively output the location where the operations\n2\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nFigure 2: The framework of Mobile-Agent.\ntake place. Therefore, we need external tools to assist GPT-4V in operation localization, allowing the operations to be\noutput onto the mobile device screen.\nText Localization. When the agent needs to tap on specific text on the screen, we use an OCR tool to detect the position\nof the corresponding text on the screen. We will discuss three scenarios:\n\u2022 When the OCR detection results do not include the specified text, the agent will be instructed to either reselect\nthe text for tapping or choose an alternative operation. This situation often occurs in complex scenarios where\nGPT-4V may have a small number of hallucinations.\n\u2022 When the OCR detection results only have one instance of the specified text, we directly generate an operation\nto click on the center coordinates of that text box.\n\u2022 When the OCR detection results include multiple instances of the specified text, we assess the number of\nresults. If there are many instances, it indicates that there is too much similar content on the current screen,\nmaking it challenging for the agent to make a selection. In such cases, the agent is requested to reselect the\ntext for tapping. If there are few instances, we crop these regions and draw detection boxes on them. Then,\nwe use these regions to let the agent choose which one to click. When cropping, we extend the text detection\nboxes outward by a certain range and then draw the detection boxes on these cropped images. This is done to\npreserve more information and facilitate the agent\u2019s decision-making process. This process is shown in the\ntop-left of Figure 2.\n3\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nIcon Localization. When the agent needs to click an icon, we use an icon detection tool and CLIP Radford et al.\n[2021] to locate the position of it. Specifically, we first request the agent to provide the attributes of the icon to click,\nincluding color and shape. Subsequently, we use Grounding DINO Liu et al. [2023f] with the prompt \u201cicon\u201d to identify\nall the icons on the screenshot. Finally, employing CLIP, we calculate the similarity between all detected icons and the\ndescription of the click region, selecting the region with the highest similarity for a click. This process is shown in the\ntop-right of Figure 2.\n2.2\nInstruction Execution\nOperation. In order to better translate the actions output by the agent into operations on the screen, we define 8\noperations for the Mobile-Agent:\n\u2022 Open App (App): Open a specific App on the desktop page.\n\u2022 Click the text (Text): Click the area of the screen where the text \u201cText\u201d is located.\n\u2022 Click the icon (Icon, Position): Click the area described by \u201cIcon\u201d in the \u201cPosition\u201d. \u201cIcon\u201d provides a\ndescription, including attributes such as color, icon shape, etc., of the tapping location. \u201cPosition\u201d needs to be\nselected from top, bottom, left, right, or center, with one or two options, to minimize the possibility of errors.\n\u2022 Type (Text): Type the \"Text\" into the current input box.\n\u2022 Page up & down: Used for scrolling up and down the current page.\n\u2022 Back: Return to the last page.\n\u2022 Exit: Return directly to the desktop from the current page.\n\u2022 Stop: When the instruction is completed, end the entire process.\nSelf-Planning. The Mobile-Agent completes each step of the operation iteratively. Before the iteration begins, the user\nneeds to input an instruction. We generate the system prompt for the entire process based on the instruction. At the\nstart of each iteration, we capture a screenshot of the current mobile screen and provide it to the agent. The agent, by\nobserving the system prompt, operation history, and the current screen capture, outputs the next step of the operation. If\nthe agent\u2019s output is to end the process, the iteration stops; otherwise, a new iteration continues. Mobile-Agent, utilizing\nthe operation history, is aware of the current task progress and, based on the system prompt, generates operation on the\ncurrent screenshot, thereby achieving an iterative self-planning process. This process is shown at the bottom of Figure 2.\nSelf-Reflection. During the iteration, the agent may encounter errors, leading to the inability to complete the instruction.\nTo improve the success rate of instruction, we have introduced a self-reflection method. This method will take effect in\ntwo situations. The first is when the agent generates an incorrect or invalid operation, causing the process to be stuck.\nWhen the agent notices that the screenshot has not changed after a particular operation, or the screenshot shows a wrong\npage, we will instruct the agent to try alternative operations or modify the parameters of the current operation. The\nsecond is when the agent may overlook certain requirements of complex instruction. After the agent completes all\noperations through self-planning, we will instruct the agent to analyze the operations, history, the current screenshot,\nand user instruction to determine if the instruction have been completed. If not, the agent needs to continue generating\noperations through self-planning. This process is shown at the bottom of Figure 2.\nPrompt Format. To better implement the functionalities described above, we drew inspiration from the prompt format\nused by ReAct. We require the agent to output three components: Observation, Thought, and Action. Observation is a\ndescription by the agent of the current screenshot and the history of operations. This helps the agent to notice updates in\nthe screenshot and promptly identify errors based on historical records. Thought represents the agent\u2019s consideration of\nthe next step of operation generated from the Observation and the instruction. The agent needs to describe the upcoming\noperation in the Thought. Action requires the agent to choose one of eight operations and parameters based on Thought.\n3\nExperiments\nIn this section, we will conduct a comprehensive evaluation of the Mobile-Agent. We use the Android operating\nsystem due to its convenient operation invocation interfaces. We will explore other operating systems in future work.\nOur experiments are primarily divided into two parts: quantitative experiments and qualitative experiments. In the\nquantitative experiments, we will evaluate the Mobile-Agent on our proposed Mobile-Eval benchmark. In the qualitative\nexperiments, we will analyze specific cases.\n4\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nApplication\nInstruction\nAlibaba.com\nHelp me find caps in Alibaba.com.\nHelp me find caps in Alibaba.com. If the \"Add to cart\" is avaliable in the item information page, please add the item to my cart.\nI want to buy a cap. I\u2019ve heard things are cheap on Alibaba.com. Maybe you can find it for me.\nAmazon Music\nSearch singer Jay Chou in Amazon Music.\nSearch a music about \"agent\" in Amazon Music and play it.\nI want to listen music to relax. Find an App to help me.\nChrome\nSearch result for today\u2019s Lakers game.\nSearch the information about Taylor Swift.\nI want to know the result for today\u2019s Lakers game. Find an App to help me.\nGmail\nSend an empty email to to {address}.\nSend an email to {address} to tell my new work.\nI want to let my friend know my new work, and his address is {address}. Find an App to help me.\nGoogle Maps\nNavigate to Hangzhou West Lake.\nNavigate to a nearby gas station.\nI want to go to Hangzhou West Lake, but I don\u2019t know the way. Find an App to help me.\nGoogle Play\nDownload WhatsApp in Play Store.\nDownload Instagram in Play Store.\nI want WhatsApp on my phone. Find an App to help me.\nNotes\nCreate a new note in Notes.\nCreate a new note in Notes and write \"Hello, this is a note\", then save it.\nI suddenly have something to record, so help me find an App and write down the following content: meeting at 3pm.\nSettings\nTurn on the dark mode.\nTurn on the airplane mode.\nI want to see the real time internet speed at the battery level, please turn on this setting for me.\nTikTok\nSwipe a video about pet cat in TikTok and click a \"like\" for this video.\nSwipe a video about pet cat in TikTok and comment \"Ohhhh, so cute cat!\".\nSwipe videos in TikTok. Click \"like\" for 3 pet video cat.\nYouTube\nSearch for videos about Stephen Curry on YouTube.\nSearch for videos about Stephen Curry on YouTube and open \"Comments\" to comment \"Oh, chef, your basketball spirit has\nalways inspired me\".\nI need you to help me show my love for Stephen Curry on YouTube.\nMulti-App\nOpen the calendar and look at today\u2019s date, then go to Notes and create a new note to write \"Today is [today\u2019s data]\".\nCheck the temperature in the next 5 days, and then create a new note in Notes and write a temperature analysis.\nSearch the result for today\u2019s Lakers game, and then create a note in Notes to write a sport news for this result.\nTable 1: The applications and instructions used in Mobile-Eval.\n3.1\nSetup\nMobile-Eval. To comprehensively evaluate the capabilities of Mobile-Agent, we introduce Mobile-Eval, a benchmark\nbased on current mainstream Apps. Mobile-Eval consists of a total of 10 commonly used Apps on mobile devices.\nTo assess the multi-application usage capability of the agent, we have also introduced instructions that require the\nsimultaneous use of two Apps. We designed three instructions for each App. The first instruction is relatively simple,\nrequiring only the completion of basic App operations. The second instruction adds some additional requirements to\nthe first one, making it more challenging. The third instruction involves abstract user instruction, where the user does\nnot explicitly specify which App to use or what operation to perform, leaving the agent to make its own judgment. In\nTable 1, we present the Apps and instructions used in Mobile-Eval.\nMetrics. We have designed four metrics to assess the performance of the Mobile-Agent from different perspectives:\n\u2022 Success (Su): If the Mobile-Agent completes the instruction, it is considered successful.\n\u2022 Process Score (PS): This metric measures the accuracy of each step in the execution of instructions. Specifically,\nit equals the number of correct steps divided by the total number of steps. Although the agent may not ultimately\nsucceed in some instructions, each correct step contributes to the Planning Score.\n\u2022 Relative Efficiency (RE). We manually performed each instruction and recorded the number of steps taken by\na human. We consider human operation as the optimal solution. We will compare the number of steps taken\nby Mobile-Agent with the steps taken by humans to demonstrate whether Mobile-Agent can use the mobile\ndevice more efficiently.\n5\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nApp\nINSTRUCTION 1\nINSTRUCTION 2\nINSTRUCTION 3\nSU\nPS\nRE\nCR\nSU\nPS\nRE\nCR\nSU\nPS\nRE\nCR\nAlibaba.com\n!\n0.75\n4 / 3\n100%\n\u2717\n0.39\n13 / 8\n62.5%\n!\n0.9\n10 / 9\n100%\nAmazon Music\n\u2717\n0.44\n9 / 5\n80.0%\n!\n0.75\n8 / 6\n100%\n\u2717\n0.50\n12 / 3\n66.7%\nChrome\n!\n1.00\n4 / 4\n100%\n!\n0.8\n5 / 4\n100%\n!\n0.43\n8 / 5\n100%\nGmail\n!\n1.00\n4 / 4\n100%\n\u2717\n0.56\n9 / 8\n37.5%\n\u2717\n0.56\n9 / 8\n37.5%\nGoogle Maps\n!\n1.00\n5 / 5\n100%\n!\n1.00\n6 / 6\n100%\n!\n1.00\n6 / 6\n100%\nGoogle Play\n!\n1.00\n3 / 3\n100%\n!\n0.50\n10 / 4\n100%\n!\n1.00\n3 / 3\n100%\nNotes\n!\n0.57\n7 / 4\n100%\n!\n0.67\n6 / 4\n100%\n!\n1.00\n5 / 5\n100%\nSettings\n!\n1.00\n4 / 4\n100%\n!\n1.00\n4 / 4\n100%\n!\n1.00\n5 / 5\n100%\nTikTok\n!\n1.00\n4 / 4\n100%\n!\n1.00\n10 / 10\n100%\n!\n1.00\n7 / 7\n100%\nYouTube\n!\n1.00\n4 / 4\n100%\n!\n1.00\n9 / 9\n100%\n!\n1.00\n7 / 7\n100%\nMulti-App\n!\n1.00\n6 / 6\n100%\n!\n1.00\n6 / 6\n100%\n!\n1.00\n10 / 10\n100%\nAvg.\n0.91\n0.89\n4.9 / 4.2\n98.2%\n0.82\n0.77\n7.9 / 6.3\n90.9%\n0.82\n0.84\n7.5 / 6.2\n91.3%\nTable 2: The overall evaluation results of Mobile-Agent on Mobile-Eval, where the two values of RE represent the\nnumber of steps taken by Mobile-Agent and human, respectively.\n\u2022 Completion Rate (CR). We calculate the number of human-operated steps that Mobile-Agent is able to\ncomplete, divided by the total number of steps taken by a human, to demonstrate the completion rate of\nMobile-Agent for a given instruction. If the instruction is completed, this metric will be equal to 1.\n3.2\nQuantitative Results\nWe present the experimental results in Table 2. Firstly, across the three instructions, Mobile-Agent achieved completion\nrates of 91%, 82%, and 82% respectively. Despite some instructions not being successfully executed, the completion\nrates for all three types of instructions exceeded 90%. Next, we can observe from the PS metric that Mobile-Agent has\na high probability of producing correct operations across the three instructions, achieving around 80%. Finally, the RE\nmetric indicates that Mobile-Agent can achieve an 80% capability of reaching human-optimal operations. The above\nresults collectively indicate the effectiveness of Mobile-Agent as a mobile device assistant.\nIt is worth noting that the PS values for some instructions don\u2019t reach 1, indicating that Mobile-Agent may make some\ninvalid or incorrect operations. However, in these cases, most instructions were ultimately completed. This suggests\nthat Mobile-Agent possesses good self-reflective capabilities. Even in the presence of invalid or incorrect operations, it\ncan reflect based on the screenshots and ultimately correct its mistakes. This is crucial for mobile device agents because,\nlike humans, they cannot guarantee that all operations are correct, and agents must have the ability to correct errors.\n3.3\nCase Study\nIn Figure 3, we demonstrate Mobile-Agent\u2019s ability to understand user instructions and autonomously plan its operations.\nWhile the instructions may not contain specific operations, Mobile-Agent successfully comprehended the user\u2019s\nrequirements and translated them into concrete executable operations. Subsequently, the agent carried out the instructions\nthrough a series of planning steps.\nIn Figure 4, we showcase Mobile-Agent\u2019s ability to reflect when faced with invalid or erroneous instructions. In\nthis case, Mobile-Agent initially used an invalid operation, resulting in no change in the screenshot. After reflection,\nMobile-Agent corrected the error, continued with the operation, and ultimately completed the instruction. In Figure 5,\nwe show another case. Faced with two consecutive invalid or incorrect operations, Mobile-Agent was able to promptly\ncorrect the operations to ensure the smooth execution of the entire process.\nIn Figure 6, 7, we showcased Mobile-Agent\u2019s capability in scenarios involving the operations across multiple apps.\nThis requires the agent to possess a certain level of memory capacity to facilitate information transfer between the two\nApps. As evident from the case, Mobile-Agent accurately conveys information from the first opened App to the second\none and can generate reprocessed content.\nIn Figure 8, we demonstrate the multilingual capability of Mobile-Agent. Although GPT-4V may currently have\nlimitations in handling Chinese, its powerful visual perception allows it to handle simple Chinese scenarios effectively.\n6\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nFigure 3: Case of instruction comprehension and execution planning.\nFigure 4: Case of self-reflection and error correction after using invalid operations.\nIn Figure 9, we showcase the ability of Mobile-Agent to play poker games. After describing the rules of the game,\nMobile-Agent can execute operations according to the specified rules.\nIn Figure 10, 11, 12, 13, 14, we demonstrate the powerful capabilities of Mobile-Agent on Mobile-Eval. Despite\nvariations in the user interfaces and operations of these Apps, and the presence of challenging instructions, Mobile-Agent\nsuccessfully completes the given instructions.\n7\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nFigure 5: Case of self-reflection and error correction after using invalid and incorrect operations, where the operation\n\u201cclick text (Add comment)\u201d leads to an incorrect page and the operation \u201cclick text (Post)\u201d is an invalid operation. The\ninvalid and incorrect operations are highlighted in red font.\nFigure 6: Case of operating multiple Apps to search game result.\n4\nRelated Work\n4.1\nLLM-based Agent\nWith the rapid advancement of Large Language Models (LLMs), agents built upon these models have notched up\nimpressive achievements across a burgeoning spectrum of tasks Li et al. [2023], Liu et al. [2023a,b,c], Shen et al. [2023],\nWu et al. [2023], Yang et al. [2023a], Shen et al. [2024], Yang et al. [2023b], Hong et al. [2023], Yang et al. [2023c].\nFunctioning as the core, these agents adeptly interpret user instructions and deploy a versatile array of tools to execute\n8\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nFigure 7: Case of operating multiple Apps to write a temperature analysis.\nFigure 8: Case of operating Chinese system and App.\nintricate tasks. The expansive integration of diverse tools liberates LLMs from the confines of pure text processing.\nCurrently, LLM-based agents are flourishing in diverse domains, showcasing prowess in tasks such as image and\nvideo editing, image generation, visual question answering, intelligent predictions, and more. This underscores the\ntransformative impact of LLMs on the landscape of AI applications.\n4.2\nAgent for Mobile Device\nThe application of agents to operate terminal devices is becoming a hotspot. AppAgent Yang et al. [2023d] is a mobile\nApp assistant based on GPT-4V. They label manipulable regions of the app\u2019s UI with semi-transparent tags by invoking\nXML files from the Android system. The agent acquires operational capabilities through three methods: self-exploration,\n9\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nFigure 9: Case of playing games.\nFigure 10: Case of wholesale caps from Alibaba.com.\nobserving user video demos, and utilizing user documents. After a certain degree of exploration, the agent gains a\nsufficient understanding of operable regions, allowing it to execute correct operations based on instructions.\n5\nConclusion\nIn this work, we introduce Mobile-Agent, an autonomous multi-modal agent, proficient in operating a broad spectrum\nof mobile applications through a unified visual perception framework. Mobile-Agent employs visual perception\ntools to precisely identify and locate visual and textual elements within the app\u2019s interface. Utilizing the perceived\nvisual context, it autonomously plans, decomposes complex tasks, and navigates through mobile apps step by step.\nUnlike previous solutions relying on XML mobile system metadata, Mobile-Agent offers enhanced adaptability\nacross diverse mobile operating environments in a vision-centric manner, obviating the need for system-specific\ncustomizations. Through experiments, we demonstrate the effectiveness and efficiency of Mobile-Agent across various\ndimensions. This showcases its potential as a versatile and adaptable solution for interacting with mobile applications\nin a language-agnostic manner.\n10\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nFigure 11: Case of searching video from YouTube and commenting this video.\nFigure 12: Case of downloading specific App in Google Play.\nFigure 13: Case of using a map App for navigation.\n11\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nFigure 14: Case of using Amazon Music to search and play a music with specific content.\nReferences\nChenliang Li, Hehong Chen, Ming Yan, Weizhou Shen, Haiyang Xu, Zhikai Wu, Zhicheng Zhang, Wenmeng Zhou,\nYingda Chen, Chen Cheng, et al. Modelscope-agent: Building your customizable agent system with open-source\nlarge language models. arXiv preprint arXiv:2309.00986, 2023.\nZhaoyang Liu, Zeqiang Lai, Zhangwei Gao, Erfei Cui, Zhiheng Li, Xizhou Zhu, Lewei Lu, Qifeng Chen, Yu Qiao,\nJifeng Dai, et al. Controlllm: Augment language models with tools by searching on graphs. arXiv preprint\narXiv:2310.17796, 2023a.\nZhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qinglong Zhang, Zeqiang Lai, Yang\nYang, Qingyun Li, et al. Interngpt: Solving vision-centric tasks by interacting with chatgpt beyond language. arXiv\npreprint arXiv:2305.05662, 3, 2023b.\nShilong Liu, Hao Cheng, Haotian Liu, Hao Zhang, Feng Li, Tianhe Ren, Xueyan Zou, Jianwei Yang, Hang Su, Jun Zhu,\net al. Llava-plus: Learning to use tools for creating multimodal agents. arXiv preprint arXiv:2311.05437, 2023c.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. Hugginggpt: Solving ai tasks\nwith chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580, 2023.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Visual chatgpt: Talking,\ndrawing and editing with visual foundation models. arXiv preprint arXiv:2303.04671, 2023.\nRui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools: Teaching large language\nmodel to use tools via self-instruction. arXiv preprint arXiv:2305.18752, 2023a.\nWeizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang.\nSmall llms are weak tool learners: A multi-llm agent. arXiv preprint arXiv:2401.07324, 2024.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael\nZeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal reasoning and action. arXiv preprint\narXiv:2303.11381, 2023b.\nSirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing\nYau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv\npreprint arXiv:2308.00352, 2023.\nHui Yang, Sifu Yue, and Yunzhong He. Auto-gpt for online decision making: Benchmarks and additional opinions.\narXiv preprint arXiv:2306.02224, 2023c.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023d.\n12\nMobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception\nA PREPRINT\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language\nunderstanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al. mplug-owl: Modularization empowers large language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023a.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale\nFung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv\npreprint arXiv:2305.06500, 2023.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. arXiv\npreprint arXiv:2310.03744, 2023e.\nJun Chen, Deyao Zhu Xiaoqian Shen Xiang Li, Zechun Liu Pengchuan Zhang, Raghuraman Krishnamoorthi Vikas\nChandra Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: Large language model as a unified interface for\nvision-language multi-task learning. arXiv preprint arXiv:2310.09478, 2023.\nQinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Haowei Liu, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou.\nmplug-owl2: Revolutionizing multi-modal large language model with modality collaboration. arXiv preprint\narXiv:2311.04257, 2023b.\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren\nZhou. Qwen-vl: A frontier large vision-language model with versatile abilities. arXiv preprint arXiv:2308.12966,\n2023.\nJi Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi,\nand Song Han. Vila: On pre-training for visual language models. arXiv preprint arXiv:2312.07533, 2023.\nBoyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, and Yu Su. Gpt-4v (ision) is a generalist web agent, if grounded.\narXiv preprint arXiv:2401.01614, 2024.\nZhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. Appagent: Multimodal agents\nas smartphone users. arXiv preprint arXiv:2312.13771, 2023d.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In\nInternational conference on machine learning, pages 8748\u20138763. PMLR, 2021.\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun\nZhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint\narXiv:2303.05499, 2023f.\n13\n"
  },
  {
    "title": "SERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning",
    "link": "https://arxiv.org/pdf/2401.16013.pdf",
    "upvote": "14",
    "text": "January 2024\nSERL: A Software Suite for Sample-Efficient\nRobotic Reinforcement Learning\nJianlan Luo1*, Zheyuan Hu1*, Charles Xu1, You Liang Tan1, Jacob Berg2, Archit Sharma3, Stefan Schaal4, Chelsea Finn3,\nAbhishek Gupta2 and Sergey Levine1\n*Equal Contribution, 1Department of EECS, University of California, Berkeley, 2Department of Computer Science and Engineering, University of\nWashington, 3Department of Computer Science, Stanford University, 4Intrinsic Innovation LLC\nFigure 1: Depiction of various tasks solved using SERL in the real world. These include PCB board insertion (left), cable routing\n(middle), and object relocation (right). SERL provides an out-of-the-box package for real-world reinforcement learning, with support for\nsample-efficient learning, learned rewards, and automation of resets.\nIn recent years, significant progress has been made in the field of robotic reinforcement learning (RL), enabling\nmethods that handle complex image observations, train in the real world, and incorporate auxiliary data, such\nas demonstrations and prior experience. However, despite these advances, robotic RL remains hard to use. It is\nacknowledged among practitioners that the particular implementation details of these algorithms are often just\nas important (if not more so) for performance as the choice of algorithm. We posit that a significant challenge\nto widespread adoption of robotic RL, as well as further development of robotic RL methods, is the comparative\ninaccessibility of such methods. To address this challenge, we developed a carefully implemented library\ncontaining a sample efficient off-policy deep RL method, together with methods for computing rewards and\nresetting the environment, a high-quality controller for a widely-adopted robot, and a number of challenging\nexample tasks. We provide this library as a resource for the community, describe its design choices, and present\nexperimental results. Perhaps surprisingly, we find that our implementation can achieve very efficient learning,\nacquiring policies for PCB board assembly, cable routing, and object relocation between 25 to 50 minutes of\ntraining per policy on average, improving over state-of-the-art results reported for similar tasks in the literature.\nThese policies achieve perfect or near-perfect success rates, extreme robustness even under perturbations, and\nexhibit emergent recovery and correction behaviors. We hope that these promising results and our high-quality\nopen-source implementation will provide a tool for the robotics community to facilitate further developments\nin robotic RL. Our code, documentation, and videos can be found at https://serl-robot.github.io/\n1. Introduction\nConsiderable progress on robotic reinforcement learn-\ning (RL) over the recent years has produced impressive\nresults, with robots playing table tennis (B\u00fcchler et al.,\n2022), manipulating objects from raw images (Gupta\net al., 2021; Kalashnikov et al., 2021; Levine et al., 2016b),\ngrasping diverse objects (Levine et al., 2018; Mahler\net al., 2017), and performing a wide range of other\nskills. However, despite the significant progress on the\nunderlying algorithms, RL remains challenging to use\nfor real-world robotic learning problems, and practical\nadoption has been more limited. We argue that part\nof the reason for this is that the implementation of\nRL algorithms, particularly for real-world robotic sys-\ntems, presents a very large design space, and it is the\nCorresponding author(s): Jianlan Luo and Zheyuan Hu, Berkeley AI Research Lab (BAIR), University of California, Berkeley: jianlanluo@eecs.berkeley.edu,\nhuzheyuan@berkeley.edu.\narXiv:2401.16013v3  [cs.RO]  13 Feb 2024\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nchallenge of navigating this design space, rather than\nlimitations of algorithms per se, that limit adoption. It\nis often acknowledged by practitioners in the field that\ndetails in the implementation of an RL algorithm might\nbe as important (if not more important) as the particular\nchoice of algorithm. Furthermore, real-world learning\npresents additional challenges with reward specifica-\ntion, implementation of environment resets, sample\nefficiency, compliant and safe control, and other diffi-\nculties that put even more stress on this issue. Thus,\nadoption and further research progress on real-world\nrobotic RL may well be bottlenecked on implementation\nrather than novel algorithmic innovations.\nTo address this challenge, our aim in this paper is to\nprovide an open-source software framework, which we\ncall Sample-Efficient Robotic reinforcement Learning\n(SERL), that aims to facilitate wider adoption of RL in\nreal-world robotics. SERL consists of the following\ncomponents: (1) a high-quality RL implementation that\nis geared towards real-world robotic learning and sup-\nports image observations and demonstrations; (2) im-\nplementations of several reward specification methods\nthat are compatible with image observations, includ-\ning classifiers and adversarial training; (3) support for\nlearning \u201cforward-backward\u201d controllers that can auto-\nmatically reset the task between trials (Eysenbach et al.,\n2018); (4) a software package that can in principle con-\nnect the aforementioned RL component to any robotic\nmanipulator; and (5) an impedance controller design\nprinciple that is particularly effective for dealing with\ncontact-rich manipulation tasks. Our aim in this paper\nis not to propose novel algorithms or methodology, but\nrather to offer a resource for the community to provide\nroboticists with a well-designed foundation both for\nfuture research on robotic RL, and other methods that\nmight employ robotic RL as a subroutine. However,\nin the process of evaluating our framework, we also\nmake a scientifically interesting empirical observation:\nwhen implemented properly in a carefully engineered\nsoftware package, current sample-efficient robotic RL\nmethods can attain very high success rates with rela-\ntively modest training times. The tasks in our evalu-\nation are illustrated in Fig. 1: precise insertion tasks\ninvolving dynamic contact, deformable object manip-\nulation with complex dynamics, and object relocation\nwhere the robot must learn without manually designed\nresets. For each of these tasks, SERL is able to learn\neffectively within 15 - 60 min of training per policy (in\nterms of total wall-clock time), achieving near-perfect\nsuccess rates, despite learning policies that operate\non image observations. This result is significant be-\ncause RL, particularly with deep networks and image\ninputs, is often considered to be highly inefficient. Our\nresults challenge this assumption, suggesting careful\nimplementations of existing techniques, combined with\nwell-designed controllers and carefully selected compo-\nnents for reward specification and resets, can provide\nan overall system that is efficient enough for real-world\nuse.\n2. Related Work\nWhile our framework combines existing RL methods\ninto a complete robotic learning system, the particular\ncombination of parts is carefully designed to provide\nfor efficient and out-of-the-box reinforcement learning\ndirectly in the real world and, as shown in our exper-\niments, achieves excellent results on a wide range of\ntasks. Here, we summarize both related prior methods\nand systems.\nAlgorithms for real-world RL: Real-world robotic\nRL demands algorithms that are sample-efficient, can\nutilize onboard perception, and support easily speci-\nfied rewards and resets. A number of algorithms have\nshown the ability to learn very efficiently directly in\nthe real world (Riedmiller et al., 2009; Westenbroek\net al., 2022; Yang et al., 2020; Zhan et al., 2021; Hou\net al., 2020; Tebbe et al., 2021; Popov et al., 2017; Luo\net al., 2019; Zhao et al., 2022; Hu et al., 2024; Johannink\net al., 2018; Schoettler et al., 2020), using variants of\noff-policy RL (Kostrikov et al., 2023; Hu et al., 2024;\nLuo et al., 2023), model-based RL (Hester and Stone,\n2013; Wu et al., 2022; Nagabandi et al., 2019; Rafailov\net al., 2021; Luo et al., 2018), and on-policy RL (Zhu\net al., 2019). These advances have been paired with\nadvances in inferring rewards from raw visual observa-\ntion through success classifiers (Fu et al., 2018; Li et al.,\n2021), foundation-model-based rewards (Du et al., 2023;\nMahmoudieh et al., 2022; Fan et al., 2022), and rewards\nfrom videos (Ma et al., 2023b;a). Additionally, to enable\nautonomous training, there have been a number of al-\ngorithmic advances in reset-free learning (Gupta et al.,\n2021; Sharma et al., 2021; Zhu et al., 2020; Xie et al., 2022;\nSharma et al., 2023) that enable autonomous training\nwith minimal human interventions. While these algo-\nrithmic advances are important, the contribution we\nmake in this work is to provide a framework and soft-\nware package to enable sample efficient reinforcement\nlearning in the real world with a ready-made choice\nof methods that can work well for a variety of tasks.\nIn doing so, we hope to lower the barrier of entry for\n2\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nnew researchers to build better algorithms and training\nmethodologies for robot learning in the real world.\nSoftware packages for RL: There are a number of\npackages (Seno and Imai, 2022; Nair and Pong; Hill et al.,\n2018; Guadarrama et al., 2018) for RL, though to our\nknowledge, none aim to directly address real-world\nrobotic RL specifically. SERL builds on the recently\nproposed RLPD algorithm, which is an off-policy RL\nalgorithm with a high update-to-data ratio. SERL is not\na library of RL algorithms for training agents in simula-\ntion, although it could be adapted to be so. Rather, SERL\noffers a full stack pipeline for robot control, going from\nlow-level controllers to the interface for asynchronous\nand efficient training with an RL algorithm to additional\nmachinery for inferring rewards and training without\nresets. In doing so, SERL provides an off-the-shelf pack-\nage to help non-experts start using RL to train their\nphysical robots in the real world, unlike prior libraries\nthat aim to provide implementations of many meth-\nods \u2013 that is, SERL offers a full \u201cvertical\u201d integration of\ncomponents, whereas prior libraries focus on the \u201chor-\nizontal.\u201d SERL is also not an RL benchmark package\nsuch as (Yu et al., 2019; James et al., 2020; Mittal et al.,\n2023). SERL allows users to define their own tasks and\nsuccess metrics directly in the real world, providing\nthe software infrastructure for actually controlling and\ntraining robotic manipulators in these tasks.\nSoftware for real-world RL: There have been several\nprevious packages that have proposed infrastructure\nfor real world RL: for dexterous manipulation (Ahn\net al., 2019), tabletop furniture assembly (Heo et al.,\n2023), legged locomotion (Kostrikov et al., 2023), and\npeg insertion (Levine et al., 2016a). These packages are\neffective in narrow situations, either using privileged\ninformation or training setups such as explicit tracking\n(Levine et al., 2016a; Ahn et al., 2019) or pure proprio-\nception (Kostrikov et al., 2023), or limited to imitation\nlearning. In SERL, we show a full stack system that can\nbe used for a wide variety of robotic manipulation tasks\nwithout requiring privileging of the training setups as\nin prior work.\n3. Preliminaries and Problem Statement\nRobotic reinforcement learning tasks can be defined via\nan MDP \ue239 = {\ue23f, \ue22d, \ud835\udf0c, \ue23c, \ud835\udc5f, \ud835\udefe}, where \ud835\udc2c \u2208 \ue23f is the state\nobservation (e.g., an image in combination with the cur-\nrent end-effector position), \ud835\udc1a \u2208 \ue22d is the action (e.g., the\ndesired end-effector pose), \ud835\udf0c(\ud835\udc2c0) is a distribution over\ninitial states, \ue23c is the unknown and potentially stochas-\ntic transition probabilities that depend on the system\ndynamics, and \ud835\udc5f \u2236 \ue23f \u00d7 \ue22d \u2192 \u211d is the reward function,\nwhich encodes the task. An optimal policy \ud835\udf0b is one\nthat maximizes the cumulative expected value of the\nreward, i.e., \ud835\udc38[\u2211\u221e\n\ud835\udc61=0 \ud835\udefe\ud835\udc61\ud835\udc5f(\ud835\udc2c\ud835\udc61, \ud835\udc1a\ud835\udc61)], where the expectation\nis taken with respect to the initial state distribution,\ntransition probabilities, and policy \ud835\udf0b.\nWhile the specification of the RL task is concise and\nsimple, turning real-world robotic learning problems\ninto RL problems requires care. First, the sample effi-\nciency of the algorithm for learning \ud835\udf0b is paramount:\nwhen the learning must take place in the real world, ev-\nery minute and hour of training comes at a cost. Sample\nefficiency can be improved by using effective off-policy\nRL algorithms (Konda and Tsitsiklis, 1999; Haarnoja\net al., 2018; Fujimoto et al., 2018), but it can also be\naccelerated by incorporating prior data and demonstra-\ntions (Rajeswaran et al., 2018; Ball et al., 2023; Nair\net al., 2020), which is important to achieve the fastest\ntraining times.\nAdditionally, many of the challenges with robotic\nRL lie beyond just the core algorithm for optimizing \ud835\udf0b.\nFor example, the reward function \ud835\udc5f might depend on\nimage observations, and difficult for the user to specify\nmanually. Additionally, for episodic tasks where the\nrobot resets to an initial state \ud835\udc2c0 \u223c \ud835\udf0c(\ud835\udc2c0) between trials,\nactually resetting the robot (and its environment) into\none of these initial states is a mechanical operation that\nmust somehow be automated.\nFurthermore, the controller layer, which interfaces\nthe MDP actions \ud835\udc1a (e.g., end-effector poses) to the actual\nlow-level robot controls, also requires great care, partic-\nularly for contact-rich tasks where the robot physically\ninteracts with objects in the environment. Not only\ndoes this controller need to be accurate, but it must\nalso be safe enough that the RL algorithm can explore\nwith random actions during training.\nSERL will aim to provide ready-made solutions to\neach of these challenges, with a high-quality implemen-\ntation of a sample-efficient off-policy RL method that\ncan incorporate prior data, several choices for reward\nfunction specification, a forward-backward algorithm\nfor learning resets, and a controller suitable for learning\ncontact-rich tasks without damaging either the robot\nor objects in the environment.\n4. Sample Efficient Robotic Reinforce-\nment Learning in the Real-World\nOur software package, which we call Sample-Efficient\nRobotic reinforcement Learning (SERL), aims to make\nrobotic RL in the real world accessible by providing\n3\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nready-made solutions to the problems detailed in the\nprevious section. This involves providing efficient\nvision-based reinforcement learning algorithms and\nthe infrastructure needed to support these learning al-\ngorithms for autonomous learning. We note that the\npurpose of such an endeavor is not to propose novel\nalgorithms or tools, but rather to develop a software\npackage that anyone can use easily for robotic learning,\nwithout complex setup procedures and painful integra-\ntion across libraries.\nThe core reinforcement learning algorithm is derived\nfrom RLPD (Ball et al., 2023), which itself is a variant\nof soft actor-critic (Haarnoja et al., 2018): an off-policy\nQ-function actor-critic method that can readily incor-\nporate prior data (either suboptimal data or demon-\nstrations) into the replay buffer for efficient learning.\nThe reward functions can be specified either with a bi-\nnary classifier or VICE (Fu et al., 2018), which provides\na method to update the classifier during RL training\nwith additional negatives from the policy. The reward\nfunction can also be specified by hand in cases where\nthe robot state is sufficient to evaluate success (e.g., as\nin our PCB board assembly task). The resets can be\nprovided via a forward-backward architecture (Sharma\net al., 2021), where the algorithm simultaneously trains\ntwo policies: a forward policy that performs the task,\nand a backward policy that resets the environment back\nto the initial state. On the robot system side, we also\nprovide a universal adapter for interfacing our method\nto arbitrary robots, as well as an impedance controller\nthat is particularly well-suited for contact-rich manip-\nulation tasks.\n4.1. Core RL Algorithm: RLPD\nThere are several desiderata for reinforcement learning\nalgorithm to be deployed in this setting: (1) it must be\nefficient and able to make multiple gradient updates\nper time step, (2) it must be able to incorporate prior\ndata easily and then continue improving with further\nexperience, (3) it must be simple to debug and build on\nfor new users. To this end, we build on the recently\nproposed RLPD (Ball et al., 2023) algorithm, which has\nshown compelling results on sample-efficient robotic\nlearning. RLPD is an off-policy actor-critic reinforce-\nment learning algorithm that builds on the success\nof temporal difference algorithms such as soft-actor\ncritic (Haarnoja et al., 2018), but makes some key mod-\nifications to satisfy the desiderata above. RLPD makes\nthree key changes: (i) high update-to-data ratio training\n(UTD), (ii) symmetric sampling between prior data and\non-policy data, such that half of each batch comes from\nprior data and half from the online replay buffer, and\n(iii) layer-norm regularization during training. This\nmethod can train from scratch, or use prior data (e.g.,\ndemonstrations) to bootstrap learning. Each step of\nthe algorithm updates the parameters of a parametric\nQ-function \ud835\udc44\ud835\udf19(\ud835\udc2c, \ud835\udc1a) and actor \ud835\udf0b\ud835\udf03(\ud835\udc1a|\ud835\udc2c) according to the\ngradient of their respective loss functions:\n\ue238\ud835\udc44(\ud835\udf19)=\ud835\udc38\ud835\udc2c,\ud835\udc1a,\ud835\udc2c\u2032[(\ud835\udc44\ud835\udf19(\ud835\udc2c, \ud835\udc1a)\u2212(\ud835\udc5f(\ud835\udc2c, \ud835\udc1a)+\ud835\udefe\ud835\udc38\ud835\udc1a\u2032\u223c\ud835\udf0b\ud835\udf03[\ud835\udc44 \u0304\ud835\udf19(\ud835\udc2c\u2032, \ud835\udc1a\u2032)]))\n2\n]\n\ue238\ud835\udf0b(\ud835\udf03)=\u2212\ud835\udc38\ud835\udc2c [\ud835\udc38\ud835\udc1a\u223c\ud835\udf0b\ud835\udf03(\ud835\udc1a)[\ud835\udc44\ud835\udf19(\ud835\udc2c, \ud835\udc1a)] + \ud835\udefc\ue234(\ud835\udf0b\ud835\udf03(\u22c5|\ud835\udc2c)] ,\nwhere \ud835\udc44 \u0304\ud835\udf19 is a target network (Mnih et al., 2013), and\nthe actor loss uses entropy regularization with an adap-\ntively adjusted weight \ud835\udefc (Haarnoja et al., 2018). Each\nupdate step uses a sample-based approximation of each\nexpectation, with half of the samples drawn from the\nprior data (e.g., demonstrations), and half drawn from\nthe replay buffer (Mnih et al., 2013). For efficient learn-\ning, multiple update steps are performed per time step\nin the environment, which is referred to as the update-\nto-date (UTD) ratio, and regularizing the critic with\nlayer normalization allows for higher UTD ratios and\nthus more efficient training (Ball et al., 2023).\n4.2. Reward Specification with Classifiers\nReward functions are difficult to specify by hand when\nlearning with image observations, as the robot typically\nrequires some sort of perception system just to deter-\nmine if the task was performed successfully. While\nsome tasks, such as the PCB board assembly task in\nFig. 1, can accommodate hand-specified rewards based\non the location of the end effector (under the assump-\ntion that the object is held rigidly in the gripper), most\ntasks require rewards to be deduced from images. In\nthis case, the reward function can be provided by a\nbinary classifier that takes in the state observation \ud835\udc2c\nand outputs the probability of a binary \u201cevent\u201d \ud835\udc52, corre-\nsponding to successful completion. The reward is then\ngiven by \ud835\udc5f(\ud835\udc2c) = log \ud835\udc5d(\ud835\udc52|\ud835\udc2c).\nThis classifier can be trained either using hand-\nspecified positive and negative examples, or via an\nadversarial method called VICE (Fu et al., 2018). The\nlatter addresses a reward exploitation problem that can\narise when learning with classifier based rewards, and\nremoves the need for negative examples in the clas-\nsifier training set: when the RL algorithm optimizes\nthe reward \ud835\udc5f(\ud835\udc2c) = log \ud835\udc5d(\ud835\udc52|\ud835\udc2c), it can potentially discover\n\u201cadversarial\u201d states that fool the classifier \ud835\udc5d(\ud835\udc52|\ud835\udc2c) to erro-\nneously output high probabilities. VICE addresses this\n4\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nissue by adding all states visited by the policy into the\ntraining set for the classifier with negative labels, and\nupdating the classifier after each iteration. In this way,\nthe RL process is analogous to a generative adversar-\nial network (GAN) (Goodfellow et al., 2014), with the\npolicy acting as the generator and the reward classi-\nfier acting as the discriminator. Our framework thus\nsupports all three types of rewards.\n4.3. Reset-Free\nTraining\nwith\nForward-\nBackward Controllers\nWhen learning episodic tasks, the robot must reset\nthe environment between task attempts. For example,\nwhen learning the object relocation task in Figure 1,\neach time the robot successfully moves the object to the\ntarget bin, it must then take it out and place it back into\nthe initial bin. To remove the need for human effort\nin \u201cresets\u201d, SERL supports \u201creset-free\u201d training by us-\ning forward and backward controllers (Han et al., 2015;\nGupta et al., 2021). In this setup, two policies are trained\nsimultaneously using two independent RL agents, each\nwith its own policy, Q-function, and reward function\n(specified via the methods in the previous section). The\nforward agent learns to perform the task, and the back-\nward agent learns to return to the initial state(s). While\nmore complex reset-free training procedures can also\nbe possible (Gupta et al., 2021), we find that this sim-\nple recipe is sufficient for learning object manipulation\ntasks like the repositioning skill in Figure 1.\n4.4. Software Components\nEnvironment adapters: SERL aims to be easily usable\nfor many robot environments. Although we provide a\nset of Gym environment wrappers and robot environ-\nments for the Franka arm as starter guides, users can\nalso use their own existing environments or develop\nnew environments as they see fit. Thus, the library\ndoes not impose additional constraints on the robot\nenvironment as long as it is Gym-like (Brockman et al.,\n2016) as shown in Fig. 2. We welcome contributions\nfrom the community to extend the support for readily\ndeployable environment wrappers for other robots and\ntasks.\nActor and learner nodes: SERL includes options to\ntrain and act in parallel to decouple inferring actions\nand updating policies with a few lines of code as illus-\ntrated in Fig. 2. We found this to be beneficial in sample-\nefficient real-world learning problems with high UTD\nratios. By separating actor and learner on two different\nthreads, SERL not only preserves the control frequency\nFigure 2: Software architecture and real-world robot training\nexample code. SERL runs three parallel processes, consisting of the\nactor, which chooses actions, and the learner node, which actually\nruns the training code, and the robot environment, which executes\nthe actions from the actor and contributes data back to the learner.\nat a fixed rate, which is crucial for tasks that require\nimmediate feedback and reactions, such as deformable\nobjects and contact-rich manipulations, but also re-\nduces the total wall-clock time spend training in the\nreal world.\n4.5. Impedance Controller for Contact-Rich\nTasks\nAlthough our package should be compatible with any\nOEM robot controller, as described in Sec. 4, we found\nthat the choice of controllers can heavily affect the final\nperformance. This is more pronounced for contact-\nrich manipulation. For example, in the PCB insertion\ntask in Fig. 1, an overly stiff controller might bend the\nfragile pins and make insertion difficult, while an overly\ncompliant controller might struggle to move the object\ninto position quickly.\nA typical setup for robotic RL employs a two-layered\ncontrol hierarchy, where an RL policy produces set-\npoint actions at a much lower frequency than the down-\nstream real-time controller. The RL controller can set\ntargets for the low-level controller to cause physically\nundesirable consequences. To illustrate this, let\u2019s con-\nsider the hierarchical controller structure presented in\nFig. 4, where a high-level RL controller \ud835\udf0b(\ud835\udc1a|\ud835\udc2c) sends\ncontrol targets at 10HZ for the low-level impedance\ncontroller to track at 1K HZ, so one timestep from RL\nwill block 100 timesteps of the low-level controller to\nexecute. A typical impedance control objective for this\ncontroller is\n\ud835\udc39 = \ud835\udc58\ud835\udc5d \u22c5 \ud835\udc52 + \ud835\udc58\ud835\udc51 \u22c5 \u0307\ud835\udc52 + \ud835\udc39\ud835\udc53 \ud835\udc53 + \ud835\udc39\ud835\udc50\ud835\udc5c\ud835\udc5f,\nwhere \ud835\udc52 = \ud835\udc5d \u2212 \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc53 , \ud835\udc5d is the measured pose, and \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc53 is\nthe target pose computed by the upstream controller,\n\ud835\udc39\ud835\udc53 \ud835\udc53 is the feed-forward force, \ud835\udc39\ud835\udc50\ud835\udc5c\ud835\udc5f is the Coriolis force,\n5\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nFigure 3: Visualization of controller logs from the robot when\ncommanded with different movements, for the z-axis of the end-\neffector. The orange line is the commanded target (the output of\nRL), red is the smoothed target sent to the real-time controller, blue\nis the clipped target, and green is the robot position after executing\nthis controller. Left: The robot end-effector was commanded to\nmove into contact with a hard surface and continue the movement\ndespite the contact. The reference limiting mechanism clipped the\ntarget to avoid a hard collision. Right: The command is a fast\nfree-space movement, which our reference limiting mechanism\ndoes not block, allowing fast motion to the target.\nFigure 4: A typical controller hierarchy for robotics RL. The\noutput from the RL policy is tracked within a block of time by the\ndownstream controller.\nthis objective will then be converted into joint space\ntorques by multiplying Jacobian transpose and offset by\nnullspace torques. It acts like a spring-damper system\naround the equilibrium set by \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc53 with the stiffness\ncoefficient being \ud835\udc58\ud835\udc5d and the damping coefficient being\n\ud835\udc58\ud835\udc51. As described above, this system will yield large\nforces if \ud835\udc5d\ud835\udc5f\ud835\udc52\ud835\udc53 is far away from the current pose, which\ncan lead to a hard collision or damage when the arm\nis in contact with something. Therefore it\u2019s crucial to\nconstrain the interaction force generated by it. How-\never, directly reducing gains will hurt the controller\u2019s\naccuracy. Thus, we should bound \ud835\udc52 so that |\ud835\udc52| \u2264 \u0394,\nand then the generated force from the spring-damper\nsystem will be bounded to \ud835\udc58\ud835\udc5d \u22c5 |\u0394| + 2\ud835\udc58\ud835\udc51 \u22c5 |\u0394| \u22c5 \ud835\udc53 , \ud835\udc53 is the\ncontrol frequency.\nOne might wonder if we should directly clip the\naction output by the RL policy. This might seem rea-\nsonable, but can be impractical in some scenarios: some\nobjects such as the PCB board may require a very small\ninteraction force, implying a very small \u0394, usually on\nthe order of micrometers; if the RL policy is only al-\nlowed to move at increments of micrometers, it would\nresult in an extremely long learning process or very un-\nstable training, because the episode would need enough\ntime steps to allow the arm to move over long distances\n(e.g., approaching the insertion point). However, if we\ndirectly clip at the real-time layer, this will largely mit-\nigate the issue without the need to constrain the RL\npolicy to small actions. It will not block the free space\nmovement of the RL policy as long as \ud835\udc40 \u22c5 |\u0394| \u2265 |\ud835\udc4e|\ud835\udc5a\ud835\udc4e\ud835\udc65,\nwhere \ud835\udc40 is the number of control time-steps inside\na block, as in Fig. 4. This value is usually large (e.g.,\n\ud835\udc40 = 100). At the same time, we strictly enforce the\nreference constraint at the real-time level whenever\nin contact. One might also wonder if it\u2019s possible to\nachieve the same result by using an external force/-\ntorque sensor. This may be undesirable for several\nreasons: (1) force/torque sensors can have significant\nnoise, and obtaining the right hardware and calibration\ncan be difficult; (2) even if we get such a threshold value,\nit\u2019s nontrivial to design robot motions to accommodate\npolicy learning as well as obeying the force constraint.\nIn practice, we found that clipping the reference in this\nway is simple but very effective, and is crucial to enable\nRL-based contact-rich manipulation tasks. We tested\nour controller on a Franka Panda robot and included\nthe Franka Panda implementation with our package.\nHowever, this principle can be easily implemented on\nany torque-controlled robot. To verify the actual perfor-\nmance of the proposed controller, we report the actual\ntracking performance of moving the robot in free space\nand in contact with a table surface as in Fig. 3, where\nwe can see the controller indeed clamps the reference\nwhenever in contact, while permitting fast movement\nin free space.\n4.6. Relative Observation and Action Frame\nThe choice of action space is particularly important\nboth for the ease of the RL training process and the\nability of the learned policy to generalize to perturba-\ntions at test time. While SERL can operate on various\naction representations via a standard RL environment\ninterface, we found that a convenient mechanism of\nrepresenting observations and actions in a relative co-\nordinate system.\nTo develop an agent capable of adapting to a dynamic\ntarget, we propose a training procedure that simulates a\nmoving target without the need for physical movement.\nThe target, for instance, the PCB insertion socket holes,\nis fixed relative to the robot base frame, and the reward\ncan be specified using any of the standard methods\nprovided in Sec. 4.2. At the beginning of each train-\ning episode, the pose of the robot\u2019s end-effector was\n6\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nFigure 5: Illustration of the robot performing each task with our method: PCB Insertion (top left), Cable Routing (top right), Object\nRelocation - Forward (bottom left), and Object Relocation - Backward (bottom right). The green box indicates a state where the robot\nreceives a high reward for completing the task.\nPackage\nTask\nTraining time\nSuccess rate\nDemos\nShaping?\nVision?\nOpen-sourced?\nGuided Policy Search(Levine et al., 2016b)\nPeg insertion\n3 hours\n70%\n0\nYes\nYes\nYes\nDDPGfD (Vecerik et al., 2018)\nPeg/clip insertion\n1.5-2.5 hours\n97% / 77%\n30\nNo\nYes\nNo\nVisual Residual RL (Schoettler et al., 2020)\nConnector insertion\nNot mentioned\n52% \u223c 100%\n0\nYes\nYes\nNo\nSHIELD (Luo et al., 2021)\nConnector insertion\n1.5 hours\n99.8%\n25\nNo\nYes\nNo\nInsertionNet (Spector and Castro, 2021)\nConnector insertion\n40 mins\n78.5% - 100%\n0\nYes\nYes\nNo\nSERL (Ours)\nPCB Insertion\n20 mins\n100%\n20\nNo\nYes\nYes\nTable 1: Comparison to results reported on similar tasks in prior work. The overall success rates for our method are generally higher,\nand the training times are generally lower, as compared to prior results. Note also that the PCB board assembly task, shown in Figure 1,\nhas very tight tolerances, likely significantly tighter than the coarser peg and connector insertion tasks studied in the prior works.\nrandomized uniformly within a pre-defined area in the\nworkspace. The robot\u2019s proprioceptive information is\nexpressed with respect to frame of the end-effector\u2019s ini-\ntial pose; the action output from the policy (6D twist) is\nrelative to the current end-effector frame. This is equiv-\nalent to physically moving the target when viewed\nrelatively from the frame attached to the end-effector.\nMore details on are described in the appendix 7. As a\nresult, the policy can succeed even if the object moves\nor, as in some of our experiments, is perturbed in the\nmiddle of the episode.\n5. Experiments\nOur experimental evaluation aims to study how effi-\nciently our system can learn a variety of robotic manip-\nulation tasks, including contact-rich tasks, deformable\nobject manipulation, and free-floating object manipu-\nlation. These experiments demonstrate the breadth of\napplicability and efficiency of SERL. We use a Franka\nPanda arm and two wrist cameras attached to the end-\neffector to get close-in views. Further details can be\nfound at https://serl-robot.github.io/. We use an Ima-\ngeNet pre-trained ResNet-10 (He et al., 2015) as a vision\nbackbone for the policy network and connect it to a\n2-layer MLP. Observations include camera images and\nrobot proprioceptive information such as end-effector\npose, twist, force, and torque. The policy outputs a 6D\nend-effector delta pose from the current pose, which\nis tracked by the low-level controller. The evaluation\ntasks are illustrated in Fig. 5 and described below:\nPCB insertion: Inserting connectors into a PCB board\ndemands fine-grained, contact-rich manipulation with\nsub-millimeter precision. This task is ideal for real-\nworld training, as simulating and transferring such\ncontact-rich interactions can be challenging. At the\nbeginning of each training and evaluation episode, the\ninitial end effector pose is sampled uniformly from a\nstarting region, as described in Table 2.\nCable routing:\nThis task involves routing a de-\nformable cable into a clip\u2019s tight-fitting slot. This task\nrequires the robot to perceive the cable and carefully\nmanipulate it so that it fits into the clip while holding it\nat another location. This is particularly difficult for any\nmethod that relies on model-based control, or makes\nrigid-object assumptions, since both visual perception\nand handling of deformable objects is essential for ac-\ncess. Tasks of this sort often arise in manufacturing and\nmaintenance scenarios. Similarly to the PCB task, the\ninitial end effector pose is sampled uniformly within a\nstarting region, as described in Table 2.\nObject relocation: This task requires moving a free-\nfloating object between bins, requiring grasping and\nrelocation. The intricacies of reward inference and\nreset-free training become especially pronounced in the\nmanipulation of such free-floating objects. We define\nthe forward task as picking up the object from the bin\n7\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nTask\n# of Demos\nImage Input\nRandom Reset\nReward Specification\nBin Size\nTraining Time\nPCB Component Insertion\n20\n2 wrist camera\nTrue\nGround Truth\n10cm \u00d7 10cm\n20 mins\nCable Routing\n20\n2 wrist camera\nTrue\nBinary Classifier\n20cm \u00d7 20cm\n31 mins\nObject Relocation (Forward-Backward)\n20\n1 wrist, 1 side camera\nFalse\nBinary Classifier\n20cm \u00d7 30cm\n105 mins\nTable 2: Task parameters: During demo collection for both BC and RL, as well as online training, each episode\u2019s initial end-effector pose\nresets uniformly at random within a fixed region for the PCB and Cable task, while the free-floating object relocation task resets above\nthe center of each bin.\nFigure 6: Success rate comparisons: When evaluated for 100 trials\nper task, learned RL policies outperformed BC policies by a large\nmargin, by 1.7x for Object Relocation, by 5x for Cable Routing,\nand by 10x for PCB Insertion.\non the right side and placing it on the left, while the\nbackward task moves the object back to the starting\nbin, undoing the forward task.\nFor each task, we initialize RL training from 20 tele-\noperated demonstrations using a Space Mouse. To con-\nfirm that demonstrations alone are insufficient to solve\nthe task, we include a behavioral cloning (BC) baseline\nusing 100 high-quality expert teleoperated demonstra-\ntions, roughly matching the total amount of data in the\nRL replay buffer when RL converges. Note that this is 5\ntimes more demonstrations than the amount provided\nby our method. Both RL and BC demonstrations are\ncollected using the initial end effector randomization\nscheme described in Table 2. All training was done on\na single Nvidia RTX 4090 GPU.\nResults: We report the results in Table 2, and show\nexample executions in Fig. 5. We evaluated both BC\nand RL policies under the same conditions and proto-\ncols as detailed in Section 5. Our RL policies achieve\nperfect success rates on all three tasks over all 100 trials.\nFor the PCB insertion and cable routing task, our RL\npolicies converge in under 30 minutes of real-world\ntraining, which includes all computation, resets, and\nintended stops. The free-floating object relocation task\nlearns two policies (forward and backward), and total\nFigure 7: Cycle time comparison: We recorded the average time\ntaken for the robot to succeed in each task. RL policies are at least\n2x faster than BC policies trained with 100 high-quality human\nteleoperated demonstrations for all three tasks.\ntime amounts to less than an hour per policy. For the\ncable routing task and PCB insertion task, our policies\noutperform BC baselines by a large margin, despite\ntraining with 5x fewer demonstrations than BC, sug-\ngesting that demos alone are insufficient. We report\nthe results in terms of success rate and cycle time in\nFig. 6 and Fig. 7. The learned RL policies not only out-\nperformed their BC counterparts by as much as 10x in\nterms of success rate but also improved on the cycle\ntime of the initial human demonstrations by up to 3x.\nComparison to prior systems: While it\u2019s difficult\nto directly compare our results to those of prior sys-\ntems due to numerous differences in the setup, lack\nof consistently open-sourced code, and other discrep-\nancies, we provide a summary of training times and\nsuccess rates reported for tasks that are most similar\nto our PCB board insertion task in Table 2. We chose\nthis task because similar insertion or assembly tasks\nhave been studied in prior work, and such tasks often\npresent challenges with precision, compliant control,\nand sample efficiency. Compared to these prior works,\nour experiments do not use shaped rewards, which\nmight require extensive engineering, though we do\nutilize a small amount of demonstration data (which\nsome prior works eschew). The results reported in\n8\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nFigure 8: Peg Insertion Task at University of Washington\nthese prior works generally have either lower success\nrates or longer training times, or both, suggesting our\nimplementation of sample-efficient RL matches or ex-\nceeds the performance of state-of-the-art methods in\nthe literature, at least on this type of task. The closest\nperformance to ours in the work of Spector et al. (Spec-\ntor and Castro, 2021) includes a number of design deci-\nsions and inductive biases that are specific to insertion,\nwhereas our method is generic and makes minimal task-\nspecific assumptions. Although the components of our\nsystem are all based on (recent) prior work, the state-\nof-the-art performance of this combination illustrates\nour main thesis: the details of how deep RL methods\nare implemented can make a big difference.\nReproducibility: A core mission of SERL is to lower\nthe setup barriers and encourage reproducible robotic\nRL on different robot systems. To this end, we demon-\nstrate the successful integration of SERL software suite\non a robot arm operated at a different institution.\nResearchers at the University of Washington set up\na Peg Insertion task using 3D printed parts from the\nFunctional Manipulation Benchmark (Luo et al., 2024)\nand used SERL to solve this challenging task. The over-\nall preparation time including setting up the relevant\nhardware and software is less than 3 hours. The policy\nconverged in 19 minutes and achieved a 100/100 success\nrate with 20 initial human demonstrations, successfully\nreproducing our results.\n6. Discussion\nWe described a system and software package for robotic\nreinforcement learning that is aimed at making real-\nworld RL more accessible both to researchers and prac-\ntitioners. Our software package provides a carefully de-\nsigned combination of ingredients for sample-efficient\nRL, automating reward design, automating environ-\nment resets with forward-backward controllers, and a\ncontroller framework that is particularly well-suited for\ncontact-rich manipulation tasks. Furthermore, our ex-\nperimental evaluation of our framework demonstrates\nthat it can learn a range of diverse manipulation tasks\nvery efficiently, with under an hour of training per\npolicy when provided with a small number of demon-\nstrations. These results qualitatively compare well to\nstate-of-the-art results in RL for manipulation in the\nliterature, indicating that the particular choices in our\nframework are well-suited for obtaining very good real-\nworld results even from image observations. Our frame-\nwork does have a number of limitations. First, we do\nnot aim to provide a comprehensive library with every\npossible RL method, and some tasks and settings might\nbe outside of our framework (e.g., non-manipulation\ntasks). Second, the full range of reward specifications\nand reset-free learning challenges still constitute an\nopen problem in robotic RL research. Our classifier-\nbased rewards and forward-backward controller might\nnot be appropriate in every setting. Further research on\nthese topics is needed to make robotic RL more broadly\napplicable. However, we hope that our software pack-\nage will provide a reasonable \u201cdefault\u201d starting point\nfor both researchers and practitioners wanting to ex-\nperiment with real-world RL methods.\nAcknowledgments\nThis research was partially supported by Intrinsic In-\nnovation LLC, the National Science Foundation under\nIIS-2150826, and ARO W911NF-21-1-0097. We would\nlike to thank Rehaan Ahmad and Siri Gadipudi for par-\nticipating in the discussion of this project.\n7. Appendix\n7.1. Details on Relative Observation and Ac-\ntion Frame\nLet the robot\u2019s base frame be {\ud835\udc60}; for the \ud835\udc56-th episode\nof rolling out the policy, we denote {\ud835\udc4f(\ud835\udc56)\n\ud835\udc61 } as the end-\neffector frame expressed w.r.t. {\ud835\udc60} at a particular time\nstep \ud835\udc61; where 1 \u2264 \ud835\udc56 \u2264 \ud835\udc40, 0 \u2264 \ud835\udc61 \u2264 \ud835\udc41. For each episode,\n{\ud835\udc4f(\ud835\udc56)\n0 } is sampled from a uniform distribution specifying\nthe area of randomization. We want to express such\nproprioceptive information with respect to {\ud835\udc4f(\ud835\udc56)\n0 }. Thus,\nthe policy will be applicable to a new location provided\nthat the relative spatial distance between the robot\u2019s\nend-effector and the target remains consistent. This\napproach prevents overfitting to specific global loca-\ntions within the reference frame {\ud835\udc60}. We achieve this by\napplying the following homogeneous transformation:\n\ud835\udc47\ud835\udc4f(\ud835\udc56)\n0 \ud835\udc4f(\ud835\udc56)\n\ud835\udc61 = \ud835\udc47 \u22121\n\ud835\udc4f(\ud835\udc56)\n0 \u22c5 \ud835\udc47\ud835\udc4f(\ud835\udc56)\n\ud835\udc61\n9\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nwhere we use \ud835\udc47\ud835\udc4e\ud835\udc4f to denote the homogeneous trans-\nformation matrix between frame {\ud835\udc4e} and {\ud835\udc4f}. We feed\nthe position and rotation information extracted from\n\ud835\udc47\ud835\udc4f(\ud835\udc56)\n0 \ud835\udc4f(\ud835\udc56)\n\ud835\udc61 to the policy. Here we use \ud835\udc47\ud835\udc4e\ud835\udc4f to denote the ho-\nmogeneous transformation matrix between frame {\ud835\udc4e}\nand {\ud835\udc4f}, defined as:\n\ud835\udc47\ud835\udc4e\ud835\udc4f = [\n\ud835\udc45\ud835\udc4e\ud835\udc4f\n\ud835\udc5d\ud835\udc4e\ud835\udc4f\n01\u00d73\n1 ] .\nThe policy generates a six degrees of freedom (6 DoF)\ntwist action, which is expressed in the reference frame\nfrom which it currently receives observations, i.e., {\ud835\udc4f(\ud835\udc56)\n\ud835\udc61 }.\nMathematically, the 6 DoF twist action \ue242(\ud835\udc56)\n\ud835\udc61\nexpressed in\nframe {\ud835\udc4f(\ud835\udc56)\n\ud835\udc61 } at timestep \ud835\udc61. To interface with the robot\u2019s\ncontrol software, which expects actions \ue242(\ud835\udc56)\n\ud835\udc61\n\u2032 expressed\nin the base frame {\ud835\udc60}, we apply the Adjoint mapping:\n\ue242(\ud835\udc56)\n\ud835\udc61\n\u2032 = [Ad(\ud835\udc56)\n\ud835\udc61 ]\n\u22121\ue242(\ud835\udc56)\n\ud835\udc61\nwhere [Ad(\ud835\udc56)\n\ud835\udc61 ] is a function of the homogeneous trans-\nformation \ud835\udc47\ud835\udc4f(\ud835\udc56)\n0 defined as:\n[Ad(\ud835\udc56)\n\ud835\udc61 ] = [\n\ud835\udc45\ud835\udc4f(\ud835\udc56)\n\ud835\udc61\n03\u00d73\n[\ud835\udc5d\ud835\udc4f(\ud835\udc56)\n\ud835\udc61 ] \u00d7 \ud835\udc45\ud835\udc4f(\ud835\udc56)\n\ud835\udc61\n\ud835\udc45\ud835\udc4f(\ud835\udc61)\n\ud835\udc61 ] .\nReferences\nMichael Ahn, Henry Zhu, Kristian Hartikainen, Hugo\nPonte, Abhishek Gupta, Sergey Levine, and Vikash\nKumar.\nROBEL: robotics benchmarks for learn-\ning with low-cost robots. In Leslie Pack Kaelbling,\nDanica Kragic, and Komei Sugiura, editors, 3rd An-\nnual Conference on Robot Learning, CoRL 2019, Os-\naka, Japan, October 30 - November 1, 2019, Proceed-\nings, volume 100 of Proceedings of Machine Learn-\ning Research, pages 1300\u20131313. PMLR, 2019. URL\nhttp://proceedings.mlr.press/v100/ahn20a.html.\nPhilip J Ball, Laura Smith, Ilya Kostrikov, and Sergey\nLevine. Efficient online reinforcement learning with\noffline data. arXiv preprint arXiv:2302.02948, 2023.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson,\nJonas Schneider, John Schulman, Jie Tang, and Wo-\njciech Zaremba.\nOpenai gym.\narXiv preprint\narXiv:1606.01540, 2016.\nDieter B\u00fcchler, Simon Guist, Roberto Calandra, Vin-\ncent Berenz, Bernhard Sch\u00f6lkopf, and Jan Peters.\nLearning to play table tennis from scratch using\nmuscular robots. IEEE Trans. Robotics, 38(6):3850\u2013\n3860, 2022. doi: 10.1109/TRO.2022.3176207. URL\nhttps://doi.org/10.1109/TRO.2022.3176207.\nYuqing Du, Ksenia Konyushkova, Misha Denil, Akhil\nRaju, Jessica Landon, Felix Hill, Nando de Freitas,\nand Serkan Cabi. Vision-language models as success\ndetectors. arXiv preprint arXiv:2303.07280, 2023.\nBenjamin Eysenbach, Shixiang Gu, Julian Ibarz, and\nSergey Levine. Leave no trace: Learning to reset\nfor safe and autonomous reinforcement learning. In\n6th International Conference on Learning Representa-\ntions, ICLR 2018, Vancouver, BC, Canada, April 30 -\nMay 3, 2018, Conference Track Proceedings. OpenRe-\nview.net, 2018. URL https://openreview.net/forum?id=\nS1vuO-bCW.\nLinxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Man-\ndlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang,\nDe-An Huang, Yuke Zhu, and Anima Anandkumar.\nMinedojo: Building open-ended embodied agents\nwith internet-scale knowledge.\nIn NeurIPS, 2022.\nURL http://papers.nips.cc/paper_files/paper/2022/hash/\n74a67268c5cc5910f64938cac4526a90-Abstract-Datasets_\nand_Benchmarks.html.\nJustin Fu, Avi Singh, Dibya Ghosh, Larry Yang, and\nSergey Levine.\nVariational inverse control with\nevents: A general framework for data-driven reward\ndefinition. Advances in neural information processing\nsystems, 31, 2018.\nScott Fujimoto, Herke van Hoof, and David Meger.\nAddressing function approximation error in actor-\ncritic methods.\nIn Jennifer G. Dy and Andreas\nKrause, editors, Proceedings of the 35th International\nConference on Machine Learning, ICML 2018, Stock-\nholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018,\nvolume 80 of Proceedings of Machine Learning Re-\nsearch, pages 1582\u20131591. PMLR, 2018.\nURL http:\n//proceedings.mlr.press/v80/fujimoto18a.html.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. Generative adversar-\nial nets. Advances in neural information processing\nsystems, 27, 2014.\nSergio Guadarrama, Anoop Korattikara, Oscar Ramirez,\nPablo Castro, Ethan Holly, Sam Fishman, Ke Wang,\nEkaterina Gonina, Neal Wu, Efi Kokiopoulou, Lu-\nciano Sbaiz, Jamie Smith, G\u00e1bor Bart\u00f3k, Jesse\n10\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nBerent, Chris Harris, Vincent Vanhoucke, and Eu-\ngene Brevdo. TF-Agents: A library for reinforcement\nlearning in tensorflow. https://github.com/tensorflow/\nagents, 2018. URL https://github.com/tensorflow/agents.\n[Online; accessed 25-June-2019].\nAbhishek Gupta, Justin Yu, Tony Z. Zhao, Vikash\nKumar, Aaron Rovinsky, Kelvin Xu, Thomas De-\nvlin, and Sergey Levine. Reset-free reinforcement\nlearning via multi-task learning: Learning dexterous\nmanipulation behaviors without human interven-\ntion. In IEEE International Conference on Robotics\nand Automation, ICRA 2021, Xi\u2019an, China, May 30\n- June 5, 2021, pages 6664\u20136671. IEEE, 2021. doi:\n10.1109/ICRA48506.2021.9561384.\nURL https://doi.\norg/10.1109/ICRA48506.2021.9561384.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and\nSergey Levine. Soft actor-critic: Off-policy maxi-\nmum entropy deep reinforcement learning with a\nstochastic actor. In International conference on ma-\nchine learning, pages 1861\u20131870. PMLR, 2018.\nWeiqiao Han, Sergey Levine, and Pieter Abbeel. Learn-\ning compound multi-step controllers under un-\nknown dynamics. In 2015 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS),\npages 6435\u20136442. IEEE, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. Deep residual learning for image recognition,\n2015.\nMinho Heo, Youngwoon Lee, Doohyun Lee, and\nJoseph J. Lim. Furniturebench: Reproducible real-\nworld benchmark for long-horizon complex manip-\nulation. In Kostas E. Bekris, Kris Hauser, Sylvia L.\nHerbert, and Jingjin Yu, editors, Robotics: Science and\nSystems XIX, Daegu, Republic of Korea, July 10-14,\n2023, 2023. doi: 10.15607/RSS.2023.XIX.041. URL\nhttps://doi.org/10.15607/RSS.2023.XIX.041.\nTodd Hester and Peter Stone.\nTexplore: real-time\nsample-efficient reinforcement learning for robots.\nMachine learning, 90:385\u2013429, 2013.\nAshley Hill, Antonin Raffin, Maximilian Ernestus,\nAdam Gleave, Anssi Kanervisto, Rene Traore, Pra-\nfulla Dhariwal, Christopher Hesse, Oleg Klimov, Alex\nNichol, Matthias Plappert, Alec Radford, John Schul-\nman, Szymon Sidor, and Yuhuai Wu. Stable baselines.\nhttps://github.com/hill-a/stable-baselines, 2018.\nZhimin Hou, Jiajun Fei, Yuelin Deng, and Jing Xu.\nData-efficient hierarchical reinforcement learning\nfor robotic assembly control applications. IEEE Trans-\nactions on Industrial Electronics, 68(11):11565\u201311575,\n2020.\nZheyuan Hu, Aaron Rovinsky, Jianlan Luo, Vikash Ku-\nmar, Abhishek Gupta, and Sergey Levine. REBOOT:\nreuse data for bootstrapping efficient real-world dex-\nterous manipulation. arXiv preprint arXiv:2309.03322,\n2024.\nStephen James, Zicong Ma, David Rovick Arrojo, and\nAndrew J. Davison. Rlbench: The robot learning\nbenchmark & learning environment. IEEE Robotics\nAutom. Lett., 5(2):3019\u20133026, 2020. doi: 10.1109/LRA.\n2020.2974707. URL https://doi.org/10.1109/LRA.2020.\n2974707.\nTobias Johannink, Shikhar Bahl, Ashvin Nair, Jianlan\nLuo, Avinash Kumar, Matthias Loskyll, Juan Aparicio\nOjea, Eugen Solowjow, and Sergey Levine. Resid-\nual reinforcement learning for robot control. CoRR,\nabs/1812.03201, 2018. URL http://arxiv.org/abs/1812.\n03201.\nDmitry Kalashnikov, Jacob Varley, Yevgen Chebotar,\nBenjamin Swanson, Rico Jonschkowski, Chelsea\nFinn, Sergey Levine, and Karol Hausman. Mt-opt:\nContinuous multi-task robotic reinforcement learn-\ning at scale.\nCoRR, abs/2104.08212, 2021.\nURL\nhttps://arxiv.org/abs/2104.08212.\nVijay R. Konda and John N. Tsitsiklis. Actor-critic al-\ngorithms. In Sara A. Solla, Todd K. Leen, and Klaus-\nRobert M\u00fcller, editors, Advances in Neural Informa-\ntion Processing Systems 12, [NIPS Conference, Den-\nver, Colorado, USA, November 29 - December 4, 1999],\npages 1008\u20131014. The MIT Press, 1999. URL http:\n//papers.nips.cc/paper/1786-actor-critic-algorithms.\nIlya Kostrikov, Laura M. Smith, and Sergey Levine.\nDemonstrating A walk in the park: Learning to walk\nin 20 minutes with model-free reinforcement learn-\ning. In Kostas E. Bekris, Kris Hauser, Sylvia L. Her-\nbert, and Jingjin Yu, editors, Robotics: Science and\nSystems XIX, Daegu, Republic of Korea, July 10-14,\n2023, 2023. doi: 10.15607/RSS.2023.XIX.056. URL\nhttps://doi.org/10.15607/RSS.2023.XIX.056.\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter\nAbbeel. End-to-end training of deep visuomotor\n11\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\npolicies. J. Mach. Learn. Res., 17:39:1\u201339:40, 2016a.\nURL http://jmlr.org/papers/v17/15-522.html.\nSergey Levine, Chelsea Finn, Trevor Darrell, and Pieter\nAbbeel. End-to-end training of deep visuomotor\npolicies. The Journal of Machine Learning Research,\n17(1):1334\u20131373, 2016b.\nSergey Levine, Peter Pastor, Alex Krizhevsky, Julian\nIbarz, and Deirdre Quillen. Learning hand-eye coor-\ndination for robotic grasping with deep learning and\nlarge-scale data collection. Int. J. Robotics Res., 37\n(4-5):421\u2013436, 2018. doi: 10.1177/0278364917710318.\nURL https://doi.org/10.1177/0278364917710318.\nKevin Li, Abhishek Gupta, Ashwin Reddy, Vitchyr H.\nPong, Aurick Zhou, Justin Yu, and Sergey Levine.\nMURAL: meta-learning uncertainty-aware rewards\nfor outcome-driven reinforcement learning. In Ma-\nrina Meila and Tong Zhang, editors, Proceedings of\nthe 38th International Conference on Machine Learn-\ning, ICML 2021, 18-24 July 2021, Virtual Event, vol-\nume 139 of Proceedings of Machine Learning Re-\nsearch, pages 6346\u20136356. PMLR, 2021.\nURL http:\n//proceedings.mlr.press/v139/li21g.html.\nJianlan Luo,\nEugen Solowjow,\nChengtao Wen,\nJuan Aparicio Ojea, and Alice M Agogino. Deep\nreinforcement learning for robotic assembly of\nmixed deformable and rigid objects. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and\nSystems (IROS), pages 2062\u20132069. IEEE, 2018.\nJianlan Luo,\nEugen Solowjow,\nChengtao Wen,\nJuan Aparicio Ojea, Alice M Agogino, Aviv Tamar,\nand Pieter Abbeel.\nReinforcement learning on\nvariable impedance controller for high-precision\nrobotic assembly. In 2019 International Conference on\nRobotics and Automation (ICRA), pages 3080\u20133087.\nIEEE, 2019.\nJianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Wen-\nzhao Lian, Chang Su, Mel Vecerik, Ning Ye, Stefan\nSchaal, and Jonathan Scholz. Robust Multi-Modal\nPolicies for Industrial Assembly via Reinforcement\nLearning and Demonstrations: A Large-Scale Study.\nIn Proceedings of Robotics: Science and Systems, Vir-\ntual, July 2021. doi: 10.15607/RSS.2021.XVII.088.\nJianlan Luo, Perry Dong, Yuexiang Zhai, Yi Ma, and\nSergey Levine.\nRlif: Interactive imitation learn-\ning as reinforcement learning.\narXiv preprint\narXiv:2311.12996, 2023.\nJianlan Luo, Charles Xu, Fangchen Liu, Liam Tan,\nZipeng Lin, Jeffrey Wu, Pieter Abbeel, and Sergey\nLevine. Fmb: a functional manipulation benchmark\nfor generalizable robotic learning. arXiv preprint\narXiv:2401.08553, 2024.\nYecheng Jason Ma, Vikash Kumar, Amy Zhang, Os-\nbert Bastani, and Dinesh Jayaraman. LIV: language-\nimage representations and rewards for robotic\ncontrol.\nIn Andreas Krause, Emma Brunskill,\nKyunghyun Cho, Barbara Engelhardt, Sivan Sabato,\nand Jonathan Scarlett, editors, International Confer-\nence on Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings\nof Machine Learning Research, pages 23301\u201323320.\nPMLR, 2023a. URL https://proceedings.mlr.press/v202/\nma23b.html.\nYecheng Jason Ma, Shagun Sodhani, Dinesh Jayaraman,\nOsbert Bastani, Vikash Kumar, and Amy Zhang. VIP:\ntowards universal visual reward and representation\nvia value-implicit pre-training. In The Eleventh In-\nternational Conference on Learning Representations,\nICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenRe-\nview.net, 2023b. URL https://openreview.net/pdf?id=\nYJ7o2wetJ2.\nJeffrey Mahler, Jacky Liang, Sherdil Niyaz, Michael\nLaskey, Richard Doan, Xinyu Liu, Juan Aparicio\nOjea, and Ken Goldberg. Dex-net 2.0: Deep learning\nto plan robust grasps with synthetic point clouds\nand analytic grasp metrics. In Nancy M. Amato,\nSiddhartha S. Srinivasa, Nora Ayanian, and Scott\nKuindersma, editors, Robotics: Science and Systems\nXIII, Massachusetts Institute of Technology, Cam-\nbridge, Massachusetts, USA, July 12-16, 2017, 2017.\ndoi: 10.15607/RSS.2017.XIII.058.\nURL http://www.\nroboticsproceedings.org/rss13/p58.html.\nParsa Mahmoudieh, Deepak Pathak, and Trevor Dar-\nrell. Zero-shot reward specification via grounded\nnatural language. In Kamalika Chaudhuri, Stefanie\nJegelka, Le Song, Csaba Szepesv\u00e1ri, Gang Niu, and\nSivan Sabato, editors, International Conference on\nMachine Learning, ICML 2022, 17-23 July 2022, Bal-\ntimore, Maryland, USA, volume 162 of Proceedings\nof Machine Learning Research, pages 14743\u201314752.\nPMLR, 2022. URL https://proceedings.mlr.press/v162/\nmahmoudieh22a.html.\nMayank Mittal, Calvin Yu, Qinxi Yu, Jingzhou Liu,\nNikita Rudin, David Hoeller, Jia Lin Yuan, Poo-\n12\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nria Poorsarvi Tehrani, Ritvik Singh, Yunrong Guo,\nHammad Mazhar, Ajay Mandlekar, Buck Babich,\nGavriel State, Marco Hutter, and Animesh Garg. Or-\nbit: A unified simulation framework for interactive\nrobot learning environments, 2023.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver,\nAlex Graves, Ioannis Antonoglou, Daan Wierstra,\nand Martin Riedmiller. Playing atari with deep rein-\nforcement learning. arXiv preprint arXiv:1312.5602,\n2013.\nAnusha Nagabandi, Kurt Konolige, Sergey Levine, and\nVikash Kumar. Deep dynamics models for learn-\ning dexterous manipulation. In Leslie Pack Kael-\nbling, Danica Kragic, and Komei Sugiura, editors,\n3rd Annual Conference on Robot Learning, CoRL 2019,\nOsaka, Japan, October 30 - November 1, 2019, Proceed-\nings, volume 100 of Proceedings of Machine Learn-\ning Research, pages 1101\u20131112. PMLR, 2019. URL\nhttp://proceedings.mlr.press/v100/nagabandi20a.html.\nAshvin Nair and Vitchyr Pong. rlkit. Github. URL\nhttps://github.com/rail-berkeley/rlkit.\nAshvin Nair, Murtaza Dalal, Abhishek Gupta, and\nSergey Levine. Accelerating online reinforcement\nlearning with offline datasets, 2020.\nIvaylo Popov, Nicolas Heess, Timothy Lillicrap, Roland\nHafner, Gabriel Barth-Maron, Matej Vecerik, Thomas\nLampe, Yuval Tassa, Tom Erez, and Martin Riedmiller.\nData-efficient deep reinforcement learning for dex-\nterous manipulation. arXiv preprint arXiv:1704.03073,\n2017.\nRafael Rafailov, Tianhe Yu, Aravind Rajeswaran, and\nChelsea Finn. Offline reinforcement learning from\nimages with latent space models. In Ali Jadbabaie,\nJohn Lygeros, George J. Pappas, Pablo A. Parrilo,\nBenjamin Recht, Claire J. Tomlin, and Melanie N.\nZeilinger, editors, Proceedings of the 3rd Annual\nConference on Learning for Dynamics and Control,\nL4DC 2021, 7-8 June 2021, Virtual Event, Switzer-\nland, volume 144 of Proceedings of Machine Learn-\ning Research, pages 1154\u20131168. PMLR, 2021. URL\nhttp://proceedings.mlr.press/v144/rafailov21a.html.\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta,\nGiulia Vezzani, John Schulman, Emanuel Todorov,\nand Sergey Levine. Learning complex dexterous\nmanipulation with deep reinforcement learning and\ndemonstrations. In Proceedings of Robotics: Science\nand Systems, Pittsburgh, Pennsylvania, June 2018.\ndoi: 10.15607/RSS.2018.XIV.049.\nMartin Riedmiller, Thomas Gabel, Roland Hafner, and\nSascha Lange. Reinforcement learning for robot soc-\ncer. Autonomous Robots, 27:55\u201373, 2009.\nGerrit Schoettler, Ashvin Nair, Jianlan Luo, Shikhar\nBahl, Juan Aparicio Ojea, Eugen Solowjow, and\nSergey Levine. Deep reinforcement learning for in-\ndustrial insertion tasks with visual inputs and natural\nrewards. In 2020 IEEE/RSJ International Conference\non Intelligent Robots and Systems (IROS), pages 5548\u2013\n5555, 2020. doi: 10.1109/IROS45743.2020.9341714.\nTakuma Seno and Michita Imai. d3rlpy: An offline deep\nreinforcement learning library. Journal of Machine\nLearning Research, 23(315):1\u201320, 2022. URL http://\njmlr.org/papers/v23/22-0017.html.\nArchit Sharma, Kelvin Xu, Nikhil Sardana, Ab-\nhishek Gupta, Karol Hausman, Sergey Levine, and\nChelsea Finn.\nAutonomous reinforcement learn-\ning: Benchmarking and formalism. arXiv preprint\narXiv:2112.09605, 2021.\nArchit Sharma, Ahmed M. Ahmed, Rehaan Ahmad,\nand Chelsea Finn. Self-improving robots: End-to-\nend autonomous visuomotor reinforcement learning.\nCoRR, abs/2303.01488, 2023. doi: 10.48550/arXiv.2303.\n01488. URL https://doi.org/10.48550/arXiv.2303.01488.\nOren Spector and Dotan Di Castro. Insertionnet \u2013 a\nscalable solution for insertion, 2021.\nJonas Tebbe, Lukas Krauch, Yapeng Gao, and Andreas\nZell.\nSample-efficient reinforcement learning in\nrobotic table tennis. In 2021 IEEE international confer-\nence on robotics and automation (ICRA), pages 4171\u2013\n4178. IEEE, 2021.\nMel Vecerik, Oleg Sushkov, David Barker, Thomas\nRoth\u00f6rl, Todd Hester, and Jon Scholz. A practical\napproach to insertion with variable socket position\nusing deep reinforcement learning, 2018.\nTyler Westenbroek, Fernando Castaneda, Ayush\nAgrawal, Shankar Sastry, and Koushil Sreenath. Lya-\npunov design for robust and efficient robotic rein-\nforcement learning. arXiv preprint arXiv:2208.06721,\n2022.\nPhilipp Wu, Alejandro Escontrela, Danijar Hafner,\nPieter Abbeel, and Ken Goldberg.\nDaydreamer:\n13\nSERL: A Software Suite for Sample-Efficient Robotic Reinforcement Learning\nWorld models for physical robot learning. In Karen\nLiu, Dana Kulic, and Jeffrey Ichnowski, editors, Con-\nference on Robot Learning, CoRL 2022, 14-18 December\n2022, Auckland, New Zealand, volume 205 of Proceed-\nings of Machine Learning Research, pages 2226\u20132240.\nPMLR, 2022. URL https://proceedings.mlr.press/v205/\nwu23c.html.\nAnnie Xie, Fahim Tajwar, Archit Sharma, and\nChelsea Finn.\nWhen to ask for help:\nProac-\ntive\ninterventions\nin\nautonomous\nreinforce-\nment learning.\nIn NeurIPS, 2022.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\n6bf82cc56a5fa0287c438baa8be65a70-Abstract-Conference.\nhtml.\nYuxiang Yang, Ken Caluwaerts, Atil Iscen, Tingnan\nZhang, Jie Tan, and Vikas Sindhwani. Data efficient\nreinforcement learning for legged robots. In Confer-\nence on Robot Learning, pages 1\u201310. PMLR, 2020.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian,\nKarol Hausman, Chelsea Finn, and Sergey Levine.\nMeta-world: A benchmark and evaluation for multi-\ntask and meta reinforcement learning. In Leslie Pack\nKaelbling, Danica Kragic, and Komei Sugiura, edi-\ntors, 3rd Annual Conference on Robot Learning, CoRL\n2019, Osaka, Japan, October 30 - November 1, 2019,\nProceedings, volume 100 of Proceedings of Machine\nLearning Research, pages 1094\u20131100. PMLR, 2019.\nURL http://proceedings.mlr.press/v100/yu20a.html.\nAlbert Zhan, Ruihan Zhao, Lerrel Pinto, Pieter Abbeel,\nand Michael Laskin.\nA framework for efficient\nrobotic manipulation. In Deep RL Workshop NeurIPS\n2021, 2021.\nTony Z. Zhao, Jianlan Luo, Oleg Sushkov, Rugile Pevce-\nviciute, Nicolas Heess, Jon Scholz, Stefan Schaal, and\nSergey Levine. Offline meta-reinforcement learning\nfor industrial insertion. In 2022 International Confer-\nence on Robotics and Automation (ICRA), pages 6386\u2013\n6393, 2022. doi: 10.1109/ICRA46639.2022.9812312.\nHenry Zhu, Abhishek Gupta, Aravind Rajeswaran,\nSergey Levine, and Vikash Kumar. Dexterous ma-\nnipulation with deep reinforcement learning: Effi-\ncient, general, and low-cost. In International Confer-\nence on Robotics and Automation, ICRA 2019, Mon-\ntreal, QC, Canada, May 20-24, 2019, pages 3651\u20133657.\nIEEE, 2019. doi: 10.1109/ICRA.2019.8794102. URL\nhttps://doi.org/10.1109/ICRA.2019.8794102.\nHenry Zhu, Justin Yu, Abhishek Gupta, Dhruv Shah,\nKristian Hartikainen, Avi Singh, Vikash Kumar, and\nSergey Levine. The ingredients of real world robotic\nreinforcement learning. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net,\n2020. URL https://openreview.net/forum?id=rJe2syrtvS.\n14\n"
  },
  {
    "title": "Divide and Conquer: Language Models can Plan and Self-Correct for Compositional Text-to-Image Generation",
    "link": "https://arxiv.org/pdf/2401.15688.pdf",
    "upvote": "10",
    "text": "Divide and Conquer: Language Models can Plan and Self-Correct for\nCompositional Text-to-Image Generation\nZHENYU WANG, Tsinghua University, China\nENZE XIE\u2217, Noah\u2019s Ark Lab, Huawei, China\nAOXUE LI, Noah\u2019s Ark Lab, Huawei, China\nZHONGDAO WANG, Noah\u2019s Ark Lab, Huawei, China\nXIHUI LIU, The University of Hong Kong, China\nZHENGUO LI, Noah\u2019s Ark Lab, Huawei, China\nA blue scooter is parked \nnear a curb in front of a \ngreen vintage car.\nCompAgent\n(ours)\nA yellow book \nand a red vase.\nPixArt-Alpha\nPlayground-v2\n(product)\nMidjourney\n(product)\nA bee on the right \nof a refrigerator.\nThe green plant is \non the left of the \nyellow lamp.\nThe blue mug is on top \nof the green coaster.\nStable \nDiffusion XL\nA cubic block and a \ntriangular building set.\nFig. 1. For compositional text-to-image generation, existing state-of-the-art text-to-image models often fail to generate images that accurately correspond\nto the description of text inputs, especially about object attributes and relationships. In comparison, our method manages to generate images with correct\nobjects and attributes according to the complex text prompts.\nDespite significant advancements in text-to-image models for generating\nhigh-quality images, these methods still struggle to ensure the controllability\nProject page: https://zhenyuw16.github.io/CompAgent/ .\n\u2217: corresponding author.\nMails: wangzy20@mails.tsinghua.edu.cn; xieenze@connect.hku.hk; lax@pku.edu.cn;\nwangzhongdao@huawei.com; xihuiliu@eee.hku.hk; Li.Zhenguo@huawei.com.\nof text prompts over images in the context of complex text prompts, espe-\ncially when it comes to retaining object attributes and relationships. In this\npaper, we propose CompAgent, a training-free approach for compositional\ntext-to-image generation, with a large language model (LLM) agent as its\ncore. The fundamental idea underlying CompAgent is premised on a divide-\nand-conquer methodology. Given a complex text prompt containing multiple\n, Vol. 1, No. 1, Article . Publication date: January 2024.\narXiv:2401.15688v2  [cs.CV]  30 Jan 2024\nconcepts including objects, attributes, and relationships, the LLM agent ini-\ntially decomposes it, which entails the extraction of individual objects, their\nassociated attributes, and the prediction of a coherent scene layout. These\nindividual objects can then be independently conquered. Subsequently, the\nagent performs reasoning by analyzing the text, plans and employs the tools\nto compose these isolated objects. The verification and human feedback\nmechanism is finally incorporated into our agent to further correct the po-\ntential attribute errors and refine the generated images. Guided by the LLM\nagent, we propose a tuning-free multi-concept customization model and\na layout-to-image generation model as the tools for concept composition,\nand a local image editing method as the tool to interact with the agent\nfor verification. The scene layout controls the image generation process\namong these tools to prevent confusion among multiple objects. Extensive\nexperiments demonstrate the superiority of our approach for compositional\ntext-to-image generation: CompAgent achieves more than 10% improvement\non T2I-CompBench, a comprehensive benchmark for open-world composi-\ntional T2I generation. The extension to various related tasks also illustrates\nthe flexibility of our CompAgent for potential applications.\nCCS Concepts: \u2022 Computing methodologies \u2192 Computer vision; Image\nmanipulation.\nAdditional Key Words and Phrases: Image Generation, Compositional Text-\nto-Image Generation, Diffusion Models, LLM Agent\n1\nINTRODUCTION\nRecent advancements in text-to-image generation [Chang et al. 2023;\nChen et al. 2023b; Ramesh et al. 2022; Rombach et al. 2022; Saharia\net al. 2022] have demonstrated the remarkable capability in creating\ndiverse and high-quality images based on language text inputs.\nHowever, even state-of-the-art text-to-image models often fail to\ngenerate images that accurately align with complex text prompts,\nwhere multiple objects with different attributes or relationships are\ncomposed into one scene [Chefer et al. 2023; Feng et al. 2023; Huang\net al. 2023]. As can be seen in Fig. 1, existing methods cannot create\ncorrect objects, attributes, or relationships within the generated\nimages given these complex text prompts.\nAddressing compositional text-to-image generation requires solv-\ning at least the following three issues: 1) Object types and quantities.\nDue to the presence of multiple objects, the generated images should\naccurately incorporate each object, avoiding issues such as incorrect\nobject types, omissions of objects, and discrepancies in object quan-\ntities. 2) Attribute binding. Objects inherently possess distinctive\nattributes, like \"color\", \"shape\" or \"texture\". It should be guaranteed\nthat these object attributes are meticulously preserved within the\ngenerated images, avoiding issues like attribute misalignment or\nleakage. 3) Object relationships. There can be interaction relation-\nships among the multiple objects, such as spatial relationships like\n\"left\", \"right\" or non-spatial ones like \"holding\", \"playing\". The gen-\neration process should encapsulate and convey these relationships\nwithin the resultant images with precision and fidelity.\nWhile existing text-to-image models do not possess the capability\nto address the aforementioned three issues, they do demonstrate\nproficiency in the generation of single objects, encompassing their\ndistinctive types and attributes. Motivated by this, we propose Com-\npAgent, a training-free approach founded upon the principle of\ndivide-and-conquer for compositional text-to-image generation, co-\nordinated by a large language model (LLM) agent. The fundamental\nidea revolves around breaking down intricate textual sentences into\nNo\ncorrect object attributes?\nYes\nA red hat is on \nthe left of a \nblue backpack.\nA red hat is on \nthe left of a \nblue backpack.\ndecompose\nplanning, self-correction\nplan / \ntool use\nagent\nverification\nself-correct\nfeedback\nediting\ntoolkits\n\u2026\ncustomization\nDiffusers\nlayout-to-image\ntext-to-image\n(a)\nNo\ncorrect object attributes?\nYes\nA red hat is on \nthe left of a \nblue backpack.\nA red hat is on \nthe left of a \nblue backpack.\ndecompose\nplanning, self-correction\nplan / \ntool use\nagent\nverification\nself-correct\nfeedback\nediting\ntool library\n\u2026\ncustomization\nDiffusers\nlayout-to-image\ntext-to-image\n(b)\nFig. 2. The overview of existing text-to-image generation methods\n(a) and our CompAgent (b). Existing methods generate images from\ntext prompts in a single step. In comparison, guided by the LLM agent,\nCompAgent divides complex text prompts into individual objects, conquers\nthem separately, and composes them into final images with the tool library.\ntheir constituent individual objects, initially ensuring the correct-\nness of these isolated objects, then composing them together to\nproduce the final images. The overview is illustrated in Fig. 2.\nThe core of our method is an AI agent implemented by LLM,\nwhich serves as the \"brain\" of the framework and is primarily re-\nsponsible for the following tasks: 1) Decomposition. The agent de-\ncomposes the complex compositional sentence, extracting and cata-\nloging all objects and their associated attributes. Simultaneously, it\ndesigns the layout of the scene, defining the positions of the objects\nthrough the specification of bounding boxes. The text-to-image\ngeneration models are then engaged to generate images for each\nindividual object. 2) Planning and Tool use. The LLM agent conducts\nreasoning according to the complex text prompt and then formu-\nlates a strategic approach to image generation that is contingent\nupon the presence of object attributes and relationships. Then it\nemploys external tools to perform image generation or editing. 3)\nVerification and Feedback. By leveraging the vision ability of LLM\nor other visual models, the agent further scrutinizes the generated\nimages, discerns and rectifies potential attribute errors. Additionally,\nhuman feedback can be seamlessly incorporated into scene layout\nrefinement, thereby enhancing the quality of the ultimate outputs.\nGuided by the LLM agent, we introduce three tools to com-\npose multiple individual objects into a single cohesive image ac-\ncording to the scene layout. The first is about tuning-free multi-\nconcept customization. Specifically, we impose spatial layout con-\nstraints by cross-attention map editing and a pre-trained Control-\nNet [Zhang et al. 2023] on a tuning-free single-concept customiza-\ntion network [Li et al. 2023a] for supporting multiple objects. It\nregards previously generated images of individual objects as user-\nspecified subjects to create customized images. In this way, the object\nattributes can be guaranteed. The second is a layout-to-image model.\nThrough latent updating [Xie et al. 2023], images are generated\nwith the bounding box layout as the condition. The specification of\nobject types and quantities is made feasible through the imposition\nof layout conditions, enabling the model to place emphasis on the\naccurate representation of object relationships. However, the layout-\nto-image model may not necessarily produce object attributes with\nabsolute accuracy. Therefore, we further present the third tool, lo-\ncal image editing. The agent examines the objects with attribute\nerrors in the generated images, which will be masked out through a\nsegmentation model [Kirillov et al. 2023]. Through cross-attention\ncontrol, subject-driven editing is conducted through the previous\nmulti-concept customization network to replace the erroneous ob-\njects with their correct attribute counterparts. In addition to the\naforementioned three tools, our toolkit also involves existing text-to-\nimage generation models and vision-language multi-modal models,\nto handle simple text prompts and assess attribute correctness.\nOur main contributions can be summarized as follows:\n\u2022 We propose CompAgent to address compositional text-to-\nimage generation through the divide-and-conquer approach.\nThe LLM agent oversees the entire task, performing decom-\nposition, reasoning and overall planning, and tool library use\nto solve complex cases in text-to-image generation.\n\u2022 By employing both global and local layout control about the\nspatial arrangement, we propose a tuning-free multi-concept\ncustomization model to address the attribute binding problem.\nWe also observe that the layout-to-image generation manner\ncan ensure the faithfulness of object relationships.\n\u2022 We introduce the verification and feedback mechanism into\nour LLM agent. By interacting with our designed local image\nediting tool, the potential attribute errors can be corrected\nand the generated images can be further refined.\nWe evaluate CompAgent on the recent T2I-CompBench bench-\nmark [Huang et al. 2023] for compositional text-to-image generation,\ninvolving extensive object attributes and relationships. Both quali-\ntative and quantitative results demonstrate the superiority of our\nmethod. We achieve more than 10% improvement in the metrics\nabout compositional text-to-image evaluation. Furthermore, Com-\npAgent can be flexibly extended to various related applications, like\nmulti-concept customized image generation, reference-based image\nediting, object placement, and so on.\n2\nRELATED WORK\nText-to-image generation. With the development of diffusion mod-\nels [Dhariwal and Nichol 2021; Ho et al. 2020], text-to-image gener-\nation has achieved remarkable success [Chen et al. 2023b; Rombach\net al. 2022; Saharia et al. 2022]. These models can typically gener-\nate highly natural and realistic images, but cannot guarantee the\ncontrollability of texts over images. Many subsequent works are\nthus proposed for controllable text-to-image generation. Control-\nNet [Zhang et al. 2023] controls Stable Diffusion with various con-\nditioning inputs like Canny edges, and [Voynov et al. 2023] adopts\nsketch images for conditions. Layout-to-image methods [Chen et al.\n2024; Li et al. 2023c; Lian et al. 2023; Xie et al. 2023] synthesize\nimages according to the given bounding boxes of objects. And some\nimage editing methods [Brooks et al. 2023; Chen et al. 2023a; Parmar\net al. 2023; Yang et al. 2023a] edit images according to the user\u2019s\ninstructions. Despite the success of these methods, they are still\nlimited in handling complex text prompts for image generation.\nLLM agent. Large language models (LLMs), like ChatGPT, GPT-\n4 [OpenAI 2023], Llama [Touvron et al. 2023a,b], have demonstrated\nimpressive capability in natural language processing. The involve-\nment of vision ability in GPT-4V [Yang et al. 2023b] further enables\nthe model to process visual data. Recently, LLMs begin to be adopted\nas agents for executing complex tasks. These works [Liu et al. 2023a;\nShen et al. 2023; Yang et al. 2023d] apply LLMs to learn to use tools\nfor tasks like visual interaction, speech processing, and more recent\nworks have extended to more impressive applications like software\ndevelopment [Qian et al. 2023], gaming [ FAIR], or APP use [Yang\net al. 2023c]. Different from them, we focus on the task of composi-\ntional text-to-image generation, and utilize a LLM agent for complex\ntext prompt analysis and method planning.\n3\nMETHOD\nThe overview of our CompAgent is illustrated in Fig. 3. The LLM\nagent coordinates the entire framework. It decomposes the complex\ntext prompt, analyzes the attributes within the text, and designs the\nplans for the tools to be used. Ultimately, it invokes tools to address\nthe challenges inherent in compositional text-to-image generation.\n3.1\nLLM Agent\nThe primary responsibilities of our LLM agent center around the\nexecution of tasks including decomposition, planning, tool use, as\nwell as the implementation of verification and feedback.\nComplex text prompt decomposition. The input text prompts usu-\nally contain multiple objects, each characterized by its distinct at-\ntributes. These objects may also interact with each other with spe-\ncific relationships. Existing image synthesis models cannot capture\nall these attributes and relationships simultaneously, making it diffi-\ncult to accurately generate images that align with the input texts.\nTo address the challenges, the LLM agent decomposes the com-\nplex texts. It extracts the discrete objects embedded within the text\nprompts, along with their associated attributes. The text-to-image\ngeneration model is then harnessed to generate several images for\nthese objects. Since only one object is involved, existing models\ncan typically generate images that match the attributes. The agent\nalso formulates the scene layout represented as captioned bounding\nboxes, where the position of each foreground object is specified in\nthe (x, y, width, height) format. The layout will guide the image\ngeneration process when composing separate objects together.\nTool use. Since the language model does not have the ability\nfor image generation, external tools are required to be utilized to\ncompose separate objects together for image generation:\n1) Tuning-free multi-concept customization. It regards previously\ngenerated images of individual objects as the target images, and\nproduces images that feature these specific objects according to\nthe input text prompts. For these objects, the presence of multiple\nimages, each characterizing them with accurate attributes, generally\nassures that the multi-concept customization model can effectively\nensure the fidelity of object attributes. However, the model tends to\nfocus excessively on preserving the features of the target images,\nthus could potentially overlook other information in the texts like\nobject relationships. Therefore, this tool can effectively address the\nattribute binding problem, but may not guarantee the relationships.\ntuning-free \nmulti-concept \ncustomization\nlayout-to-image \ngeneration\nYou are an intelligent research \nassistant. You have access to the \nfollowing tools to address the \ncompositional text-to-image \ngeneration problem.\n<tools>\nText prompt: \nA green bench and a blue bowl.\nA black chair is on top of a blue rug.\nlocal image \nediting\nattribute \nonly\nwith rela\non top o\nblack, bl\nverification\nhuman feedback\nblue rug\nlayout\nindividual \nobjects\ncompositional texts\nreasoning: text prompt analysis\nNo\nrelationship\nonly\nattribute + \nrelationship\ncorrect object types, numbers, attributes?\nappropriate layout?\ncorrect object attributes?\nindividual\nobjects\nobject\nattributes\nIndividual object images\nself-\ncorrect\nYes\nblack chair\ngreen bench\nblue bowl\nblue rug\nlayout\nindividual object images\nscene layout\ngreen bench\nblack chair\nblue bowl\nblue rug\ntext-to-image\ngeneration\nblack \nchair? \n\u2713\n\uf0fb\nblack \nchair? \nPrompt Input\nImage Output\nVerification and Feedback\nDecomposition\nPlanning\nTool Library\nsimple\nprompt\nFig. 3. The framework of CompAgent. Given the input containing the complex text prompt, the LLM agent conducts the decomposition and planning tasks\nto invoke external tools for image generation. It then performs verification or involves human feedback and interacts with the tools for image self-correction.\nThe final image output will well satisfy the requirements from the input text prompt.\n2) Layout-to-image generation. It generates images with the previ-\nously established scene layout as the condition, and does not utilize\nthe target images of objects. The object types and quantities can be\nspecified through the scene layout, thereby facilitating the model\u2019s\nenhanced attention to information beyond the objects, i.e. the object\nrelationships. As a result, such a layout-to-image generation tool\ncan address the object relationship problem well, but the layout\nguidance only is not enough to guarantee the object attributes.\n3) Local image editing. Since the layout-to-image generation\nmodel may not consistently produce objects with correct attributes,\nwe further design the local image editing tool to replace the ob-\nject with the correct attribute one. Previously generated images of\nindividual objects are leveraged here as the reference for object re-\nplacement. This tool will interact reciprocally with the verification\nmechanism of the agent, collaboratively determining which object\nrequires to be modified.\n4) Text-to-image generation. Existing text-to-image generation\nmodels are utilized to generate individual object images during the\ndecomposition step. They will also be used as the tool to generate\nimages given simple text prompts for non-compositional generation.\n5) Vision-text multi-modal model. It assesses whether the object\nattributes in the images are correct for the verification function by\nleveraging the visual question answering ability of these models.\nPlanning. In light of the diverse proficiency exhibited by the tools,\nthe strategic selection of tool deployment is a core responsibility of\nour LLM agent. It mainly analyzes the input text prompt. If the text\nprimarily centers around object attributes and the relationships are\nrelatively straightforward (like spatial relationships \"left\", \"right\"),\nthe customization tool will be employed. If object relationships are\ncontained while their attributes are simple (like the naive \"white\"\nsnow color attribute), the layout-to-image generation tool is suitable.\nIf the text involves both attributes and relationships, the LLM agent\nfirst employs the layout-to-image model to represent object rela-\ntionships. Then it will leverage the vision-text multi-modal models\nto scrutinize the correctness of object attributes for verification, and\ndecide whether to adopt the local image editing tool. For simple text\nprompts with only straightforward attributes or relationships, the\ntext-to-image generation tool will be directly utilized.\nVerification and feedback. Considering the potential limitation of\nthe layout-to-image generation tool in accurately rendering object\nattributes, a verification process becomes imperative. We employ\nexisting vision-language multi-modal models like GPT-4V [Yang\net al. 2023b], MiniGPT-4 [Zhu et al. 2023], LLaVA [Liu et al. 2023b],\nand query it about whether the attributes are correct. If the attributes\nof some objects are incorrect, the LLM agent will invoke the local\nimage editing tool to substitute the objects with the correct ones.\nBesides, for too complicated text prompts with a higher num-\nber of objects, together with intricate attributes and relationships,\nrelying solely on the agent to automatically decompose the text\nand design scene layout may not necessarily be entirely accurate.\nIn this situation, human feedback can be involved. Humans can\nmake adjustments to the scene layout, such as inappropriate ob-\nject sizes, positions, missing or extra objects. They can also make\nmodifications to planning and verification errors. Introducing hu-\nman feedback in the form of layout can reduce the cost of human\nlabor. The accommodation of our framework about human feedback\nmakes our LLM agent more flexible for compositional generation.\n3.2\nTuning-Free Multi-Concept Customization\nIn this section, we mainly introduce our tuning-free multi-concept\ncustomization tool. Its overview is illustrated in Fig. 4. Training\na tuning-free customization image generation model typically re-\nquires large-scale pre-training for subject representation learning.\nCurrently, there are already tuning-free methods available that sup-\nport single-concept customization. For our approach, we build upon\nthe existing single-concept customization model, BLIP-Diffusion [Li\net al. 2023a], and extend its capability to accommodate multiple con-\ncepts, aided by the incorporation of the scene layout. Remarkably,\nwe eliminate the need for large-scale upstream pre-training and\ndirectly construct a tuning-free multi-concept customization model\nto uphold the integrity of object attributes.\nA desk with \ntwo open \nlaptops and a \ncoffee mug.\nImage\nEncoder\nred book\nyellow vase\nU-Net\nlayout\nControlNet\nMulti-modal \nEncoder\nA red book and a \nyellow vase.\nControlNet-guided \nlayout control\nText\nEncoder\nconcat\na\ncross-attention \nediting guided \nlayout control\nred\nbook\nand\na\nyellow\nvase\nControlNet\nU-Net\nTraining\nInference\nFig. 4. Illustration of our tuning-free multi-concept customization\ntool. ControlNet and cross-attention editing control the scene layout. The\nembeddings of multiple image concepts and the text prompt are concate-\nnated together and forwarded into U-Net for image generation.\nSpecifically, for each concept, we extract its subject prompt em-\nbedding with the BLIP-2 encoders and the multi-modal encoder [Li\net al. 2023b]. To gain a comprehensive understanding of the precise\nobject attributes, we harness the information contained within mul-\ntiple images corresponding to a single concept. We collect all the\nembeddings derived from these images and compute their average,\ngenerating the definitive subject prompt embedding for subsequent\nuse. They are concatenated with the text prompt embedding and for-\nwarded into the U-Net of the single-concept customization model [Li\net al. 2023a] for image generation.\nHowever, directly aggregating embeddings from multiple con-\ncepts can easily lead to interference between different objects during\nimage generation, resulting in the issue of concept confusion. To\navoid this, we leverage the scene layout to regulate the position-\ning of each object, thereby mitigating the risk of their interference.\nWe employ two levels of layout control - globally and locally. As\nis seen in the top part of Fig. 4, we mask the background of the\nCOCO dataset [Lin et al. 2014], and train a ControlNet [Zhang et al.\n2023] via a layout-to-image paradigm. The ControlNet is utilized\nto control the U-Net via residuals. It provides strong control at the\nglobal level, effectively distinguishing multiple objects, thus well\navoiding their confusion. However, it can only control globally and\ncannot independently control the position of each individual object.\nFor local layout control of individual objects separately, we fur-\nther propose to edit the cross-attention map according to the scene\nlayout, motivated by the fact that the cross-attention map directly\naffects the spatial layout of the generated image [Hertz et al. 2023].\nSpecifically, we collect the cross-attention map of each object words\nand their attribute words. We add a positive constant number \ud835\udefc+\nto the regions corresponding to the presence of the object, while\nconcurrently adding a negative constant number \ud835\udefc\u2212 to the rest\nregions. Compared to ControlNet, cross-attention editing realizes\nsignificantly weaker layout control but can independently govern\nthe position of each object. Therefore, when synergistically inte-\ngrated with ControlNet, it effectively controls the overall layout\nImage\nEncoder\nblue mug\nU-Net\nMulti-modal \nEncoder\nThe green plant is on top \nof the silver nightstand.\nText\nEncoder\nconcat\ncross-attention \nediting guided \ncontrol\nDDIM \ninversion\nlatent\nGrounding DINO   \n+ SAM\nFig. 5. Illustration of our local image editing tool. The concept images\npass into the customization network for embedding extraction, and the\nmasked segmentation map guides the image generation process.\nof the entire image. Ultimately, guided by the layout, different ob-\njects can be distinguished from each other, avoiding the confusion\nproblem and achieving multi-concept customization.\n3.3\nLayout-to-Image Generation\nTo guarantee object relationships, we generate images directly from\nthe scene layout. While our previously employed ControlNet and\ncross-attention editing approach can indeed tackle the layout-to-\nimage problem, they are characterized by the imposition of too\nstrong constraints upon the layout. Once the scene layout devi-\nates from the precise depiction of object relationships, it becomes\nchallenging to guarantee the accurate representation of these rela-\ntionships. Therefore, we utilize the strategy of latent updating by\nbackwarding the box-constrained loss [Xie et al. 2023] for image\ngeneration from the layout. It provides a relatively loose control over\nthe layout, thus allowing a flexible assurance of object relationships.\n3.4\nLocal Image Editing\nTo rectify objects with incorrect attributes, we introduce our local\nimage editing tool, as illustrated in Fig. 5. By querying our LLM\nagent for verification, we can identify which object attributes are\nerroneous and require modification. We utilize the combination of\nGrounding DINO [Liu et al. 2023c] and SAM [Kirillov et al. 2023] to\nsegment the object out. The resulting segmented mask is utilized\nfor cross-attention editing to provide position guidance for image\nediting. The image requiring editing is reconverted into the latent\nthrough DDIM inversion, serving as the initial latent for the subse-\nquent image generation process. Images featuring objects character-\nized by correct attributes have already been generated earlier. These\nimages, together with the text prompts, are processed in a manner\nanalogous to the previous customization model, which serves as the\nconditional input for the U-Net. The process of image generation\ngenerally follows the previous multi-concept customization, with\nthe image DDIM inversion as the initial latent. The segmentation\nmasks are used as the guidance for cross-attention editing, while\nControlNet is not employed. In this way, objects with incorrect\nattributes can be effectively substituted and rectified.\n4\nRESULTS\nWe mainly conduct experiments on the recent T2I-CompBench\nbenchmark [Huang et al. 2023], which mainly divides compositional\ntext prompts into six sub-categories. For quantitative comparison,\nTable 1. Quantitative Comparison on T2I-CompBench with existing\ntext-to-image generation models and compositional methods. Our\nmethod demonstrates superior compositional generation ability in both\nattribute binding, object relationships, and complex compositions. The best\nvalue is bolded, and the second-best value is underlined.\nModel\nAttribute Binding\nObject Relationship\nComplex\u2191\nColor \u2191\nShape\u2191\nTexture\u2191\nSpatial\u2191\nNon-Spatial\u2191\nStable v1.4\n0.3765\n0.3576\n0.4156\n0.1246\n0.3079\n0.3080\nStable v2\n0.5065\n0.4221\n0.4922\n0.1342\n0.3096\n0.3386\nDALL-E 2\n0.5750\n0.5464\n0.6374\n0.1283\n0.3043\n0.3696\nSDXL\n0.6369\n0.5408\n0.5637\n0.2032\n0.3110\n0.4091\nPixArt-\ud835\udefc\n0.6886\n0.5582\n0.7044\n0.2082\n0.3179\n0.4117\nDALL-E 3\n0.8110\n0.6750\n0.8070\n-\n-\n-\nComposable Diffusion\n0.4063\n0.3299\n0.3645\n0.0800\n0.2980\n0.2898\nAttn-Mask-Control\n0.4119\n0.4649\n0.4505\n0.1249\n0.3046\n0.3779\nStructureDiffusion\n0.4990\n0.4218\n0.4900\n0.1386\n0.3111\n0.3355\nTokenCompose\n0.5055\n0.4852\n0.5881\n0.1815\n0.3173\n0.2937\nAttn-Exct\n0.6400\n0.4517\n0.5963\n0.1455\n0.3109\n0.3401\nGORS\n0.6603\n0.4785\n0.6287\n0.1815\n0.3193\n0.3328\nECLIPSE\n0.6119\n0.5429\n0.6165\n0.1903\n0.3139\n-\nLMD\n0.4838\n0.5266\n0.6215\n0.4594\n0.2735\n0.3827\nCompAgent (ours)\n0.8488\n0.7233\n0.7916\n0.4837\n0.3212\n0.4863\nwe following its setting, and utilize the BLIP-VQA metric for at-\ntribute binding evaluation, the UniDet-based metric for spatial re-\nlationship evaluation, CLIPScore for non-spatial relationship, and\nthe 3-in-1 metric for complex prompts. We also include qualitative\nresults in both the main paper and supplementary materials.\n4.1\nQuantitative Comparison\nWe list the quantitative metric results of our CompAgent in Tab. 1.\nWe compare with existing state-of-the-art text-to-image synthesis\nmethods and models that are designed for complex text prompts.\nFor text-to-image generation, we compare with the recent Sta-\nble Diffusion [Rombach et al. 2022] v1.4, v2 and XL [Podell et al.\n2023] model, DALL-E 2 [Ramesh et al. 2022], PixArt-\ud835\udefc [Chen et al.\n2023b] and DALL-E 3 [Betker et al. 2023]. For compositional text-\nto-image generation methods, we compare with Composable Dif-\nfusion [Liu et al. 2022], StructureDiffusion [Feng et al. 2023], Attn-\nMask-Control [Wang et al. 2023a], GORS [Huang et al. 2023]. To-\nkenCompose [Wang et al. 2023b], Attn-Exct [Chefer et al. 2023],\nECLIPSE [Patel et al. 2023] and LMD [Lian et al. 2023] target at\nmultiple objects within a sentence so we also compare with them.\nFor attribute binding, our method achieves significantly higher\nBLIP-VQA metric compared to previous methods, 16.02%, 16.51% and\n8.72% higher for the color, shape and texture attributes compared\nwith PixArt-\ud835\udefc respectively. Compared to DALL-E 3, the current state-\nof-the-art method for controllability in text-to-image generation,\nour approach performs on par with it in terms of the performance.\nFor the color and shape attributes, our CompAgent is even 3.78%\nand 4.83% higher than it, based on the Stable Diffusion model. This\ndemonstrates the capability of CompAgent to accurately generate\nobject types as well as their attributes. Compared to single-step\ngeneration, the superiority of such the divide-and-conquer multi-\nstep generation manner can thus be observed in attribute binding.\nFor object relationships, CompAgent excels in both spatial and non-\nspatial relationships. In contrast, previous methods either lack such\nability or are limited to handling only a single type of relationship.\nWith the ability for both attribute binding and object relationship,\nour CompAgent can well address complex text prompts: we achieve\nthe 48.63% 3-in-1 metric, which surpasses previous methods by more\nTable 2. Ablation study on T2I-CompBench about LLM agent plan-\nning and verification. \"Customization\" denotes our tuning-free multi-\nconcept customization tool, and \"layout-to-image\" denotes our layout-to-\nimage generation tool.\nModel\nColor \u2191\nShape\u2191\nTexture\u2191\nComplex \u2191\ncustomization\n0.8160\n0.6839\n0.7692\n0.4184\nlayout-to-image\n0.6626\n0.5505\n0.6707\n0.4387\n+ planning\n0.8458\n0.7211\n0.7916\n0.4642\n+ verification\n0.8488 (+3.28%)\n0.7233 (+3.94%)\n0.7916 (+2.24%)\n0.4863 (+4.76%)\nthan 7%. The quantitative results well demonstrate that our method\neffectively addresses the challenges associated with compositional\ntext-to-image generation.\n4.2\nAblation Study\nWe further conduct ablation study in this section to analyze the\neffect of the LLM agent under our framework.\nLLM agent planning and verification. We first analyze the effect of\nthe planning and verification mechanism of our LLM agent in Tab. 2.\nIt can be observed that by leveraging the individual object images,\nthe multi-concept customization tool performs well for attribute\nbinding. However, because of the utilized ControlNet, the customiza-\ntion model can be inflexible for expressing the object relationships,\nwhich leads to its limited metric scores in complex compositions. In\ncomparison, the layout-to-image generation tool can well generate\nimages with object relationships, but cannot guarantee the accuracy\nof object attributes. Our LLM agent can analyze the complex text\nprompts, and plan the most suitable tool to use. As a result, LLM\nagent planning well helps address most situations for compositional\ntext-to-image generation. The verification mechanism of our LLM\nagent further helps correct some attribute errors, especially in com-\nplex compositions where layout-to-image generation cannot handle\nthe object attributes, which thus contributes to the 2.21% improve-\nment. As we can see, by planning and verification, our LLM agent\nwell utilizes the tools for compositional text-to-image generation.\nWe provide some visualized examples in Fig. 6. As we can see,\nthe LLM agent well plans and employs the most suitable tool. The\ncustomization tool is utilized to strictly constrain object attributes,\nand the layout-to-image tool can make appropriate adjustments to\nthe layout for object relationships. Besides, the local image editing\ntool can assist in rectifying objects with incorrect attributes.\nHuman feedback. We further provide some visualized examples\nin Fig. 7 to show the effect of human feedback. For the first example,\nalthough object types and attributes are correct, the size of the glass\ncup is too large. By involving human feedback to modify the scene\nlayout, such a problem can be addressed. For the second example,\nthe texts are quite complex and there are some mistakes in the scene\nlayout - one less hot dog, small table and too small car. Humans can\ninspect and correct them, then CompAgent can generate accurate\nimages. This also applies to the third example. Our CompAgent can\nincorporate human feedback, enabling it to generate more realistic\nimages and handle more complex text prompts.\n4.3\nQualitative Comparison\nWe provide more visualized results in Fig. 8, Fig. 9, Fig. 10. CompA-\ngent generates correctly for object types, attributes and relationships.\nA blue backpack and a red chair.\nA person is looking at a rainbow and marveling at its beauty.\nThe blue water bottle is on top of the red backpack.\nAttribute Binding\nObject Relationship\nComplex Composition\nA couch on the right of a chicken.\nThe silver clock is resting on the brown nightstand.\ncustomization tool \u2713\nlayout-to-image tool\nA bathroom with red tile and a green shower curtain.\nlayout-to-image tool\ncustomization tool\nlayout-to-image tool \u2713\nlayout-to-image tool \u2713\ncustomization tool\ncustomization tool\nlayout-to-image tool \u2713\ncustomization tool\nlayout-to-image tool \u2713\nlocal  image editing tool \u2713\nlocal  image editing tool \u2713\ncustomization tool \u2713\nFig. 6. Visualized examples of CompAgent to show the generated images from different tools and how LLM agent plans to use the tools. The LLM\nagent analyzes the text prompt. It employs the customization tool to address attribute binding, and the layout-to-image tool to address object relationships.\nFor complex composition, the layout-to-image tool is utilized for object relationship, then the local image editing tool is used for attribute correction.\nA leather chair and a glass cup.\nTwo hot dogs sit on a white paper plate near a \nsoda cup which are sitting on a green picnic table \nwhile a bike and a white car are parked nearby.\nThe glass candle holder and metallic \ncandlestick illuminate the wooden dining \ntable on the fabric tablecloth.\nw/o human \nfeedback\nw/ human \nfeedback\na leather chair\na glass cup\na leather chair\na glass cup\nreduce the glass cup size\na bike\na soda cup\na green picnic table\nhot dog\nhot dog\na white car\na bike\na soda cup\na white paper plate\nhot dog\na white car\na green picnic table\nadd one more hot dog\nenlarge the picnic table size\nenlarge the car box size\na wooden dining table\na fabric tablecloth\na glass candle holder\na white paper plate\na metallic candlestick\na wooden dining table\na fabric tablecloth\na glass candle holder\na metallic candlestick\nmove the glass candle holder position\nFig. 7. Visualized examples to show the effect of human feedback. The green boxes are well-generated by the LLM agent. The red boxes are the ones\ngenerated by the LLM agent with some issues, which are modified through human feedback into the blue boxes in the second row.\nThe excellent ability of our method for compositional text-to-image\ngeneration can thus be further demonstrated.\nWe then provide the qualitative comparison with existing text-\nto-image generation methods and compositional text-to-image gen-\neration methods in Fig. 11. For the text \"a black guitar and a brown\namplifier\", existing methods are easy to confuse the color of the\nguitar and the amplifier. In the second example, where four objects\nexist in the text, the correct object number also cannot be guaran-\nteed for existing methods. For some uncommon attributes, like the\ntriangular shelf in the third example and the blue sink in the last\nexample, existing models are also easy to make mistakes about the\nattributes. Besides, in the fifth example, most of existing methods\ncannot express the \"left\" relationship accurately. Our CompAgent\ngenerates accurately for all these text prompts. This further demon-\nstrates the superiority of our method over existing models when it\ncomes to compositional text-to-image generation.\n5\nCONCLUSION\nIn this paper, we propose a training-free approach, CompAgent, for\ncompositional text-to-image generation. By decomposing, planning,\nverifying, and involving human feedback, the LLM agent coordi-\nnates the whole system and employs external tools to generate\nhigh-fidelity and accurate images according to the given complex\ntext prompts. Extensive results demonstrate that CompAgent well\naddresses the object type and quantity, attribute binding, and object\nrelationship problems in compositional text-to-image generation.\nWe consider CompAgent as an important step towards the future\nof autonomous agents empowered by language models and the\ncontrollability in text-to-image generation.\nA black and beige tile \nbathroom with a yellow \ntoilet and a black bucket on \nthe floor.\nA blue cup and \na brown horse.\nA black taxi and a \nyellow tire.\nA dining area features a \nwood table and chairs, a \nsilver refrigerator and \nbrown cabinets.\nA blue boat and \na red suitcase.\nA cubic block and a \ncylindrical jar of honey.\nAn oval coffee table \nand a rectangular rug.\nThe conical hat and the \ncrescent moon were \nthe only things visible \nin the night sky.\nA metallic jewelry \nand a fabric dress.\nThe fluffy rug and leather \nottoman rest on the \nwooden floor in the cozy \nreading nook.\nThe man on the large \npink motorcycle has \nblack chaps.\nThe leather chair and \nmetallic lamp provide comfort \nand light for the wooden desk \non the fluffy rug.\nFig. 8. Visualized results of our method for attribute binding.\nA child is playing with a toy \nairplane and making it fly.\nA balloon on the \nbottom of a chicken.\nA bowl on the right \nof a butterfly.\nA man is holding a compass \nand navigating through the \nwilderness.\nA cow near a plate.\nA frog on top of a cup.\nA candle on the \nleft of a mouse.\nA river is flowing \nthrough a canyon.\nA rabbit next to a train.\nA man is standing on a \nladder and painting a wall.\nA cat is wearing a \ncollar with a bell on it.\nA scientist is conducting \nexperiments in a high-tech lab.\nFig. 9. Visualized results of our method for object relationship.\nThe sharp black cat clawed \nat the soft red blanket.\nThe black chair is on \ntop of the blue rug.\nThe black camera was \nnext to the white tripod.\nThe soft pink blanket draped \nover the hard wooden chair.\nThe round clock is on the \nright of the wooden frame.\nThe red book is on top \nof the yellow bookshelf.\nThe blue bowl is on top \nof the white placemat.\nThe sleek black pen wrote \non the crisp white notepad.\nThe green plant was on the \nright of the white window.\nThe colorful hot air balloon \nfloated near the dark grey \nstorm clouds.\nThe smooth black leather \njacket hung next to the \nrugged brown boots.\nThe bright orange \nconstruction cones marked \nthe rough asphalt road.\nFig. 10. Visualized results of our method for complex composition.\nTwo black sheep, a \nwhite goat and a white \ngoat in a field.\nCompAgent\n(ours)\nA black guitar and \na brown amplifier.\nGORS\nA circular pendant light \nand a triangular shelf.\nA plastic toy and a \nmetallic jewelry.\nThe black sofa was \non the left of the \nwhite coffee table.\nThe rectangular \nmirror was hung \nabove the blue sink.\nLMD\nAttn-Exct\nPixArt-Alpha\nStable \nDiffusion XL\nMidjourney\nFig. 11. Qualitative comparison between our approach and previous methods. The first three lines are existing text-to-image generation models, and\nlines four to six are existing compositional text-to-image generation methods. Our approach performs significantly better in both attribute binding and object\nrelationship aspects compared to previous methods.\nA\nIMPLEMENTATION DETAILS\nThe main part of our experiments applies GPT-4 [OpenAI 2023]\nas our LLM agent. For our toolkits, the multi-modal models uti-\nlized for attribute verification are implemented by GPT-4V. The\nimage generation tools are implemented based on Stable Diffusion\n[Rombach et al. 2022]. The text-to-image generation tool and the\nlayout-to-image generation tool are based on Stable Diffusion v2,\nand the tuning-free multi-concept customization tool and the local\nimage editing tool are based on Stable Diffusion v1-4. For training\nControlNet [Zhang et al. 2023] in the layout-to-image manner, we\nutilize the COCO dataset [Lin et al. 2014] and train for 10 epochs.\n\ud835\udefc+ is set to 2.5 and \ud835\udefc\u2212 is set to -10,000 for cross attention editing.\nB\nAGENT PROMPT\nOur prompt input for the LLM agent mainly contains the following\nparts:\n1. Task specification:\nYou are an intelligent research assistant. You have access to\nthe following tools to address the compositional text-to-image\ngeneration problem:\n2. Tool library introduction:\n1) The multi-concept customization tool. It is good at texts where\nobjects are coupled complex attributes, like color, shape, texture.\nIt can also handle certain spatial relationships, like \u201con the left\nof, on the right of\u201d and some simple and straightforward object\nrelationships, like the mirror hanging above, the snow covering.\n2) The layout-to-image generation tool. It is good at texts where\nobjects are interacted with complicated relationships or actions,\nlike playing, holding. It can also process attributes that are easy\nor straightforward, like a red apple, the white snow.\n3) The text-to-image generation tool. It can handle those simple\ntexts without complex attributes and relationships.\n3. Decomposition details:\nI will provide you with a caption for an image. Your should also\nextract individual objects and generate the bounding boxes for\nthe objects mentioned in the caption. The images are of size\n512x512. The top-left corner has coordinate [0, 0]. The bottom-\nright corner has coordinate [512, 512]. The bounding boxes\nshould not go beyond the boundaries and it is better to avoid\noverlapping bounding boxes. Each bounding box should be in\nthe format of (object name, [top-left x coordinate, top-left y\ncoordinate, box width, box height]) and include exactly one\nobject (i.e., start the object name with a or an if possible). If\nneeded, you can make reasonable guesses. Please refer to the\nexample below for the desired format.\n4. In-context examples:\nQ1:\nCaption: a blue horse and a brown vase.\nA1:\nAnalysis: attribute-only.\nObjects: [(\u2019a blue horse\u2019, [50, 70, 220, 300]), (\u2019a brown vase\u2019, [300,\n113, 150, 250])]\nQ2:\nCaption: a fabric rug and a leather belt.\nA2:\nAnalysis: attribute-only.\nObjects: [(\u2019a fabric rug\u2019, [20, 200, 470, 150]), (\u2019a leather belt\u2019, [100,\n250, 300, 20])]\nQ3:\nCaption: A cat is wearing a collar with a bell on it.\nA3:\nAnalysis: relationship-only.\nObjects: [(\u2019a cat\u2019, [120, 150, 300, 300]), (\u2019a collar\u2019, [120, 300, 300,\n50]), (\u2019a bell, [250, 320, 110, 100])]\nQ4:\nCaption: The blue bowl was on top of the white placemat.\nA4:\nAnalysis: both.\nObjects: [(\u2019a rectangular mirror\u2019, [170, 80, 172, 100]), (\u2019a white\nsink\u2019, [150, 200, 212, 150])]\nQ5:\nCaption: The red apple was on top of the plate.\nA5:\nAnalysis: relationship-only.\nObjects: [(\u2019a red apple\u2019, [235, 230, 60, 60]), (\u2019a plate\u2019, [175, 210,\n180, 180])]\nAfter providing the above prompt to the agent, we then prompt\nthe LLM agent for completion:\nCaption: [input prompt from the user]\nThe resulting text analysis and decomposition results from the\nLLM agent is then parsed and used for the subsequent image gener-\nation process.\nC\nFURTHER ABLATIONS AND EXPERIMENTS\nC.1\nAgent Type\nTable 3. The impact of LLM agent type on T2I-CompBench for com-\npositional text-to-image generation.\nLLM Agent\nAttribute Binding\nObject Relationship\nComplex\u2191\nColor \u2191\nShape\u2191\nTexture\u2191\nSpatial\u2191\nNon-Spatial\u2191\nLlama-7B\n0.6994\n0.5740\n0.6927\n0.3138\n0.3102\n0.3515\nLlama-70B\n0.7400\n0.6305\n0.7102\n0.3698\n0.3104\n0.4475\nGPT-3.5\n0.7925\n0.6647\n0.7743\n0.4439\n0.3195\n0.4752\nGPT-4\n0.8488\n0.7233\n0.7916\n0.4837\n0.3212\n0.4863\nWe first conduct compositional text-to-image generation exper-\niments on the T2I-CompBench [Huang et al. 2023] guided by the\nagent implemented by various LLMs. The experimental results are\nlisted in Tab. 3. We adopt GPT-3.5, GPT-4 and the open-sourced\nLlama 2 [Touvron et al. 2023b].\nTable 4. Evaluation for layout-to-image generation. We adopt the YOLO\nscore to evaluate the correspondence of images and their layouts.\nMethods\nAP \u2191\nAP50 \u2191\nAP75 \u2191\nLostGAN\n0.053\n0.089\n0.056\nLAMA\n0.102\n0.153\n0.117\nTwFA\n0.106\n0.147\n0.126\nStable Diffusion\n0.028\n0.092\n0.011\nGLIGEN\n0.297\n0.458\n0.339\nGLIGEN + BoxDiff\n0.402\n0.620\n0.462\nlatent updating\n0.224\n0.468\n0.178\ncross-attention editing\n0.060\n0.190\n0.021\nControlNet\n0.338\n0.521\n0.339\nControlNet + cross-attention editing\n0.508\n0.778\n0.534\nIt can be seen that as the language understanding ability of large\nlanguage models improves, the performance of our CompAgent\nfor compositional text-to-image generation also enhances. When\nadopting GPT-4 as the agent, the metric reaches the highest. This\nis because GPT-4 can not only decompose individual objects suc-\ncessfully, but also generate the scene layout where the shapes of\nbounding boxes are more suitable for the object types. Meanwhile,\neven utilizing the Llama-7B model as our agent, the compositional\ntext-to-image generation metric is still higher than most of existing\nmethods. This demonstrates the flexibility of our CompAgent across\ndifferent large language models.\nC.2\nImage Generation with Layout Guidance\nFor composing multiple individual objects into the image, the gener-\nated scene layouts guide the generation process under our CompA-\ngent framework. We compare the performance of different layout-to-\nimage generation methods and list the results in Tab. 4. We compare\nwith previous methods, including LostGAN [Sun and Wu 2021],\nLAMA [Li et al. 2021], TwFA [Yang et al. 2022], Stable Diffusion\n[Rombach et al. 2022], GLIGEN [Li et al. 2023c] and GLIGEN + BoxD-\niff [Xie et al. 2023]. Under our CompAgent framework, we mainly\nutilize latent updating, cross-attention editing and ControlNet three\nstrategies. We conduct experiments on the benchmark proposed in\n[Xie et al. 2023]. We apply the YOLOv4 [Bochkovskiy et al. 2020] to\ndetect objects and obtain the YOLO score, including AP, AP50 and\nAP75 to evaluate the precision of the layout-to-image performance.\nIt can be seen that by utilizing cross-attention editing only to con-\ntrol image generation through bounding boxes, the obtained YOLO\nscore is quite low, only 0.06, slightly higher than Stable Diffusion.\nThis demonstrates that although editing cross-attention maps can\nprovide layout guidance, it is still insufficient to control image gen-\neration accurately. Utilizing our trained ControlNet is much more\neffective, achieving the 0.338 AP, 4.1% higher than GLIGEN. How-\never, ControlNet can only provide global control, not object-level.\nAfter combining ControlNet and cross-attention editing, the YOLO\nscore reaches to 0.508, 10.6% higher than GLIGEN + BoxDiff. This\ndemonstrates that our design can control the positions of objects\nmore accurately. This well avoids the confusion of multiple objects,\nthus is suitable for the attribute binding problem. However, the\nreliance on the scene layout is too strong in this way, making it\ninappropriate for generating correct object relationships. Instead,\nlatent updating can well follow the scene layout and can also be flex-\nible meanwhile. Therefore, we utilize the latent updating strategy\nfor the object relationship issue.\nC.3\nQualitative Comparison\nWe then provide more visualized comparisons with existing state-\nof-the-art text-to-image generation methods in Fig. 12. Existing\nmethods are highly prone to the following errors. 1) Attribute con-\nfusion. For example, for the text \"a blue backpack and a red book\",\nexisting models are easy to confuse the color of the backpack and\nthe red book, or generate a backpack with red parts. 2) Constrained\nby common attributes or scenarios. For example, for the second\nexample, existing models tend to generate a white sink since it is\nmore common in reality, rather than the brown one. For the third\nexample, existing methods also tend to generate a common living\nroom scenario, while ignoring the required objects and attributes.\n3) Incorrect relationship. For example, the \"left\" relationship can-\nnot be correctly expressed in the fifth example. In comparison, our\nCompAgent well avoids these problems, thus generates images that\nmore accurately align with the description of input texts.\nWe also provide visualized comparison with existing composi-\ntional text-to-image generation methods in Fig. 13. We can observe\nthat existing compositional text-to-image methods also cannot ad-\ndress the above mentioned problems. As a result, the correct object\ntypes and quantities, object attributes and relationships cannot be\nguaranteed. CompAgent behaves equally well for these examples.\nMore visualized examples are provided in Fig. 14, Fig. 15 and\nFig. 16. These examples further demonstrate the excellent ability\nof our CompAgent for addressing the compositional text-to-image\ngeneration problem.\nD\nEXTENSION TO OTHER TASKS\nBesides the compositional text-to-image generation task, our Com-\npAgent can also be flexibly extended to other related image gen-\neration tasks with the help of the LLM agent and our toolkits. We\nmainly introduce about the multi-concept customization, the image\nediting and the object placement tasks.\nD.1\nMulti-Concept Customization\nWe first conduct the multi-concept customization task to generate\nimages according to the input text prompts containing the given\nsubjects. We compare with existing state-of-the-art customization\nmethods, including DreamBooth [Ruiz et al. 2023], Custom Diffusion\n[Kumari et al. 2023] and Cones 2 [Liu et al. 2023d]. The comparison\nresults are provided in Fig. 17. It can be seen that DreamBooth and\nCustom Diffusion cannot generate the corresponding objects or their\ncorrect attributes. For example, for the first example, DreamBooth\ndoes not generate the vase in the image, and the object attributes\nfrom the Custom Diffusion image are incorrect. For the second\nexample, Custom Diffusion does not generate the cup, while Dream-\nBooth generates the cup with the incorrect color. Cones 2 performs\nbetter than them, generating accurate images with a red book and a\nyellow vase. However, it is also limited by common attributes. For\nexample, it does not generate the correct color for the cow in the\nsecond example, which also applies to the car in the third example.\nA small bathroom with a \nsmall white toilet next \nto a brown sink.\nCompAgent\n(ours)\nA blue backpack \nand a red book.\nPixArt-Alpha\nPlayground\n-v2\nMidjourney\nA plastic container \nand a glass jar.\nA bird on the left \nof a phone.\nThe round clock is \nnext to the white \npicture frame.\nStable \nDiffusion XL\nStable \nDiffusion v2\nAn oval coffee table \nand a square ottoman.\nFig. 12. Qualitative comparison between our approach and existing state-of-the-art text-to-image generation methods.\nBesides, in the fourth example, Cones 2 confuses the features of\nthe cat and the dog, thus generates the image with two dogs. In\ncomparison, our CompAgent accurately captures the subject fea-\ntures and avoids the object confusion problem, thus handles the\nmulti-concept customization task better. Note that these previous\nmethods are all tuning-based, while our CompAgent is tuning-free.\nTherefore, CompAgent can accurately address the customization\ntask more efficiently.\nD.2\nLocal Image Editing\nWe then conduct the local image editing experiments and compare\nwith the Paint-by-Example method [Yang et al. 2023a]. It can be seen\nthat although Paint-by-Example can perform the local image editing\ntask, it cannot precisely catch the object attributes. For example, for\nthe car example, the front window color from the Paint-by-Example\ngenerated image turns to blue. For the computer-desk example,\nPaint-by-Example does not edit the color of the table, and for the\nmirror-sink example, Paint-by-Example also does not modify the\nA blue book and \na red cake.\nCompAgent\n(ours)\nComposable\nDiffusion\nA blue bench and \na green bowl.\nGORS\nA big bathtub and a \nsquare showerhead.\nA leather shoe \nand a glass cup.\nThe pink phone case \nis on top of the \nwhite nightstand.\nThe rectangular \nbook was on top \nof the blue shelf.\nLMD\nAttn-Exct\nStructure\nDiffusion\nAttn-Mask-\nControl\nFig. 13. Qualitative comparison between our approach and existing compositional text-to-image generation methods.\nA pink bathroom with a \nmirror and a green sink.\nA blue blackpack\nand a red train.\nA black dove and \na white feather.\nThe tall ladder and the short \nstool were both necessary \nfor reaching high places.\nA cubic box and a \ncylindrical bottle of lotion.\nA metallic fork and \na wooden spoon.\nA brown chair and a \nred elephant.\nThe circular wreath and the \nrectangular picture frame \nhung on the front door.\nA fabric bag \nand a glass vase.\nA man in a gray jacket \nstanding in a kitchen \nnext to a black dog.\nA black and white cat \nsits in a white sink.\nA black motorcycle \nis parked on grey \ncobblestones.\nA couple is \nstargazing in a field.\nA bag on the \nright of a dog.\nA desk on the \nright of a horse.\nA person is playing \nguitar on the beach.\nA desk on top of a cat.\nA giraffe on the \nright of a bowl.\nA couch on the\nleft of a dog.\nA dog is playing with a \ntennis ball and running \nafter it.\nA frog on the\nleft of a book.\nA skier is gliding down \na snow-covered \nmountain.\nA cat is lazily lounging \nin a sunny windowsill.\nThe woodcarver is \ncreating a sculpture of a \nbird from a block of wood.\nThe fluffy white clouds \nfloated above the dark \ngrey city skyline.\nThe black chair was on the \nleft of the white table.\nThe fluffy cat is on the \nleft of the soft pillow.\nThe prickly green cactus \ncontrasted with the \nsmooth white sand.\nThe green plant was \nnext to the white vase.\nThe red apple was on \ntop of the black plate.\nThe bright blue bird \nperched on the rough \nbrown branch.\nThe long red scarf \ndraped over the short \nblack jacket.\nThe rectangular box is nestled \nbetween the cylindrical tube \nand the conical funnel.\nThe colorful parrot \nsat next to the \nsleek black cage.\nThe brown dog was \nlying on the blue mat.\nThe soft pink petals of the \nflower contrasted with the \nrough grey sidewalk.\nFig. 14. Visualized results of our method for attribute binding.\nA pink bathroom with a \nmirror and a green sink.\nA blue blackpack\nand a red train.\nA black dove and \na white feather.\nThe tall ladder and the short \nstool were both necessary \nfor reaching high places.\nA cubic box and a \ncylindrical bottle of lotion.\nA metallic fork and \na wooden spoon.\nA brown chair and a \nred elephant.\nThe circular wreath and the \nrectangular picture frame \nhung on the front door.\nA fabric bag \nand a glass vase.\nA man in a gray jacket \nstanding in a kitchen \nnext to a black dog.\nA black and white cat \nsits in a white sink.\nA black motorcycle \nis parked on grey \ncobblestones.\nA couple is \nstargazing in a field.\nA bag on the \nright of a dog.\nA desk on the \nright of a horse.\nA person is playing \nguitar on the beach.\nA desk on top of a cat.\nA giraffe on the \nright of a bowl.\nA couch on the\nleft of a dog.\nA dog is playing with a \ntennis ball and running \nafter it.\nA frog on the\nleft of a book.\nA skier is gliding down \na snow-covered \nmountain.\nA cat is lazily lounging \nin a sunny windowsill.\nThe woodcarver is \ncreating a sculpture of a \nbird from a block of wood.\nThe fluffy white clouds \nfloated above the dark \ngrey city skyline.\nThe black chair was on the \nleft of the white table.\nThe fluffy cat is on the \nleft of the soft pillow.\nThe prickly green cactus \ncontrasted with the \nsmooth white sand.\nThe green plant was \nnext to the white vase.\nThe red apple was on \ntop of the black plate.\nThe bright blue bird \nperched on the rough \nbrown branch.\nThe long red scarf \ndraped over the short \nblack jacket.\nThe rectangular box is nestled \nbetween the cylindrical tube \nand the conical funnel.\nThe colorful parrot \nsat next to the \nsleek black cage.\nThe brown dog was \nlying on the blue mat.\nThe soft pink petals of the \nflower contrasted with the \nrough grey sidewalk.\nFig. 15. Visualized results of our method for object relationship.\nA pink bathroom with a \nmirror and a green sink.\nA blue blackpack\nand a red train.\nA black dove and \na white feather.\nThe tall ladder and the short \nstool were both necessary \nfor reaching high places.\nA cubic box and a \ncylindrical bottle of lotion.\nA metallic fork and \na wooden spoon.\nA brown chair and a \nred elephant.\nThe circular wreath and the \nrectangular picture frame \nhung on the front door.\nA fabric bag \nand a glass vase.\nA man in a gray jacket \nstanding in a kitchen \nnext to a black dog.\nA black and white cat \nsits in a white sink.\nA black motorcycle \nis parked on grey \ncobblestones.\nA couple is \nstargazing in a field.\nA bag on the \nright of a dog.\nA desk on the \nright of a horse.\nA person is playing \nguitar on the beach.\nA desk on top of a cat.\nA giraffe on the \nright of a bowl.\nA couch on the\nleft of a dog.\nA dog is playing with a \ntennis ball and running \nafter it.\nA frog on the\nleft of a book.\nA skier is gliding down \na snow-covered \nmountain.\nA cat is lazily lounging \nin a sunny windowsill.\nThe woodcarver is \ncreating a sculpture of a \nbird from a block of wood.\nThe fluffy white clouds \nfloated above the dark \ngrey city skyline.\nThe black chair was on the \nleft of the white table.\nThe fluffy cat is on the \nleft of the soft pillow.\nThe prickly green cactus \ncontrasted with the \nsmooth white sand.\nThe green plant was \nnext to the white vase.\nThe red apple was on \ntop of the black plate.\nThe bright blue bird \nperched on the rough \nbrown branch.\nThe long red scarf \ndraped over the short \nblack jacket.\nThe rectangular box is nestled \nbetween the cylindrical tube \nand the conical funnel.\nThe colorful parrot \nsat next to the \nsleek black cage.\nThe brown dog was \nlying on the blue mat.\nThe soft pink petals of the \nflower contrasted with the \nrough grey sidewalk.\nFig. 16. Visualized results of our method for complex composition.\nCustomized Subjects\nCustom Diffusion\nA book and a vase.\nDreamBooth\nCones 2\nCompAgent (ours)\nA cow and a cup.\nA sheep and a car on the grass.\nA cat and a dog on the beach.\nFig. 17. Visualized results of our method for multi-concept customization. Note that DreamBooth, Custom Diffusion and Cones 2 are all tuning-based\nmethods, while our CompAgent is tuning-free.\nmirror shape from oval to rectangle. In comparison, our CompAgent\ncan well guarantee the object attributes, thus better addressing the\nlocal image editing task. Besides, in the cup-coaster example, the\nshape of the cup from Paint-by-Example looks weird. In comparison,\nthe generated image from our CompAgent looks more realistic.\nD.3\nObject Placement\nFinally we conduct the object placement task: to put a given object\ninto the specified position in an image. The comparison results with\nPaint-by-Example are illustrated in Fig. 19. Our method performs\nequally better. For example, for the vase example, Paint-by-Example\nonly generates the flowers and does not understand the object type.\nFor the bear-cup example, Paint-by-Example does not capture the\ncup color. Besides, Paint-by-Example also does not generate natural\nimages for the dog example. Our CompAgent performs better for\nall these examples, which further demonstrates its ability.\nREFERENCES\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang,\nJuntang Zhuang, Joyce Lee, Yufei Guo, et al. 2023. Improving image generation with\nbetter captions. Computer Science. https://cdn. openai.com/papers/dall-e-3.pdf (2023).\nAlexey Bochkovskiy, Chien-Yao Wang, and Hong-Yuan Mark Liao. 2020. Yolov4: Opti-\nmal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934 (2020).\nTim Brooks, Aleksander Holynski, and Alexei A Efros. 2023. Instructpix2pix: Learning\nto follow image editing instructions. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition.\nPaint-by-Example\nCompAgent (ours)\nPaint-by-Example\nCompAgent (ours)\nFig. 18. Visualized results of our method for local image editing.\na leather chair\nPaint-by-Example\nCompAgent (ours)\nPaint-by-Example\nCompAgent (ours)\nFig. 19. Visualized results of our method for object placement.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-\nHsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. 2023.\nMuse: Text-to-image generation via masked generative transformers. arXiv preprint\narXiv:2301.00704 (2023).\nHila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-\nand-excite: Attention-based semantic guidance for text-to-image diffusion models.\nACM Transactions on Graphics (2023).\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao\nWang, James Kwok, Ping Luo, Huchuan Lu, et al. 2023b. PixArt-\ud835\udefc: Fast Training of\nDiffusion Transformer for Photorealistic Text-to-Image Synthesis. arXiv preprint\narXiv:2310.00426 (2023).\nMinghao Chen, Iro Laina, and Andrea Vedaldi. 2024. Training-free layout control\nwith cross-attention guidance. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision.\nXi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao.\n2023a.\nAnydoor: Zero-shot object-level image customization.\narXiv preprint\narXiv:2307.09481 (2023).\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image\nsynthesis. Advances in neural information processing systems (2021).\nMeta Fundamental AI Research Diplomacy Team (FAIR)\u2020, Anton Bakhtin, Noam Brown,\nEmily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew Goff, Jonathan\nGray, Hengyuan Hu, et al. 2022. Human-level play in the game of Diplomacy by\ncombining language models with strategic reasoning. Science (2022).\nWeixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna\nNarayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. 2023. Training-\nFree Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. In\nInternational Conference on Learning Representations.\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel\nCohen-or. 2023. Prompt-to-Prompt Image Editing with Cross-Attention Control. In\nInternational Conference on Learning Representations.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in neural information processing systems (2020).\nKaiyi Huang, Kaiyue Sun, Enze Xie, Zhenguo Li, and Xihui Liu. 2023. T2i-compbench: A\ncomprehensive benchmark for open-world compositional text-to-image generation.\nAdvances in Neural Information Processing Systems (2023).\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\n2023. Segment anything. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision.\nNupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu.\n2023. Multi-concept customization of text-to-image diffusion. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition.\nDongxu Li, Junnan Li, and Steven CH Hoi. 2023a. Blip-diffusion: Pre-trained subject\nrepresentation for controllable text-to-image generation and editing. arXiv preprint\narXiv:2305.14720 (2023).\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023b. Blip-2: Bootstrapping\nlanguage-image pre-training with frozen image encoders and large language models.\narXiv preprint arXiv:2301.12597 (2023).\nYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao,\nChunyuan Li, and Yong Jae Lee. 2023c. Gligen: Open-set grounded text-to-image\ngeneration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition.\nZejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and Lingyun Sun. 2021. Image\nsynthesis from layout with locality-aware mask adaption. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision.\nLong Lian, Boyi Li, Adam Yala, and Trevor Darrell. 2023. LLM-grounded Diffusion:\nEnhancing Prompt Understanding of Text-to-Image Diffusion Models with Large\nLanguage Models. arXiv preprint arXiv:2305.13655 (2023).\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in\ncontext. In European conference on computer vision.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction\ntuning. Advances in Neural Information Processing Systems (2023).\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. 2022.\nCompositional visual generation with composable diffusion models. In European\nConference on Computer Vision.\nShilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li,\nJianwei Yang, Hang Su, Jun Zhu, et al. 2023c. Grounding dino: Marrying dino with\ngrounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499\n(2023).\nZhaoyang Liu, Yinan He, Wenhai Wang, Weiyun Wang, Yi Wang, Shoufa Chen, Qing-\nlong Zhang, Yang Yang, Qingyun Li, Jiashuo Yu, et al. 2023a. Internchat: Solving\nvision-centric tasks by interacting with chatbots beyond language. arXiv preprint\narXiv:2305.05662 (2023).\nZhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli\nZhao, Jingren Zhou, and Yang Cao. 2023d. Cones 2: Customizable Image Synthesis\nwith Multiple Subjects. arXiv preprint arXiv:2305.19327 (2023).\nOpenAI. 2023. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (2023).\nGaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-\nYan Zhu. 2023. Zero-shot image-to-image translation. In ACM SIGGRAPH 2023\nConference Proceedings.\nMaitreya Patel, Changhoon Kim, Sheng Cheng, Chitta Baral, and Yezhou Yang. 2023.\nECLIPSE: A Resource-Efficient Text-to-Image Prior for Image Generations. arXiv\npreprint arXiv:2312.04655 (2023).\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas\nM\u00fcller, Joe Penna, and Robin Rombach. 2023. Sdxl: Improving latent diffusion\nmodels for high-resolution image synthesis. arXiv preprint arXiv:2307.01952 (2023).\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,\nand Maosong Sun. 2023. Communicative agents for software development. arXiv\npreprint arXiv:2307.07924 (2023).\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022.\nHierarchical text-conditional image generation with clip latents. arXiv preprint\narXiv:2204.06125 (2022).\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer.\n2022. High-resolution image synthesis with latent diffusion models. In Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir\nAberman. 2023. Dreambooth: Fine tuning text-to-image diffusion models for subject-\ndriven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. 2022. Photorealistic text-to-image diffusion models with deep language under-\nstanding. Advances in Neural Information Processing Systems (2022).\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\n2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\nAdvances in Neural Information Processing Systems (2023).\nWei Sun and Tianfu Wu. 2021. Learning layout and style reconfigurable gans for\ncontrollable image synthesis. IEEE transactions on pattern analysis and machine\nintelligence (2021).\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\n2023a. Llama: Open and efficient foundation language models. arXiv preprint\narXiv:2302.13971 (2023).\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\n2023b. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288 (2023).\nAndrey Voynov, Kfir Aberman, and Daniel Cohen-Or. 2023. Sketch-guided text-to-image\ndiffusion models. In ACM SIGGRAPH 2023 Conference Proceedings.\nRuichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. 2023a.\nCompositional text-to-image synthesis with attention map control of diffusion\nmodels. arXiv preprint arXiv:2305.13921 (2023).\nZirui Wang, Zhizhou Sha, Zheng Ding, Yilin Wang, and Zhuowen Tu. 2023b. To-\nkenCompose: Grounding Diffusion with Token-level Supervision. arXiv preprint\narXiv:2312.03626 (2023).\nJinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng,\nand Mike Zheng Shou. 2023. Boxdiff: Text-to-image synthesis with training-free\nbox-constrained diffusion. In Proceedings of the IEEE/CVF International Conference\non Computer Vision.\nBinxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong\nChen, and Fang Wen. 2023a. Paint by example: Exemplar-based image editing with\ndiffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition.\nHui Yang, Sifu Yue, and Yunzhong He. 2023d. Auto-GPT for Online Decision Making:\nBenchmarks and Additional Opinions. arXiv preprint arXiv:2306.02224 (2023).\nZhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu,\nand Lijuan Wang. 2023b. The dawn of lmms: Preliminary explorations with gpt-4v\n(ision). arXiv preprint arXiv:2309.17421 (2023).\nZuopeng Yang, Daqing Liu, Chaoyue Wang, Jie Yang, and Dacheng Tao. 2022. Modeling\nimage composition for complex scene generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition.\nZhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang\nYu. 2023c. AppAgent: Multimodal Agents as Smartphone Users. arXiv preprint\narXiv:2312.13771 (2023).\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control\nto text-to-image diffusion models. In Proceedings of the IEEE/CVF International\nConference on Computer Vision.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. 2023. Minigpt-\n4: Enhancing vision-language understanding with advanced large language models.\narXiv preprint arXiv:2304.10592 (2023).\n"
  },
  {
    "title": "Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with Prototypical Embedding",
    "link": "https://arxiv.org/pdf/2401.15708.pdf",
    "upvote": "9",
    "text": "Object-Driven One-Shot Fine-tuning of Text-to-Image Diffusion\nwith Prototypical Embedding\nJianxiang Lu\nTencent\nChina\njianxianglu2023@gmail.com\nCong Xie\nTencent\nChina\nxie.cong@outlook.com\nHui Guo\nTencent\nChina\nemmaguo7033@gmail.com\nFigure 1: Given an image and its corresponding mask of an object, our method generates diverse images of the same object in\ndifferent contexts. Moreover, our method allows for the implantation of multiple objects, as demonstrated in the lower right\ncorner of the figure.\nABSTRACT\nAs large-scale text-to-image generation models have made remark-\nable progress in the field of text-to-image generation, many fine-\ntuning methods have been proposed. However, these models often\nstruggle with novel objects, especially with one-shot scenarios.\nOur proposed method aims to address the challenges of general-\nizability and fidelity in an object-driven way, using only a single\ninput image and the object-specific regions of interest. To improve\ngeneralizability and mitigate overfitting, in our paradigm, a proto-\ntypical embedding is initialized based on the object\u2019s appearance\nand its class, before fine-tuning the diffusion model. And during\nfine-tuning, we propose a class-characterizing regularization to pre-\nserve prior knowledge of object classes. To further improve fidelity,\nwe introduce object-specific loss, which can also use to implant\nmultiple objects. Overall, our proposed object-driven method for\nimplanting new objects can integrate seamlessly with existing con-\ncepts as well as with high fidelity and generalization. Our method\noutperforms several existing works. The code will be released.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192 Image-based rendering; Non-\nphotorealistic rendering.\nKEYWORDS\nobject-driven, one-shot, diffusion model\nACM Reference Format:\nJianxiang Lu, Cong Xie, and Hui Guo. 2024. Object-Driven One-Shot Fine-\ntuning of Text-to-Image Diffusion with Prototypical Embedding. In Proceed-\nings of ACM Conference (Conference\u201917). ACM, New York, NY, USA, 8 pages.\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nRecently, text-to-image generation using deep learning models have\nsignificant development [5, 26, 28, 30, 31, 36]. With intricate designs\nand vast quantities of training data, these models are capable of\ngenerating images of unparalleled quality from unrestricted natural\nlanguage inputs, provided by the user. These systems can create\ndiverse objects, styles, and scenes, as well as generating different\nobject compositions, resulting in aesthetically pleasing outputs.\nApart from general image generation, users are also concerned\nwith whether it is possible to generate images of a specific object\nin various text descriptions while providing limited samples, such\nas a single image. This is a one-shot generation task that requires\naccurate understanding of the specific object and effective model\nfine-tuning while ensuring the object similarity in generated images\nand the generation controllability of the resultant model. However,\narXiv:2401.15708v1  [cs.CV]  28 Jan 2024\nConference\u201917, July 2017, Washington, DC, USA\nJianxiang Lu, Cong Xie, and Hui Guo\nthis task poses significant challenges due to limited training samples\nand a general lack of specific objects in the model training set.\nIn order to improve the effectiveness of object fine-tuning, cur-\nrent methods [3, 11, 19, 22, 29] employ a paradigm that involves\noptimizing a pre-trained synthesis model, or a text prompt [11, 22]\nrepresenting the object using multiple training examples. The opti-\nmized prompt is then integrated with user prompts to create images\nwith diverse styles and content. However, these methods usually\nrely on the availability of multiple images of the same object, which\nexacerbates the problem of object accuracy and model overfitting\nin one-shot scenarios.\nFocusing on the task of accurate inplantation of a user speci-\nfied object into a generative model using only one sample, while\nmaintaining the model\u2019s generalization, we propose a novel fine-\ntuning framework. Unlike existing works [19, 22, 29, 35] that start\nfrom random initialization and iterative optimize a unique prompt\nrepresentation of a target object, we propose a novel fine-tuning\nmethod. Based on stable diffusion, a prototypical embedding for\nthe object is firstly initialized and then trained in the generation\nmodel with a class-characteristic regularization to balance object\nidentity and generalization variations. And an object-specific loss\nfunction supervised by the object in the given image is proposed,\nto achieve both fidelity and generalization in later image synthesis.\nWhile seemingly simple, one-shot generation fine-tuning re-\nmains a non-trivial task that poses several challenges, including: 1)\nadapting text-to-image models with conditioned object embeddings,\n2) designing a training process to improve object fidelity using only\none image, and 3) enhancing editing capabilities to generate dif-\nferent object compositions using diverse text prompts. Given a\ntarget image and its class, the text embedding of the specific object\nis initialized by finding the best prototypical embedding between\nmulti-modal representations of the object\u2019s class and its character-\nistics. For the fine-tuning quality, we insert additional attention\nmodules[15] to the text-to-image network, and an object-specific\nmask loss is used to preserve fidelity in downstream image syn-\nthesis. However, directly fine-tuning the network with the object\nembedding results in model overfitting with deteriorated editing\ncapability, i.e. high Kernel Inception Distance (KID) [1] in gen-\neration results, where the model generates solely with the user\nspecific object but not the text instruction. We therefore propose\na class-characterizing regularization to the object embedding for\nthe generalization of the model. Our method essentially increases\nobject fidelity and model generalization, leading to performance\ngain in terms of output quality, appearance diversity, and object\nfidelity.\nGenerating personalized content based on one-shot samples\nrequires an efficient algorithm to improve its applicability. In this\nwork, we take a step forward and demonstrate how the fine-tuning\nprocess can help the model maintain object identity as long as\ngeneralization editability. Our method outperforms several existing\nmethods, which generates high-fidelity photorealistic images and\nsupports content-rich synthesis in one-shot generation. What\u2019s\nmore, our method can also used in multi-object fine-tuning, as\nshown in Figure 2, which achieve better composing results of multi\nuser specific objects.\nFigure 2: Methodology overview. Our method takes an input\nimage along with its corresponding masks and relevant class\nnames as input, generating object-specific text embeddings\nand personalized LoRA weights. During inference, the text\nembedding and LoRA weight is combined with other features\nto generate a wide range of variations for the object.\n2\nRELATED WORK\n2.1\nText-to-Image Synthesis\nIn recent years, there has been significant progress in the develop-\nment of text-to-image generative models that synthesize images\nbased on unconditioned text prompts. Such models include Gen-\nerative Adversarial Networks (GANs) [2, 12, 16, 31], Vector Quan-\ntization (VQs) approaches [4, 7, 9, 27, 36], and diffusion models\n[6, 14, 23, 28, 30, 32\u201334]. With the rise of large-scale text-image\ndatasets, these models have been scaled up to produce high-quality\nand semantically rich image synthesis results. Examples of such\nlarge text-to-image models include StyleGAN-T [31], CogView2\n[7], Imagen [30], DALL-E2 [26], and Stable Diffusion [28]. The in-\ntroduction of text conditioning to the StyleGAN architecture in\nStyleGAN-T has showcased the effectiveness of GANs in text-to-\nimage synthesis. In contrast, VQs utilize an autoencoder to learn a\nquantization codebook and predict text-to-image tokens through\ntransformers for generating the final image. Likewise, diffusion\nmodels leverage a UNet module to iteratively denoise a noise image\nin a latent space, conditioned on the text prompt injected to the\nmodel via transformer layers, to synthesize images.\nWhile text-to-image models trained on a specific dataset can\ngenerate high-quality images, they often struggle to generate novel\ninstances or user-specified objects with identity consistency [19].\nThis can be attributed to their training set limitation in represent-\ning new objects. Our work relates to the Stable Diffusion model\nand aims to enhance its ability to generate high-fidelity images of\nuser-specified objects, thereby expanding the model\u2019s applicability\nto content creation tasks that require personalized text-to-image\nsynthesis.\n2.2\nPersonalized image Synthesis\nFine-tuning method with a pre-trained synthesis model for per-\nsonalized text-to-image generation purposs mainly includes the\nfollowing four methods: parts of the network parameter tuning\n[19, 29], image augmentation based method [3, 9], prompt tuning\nor text encoder adapting [11, 22] and injecting additional module\nwhich adapts to the new objects, i.e., Low-Rank Adaptation (LoRA)\n[15]. Adapting the whole model or part of the parameters [29]\nObject-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with Prototypical Embedding\nConference\u201917, July 2017, Washington, DC, USA\nusing a one-shot image can easily lead to overfitting, as it modi-\nfies well-trained parameters and can cause catastrophic forgetting\n[10, 18, 20, 21, 25]. While augmentation [9] or prompt tuning [11]\nbased methods provide a wide variety of samples for regulariza-\ntion or text encoder fine-tuning in training process, they not only\nstruggle to preserve object identity but also require additional train-\ning information. Methods by adding an extra module can ease the\noverfitting problem while sharing the same low fidelity problem.\nIn this paper, a small number of parameters, which relates to text\nencoder and transformer cross attention module, is adapted for the\nnew object and the model is fine-tuned with object-driven method\nto preserve fidelity and generalization ability of composing new\nobject with the existing ones.\n2.3\nOne-shot Synthesis\nFine-tuning a diffusion model with only one sample is an extremely\nchallenging task. While existing fine-tuning methods [29] or prompt-\ntuning approaches [11] suffer from overfitting issues in one-shot\nscenarios, additional training strategies need to be developed. To\nmaintain generation controllability, Dong et al. [8] use positive and\nnegative prompts to contrastively fine-tune the representation of\na pseudo prompt, which had generation failures in multi-object\ncompositions. Another method [9] considers the image background\nof the one-shot sample while implanting the object and uses a\nbackground-masked object embedding for fine-tuning. It trains the\nentire network and the text encoder with side information such\nas original training samples for regularization, which faces the\nproblem of generation defects of the specific object.\nIn contrast to the aforementioned paradigms, our method only\nrequires one image and the object\u2019s region of interest, and focusing\non the specific fidelity and generalization variations with specific\nobject synthesis with . It differs from existing methods in several\naspects. Firstly, we introduce an object-driven prototypical embed-\nding initialization for the new object, which alleviates the difficulty\nof representing the object with only one image and improve the\nefficiency of object inplantation. Secondly, we introduce an object-\ndriven specific loss for precise object appearance learning, where\n[17] is used for the object mask. Thirdly, a LoRA [15] module where\nthe main denoising UNet is maintained and a class-characteristic\nregularization to protect class prior information for semantic gener-\nalization with other objects and preventing catastrophic forgetting.\nMoreover, our method is capable of multiple object fine-tuning,\nwhich is challenging in existing methods [19, 29].\n3\nMETHOD\n3.1\nOverview\nOur proposed method focuses on object-driven fine-tuning of single\nor multiple objects specified by the user in one image, as shown\nin Figure 3. To overcome the limitations of existing fine-tuning\nmethods, we use prototypical embedding as the initialization em-\nbedding and propose a regularized loss function to increase the\ndiversity of the generated images and effectively preserve the prior\nknowledge of the pre-trained model. In addition, we introduce an\nobject-specific mask loss function to synthesize high-fidelity im-\nages, which can also be used for multi-object implantation. In this\nsection, we explain the proposed method in detail.\n3.2\nLatent Diffusion Model\nIn this work, we adopt the stable diffusion model, a well-established\ntext-to-image diffusion model, as our synthetic network. Stable dif-\nfusion is a Latent Diffusion Model (LDM) [28], which performs\ndenoising operations in the latent space instead of the image space.\nIn simple terms, first, the RGB image \ud835\udc65 is encoded into the latent rep-\nresentation \ud835\udc67 = \ud835\udf00(\ud835\udc65) by the encoder \ud835\udf00 of an Variable Auto Encoder\n(VAE). In the latent space, the denoising U-Net of LDM employs the\ncross-attention mechanism for text-conditional image generation.\nThe training loss of the conditional LDM is formulated as:\n\ud835\udc3f\ud835\udc3f\ud835\udc37\ud835\udc40 = EE(\ud835\udc65),\ud835\udc66,\ud835\udf16\u223cN(0,1),\ud835\udc61\n\u0002\n\u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udc50)\u22252\n2\n\u0003\n(1)\nwhere \ud835\udc50 is the text embedding, \ud835\udf16 denotes standard Gaussian, and \ud835\udf16\ud835\udf03\nis the model prediction. \ud835\udc67\ud835\udc61 is the noisy version of input \ud835\udc67 in timestep\n\ud835\udc61.\n3.3\nPrototypical Embedding\nWhen fine-tuning a diffusion model, the text embedding of the\nobject is usually trained. However, when the training data is only\none image, it sometimes causes overfitting, leading the network\nto generate output based only on the text embeddings of the ob-\njects, while other textual conditions are ignored. In practice, proper\ninitialization of text embeddings can enable faster fitting of the\nnetwork and alleviate overfitting, such as Textual Inversion (TI)\n[11] initializing text embeddings based on the class of objects. In\nthis work, in order to achieve more efficient initialization, we find\nprototype embeddings based on the embedding of the input im-\nage and the text embedding of the class name (e.g., dog). Prior\nto commencing the training of the diffusion model, we compute\nprototypical embedding via:\n\ud835\udc3f\ud835\udc43\ud835\udc38 = 1 \u2212\nT (\ud835\udc50\ud835\udc5d)\ud835\udf03\ud835\udc5a(I(\ud835\udc65), I(\ud835\udc65\ud835\udc5a), T (\ud835\udc50\ud835\udc50))\n\r\rT (\ud835\udc50\ud835\udc5d)\n\r\r \u2225\ud835\udf03\ud835\udc5a(I(\ud835\udc65), I(\ud835\udc65\ud835\udc5a), T (\ud835\udc50\ud835\udc50))\u2225\n(2)\nwhere \ud835\udc65 is the training image, the image encoder I and the\ntext encoder T of CLIP [24] are used to obtain the whole image\nembedding I(\ud835\udc65) , the object mask image embedding I(\ud835\udc65\ud835\udc5a), T (\ud835\udc50\ud835\udc50)\nis the class name text embedding of the object and \ud835\udf03\ud835\udc5a is the way\nof embedding fusion e.g. averaging. We aim to obtain a prototype\ntext embedding T (\ud835\udc50\ud835\udc5d) similar to the target image embedding and\nthe class text embedding as initialization by this loss function.\n3.4\nClass-characterizing Regularization\nAdditionally, in order to preserve the synthesis ability of the class of\nobjects in the pre-trained model, we adjust the text embedding us-\ning class-characterizing regularization during the training process.\nClass-characterizing loss is formulated as:\n\ud835\udc3f\ud835\udc36\ud835\udc3f =\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n1 \u2212 \ud835\udefc\ud835\udc50\ud835\udc59\nT(\ud835\udc50\ud835\udc5d ) T(\ud835\udc50\ud835\udc50 )\n\u2225T(\ud835\udc50\ud835\udc5d )\u2225\u2225 T(\ud835\udc50\ud835\udc50 ) \u2225\nif \ud835\udc5d < \ud835\udc5d\ud835\udc50\ud835\udc59\n0\notherwise\n(3)\nwhere T (\ud835\udc50\ud835\udc50) is the class name text embedding of the object, \ud835\udefc\ud835\udc50\ud835\udc59\nrepresents the weight of the cosine loss, \ud835\udc5d \u223c \ud835\udc48\ud835\udc5b\ud835\udc56(0, 1), and \ud835\udc5d\ud835\udc50\ud835\udc59 is an\nadjustable threshold. In this context, it is necessary to predetermine\nthe class name of each object. Further experiments indicate that the\nintroduction of this loss function leads to improved generalizability\nin synthesis.\nConference\u201917, July 2017, Washington, DC, USA\nJianxiang Lu, Cong Xie, and Hui Guo\nFigure 3: Fine-tuning details. Given one image with single or multiple objects, our method fine-tunes a text-to-image diffusion\nmodel. Taking single object as an example, our method utilizes prototypical embedding for initialization and employs class-\ncharacterizing regularization to enhance generation diversity, along with a class-specific loss function to ensure fidelity of the\nsynthesized images.\n3.5\nObject-specific Loss\nOur task is to implant the selected objects into the output domain\nof the model and bind them with a unique identifier. Noted that the\nselected objects are often parts of the training image rather than\nthe whole image, for this reason we propose object-specific loss for\nimplantation of selected objects with improved fidelity. First, we\nuse an image segmentation algorithm such as SAM[17] to obtain\nthe mask images \ud835\udc5a of the objects. The mask images are introduced\ninto the latent space and the training process. For single-object\nimplantation is trained as follow:\n\ud835\udc3f\ud835\udc46\ud835\udc43 = \u2225 \u02dc\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 ( \u02dc\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udc50\ud835\udc5a)\u22252\n2 + \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udc50)\u22252\n2\n(4)\nwhere\ud835\udc50\ud835\udc5a is the text condition of this object with mask, object target\nnoise \u02dc\ud835\udf16 = \ud835\udf16 \u2297 \ud835\udc5a + \ud835\udf16\ud835\udf03 \u2297 (1 \u2212 \ud835\udc5a), and mask latent representation\n\u02dc\ud835\udc67 = \ud835\udc67 \u2297\ud835\udc5a. Our goal is to focus on the mask region when performing\nthe loss calculation. Further, for multiple object implantation we\nmake object-specific loss function combinations, assuming that\nthere are a set of \ud835\udc5f objects to be implanted and a subset \ud835\udc46 of \ud835\udc58\ndistinct objects are taken at a time, the number of \ud835\udc58-combinations\nis \ud835\udc36\ud835\udc58\ud835\udc5b. So, in one step of training, the overall object-specific loss is:\n\ud835\udc3f\ud835\udc46\ud835\udc43 =\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc46\n\r\r \u02dc\ud835\udf16\ud835\udc56 \u2212 \ud835\udf16\ud835\udf03\n\u0000\u02dc\ud835\udc67\ud835\udc61,\ud835\udc56,\ud835\udc61,\ud835\udc50\ud835\udc5a,\ud835\udc56\n\u0001\r\r2\n2 + \u2225\ud835\udf16 \u2212 \ud835\udf16\ud835\udf03 (\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udc50)\u22252\n2\n(5)\nNote that the text condition \ud835\udc50\ud835\udc5a,\ud835\udc56 is different for each mask, and the\nglobal text condition \ud835\udc50 is based on unique identifiers for all objects.\n4\nEXPERIMENTS\n4.1\nSettings\nWe use the pre-trained stable diffusion model as the text-to-image\nnetwork. The CLIP image and text encoders are used to compute\nprototypical embeddings and take LoRA as the fine-tuned model.\nWe use the images on unsplash as fine-tuning data and the mask\nimages of the objects are obtained using SAM. In the object-driven\nfine-tuning, the learning rate is 10\u22124 and the training is 100 steps\nwith a batchsize of 1, on a v100 GPU. We did not over-tune the\nsuperparameter, \ud835\udefc\ud835\udc50\ud835\udc59, \ud835\udc5d\ud835\udc50\ud835\udc59 are set to 1, and \ud835\udc58 is set to 2 for object-\nspecific loss combinations. We use Dreambooth, TI and LoRA as\nthree state-of-the-art methods for comparison, and use their pub-\nlicly released implementation code. In addition, their learning rates\nand training steps are consistent in training.\n4.2\nComparison\nAs shown in Figure 4, we compare our results with existing fine-\ntuning methods. Because of the different fine-tuning strategies, the\nexisting methods may overfit, or not be fidelity, when only one\nimage is given as input. For action generation, the results of TI and\nLoRA align the actions in the prompt, but the generated objects\ndiffer significantly from the reference image. And DreamBooth\noverfits, resulting in the inability to generate actions. In contrast,\nour method is able to generate actions with higher fidelity. For\nObject-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with Prototypical Embedding\nConference\u201917, July 2017, Washington, DC, USA\nFigure 4: Qualitative comparison. For one-shot tasks, existing methods face challenges in achieving both fidelity and generaliz-\nability with the given text. Our method generates images that better match the reference image and are consistent with the text\nsemantics under multiple cue words. Note that the * symbol represents a unique identifier.\nhuman faces, all methods except ours cannot generate expressions\nand preserve identities at the same time. The same observation is\nmade for animals and style transformation.\nFurther, we compare other methods using quantitative metrics.\nWe quantitatively evaluated 5 categories and 750 images, and the\nresults are tabulated in Table 1. we used Text Alignment (TA) [13],\nImage Alignment (IA) [11] and kernel inception distance as met-\nrics, where TA is the generalizability of the alignment ability to\ncharacterize the method on the new prompt, and IA characterizes\nthe generalizability to the image similarity. Thus, it is a trade-off,\nas shown in Figure 6. It can be observed that our proposed method\nperforms well in this trade-off, with both fidelity and generalization.\nConference\u201917, July 2017, Washington, DC, USA\nJianxiang Lu, Cong Xie, and Hui Guo\nFigure 5: Prototypical embedding initialization. Our pro-\nposed method, utilizing prototypical embedding as the ini-\ntialization, ensures the generation of images that are more\ncontextually relevant.\nTable 1: Quantitative comparison. We use three metrics to\nevaluate the generalization and fidelity of the fine-tuning\nmethod.\nMethods\nTI\nDreamBooth\nLoRA\nOurs\nIA \u2191\n0.6084\n0.6216\n0.6284\n0.6431\nTA \u2191\n0.2609\n0.2434\n0.2774\n0.2800\nKID \u2193\n0.1322\n0.2630\n0.1222\n0.1882\nOverall, our proposed method has both fidelity and generaliz-\nability, which indicates that it effectively mitigates the overfitting\nand learns the characterization of the object.\n4.3\nAblation Study\n4.3.1\nPrototypical Embedding Initialization. As indicated in Section\n3.3, we propose prototypical embedding to mitigate the problem of\noverfitting for fine-tuning, and to demonstrate its importance we\ncompare the results of random initialization embedding. Note that\nin all comparisons the initialization of the text embedding is based\non four token vectors. As shown in Figure 5, without the prototypi-\ncal embedding, the synthesized image has only the reference object\nand ignores other information in the prompt. On the contrary, when\nprototypical embedding is adopted, the model is able to generate\nFigure 6: Quantitative assessment. We visualize the metrics\nfor each method, the point towards the lower right, the better\nperformance of the method.\nFigure\n7:\nClass-characterizing\nregularization.\nClass-\ncharacterizing regularization preserves the prior representa-\ntion of object classes during fine-tuning, resulting in a more\nnatural and diverse synthesis of objects.\nelements other than objects (e.g., buckets). It demonstrates that\nprototypical embedding is effective in overcoming the limitations\nof overfitting and improving the diversity of image generation.\n4.3.2\nClass-characterizing Regularization. Since prototypical em-\nbedding is only used as a way to initialize without involving the\ntraining process, the prior representations of object classes are\nsometimes lost during fine-tuning. As shown in Figure 7, when\nwithout class-characterizing regularization, the generated hats are\nnot well integrated with the person, and customized styles cannot\nbe generated. We observe that with class-characterizing regulariza-\ntion, the prior knowledge of the object\u2019s class (e.g., hat) is preserved\nObject-Driven One-Shot Fine-tuning of Text-to-Image Diffusion with Prototypical Embedding\nConference\u201917, July 2017, Washington, DC, USA\nFigure 8: Object-specific loss. Our object-specific loss func-\ntion enhances attention to the specific object region, thereby\neffectively preserving the fidelity of the generated objects.\nin fine-tuning, and the generated images possess greater diversity\nand naturalness.\n4.3.3\nObject-specific Loss. In object-driven training, we adopt an\nobject-specific loss function that focuses on the object region to\nenhance object fidelity. To demonstrate the effectiveness of the\nloss, we compare the results without the loss function. In Figure\n8, we observe that our results trained with the object-specific loss\nfunction have greater fidelity in the color, shape, and details of the\nobjects. For example, the color of the object \"flower\" does not match\nthe reference image without this loss function, and the object \"toy\nduck\" exhibits alterations in its shape and details.\n4.3.4\nMulti-object Implantation. As discussed above, the proposed\none-shot method supports multiple objects implantation, which can\nderive many interesting applications. We first obtain the mask im-\nages of the objects separately using the mature image segmentation\nalgorithm and then introduce the object-specific loss combinations\nmentioned in Section 3.5 for training. As shown in Figure 9, the\nimplanted objects do not affect each other when generating images\nusing their unique identifiers and can generate output related to\nthe prompt. In addition, we generate images using multiple object\nidentifiers at the same time (e.g., [strawberry*] in the [cup*]), it\ncan be observed that the output resulting from the combination of\nconcepts is both natural and faithful.\nFigure 9: Multi-object implantation. After implanting multi-\nple objects, our method can generate multi objects simulta-\nneously.\n5\nCONCLUSION\nThe main goal of our work is to overcome the challenges of one-\nshot fine-tuning of text-to-image diffusion, and only one image is\nrequired for single or multiple object implantation. First we propose\nthe prototypical embedding as an initialization for fine-tuning, and\nexperiments show that it can effectively mitigate overfitting.During\ntraining, we introduce class-characterizing regularization to protect\nthe prior knowledge of the pre-trained model and further enhance\nthe synthetic diversity. In addition, the object-specific Loss effective\nfor improving the fidelity of the generated images and can handle\nmultiple objects. Our approach is object-driven and competitive\nwith other existing approaches. We anticipate that this work will\ngreatly enhance the quality and efficiency of personalized image\ngeneration.\nAt the same time, we also found that our method has some\nlimitations, such as errors in the mask region for objects with\ncomplex edges, which sometimes cause degradation in the quality of\nthe generated image edges. In addition, the fidelity of the generated\nimages is slightly degraded when implanting smaller objects. In\norder to solve the above problems, future work will be devoted to\nimproving the way of acquiring mask images and adding multi-scale\nperception mechanism for objects.\nREFERENCES\n[1] Miko\u0142aj Bi\u0144kowski, Danica J Sutherland, Michael Arbel, and Arthur Gretton.\n2018. Demystifying mmd gans. arXiv preprint arXiv:1801.01401 (2018).\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large scale GAN\ntraining for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096\n(2018).\n[3] Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. Instructpix2pix:\nLearning to follow image editing instructions. arXiv preprint arXiv:2211.09800\n(2022).\n[4] Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang,\nMing-Hsuan Yang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al.\n2023. Muse: Text-To-Image Generation via Masked Generative Transformers.\narXiv preprint arXiv:2301.00704 (2023).\n[5] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. 2022.\nRe-imagen: Retrieval-augmented text-to-image generator.\narXiv preprint\narXiv:2209.14491 (2022).\nConference\u201917, July 2017, Washington, DC, USA\nJianxiang Lu, Cong Xie, and Hui Guo\n[6] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on\nimage synthesis. Advances in Neural Information Processing Systems 34 (2021),\n8780\u20138794.\n[7] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. 2022. Cogview2: Faster\nand better text-to-image generation via hierarchical transformers. arXiv preprint\narXiv:2204.14217 (2022).\n[8] Ziyi Dong, Pengxu Wei, and Liang Lin. 2022. Dreamartist: Towards controllable\none-shot text-to-image generation via contrastive prompt-tuning. arXiv preprint\narXiv:2211.11337 (2022).\n[9] Patrick Esser, Robin Rombach, and Bjorn Ommer. 2021. Taming transformers\nfor high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. 12873\u201312883.\n[10] Robert M French. 1999. Catastrophic forgetting in connectionist networks. Trends\nin cognitive sciences 3, 4 (1999), 128\u2013135.\n[11] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal\nChechik, and Daniel Cohen-Or. 2022. An image is worth one word: Personalizing\ntext-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618\n(2022).\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial\nnetworks. Commun. ACM 63, 11 (2020), 139\u2013144.\n[13] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021.\nClipscore: A reference-free evaluation metric for image captioning. arXiv preprint\narXiv:2104.08718 (2021).\n[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\n[15] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean\nWang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large\nlanguage models. arXiv preprint arXiv:2106.09685 (2021).\n[16] Tero Karras, Miika Aittala, Samuli Laine, Erik H\u00e4rk\u00f6nen, Janne Hellsten, Jaakko\nLehtinen, and Timo Aila. 2021. Alias-free generative adversarial networks. Ad-\nvances in Neural Information Processing Systems 34 (2021), 852\u2013863.\n[17] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.\n2023. Segment anything. arXiv preprint arXiv:2304.02643 (2023).\n[18] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume\nDesjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka\nGrabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural\nnetworks. Proceedings of the national academy of sciences 114, 13 (2017), 3521\u2013\n3526.\n[19] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan\nZhu. 2022. Multi-Concept Customization of Text-to-Image Diffusion. arXiv\npreprint arXiv:2212.04488 (2022).\n[20] Dingcheng Li, Zheng Chen, Eunah Cho, Jie Hao, Xiaohu Liu, Fan Xing, Chenlei\nGuo, and Yang Liu. 2022. Overcoming catastrophic forgetting during domain\nadaptation of seq2seq language generation. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies. 5441\u20135454.\n[21] Zhizhong Li and Derek Hoiem. 2017. Learning without forgetting. IEEE transac-\ntions on pattern analysis and machine intelligence 40, 12 (2017), 2935\u20132947.\n[22] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022.\nNull-text Inversion for Editing Real Images using Guided Diffusion Models. arXiv\npreprint arXiv:2211.09794 (2022).\n[23] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,\nBob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic\nimage generation and editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741 (2021).\n[24] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,\nSandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from natural language supervision.\nIn International conference on machine learning. PMLR, 8748\u20138763.\n[25] Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. 2022. Effect of\nscale on catastrophic forgetting in neural networks. In International Conference\non Learning Representations.\n[26] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.\n2022. Hierarchical text-conditional image generation with clip latents. arXiv\npreprint arXiv:2204.06125 (2022).\n[27] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. 2019. Generating diverse\nhigh-fidelity images with vq-vae-2. Advances in neural information processing\nsystems 32 (2019).\n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn\nOmmer. 2022. High-resolution image synthesis with latent diffusion models. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\n10684\u201310695.\n[29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and\nKfir Aberman. 2022. Dreambooth: Fine tuning text-to-image diffusion models\nfor subject-driven generation. arXiv preprint arXiv:2208.12242 (2022).\n[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L\nDenton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim\nSalimans, et al. 2022. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. Advances in Neural Information Processing Systems 35\n(2022), 36479\u201336494.\n[31] Axel Sauer, Tero Karras, Samuli Laine, Andreas Geiger, and Timo Aila. 2023.\nStylegan-t: Unlocking the power of gans for fast large-scale text-to-image syn-\nthesis. arXiv preprint arXiv:2301.09515 (2023).\n[32] Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya\nNachmani, and Yaniv Taigman. 2022. Knn-diffusion: Image generation via large-\nscale retrieval. arXiv preprint arXiv:2204.02849 (2022).\n[33] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli.\n2015. Deep unsupervised learning using nonequilibrium thermodynamics. In\nInternational Conference on Machine Learning. PMLR, 2256\u20132265.\n[34] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502 (2020).\n[35] Yuxin Wen, Neel Jain, John Kirchenbauer, Micah Goldblum, Jonas Geiping, and\nTom Goldstein. 2023. Hard prompts made easy: Gradient-based discrete op-\ntimization for prompt tuning and discovery. arXiv preprint arXiv:2302.03668\n(2023).\n[36] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang,\nVijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. 2022.\nScaling autoregressive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789 (2022).\n"
  },
  {
    "title": "Overcoming the Pitfalls of Vision-Language Model Finetuning for OOD Generalization",
    "link": "https://arxiv.org/pdf/2401.15914.pdf",
    "upvote": "5",
    "text": "Published as a conference paper at ICLR 2024\nOVERCOMING THE PITFALLS OF VISION-LANGUAGE\nMODEL FINETUNING FOR OOD GENERALIZATION\nYuhang Zang\u22171, Hanlin Goh2, Josh Susskind2, Chen Huang2\n1Nanyang Technological University 2Apple Inc.\n{zang0012}@ntu.edu.sg\n{hanlin,jsusskind,chen-huang}@apple.com\nABSTRACT\nExisting vision-language models exhibit strong generalization on a variety of vi-\nsual domains and tasks. However, such models mainly perform zero-shot recogni-\ntion in a closed-set manner, and thus struggle to handle open-domain visual con-\ncepts by design. There are recent finetuning methods, such as prompt learning,\nthat not only study the discrimination between in-distribution (ID) and out-of-\ndistribution (OOD) samples, but also show some improvements in both ID and\nOOD accuracies. In this paper, we first demonstrate that vision-language mod-\nels, after long enough finetuning but without proper regularization, tend to overfit\nthe known classes in the given dataset, with degraded performance on unknown\nclasses. Then we propose a novel approach OGEN to address this pitfall, with the\nmain focus on improving the OOD GENeralization of finetuned models. Specif-\nically, a class-conditional feature generator is introduced to synthesize OOD fea-\ntures using just the class name of any unknown class. Such synthesized features\nwill provide useful knowledge about unknowns and help regularize the decision\nboundary between ID and OOD data when optimized jointly. Equally important\nis our adaptive self-distillation mechanism to regularize our feature generation\nmodel during joint optimization, i.e., adaptively transferring knowledge between\nmodel states to further prevent overfitting. Experiments validate that our method\nyields convincing gains in OOD generalization performance in different settings.\n1\nINTRODUCTION\nLarge-scale pre-trained vision-language models like CLIP (Radford et al., 2021) demonstrate\npromising generalizability on various visual domains and tasks in the real world. However, their\nzero-shot in-distribution (ID) performance can be limited for some downstream datasets. Also due\nto their zero-shot evaluation in a closed-set manner (i.e., to match input image to a predefined set\nof classes), vision-language models often struggle to handle the out-of-distribution (OOD) samples\nfrom novel classes. Such shortcomings create major safety risks in the open domain that often\nrequire capabilities of OOD detection and/or accurate identification of both novel and seen classes.\nSome recent works attempt to improve the zero-shot OOD detection performance of existing vision-\nlanguage models, either by simple softmax scaling (Ming et al., 2022) or training an extra text\ngenerator (Esmaeilpour et al., 2022). Alternatively, Fort et al. (2021) first show the promise of CLIP\nmodels finetuned on an ID dataset. Encouragingly both ID and OOD accuracies are improved after\nfinetuning. Parameter-efficient finetuning methods, such as prompt learning (Zhou et al., 2022a;b)\nor adaptor tuning (Zhang et al., 2022), illustrate similar benefits without heavy training.\nDespite the success of prior finetuning methods, we found from our extensive benchmarking that\nfinetuning on ID datasets is prone to overfitting (Fig. 1(b)). More specifically, we observed that\nmodels after long enough finetuning but without proper regularization, tend to overfit the known\nclasses in the given dataset, with inferior generalization on unknown classes. Unfortunately, an ex-\nplicit regularization mechanism has not been explored in literature to address this pitfall, and simple\n\u2217Work done while interning at Apple.\n1\narXiv:2401.15914v1  [cs.CV]  29 Jan 2024\nPublished as a conference paper at ICLR 2024\nInternal use only\u2013do not distribute.\nVision-language model\nFinetuning\nDownstream tasks\n\u2026\nImageNet\nEuroSAT\n(a)\nID \n(known)\nOOD \n(unknown)\nImageNet (within-dataset)\nEuroSAT (OOD)\nImageNet (ID)\ncross-\ndataset\nEuroSAT\nDTD\nUCF101\n(b)\nunknown class\nknown class\nFigure 1: (a) We study OOD generalization when finetuning the vision-language model CLIP on\nvarious downstream tasks. We consider both within-dataset generalization where one dataset has\nID vs. OOD (or known vs. unknown) class splits for finetuning and evaluation respectively,\nand the more challenging cross-dataset generalization setting. More clarifications on the problem\ndefinition in Appendix A. (b) Examples of within-dataset generalization: we show learning curves\nof the prompt learning method CoOp (Zhou et al., 2022a) that finetunes CLIP for long enough (200\nepochs) on three datasets (more in Appendix B). Apparently, CoOp overfits the known classes of\neach dataset with notable accuracy drop on the unknowns. Our proposed method OGEN largely\nreduces such overfitting through effective regularization.\nregularization strategies like early stopping seem insufficient. E.g. in Fig. 1(b), it is difficult to find\nan early model checkpoint with good trade-off between the known and unknown class performance.\nOne main challenge of effective model regularization is the missing knowledge about unknowns.\nSuch knowledge could actually offer useful supervision signals to avoid overconfident predictions\non OOD data. In this paper, we propose a novel method that features 1) image feature synthesis for\nunknown classes and 2) an unknown-aware finetuning algorithm with effective model regularization.\nThe goal is to improve OOD generalization without hurting the ID performance of finetuned models.\nTo synthesize unknown features, we introduce a class-conditional feature generator: i.e., generating\nimage features just given the name of an unknown class. This is made possible by CLIP\u2019s well-\naligned image-text feature spaces. Our feature generator is implemented by a lightweight attention\nmodule, with an \u201cextrapolating bias\u201d on the unknown classes. It generalizes well to \u201cunknown un-\nknowns\u201d and hence can model the complex distributions of visual classes in the open domain. Then\nwe use both the ID and synthesized OOD data for joint optimization, leading to a better regularized\ndecision boundary. Another contribution is an adaptive self-distillation mechanism that regularizes\nour feature generator to further reduce overfitting during joint optimization. The idea is to find an\nadaptive teacher model of the feature generator from historical training epochs (with less overfitting)\nto guide optimization at the current epoch (student model, often with more overfitting).\nOur overall approach OGEN is applicable to different finetuning methods e.g., (Zhou et al., 2022a;b;\nJia et al., 2022) for CLIP-like models. OGEN is shown to consistently improve their OOD gener-\nalization performance (by up to absolute 18.77%) under two settings: within-dataset (base-to-new\nclass) generalization and cross-dataset generalization. Summarizing, our main contributions are:\n\u2022 Provide the first comprehensive study on OOD generalization that unveils the pitfalls of\nfinetuning methods (based on prompt learning) for vision-language models.\n\u2022 A class-conditional feature generator to synthesize OOD data for effective regularization.\n\u2022 Adaptive self-distillation on our feature generator to further reduce overfitting.\n2\nRELATED WORK\nVision-Language Models. Recent large-scale vision-language models like ViLT (Kim et al., 2021)\nand PaLI (Chen et al., 2023) simply consume image-and-text features for multimodal downstream\ntasks with remarkable performance. Another popular paradigm used in CLIP (Radford et al., 2021)\nand ALIGN (Jia et al., 2021) contrastively aligns image and text encoders. These contrastive mod-\nels are trained on massive web-scale image-text pairs, also showing strong adaptability to a range of\n2\nPublished as a conference paper at ICLR 2024\ndownstream tasks, such as semantic segmentation (Zang et al., 2022a; Ghiasi et al., 2021) and video\nclassification (Qian et al., 2022). Numerous follow-up works (Li et al., 2022; Zhou et al., 2022a)\naim to improve CLIP-like models in data efficiency or generalization. However, the zero-shot per-\nformance on some tasks can still be limited for existing vision-language models. Hu et al. (2023)\nfound that they make different kinds of errors, e.g., PaLI is erroneous at tail visual concepts while\nCLIP may fail for common ones. This paper mainly studies and improves the generalization of fine-\ntuned CLIP models, but our approach is model-agnostic and thus applicable to other vision-language\nmodels as well.\nFinetuning methods have been studied to improve the downstream performance of vision-language\nmodels over their zero-shot counterparts. Fort et al. (2021) showed that after finetuning the CLIP\nmodel on datasets of interest, both the ID and OOD generalization performance will be improved.\nMore parameter-efficient finetuning methods are popularized in recent years. In particular, prompt\nlearning focuses on learning visual (Jia et al., 2022), textual (Zhou et al., 2022a;b; Yao et al., 2023;\nWang et al., 2023; Shu et al., 2023; Khattak et al., 2023b) or multi-modal Zang et al. (2022b);\nKhattak et al. (2023a) prompts, while adaptor tuning (Zhang et al., 2022) optimizes feature repre-\nsentations with the model backbone kept frozen. In this paper, we first unveil the overfitting issue of\nrecent finetuning methods, and then propose a new regularization method to prevent overfitting. Our\napproach is orthogonal to the finetuning research, and shows consistent gains over various finetuning\nbaselines.\nOutlier synthesis proves effective for model regularization in the absence of OOD data. Previous\nmethods rely on GANs (Lee et al., 2018) to synthesize outlier images. More recent methods like\nVOS (Du et al., 2022) directly synthesize virtual features which allows greater flexibility. Tao et al.\n(2023) propose non-parametric outlier synthesis, without the restrictive Gaussian assumption on\nfeature distributions in VOS. Here we present a new feature synthesis method that has the same\nformat as the CLIP framework and hence facilitates multimodal regularization. Specifically, given\nthe name of an unknown class, we synthesize its example features in a generalizable way.\nModel distillation techniques transfer knowledge from a teacher model to student models, e.g., from\na large model to its efficient counterparts (Hinton et al., 2015) or from a weakly augmented model\nto the strongly augmented (Sohn et al., 2020). Here we aim to reduce overfitting for unseen classes\nand propose to distill knowledge from early to current epochs (i.e., self-distillation). Specifically, we\nextend Mean teacher (Tarvainen & Valpola, 2017) to an adaptive localized one with suitable teacher\ncurriculum. In the vision-language domain, our approach differs from distillation into smaller mod-\nels (Li et al., 2023) or towards various downstream tasks (Gu et al., 2022; Dai et al., 2022; Mal\net al., 2022). Our approach is also orthogonal (and applicable) to recent distillation frameworks for\nimproved multimodal pretraining (Dong et al., 2023; Li et al., 2021; Zhong et al., 2022).\n3\nMETHODOLOGY\n3.1\nPRELIMINARIES\nCLIP (Radford et al., 2021) is the vision-language model that we mainly study in this paper, al-\nthough our study is applicable to other popular models. CLIP consists of an image encoder \u03d5 and a\ntext encoder \u03c8, which map the image and text inputs into a joint feature space. The CLIP training\naims at aligning the image and text modalities by maximizing their feature similarity. Given an input\nimage x that belongs to one of the classes Y = {y1, y2, ..., yC}, the image encoder \u03d5 first extracts\nimage features z = f\u03d5(x) \u2208 Rd. To obtain the corresponding text features wc\u2208{1,...,C}, all the given\nclass names can be fed into a fixed prompt template {a photo of a [CLASS]}, leading to text\ndescriptions A which are further encoded by \u03c8 into the text embeddings W = f\u03c8(A) \u2208 Rd\u00d7C\n(hence wc = W:,c). The image-text alignment is optimized based on the cosine feature similarity:\np(y = c | x) =\nexp (cos (wc, z) /\u03c4)\nPC\ni=1 exp (cos (wi, z) /\u03c4)\n,\n(1)\nwhere \u03c4 is the temperature. A larger cosine score often indicates stronger image-text alignment in\ntheir underlying semantics.\nPrompt Learning. For efficient model finetuning on downstream tasks, recent prompt learning\napproaches like CoOp (Zhou et al., 2022a) replace the aforementioned fixed prompts with learnable\n3\nPublished as a conference paper at ICLR 2024\nInternal use only\u2013do not distribute.\n+\n(a)\n(b)\n(c)\nText \nEncoder \u03c8\nLearned \nprompts \n[CLASS]\nunknown \ntext features\nunknown \nimage features\nMatching\nunknown\n,\n}\n{\nkNN known \n{text, image} features\nRetrieval\nClass-conditional  \nfeature generator \u03b8\nknown \ntext features\n\u2714\n\u2717\n\u2717\n\u0302wn\n\u0302wb\n2\n\u0302wb\n1\n\u2026\nKnown \nconcepts\nRegularized decision boundary\nSynthesized \nunknowns\nImage \nfeature space\nNo \ntraining \nimage\n[CLASS]\nunknown\n\u2799\n\u2799\nSynthesizing\nMulti-Head  \nCross-Attention\nunknown \ntext\nkNN \nknown \ntext\nkNN \nknown \nimage\nLayer Normalization\nQuery\nunknown \nimage\nKey\nValue\n\u0302Wb\nR\n\u0302Wb\nR\n\u0302wn\nZb\nR\nZb\nR\nzn\nzn\nFFN\n\u0302zn\nFigure 2: (a) To improve OOD generalization, we propose to gain knowledge of unknown classes\nby directly synthesizing their image features. This helps to learn a more reliable decision boundary\nbetween known and unknown classes in the feature space. (b) Prompt learning based on discriminat-\ning both the known and synthesized unknown features (from our class-conditional feature generator\n\u03b8, see details in text). (c) Implementation of \u03b8 using a lightweight attention module.\nones V = [v1, v2, . . . , vL] \u2208 Rd\u00d7L where L is the prompt length. Then the text encoder \u03c8 of\nCLIP will be able to convert the learned prompts V (together with Y ) into adapted text embeddings\n\u02c6\nW = f\u03c8([V , Y ]) \u2208 Rd\u00d7C. Note V is learned on each downstream task using the task-specific\nloss. The image encoder \u03d5 and text encoder \u03c8 of CLIP are kept frozen during prompt learning.\n3.2\nCLASS-CONDITIONAL FEATURE GENERATOR\nAs shown in Fig. 1(b), the \u201cprompt-tuned\u201d CLIP model tends to overfit the known classes (aka base\nclasses Y b = {y1, y2, ..., yCb}) from the downstream task, while OOD generalization on unknown\nclasses (aka new classes Y n with |Y n| = Cn) will deteriorate. To reduce overfitting, one might\nchoose model regularization strategies, which will inevitably suffer from the missing knowledge\nabout unknowns. Moreover, the potential number of unknown classes Cn is huge and Cn \u226b Cb.\nHence it is very challenging to model their complex distributions for effective regularization.\nHere we make one step towards gaining knowledge of unknowns in a class-conditional manner, in\norder to provide supervision signals for the vast space of unknown data. Given a textual description\nor simply the class name of any unknown class, we aim to synthesize the class example features\nwithout seeing labeled instances (Fig. 2(a)), leveraging the well-aligned image-text feature spaces of\nCLIP. Such synthesized image features will then facilitate learning a regularized decision boundary\nbetween known and unknown classes, leading to improved OOD generalization capabilities.\nIn early experiments, we found that directly generating OOD image features out of class names\nis hard due to the highly non-linear and high-dimensional nature of the former. This is similarly\nobserved in those strong cases of OOD generalization in (Abbe et al., 2023), where the manifold\nembeddings are typically nonlinear and, more critically, part of the distribution domain is entirely\nunseen at training. It is proved that successful learning under such extreme distribution shift leads\nto extrapolating solutions since memorization is voided on the unseen domain. Following the \u201cex-\ntrapolating bias\u201d on the unknown, we reframe our feature synthesis problem as an easier one \u2014\nextrapolating from the most similar classes of the seen data, e.g., to generate features of the un-\nknown class raccoon by extrapolating features of the similar training classes like cat and bear.\nMore specifically, for prompt learning, given the learned prompts and one unknown [CLASS] from\nthe open set Y n, we first obtain the corresponding text features \u02c6wn \u2208 Rd through the text encoder\n\u03c8 of CLIP. Then we find for \u02c6wn its kNN classes from the entire set of text features of known\nclasses \u02c6\nW b \u2208 Rd\u00d7Cb, resulting in \u02c6\nW b\nR \u2208 Rd\u00d7K where R is the neighbor set with |R| = K. From\neach of the kNN classes, we randomly sample only one class example and obtain its text-aligned\nimage features from the image encoder \u03d5, leading to the same number of K image feature vectors\nZb\nR \u2208 Rd\u00d7K. Our goal is to train a class-conditional feature generator f\u03b8( \u02c6wn, \u02c6\nW b\nR, Zb\nR) that can\nsynthesize unknown image features conditioned on the text features \u02c6wn of an unknown class and\nauxiliary text/image features ( \u02c6\nW b\nR, Zb\nR) of kNN known classes, see Fig. 2 (b).\nRemarks. To retrieve semantically similar kNN classes \u02c6\nW b\nR from \u02c6\nW b, we choose to use the cosine\nsimilarity score between the text features (not image features) of class pairs. Then the kNN retrieval\n4\nPublished as a conference paper at ICLR 2024\nprocess can be formally defined as:\narg max\nR\u2282{1,...,Cb}:|R|=K\nX\ni\u2208R\ncos\n\u0000 \u02c6wn, \u02c6wb\ni\n\u0001\n, where\n\u02c6wb\ni = \u02c6\nW b\n:,i.\n(2)\nOn another note, our empirical study shows that the one random example sampled from each kNN\nclass is enough for assisting new feature generation. Such randomness encourages the diversity of\nthe synthesized features for new classes.\nExtrapolating per class. Recall the tuple ( \u02c6\nW b\nR, Zb\nR) consists of K text and image feature vectors\nrespectively (one for each similar known class). One straightforward feature synthesis method for\nan unknown class (with text features \u02c6wn) is to extrapolate each image feature vector in Zb\nR based\non some notion of similarity with \u02c6wn, leading to a total of K extrapolated image features from K\nknown classes (e.g., cat\u2192raccoon, bear\u2192raccoon,...). The similarity notion can be well learned\nby Multi-Head Cross-Attention (MHCA) that operates on the triplets of queries, keys and values\n( \u02c6wn, \u02c6\nW b\nR, Zb\nR). This way, we can effectively take into account the similarity between the unknown\nclass and each known class in R as well as all other between-class similarities. Summarizing, the\nmatrix form of our \u201cextrapolating-per-class\u201d scheme is given as:\nZn = LN(Zb\nR + \u02c6Zn) \u2208 Rd\u00d7K,\n\u02c6Zn = MHCA( \u02c6wn \u00b7 1\u22a4\nK, \u02c6\nW b\nR, Zb\nR) \u2208 Rd\u00d7K,\n(3)\nwhere \u02c6Zn are the learned feature residuals when extrapolating each of the K known classes. LN\ndenotes layer normalization. Obviously, our feature generator \u03b8 is lightweight with only one MHCA\nlayer and one LN layer. The simplicity benefits from the \u201cextrapolating bias\u201d in our generator design.\nFinally, we use the synthesized features Zn to regularize prompt learning and perform joint discrim-\nination of Cb known and Cn unknown class features. The objective of maximizing the image-text\nalignment in Eq. (1) now becomes:\np(y = c | Zn) = 1\nK\nK\nX\nk=1\nexp (cos ( \u02c6wc, zn\nk ) /\u03c4)\nPCb+Cn\ni=1\nexp (cos ( \u02c6wi, zn\nk ) /\u03c4)\n, \u2200c \u2208 {1, . . . , Cb + Cn},\n(4)\nwhere \u02c6wc = [ \u02c6\nW b, \u02c6\nW n]:,c and zn\nk = Zn\n:,k. Note under the \u201cextrapolating-per-class\u201d scheme, we\nhave synthesized K image features for the same unknown class. We simply aggregate them at the\nscore level when computing the cosine feature similarity score in Eq. (4).\nExtrapolating jointly is a more collaborative approach for new feature synthesis. As the name hints,\nwe extrapolate a single image feature vector zn from all the kNN known class features ( \u02c6\nW b\nR, Zb\nR),\nbased on the cross attention against \u02c6wn:\nzn = LN(FFN( \u02c6wn) + \u02c6zn) \u2208 Rd,\n\u02c6zn = MHCA( \u02c6wn, \u02c6\nW b\nR, Zb\nR) \u2208 Rd,\n(5)\nwhere \u02c6zn is the residual image feature vector, while text features \u02c6wn are projected into the image\nfeature space via a two-layer fully connected feed-forward network FFN. Note FFN( \u02c6wn) could be\nreplaced by some anchor point directly searched in the image feature space, e.g., a weighted average\nof kNN image features from Zb\nR. However, searching is a hard problem itself and learning an explicit\ntext-to-image feature mapping works consistently better in our experiments. Fig. 2 (c) summarizes\nthe overall network architecture, and the objective function in Eq. (4) could be updated as:\np(y = c | zn) =\nexp (cos ( \u02c6wc, zn) /\u03c4)\nPCb+Cn\ni=1\nexp (cos ( \u02c6wi, zn) /\u03c4)\n, \u2200c \u2208 {1, . . . , Cb + Cn}.\n(6)\nRemarks. Our ablation study (Table 4) shows that \u201cextrapolating jointly\u201d (our default approach) is\nbetter than \u201cextrapolating per class\u201d at synthesizing useful unknown features for joint optimization.\nWe train our class-conditional feature generator using the \u201cknown\u201d and \u201cunknown\u201d class splits from\nthe training set of downstream tasks. Fig. 3 demonstrates the ability of our feature generator to\ngeneralize to \u201cunknown unknowns\u201d during testing, with faithful image feature synthesis.\n3.3\nADAPTIVE SELF-DISTILLATION\nOptimizing both known and synthesized unknown features generally improves OOD generalization\nand oftentimes the ID performance too. However, that does not take into account the optimization\n5\nPublished as a conference paper at ICLR 2024\nInternal use only\u2013do not distribute.\nunknown class \nJapanese anemone\nkNN class \nColumbine\nkNN class \nHippeastrum\nkNN class \nDesert-rose\nknown class\nunknown class\nsynthesized\nImage feature space\nkNN retrieval\nunseen instance\nunknown class \nJapanese \nanemone\nkNN class \nColumbine\nkNN class \nHippeastrum\nkNN class \nDesert-rose\nFigure 3: Visualizing image feature synthesis based on the joint extrapolation scheme (Eq. (5))\non Flowers102 dataset. Note our feature generator is not trained on the unknown classes, but can\nstill synthesize faithful image features (red triangle) lying close to the real ones (gray cross). This is\nachieved by extrapolating an unseen instance from the kNN class examples (only a random one per\nkNN class is used), effectively combining their related patterns like the shape and texture of flowers.\ndynamics that could also impact the ID-OOD performance tradeoff, especially with long finetuning\nruns. Take Fig. 1(b) for example. Without proper regularization, the CoOp baseline achieves either\nsuboptimal ID performance at early epochs, or saturated ID performance but decreasing OOD per-\nformance (i.e., overfitting) later on. To address this issue, we introduce an adaptive self-distillation\nmethod that regularizes optimization dynamics to further reduce overfitting.\nMore specifically, we use the model checkpoints from earlier epochs (i.e., teacher model often with\nless overfitting) to guide optimization at the current epoch (i.e., student model often with more\noverfitting). Since the CLIP model is frozen during prompt learning, the \u201cmodel\u201d we consider here is\nour feature generator \u03b8 whose synthesized OOD features will impact the joint ID-OOD optimization.\nHence we enforce the consistency between the final prediction probabilities (Eq. (4) or (6)) induced\nby the teacher model pT and student model pS using the mean squared error MSE(pT , pS). Ideally,\nthis will help us to avoid OOD performance drop while preserving the ID performance.\nThe key to our self-distillation method is the choice of teacher model \u03b8T . Obviously, selecting\n\u03b8T as one single model checkpoint at a historical epoch time is unlikely to strike a good trade-off\nbetween the ID and OOD performance. Mean Teacher (MT) (Tarvainen & Valpola, 2017) is a better\nalternative, which calculates an Exponential Moving Average (EMA) over the past checkpoints up\nuntil the current time t (Eq. (7)). Here we propose Adaptive Local Mean Teacher (ALMT) that\nextends MT in two ways: 1) calculating EMA only within a local time window [t \u2212 mt, t] using the\nlast mt checkpoints. This avoids the negative impact on the teacher\u2019s ID performance from those\nunderfit early checkpoints. 2) the window size mt is time-adaptive such that mt is small in the early\nstage of finetuning (for the same purpose of ruling out underfit checkpoints), and then mt gradually\nincreases in order to cover older checkpoints with improved ID performance but less overfitting.\nSuch curriculum is summarized in Eq. (8) as below:\nMT[1,t] : \u03b8T\ni = \u03b1\u03b8T\ni\u22121 + (1 \u2212 \u03b1)\u03b8i,\nfor\ni = {1, . . . , t},\n(7)\nALMTt : MT[t\u2212mt,t], mt =\n\u0016\u0012\n1 + cos\n\u0012tmax + t\ntmax\n\u03c0\n\u0013\u0013\n\u00b7 1\n2(mmax \u2212 mmin) + mmin\n\u0017\n, (8)\nwhere mmax = 9, mmin = 2, tmax is the maximum number of finetuning epochs, and the window\nsize mt is increased following a cosine schedule. Note our ALMT method requires maintaining\na queue of past mt checkpoints and re-calculating EMA for each time t, both of which are cheap\nthanks to our compact model size of \u03b8i and the small window size mt \u2208 {2, . . . , 9}.\n4\nEXPERIMENTS\nWe evaluate OOD generalization under the two settings introduced in (Zhou et al., 2022b) (more\ndetails in Appendix A): 1) generalization from ID (base) to OOD (new) classes within one dataset.\nThe base and new class splits are used for finetuning and evaluation respectively. 2) cross-dataset\ngeneralization with one ID dataset for finetuning and other datasets for OOD evaluation. The cross-\n6\nPublished as a conference paper at ICLR 2024\nTable 1: Base-to-new class generalization. Our OGEN approach consistently improves the new class gener-\nalization for all prompt learning baselines on average (across 11 datasets). OGEN also maintains or improves\nthe average performance on base classes. H: Harmonic mean of base and new accuracies (%).\nCoOp\nCoCoOp\nVPT\nSHIP\nKgCoOp\nMaPLe\nPromptSRC\n+OGEN\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\n\u2717\n\u2713\nAvg across\n11 datasets\nBase\n82.69\n83.47\n80.47\n79.86\n82.51\n82.52\n80.03\n80.79\n80.73\n81.34\n82.28\n82.40\n84.26\n84.17\nNew\n63.22\n69.54\n71.69\n73.35\n69.01\n70.61\n73.69\n76.14\n73.60\n75.68\n75.14\n76.37\n76.10\n76.86\n\u2206\n+6.32\n+1.66\n+1.60\n+2.45\n+2.08\n+1.23\n+0.76\nH\n71.66\n75.87\n75.83\n76.47\n75.16\n76.10\n76.73\n78.40\n77.00\n78.40\n78.55\n79.27\n79.97\n80.34\nImageNet\nBase\n76.47\n76.40\n75.98\n76.50\n75.96\n75.09\n75.87\n76.14\n75.83\n75.88\n76.66\n77.02\n77.60\n77.50\nNew\n67.88\n68.80\n70.43\n70.23\n67.32\n67.66\n69.95\n71.18\n69.96\n70.93\n70.54\n70.73\n70.73\n70.97\nH\n71.92\n72.40\n73.10\n73.23\n71.38\n71.18\n72.79\n73.58\n72.78\n73.32\n73.47\n73.74\n74.01\n74.09\nCaltech101\nBase\n98.00\n96.67\n97.96\n96.67\n97.50\n96.33\n97.55\n98.09\n97.72\n98.52\n97.74\n98.37\n98.10\n98.32\nNew\n89.81\n92.61\n93.81\n94.79\n94.10\n92.36\n95.20\n95.26\n94.39\n94.12\n94.36\n94.54\n94.03\n94.76\nH\n93.73\n94.59\n95.84\n95.72\n95.77\n94.30\n96.36\n96.65\n96.03\n96.27\n96.02\n96.42\n96.02\n96.50\nOxfordPets\nBase\n93.67\n95.18\n95.20\n96.49\n96.05\n96.05\n95.37\n96.95\n94.65\n95.91\n95.43\n95.11\n95.33\n95.96\nNew\n95.29\n96.45\n97.69\n97.86\n95.84\n96.84\n97.87\n97.33\n97.76\n97.65\n97.76\n97.89\n97.30\n97.48\nH\n94.47\n95.81\n96.43\n97.17\n95.94\n96.45\n96.61\n97.14\n96.18\n96.77\n96.58\n96.47\n96.30\n96.71\nStanford\nCars\nBase\n78.12\n78.65\n70.49\n68.96\n75.00\n74.23\n68.57\n68.63\n71.76\n71.86\n72.94\n73.63\n78.27\n77.59\nNew\n60.40\n65.28\n73.59\n74.23\n63.45\n67.97\n73.90\n75.45\n75.04\n75.95\n74.00\n74.30\n74.97\n75.17\nH\n68.13\n71.35\n72.01\n71.50\n68.74\n70.96\n71.14\n71.88\n73.36\n73.84\n73.47\n73.96\n76.58\n76.38\nFlowers102\nBase\n97.60\n97.38\n94.87\n93.95\n96.89\n98.03\n94.02\n94.67\n95.00\n95.83\n95.92\n96.52\n98.07\n97.34\nNew\n59.67\n67.70\n71.75\n72.08\n70.02\n69.15\n74.40\n76.49\n74.73\n74.75\n72.46\n74.46\n76.50\n77.67\nH\n74.06\n79.87\n81.71\n81.57\n81.29\n81.09\n83.06\n84.61\n83.65\n83.98\n82.56\n84.06\n85.95\n86.39\nFood101\nBase\n88.33\n89.21\n90.70\n91.17\n88.88\n91.50\n90.54\n91.07\n90.50\n90.80\n90.71\n91.02\n90.67\n90.69\nNew\n82.26\n87.22\n91.29\n91.67\n88.95\n88.53\n91.03\n92.79\n91.70\n92.01\n92.05\n92.02\n91.53\n91.68\nH\n85.19\n88.21\n90.99\n91.42\n88.91\n89.99\n90.78\n91.92\n91.09\n91.40\n91.38\n91.52\n91.10\n91.19\nFGVC\nAircraft\nBase\n40.44\n41.67\n33.41\n35.33\n38.33\n39.33\n34.27\n35.47\n36.21\n37.08\n37.44\n37.07\n42.73\n41.26\nNew\n22.30\n29.14\n23.71\n34.41\n25.27\n26.55\n32.33\n34.32\n33.55\n37.19\n35.61\n37.41\n37.87\n40.26\nH\n28.75\n34.29\n27.74\n34.86\n30.46\n31.70\n33.28\n34.89\n34.83\n37.14\n36.50\n37.24\n40.15\n40.75\nSUN397\nBase\n80.60\n80.86\n79.74\n80.27\n80.27\n79.06\n79.54\n81.14\n80.29\n81.91\n80.82\n81.06\n82.67\n82.57\nNew\n65.89\n67.49\n76.86\n75.69\n74.36\n74.49\n75.27\n75.94\n76.53\n78.83\n78.70\n81.07\n78.47\n78.83\nH\n72.51\n73.57\n78.27\n77.91\n77.20\n76.71\n77.35\n78.45\n78.36\n80.34\n79.75\n81.06\n80.52\n80.65\nDTD\nBase\n79.44\n79.16\n77.01\n75.00\n77.08\n77.43\n74.88\n76.02\n77.55\n78.01\n80.36\n79.73\n83.37\n83.75\nNew\n41.18\n50.96\n56.00\n56.44\n53.62\n55.79\n56.88\n64.62\n54.99\n62.56\n59.18\n62.68\n62.97\n62.54\nH\n54.24\n62.01\n64.85\n64.41\n63.24\n64.85\n64.65\n69.86\n64.35\n69.43\n68.16\n70.18\n71.75\n71.60\nEuroSAT\nBase\n92.19\n91.67\n87.49\n78.33\n91.67\n90.00\n88.62\n89.17\n85.64\n86.05\n94.07\n93.83\n92.90\n93.40\nNew\n54.74\n73.51\n60.04\n64.69\n58.31\n66.75\n66.87\n74.28\n64.34\n70.18\n73.23\n74.30\n73.90\n76.74\nH\n68.69\n81.59\n71.21\n70.86\n71.28\n76.65\n76.22\n81.05\n73.48\n77.30\n82.35\n82.93\n82.32\n84.25\nUCF101\nBase\n84.69\n91.33\n82.33\n85.78\n80.07\n90.68\n81.08\n81.33\n82.89\n82.84\n83.00\n82.99\n87.10\n87.44\nNew\n56.05\n65.81\n73.45\n74.78\n74.50\n70.54\n76.85\n79.83\n76.67\n78.28\n78.66\n80.68\n78.80\n79.28\nH\n67.46\n76.50\n77.64\n79.90\n77.18\n79.35\n78.91\n80.57\n79.65\n80.49\n80.77\n81.82\n82.74\n83.16\ndataset setting is more challenging since there will be both domain- and class-incremental distribu-\ntion shift, e.g., from generic object classification on ImageNet (Deng et al., 2009) to satellite imagery\nrecognition on EuroSAT (Helber et al., 2019).\nDatasets. For both settings we use 11 datasets: ImageNet (Deng et al., 2009), Caltech101 (Fei-Fei\net al., 2004), OxfordPets (Parkhi et al., 2012), StanfordCars (Krause et al., 2013), Flowers102 (Nils-\nback & Zisserman, 2008), Food101 (Bossard et al., 2014), FGVC-Aircraft (Maji et al., 2013),\nSUN397 (Xiao et al., 2010), UCF101 (Soomro et al., 2012), DTD (Cimpoi et al., 2014) and Eu-\nroSAT (Helber et al., 2019).\nBaselines. For finetuning, we consider prompt learning approaches CoOp (Zhou et al., 2022a), Co-\nCoOp (Zhou et al., 2022b), VPT (Jia et al., 2022), and the state-of-the-art methods SHIP (Wang\net al., 2023), KgCoOp (Yao et al., 2023), MaPLe (Khattak et al., 2023a) and PromptSRC (Khattak\net al., 2023b). For each baseline, we apply our method (dubbed OGEN) to obtain an OOD GENer-\nalization improved version. For fairness, we use the same implementation details of each baseline,\nincluding the prompt length, vision backbone in CLIP (Radford et al., 2021) (i.e., ViT-B/16) and\ntrain/test data splitting. The reported results are an average over three random seeds.\n4.1\nGENERALIZATION FROM BASE TO NEW CLASSES\nThe base-to-new generalization setting creates a strictly class-incremental distribution shift since\nthe base and new class splits in one dataset are disjoint. All prompt learners are trained on the base\nclasses, and tested on the base and new classes separately to evaluate the trade-off between ID and\nOOD performance. Here we follow (Xian et al., 2017) to report the harmonic mean of base and new\nclass accuracies to quantify such trade-off.\n7\nPublished as a conference paper at ICLR 2024\nTable 2: Cross-dataset generalization: CLIP finetuning (prompt learning) on the source dataset ImageNet,\nfollowed by testing on 10 target datasets. Our method OGEN improves the generalization performance of both\nCoOp and CoCoOp on all the target datasets.\nSource\nTarget\nImageNet\nCaltech101\nOxfordPets\nStanfordCars\nFlowers102\nFood101\nFGVCAir\nSUN397\nDTD\nEuroSAT\nUCF101\nAverage\nCoOp\n71.51\n93.70\n89.14\n64.51\n68.71\n85.30\n18.47\n64.15\n41.92\n46.39\n66.55\n63.88\nOGEN-CoOp\n71.52\n94.60\n90.73\n65.07\n70.55\n87.26\n19.84\n65.77\n44.90\n49.53\n69.36\n65.76\nCoCoOp\n71.02\n94.43\n90.14\n65.32\n71.88\n86.06\n22.94\n67.36\n45.73\n45.37\n68.21\n65.74\nOGEN-CoCoOp\n71.28\n95.12\n91.37\n66.04\n72.90\n86.54\n22.95\n68.42\n46.38\n45.82\n69.74\n66.53\nTable 1 summarizes the results on 11 datasets. On average, our OGEN method consistently im-\nproves the new class accuracy for all the prompt learning baselines. CoOp is particularly interesting\nsince its default learning schedule (200 epochs) is much longer than that of CoCoOp and VPT (10\nepochs). Without proper regularization, CoOp inevitably shows more serious overfitting to the base\nclasses (82.69% on average) with low performance on new classes (63.22%) after long training runs.\nOur OGEN is especially useful in this case, significantly improving the average new class accuracy\nof CoOp from 63.22% to 69.54%. As also visualized in Appendix C - Fig. 6(a), the new class gen-\neralization sees notable gains on 3 datasets \u2014 DTD for texture classification, EuroSAT for satellite\nimage recognition and UCF101 for action recognition, which all demonstrate large inter-class varia-\ntions. This validates the superior generalizability of OGEN, thanks to its capability of OOD feature\nsynthesis and regularization. OGEN also improves the average base class accuracy of CoOp from\n82.69% to 83.47%. Specifically, OGEN improves on 6 datasets with negligible performance drop\non other 5, see Fig. 6(b). The gains on base classes can be attributed to 1) the joint discrimination of\nknown and unknown classes and 2) our adaptive self-distillation method that strikes a good ID-OOD\nperformance tradeoff.\nFor CoCoOp and VPT with a significantly shorter training schedule, they suffer from much less\noverfitting with higher new but lower base accuracies than CoOp. This makes our OGEN unable to\nunleash its full potential to address overfitting. That said, we find both OGEN-CoCoOp and OGEN-\nVPT can still improve the average new class accuracy while achieving a similar base class accuracy.\nWe are likely to further improve the base accuracy when given a longer optimization schedule that\nallows more ID-OOD performance balancing.\nAmong the state-of-the-art methods, SHIP (+CoOp) and PromptSRC are related to our OGEN ap-\nproach in the use of similar techniques of feature synthesis and self-regularization respectively. Ta-\nble 1 shows that OGEN can improve the new class generalization of both SHIP and PromptSRC by\nexploring the synergy between regularization and OOD feature synthesis. OGEN also consistently\nimproves the average base and new class accuracies for KgCoOp and MaPLe. Fig. 5 uses KgCoOp\nto exemplify how these methods still suffer from overfitting (although reduced to some extent by\nvarious techniques), and how our OGEN improves the learning curves of both base and new classes.\nIt is worth noting that different methods are trained for different numbers of epochs, thus again, they\nhave different levels of overfitting. OGEN improves generalization more over SHIP (200 epochs)\nand KgCoOp (100 epochs) with long learning schedules (more serious overfitting). Our gains are\nsmaller over MaPLe (5 epochs) and PromptSRC (20 epochs) with short training runs, but larger\ngains are expected when trained for longer runs.\n4.2\nCROSS-DATASET GENERALIZATION\nTable 2 shows the generalization performance from ImageNet to 10 target datasets. We consider\nthe representative CoOp and CoCoOp baselines with long and short training runs respectively. As\nshown in the table, our OGEN uniformly improves the generalization performance (across baselines\nand target datasets) with competitive source dataset performance. The improvements are especially\nlarge on those low performing datasets DTD, EuroSAT, UCF101 with large distribution shift from\nImageNet. This highlights the benefits of our OOD feature generation module. OGEN also ob-\ntains reasonable gains on the high performing datasets like OxfordPets that contains similar classes\n(e.g., different dog breeds) with ImageNet, demonstrating the universality of our approach.\n8\nPublished as a conference paper at ICLR 2024\nTable 3: Ablating our class-conditional feature\ngenerator \u03b8 (Eq. (5)) and self-distillation method\nALMT (Eq. (8)). H: Harmonic mean of the base\nand new class accuracies averaged on 11 datasets.\n\u03b8\nALMT\nBase\nNew\nH\n\u2717\n\u2717\n82.69\u00b11.08\n63.22\u00b10.51\n71.66\u00b10.54\n\u2713\n\u2717\n82.49\u00b10.95\n69.02\u00b10.64\n75.15\u00b11.12\n\u2713\n\u2713\n83.47\u00b10.30\n69.54\u00b10.34\n75.88\u00b10.11\nTable 4: Class-conditional feature generator:\ndifferent design choices of no extrapolation from\nkNN classes, extrapolating per class (Eq. (3)) and\nextrapolating jointly (Eq. (5)).\nBase\nNew\nH\nNo Extrap\n83.34\u00b10.26\n64.08\u00b10.95\n72.46\u00b10.68\nExtrap per class\n82.90\u00b10.35\n66.04\u00b10.89\n73.52\u00b10.63\nExtrap jointly\n82.49\u00b10.95\n69.02\u00b11.25\n75.15\u00b11.12\nTable 5: Class-conditional feature generator:\nkNN retrieval vs.\nRandom sampling of known\nclasses with varying K.\nK\nBase\nNew\nH\nkNN\n1\n82.76\u00b10.49\n67.01\u00b11.35\n74.06\u00b10.63\n2\n82.35\u00b10.76\n67.79\u00b12.37\n74.36\u00b11.19\n3\n82.49\u00b10.95\n69.02\u00b11.25\n75.15\u00b11.12\n4\n82.37\u00b10.46\n68.85\u00b10.52\n75.00\u00b10.13\nRand\n3\n81.69\u00b10.35\n68.30\u00b10.38\n74.40\u00b10.36\nTable 6: Self-distillation: Mean Teacher (MT)\nvs. Adaptive Local Mean Teacher (ALMT) with\nfixed window size m or adaptive mt (default).\nBase\nNew\nH\nNo distillation\n82.49\u00b10.65\n69.02\u00b10.64\n75.15\u00b11.12\nMT\n83.34\u00b10.15\n68.30\u00b10.77\n75.08\u00b10.47\nALMT (m = 2)\n81.70\u00b10.59\n68.47\u00b10.59\n74.50\u00b10.55\nALMT (m = 9)\n82.21\u00b10.80\n68.57\u00b10.85\n74.77\u00b10.29\nALMT (mt)\n83.47\u00b10.30\n69.54\u00b10.34\n75.88\u00b10.11\n4.3\nABLATION STUDIES\nOur ablation studies are conducted using OGEN-CoOp with a meaningfully long learning schedule.\nWe start with ablating the two main components of OGEN: class-conditional feature generator \u03b8 and\nadaptive self-distillation method ALMT. Table 3 confirms both components are useful (more visuals\nin Appendix B). We see the feature generator improves the new class accuracy by a large margin\nwithout hurting the base class accuracy. This suggests the high quality of its generated OOD features\nand the need of joint ID-OOD feature optimization. ALMT is shown to further improve on both base\nand new classes, leading to a higher Harmonic mean and a much lower performance variance. This\nhighlights the need of regularizing joint optimization for a good performance tradeoff.\nTable 4 compares different design choices of our class-conditional feature generator. Recall that we\nadopt an extrapolation scheme that extrapolates new image features from the kNN base class fea-\ntures. What if we use no extrapolation at all, and directly learn a mapping from the new class name\nto new image features? As shown in the table, this only slightly helps the new class generalization\nprobably because the generated features are not faithful enough from an unconstrained text-to-image\nmapping. Then between the \u201cExtrapolating per class\u201d and \u201cExtrapolating jointly\u201d schemes, the latter\nimproves on new classes much more significantly, showing the benefits of collaborative class rela-\ntion modeling for extrapolation. Table 5 further ablates on the number of kNN base classes needed\nfor extrapolation, arriving at the best K = 3. By comparison, randomly selecting 3 base classes\ndoes not perform as well. Finally, Appendix D illustrates the advantage of our feature synthesis\napproach over replay-based methods using real data.\nTable 6 compares various self-distillation baselines applied on top of our feature generator. No-\ntably, the simple Mean Teacher (MT) is not helping, which inspires us to use a local version to\ncompletely rule out the early-stage underfit model checkpoints. We further propose Adaptive Local\nMean Teacher (ALMT) that calculates EMA within a local time window of increasing size mt (from\nmmin = 2 to mmax = 9). As shown in the table, ALMT achieves the best performance due to the\nadaptiveness of mt, which effectively avoids both the underfitting (from early epochs) and overfit-\nting (from recent epochs) effects in the teacher model. Apparently, this is not possible with a fixed\nwindow size (m = mmin or mmax) which hurts performance.\n5\nCONCLUSIONS AND FUTURE WORK\nIn this paper, we study the OOD generalization of recent CLIP finetuners and propose an effective\napproach to reduce their overfitting to seen classes. For that, a class-conditional feature generator\nis used to synthesize OOD features for joint optimization, and the optimization dynamics are fur-\nther regularized by an adaptive distillation scheme. The superior generalization capability of our\napproach is demonstrated under different OOD settings. In the future, we plan to go beyond prompt\nlearning and evaluate how much our benefits hold for other finetuning methods like adaptor tuning.\nMoreover, it would be interesting to figure out how well our \u201cunknown-aware\u201d approach can model\nuncertainties on unseen data, which can be evaluated on existing OOD detection benchmarks.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nEmmanuel Abbe, Samy Bengio, Aryo Lotfi, and Kevin Rizk. Generalization on the unseen, logic\nreasoning and degree curriculum. In ICML, 2023.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2013mining discriminative compo-\nnents with random forests. In ECCV, 2014.\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebas-\ntian Alexander Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov,\nJoan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish\nThapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol\nAyan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and\nRadu Soricut. PaLI: A jointly-scaled multilingual language-image model. In ICLR, 2023.\nMircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-\nscribing textures in the wild. In CVPR, 2014.\nWenliang Dai, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, and Pascale Fung. Enabling multimodal\ngeneration on CLIP via vision-language knowledge distillation. In ACL, 2022.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale\nhierarchical image database. In CVPR, 2009.\nXiaoyi Dong, Jianmin Bao, Yinglin Zheng, Ting Zhang, Dongdong Chen, Hao Yang, Ming Zeng,\nWeiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. MaskCLIP: Masked self-\ndistillation advances contrastive language-image pretraining. In CVPR, 2023.\nXuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. Vos: Learning what you don\u2019t know by\nvirtual outlier synthesis. In ICLR, 2022.\nSepideh Esmaeilpour, Bing Liu, Eric Robertson, and Lei Shu. Zero-shot out-of-distribution detec-\ntion based on the pretrained model clip. In AAAI, 2022.\nLi Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training\nexamples: An incremental bayesian approach tested on 101 object categories. In CVPR workshop,\n2004.\nStanislav Fort, Jie Jessie Ren, and Balaji Lakshminarayanan.\nExploring the limits of out-of-\ndistribution detection. In NeurIPS, 2021.\nGolnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Open-vocabulary image segmentation. arXiv\npreprint arXiv:2112.12143, 2021.\nXiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision\nand language knowledge distillation. In ICLR, 2022.\nPatrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. Eurosat: A novel dataset\nand deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl.\nEarth Obs. Remote Sens., 2019.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\nHexiang Hu, Yi Luan, Yang Chen, Urvashi Khandelwal, Mandar Joshi, Kenton Lee, Kristina\nToutanova, and Ming-Wei Chang. Open-domain visual entity recognition: Towards recogniz-\ning millions of wikipedia entities. arXiv preprint arXiv:2302.11154, 2023.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan\nSung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning\nwith noisy text supervision. In ICML, 2021.\nMenglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and\nSer-Nam Lim. Visual prompt tuning. In ECCV, 2022.\n10\nPublished as a conference paper at ICLR 2024\nMuhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shah-\nbaz Khan. MaPLe: Multi-modal prompt learning. In CVPR, 2023a.\nMuhammad Uzair Khattak, Syed Talal Wasim, Muzammal Naseer, Salman Khan, Ming-Hsuan\nYang, and Fahad Shahbaz Khan. Self-regulating prompts: Foundational model adaptation without\nforgetting. In ICCV, 2023b.\nWonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convo-\nlution or region supervision. In ICML, 2021.\nJonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained\ncategorization. In ICCV workshops, 2013.\nKimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers\nfor detecting out-of-distribution samples. In ICLR, 2018.\nJunnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak Gotmare, Shafiq Joty, Caiming Xiong, and\nSteven Hoi. Align before fuse: Vision and language representation learning with momentum\ndistillation. In NeurIPS, 2021.\nXuanlin Li, Yunhao Fang, Minghua Liu, Zhan Ling, Zhuowen Tu, and Hao Su. Distilling large\nvision-language model with out-of-distribution generalizability. In ICCV, 2023.\nYangguang Li, Feng Liang, Lichen Zhao, Yufeng Cui, Wanli Ouyang, Jing Shao, Fengwei Yu,\nand Junjie Yan. Supervision exists everywhere: A data efficient contrastive language-image pre-\ntraining paradigm. In ICLR, 2022.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\nZ. Mal, G. Luo, J. Gao, L. Li, Y. Chen, S. Wang, C. Zhang, and W. Hu. Open-vocabulary one-stage\ndetection with hierarchical visual-language knowledge distillation. In CVPR, 2022.\nYifei Ming, Ziyang Cai, Jiuxiang Gu, Yiyou Sun, Wei Li, and Yixuan Li. Delving into out-of-\ndistribution detection with vision-language representations. In NeurIPS, 2022.\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number\nof classes. In ICVGIP, 2008.\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In CVPR,\n2012.\nRui Qian, Yeqing Li, Zheng Xu, Ming-Hsuan Yang, Serge Belongie, and Yin Cui. Multimodal\nopen-vocabulary video classification via pre-trained vision and language models. arXiv preprint\narXiv:2207.07646, 2022.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset\nof clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\nYang Shu, Xingzhuo Guo, Jialong Wu, Ximei Wang, Jianmin Wang, and Mingsheng Long.\nCLIPood: Generalizing clip to out-of-distributions. In ICML, 2023.\nKihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel,\nEkin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised\nlearning with consistency and confidence. In NeurIPS, 2020.\nKhurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: A dataset of 101 human actions\nclasses from videos in the wild. arXiv preprint arXiv:1212.0402, 2012.\n11\nPublished as a conference paper at ICLR 2024\nLeitian Tao, Xuefeng Du, Jerry Zhu, and Yixuan Li. Non-parametric outlier synthesis. In ICLR,\n2023.\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consis-\ntency targets improve semi-supervised deep learning results. In NeurIPS, 2017.\nZhengbo Wang, Jian Liang, Ran He, Nan Xu, Zilei Wang, and Tieniu Tan. Improving zero-shot\ngeneralization for clip with synthesized prompts. In ICCV, 2023.\nYongqin Xian, Bernt Schiele, and Zeynep Akata. Zero-shot learning-the good, the bad and the ugly.\nIn CVPR, 2017.\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database:\nLarge-scale scene recognition from abbey to zoo. In CVPR, 2010.\nHantao Yao, Rui Zhang, and Changsheng Xu. Visual-language prompt tuning with knowledge-\nguided context optimization. In CVPR, 2023.\nYuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Open-vocabulary detr\nwith conditional matching. In ECCV, 2022a.\nYuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Unified vision and\nlanguage prompt learning. arXiv preprint arXiv:2210.07225, 2022b.\nRenrui Zhang, Zhang Wei, Rongyao Fang, Peng Gao, Kunchang Li, Jifeng Dai, Yu Jiao Qiao, and\nHongsheng Li. Tip-adapter: Training-free adaption of clip for few-shot classification. In ECCV,\n2022.\nYiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li,\nLuowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image\npretraining. In CVPR, 2022.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-\nlanguage models. IJCV, 2022a.\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for\nvision-language models. In CVPR, 2022b.\n12\nPublished as a conference paper at ICLR 2024\nInternal use only\u2013do not distribute.\nEuroSAT\nDTD\nUCF101\nunknown class\nknown class\nFlowers102\nStanfordCars\nOxfordPets\nFigure 4: More example learning curves of the long finetuning runs (200 epochs) with CoOp (Zhou\net al., 2022a) method. Under the within-dataset generalization setting, CoOp typically overfits the\nknown classes and achieves decreasing accuracy for the unknown classes. The class-conditional\nfeature generator plays a key role in our full method OGEN, which reduces overfitting by generating\nOOD features for the unknown-aware optimization. Our adaptive self-distillation method further\nreduces overfitting via regularizing the optimization dynamics.\nInternal use only\u2013do not distribute.\nEuroSAT\nDTD\nUCF101\nunknown class\nknown class\nFigure 5: Learning curves of the long finetuning runs (100 epochs) with KgCoOp (Yao et al., 2023)\nvs. OGEN-KgCoOp methods (within-dataset generalization setting). Despite the overfitting reduc-\ning technique used in KgCoOp, it still suffers from some extent of overfitting. See how OGEN often\nimproves the learning curves of both base and new classes.\nA\nREMARKS ON PROBLEM DEFINITION\nThe focus of this paper is to understand the behaviors of and improve OOD generalization of CLIP\nfinetuning. As shown in Fig. 1(a) of the main paper, the CLIP model can be finetuned on various\ndownstream tasks before evaluating OOD generalization. Two settings are considered: 1) within-\ndataset generalization where one dataset has ID (base) vs. OOD (new) class splits for finetuning and\nevaluation respectively; 2) cross-dataset generalization with one ID dataset for finetuning and other\nOOD datasets for evaluation.\nNote CLIP is pretrained on enormous volumes of data, which inevitably could have class overlap\nwith some \u201cOOD\u201d data for evaluation. Also, there could be potential class overlap between the\ncurated ID and OOD data themselves, e.g., under the cross-dataset generalization setting where the\ndataset pair may include similar categories in their class taxonomies. Therefore, we consider more of\na generalized OOD generalization test for the large-scale pretrained CLIP model. That is, whenever\nthe class overlap happens, the domain-incremental distribution shift is considered in our evaluation,\notherwise we evaluate under the strictly class-incremental distribution shift.\n13\nPublished as a conference paper at ICLR 2024\nAbsolute gain (%)\n4\n12\n16\n20\n8\n(a)  OGEN-CoOp vs. CoOp on new classes\n0\n18.77\n9.76\n9.78\n1.60\n6.84\n4.96\n8.03\n4.88\n1.16\n2.80\n0.92\nImageNet\nOxfordPets\nCaltech101\nStanfordCars\nFlowers102\nFood101\n FGVCAircraft\nSUN397\nDTD\nEuroSAT\nUCF101\nAbsolute change (%)\n0\n4\n6\n8\n2\n(b)   OGEN-CoOp vs. CoOp on base classes\n-2\n-0.52\n6.64\n-0.28\n0.26\n1.23\n0.88\n-0.22\n0.53\n1.51\n-1.33\n-0.07\nImageNet\nOxfordPets\nCaltech101\nStanfordCars\nFlowers102\nFood101\n FGVCAircraft\nSUN397\nDTD\nEuroSAT\nUCF101\nFigure 6: Base-to-new class generalization when our OGEN approach is applied to the CoOp\nbaseline that suffers from overfitting due to a long learning schedule (200 epochs). OGEN largely\novercomes overfitting and (a) improves OOD generalization on new classes for all 11 datasets, some-\ntimes by a large margin. (b) At the same time, OGEN is able to improve the base class accuracies\non most datasets, with only minor accuracy drop on a few others.\nB\nOVERFITTING WITH CLIP FINETUNING: MORE EXAMPLES\nFig. 4 shows more learning curves of the prompt learning method CoOp (Zhou et al., 2022a) that\nfinetunes CLIP on a long schedule. Clearly, CoOp overfits on all the considered datasets with de-\ncreasing generalization performance on the unknown classes. While both components of our pro-\nposed OGEN method \u2013 class-conditional feature generator and adaptive self-distillation \u2013 are found\nuseful to address overfitting. Our feature generator is observed to play a key role by generating OOD\nfeatures for the following unknown-aware optimization and regularization.\nFig. 5 shows example learning curves of one of the state-of-the-art methods KgCoOp (Yao et al.,\n2023). This method has relatively long finetuning runs by default (100 epochs). We can see that\nKgCoOp still suffers from some extent of overfitting, although there is an overfitting reducing com-\nponent in it. Our OGEN can further alleviate overfitting with KgCoOp, improving its learning curves\nof both base and new classes.\nC\nVISUALIZING PER-DATASET RESULTS\nFig. 6 breaks down the performance gap between CoOp and our OGEN-CoOp on both the base\nand new classes for 11 datasets. The base-to-new class generalization setting is considered on each\ndataset.\nD\nFEATURE SYNTHESIS VS. REPLAY-BASED METHOD\nRecall the goal of our class-conditional feature generator is to model the vast space and complex\ndistribution of unknown class data in OOD domains. Here we investigate how well our synthesized\nOOD features can represent the real world of unknown data. Specifically, we explore the use of\nreplay methods by sampling real OOD data from the large-scale LAION-400M dataset (Schuhmann\net al., 2021) and using the sampled data for replay. We compare our synthesized OOD features\nagainst those real OOD data in terms of their contribution to training regularization on downstream\ntasks.\nExperimental details. We experiment under the base-to-new class generalization setting where\nCoOp (Zhou et al., 2022a) is the CLIP finetuning baseline on each of the 11 downstream datasets.\nFor fair comparison between the OGEN- and replay-boosted CoOp variants, we always use ALMT\nwith the only difference in the OOD data source (synthesis vs. replay).\nSampling of replay data.\n\u2022 Class filtering: to ensure the replay data serve as a good proxy of OOD data for each\ndownstream task, we need to perform class filtering when sampling image-text-pairs from\nLAION-400M, i.e., we filter image-text-pairs that are semantically close to those \u201cknown\u201d\nclasses on the considered dataset. We do so by using CLIP to calculate the cosine similarity\n14\nPublished as a conference paper at ICLR 2024\nCoOp+OGEN\nCoOp+replay (hard)\nCoOp+replay (random)\nReplay data scale\n71\n72\n73\n74\n75\n76\n77\n78\n0\n1x\n2x\n3x\n4x\n6x\n7x\n8x\n5x\n~3.5x e\ufb03ciency\n~7.5x e\ufb03ciency\nHarmonic mean acc %\n(a) Average across 11 datasets\nReplay data scale\n93.7\n93.9\n94.1\n94.3\n94.5\n94.7\n94.9\n95.1\n0\n1x\n2x\n3x\n4x\n5x\nHarmonic mean acc %\n(b) Caltech101\nReplay data scale\n52\n54\n56\n58\n60\n62\n64\n66\n0\n2x\n4x\n6x\n8x\n12x\n10x\nHarmonic mean acc %\n(c) DTD\n~2x e\ufb03ciency\n~5x e\ufb03ciency\n1x\n3x\n5x\n7x\n9x\n11x\n~9x e\ufb03ciency\n~12x e\ufb03ciency\nFigure 7: OOD feature synthesis is much more data efficient than replaying real OOD data.\nReal OOD data are sampled from LAION-400M dataset with varying data scale, i.e., multiple times\nmore than the synthesized data. Here we use the CoOp baseline for base-to-new class generalization,\nand measure the Harmonic mean of base and new accuracies. (a) We can see that OGEN with OOD\nfeature synthesis creates about 3.5x and 7.5x gain in data efficiency on average across 11 datasets,\nwhen compared to the replay method with hard sample mining and random sampling strategy re-\nspectively (see text for details). (b-c) The replayed OOD samples, despite being data inefficient,\nare more helpful when they have a closer semantic or data distribution with the downstream dataset\n(e.g., Caltech101) so to act as semi-hard negatives. They are far less useful for distributionally dif-\nferent DTD, the texture database. On the other hand, OGEN benefits from dataset-specific OOD\nfeature synthesis, which often offers hard negatives to boost data efficiency for training.\nbetween the text features of the query text and known class names, and then dropping query\ntexts with the maximum similarity score higher than 0.82. Note our synthesized features\nare guaranteed to be OOD since they are generated in a class-conditional manner with the\nquery classes disjoint from known classes.\n\u2022 Sampling strategy: with class filtering in place, we consider both random data sampling and\nhard sample mining to retrieve replay data from LAION-400M. For hard sample mining,\nwe first rank the query texts from a sufficiently large pool (randomly sampled) in terms\nof the maximum similarity score mentioned above. Then we simply select the top similar\nquery texts and use the corresponding images as hard replay data (similar to hard negatives\nfrom hard negative sampling).\n\u2022 Sampling data size: given the large amount of image-text-pairs in LAION-400M, we in-\ncrease the data size of the replay data (either random or hard) by 1-to-12x more than the\nsize of our synthesized OOD data on each downstream dataset. Note our feature generator\nsynthesizes the same amount of OOD features as the \u201cunknown\u201d class splits of the training\nset of considered dataset.\nObservations from Fig. 7.\n\u2022 Better performance is generally obtained with the use of more real OOD data for replay,\nand performance grows faster with hard mined replay data. When sufficient replay data are\nused, the performance can surpass that of our synthesized OOD features. This demonstrates\nthe benefits of sampling big and diverse OOD data for training regularization purposes.\n\u2022 However, our feature synthesis approach is much more data efficient than replay-based\nmethods. The key reason behind such advantage is that our feature generator is more likely\nto generate hard OOD samples to better regularize decision boundaries, in comparison to\nreal-world samples. In our case, 1) the feature generator itself is trained on the downstream\ndataset, thus can synthesize dataset-specific OOD features that adapt better to the task at\nhand. 2) Recall that we extrapolate OOD features from kNN known class features. This\nsuggests there is inevitable shared information between the known class and synthesized\nfeatures, further increasing the hardness of the latter. On the other hand, both of the afore-\nmentioned factors are missing for the real OOD data sampled from a separate domain,\nwhich contributes to their data inefficiency in training regularization. Real OOD data are\nmost useful when they are distributionally similar to the downstream dataset (e.g., Cal-\ntech101) and hence can act as semi-hard negatives. Otherwise, data efficiency will see a\nsignificant drop when replaying OOD data on distributionally distant dataset like DTD. On\n15\nPublished as a conference paper at ICLR 2024\naverage (across 11 datasets), our feature synthesis method is about 3.5x and 7.5x more data\nefficient than the replay method with hard mining and random sampling respectively.\nSummary. Replay-based methods perform well with large data size, but suffer from low data effi-\nciency as well as large memory cost (to maintain replay data). Our class-conditional feature gener-\nator avoids these issues by synthesizing hard OOD features on the fly. Note our feature generator is\nlightweight and only incurs small computational cost. Its runtime on GPU is 0.019 seconds, which\nis significantly smaller than that of the feature extraction step of CLIP (text encoder: 0.046 seconds,\nimage encoder: 1.016 seconds). One promising future direction is the combination of our feature\nsynthesis method and replay methods, aiming to take advantage of their respective benefits of data\nefficiency and diversity.\n16\n"
  }
]