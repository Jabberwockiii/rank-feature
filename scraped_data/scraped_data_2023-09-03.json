[
  {
    "title": "MVDream: Multi-view Diffusion for 3D Generation",
    "link": "https://arxiv.org/pdf/2308.16512.pdf",
    "upvote": "99",
    "text": "MVDREAM:\nMULTI-VIEW DIFFUSION FOR 3D GENERATION\nYichun Shi1, Peng Wang1, Jianglong Ye2\u2217, Long Mai1, Kejie Li1, Xiao Yang1\n1 ByteDance, USA, 2 University of California, San Diego\n{yichun.shi,peng.wang,kejie.li,xiao.yang}@bytedance.com\n{jianglong.yeh,mai.t.long88}@gmail.com\nABSTRACT\nWe introduce MVDream, a diffusion model that is able to generate consistent multi-\nview images from a given text prompt. Learning from both 2D and 3D data, a multi-\nview diffusion model can achieve the generalizability of 2D diffusion models and\nthe consistency of 3D renderings. We demonstrate that such a multi-view diffusion\nmodel is implicitly a generalizable 3D prior agnostic to 3D representations. It\ncan be applied to 3D generation via Score Distillation Sampling, significantly\nenhancing the consistency and stability of existing 2D-lifting methods. It can also\nlearn new concepts from a few 2D examples, akin to DreamBooth, but for 3D\ngeneration. Our project page is https://MV-Dream.github.io\n1\nINTRODUCTION\n3D content creation is an important step in the pipeline of modern game and media industry, yet it\nis a labor-intensive task that requires well-trained designers to work for hours or days to create a\nsingle 3D asset. A system that can generate 3D content in an easy way for non-professional users\nis thus of great value. Existing 3D object generation methods can be categorized into three types:\n(1) template-based generation pipeline, (2) 3D generative models, and (3) 2D-lifting methods. Due\nto limited accessible 3D models and large data complexity, both template-based generators and 3D\ngenerative models struggle to generalize effectively to arbitrary object generation. Their generated\ncontent is often confined to common real-world objects with relatively simple topology and texture.\nYet in industry, popular 3D assets usually come as a mixture of complicated, artistic, and sometimes\nnon-realistic structures and styles (Ske).\nRecently, 2D-lifting methods have shown that pre-trained 2D generation models can be potentially\napplied to 3D generation. The typical representations are Dreamfusion (Poole et al., 2023) and\nMagic3D (Lin et al., 2023a) systems, which utilize 2D diffusion models as supervision for the\noptimization of a 3D representation via score distillation sampling (SDS). Trained on large-scale 2D\nimage datasets, these 2D models are able to generate unseen and counterfactual scenes whose details\ncan be specified through a text input, making them great tools for creating artistic assets.\nNevertheless, in 2D-lifting techniques, challenges arise due to the lack of comprehensive multi-\nview knowledge or 3D-awareness during score distillation. These challenges encompass: (1) The\nmulti-face Janus issue: The system frequently regenerates content described by the text prompt. (2)\nContent drift across different views. Examples can be seen in Fig. (1). The multi-face issue can\nstem from various factors. For instance, certain objects, like blades, may be nearly invisible from\nsome angles. Meanwhile, vital parts of a character or animal might be hidden or self-occluded from\nspecific viewpoints. While humans assess these objects from multiple angles, a 2D diffusion model\ncannot, leading it to produce redundant and inconsistent content.\nIn spite of all the weaknesses of 2D-lifting methods, we believe that large-scale 2D data is crucial\nto generalizable 3D generation. Therefore, we propose multi-view diffusion models, which can be\nused as a multi-view 3D prior agnostic to 3D representations. The proposed model simultaneously\ngenerates a set of multi-view images that are consistent with each other. It can leverage pre-trained\n2D diffusion models for transfer learning to inherit the generalizability. Then, by jointly training the\nmodel on multi-view images (from 3D assets) and 2D image-text pairs, we find that it can achieve\nboth good consistency and generalizability. When applied to 3D generation through score distillation,\n\u2217Work done during internship at ByteDance\n1\narXiv:2308.16512v3  [cs.CV]  16 Mar 2024\nMulti-face Janus Problem\nContent Drift Problem\nFigure 1: Typical multi-view consistency problems of 2D-lifting methods for 3D generation. Left: \u201cA\nphoto of a horse walking\u201d where the horse has two faces. Right: \u201ca DSLR photo of a plate of fried\nchicken and waffles with maple syrup on them\u201d where the chicken gradually becomes a waffle.\nour multi-view supervision proves significantly more stable than that of single-view 2D diffusion\nmodels. And we can still create unseen, counterfactual 3D contents as from pure 2D diffusion models.\nInspired by DreamBooth (Ruiz et al., 2023), we can also employ our multi-view diffusion model to\nassimilate identity information from a collection of 2D images and it demonstrates robust multi-view\nconsistency after fine-tuning. Overall, our model, namely MVDream, successfully generates 3D Nerf\nmodels without the multi-view consistency issue. It either surpasses or matches the diversity seen in\nother state-of-the-art methods.\n2\nRELATED WORK AND BACKGROUND\n2.1\n3D GENERATIVE MODELS\nThe significance of 3D generation has driven the application of nearly all deep generative models\nto this task. Handerson et al. explored Variational Auto Encoders (Kingma & Welling, 2014)\nfor textured 3D generation (Henderson & Ferrari, 2020; Henderson et al., 2020). However, their\nstudies mainly addressed simpler models leveraging multi-view data. With Generative Adversarial\nNetworks (GANs) yielding improved results in image synthesis, numerous studies have investigated\n3D-aware GANs (Nguyen-Phuoc et al., 2019; 2020; Niemeyer & Geiger, 2021; Deng et al., 2022;\nChan et al., 2022; Gao et al., 2022). These methods, attributed to the absence of reconstruction loss\ninvolving ground-truth 3D or multi-view data, can train solely on monocular images. Yet, akin to 2D\nGANs, they face challenges with generalizability and training stability for general objects and scenes.\nConsequently, diffusion models, which have shown marked advancements in general image synthesis,\nhave become recent focal points in 3D generation studies. Various 3D diffusion models have been\nintroduced for tri-planes (Shue et al., 2023; Wang et al., 2023b) or feature grids (Karnewar et al.,\n2023). Nevertheless, these models primarily cater to specific objects like faces and ShapeNet objects.\nTheir generalizability to the scope of their 2D counterparts remains unverified, possibly due to 3D\nrepresentation constraints or architectural design. It is pertinent to note ongoing research endeavors\nto reconstruct object shapes directly from monocular image inputs (Wu et al., 2023; Nichol et al.,\n2022; Jun & Nichol, 2023), aligning with the increasing stability of image generation techniques.\n2.2\nDIFFUSION MODELS FOR OBJECT NOVEL VIEW SYNTHESIS\nRecent research has also pursued the direct synthesis of novel 3D views without undergoing recon-\nstruction. For instance, Watson et al. (2023) first applied diffusion models to view synthesis using the\nShapeNet dataset (Sitzmann et al., 2019). Zhou & Tulsiani (2023) extended the idea to the latent space\nof stable diffusion models (Rombach et al., 2022) with an epipolar feature transformer. Chan et al.\n(2023) enhanced the view consistency by re-projecting the latent features during diffusion denoising.\nSzymanowicz et al. (2023) proposed a multi-view reconstructer using unprojected feature grids. A\nlimitation shared across these approaches is the boundedness to their respective training data, with no\nestablished evidence of generalizing to diverse image inputs. Liu et al. (2023) propose to fine-tune a\npre-trained image variation diffusion model (sdv) on an extensive 3D render dataset for novel view\nsynthesis. Notwithstanding, the synthesized images from such studies still grapple with geometric\nconsistency, leading to a discernible blurriness in the output 3D models. Recently, Tang et al. (2023b)\nproposed a multi-view diffusion model for panorama with homography-guided attention, which is\ndifferent from ours where explicit 3D correspondence is not available.\n2.3\nLIFTING 2D DIFFUSION FOR 3D GENERATION\nGiven the bounded generalizability of 3D generative models, another thread of studies have attempted\nto apply 2D diffusion priors to 3D generation by coupling it with a 3D representation, such as a\n2\n3D SelfAttention\nResBlock\nCrossAttention\nTimestep\nCamera\nRendered images\n3D model\nMulti-view Diffusion UNet\nText\nTraining Loss\nScore Distillation\nMulti-view Generation\n3D Generation\nFigure 2: Illustration of the multi-view diffusion model. We keep the structure of text-to-image\nUNets by making two slight changes: (1) changing the self-attention from 2D to 3D for cross-view\nconnection (2) adding camera embeddings for each view. Multi-view renderings are used to train the\ndiffusion model. During testing, the pipeline is used in a reverse way: the multi-view diffusion model\nserves as 3D prior to optimize the 3D representation via Score Distillation Sampling (SDS).\nNeRF (Mildenhall et al., 2021). The key technique of such methods is the score distillation sampling\n(SDS) proposed by Poole et al. (2023), where the diffusion priors are used as score functions to\nsupervise the optimization of a 3D representation. Concurrent with Dreamfusion, SJC (Wang et al.,\n2023a) proposed a similar technique using the publicly available stable-diffusion model (Rombach\net al., 2022). Following works have been further improving the 3D representations (Lin et al., 2023a;\nTsalicoglou et al., 2023; Tang et al., 2023a; Chen et al., 2023), sampling schedules (Huang et al.,\n2023), and loss design (Wang et al., 2023c). Although these methods can generate photo-realistic and\narbitrary types of objects without training on any 3D data, they are known to suffer from the multi-\nview consistency problem, as discussed in Section 1. In addition, as discussed in (Poole et al., 2023),\neach generated 3D model is individually optimized by tuning prompts and hyper-parameters to avoid\ngeneration failures. Nevertheless, in MVDream, we significantly improve the generation robustness,\nand are able to produce satisfactory results with a single set of parameters without individual tuning.\n3\nMETHODOLOGY\n3.1\nMULTI-VIEW DIFFUSION MODEL\nTo mitigate the multi-view consistency issue in 2D-lifting methods, a typical solution is to improve its\nviewpoint-awareness. For example, Poole et al. (2023) add viewpoint descriptions to text conditions.\nA more sophisticated method would be incorporating exact camera parameters like in novel view\nsynthesis methods (Liu et al., 2023). However, we hypothesize that even a perfect camera-conditioned\nmodel is not sufficient to solve the problem and the content in different views still could mismatch.\nFor example, an eagle might look to its front from front view while looking to its right from back\nview, where only its body is complying with the camera condition.\nOur next inspiration draws from video diffusion models. Since humans do not have a real 3D sensor,\nthe typical way to perceive a 3D object is to observe it from all possible perspectives, which is similar\nto watching a turnaround video. Recent works on video generation demonstrate the possibility of\nadapting image diffusion models to generate temporally consistent content (Ho et al., 2022; Singer\net al., 2022; Blattmann et al., 2023; Zhou et al., 2022). However, adapting such video models to our\nproblem is non-trivial as geometric consistency could be more delicate than temporal consistency.\nOur initial experiment shows that content drift could still happen between frames for video diffusion\nmodels when there is a large viewpoint change. Moreover, video diffusion models are usually trained\non dynamic scenes, which might suffer from a domain gap when applied to generating static scenes.\nWith such observations, we found it important to directly train a multi-view diffusion model, where we\ncould utilize 3D rendered datasets to generate static scenes with precise camera parameters. Fig. (2)\nshows an illustration of our text-to-multi-view diffusion model. We leverage the 3D datasets to render\nconsistent multi-view images to supervise the diffusion model training. Formally, given a set of noisy\nimage xt \u2208 RF \u00d7H\u00d7W \u00d7C, a text prompt as condition y, and a set of extrinsic camera parameters\nc \u2208 RF \u00d716, multi-view diffusion model is trained to generate a set of images x0 \u2208 RF \u00d7H\u00d7W \u00d7C\n3\nParameter\nPreservation Loss\nMulti-view\nDiffusion Loss\nImage\nDiffusion Loss\nMulti-view Diffusion Training\nMulti-view Dreambooth\nA [v] dog\nImage\nDiffusion Loss\nFrench cat\nRobot\nImage\nImage\nView 1\nView 2\nView 3\nView 4\ncamera\nFigure 3: The training pipeline of MVDream. Left: Training of multi-view diffusion with two\nmodes: image mode with 2D attention (upper) and multi-view mode with 3D attention and camera\nembeddings (lower). Right: Training of DreamBooth, where the pre-trained multi-view diffusion\nmodel is fine-tuned with the image mode of 2D attention and a preservation loss.\nof the same scene from F different view angles. After the training, the model can be used as a\nmulti-view prior for 3D generation with techniques such as Score Distillation Sampling (SDS).\nTo inherit the generalizability of the 2D diffusion models, we would like to keep their architecture as\nmuch as possible for fine-tuning. However, such models can only generate one image at a time and\ndo not take camera conditions as inputs. So the main questions here are: (1) how to generate a set of\nconsistent images from the same text prompt (Sec. 3.1.1), (2) how to add the camera pose control\n(Sec. 3.1.2), and (3) how to maintain the quality and generalizability (Sec. 3.1.3).\n3.1.1\nMULTI-VIEW CONSISTENT GENERATION WITH INFLATED 3D SELF-ATTENTION\nSimilar to video diffusion models (Ho et al., 2022; Singer et al., 2022; Blattmann et al., 2023; Zhou\net al., 2022), we would like to adapt the attention layers to model the cross-view dependency while\nkeeping the remaining network as a 2D model that only operates within a single image. However,\nwe found that a simple temporal attention fails to learn multi-view consistency and content drift still\nhappens even if we fine-tune the model on a 3D rendered dataset. Instead, we choose to use a 3D\nattention. Specifically, we can inflate the original 2D self-attention layer into 3D by connecting all\ndifferent views in self-attention (See Fig. 3), which we found able to generate rather consistent images\neven when the view gap is very large. Specifically, given a tensor of shape B \u00d7 F \u00d7 H \u00d7 W \u00d7 C,\nthe we format it as B \u00d7 FHW \u00d7 C for self-attention, where the second dimension is the sequence\ndimension representing the tokens. In such a way, we could also inherit all the module weights from\noriginal 2D self-attention. Note that we also experimented with incorporating a new 3D self-attention\nlayer rather than modifying the existing 2D one. However, we found that such a design compromised\nthe generation quality of multi-view images.\n3.1.2\nCAMERA EMBEDDINGS\nLike video diffusion models, position encoding is necessary for our model to distinguish between\ndifferent views. To this end, we compared relative position encoding (Singer et al., 2022), rotary\nembeddings (Su et al., 2021), and absolute camera parameters. We found that embedding camera\nparameters with a 2-layer MLP leads to the most satisfying image quality with distinguishable\nview differences. Specifically, we consider two methods to inject camera parameters: (1) adding\ncamera embeddings to time embeddings as residuals, and (2) appending camera embeddings to text\nembeddings for cross attention. Our experiment shows that both methods work but the former turns\nout to be more robust possibly because the camera embeddings would be less entangled with the text.\n3.1.3\nTRAINING LOSS FUNCTION\nWe found that the details in data curation and training strategy are important to the image generation\nquality as well. For the space limit, we refer the readers to the Appendix for the details on data\nprocessing. For the training, we fine-tune our model from the Stable Diffusion v2.1 base model\n(512\u00d7512 resolution), where we keep their settings for the optimizers and \u03f5-prediction but reduce\nimage size to 256\u00d7256. We found that a joint training with a larger scale text-to-image dataset is\n4\nhelpful for the generalizability of the fine-tuned model, as illustrated in the left Fig. (3). Formally,\ngiven text-image dataset X and a multi-view dataset Xmv, for training samples {x, y, c} \u2208 X \u222a Xmv\n(c is empty for X), the multi-view diffusion loss is defined as\nLMV (\u03b8, X, Xmv) = Ex,y,c,t,\u03f5\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(xt; y, c, t)\u22252\n2\ni\n(1)\nwhere xt is the noisy image generated from random noise \u03f5 and images x, y is the condition, c is the\ncamera condition and the \u03f5\u03b8 is the multi-view diffusion model. In practice, with a 30% chance we train\nthe multi-view model as a simple 2D text-to-image model on a subset of LAION dataset (Schuhmann\net al., 2022) by turning off the 3D attention and camera embeddings.\n3.2\nTEXT-TO-3D GENERATION\nWe recognize two ways to utilize a multi-view diffusion model for 3D generation:\n\u2022 Using the generated multi-view images as inputs for a few-shot 3D reconstruction method.\n\u2022 Using the multi-view diffusion model as a prior for Score Distillation Sampling (SDS).\nAlthough more straightforward, 3D reconstruction requires a robust few-shot 3D reconstruction\nmethod, which is not available at the moment during this project. Therefore, we focus our experiments\non the latter, where we modify existing SDS pipelines by replacing Stable Diffusion model with\nour multi-view diffusion model. This leads to two modifications: (1) changing the camera sampling\nstrategy and (2) feeding the camera parameters as inputs. Instead of using direction-annotated prompts\nas in Dreamfusion (Poole et al., 2023), we use original prompts for extracting the text embeddings.\nIn spite that such a multi-view SDS can generate consistent 3D models, the content richness and\ntexture quality still fall short of those images directly sampled by the denoising diffusion process.\nThus, we propose several techniques to alleviate the issue. First, we linearly anneal the maximum\nand minimum time step for SDS during optimization. Second, to prevent the model from generating\nstyles of low quality 3D models in the dataset, we add a few fixed negative prompts during SDS.\nFinally, to alleviate the color saturation from large classifier free guidance (CFG), we would like to\napply clamping techniques such as dynamic thresholding (Saharia et al., 2022) or CFG rescale from\n(Lin et al., 2023b). Since these tricks only apply to \u02c6x0, we propose to use an x0-reconstruction loss\ninstead of original SDS formulation:\nLSDS(\u03d5, x = g(\u03d5)) = Et,c,\u03f5\nh\n\u2225x \u2212 \u02c6x0\u22252\n2\ni\n.\n(2)\nIt can be shown that Eq. (2) is equivalent to the original SDS with w(t), a hyper-parameter in\nSDS, equal to signal-to-noise ratio (See Appendix). Here x = g(\u03d5) refers to rendered images from\n3D representation \u03d5 and \u02c6x0 is the estimated x0 from \u03f5\u03b8(xt; y, c, t), whose gradient is detached.\nEmpirically, we found that x0-reconstruction loss performs similarly to the original SDS but it\nmitigates the color saturation after we apply the CFG rescale trick (Lin et al., 2023b) on \u02c6x0.\nWe also use the point lighting (Poole et al., 2023) and soft shading (Lin et al., 2023a) to regularize the\ngeometry. For regularization loss, we only use the orientation loss proposed by (Poole et al., 2023).\nWe note that both of these two techniques only help to smooth the geometry and have little effect on\nthe content in our case. We do not use any sparsity loss to force the separation between foreground\nthe background but instead achieve it by replacing background with random colors. It is also worth\nnoting that MVDream is theoretically combinable with other SDS variants such as SJC (Wang et al.,\n2023a) and VSD(Wang et al., 2023c), but this is out of the scope of this paper.\n3.3\nMULTI-VIEW DREAMBOOTH FOR 3D GENERATION\nAs shown in the right of Fig. (3), after training the multi-view diffusion model, we consider extending\nit to a DreamBooth model (Ruiz et al., 2023) for 3D DreamBooth applications. Thanks to the\ngeneralization of the multi-view diffusion model, we found its multi-view ability can be maintained\nafter fine-tuning. Specifically, we consider two types of loss, an image fine-tuning loss and a parameter\npreservation loss. Formally, let Xid indicates the set of identity images, our loss for DreamBooth is\nLDB(\u03b8, Xid) = LLDM(Xid) + \u03bb\u2225\u03b8 \u2212 \u03b80|\u22251\nN\u03b8\n,\n(3)\n5\n(a) Temporal Attention\n(b) Additional 3D Self-Attention module\n(c) Inflated 3D Self-Attention\nFigure 4: Comparison between using different types of attention modules for learning multi-view\nconsistency as discussed in Sec 3.1.1. Same random seed used for testing.\nwhere LLDM is the image diffusion loss (Rombach et al., 2022), \u03b80 is the initial parameter of original\nmulti-view diffusion, N\u03b8 is the number of parameters, and \u03bb is a balancing parameter set to 1. See\nAppendix for more training details.\nTo obtain a 3D model, we follow the process in Sec. 3.2 by replacing the diffusion model with\nthe DreamBooth model. Note that original DreamBooth3D (Raj et al., 2023) used a three-stage\noptimization: partial DreamBooth, multi-view data generation, and multi-view DreamBooth. In\ncomparison, our method capitalizes on the consistency of diffusion model and streamlines the process\nby training a multi-view (MV) DreamBooth model directly followed by 3D NeRF optimization.\n4\nEXPERIMENTS\nTo evaluate the multi-view diffusion models, we fine-tune the open-sourced stable diffusion 2.1\nmodel (sta) on the Objaverse dataset (Deitke et al., 2023) and LAION dataset (Schuhmann et al.,\n2022) for experiments. We refer the readers to the Sec. A.6 for more training details. We evaluate our\nmethod on three tasks: (1) multi-view image generation for evaluating image quality and consistency\n(Sec. 4.1), (2) 3D (NeRF) generation with multi-view score distillation as a main downstream task\n(Sec. 4.2), and (3) DreamBooth for personalized 3D generation (Sec. 4.3).\n4.1\nMULTI-VIEW IMAGE GENERATION\nWe first compare the three choices of attention module for modeling cross-view consistency: (1) 1D\ntemporal self-attention that is widely used in video diffusion models, (2) a new 3D self-attention\nmodule that is added onto existing model, and (3) inflating existing 2D self-attention module for\n3D attention, as explained in 3.1.1. To show the difference between these modules, we trained the\nmodel with 8 frames across a 90 degree view change, which is closer to a video setting. We keep the\nimage resolution at 512\u00d7512 as the original SD model in this experiment. As shown in Fig. (4), we\nfound that even with such limited view angle change on static scenes, temporal self-attention still\nsuffers from content drift. We hypothesize that this is because temporal attention can only interchange\ninformation between the same pixel from different frames while the corresponding pixels could be\nfar away from each other in different views. Adding new 3D attention, on the other hand, leads to\nsevere quality degradation without learning consistency. We believe this is because learning new\nparameters from scratch takes more training data and time, which is unsuitable for our case. The\nproposed strategy of inflated 2D self-attention achieves the best consistency among all without losing\ngeneration quality. We note that the difference between these modules would be smaller if we reduce\nthe image size to 256 and the number of views to 4. However, to achieve the best consistency, we\nkeep our choice for the following experiments based on our initial observations.\n6\nModel\nFID\u2193\nIS\u2191\nCLIP\u2191\nValidation set\nN/A\n12.90 \u00b1 0.66\n30.12 \u00b1 3.15\nMulti-view Diffusion (3D data only)\n40.38\n12.33 \u00b1 0.63\n29.69 \u00b1 3.36\nMulti-view Diffusion (3D + LAION 2D data)\n39.04\n12.97 \u00b1 0.60\n30.38 \u00b1 3.50\nTable 1: Quantitative evaluation on image synthesis quality. DDIM sampler is used for testing.\nZombie bust, terror, 123dsculpt, bust, zombie\nBattletech Zeus with a sword!, tabletop, miniature, bat-\ntletech, miniatures, wargames, 3d asset\nMedieval House, grass, medieval, vines, farm, middle-\nage, medieval-house, stone, house, home, wood,\nmedieval-decor, 3d asset\nIsometric Slowpoke Themed Bedroom, fanart, poke-\nmon, bedroom, assignment, isometric, pokemon3d,\nisometric-room, room-low-poly, 3d asset\nAn astronaut riding a horse\nA bald eagle carved out of wood\nA bull dog wearing a black pirate hat\na DSLR photo of a ghost eating a hamburger\nFigure 5: Example images generated by our model using training and testing prompts. The first two\nrows show the synthesized images from training prompts. The bottom four examples are images\ngenerated using unseen prompts. See Appendix for more results.\nIn Table 1, we conduct a quantitative comparison over generation quality and text-image consistency.\nWe randomly choose 1,000 subjects from the held-out validation set and generate 4-view images\nusing the given prompts and camera parameters. Frechet Inception Distance (FID) (Heusel et al.,\n2017) and Inception Score (IS) (Salimans et al., 2016) are calculated to measure the image quality\nwhile the CLIP score (Radford et al., 2021) is used to measure the text-image consistency, which\nis averaged across all views and subjects. Overall, our models can achieve a similar IS and CLIP\nscore as the training set, indicating good image quality and text-image consistency. We found that\nfine-tuning with 3D data alone could lead to deteriorated image quality. On the other side, combining\n3D data with a 2D dataset (LAION) for joint training mitigates the problem. Please also refer to\nSec. A.4 and Fig. (12) for more qualitative examples for comparison.\nIn Fig. (5), we show a few qualitative examples from our model. It can be seen that our multi-view\nmodel can generate images from unseen prompts that are possibly counterfactual and in a different\nstyle from training prompts. Like training, we append the text with \u201c, 3d asset\u201d for generation.\n4.2\n3D GENERATION WITH MULTI-VIEW SCORE DISTILLATION\nWe compare our multi-view SDS with existing text-to-3D methods. We implement our method in\nthreestudio (thr) and use reproduced baselines in this framework for comparison. We consider the\nfollowing baselines: dreamfusion-IF (Poole et al., 2023), magic3d-IF (Lin et al., 2023a), text2mesh-\nIF (Tsalicoglou et al., 2023), and prolificdreamer (Wang et al., 2023c). Note that the implementation in\n7\n0\u25e6\n45\u25e6\n90\u25e6\n135\u25e6\n0\u25e6\n45\u25e6\n90\u25e6\n135\u25e6\nDreamFusion-IF\nMagic3D-IF\nTextMesh\nProlificDreamer\nOurs\nOurs(Normal)\nA bulldog wearing a black pirate hat\nbeautiful, intricate butterfly\nFigure 6: Comparison of text-to-3D (NeRF) generation. See Appendix for more results.\noriginal SDS\n+ timestep annealing\n+ neg prompts\n+ rescale\nFigure 7: Effects of different techniques that we apply in multi-view SDS. The input text is \"a DSLR\nphoto of a frog wearing a sweater\".\nthreestudio could be different from the original papers. For example, dreamfusion-IF, magic3d-IF and\ntext2mesh-IF use DeepFloyd (dee) as their guidance model while original ones use Imagen (Saharia\net al., 2022) and eDiff-I (Balaji et al., 2022), which are private. Original dreamfusion uses Mip-\nNeRF 360 (Barron et al., 2022) for 3D representation, but all methods here (including ours) use\nmulti-resolution hash-grid (M\u00fcller et al., 2022) (except Text2Mesh that uses SDF). We believe\nthat these baselines represent the best re-implementation we could find. To test the performance\ncomprehensively, we collected 40 prompts from various sources, including prior studies, internet\nprompts for image generation, and user inputs from a 3D generation website (lum). As shown in\nFig. (6), all baselines suffer from severe multi-view inconsistency. Magic3D performs relatively better\nby using DMTet Shen et al. (2021) for second stage refinement. Prolificdreamer shows good texture\nquality for every view, but jointly they do not appear to be a reasonable 3D object. In comparison, the\nproposed method generates high quality 3D assets in a much more stable way. In terms of running\ntime, dreamfusion-IF, text2mesh-IF and MVDream takes about 2 hours on a single V100 GPU.\nMagic3D-IF-SD takes about 3.5 hours and prolificdreamer takes more than 10 hours.\nFig. (7) shows an ablation study of the proposed techniques for score distillation introduced in\nSec. 3.2. Adding time step annealing helps to make the shape more complete. Adding negative\n8\nPhoto of a [v] dog\nDreamBooth3D\nOurs\nFigure 9: Illustration of MVDreamBooth Results. From the inputs, we show the generated multi-view\nimages given a description prompt at bottom. On the right, we show comparison of NeRF rendered\nresults from ours with and DreamBooth3D (Raj et al., 2023). Notice ours perform better on the object\ndetails such as furry skin. See Appendix for more results.\nprompts significantly improves the visual style. Adding the CFG rescale further makes the texture\ncolor more natural.\nproposed\n78%\ndreamfusion-if\n2%\nmagic3d-if-sd\n11%\ntextmesh-if\n1%\nprolificdreamer\n8%\nFigure 8: User study.\nTo further validate the stability and quality of our model,\nwe conducted a user study on the generated models from 40\nprompts. Each user is given 5 rendered videos and their corre-\nsponding text input and is asked to select a preferred 3D model\namong all. 914 feedbacks from 38 users were collected and the\nresult is shown in Fig. (8). On average, 78% users prefer our\nmodel over others. That is, our model is preferred over the best\nof all baselines in most cases. We believe this is a strong proof\nof the robustness and quality of the proposed method. Please\nsee the supplementary materials for more visual results.\n4.3\nMULTI-VIEW DREAMBOOTH.\nIn Fig. (9), we compare 3D models generated from MVDream and DreamBooth3D (Raj et al., 2023).\nWe found that our results have higher quality with better object details such as the curly hair and\nfur texture on the dog. This is because during the NeRF training process with SDS loss, our MV\nDreamBooth diffusion models produce higher geometry consistency. We provide additional results in\nour project page and supplementary materials.\n5\nDISCUSSION AND CONCLUSION\nConclusion\nIn this paper, we present the first multi-view diffusion model that is able to generate a\nset of multi-view images of an object from any given text. By fine-tuning a pre-trained text-to-image\ndiffusion model on a mixture of 3D rendered data and large scale text-to-image data, our model is able\nto maintain the generalizability of the base model while achieving multi-view consistent generation.\nWe show that the multi-view diffusion model can serve as a good 3D prior and can be applied to\n3D generation via SDS, which leads to better stability and quality than current open-sourced 2D\nlifting methods. Finally, the multi-view diffusion model can also be trained in a few-shot setting for\npersonalized 3D generation (multi-view DreamBooth).\nLimitation\nWe observe the following limitations of our current multi-view diffusion model. For\nthe moment, the model can only generate images at a resolution of 256\u00d7256, which is smaller than\nthe 512\u00d7512 of the original stable diffusion. Also, the generalizability of our current model seems to\nbe limited by the base model itself. For both aforementioned problems, we expect them to be solved\nby increasing the dataset size and replacing the base model with a larger diffusion model, such as\nSDXL (SDX). Furthermore, we do observe that the generated styles (lighting, texture) of our model\nare affected by the rendered dataset. Although it can be alleviated by adding more style text prompts,\nit also indicates that a more diverse and realistic rendering is necessary to learn a better multi-view\ndiffusion model, which could be costly.\n9\n6\nETHICS STATEMENT\nThe multi-view diffusion model proposed in this paper aims to facilitate the 3D generation task that\nis widely demanded in gaming and media industry. We do note that it could be potentially applied\nto unexpected scenarios such as generating violent and sexual content by third-party fine-tuning.\nBuilt upon the Stable Diffusion model (Rombach et al., 2022), it might also inherit the biases and\nlimitations to generate unwanted results. Therefore, we believe that the images or models synthesized\nusing our approach should be carefully examined and be presented as synthetic. Such generative\nmodels may also have the potential to displace creative workers via automation. That being said,\nthese tools may also enable growth and improve accessibility for the creative industry.\n7\nREPRODUCIBILITY STATEMENT\nOur MVDream model is built from publicly available models and datasets, such as Stable Diffu-\nsion (Rombach et al., 2022), LAION (Schuhmann et al., 2022) and Objaverse (Deitke et al., 2023).\nWith all the implementation details provided, we believe it should be straightforward to re-produce\nthe algorithm. Besides, we will release our code as well as model checkpoints publicly after the paper\nsubmission.\nREFERENCES\nstable-diffusion-xl-base-1.0.\nhttps://huggingface.co/stabilityai/\nstable-diffusion-xl-base-1.0. Accessed: 2023-08-29. 9\nSketchfab. https://sketchfab.com/3d-models/popular. Accessed: 2023-08-30. 1\nDeepfloyd. https://huggingface.co/DeepFloyd. Accessed: 2023-08-25. 8\nLuma.ai. https://lumalabs.ai/dashboard/imagine. Accessed: 2023-08-25. 8\nStable diffusion image variation.\nhttps://huggingface.co/spaces/lambdalabs/\nstable-diffusion-image-variations. 2\nStable\ndiffusion\n2.1\nbase.\nhttps://huggingface.co/stabilityai/\nstable-diffusion-2-1-base. Accessed: 2023-07-14. 6, 17\nThreestudio project.\nhttps://github.com/threestudio-project/threestudio.\nAccessed: 2023-08-25. 7, 18\nYogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala,\nTimo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an\nensemble of expert denoisers. arXiv:2211.01324, 2022. 8\nJonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-nerf\n360: Unbounded anti-aliased neural radiance fields. CVPR, 2022. 8\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and\nKarsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In\nCVPR, 2023. 3, 4\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d\ngenerative adversarial networks. In CVPR, 2022. 2\nEric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W. Bergman, Jeong Joon Park, Axel Levy,\nMiika Aittala, Shalini De Mello, Tero Karras, and Gordon Wetzstein. GeNVS: Generative novel\nview synthesis with 3D-aware diffusion models. In arXiv, 2023. 2\nRui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. arXiv:2303.13873, 2023. 3\n10\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of annotated\n3d objects. In CVPR, pp. 13142\u201313153, 2023. 6, 10, 17\nYu Deng, Jiaolong Yang, Jianfeng Xiang, and Xin Tong. Gram: Generative radiance manifolds for\n3d-aware image generation. In CVPR, pp. 10673\u201310683, 2022. 2\nJun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan\nGojcic, and Sanja Fidler. Get3d: A generative model of high quality 3d textured shapes learned\nfrom images. NeurIPS, 2022. 2\nPaul Henderson and Vittorio Ferrari. Learning single-image 3d reconstruction by generative modelling\nof shape, pose and shading. International Journal of Computer Vision, 2020. 2\nPaul Henderson, Vagia Tsiminaki, and Christoph H Lampert. Leveraging 2d data to learn textured 3d\nmesh generation. In CVPR, 2020. 2\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans\ntrained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 2017. 7\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P\nKingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition\nvideo generation with diffusion models. arXiv:2210.02303, 2022. 3, 4\nYukun Huang, Jianan Wang, Yukai Shi, Xianbiao Qi, Zheng-Jun Zha, and Lei Zhang. Dreamtime:\nAn improved optimization strategy for text-to-3d content creation. arXiv:2306.12422, 2023. 3\nHeewoo Jun and Alex Nichol.\nShap-e:\nGenerating conditional 3d implicit functions.\narXiv:2305.02463, 2023. 2\nAnimesh Karnewar, Andrea Vedaldi, David Novotny, and Niloy J Mitra. Holodiffusion: Training a\n3d diffusion model using 2d images. In CVPR, 2023. 2\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014. 18\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 2\nChen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten\nKreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content\ncreation. In CVPR, 2023a. 1, 3, 5, 7, 18\nShanchuan Lin, Bingchen Liu, Jiashi Li, and Xiao Yang. Common diffusion noise schedules and\nsample steps are flawed. arXiv:2305.08891, 2023b. 5, 17\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. arXiv:2303.11328, 2023. 2, 3, 14\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2021. 3\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics\nprimitives with a multiresolution hash encoding. ACM Trans. Graph., 2022. 8\nThu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. Hologan:\nUnsupervised learning of 3d representations from natural images. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2019. 2\nThu H Nguyen-Phuoc, Christian Richardt, Long Mai, Yongliang Yang, and Niloy Mitra. Blockgan:\nLearning 3d object-aware scene representations from unlabelled images. NeurIPS, 2020. 2\nAlex Nichol, Heewoo Jun, Prafulla Dhariwal, Pamela Mishkin, and Mark Chen. Point-e: A system\nfor generating 3d point clouds from complex prompts. arXiv:2212.08751, 2022. 2\nMichael Niemeyer and Andreas Geiger. Giraffe: Representing scenes as compositional generative\nneural feature fields. In CVPR, 2021. 2\n11\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv:2307.01952, 2023. 14\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. In ICLR, 2023. 1, 3, 5, 7\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021. 7\nAmit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer, Nataniel Ruiz, Ben Mildenhall, Shiran\nZada, Kfir Aberman, Michael Rubinstein, Jonathan Barron, et al. Dreambooth3d: Subject-driven\ntext-to-3d generation. arXiv:2303.13508, 2023. 6, 9, 18\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3, 6, 10\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR,\n2023. 2, 5\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. NeurIPS, 2022. 5, 8, 17\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. NeurIPS, 2016. 7\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi\nCherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An\nopen large-scale dataset for training next generation image-text models. NeurIPS, 2022. 5, 6, 10,\n17\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra:\na hybrid representation for high-resolution 3d shape synthesis. In NeurIPS, 2021. 8\nJ Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner, Jiajun Wu, and Gordon Wetzstein. 3d\nneural field generation using triplane diffusion. In CVPR, 2023. 2\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\nYang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video\ndata. arXiv:2209.14792, 2022. 3, 4\nVincent Sitzmann, Michael Zollh\u00f6fer, and Gordon Wetzstein. Scene representation networks: Contin-\nuous 3d-structure-aware neural scene representations. NeurIPS, 32, 2019. 2\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced\ntransformer with rotary position embedding. arXiv:2104.09864, 2021. 4\nStanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset diffusion:(0-) image-\nconditioned 3d generative models from 2d data, 2023. 2\nJunshu Tang, Tengfei Wang, Bo Zhang, Ting Zhang, Ran Yi, Lizhuang Ma, and Dong Chen. Make-it-\n3d: High-fidelity 3d creation from a single image with diffusion prior. arXiv:2303.14184, 2023a.\n3\nShitao Tang, Fuayng Zhang, Jiacheng Chen, Peng Wang, and Furukawa Yasutaka. Mvdiffusion:\nEnabling holistic multi-view image generation with correspondence-aware diffusion. arXiv:\n2307.01097, 2023b. 2\nChristina Tsalicoglou, Fabian Manhardt, Alessio Tonioni, Michael Niemeyer, and Federico Tombari.\nTextmesh: Generation of realistic 3d meshes from text prompts. arXiv:2304.12439, 2023. 3, 7\n12\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian\nchaining: Lifting pretrained 2d diffusion models for 3d generation. In CVPR, 2023a. 3, 5\nTengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen,\nDong Chen, Fang Wen, Qifeng Chen, et al. Rodin: A generative model for sculpting 3d digital\navatars using diffusion. In CVPR, 2023b. 2\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Pro-\nlificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation.\narXiv:2305.16213, 2023c. 3, 5, 7\nDaniel Watson, William Chan, Ricardo Martin-Brualla, Jonathan Ho, Andrea Tagliasacchi, and\nMohammad Norouzi. Novel view synthesis with diffusion models. In ICLR, 2023. 2\nChao-Yuan Wu, Justin Johnson, Jitendra Malik, Christoph Feichtenhofer, and Georgia Gkioxari.\nMultiview compressive coding for 3d reconstruction. In CVPR, 2023. 2\nDaquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo:\nEfficient video generation with latent diffusion models. arXiv:2211.11018, 2022. 3, 4\nZhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling view-conditioned diffusion for 3d\nreconstruction. In CVPR, 2023. 2\n13\nA\nAPPENDIX\nA.1\nCOMPARISON WITH IMAGE-TO-3D METHOD\nIn Fig. (10), we compare our results with an state-of-the-art image-to-3D method, namely Zero123-\nXL (Liu et al., 2023). In particular, we first generate an image using the same prompt with\nSDXL (Podell et al., 2023), segment the object to remove background, and then provide the image as\ninput to the image-to-3D system using Zero123-XL in threestudio. We notice two drawbacks of such\na pipeline compared to our text-to-multi-view diffusion model:\n1. Zero123-XL is trained on 3D datasets only, whose objects are mostly aligned with simple\nposes (e.g. T-pose for characters). So the image-to-3D results tend to be distorted if the input\nimage has complicated poses and view angles that are different from 3D dataset. Specifically,\nwhen the object is geometrically spanned in the direction of the depth, the generated 3D\ntends to be too flat. This can be seen from the examples of eagle.\n2. Zero123-XL can hardly generate geometrically consistent and richly textured novel-view\nimages, and therefore the score distilled objects tend to have less details, yielding a smooth,\nblurred and relatively uniform appearance on the other sides different from the input view,\nas shown with the example of bulldog.\nA.2\nEFFECT OF NUMBER OF VIEWS\nIn Fig. (11), we compare the SDS with diffusion models trained with different number of views. All\nthe models are trained with the same settings except the number of views. Camera embeddings is\nused for all models. We found that the 1-view model, in spite of its camera-awareness, still suffer\nZero123-XL\nOurs\na bald eagle carved out of wood\nZero123-XL\nOurs\na bulldog wearing a black pirate hat\nZero123-XL\nOurs\nbeautiful, intricate butterfly\nFigure 10: Comparison with text-to-image-to-3D using SDXL + Zero123XL.\n14\n1 view\n2 view\n4 view (proposed)\nan astronaut riding a horse\nbaby yoda in the style of Mormookiee\nFigure 11: Comparison between SDS with diffusion models trained with different number of views.\nAll models have camera embeddings as inputs.\n3D data only\n3D + 2D (LAION) data\na bulldog wearing a black pirate hat\nan astronaut riding a horse\nFigure 12: Qualitative comparison between different multi-view diffusion models in Table 1. The\nsame random seed is used for each row for comparison.\n15\nfrom a severe Janus problem, partially due to imperfect camera annotations. The 2-view model\ngreatly reduces the multi-face problem, but it still could happen to some extent. Finally, the 4-view\nmodel barely suffers from any multi-view consistency issues.\nA.3\nTRAINING WITH 2D DATA\nFig. (12) shows the qualitative comparison between training with and without 2D data for multi-view\ndiffusion model. Empirically, we found that adding the 2D data has a clear improvement for the\nmodel generalizability, leading to better image quality and text-image correspondence.\nA.4\nTRAINING WITH RANDOM CAMERA\nIn the main paper, our model is trained for 4 orthogonal views with the same elevation angle. Thus, a\nquestion arises: can the method generalize to scenarios involving random camera views rather than\northogonal ones? We conduct such an experiment and found that the proposed 3D self-attention\nnetwork can indeed generate random multi-view images if trained on such data. In spite of worse\nquality compared to the orthogonal model, an interesting finding is that such a random-view model\nable to generate a much larger number of views during inference, as shown in Fig. (13). In our\nexperiments, we found it able to generate 64 views and potentially more, even though it is trained on\n4 views.\nFigure 13: Multi-view diffusion model trained on random views is able to generate more a larger\nnumber of views than training (40 views for example here).\nA.5\nx0 RECONSTRUCTION LOSS FOR SDS\nThe original SDS loss does not have an explicit form, instead it is defined by its gradient:\n\u2207\u03d5LSDS(\u03b8, x = g(\u03d5)) = Et,c,\u03f5\n\u0002\nw(t)(\u03f5\u03b8(xt; y, c, t) \u2212 \u03f5)\u2202x\n\u2202\u03d5\n\u0003\n(4)\nwhere c is the additional camera condition for our specific model, xt = \u03b1tx + \u03c3t\u03f5 is the sampled\nnoisy image, and w(t) is a hyper-parameter that controls the loss weight at different time steps. \u03b1t\nand \u03c3t are the signal and noise scale controlled by the noise schedule, where \u03b12\nt + \u03c32\nt = 1. Let\nus denote \u03f5\u03b8(xt; y, c, t) as \u03f5\u03b8 for simplicity. Given estimated \u03f5\u03b8 predicted by the model, we can\nreversely estimate \u02c6x0 by:\n\u02c6x0 = xt \u2212 \u03c3t\u03f5\u03b8\n\u03b1t\n= \u03b1tx + \u03c3t\u03f5 \u2212 \u03c3t\u03f5\u03b8\n\u03b1t\n= x + \u03c3t\n\u03b1t\n(\u03f5 \u2212 \u03f5\u03b8)\n(5)\n16\nAlgorithm 1: Pseudocode for MVDream training\nData: X, Xmv\nfor i \u2190 1 to n \u2212 1 do\nsample mode \u223c U(0, 1);\nif mode \u2264 0.7 then\nselect a random 3D sample from Xmv;\nx \u2190 4 random orthogonal views out of 32 views;\nc \u2190 camera extrinsics;\nelse\nx \u2190 4 random images from X;\nc \u2190 \u2205 ;\ny \u2190 text descriptions;\nsample t \u223c U(0, 1000);\nxt \u2190 add_noise(x, t);\nforward_and_backward(\u03b8, x, xt, y, c, t)\nSo for the reconstruction loss LSDS = Et,c,\u03f5\n\u0002\n\u2225\u02c6x0 \u2212 x\u22252\n2\n\u0003\n, if we ignore \u2202\u02c6x0\n\u2202\u03d5 like in SDS, its gradient\nis given by:\n\u2207\u03d5LSDS(\u03b8, x = g(\u03d5)) = Et,c,\u03f5\nh\n2(x \u2212 \u02c6x0)\u2202x\n\u2202\u03d5\ni\n= Et,c,\u03f5\nh2\u03c3t\n\u03b1t\n(\u03f5\u03b8 \u2212 \u03f5)\u2202x\n\u2202\u03d5\ni\n,\n(6)\nwhich is equivalent to the original SDS with w(t) = 2\u03c3t\n\u03b1t . Thus, we can further adjust the \u02c6x0 here\nwith other tricks such as dynamic threshold (Saharia et al., 2022) or CFG rescale (Lin et al., 2023b).\nA.6\nIMPLEMENTATION DETAILS\nA.6.1\nDATA PREPARATION AND DIFFUSION MODEL TRAINING\nWe use the public Objaverse dataset (Deitke et al., 2023) as our 3D rendered dataset, which was\nthe largest 3D dataset available by the time of this project. We simply use the names and tags of\nthe objects as their text descriptions. Since this dataset is rather noisy, we filter the dataset with a\nCLIP score to remove the objects whose rendered images are not relevant to its name, which leads\nto about 350K objects at the end. For each object, we first normalize the object to be at the center\nwithin a bounding box between [\u22120.5, 0.5], and then choose a random camera fov between [15, 60]\nand elevation between [0, 30]. The camera distance is chosen as the object size (0.5) times NDC\nfocal length with a random scaling factor between [0.9, 1.1]. A random HDRI from blender is used\nfor lighting. 32 uniformly distributed azimuth angles are used for rendering, starting from 0 degree.\nTo increase the number of training examples, we render each object twice with different random\nsettings. The rendered images are saved in RGBA format where the background is filled with a\nrandom grayscale color during training.\nDuring the training, we sample a data batch with a 70% chance from the 3D dataset and 30% chance\nfrom a subset of LAION dataset (Schuhmann et al., 2022). For the 3D dataset, 4 random views which\nare orthogonal to each other are chosen from the 32 views for training. The 4 \u00d7 4 camera parameters\nare normalized onto the sphere for inputs, i.e. it only represents the rotation. We fine-tune our model\nfrom the Stable Diffusion v2.1 base model (512\u00d7512 resolution) (sta), where we keep their settings\nfor the optimizer and \u03f5-prediction. We use a reduced image size of 256\u00d7256 and a total batch size of\n1,024 (4,096 images) for training and fine-tune the model for 50,000 steps. The training takes about\n3 days on 32 Nvidia Tesla A100 GPUs. To compensate for the visual style difference between the\nObjaverse and LAION dataset, we append the text \u201c, 3d asset\u201d to the 3D data if the keyword \u201c3d\u201d\nis not in the prompt. For DreamBooth fine-tuning, we train the model around for 600 steps with a\nlearning-rate as 2e \u2212 6, weight decay as 0.01 and a batch size of 4. A pseudo-code for training is\nprovided in Algorithm. 1.\n17\nA.6.2\nSDS OPTIMIZATION\nFor multi-view SDS, we implement our multi-view diffusion guidance in the threestudio (thr) library,\nwhich has implemented most state-of-the-art methods for text-to-3D generation under a unified\nframework. We use the implicit-volume implementation in threestudio as our 3D representation,\nwhich includes a multi-resolution hash-grid and a MLP to predict density and RGB. For camera\nviews, we sample the camera in the same way as how we render the 3D dataset. See Sec. A.6.1 for\nmore details. The 3D model is optimized for 10,000 steps with an AdamW optimizer (Kingma & Ba,\n2014) at a learning rate of 0.01. For SDS, the maximum and minimum time steps are decreased from\n0.98 to 0.5 and 0.02, respectively, over the first 8,000 steps. We use a rescale factor of 0.5 for the\nCFG rescale trick. The rendering resolution starts at 64\u00d764 and is increased to 256\u00d7256 after the\n5,000 steps. We also turn on soft shading as in (Lin et al., 2023a) after 5,000 steps. The background\nis replaced with 50% chance. The SDS process takes about 1.5 hour on a Tesla V100 GPU with\nshading and 1 hour without shading.\nA.7\nADDITIONAL RESULTS\nIn Fig. (14), we show additonal image generation results from our model using the prompts from the\ntraining dataset. In Fig. (15) and (16), we show additional comparison results of 3D generation using\nour method and baseline. In Fir. (17) we show additional results of DreamBooth using our method\nand DreamBooth3D (Raj et al., 2023).\n18\nZombie bust, terror, 123dsculpt, bust, zombie\nBattletech Zeus with a sword!, tabletop, miniature, bat-\ntletech, miniatures, wargames\nMedieval House, grass, medieval, vines, farm, middle-\nage, medieval-house, stone, house, home, wood,\nmedieval-decor\nIsometric Slowpoke Themed Bedroom, fanart, poke-\nmon, bedroom, assignment, isometric, pokemon3d,\nisometric-room, room-low-poly\nOld Boot (Left), cloth, boot, old, substancepainter, sub-\nstance, character\nOld Treasure Chest, chest, vintage, treasure, old, sub-\nstancepainter, substance\nUCSF\u2019s Bear Mascot Sculpture, bear, library, sanfran-\ncisco, photogramm, ucsf, kalmanovitz, bufano\nDSLR Camera, photography, dslr, camera, noobie, box-\nmodeling, maya\n\"Legolas\" Custom Outfit, custom, free-model, fortnite- Digital Wristwatch, wristwatch, timepiece, digital,\nwatch\nFigure 14: Example images generated by our multiview diffusion model using prompts in training\nset. In each example, the top and bottom row are the real and synthesized images, respectively.\n19\nDreamFusion-IF\nMagic3D-IF-SD\nTextMesh\nProlificDreamer\nOurs\nOurs(Normal)\nA bulldog wearing a black pirate hat\nDreamFusion-IF\nMagic3D-IF-SD\nTextMesh\nProlificDreamer\nOurs\nOurs(Normal)\nViking axe, fantasy, weapon, blender, 8k, HD\nFigure 15: Comparison of 3D generation between baselines and our method.\n20\nDreamFusion-IF\nMagic3D-IF-SD\nTextMesh\nProlificDreamer\nOurs\nOurs(Normal)\nbeautiful, intricate butterfly\nDreamFusion-IF\nMagic3D-IF-SD\nTextMesh\nProlificDreamer\nOurs\nOurs(Normal)\nmecha vampire girl chibi\nFigure 16: Comparison of 3D generation between baselines and our method.\n21\nPhoto of a dog\n\u2026 wearing an umbrella\n\u2026 sitting on a rainbow\nDreamBooth3D\nOurs\nDreamBooth3D\nOurs\nDreamBooth3D\nOurs\nFigure 17: MVDream DreamBooth Results.\n22\n"
  },
  {
    "title": "Emergence of Segmentation with Minimalistic White-Box Transformers",
    "link": "https://arxiv.org/pdf/2308.16271.pdf",
    "upvote": "13",
    "text": "Emergence of Segmentation with Minimalistic\nWhite-Box Transformers\nYaodong Yu1,\u22c6 Tianzhe Chu1,2,\u22c6 Shengbang Tong1,3 Ziyang Wu1 Druv Pai1\nSam Buchanan4 Yi Ma1,5\n1UC Berkeley\n2ShanghaiTech\n3NYU\n4TTIC\n5HKU\nhttps://ma-lab-berkeley.github.io/CRATE\nFigure 1: Self-attention maps from a supervised crate with 8 \u00d7 8 patches trained using classification. The\ncrate architecture automatically learns to perform object segmentation without a complex self-supervised\ntraining recipe or any fine-tuning with segmentation-related annotations. For each image pair, we visualize the\noriginal image on the left and the self-attention map of the image on the right.\nAbstract\nTransformer-like models for vision tasks have recently proven effective for a wide\nrange of downstream applications such as segmentation and detection. Previous\nworks have shown that segmentation properties emerge in vision transformers\n(ViTs) trained using self-supervised methods such as DINO, but not in those trained\non supervised classification tasks. In this study, we probe whether segmentation\nemerges in transformer-based models solely as a result of intricate self-supervised\nlearning mechanisms, or if the same emergence can be achieved under much broader\nconditions through proper design of the model architecture. Through extensive\nexperimental results, we demonstrate that when employing a white-box transformer-\nlike architecture known as crate, whose design explicitly models and pursues\nlow-dimensional structures in the data distribution, segmentation properties, at\nboth the whole and parts levels, already emerge with a minimalistic supervised\ntraining recipe. Layer-wise finer-grained analysis reveals that the emergent prop-\nerties strongly corroborate the designed mathematical functions of the white-box\nnetwork. Our results suggest a path to design white-box foundation models that are\nsimultaneously highly performant and mathematically fully interpretable. Code is\nat https://github.com/Ma-Lab-Berkeley/CRATE.\n1. Introduction\nRepresentation learning in an intelligent system aims to transform high-dimensional, multi-modal\nsensory data of the world\u2014images, language, speech\u2014into a compact form that preserves its es-\nsential low-dimensional structure, enabling efficient recognition (say, classification), grouping (say,\nsegmentation), and tracking [26, 31]. Classical representation learning frameworks, hand-designed\nfor distinct data modalities and tasks using mathematical models for data [12, 38, 39, 48, 49], have\nlargely been replaced by deep learning-based approaches, which train black-box deep networks with\nmassive amounts of heterogeneous data on simple tasks, then adapt the learned representations on\n\u22c6Equal contribution.\narXiv:2308.16271v1  [cs.CV]  30 Aug 2023\nFigure 2: (Left) Visualizing the self-attention map for an input image using the crate model. The input\ntokens for crate consist of N non-overlapping image patches and a [CLS] token. We use the crate model\nto transform these tokens to their representations, and de-rasterize the self-attention map associated to the\n[CLS] token and the image patch tokens at the penultimate layer to generate a heatmap visualization. Details\nare provided in Section 3.1. (Right) Overview of one layer of crate architecture. The crate model is a\nwhite-box transformer-like architecture derived via unrolled optimization on the sparse rate reduction objective\n(Section 2). Each layer compresses the distribution of the input tokens against a local signal model, and sparsifies\nit in a learnable dictionary. This makes the model mathematically interpretable and highly performant [51].\ndownstream tasks [3, 4, 35]. This data-driven approach has led to tremendous empirical successes\u2014\nin particular, foundation models [3] have demonstrated state-of-the-art results in fundamental vision\ntasks such as segmentation [22] and tracking [45]. Among vision foundation models, DINO [6,\n35] showcases a surprising emergent properties phenomenon in self-supervised vision transformers\n(ViTs [11])\u2014ViTs contain explicit semantic segmentation information even without trained with\nsegmentation supervision. Follow-up works have investigated how to leverage such segmenta-\ntion information in DINO models and achieved state-of-the-art performance on downstream tasks,\nincluding segmentation, co-segmentation, and detection [2, 46].\nAs demonstrated in Caron et al. [6], the penultimate-layer features in ViTs trained with DINO corre-\nlate strongly with saliency information in the visual input\u2014for example, foreground-background\ndistinctions and object boundaries (similar to the visualizations shown in Figure 1)\u2014which allows\nthese features to be used for image segmentation and other tasks. However, to bring about the\nemergence of these segmentation properties, DINO requires a delicate blend of self-supervised\nlearning, knowledge distillation, and weight averaging during training. It remains unclear if every\ncomponent introduced in DINO is essential for the emergence of segmentation masks. In particular,\nthere is no such segmentation behavior observed in the vanilla supervised ViT models that are\ntrained on classification tasks [6], although DINO employs the same ViT architecture as its backbone.\nIn this paper, we question the prevailing wisdom, stemming from the successes of DINO, that a\ncomplex self-supervised learning pipeline is necessary to obtain emergent properties in transformer-\nlike vision models. We contend that an equally-promising approach to promote segmentation\nproperties in transformer is to design the transformer architecture with the structure of the input data in\nmind, representing a marrying of the classical approach to representation learning with the modern,\ndata-driven deep learning framework. We call such an approach to transformer architecture design\nwhite-box transformer, in contrast to the black-box transformer architectures (e.g., ViTs [11]) that\ncurrently prevail as the backbones of vision foundation models. We experiment with the white-box\ntransformer architecture crate proposed by Yu et al. [51], an alternative to ViTs in which each layer\nis mathematically interpretable, and demonstrate through extensive experiments that:\nThe white-box design of crate leads to the emergence of segmentation properties in the network\u2019s\nself-attention maps, solely through a minimalistic supervised training recipe\u2014the supervised\nclassification training used in vanilla supervised ViTs [11].\nWe visualize the self-attention maps of crate trained with this recipe in Figure 1, which share\nsimilar qualitative (object segmentation) behaviors to the ones shown in DINO [6]. Furthermore,\nas to be shown in Figure 7, each attention head in the learned white-box crate seems to capture a\ndifferent semantic part of the objects of interest. This represents the first supervised vision model with\nemergent segmentation properties, and establishes white-box transformers as a promising direction for\ninterpretable data-driven representation learning in foundation models.\nOutline. The remainder of the paper is organized as follows. In Section 2, we review the design of\ncrate, the white-box transformer model we study in our experiments. In Section 3, we outline our\n2\nexperimental methodologies to study segmentation in transformer-like architectures, and provide a\nbasic analysis which compares the segmentation in supervised crate to the vanilla supervised ViT\nand DINO. In Section 4, we present extensive ablations and more detailed analysis of the segmentation\nproperty which utilizes the white-box structure of crate, and we obtain strong evidence that the\nwhite-box design of crate is the key to the emergent properties we observe.\nNotation. We denote the (patchified) input image by X = [x1, . . . , xN] \u2208 RD\u00d7N, where xi \u2208 RD\u00d71\nrepresents the i-th image patch and N represents the total number of image patches. xi is referred\nto as a token, and this term can be used interchangeably with image patch. We use f \u2208 F : RD\u00d7N \u2192\nRd\u00d7(N+1) to denote the mapping induced by the model; it is a composition of L + 1 layers, such that\nf = f L \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f \u2113 \u25e6 \u00b7 \u00b7 \u00b7 \u25e6 f 1 \u25e6 f 0, where f \u2113 : Rd\u00d7(N+1) \u2192 Rd\u00d7(N+1), 1 \u2264 \u2113 \u2264 L represents the mapping\nof the \u2113-th layer, and f 0 : X \u2208 RD\u00d7N \u2192 Z1 \u2208 Rd\u00d7(N+1) is the pre-processing layer that transforms\nimage patches X = [x1, . . . , xN] to tokens Z1 =\n\u0002z1\n[CLS], z1\n1, . . . , z1\nN\n\u0003\n, where z1\n[CLS] denotes the \u201cclass\ntoken\u201d, a model parameter eventually used for supervised classification in our training setup. We let\nZ\u2113 =\n\u0002\nz\u2113\n[CLS], z\u2113\n1, . . . , z\u2113\nN\n\u0003\n\u2208 Rd\u00d7(N+1)\n(1)\ndenote the input tokens of the \u2113th layer f \u2113 for 1 \u2264 \u2113 \u2264 L, so that z\u2113\ni \u2208 Rd gives the representation of\nthe ith image patch xi before the \u2113th layer, and z\u2113\n[CLS] \u2208 Rd gives the representation of the class token\nbefore the \u2113th layer. We use Z = ZL+1 to denote the output tokens of the last (Lth) layer.\n2. Preliminaries: White-Box Vision Transformers\nIn this section, we revisit the crate architecture (Coding RAte reduction TransformEr)\u2014a white-box\nvision transformer proposed in Yu et al. [51]. crate has several distinguishing features relative to\nthe vision transformer (ViT) architecture [11] that underlie the emergent visual representations we\nobserve in our experiments. We first introduce the network architecture of crate in Section 2.1, and\nthen present how to learn the parameters of crate via supervised learning in Section 2.2.\n2.1. Design of crate\u2014A White-Box Transformer Model\nRepresentation learning via unrolling optimization. As described in Yu et al. [51], the white-box\ntransformer crate f : RD\u00d7N \u2192 Rd\u00d7(N+1) is designed to transform input data X \u2208 RD\u00d7N drawn\nfrom a potentially nonlinear and multi-modal distribution to piecewise linearized and compact feature\nrepresentations Z \u2208 Rd\u00d7(N+1). It does this by posing a local signal model for the marginal distribution\nof the tokens zi. Namely, it asserts that the tokens are approximately supported on a union of several,\nsay K, low-dimensional subspaces, say of dimension p \u226a d, whose orthonormal bases are given by\nU[K] = (Uk)K\nk=1 where each Uk \u2208 Rd\u00d7p. With respect to this local signal model, the crate model is\ndesigned to optimize the sparse rate reduction objective [51]:\nmax\nf\u2208F EZ\n\u0002\n\u2206R(Z | U[K]) \u2212 \u03bb\u2225Z\u22250\n\u0003\n= max\nf\u2208F EZ\n\u0002\nR(Z) \u2212 \u03bb\u2225Z\u22250 \u2212 Rc(Z; U[K])\n\u0003\n,\n(2)\nwhere Z = f(X), the coding rate R(Z) is (a tight approximation for [30]) the average number of\nbits required to encode the tokens zi up to precision \u03b5 using a Gaussian codebook, and Rc(Z | U[K])\nis an upper bound on the average number of bits required to encode the tokens\u2019 projections onto\neach subspace in the local signal model, i.e., U \u2217\nkzi, up to precision \u03b5 using a Gaussian codebook [51].\nWhen these subspaces are sufficiently incoherent, the minimizers of the objective (2) as a function of\nZ correspond to axis-aligned and incoherent subspace arrangements [52].\nHence, a network designed to optimize (2) by unrolled optimization [7, 16, 32] incrementally\ntransforms the distribution of X towards the desired canonical forms: each iteration of unrolled\noptimization becomes a layer of the representation f, to wit\nZ\u2113+1 = f \u2113(Z\u2113),\n(3)\nwith the overall representation f thus constructed as\nf : X\nf 0\n\u2212\u2212\u2192 Z1 \u2192 \u00b7 \u00b7 \u00b7 \u2192 Z\u2113\nf \u2113\n\u2212\u2212\u2192 Z\u2113+1 \u2192 \u00b7 \u00b7 \u00b7 \u2192 ZL+1 = Z.\n(4)\nImportantly, in the unrolled optimization paradigm, each layer f \u2113 has its own, untied, local signal model\n3\nU \u2113\n[K]: each layer models the distribution of input tokens Z\u2113, enabling the linearization of nonlinear\nstructures in the input distribution X at a global scale over the course of the application of f.\nThe above unrolled optimization framework admits a variety of design choices to realize the layers\nf \u2113 that incrementally optimize (2). crate employs a two-stage alternating minimization approach\nwith a strong conceptual basis [51], which we summarize here and describe in detail below:\n1. First, the distribution of tokens Z\u2113 is compressed against the local signal model U \u2113\n[K] by an\napproximate gradient step on Rc(Z | U \u2113\n[K]) to create an intermediate representation Z\u2113+1/2;\n2. Second, this intermediate representation is sparsely encoded using a learnable dictionary D\u2113 to\ngenerate the next layer representation Z\u2113+1.\nExperiments demonstrate that even after supervised training, crate achieves its design goals for\nrepresentation learning articulated above [51].\nCompression operator: Multi-Head Subspace Self-Attention (MSSA). Given local models U \u2113\n[K],\nto form the incremental transformation f \u2113 optimizing (2) at layer \u2113, crate first compresses the token\nset Z\u2113 against the subspaces by minimizing the coding rate Rc( \u00b7 | U \u2113\n[K]). As Yu et al. [51] show,\ndoing this minimization locally by performing a step of gradient descent on Rc( \u00b7 | U \u2113\n[K]) leads to the\nso-called multi-head subspace self-attention (MSSA) operation, defined as\nMSSA(Z | U[K]) .=\np\n(N + 1)\u03b52 [U1, . . . , UK]\n\uf8ee\n\uf8ef\uf8f0\n(U \u2217\n1 Z) softmax((U \u2217\n1 Z)\u2217(U \u2217\n1 Z))\n...\n(U \u2217\nKZ) softmax((U \u2217\nKZ)\u2217(U \u2217\nKZ))\n\uf8f9\n\uf8fa\uf8fb ,\n(5)\nand the subsequent intermediate representation\nZ\u2113+1/2 = Z\u2113 \u2212\u03ba\u2207ZRc(Z\u2113 | U[K]) \u2248\n\u0012\n1 \u2212 \u03ba \u00b7\np\n(N + 1)\u03b52\n\u0013\nZ\u2113 +\u03ba\u00b7\np\n(N + 1)\u03b52 \u00b7MSSA(Z\u2113 | U[K]), (6)\nwhere \u03ba > 0 is a learning rate hyperparameter. This block bears a striking resemblance to the ViT\u2019s\nmulti-head self-attention block, with a crucial difference: the usual query, key, and value projection\nmatrices within a single head are here all identical, and determined by our local model for the\ndistribution of the input tokens. We will demonstrate via careful ablation studies that this distinction\nis crucial for the emergence of useful visual representations in crate.\nSparsification operator: Iterative Shrinkage-Thresholding Algorithm (ISTA). The remaining\nterm to optimize in (2) is the difference of the global coding rate R(Z) and the \u21130 norm of the tokens,\nwhich together encourage the representations to be both sparse and non-collapsed. Yu et al. [51]\nshow that local minimization of this objective in a neighborhood of the intermediate representations\nZ\u2113+1/2 is approximately achieved by a LASSO problem with respect to a sparsifying orthogonal\ndictionary D\u2113. Taking an iterative step towards solving this LASSO problem gives the iterative\nshrinkage-thresholding algorithm (ISTA) block [47, 51]:\nZ\u2113+1 = f \u2113(Z\u2113) = ReLU(Z\u2113+1/2 + \u03b7D\u2113\u2217(Z\u2113+1/2 \u2212 D\u2113Z\u2113+1/2) \u2212 \u03b7\u03bb1) .= ISTA(Z\u2113+1/2 | D\u2113).\n(7)\nHere, \u03b7 > 0 is a step size, and \u03bb > 0 is the sparsification regularizer. The ReLU nonlinearity\nappearing in this block arises from an additional nonnegativity constraint on the representations\nin the LASSO program, motivated by the goal of better separating distinct modes of variability in\nthe token distribution [17]. The ISTA block is reminiscent of the MLP block in the ViT, but with a\nrelocated skip connection.\nThe overall crate architecture. Combining the MSSA and the ISTA block, as above, together with\na suitable choice of hyperparameters, we arrive at the definition of a single crate layer:\nZ\u2113+1/2 .= Z\u2113 + MSSA(Z\u2113 | U \u2113\n[K]),\nf \u2113(Z\u2113) = Z\u2113+1 .= ISTA(Z\u2113+1/2 | D\u2113).\n(8)\nThese layers are composed to obtain the representation f, as in (4). We visualize the crate architec-\nture in Figure 2. Full pseudocode (both mathematical and PyTorch-style) is given in Appendix A.\nThe forward and backward pass of crate. The above conceptual framework separates the role\nof forward \u201coptimization,\u201d where each layer incrementally transforms its input towards a compact\n4\nCRATE\nCRATE\nCRATE\nViT\nViT\nViT\nFigure 3: Visualization of PCA components. We compute the PCA of the patch-wise representations of\neach column and visualize the first 3 components for the foreground object. Each component is matched to a\ndifferent RGB channel and the background is removed by thresholding the first PCA component of the full\nimage. The representations of crate are better aligned, and with less spurious correlations, to texture and\nshape components of the input than those of ViT. See the pipeline in Appendix B.2 for more details.\nand structured representation via compression and sparsification of the token representations using\nthe local signal models U \u2113\n[K] and sparsifying dictionaries D\u2113 at each layer, and backward \u201clearning,\u201d\nwhere the local signal models and sparsifying dictionaries are learned from supervised (as in our\nexperiments) or self-supervised training via back propagation to capture structures in the data.\nWe believe that such mathematically clear designs of crate play a key role in the emergence of\nsemantically meaningful properties in the final learned models, as we will soon see.\n2.2. Training crate with Supervised Learning\nAs described in previous subsection, given the local signal models (U \u2113\n[K])L\n\u2113=1 and sparsifying dictionaries\n(D\u2113)L\n\u2113=1, each layer of crate is designed to optimize the sparse rate reduction objective (2). To enable\nmore effective compression and sparsification, the parameters of local signal models need to be\nidentified. Previous work [51] proposes to learn the parameters (U \u2113\n[K], D\u2113)L\n\u2113=1 from data, specifically\nin a supervised manner by solving the following classification problem:\nmin\nW ,f\nX\ni\n\u2113CE(W zL+1\ni,[CLS], yi)\nwhere\nh\nzL+1\ni,[CLS], zL+1\ni,1 , . . . , zL+1\ni,N\ni\n= f(Xi),\n(9)\nwhere (Xi, yi) is the ith training (image, label) pair, W \u2208 Rd\u00d7C maps the [CLS] token to a vector of\nlogits, C is the number of classes, and \u2113CE(\u00b7, \u00b7) denotes the softmax cross-entropy loss.1\n3. Measuring Emerging Properties in crate\nWe now study the emergent segmentation properties in supervised crate both qualitatively and\nquantitatively. As demonstrated in previous work [6], segmentation within the ViT [11] emerges only\nwhen applying DINO, a very specialized self-supervised learning method [6]. In particular, a vanilla\nViT trained on supervised classification does not develop the ability to perform segmentation. In contrast,\nas we demonstrate both qualitatively and quantitatively in Section 3 and Section 4, segmentation\nproperties emerge in crate even when using standard supervised classification training.\nOur empirical results demonstrate that self-supervised learning, as well as the specialized design\noptions in DINO [6] (e.g., momentum encoder, student and teacher networks, self-distillation, etc.)\nare not necessary for the emergence of segmentation. We train all models (crate and ViT) with\nthe same number of data and iterations, as well as optimizers, to ensure experiments and ablations\nprovide a fair comparison\u2014precise details are provided in Appendix C.1.\n1This is similar to the supervised ViT training used in Dosovitskiy et al. [11].\n5\nSupervised CRATE\nSupervised ViT\nSupervised CRATE\nSupervised ViT\nViT\nCRATE\n(a) Visualization of coarse semantic segmentation.\nModel\nTrain\nmIoU\ncrate-S/8\nSupervised\n23.9\ncrate-B/8\nSupervised\n23.6\nViT-S/8\nSupervised\n14.1\nViT-B/8\nSupervised\n19.2\nViT-S/8\nDINO\n27.0\nViT-B/8\nDINO\n27.3\n(b) mIoU evaluation.\nFigure 4: Coarse semantic segmentation via self-attention map. (a) We visualize the segmentation masks for\nboth crate and the supervised ViT. We select the attention head with the best segmentation performance for\ncrate and ViT separately. (b) We quantitatively evaluate the coarse segmentation mask by evaluating the\nmIoU score on the validation set of PASCAL VOC12 [13]. Overall, crate demonstrates superior segmentation\nperformance to the supervised ViT both qualitatively (e.g., in (a), where the segmentation maps are much\ncleaner and outline the desired object), and quantitatively (e.g., in (b)).\n3.1. Qualitative Measurements\nVisualizing self-attention maps. To qualitatively measure the emergence phenomenon, we adopt\nthe attention map approach based on the [CLS] token, which has been widely used as a way to\ninterpret and visualize transformer-like architectures [1, 6]. Indeed, we use the same methodology as\n[1, 6], noting that in crate the query-key-value matrices are all the same; a more formal accounting\nis deferred to Appendix B.1. The visualization results of self-attention maps are summarized in\nFigure 1 and Figure 7. We observe that the self-attention maps of the crate model correspond to\nsemantic regions in the input image. Our results suggest that the crate model encodes a clear\nsemantic segmentation of each image in the network\u2019s internal representations, which is similar to\nthe self-supervised method DINO [6]. In contrast, as shown in Figure 14 in the Appendices, the\nvanilla ViT trained on supervised classification does not exhibit similar segmentation properties.\nPCA visualization for patch-wise representation. Following previous work [2, 35] on visualizing\nthe learned patch-wise deep features of image, we study the principal component analysis (PCA) on\nthe deep token representations of crate and ViT models. Again, we use the same methodology as\nthe previous studies [2, 35], and a more full accounting of the method is deferred to Appendix B.2.\nWe summarize the PCA visualization results of supervised crate in Figure 3. Without segmentation\nsupervision, crate is able to capture the boundary of the object in the image. Moreover, the principal\ncomponents demonstrate feature alignment between tokens corresponding to similar parts of the\nobject; for example, the red channel corresponds to the horse\u2019s leg. On the other hand, the PCA\nvisualization of the supervised ViT model is considerably less structured. We also provide more\nPCA visualization results in Figure 9.\n3.2. Quantitative Measurements\nBesides qualitatively assessing segmentation properties through visualization, we also quantitatively\nevaluate the emergent segmentation property of crate using existing segmentation and object\ndetection techniques [6, 46]. Both methods apply the internal deep representations of transformers,\nsuch as the previously discussed self-attention maps, to produce segmentation masks without further\ntraining on special annotations (e.g., object boxes, masks, etc.).\nCoarse segmentation via self-attention map. As shown in Figure 1, crate explicitly captures the\nobject-level semantics with clear boundaries. To quantitatively measure the quality of the induced\nsegmentation, we utilize the raw self-attention maps discussed earlier to generate segmentation\nmasks. Then, we evaluate the standard mIoU (mean intersection over union) score [28] by comparing\nthe generated segmentation masks against ground truth masks. This approach has been used in\nprevious work on evaluating the segmentation performance of the self-attention maps [6]. A more\ndetailed accounting of the methodology is found in Appendix B.3. The results are summarized in\nFigure 4. crate largely outperforms ViT both visually and in terms of mIoU, which suggests that\nthe internal representations in crate are much more effective for producing segmentation masks.\nObject detection and fine-grained segmentation. To further validate and evaluate the rich semantic\ninformation captured by crate, we employ MaskCut [46], a recent effective approach for object\ndetection and segmentation that does not require human annotations. As usual, we provide a more\ndetailed methodological description in Appendix B.4. This procedure allows us to extract more\n6\nViT\nCRATE\nSupervised CRATE\nSupervised ViT\nSupervised CRATE\nSupervised ViT\nFigure 5: Visualization of on COCO val2017 [27] with MaskCut. (Top Row) Supervised crate architecture\nclearly detects the major objects in the image. (Bottom Row) Supervised ViT sometimes fails to detect the major\nobjects in the image (columns 2, 3, 4).\nDetection\nSegmentation\nModel\nTrain\nAP50\nAP75\nAP\nAP50\nAP75\nAP\ncrate-S/8\nSupervised\n2.9\n1.0\n1.1\n1.8\n0.7\n0.8\ncrate-B/8\nSupervised\n2.9\n1.0\n1.3\n2.2\n0.7\n1.0\nViT-S/8\nSupervised\n0.1\n0.1\n0.0\n0.0\n0.0\n0.0\nViT-B/8\nSupervised\n0.8\n0.2\n0.4\n0.7\n0.5\n0.4\nViT-S/8\nDINO\n5.0\n2.0\n2.4\n4.0\n1.3\n1.7\nViT-B/8\nDINO\n5.1\n2.3\n2.5\n4.1\n1.3\n1.8\nTable 1: Object detection and fine-grained segmentation via MaskCut on COCO val2017 [27]. We consider\nmodels with different scales and evaluate the average precision measured by COCO\u2019s official evaluation metric.\nThe first four models are pre-trained with image classification tasks under label supervision; the bottom two\nmodels are pre-trained via the DINO self-supervised technique [6]. crate conclusively performs better than\nthe ViT at detection and segmentation metrics when both are trained using supervised classification.\nfine-grained segmentation from an image based on the token representations learned in crate.\nWe visualize the fine-grained segmentations produced by MaskCut in Figure 5 and compare the\nsegmentation and detection performance in Table 1. Based on these results, we observe that MaskCut\nwith supervised ViT features completely fails to produce segmentation masks in certain cases, for\nexample, the first image in Figure 5 and the ViT-S/8 row in Table 1. Compared to ViT, crate provides\nbetter internal representation tokens for both segmentation and detection.\n4. White-Box Empowered Analysis of Segmentation in crate\nIn this section, we delve into the segmentation properties of crate using analysis powered by our\nwhite-box perspective. To start with, we analyze the internal token representations from different\nlayers of crate and study the power of the network segmentation as a function of the layer depth.\nWe then perform an ablation study on various architectural configurations of crate to isolate the\nessential components for developing segmentation properties. Finally, we investigate how to identify\nthe \u201csemantic\u201d meaning of certain subspaces and extract more fine-grained information from crate.\nWe use the crate-B/8 and ViT-B/8 as the default models for evaluation in this section.\nRole of depth in crate. Each layer of crate is designed for the same conceptual purpose: to\noptimize the sparse rate reduction and transform the token distribution to compact and structured\nforms (Section 2). Given that the emergence of semantic segmentation in crate is analogous to\nthe clustering of tokens belonging to similar semantic categories in the representation Z, we therefore\nexpect the segmentation performance of crate to improve with increasing depth. To test this, we\nutilize the MaskCut pipeline (described in Section 3.2) to quantitatively evaluate the segmentation\nperformance of the internal representations across different layers. Meanwhile, we apply the PCA\nvisualization (described in Section 3.1) for understanding how segmentation emerges with respect\nto depth. Compared to the results in Figure 3, a minor difference in our visualization is that we show\nthe first four principal components in Figure 6 and do not filter out background tokens.\n7\n7\n8\n9\n10\n11\n12\nLayer index\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAP Score on Segmentation\nCRATE-B/8\nViT-B/8\nShallow\nDeep\nlayer 1\nlayer 4\nlayer 6\nlayer 8\nlayer 11\nFigure 6: Effect of depth for segmentation in supervised crate. (Left) Layer-wise segmentation performance\nof crate and ViT via MaskCut pipeline on COCO val2017 (Higher AP score means better segmentation\nperformance). (Right) We follow the implementation in Amir et al. [2]: we first apply PCA on patch-wise\nfeatures. Then, for the gray figure, we visualize the 1st components, and for the colored figure, we visualize the\n2nd, 3rd and 4th components, which correspond to the RGB color channels. See more results in Figure 9.\nCOCO Detection\nVOC Seg.\nModel\nAttention\nNonlinearity\nAP50\nAP75\nAP\nmIoU\ncrate\nMSSA\nISTA\n2.1\n0.7\n0.8\n23.9\ncrate-MLP\nMSSA\nMLP\n0.2\n0.2\n0.2\n22.0\ncrate-MHSA\nMHSA\nISTA\n0.1\n0.1\n0.0\n18.4\nViT\nMHSA\nMLP\n0.1\n0.1\n0.0\n14.1\nTable 2: Ablation study of different crate variants. We use the Small-Patch8 (S-8) model configuration\nacross all experiments in this table.\nThe results are summarized in Figure 6. We observe that the segmentation score improves when\nusing representations from deeper layers, which aligns well with the incremental optimization design\nof crate. In contrast, even though the performance of ViT-B/8 slightly improves in later layers, its\nsegmentation scores are significantly lower than those of crate (c.f. failures in Figure 5, bottom\nrow). The PCA results are presented in Figure 6 (Right). We observe that representations extracted\nfrom deeper layers of crate increasingly focus on the foreground object and are able to capture\ntexture-level details. Figure 9 in the Appendix has more PCA visualization results.\nAblation study of architecture in crate. Both the attention block (MSSA) and the MLP block (ISTA)\nin crate are different from the ones in the ViT. In order to understand the effect of each component\nfor emerging segmentation properties of crate, we study three different variants of crate: crate,\ncrate-MHSA, crate-MLP, where we denote the attention block and MLP block in ViT as MHSA and\nMLP respectively. We summarize different model architectures in Table 2.\nFor all models in Table 2, we apply the same pre-training setup on the ImageNet-21k dataset. We\nthen apply the coarse segmentation evaluation (Section 3.2) and MaskCut evaluation (Section 3.2)\nto quantitatively compare the performance of different models. As shown in Table 2, crate sig-\nnificantly outperforms other model architectures across all tasks. Interestingly, we find that the\ncoarse segmentation performance (i.e., VOC Seg) of the ViT can be significantly improved by simply\nreplacing the MHSA in ViT with the MSSA in crate, despite the architectural differences between MHSA\nand MSSA being small. This demonstrates the effectiveness of the white-box design.\nIdentifying semantic properties of attention heads. As shown in Figure 1, the self-attention map\nbetween the [CLS] token and patch tokens contains clear segmentation masks. We are interested in\ncapturing the semantic meaning of certain attention heads; this is an important task for interpretability,\nand is already studied for language transformers [34]. Intuitively, each head captures certain features\nof the data. Given a crate model, we first forward an input image (e.g. a horse image as in Figure 7)\nand select four attention heads which seem to have semantic meaning by manual inspection. After\nidentifying the attention heads, we visualize the self-attention map of these heads on other input\nimages. We visualize the results in Figure 7. Interestingly, we find that each of the selected attention\nheads captures a different part of the object, and even a different semantic meaning. For example,\nthe attention head displayed in the first column of Figure 7 captures the legs of different animals,\n8\nHead 0\n\u201cLeg\u201d\nHead 1\n\u201cBody\u201d\nHead 3\n\u201cFace\u201d\nHead 4\n\u201cEar\u201d\nHead 0\n\u201cLeg\u201d\nHead 1\n\u201cBody\u201d\nHead 3\n\u201cFace\u201d\nHead 4\n\u201cEar\u201d\nFigure 7: Visualization of semantic heads. We forward a mini-batch of images through a supervised crate\nand examine the attention maps from all the heads in the penultimate layer. We visualize a selection of attention\nheads to show that certain heads convey specific semantic meaning, i.e. head 0 \u2194 \"Legs\", head 1 \u2194 \"Body\", head\n3 \u2194 \"Face\", head 4 \u2194 \"Ear\".\nand the attention head displayed in the last column captures the ears and head. This parsing of the\nvisual input into a part-whole hierarchy has been a fundamental goal of learning-based recognition\narchitectures since deformable part models [14, 15] and capsule networks [20, 40]\u2014strikingly, it\nemerges from the white-box design of crate within our simple supervised training setup.2\n5. Related Work\nVisual attention and emergence of segmentation. The concept of attention has become increasingly\nsignificant in intelligence, evolving from early computational models [21, 23, 41] to modern neural\nnetworks [11, 44]. In deep learning, the self-attention mechanism has been widely employed in\nprocessing visual data [11] with state-of-the-art performance on various visual tasks [6, 18, 35].\nDINO [6] demonstrated that attention maps generated by self-supervised Vision Transformers\n(ViT)[11] can implicitly perform semantic segmentation of images. This emergence of segmen-\ntation capability has been corroborated by subsequent self-supervised learning studies [6, 18, 35].\nCapitalizing on these findings, recent segmentation methodologies [2, 22, 46] have harnessed these\nemergent segmentations to attain state-of-the-art results. Nonetheless, there is a consensus, as high-\nlighted in studies like Caron et al. [6], suggesting that such segmentation capability would not\n2In this connection, we note that Hinton [19] recently surmised that the query, key, and value projections in\nthe transformer should be made equal for this reason\u2014the design of crate and Figure 7 confirm this.\n9\nmanifest in a supervised ViT. A key motivation and contribution of our research is to show that\ntransformer-like architectures, as in crate, can exhibit this ability even with supervised training.\nWhite-box models. In data analysis, there has continually been significant interest in developing\ninterpretable and structured representations of the dataset. The earliest manifestations of such interest\nwere in sparse coding via dictionary learning [47], which are white-box models that transform the\n(approximately linear) data into human-interpretable standard forms (highly sparse vectors). The\nadvent of deep learning has not changed this desire much, and indeed attempts have been made\nto marry the power of deep learning with the interpretability of white-box models. Such attempts\ninclude scattering networks [5], convolutional sparse coding networks [36], and the sparse manifold\ntransform [9]. Another line of work constructs deep networks from unrolled optimization [7, 43,\n50, 51]. Such models are fully interpretable, yet only recently have they demonstrated competitive\nperformance with black-box alternatives such as ViT at ImageNet scale [51]. This work builds on\none such powerful white-box model, crate [51], and demonstrates more of its capabilities, while\nserving as an example for the fine-grained analysis made possible by white-box models.\n6. Discussions and Future Work\nIn this study, we demonstrated that when employing the white-box model crate as a foundational\narchitecture in place of the ViT, there is a natural emergence of segmentation masks even while using\na straightforward supervised training approach. Our empirical findings underscore the importance\nof principled architecture design for developing better vision foundation models. As simpler models\nare more interpretable and easier to analyze, we are optimistic that the insights derived from white-\nbox transformers in this work will contribute to a deeper empirical and theoretical understanding of\nthe segmentation phenomenon. Furthermore, our findings suggest that white-box design principles\nhold promise in offering concrete guidelines for developing enhanced vision foundation models. Two\ncompelling directions for further research would be investigating how to better engineer white-box\nmodels such as crate to match the performance of self-supervised learning methods (such as\nDINO), and expanding the range of tasks for which white-box models are practically useful.\nAcknowledgements. We thank Xudong Wang and Baifeng Shi for valuable discussions on segmen-\ntation properties in vision transformers.\nReferences\n[1]\nSamira Abnar and Willem Zuidema. \u201cQuantifying attention flow in transformers\u201d. arXiv\npreprint arXiv:2005.00928 (2020). 6, 16.\n[2]\nShir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. \u201cDeep ViT Features as Dense Visual\nDescriptors\u201d. ECCVW What is Motion For? (2022). 2, 6, 8, 9, 16, 17.\n[3]\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von\nArx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. \u201cOn the\nopportunities and risks of foundation models\u201d. arXiv preprint arXiv:2108.07258 (2021). 2.\n[4]\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. \u201cLanguage models\nare few-shot learners\u201d. Advances in neural information processing systems 33 (2020), pp. 1877\u2013\n1901. 2.\n[5]\nJoan Bruna and St\u00e9phane Mallat. \u201cInvariant scattering convolution networks\u201d. IEEE transactions\non pattern analysis and machine intelligence 35.8 (Aug. 2013), pp. 1872\u20131886. 10.\n[6]\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. \u201cEmerging properties in self-supervised vision transformers\u201d. Proceedings\nof the IEEE/CVF international conference on computer vision. 2021, pp. 9650\u20139660. 2, 5\u20137, 9, 16, 19.\n[7]\nKwan Ho Ryan Chan, Yaodong Yu, Chong You, Haozhi Qi, John Wright, and Yi Ma. \u201cReduNet:\nA White-box Deep Network from the Principle of Maximizing Rate Reduction\u201d. Journal of\nMachine Learning Research 23.114 (2022), pp. 1\u2013103. 3, 10.\n[8]\nXiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham,\nXuanyi Dong, Thang Luong, Cho-Jui Hsieh, et al. \u201cSymbolic discovery of optimization algo-\nrithms\u201d. arXiv preprint arXiv:2302.06675 (2023). 18.\n10\n[9]\nYubei Chen, Dylan Paiton, and Bruno Olshausen. \u201cThe sparse manifold transform\u201d. Advances\nin neural information processing systems 31 (2018). 10.\n[10]\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. \u201cImagenet: A large-scale\nhierarchical image database\u201d. 2009 IEEE conference on computer vision and pattern recognition.\nIeee. 2009, pp. 248\u2013255. 18.\n[11]\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\net al. \u201cAn image is worth 16x16 words: Transformers for image recognition at scale\u201d. arXiv\npreprint arXiv:2010.11929 (2020). 2, 3, 5, 9, 18.\n[12]\nMichael Elad and Michal Aharon. \u201cImage denoising via sparse and redundant representations\nover learned dictionaries\u201d. IEEE transactions on image processing: a publication of the IEEE Signal\nProcessing Society 15.12 (Dec. 2006), pp. 3736\u20133745. 1.\n[13]\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman. The\nPASCAL Visual Object Classes Challenge 2012 (VOC2012) Results. http://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.html. 6, 18.\n[14]\nPedro Felzenszwalb, David McAllester, and Deva Ramanan. \u201cA discriminatively trained,\nmultiscale, deformable part model\u201d. 2008 IEEE Conference on Computer Vision and Pattern\nRecognition. ieeexplore.ieee.org, June 2008, pp. 1\u20138. 9.\n[15]\nPedro F Felzenszwalb and Daniel P Huttenlocher. \u201cPictorial Structures for Object Recognition\u201d.\nInternational journal of computer vision 61.1 (Jan. 2005), pp. 55\u201379. 9.\n[16]\nKarol Gregor and Yann LeCun. \u201cLearning fast approximations of sparse coding\u201d. Proceedings\nof the 27th International Conference on International Conference on Machine Learning. Omnipress.\n2010, pp. 399\u2013406. 3.\n[17]\nFlorentin Guth, John Zarka, and St\u00e9phane Mallat. \u201cPhase Collapse in Neural Networks\u201d.\nInternational Conference on Learning Representations. 2022. 4.\n[18]\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. \u201cMasked\nautoencoders are scalable vision learners\u201d. Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition. 2022, pp. 16000\u201316009. 9.\n[19]\nGeoffrey Hinton. \u201cHow to represent part-whole hierarchies in a neural network\u201d (Feb. 2021).\narXiv: 2102.12627 [cs.CV]. 9.\n[20]\nGeoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. \u201cTransforming Auto-Encoders\u201d.\nArtificial Neural Networks and Machine Learning \u2013 ICANN 2011. Springer Berlin Heidelberg, 2011,\npp. 44\u201351. 9.\n[21]\nLaurent Itti and Christof Koch. \u201cComputational modelling of visual attention\u201d. Nature reviews\nneuroscience 2.3 (2001), pp. 194\u2013203. 9.\n[22]\nAlexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. \u201cSegment anything\u201d.\narXiv preprint arXiv:2304.02643 (2023). 2, 9.\n[23]\nChristof Koch and Shimon Ullman. \u201cShifts in selective visual attention: towards the underlying\nneural circuitry.\u201d Human neurobiology 4.4 (1985), pp. 219\u2013227. 9.\n[24]\nPhilipp Kr\u00e4henb\u00fchl and Vladlen Koltun. \u201cEfficient inference in fully connected crfs with\ngaussian edge potentials\u201d. Advances in neural information processing systems 24 (2011). 17.\n[25]\nAlex Krizhevsky, Geoffrey Hinton, et al. \u201cLearning multiple layers of features from tiny images\u201d\n(2009). 18.\n[26]\nYann LeCun. \u201cA path towards autonomous machine intelligence version 0.9. 2, 2022-06-27\u201d.\nOpen Review 62 (2022). 1.\n[27]\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. \u201cMicrosoft coco: Common objects in context\u201d. Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part\nV 13. Springer. 2014, pp. 740\u2013755. 7, 18.\n[28]\nJonathan Long, Evan Shelhamer, and Trevor Darrell. \u201cFully convolutional networks for seman-\ntic segmentation\u201d. Proceedings of the IEEE conference on computer vision and pattern recognition.\n2015, pp. 3431\u20133440. 6, 17.\n[29]\nIlya Loshchilov and Frank Hutter. \u201cDecoupled weight decay regularization\u201d. arXiv preprint\narXiv:1711.05101 (2017). 18.\n11\n[30]\nYi Ma, Harm Derksen, Wei Hong, and John Wright. \u201cSegmentation of multivariate mixed data\nvia lossy data coding and compression\u201d. PAMI (2007). 3.\n[31]\nYi Ma, Doris Tsao, and Heung-Yeung Shum. \u201cOn the principles of parsimony and self-\nconsistency for the emergence of intelligence\u201d. Frontiers of Information Technology & Electronic\nEngineering 23.9 (2022), pp. 1298\u20131323. 1.\n[32]\nVishal Monga, Yuelong Li, and Yonina C Eldar. \u201cAlgorithm Unrolling: Interpretable, Efficient\nDeep Learning for Signal and Image Processing\u201d. IEEE Signal Processing Magazine 38.2 (Mar.\n2021), pp. 18\u201344. 3.\n[33]\nMaria-Elena Nilsback and Andrew Zisserman. \u201cAutomated flower classification over a large\nnumber of classes\u201d. 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing.\nIEEE. 2008, pp. 722\u2013729. 18.\n[34]\nCatherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom\nHenighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain,\nDeep Ganguli, Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson\nKernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan,\nSam McCandlish, and Chris Olah. \u201cIn-context Learning and Induction Heads\u201d. Transformer Cir-\ncuits Thread (2022). https://transformer-circuits.pub/2022/in-context-learning-and-induction-\nheads/index.html. 8.\n[35]\nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khali-\ndov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. \u201cDinov2:\nLearning robust visual features without supervision\u201d. arXiv preprint arXiv:2304.07193 (2023).\n2, 6, 9, 16.\n[36]\nVardan Papyan, Yaniv Romano, Jeremias Sulam, and Michael Elad. \u201cTheoretical Foundations\nof Deep Learning via Sparse Representations: A Multilayer Sparse Model and Its Connection to\nConvolutional Neural Networks\u201d. IEEE Signal Processing Magazine 35.4 (July 2018), pp. 72\u201389.\n10.\n[37]\nOmkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. \u201cCats and dogs\u201d.\n2012 IEEE conference on computer vision and pattern recognition. IEEE. 2012, pp. 3498\u20133505. 18.\n[38]\nIgnacio Ramirez, Pablo Sprechmann, and Guillermo Sapiro. \u201cClassification and clustering via\ndictionary learning with structured incoherence and shared features\u201d. 2010 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recognition. IEEE. 2010, pp. 3501\u20133508. 1.\n[39]\nShankar Rao, Roberto Tron, Ren\u00e9 Vidal, and Yi Ma. \u201cMotion segmentation in the presence\nof outlying, incomplete, or corrupted trajectories\u201d. IEEE transactions on pattern analysis and\nmachine intelligence 32.10 (Oct. 2010), pp. 1832\u20131845. 1.\n[40]\nSara Sabour, Nicholas Frosst, and Geoffrey E Hinton. \u201cDynamic Routing Between Capsules\u201d.\nAdvances in Neural Information Processing Systems. Ed. by I Guyon, U Von Luxburg, S Bengio,\nH Wallach, R Fergus, S Vishwanathan, and R Garnett. Vol. 30. Curran Associates, Inc., 2017. 9.\n[41]\nBrian J Scholl. \u201cObjects and attention: The state of the art\u201d. Cognition 80.1-2 (2001), pp. 1\u201346. 9.\n[42]\nJianbo Shi and Jitendra Malik. \u201cNormalized cuts and image segmentation\u201d. IEEE Transactions\non pattern analysis and machine intelligence 22.8 (2000), pp. 888\u2013905. 17.\n[43]\nBahareh Tolooshams and Demba Ba. \u201cStable and Interpretable Unrolled Dictionary Learning\u201d.\narXiv preprint arXiv:2106.00058 (2021). 10.\n[44]\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. \u201cAttention is all you need\u201d. Advances in neural information\nprocessing systems 30 (2017). 9.\n[45]\nQianqian Wang, Yen-Yu Chang, Ruojin Cai, Zhengqi Li, Bharath Hariharan, Aleksander Holyn-\nski, and Noah Snavely. \u201cTracking Everything Everywhere All at Once\u201d. arXiv:2306.05422\n(2023). 2.\n[46]\nXudong Wang, Rohit Girdhar, Stella X Yu, and Ishan Misra. \u201cCut and learn for unsuper-\nvised object detection and instance segmentation\u201d. Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 2023, pp. 3124\u20133134. 2, 6, 9, 17, 18.\n[47]\nJohn Wright and Yi Ma. High-Dimensional Data Analysis with Low-Dimensional Models: Principles,\nComputation, and Applications. Cambridge University Press, 2022. 4, 10.\n[48]\nAllen Y Yang, John Wright, Yi Ma, and S Shankar Sastry. \u201cUnsupervised segmentation of\nnatural images via lossy data compression\u201d. Computer Vision and Image Understanding 110.2\n(2008), pp. 212\u2013225. 1.\n12\n[49]\nJianchao Yang, John Wright, Thomas S Huang, and Yi Ma. \u201cImage super-resolution via sparse\nrepresentation\u201d. IEEE transactions on image processing: a publication of the IEEE Signal Processing\nSociety 19.11 (Nov. 2010), pp. 2861\u20132873. 1.\n[50]\nYongyi Yang, David P Wipf, et al. \u201cTransformers from an optimization perspective\u201d. Advances\nin Neural Information Processing Systems 35 (2022), pp. 36958\u201336971. 10.\n[51]\nYaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, Benjamin D.\nHaeffele, and Yi Ma. White-Box Transformers via Sparse Rate Reduction. 2023. arXiv: 2306.01129\n[cs.LG]. 2\u20135, 10, 14, 18.\n[52]\nYaodong Yu, Kwan Ho Ryan Chan, Chong You, Chaobing Song, and Yi Ma. \u201cLearning Diverse\nand Discriminative Representations via the Principle of Maximal Coding Rate Reduction\u201d.\nAdvances in Neural Information Processing Systems 33 (2020), pp. 9422\u20139434. 3.\n13\nAppendix\nA. crate Implementation\nIn this section, we provide the details on our implementation of crate, both at a higher level for use\nin mathematical analysis, and at a code-based level for use in reference implementations. While we\nused the same implementation as in Yu et al. [51], we provide the details here for completeness.\nA.1. Forward-Pass Algorithm\nWe provide the details on the forward pass of crate in Algorithm 1.\nAlgorithm 1 CRATE Forward Pass.\nHyperparameter: Number of layers L, feature dimension d, subspace dimension p, image dimension\n(C, H, W), patch dimension (PH, PW ), sparsification regularizer \u03bb > 0, quantization error \u03b5,\nlearning rate \u03b7 > 0.\nParameter: Patch projection matrix W \u2208 Rd\u00d7D.\n\u25b7 D .= PHPW .\nParameter: Class token z0\n[CLS] \u2208 Rd.\nParameter: Positional encoding Epos \u2208 Rd\u00d7(N+1).\n\u25b7 N .=\nH\nPH \u00b7 W\nPW .\nParameter: Local signal models (U \u2113\n[K])L\n\u2113=1 where each U \u2113\n[K] = (U \u2113\n1, . . . , U \u2113\nK) and each U \u2113\nk \u2208 Rd\u00d7p.\nParameter: Sparsifying dictionaries (D\u2113)L\n\u2113=1 where each D\u2113 \u2208 Rd\u00d7d.\nParameter: Sundry LayerNorm parameters.\n1: function MSSA(Z \u2208 Rd\u00d7(N+1) | U[K] \u2208 RK\u00d7d\u00d7p)\n2:\nreturn\np\n(N + 1)\u03b52\nK\nX\nk=1\nUk(U \u2217\nkZ) softmax((U \u2217\nkZ)\u2217(U \u2217\nkZ))\n\u25b7 Eq. (5)\n3: end function\n4: function ISTA(Z \u2208 Rd\u00d7(N+1) | D \u00d7 Rd\u00d7d)\n5:\nreturn ReLU(Z + \u03b7D\u2217(Z \u2212 DZ) \u2212 \u03b7\u03bb1)\n\u25b7 Eq. (7)\n6: end function\n7: function CRATEForwardPass(IMG \u2208 RC\u00d7H\u00d7W )\n8:\nX .= [x1, . . . , xN] \u2190 Patchify(IMG)\n\u25b7 X \u2208 RD\u00d7N and each xi \u2208 RD.\n9:\n# f 0 Operator\n10:\n\u0002\nz1\n1, . . . , z1\nN\n\u0003\n\u2190 W X\n\u25b7 z1\ni \u2208 Rd.\n11:\nZ1 \u2190\n\u0002z1\n[CLS], z1\n1, . . . , z1\nN\n\u0003\n+ Epos\n\u25b7 Z1 \u2208 Rd\u00d7(N+1).\n12:\n# f \u2113 Operators\n13:\nfor \u2113 \u2208 {1, . . . , L} do\n14:\nZ\u2113\nn \u2190 LayerNorm(Z\u2113)\n\u25b7 Z\u2113\nn \u2208 Rd\u00d7(N+1)\n15:\nZ\u2113+1/2 \u2190 Z\u2113\nn + MSSA(Z\u2113\nn | U \u2113\n[K])\n\u25b7 Z\u2113+1/2 \u2208 Rd\u00d7(N+1)\n16:\nZ\u2113+1/2\nn\n\u2190 LayerNorm(Z\u2113+1/2)\n\u25b7 Z\u2113+1/2\nn\n\u2208 Rd\u00d7(N+1)\n17:\nZ\u2113+1 \u2190 ISTA(Z\u2113+1/2\nn\n| D\u2113)\n\u25b7 Z\u2113+1 \u2208 Rd\u00d7(N+1)\n18:\nend for\n19:\nreturn Z \u2190 ZL+1\n20: end function\n14\nA.2. PyTorch-Like Code for Forward Pass\nSimilar to the previous subsection, we provide the pseudocode for the MSSA block and ISTA block in\nAlgorithm 2, and then present the pseudocode for the forward pass of crate in Algorithm 3.\nAlgorithm 2 PyTorch-Like Code for MSSA and ISTA Forward Passes\n1 class ISTA:\n2\n# initialization\n3\ndef __init__(self, dim, hidden_dim, dropout = 0., step_size=0.1, lambd=0.1):\n4\nself.weight = Parameter(Tensor(dim, dim))\n5\ninit.kaiming_uniform_(self.weight)\n6\nself.step_size = step_size\n7\nself.lambd = lambd\n8\n# forward pass\n9\ndef forward(self, x):\n10\nx1 = linear(x, self.weight, bias=None)\n11\ngrad_1 = linear(x1, self.weight.t(), bias=None)\n12\ngrad_2 = linear(x, self.weight.t(), bias=None)\n13\ngrad_update = self.step_size * (grad_2 - grad_1) - self.step_size * self.lambd\n14\noutput = relu(x + grad_update)\n15\nreturn output\n16 class MSSA:\n17\n# initialization\n18\ndef __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n19\ninner_dim = dim_head * heads\n20\nproject_out = not (heads == 1 and dim_head == dim)\n21\nself.heads = heads\n22\nself.scale = dim_head ** -0.5\n23\nself.attend = Softmax(dim = -1)\n24\nself.dropout = Dropout(dropout)\n25\nself.qkv = Linear(dim, inner_dim, bias=False)\n26\nself.to_out = Sequential(Linear(inner_dim, dim), Dropout(dropout)) if project_out\nelse nn.Identity()\n27\n# forward pass\n28\ndef forward(self, x):\n29\nw = rearrange(self.qkv(x), \u2019b n (h d) -> b h n d\u2019, h = self.heads)\n30\ndots = matmul(w, w.transpose(-1, -2)) * self.scale\n31\nattn = self.attend(dots)\n32\nattn = self.dropout(attn)\n33\nout = matmul(attn, w)\n34\nout = rearrange(out, \u2019b h n d -> b n (h d)\u2019)\n35\nreturn self.to_out(out)\nAlgorithm 3 PyTorch-Like Code for CRATE Forward Pass\n1 class CRATE:\n2\n# initialization\n3\ndef __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n4\n# define layers\n5\nself.layers = []\n6\nself.depth = depth\n7\nfor _ in range(depth):\n8\nself.layers.extend([LayerNorm(dim), MSSA(dim, heads, dim_head, dropout)])\n9\nself.layers.extend([LayerNorm(dim), ISTA(dim, mlp_dim, dropout)])\n10\n# forward pass\n11\ndef forward(self, x):\n12\nfor ln1, attn, ln2, ff in self.layers:\n13\nx_ = attn(ln1(x)) + ln1(x)\n14\nx = ff(ln2(x_))\n15\nreturn x\n15\nB. Detailed Experimental Methodology\nIn this section we formally describe each of the methods used to evaluate the segmentation property\nof crate in Section 3 and Section 4, especially compared to DINO and supervised ViT. This section\nrepeats experimental methodologies covered less formally in other works; we strive to rigorously\ndefine the experimental methodologies in this section.\nB.1. Visualizing Attention Maps\nWe recapitulate the method to visualize attention maps in Abnar and Zuidema [1] and Caron et al.\n[6], at first specializing their use to instances of the crate model before generalizing to the ViT.\nFor the kth head at the \u2113th layer of crate, we compute the self-attention matrix A\u2113\nk \u2208 RN defined as\nfollows:\nA\u2113\nk =\n\uf8ee\n\uf8ef\uf8f0\nA\u2113\nk,1\n...\nA\u2113\nk,N\n\uf8f9\n\uf8fa\uf8fb \u2208 RN,\nwhere\nA\u2113\nk,i =\nexp(\u27e8U \u2113\u2217\nk z\u2113\ni, U \u2113\u2217\nk z\u2113\n[CLS]\u27e9)\nPN\nj=1 exp(\u27e8U \u2113\u2217\nk z\u2113\nj, U \u2113\u2217\nk z\u2113\n[CLS]\u27e9)\n.\n(10)\nWe then reshape the attention matrix A\u2113\nk into a\n\u221a\nN \u00d7\n\u221a\nN matrix and visualize the heatmap as\nshown in Figure 1. For example, the ith row and the jth column element of the heatmap in Figure 1\ncorresponds to the mth component of A\u2113\nk if m = (i \u2212 1) \u00b7\n\u221a\nN + j. In Figure 1, we select one attention\nhead of crate and visualize the attention matrix A\u2113\nk for each image.\nFor the ViT, the entire methodology remains the same, except that the attention map is defined in\nthe following reasonable way:\nA\u2113\nk =\n\uf8ee\n\uf8ef\uf8f0\nA\u2113\nk,1\n...\nA\u2113\nk,N\n\uf8f9\n\uf8fa\uf8fb \u2208 RN,\nwhere\nA\u2113\nk,i =\nexp(\u27e8K\u2113\u2217\nk z\u2113\ni, Q\u2113\u2217\nk z\u2113\n[CLS]\u27e9)\nPN\nj=1 exp(\u27e8K\u2113\u2217\nk z\u2113\nj, Q\u2113\u2217\nk z\u2113\n[CLS]\u27e9)\n.\n(11)\nwhere the \u201cquery\u201d and \u201ckey\u201d parameters of the standard transformer at head k and layer \u2113 are\ndenoted K\u2113\nk and Q\u2113\nk respectively.\nB.2. PCA Visualizations\nAs in the previous subsection, we recapitulate the method to visualize the patch representations\nusing PCA from Amir et al. [2] and Oquab et al. [35]. As before we specialize their use to instances\nof the crate model before generalizing to the ViT.\nWe first select J images that belong to the same class, {Xj}J\nj=1, and extract the token representations\nfor each image at layer \u2113, i.e.,\n\u0002z\u2113\nj,[CLS], z\u2113\nj,1, . . . , z\u2113\nj,N\n\u0003\nfor j \u2208 [J]. In particular, z\u2113\nj,i represents the ith\ntoken representation at the \u2113th layer for the jth image. We then compute the first PCA components\nof bZ\u2113 = {bz\u2113\n1,1, . . . , bz\u2113\n1,N, . . . , bz\u2113\nJ,1, . . . , bz\u2113\nJ,N}, and use bz\u2113\nj,i to denote the aggregated token represen-\ntation for the i-th token of Xj, i.e., bz\u2113\nj,i = [(U \u2217\n1 bz\u2113\nj,i)\u22a4, . . . , (U \u2217\nK bz\u2113\nj,i)\u22a4]\u22a4 \u2208 R(p\u00b7K)\u00d71. We denote the\nfirst eigenvector of the matrix bZ\u2217 bZ by u0 and compute the projection values as {\u03c3\u03bb(\u27e8u0, z\u2113\nj,i\u27e9)}i,j,\nwhere \u03c3\u03bb(x) =\n\u001ax,\n|x| \u2265 \u03bb\n0,\n|x| < \u03bb is the hard-thresholding function. We then select a subset of token\nrepresentations from bZ with \u03c3\u03bb(\u27e8u0, z\u2113\nj,i\u27e9) > 0. which correspond to non-zero projection values\nafter thresholding, and we denote this subset as bZs \u2286 bZ. This selection step is used to remove\nthe background [35]. We then compute the first three PCA components of bZs with the first three\neigenvectors of matrix bZ\u2217\ns bZs denoted as {u1, u2, u3}. We define the RGB tuple for each token as:\n[rj,i, gj,i, bj,i] = [\u27e8u1, z\u2113\nj,i\u27e9, \u27e8u2, z\u2113\nj,i\u27e9, \u27e8u3, z\u2113\nj,i\u27e9],\ni \u2208 [N], j \u2208 [J], z\u2113\nj,i \u2208 bZs.\n(12)\nNext, for each image Xj we compute Rj, Gj, Bj, where Rj = [rj,1, . . . , rj,N]\u22a4 \u2208 Rd\u00d71 (similar for\nGj and Bj). Then we reshape the three matrices into\n\u221a\nN \u00d7\n\u221a\nN and visualize the \u201cPCA components\u201d\nof image Xj via the RGB image (Rj, Gj, Bj) \u2208 R3\u00d7\n\u221a\nN\u00d7\n\u221a\nN.\n16\nThe PCA visualization of ViTs are evaluated similarly, with the exception of utilizing the \u201cKey\u201d\nfeatures bz\u2113\nj,i = [(K\u2217\n1 bz\u2113\nj,i)\u22a4, . . . , (K\u2217\nK bz\u2113\nj,i)\u22a4]\u22a4. Previous work [2] demonstrated that the \u201cKey\u201d features\nlead to less noisy space structures than the \u201cQuery\u201d features. In the experiments (such as in Figure 3),\nwe set the threshold value \u03bb = 1\n2.\nB.3. Segmentation Maps and mIoU Scores\nWe now discuss the methods used to compute the segmentation maps and the corresponding\nmean-Union-over-Intersection (mIoU) scores.\nIndeed, suppose we have already computed the attention maps A\u2113\nk \u2208 RN for a given image as\nin Appendix B.1. We then threshold each attention map by setting its top P = 60% of entries to\n1 and setting the rest to 0. The remaining matrix, say \u02dc\nA\u2113\nk \u2208 {0, 1}N, forms a segmentation map\ncorresponding to the kth head in the \u2113th layer for the image.\nSuppose that the tokens can be partitioned into M semantic classes, and the mth semantic class has\na boolean ground truth segmentation map Sm \u2208 {0, 1}N. We want to compute the quality of the\nattention-created segmentation map above, with respect to the ground truth maps. For this, we\nuse the mean-intersection-over-union (mIoU) metric [28] as described in the sequel. Experimental\nresults yield that the heads at a given layer correspond to different semantic features. Thus, for each\nsemantic class m and layer \u2113, we attempt to find the best-matched head at layer \u2113 and use this to\ncompute the intersection-over-union, obtaining\nmIoU\u2113\nm\n.= max\nk\u2208[K]\n\u2225Sm \u2299 A\u2113\nk\u22250\n\u2225Sm\u22250 + \u2225A\u2113\nk\u22250 \u2212 \u2225Sm \u2299 A\u2113\nk\u22250\n,\n(13)\nwhere \u2299 denotes element-wise multiplication and \u2225\u00b7\u22250 counts the number of nonzero elements\nin the input vector (and since the inputs are boolean vectors, this is equivalent to counting the\nnumber of 1\u2019s). To report the overall mIoU score for layer \u2113 (or without referent, for the last layer\nrepresentations), we compute the quantity\nmIoU\u2113 .= 1\nM\nM\nX\nm=1\nmIoU\u2113\nm,\n(14)\nand average it amongst all images for which we know the ground truth.\nB.4. MaskCut\nWe apply the MaskCut pipeline (Algorithm 4) to generate segmentation masks and detection\nbounding box (discussed in Section 3.2). As described by Wang et al. [46], we iteratively apply\nNormalized Cuts [42] on the patch-wise affinity matrix M \u2113, where M \u2113\nij = PK\nk=1\u27e8U \u2113\u2217\nk z\u2113\ni, U \u2113\u2217\nk z\u2113\nj\u27e9. At\neach iterative step, we mask out the identified patch-wise entries on M \u2113. To obtain more fine-grained\nsegmentation masks, MaskCut employs Conditional Random Fields (CRF) [24] to post-process the\nmasks, which smooths the edges and filters out unreasonable masks. Correspondingly, the detection\nbounding box is defined by the rectangular region that tightly encloses a segmentation mask.\nAlgorithm 4 MaskCut\nHyperparameter: n, the number of objects to segment.\n1: function MaskCut(M)\n2:\nfor i \u2208 {1, . . . , n} do\n3:\nmask \u2190 NCut(M)\n\u25b7 mask is a boolean array\n4:\nM \u2190 M \u2299 mask\n\u25b7 Equivalent to applying the mask to M\n5:\nmasks[i] \u2190 mask\n6:\nend for\n7:\nreturn masks\n8: end function\nFollowing the official implementation by Wang et al. [46], we select the parameters as n = 3, \u03c4 = 0.15,\nwhere n denotes the expected number of objects and \u03c4 denotes the thresholding value for the affinity\nmatrix M \u2113, i.e. entries smaller than 0.15 will be set to 0. In Table 1, we remove the post-processing\nCRF step in MaskCut when comparing different model variants.\n17\nC. Experimental Setup and Additional Results\nIn this section, we provide the experimental setups for experiments presented in Section 3 and\nSection 4, as well as additional experimental results. Specifically, we provide the detailed experimen-\ntal setup for training and evaluation on Appendix C.1. We then present additional experimental\nresults on the transfer learning performance of crate when pre-trained on ImageNet-21k [10] in Ap-\npendix C.2. In Appendix C.3, we provide additional visualizations on the emergence of segmentation\nmasks in crate.\nC.1. Setups\nModel setup We utilize the crate model as described by Yu et al. [51] at scales -S/8 and -B/8.\nIn a similar manner, we adopt the ViT model from Dosovitskiy et al. [11] using the same scales\n(-S/8 and -B/8), ensuring consistent configurations between them. One can see the details of crate\ntransformer in Appendix A.\nTraining setup All visual models are trained for classification tasks(see Section 2.2) on the complete\nImageNet dataset [10], commonly referred to as ImageNet-21k. This dataset comprises 14,197,122\nimages distributed across 21,841 classes. For training, each RGB image was resized to dimen-\nsions 3 \u00d7 224 \u00d7 224, normalized using means of (0.485, 0.456, 0.406) and standard deviations of\n(0.229, 0.224, 0.225), and then subjected to center cropping and random flipping. We set the mini-\nbatch size as 4,096 and apply the Lion optimizer [8] with learning rate 9.6 \u00d7 10\u22125 and weight decay\n0.05. All the models, including crates and ViTs are pre-trained with 90 epochs on ImageNet-21K.\nEvaluation setup We evaluate the coarse segmentation, as detailed in Section Section 3.2, using\nattention maps on the PASCAL VOC 2012 validation set [13] comprising 1,449 RGB images. Addition-\nally, we implement the MaskCut [46] pipeline, as described in Section 3.2, on the COCO val2017 [27],\nwhich consists of 5,000 RGB images, and assess our models\u2019 performance for both object detection\nand instance segmentation tasks. All evaluation procedures are unsupervised, and we do not update the\nmodel weights during this process.\nC.2. Transfer Learning Evaluation\nWe evaluate transfer learning performance of crate by fine-tuning models that are pre-trained\non ImageNet-21k for the following downstream vision classification tasks: ImageNet-1k [10], CI-\nFAR10/CIFAR100 [25], Oxford Flowers-102 [33], Oxford-IIIT-Pets [37]. We also finetune on two\npre-trained ViT models (-T/8 and -B/8) for reference. Specifically, we use the AdamW optimizer [29]\nand configure the learning rate to 5 \u00d7 10\u22125, weight decay as 0.01. Due to memory constraints, we set\nthe batch size to be 128 for all experiments conducted for the base models and set it to be 256 for the\nother smaller models. We report our results in Table 3.\nDatasets\ncrate-T\ncrate-S\ncrate-B\nViT-T\nViT-B\n# parameters\n5.74M\n14.12M\n38.83M\n10.36M\n102.61M\nImageNet-1K\n62.7\n74.2\n79.5\n71.8\n85.8\nCIFAR10\n94.1\n97.2\n98.1\n97.2\n98.9\nCIFAR100\n76.7\n84.1\n87.9\n84.4\n90.1\nOxford Flowers-102\n82.2\n92.2\n96.7\n92.1\n99.5\nOxford-IIIT-Pets\n77.0\n86.4\n90.7\n86.2\n91.8\nTable 3: Top 1 accuracy of crate on various datasets with different model scales when pre-trained on ImageNet-\n21K and fine-tuned on the given dataset.\n18\nC.3. Additional Visualizations\nSupervised CRATE\nDINO\nFigure 8: Additional visualizations of the attention map of crate-S/8 and comparison with DINO [6]. Top\n2 rows: visualizations of attention maps from supervised crate-S/8. Bottom 2 rows: visualizations of attention\nmaps borrowed from DINO\u2019s paper. The figure shows that supervised crate has at least comparable attention\nmaps with DINO. Precise methodology is discussed in Appendix B.1.\nSupervised CRATE\nSupervised ViT\nShallow\nDeep\nFigure 9: Additional layer-wise PCA visualization. Top 2 rows: visualizations of the PCA of the features\nfrom supervised crate-B/8. Bottom 2 rows: visualizations of the PCA of the features from supervised ViT-B/8.\nThe figure shows that supervised crate shows a better feature space structure with an explicitly-segmented\nforeground object and less noisy background information. The input image is shown in Figure 1\u2019s top left corner.\nPrecise methodology is discussed in Appendix B.2.\n19\n2\n3\n5\n9\n20\n50\n90\nEpocs\n0.0\n0.2\n0.4\n0.6\n0.8\nAP Score on Detection\nCRATE-S/8\nRandom\nConverged\ninit\nepo. 2\nepo. 5\nepo. 20\nepo. 50\nFigure 10: Effect of training epochs in supervised crate. (Left) Detection performance computed at each epoch\nvia MaskCut pipeline on COCO val2017 (Higher AP score means better detection performance). (Right) We\nvisualize the PCA of the features at the penultimate layer computed at each epoch. As training epochs increase,\nforeground objects can be explicitly segmented and separated into different parts with semantic meanings.\n50%, N(0, 10)\n50%, N(0, 25)\n50%, N(0, 50)\n50%, N(0, 75)\nFigure 11: Adding Gaussian noise with different standard deviation. We add Gaussian noise to the input\nimage on a randomly chosen set of 50% of the pixels, with different standard deviations, and visualize all 6\nheads in layer 10 of crate-S/8. The values of each entry in each color of the image are in the range (0, 255).\nThe right 2 columns, which contain edge information, remain unchanged with different scales of Gaussian noise.\nThe middle column shows that texture-level information will be lost as the input becomes noisier.\n20\n10%, N(0, 75)\n25%, N(0, 75)\n50%, N(0, 75)\n75%, N(0, 75)\nFigure 12: Adding Gaussian noise to a different percentage of the pixels. We add Gaussian noise with\nstandard deviation 75 to a randomly chosen set of pixels within the input image, taking a different number\nof pixels in each experiment. We visualize all 6 heads in layer 10 of crate-S/8. The values of each entry in\neach channel of the image are in the range (0, 255). In addition to the observation in Figure 11, we find that\ncrate shifts its focus as the percentage of noisy pixels increases. For example, in the middle column, the head\nfirst focuses on the texture of the door. Then, it starts to refocus on the edges. Interestingly, the tree pops up in\nnoisier cases\u2019 attention maps.\n21\nCRATE \nCRATE-sth\nCRATE-MLP \nCRATE-MHSA \nFigure 13: Attention map of crate\u2019s variants in second-to-last layer. In addition to the quantitative results\ndiscussed in Section 4, we provide visualization results for the architectural ablation study. crate-MLP and\ncrate-MHSA have been discussed in Section 4 while crate-sth maintains both MSSA and ISTA blocks, and\ninstead switches the activation function in the ISTA block from ReLU to soft thresholding, in accordance with\nan alternative formulation of the ISTA block which does not impose a non-negativity constraint in the LASSO\n(see Section 2.1 for more details). Attention maps with clear segmentations emerge in all architectures with the\nMSSA block.\nCRATE \nViT\nViT\nCRATE \nFigure 14: More attention maps of supervised crate and ViT on images from COCO val2017. We select the\nsecond-to-last layer attention maps for crate and the last layer for ViT.\n22\n"
  },
  {
    "title": "Any-Size-Diffusion: Toward Efficient Text-Driven Synthesis for Any-Size HD Images",
    "link": "https://arxiv.org/pdf/2308.16582.pdf",
    "upvote": "10",
    "text": "Any-Size-Diffusion: Toward Efficient Text-Driven\nSynthesis for Any-Size HD Images\nQingping Zheng1, 2 *, Yuanfan Guo2 *, Jiankang Deng3,\nJianhua Han2, Ying Li1 \u2020, Songcen Xu2, Hang Xu2 \u2020\n1Northwestern Polytechnical University\n2Huawei Noah\u2019s Ark Lab\n3Huawei UKRD\nAbstract\nStable diffusion, a generative model used in text-to-image\nsynthesis, frequently encounters resolution-induced compo-\nsition problems when generating images of varying sizes.\nThis issue primarily stems from the model being trained on\npairs of single-scale images and their corresponding text de-\nscriptions. Moreover, direct training on images of unlimited\nsizes is unfeasible, as it would require an immense number\nof text-image pairs and entail substantial computational ex-\npenses. To overcome these challenges, we propose a two-\nstage pipeline named Any-Size-Diffusion (ASD), designed\nto efficiently generate well-composed images of any size,\nwhile minimizing the need for high-memory GPU resources.\nSpecifically, the initial stage, dubbed Any Ratio Adaptability\nDiffusion (ARAD), leverages a selected set of images with\na restricted range of ratios to optimize the text-conditional\ndiffusion model, thereby improving its ability to adjust com-\nposition to accommodate diverse image sizes. To support the\ncreation of images at any desired size, we further introduce a\ntechnique called Fast Seamless Tiled Diffusion (FSTD) at the\nsubsequent stage. This method allows for the rapid enlarge-\nment of the ASD output to any high-resolution size, avoid-\ning seaming artifacts or memory overloads. Experimental\nresults on the LAION-COCO and MM-CelebA-HQ bench-\nmarks demonstrate that ASD can produce well-structured im-\nages of arbitrary sizes, cutting down the inference time by 2\u00d7\ncompared to the traditional tiled algorithm.\nIntroduction\nIn text-to-image synthesis, Stable Diffusion (SD) (Rombach\net al. 2022) has emerged as a significant advancement. Ex-\nisting SD models (Ruiz et al. 2023; Meng et al. 2023) trans-\nform text aligned with image components into high-quality\nimages, typically sized at 512 \u00d7 512 pixels. Despite these\nmodels having the ability to handle varying sizes, they no-\nticeably struggle with resolution changes, resulting in poor\ncomposition (e.g., improper cropping and unnatural appear-\nance), a problem demonstrated in Figure 1(a). The root of\nthis issue lies in the models trained mainly on pairs of text\nand images of a uniform size, overlooking the complexities\nof handling images at multiple resolutions. Consequently,\nthis leads to observed deficiencies in image composition.\nIn pursuit of generating well-structured images at arbi-\ntrary aspect ratios, guided by textual descriptions, the Mul-\n* Equal Contribution\n\u2020Contact\n(a) \n(b) \n(c) Ours ASD\n900 x 1024\n1024 x 512\n1024 x 1024\nA cute teddy bear in front of a plain white wall.  The teddy bear has a warm, \nbrown fur that looks soft and fluffy, sitting on the brown wooden tabletop.\nFigure 1: Resolution-induced Poor Composition. Given\nthe text, (a) SD2.1 and (b) MD2.1, a MultiDiffusion model,\nraise poor composition issues in red boxes when synthesiz-\ning images of varying sizes, as opposed to (c) our ASD.\ntiDiffusion methodology (Bar-Tal et al. 2023) leverages a\npretrained text-conditional diffusion (e.g., stable diffusion),\nas a reference model and controls image synthesis through\nthe utilization of several reference diffusion processes. Re-\nmarkably, the entire process is realized without requiring\nfurther training or fine-tuning. While efficient, it does not\ncompletely resolve the limitations associated with handling\nthe reference model\u2019s multi-resolution images. As a result,\nthe production of images may exhibit suboptimal composi-\ntional quality. The underlying reason is also tied to the ref-\nerence model\u2019s training on images constrained to a single-\nscale size, as illustrated in Figure 1(b).\nA direct and appealing solution to the problem is to\ntrain the SD model to cope with every possible image size.\nYet, this approach encounters an immediate and signifi-\ncant barrier: the infinite diversity of image ratios, which\nmakes it practically unfeasible. Furthermore, it\u2019s challeng-\narXiv:2308.16582v2  [cs.CV]  11 Sep 2023\ning to gather an extensive collection of high-resolution im-\nages and corresponding text pairs. Even with a plethora of\nhigh-quality datasets available, the intrinsic pixel-based na-\nture of SD requires substantial computational resources, par-\nticularly when dealing with high-resolution images of var-\nious sizes. The problem is further aggravated when con-\nsidering the use of megapixel images for SD training, as\nthis involves extensive repeated function equations and gra-\ndient computations in the high-dimensional space of RGB\nimages (Ho, Jain, and Abbeel 2020). Even when a trained\nmodel is ready, the inference step is also time-consuming\nand memory-intensive. Through empirical observation, we\nhave found that attempts to generate 4K HD images using\nthe SD model trigger out-of-memory errors when executed\non a GPU with a 32GB capacity.\nThe key insight of this paper is to introduce a pioneering\nAny-Size-Diffusion (ASD) model, executed in two stages,\nwhich has the capability to synthesize high-resolution im-\nages of arbitrary sizes from text prompts. In its dual-\nphase approach, our ASD not only efficiently handles the\nresolution-induced poor composition but also successfully\ncircumvents out-of-memory challenges. At the outset, we\nare faced with the complexity of accommodating all con-\nceivable image sizes, a challenge that might seem in-\ntractable. To address this, in the first stage, we introduce\na multi-aspect ratio training strategy that operates within a\nwell-defined, manageable range of ratios. This strategy is\nused to optimize our proposed Any Ratio Adaptability Diffu-\nsion (ARAD) model. As a result, it enables the production of\nwell-composed images that are adaptable to any size within\na specified range, while also ensuring a reduced consump-\ntion of GPU memory. In order to yield images that can fit any\nsize, in the second stage, we propose an additional method\ncalled Fast Seamless Tiled Diffusion (FSTD) to magnify the\nimage output originating from the preceding ARAD. Con-\ntrary to the existing tiled diffusion methods ( \u00b4Alvaro Bar-\nbero Jim\u00b4enez 2023), which address the seaming issue but\ncompromise on the speed of inference, our proposed FSTD\ndesigns an implicit overlap within the tiled sampling dif-\nfusion process. This innovation manages to boost inference\nspeed without the typical seaming problems, achieving the\nhigh-fidelity image magnification. To sum up, the contribu-\ntions of this paper are as follows:\n\u2022 We are the first to develop the Any-Size-Diffusion (ASD)\nmodel, a two-phase pipeline that synthesizes high-\nresolution images of any size from text, addressing both\ncomposition and memory challenges.\n\u2022 We introduce a multi-aspect ratio training strategy, im-\nplemented within a defined range of ratios, to optimize\nARAD, allowing it to generate well-composed images\nadaptable to any size within a specified range.\n\u2022 We propose an implicit overlap in FSTD to enlarge im-\nages to arbitrary sizes, effectively mitigating the seaming\nproblem and simultaneously accelerating the inference\ntime by 2\u00d7 compared to the traditional tiled algorithm.\nRelated Work\nStable Diffusion.\nBuilding upon the foundations laid by\nthe Latent Diffusion Model (LDM) (Rombach et al. 2022),\ndiffusion models (Ho, Jain, and Abbeel 2020; Song et al.\n2021) have achieved substantial success across various\ndomains, including text-to-image generation (Nichol et al.\n2022; Ramesh et al. 2022; Saharia et al. 2022), image-to-\nimage translation (Dhariwal and Nichol 2021; Nichol and\nDhariwal 2021), and multi-modal generation (Ruan et al.\n2023). Owing to their robust ability to capture complex\ndistributions and create diverse, high-quality samples,\ndiffusion models excel over other generative methods\n(Goodfellow et al. 2014). In the field, Stable Diffusion (SD)\n(Rombach et al. 2022) has emerged as a leading model for\ngenerating photo-realistic images from text. While adept at\nproducing naturalistic images at certain dimensions (e.g.,\n512 \u00d7 512), it often yields unnatural outputs with sizes\nbeyond this threshold. This constraint principally originates\nfrom the fact that existing stable diffusion models are\nexclusively trained on images of a fixed size, leading to a\ndeficiency in high-quality composition on other sizes. In\nthis paper, we introduce our Any-Size-Diffusion (ASD)\nmodel, designed to generate high-fidelity images without\nsize constraints.\nDiffusion-based Image Super-Resolution.\nThe objective\nof Image Super-Resolution (SR) is infer a high-resolution\n(HR) image from a corresponding low-resolution (LR)\ncounterpart. The utilization of generative models to mag-\nnify images often omits specific assumptions about degra-\ndation, leading to challenging situations in real-world ap-\nplications. Recently, diffusion-based methods (Sahak et al.\n2023; Saharia et al. 2023; Li et al. 2023; Ma et al. 2023) have\nshown notable success in real-world SR by exploiting gener-\native priors within these models. Though effective, these ap-\nproaches introduce considerable computational complexity\nduring training, with a quadratic increase in computational\ndemands as the latent space size increases. An optimized\nmethod, known as StableSR(Wang et al. 2023), was devel-\noped to enhance performance while reducing GPU mem-\nory consumption. However, this method can become time-\ninefficient when processing images divided into numerous\noverlapping regions. In the next phase of our ASD pipeline,\nwe present a fast seamless tiled diffusion technique, aimed\nat accelerating inference time in image SR.\nMethod\nTo resolve the issue of resolution-induced poor composition\nwhen creating high-fidelity images of various sizes from any\ntext prompt, we propose a straightforward yet efficient ap-\nproach called Any Size Diffusion (ASD). This approach sim-\nplifies the process of text-to-image synthesis by breaking it\ndown into two stages (see Figure 2).\n\u2022 Stage-I, termed as Any Ratio Adaptability Diffusion\n(ARAD), trains on multiple aspect-ratio images and gen-\nerates an image conditioned on a textual description and\nnoise size, avoiding poor composition issues.\n\ud835\udcd4\n\ud835\udcd3\nStage-II: Fast Seamless Tiled Diffusion\noffset\nNon\u2212overlap tiles\nupscale\n(c) Implicit-Overlap in Tiled Sampling\n\ud835\udc34ny\u2212\ud835\udc46ize Image\nT-steps\n~\ud835\udca9(0,1)\nInput-texts\nGandalf from\nthe lord of rings\nT-steps\nImage of varying size\n\ud835\udcd3\nT-steps\n\ud835\udcd4\nStage-I: Any Ratio Adaptability Diffusion\n(a) Multi-aspect Ratio Training\n(b) Inference\nA set of ratios\nresize\n\ud835\udc5f1\n\ud835\udc5f\ud835\udc5b\n\u22ee\ud835\udc5f\ud835\udc5a\n\u22ee\u22ee\nmatch\n\u2112\nGenerated images\nGT\n\ud835\udcd3\nInput text & size\nEmbedding\nText-condition\nText-condition\nt -1\nt \ncopy from step t to t-1\nLoRA Trainer\nFrozen\nW x H\nFigure 2: The Any-Size-Diffusion (ASD) pipeline, including: 1) Stage-I, Any Ratio Adaptability Diffusion, translates text into\nimages, adapting to various aspect ratios, and 2) is responsible for transforming low-resolution images from the Stage-I into\nhigh-resolution versions of any specified size. For procedure (c), the implicit overlap in tiled sampling, only the solid green line\nregion is sent to the UNetModel for current denoising. At Stage-II, the dashed green arrow represents regions that are directly\ncopied from the preceding denoised latents, potentially enhancing efficiency and consistency within the overall process.\n\u2022 Stage-II, known as Fast Seamless Tiled Diffusion\n(FSTD), magnifies the image from Stage-I to a predeter-\nmined larger size, ultimately producing a high-resolution\nsynthesized image, adjustable to any specified size.\nPipeline\nAs depicted in Figure 2, ARAD is implemented based on the\ntext-conditioned latent diffusion model (LDM) (Rombach\net al. 2022) for arbitrary ratio image synthesis. During the in-\nference process, ARAD receives a user-defined text prompt\nand noise size (e.g., \u201cGandalf from the lord for the rings\u201d).\nInitially, the pre-trained text encoder (Cherti et al. 2023) is\nadapted to process this prompt, subsequently generating a\ncontextual representation referred to as a textual embedding\n\u03c4\u03b8(y). Then, a random noise of the base resolution size, de-\nnoted as \u03f5, is initialized. The noisy input conditioned on\nthe textual embedding p(\u03f5|y) is progressively denoised by\nthe UNetModel (Cherti et al. 2023). This process is iterated\nthrough T times, leveraging the DDPM algorithm (Song,\nMeng, and Ermon 2020) to continuously remove noises and\nrestore the latent representation z. Ultimately, a decoder D\nis employed to convert the denoised latent back into an im-\nage I \u2208 RH\u00d7W \u00d73. Here, H and W represent the height and\nwidth of the image, respectively.\nSubsequently, the FSTD model accepts the image gen-\nerated in the previous step as input and performs infer-\nence based on the image-conditional diffusion (Wang et al.\n2023). In detail, the image is magnified by a specified size.\nA pretrained visual encoder E is employed to map the re-\nsulting image I\u2032 \u2208 RH\u2032\u00d7W \u2032\u00d73 into a latent representation\nz = E(I\u2032). A normal distribution-based noise \u03f5 \u223c N(0, 1)\nis then added to it, yielding the noisy latent variable z\u2032 =\nA(z). The image, conditioned on itself p(z\u2032|z), undergoes\nprogressive iterations by the UNetModel, utilizing our pro-\nposed tiled sampling I \u2208 RH\u00d7W \u00d73 for T cycles. Lastly, the\ndecoder D is employed to project the denoised latent vari-\nable into the final output, effectively transforming the latent\nspace back into the image domain.\nAny Ratio Adaptability Diffusion (ARAD)\nIn this stage, ARAD is proposed to make the model have\nthe capability of generating an image, adjustable to varying\naspect ratios, resolving the issue of resolution-induced poor\ncomposition. This stage is mainly achieved by our designed\nmulti-aspect ratio training strategy.\nMulti-aspect ratio training. Instead of directly training on\nthe original image and text pairs, we employ our aspect-\nratio strategy to map the original image into an image with\na specific ratio. To be more precise, we define a set of\nratios {r1, r2, ..., rn}, each corresponding to specific sizes\n{s1, s2, ..., sn}, where n represents the number of prede-\nfined aspect ratios. For each training image x \u2208 RH\u00d7W \u00d73,\nwe calculate the image ratio as r = H/W. This ratio r is\n(\ud835\udc99, \ud835\udc9a)\n(a) w/o overlap\nt-1\nt =T\nseam | fast\n\u22ef\nt-1\nt =T\nseamless | slow\n\u22ef\n(c) Ours implicitly overlap\n(b) explicitly overlap\nt-1\nt =T\n\u22ef\n\ud835\udc67\ud835\udc50\nseamless | fast\n(\ud835\udc99 + \u2206\ud835\udc99, \ud835\udc9a + \u2206\ud835\udc9a)\n\ud835\udc67\ud835\udc50\n\ud835\udc67\ud835\udc60\nFigure 3: Comparison of various tiling strategies: (a)\nwithout overlapping, (b) with explicit overlapping, and (c)\nwith implicit overlapping. Green tiles are explicit overlaps,\nand the orange tile is our implicit overlap at step t-1.\nthen compared with each predefined ratio, selecting the one\nm with the smallest distance as the reference ratio. The in-\ndex m is determined by\narg min\nm f(m) = {|r1\u2212r|, \u00b7 \u00b7 \u00b7 , |rm\u2212r|, \u00b7 \u00b7 \u00b7 , |rn\u2212r|}, (1)\nwhere f(m) represents the smallest distance between the\ncurrent ratio and the predefined ratio. Therefore, if the image\nhas a ratio similar to the mth predefined size sm, the original\nsize of the training image is resized to sm.\nForward ARAD process. During the training process, a\npair consisting of an image and its corresponding text (x, y)\nis processed, where x represents an image in the RGB space\nRH\u00d7W \u00d73, and y denotes the associated text. A fixed visual\nencoder, E, is used to transform the resized image sm into\na spatial latent code z. Meanwhile, the corresponding text is\nconverted into a textual representation \u03c4\u03b8(y) via OpenCLIP\n(Cherti et al. 2023). For the total steps T, conditional dis-\ntributions of the form p(zt|y), t = 1 \u00b7 \u00b7 \u00b7 T, can be modeled\nusing a denoising autoencoder \u03f5\u03b8(zt, t, y). Consequently, the\nproposed ARAD can be learned using an objective function\nLARAD = EE(x),y,\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5\u2212\u03f5\u03b8(zt, t, \u03c4\u03b8(y))\u22252\n2\ni\n. (2)\nFast Seamless Tiled Diffusion (FSTD)\nIn the second stage, we propose FSTD, a training-free ap-\nproach built on StableSR (Wang et al. 2023) that ampli-\nfies the ARAD-generated image to any preferred size. To\nachieve efficient image super-resolution without heavy com-\nputational demands during inference, we devise an implicit\noverlap technique within the tiled sampling method.\nTiled sampling. For clarity, consider an upscaled image I \u2208\nRH\u2032\u00d7W \u2032\u00d73, partitioned into M petite tiles, symbolized as\n{Pi \u2208 Rh\u00d7w\u00d73 | 1 \u2264 i \u2264 M}, where w and h denote the\nwidth and height of each tile. We initially encode each tile\nPi using an encoder function E, adding the random noise,\nto generate a set of noisy latent representations Z = {Zi =\nE(Pi) + \u03f5i | \u03f5i \u223c N(0, 1), 1 \u2264 i \u2264 M}. Subsequently,\neach noisy tile is processed by the UNetModel conditioned\non the original tile for T steps, resulting in a set of denoised\nlatents Z\u2032 = {Z\u2032i| \u03f5i \u223c N(0, 1), 1 \u2264 i \u2264 M}. Finally,\na decoder fD is applied to convert them back into image\nspace, culminating in the reconstructed image\nI\u2032 = {P \u2032\ni \u2208 Rh\u00d7w\u00d73 | P \u2032\ni = fD(Z\u2032\ni), 1 \u2264 i \u2264 M}.\n(3)\nHerein, P \u2032\ni represents the ith tile decoded from its corre-\nsponding denoised latent tile.\nHowever, a seaming problem emerges when any two tiles\nin the set are disjoint, as depicted in Figure 3(a). To tackle\nthis, we implement overlaps between neighboring tiles that\nshare common pixels (Figure 3(b)). While increasing ex-\nplicit overlap can effectively mitigate this issue, it substan-\ntially escalates the denoising time. As a consequence, the in-\nference time quadratically increases with the growth in over-\nlapping patches. Indeed, it\u2019s practically significant to strike\na balance between inference time and the amount of overlap.\nImplicit overlap in tiled sampling. To speed up the in-\nference time while avoiding the seaming problem, we pro-\npose an implicit overlap in tiled sampling. As depicted in\nFigure 3(c), the magnified image is divided into L non-\noverlapping tiles and we keep the quantity of disjoint noisy\nlatent variables constant during the reverse sampling pro-\ncess. Prior to each sampling step, we apply a random offset\nto each tile, effectively splitting Z into two components: Zs\n(the shifted region with tiling) and Zc (the constant region\nwithout tiling). This can be mathematically represented as\nZ = Zs\u222aZc. Take note that at the initial time step, Zc = \u00d8.\nAt each sampling, the shifted part, Zs, is a collection of L\ndisjoint tiles, denoted as Zs = {Zs\ni | 1 \u2264 i \u2264 L}. Here,\neach Zs\ni symbolizes a shifted tile. The shifted portion, Zs,\ncomprises L disjoint tiles that change dynamically through-\nout the sampling process. Within this segment, each tile is\nexpressed as Zs\ni,x,y = Zyi+\u2206yi,xi+\u2206xi for 1 \u2264 i \u2264 L.\nHere, \u2206xi and \u2206yi denote the random offsets for tile Zs\ni\nimplemented in the preceding step. As for the constant sec-\ntion without tiling, denoted as Zc, the pixel value is sourced\nfrom the corresponding latent variable in the previous sam-\npling step. It is worth noting that after each time step, Zc\nis non-empty, symbolically represented as Zc \u0338= \u00d8. This\napproach ensures implicit overlap during tiled sampling, ef-\nfectively solving the seaming issue.\nExperiments\nExperimental Settings\nDatasets. The ARAD of our ASD is trained on a sub-\nset of LAION-Aesthetic (Schuhmann 2022) with 90k text-\nimage pairs in different aspect ratios. It is evaluated on MA-\nLAION-COCO with 21,000 images across 21 ratios (se-\nlecting from LAION-COCO (Schuhmann et al. 2022)), and\nMA-COCO built from MS-COCO (Lin et al. 2014) con-\ntaining 2,100 images for those ratios. A test split of MM-\nCelebA-HQ (Xia et al. 2021), consisting of 2,824 face im-\nage pairs in both low and high resolutions, is employed to\nevaluate our FSTD and whole pipeline.\n(a) SR-Plus\n(d) AR-Plus\nA white, grand and elegant victorian mansion with intricate details in a sunny day. [4096 x 1024]\n(c) SR-Tile-Plus\n(f) ASD (Ours)\n(b) SR-Tile\n(e) AR-Tile\n(a) SR-Plus\n(b) SR-Tile\n(c) SR-Tile-Plus\n(d) AR-Plus\n(e) AR-Tile\n(f) ASD (Ours) \nA panoramic dream castle under the blue sky at the Disney.                         [1024 x 2048]\nFigure 4: Qualitative comparison of our proposed ASD method with other baselines, including (a) SR-Plus, (b) SR-Tile,\n(c) SR-Tile-Plus, (d) AR-Plus, (e) AR-Tile and (f) our proposed ASD. The yellow box indicates the resolution-induced poor\ncomposition. The orange box indicates better composition. The red solid line box is the zoom-in of the red dashed line box,\naiming to inspect if there are any seaming issues. Our ASD outperforms others in both composition quality and inference time.\nTable 1: Quantitative evaluation against baselines. (a) SR-Plus, (b) SR-Tile, (c) SR-Tile-Plus, (d) AR-Plus, (e) AR-Tile\nand (f) our ASD. \u2018S\u2019 and \u2018A\u2019 denote single and arbitrary ratios, respectively. All tests run on a 32G GPU. Notably, under the\nsame GPU memory, our ASD achieves at least 9\u00d7 higher resolution than the original SD model.\nExp.\nStage-I\nStage-II\nCapability\nMM-CelebA-HQ\nRatio\nTile\nOverlap\nComposition\nMax Resolution\nSeam\nFID \u2193\nIS \u2191\nCLIP \u2191\n(a)\nS\n%\n%\nPoor\n20482\nN\n118.83\n2.11\n27.22\n(b)\nS\n!\n%\nPoor\n184322\nY\n111.96 (- 6.87)\n2.46 (+ 0.35)\n27.46 (+ 0.24)\n(c)\nS\n!\n!\nPoor\n184322\nN\n111.06 (- 7.77)\n2.53 (+ 0.42)\n27.55 (+ 0.33)\n(d)\nA\n%\n%\nExcellent\n20482\nN\n92.80 (- 26.03)\n3.97 (+ 1.86)\n29.15 (+ 1.93)\n(e)\nA\n!\n%\nExcellent\n184322\nY\n85.66 (- 33.17)\n3.98 (+ 1.87)\n29.17 (+ 1.95)\n(f)\nA\n!\n!\nExcellent\n184322\nN\n85.34 (- 33.49)\n4.04 (+ 1.93)\n29.23 (+ 2.01)\nImplementation Details. Our proposed method is imple-\nmented in PyTorch (Paszke et al. 2019). A multi-aspect ra-\ntio training method is leveraged to finetune ARAD (using\nLoRA (Hu et al. 2021)) for 10,000 steps with a batch size of\n8. We use Adam (Kingma and Ba 2014) as an optimizer and\nthe learning rate is set to 1.0e-4. Our FSTD (the second stage\nmodel) is training-free and is built upon StableSR (Wang\net al. 2023). During inference, DDIM sampler (Song, Meng,\nand Ermon 2020) of 50 steps is adopted in ARAD to gen-\nerate the image according to the user-defined aspect ratio.\nIn the second stage, we follow StableSR to use 200 steps\nDDPM sampler (Ho, Jain, and Abbeel 2020) for FSTD.\nEvaluation metrics. For benchmarks, we employ common\nperceptual metrics to assess the generative text-to-image\nmodels, including FID (Heusel et al. 2017), IS (Salimans\net al. 2016) and CLIP (Radford et al. 2021). IS correlates\nwith human judgment, important to evaluate the metric on\na large enough number of samples. FID captures the distur-\nbance level very well and is more consistent with the noise\nlevel than the IS. CLIP score is used to measure the cosine\nsimilarity between the text prompt and the image embed-\ndings. Besides, the extra metrics (e.g., PSNR, SSIM (Wang\net al. 2004) and LPIPS (Zhang et al. 2018)) are employed\nto assess the super-resolution ability of the second stage of\nour ASD. PSNR and SSIM scores are evaluated on the lumi-\nnance channel in the YCbCr color space. LPIPS quantifies\nthe perceptual differences between images.\nBaseline Comparisons\nBased on state-of-the-art diffusion models, we build the fol-\nlowing six baselines for comparison.\n\u2022 SR-Plus: employs SD 2.1 (Rombach et al. 2022) for the\ndirect synthesis of text-guided images with varying sizes.\n\u2022 SR-Tile: utilizes SD 2.1 for initial image generation,\nmagnified using StableSR (Wang et al. 2023) with a non-\noverlap in tiled sampling( \u00b4Alvaro Barbero Jim\u00b4enez 2023).\n\u2022 SR-Tile-Plus: A two-stage method that initiates with SD\n2.1 (Rombach et al. 2022) and refines the output using\nour proposed FSTD, facilitating the synthesis of images\nof arbitrary dimensions.\n\u2022 AR-Plus: deploys our proposed ARAD model for direct,\ntext-driven image synthesis across a spectrum of sizes.\n\u2022 AR-Tile: commences with our ARAD model for ini-\ntial image generation, followed by magnification via Sta-\nbleSR employing a non-overlap in tiled sampling.\n\u2022 ASD: is our proposed novel framework, integrating\nARAD in Stage I and FTSD in Stage II, designed to syn-\nthesize images with customizable dimensions.\nQuantitative evaluation. As reported in Table 1, our pro-\nposed ASD method consistently outperforms the baseline\nmethods. Specifically, our ASD model shows a 33.49 reduc-\ntion in FID score compared to (a) SR-Plus, and an increase\nof 1.92 and 2.01 in IS and CLIP scores, respectively. On a\n32GB GPU, SR-Plus fails to synthesize images exceeding\n20482 resolution. In contrast, our ASD effectively mitigates\nthis constraint, achieving at least 9\u00d7 higher resolution than\nSR-Plus under identical hardware conditions. Additionally,\nTable 2: Comparison of our ARAD and other diffusion-\nbased approaches. We compare their compositional ability\nto handle the synthesis of images across 21 different sizes.\nMethod\nMA-LAION-COCO\nMA-COCO\nFID \u2193\nIS \u2191\nCLIP \u2191\nFID \u2193\nIS \u2191\nCLIP \u2191\nSD2.1\n14.32\n31.25\n31.92\n42.50\n30.20\n31.63\nMD2.1\n14.57\n28.95\n32.11\n43.25\n28.92\n30.92\nARAD\n13.98\n34.03\n32.60\n40.28\n29.77\n31.87\n(a) \ud835\udc46\ud835\udc372.1\n(c) \ud835\udc34\ud835\udc45\ud835\udc34\ud835\udc37\nA Pomeranian dog sitting in front of a mini tipi tent.\nA fine style toy sport sedan, CG art.\nA mannequin in white suit.\n(b) \ud835\udc40\ud835\udc372.1\n1024 X 640\n576 X 768 512 X 1024\n1024 X 576\nFigure 5: Comparison of visual results. Composition qual-\nity of the text-to-image synthesis using (a) SD2.1, a stable\ndiffusion 2.1, (b) MD2.1, a multi-diffusion based on SD 2.1,\nand (c) our ARAD. Color boxes indicate poor composition.\nwe also have the following observations: (i) Utilizing multi-\naspect ratio training results in notable improvements across\nvarious comparisons, specifically reducing FID scores from\n118.83 to 92.80 in (a)-(d), 111.96 to 85.66 in (b)-(e), and\n111.06 to 85.34 in (c)-(f). (ii) Introducing a tiled algorithm at\nthe second stage enables the generation of images with un-\nlimited resolution, while simultaneously enhancing perfor-\nmance, e.g., FID scores improve from 92.80 to 85.66 when\ncomparing (a)-(b) and (d)-(c). (iii) Implementing overlap in\ntiled sampling effectively addresses the seaming issue, as ev-\nidenced by the comparisons between (b)-(c) and (e)-(f).\nQualitative comparison. As depicted in Fig. 4, the images\nsynthesized by ASD exhibit superior composition quality\n(e.g. proper layout) when compared to other baseline meth-\nods. Additionally, ASD can generate 4K HD images that are\nnot only well-composed but also free from seaming artifacts.\nSpecifically, when guided by a text description, the AR-Plus\nmethod is observed to generate a more complete castle than\nSR-Plus, as demonstrated in Fig.4(a) vs. Fig.4(d). Compared\nwith SR-Plus, AR-Tile can produce realistic images but is\nhindered by the presence of seaming issues (see Fig. 4(e)).\nIn contrast, Fig. 4(f) shows that our ASD successfully elim-\ninates seaming artifacts and ensures the production of well-\ncomposed images, while minimizing GPU memory usage.\nTable 3: Performance on ARAD trained on the various\ntypes of aspect ratios. \u201cAll\u201d denotes the 9 aspect ratios.\nTypes\nMA-LAION-COCO\nMA-COCO\nFID \u2193\nIS \u2191\nCLIP \u2191\nFID \u2193\nIS \u2191\nCLIP \u2191\n3\n14.36\n32.53\n32.38\n41.28\n29.58\n31.71\n5\n14.10\n33.61\n32.58\n40.25\n29.63\n31.80\nAll\n13.98\n34.03\n32.60\n40.28\n29.77\n31.87\n(a) Zoomed LR\n(b) w/o overlap\n(c) Explicit\n(d) Implicit\nFigure 6: The super-resolution results of \u00d74 for different\nmethods. We visually compare the (a) Zoomed LR (bicubic\nmethod), tiled diffusion with (b) non-overlap and (c) explicit\noverlap tiles; and (d) our FSTD which uses implicit overlap\nin tiled sampling. Notably, (d) is 2\u00d7 faster than (c).\nARAD Analysis\nTo verify the superiority of our proposed ARAD in address-\ning resolution-induced poor composition issues, we conduct\nthe ablation study, specifically at the initial stage.\nImpact of ARAD. Table 2 highlights the performance of\nARAD, showing improvements of 13.98, 34.03, and 32.60\nin FID, IS, and CLIP, respectively, on MA-LAION-COCO\nover original SD 2.1 and MultiDiffusion (Bar-Tal et al.\n2023) (MD2.1). This superiority is further illustrated in\nFig. 5. While SD2.1 and MD2.1 exhibit composition prob-\nlems, our ASD produces images that are consistent with\nuser-defined textual descriptions. For example, MD2.1 incor-\nrectly generates two overlapped blue suits from a prompt for\na white suit, a mistake not present in our ASD\u2019s results.\nInfluence on the number of aspect ratios. Table 3 reveals\nthe model\u2019s performance across various aspect ratios. The\ndata shows that increasing the number of aspect ratios in\nthe training dataset improves performance, with FID scores\nfalling from 14.36 to 13.98. A comparison between 3 and\n5 aspect ratios highlights a significant improvement, as the\nFID score drops from 14.36 to 14.10. Further increasing the\naspect ratios continues this trend, reducing the FID score\nto 13.98. This pattern emphasizes the importance of aspect\nratios in enhancing model performance.\nTable 4:\nThe versatility of tiled sampling in FSTD.\nWe conduct ablation on the MM-CelebA-HD benchmark.\n\u201cw/o\u201d, \u201cexplicit\u201d, and \u201cimplicit\u201d describe non-overlapping,\nexplicit, and implicit overlap in tile sampling respectively.\n\u201cfixed\u201d, and \u201crandom\u201d refer to different tile offset strategies.\nHere, the overlap of two adjacent tiles is 32\u00d732.\nMethod\nMM-CelebA-HQ\nTime\nOverlap & Offset\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nFID\u2193\nper frame\nw/o overlap\n26.89\n0.76\n0.09\n22.80\n75.08s\nexplicit\n27.49\n0.76\n0.09\n24.15\n166.8s\nimplicit & fixed\n26.83\n0.75\n0.08\n21.37\n75.01s\nimplicit & random\n27.53\n0.76\n0.08\n22.25\n75.19s\nFSTD Analysis\nAlthough we have proved the effectiveness of the proposed\nFSTD in Fig. 4 and Table 1, we now explore its design in the\nimage super-resolution performance on MM-CelebA-HD.\nTable 4 report the ablation study on the versatility of tiled\nsampling; see more details in the supplementary material.\nImportance of tiles with overlap. The first two lines from\nTable 4 reveal a comparison between the perceptual per-\nformance of explicit overlap and non-overlap in tiled sam-\npling. Specifically, the explicit overlap exhibits superior per-\nformance (e.g., 27.49 vs. 26.89 on PSNR). However, non-\noverlap tiled sampling offers an approximately 2\u00d7 faster in-\nference time compared to the explicit overlap. Despite this\nadvantage in speed, Fig. 6(b) clearly exposes the seaming\nproblem associated with non-overlap tiled sampling, high-\nlighting the trade-off between performance and efficiency.\nImplicit vs. explicit overlap. An analysis of the results pre-\nsented in Table 4 and Fig.6(c)-(d) confirms that the use of\nimplicit overlap in tiled sampling yields the best perfor-\nmance across both perceptual metrics and visual represen-\ntation. Further examination of the last column in Table4\ndemonstrates that the inference time for implicit overlap in\ntiled sampling is nearly equivalent to that of tiling without\noverlap. Moreover, the implementation of implicit overlap\nsuccessfully reduces the inference time from 166.8s to ap-\nproximately 75.0s. This ablation study validates the supe-\nriority of our proposed FSTD method, accentuating its ca-\npacity to achieve an optimal balance between performance\nquality and inference time.\nEffect of various offset strategies. The last two lines of Ta-\nble 4 demonstrate the advantage of using a random offset in\nimplicit overlap tiled sampling. Specifically, when compar-\ning the fixed and random offset methods in implicit overlap,\nthe random offset yields a PSNR value of 27.53, outperform-\ning the fixed offset, which registered at 26.83. The results\nfor other perceptual metrics and visual performance indica-\ntors are found to be nearly identical, further emphasizing the\npreference for a random offset in this context.\nConclusion\nIn this study, we address the challenge of resolution-induced\npoor composition in creating high-fidelity images from\nany text prompt. We propose Any Size Diffusion (ASD), a\nmethod consisting of ARAD and FSTD. Trained with multi-\naspect ratio images, ARAD generates well-composed im-\nages within specific sizes. FSTD, utilizing implicit overlap\nin tiled sampling, enlarges previous-stage output to any size,\nreducing GPU memory consumption. Our ASD is validated\nboth quantitatively and qualitatively on real-world scenes,\noffering valuable insights for future work.\nAppendix\nDetails of Our ASD Methodology\nIn this section, we present a comprehensive analysis of our\nproposed Any Size Diffusion (ASD) pipeline. We first delin-\neate the multi-aspect ratio training strategy implemented in\nStage-I. Subsequently, we provide a thorough examination\nof the implicit overlap characteristics inherent to the tiled\nsampling approach adopted in Stage II.\nStage-I: Any Ratio Adaptability Diffusion (ARAD)\nAl-\ngorithm 1 presents a detailed description of our proposed\nmulti-aspect ratio training strategy. We establish nine pre-\ndefined key-value pairs, where each key represents a dis-\ntinct aspect ratio and each value corresponds to a specific\nsize. The training images, which vary in size, are processed\naccording to this multi-aspect ratio strategy, as specified in\nlines 4-12 of Algorithm 1.\nAlgorithm 1: Multi-Aspect Ratio Training Strategy\n1 Input: Image I \u2208 RH\u00d7W \u00d73, Ratio-Size Dictionary\nDr\u2192s = {r1 : s1, r2 : s2, \u00b7 \u00b7 \u00b7 , r9 : s9} ;\n2 Output: Resized Image for Model Training ;\n3 Compute image ratio r = H\nW ;\n4 Initialize minValue = \u221e ;\n5 Initialize minIndex = 0 ;\n6 for each ratio ri in {r1, r2, \u00b7 \u00b7 \u00b7 , r9} do\n7\nCompute distance = |r \u2212 ri| ;\n8\nif distance \u2264 minValue then\n9\nminValue = distance ;\n10\nminIndex = i\n11\ni = i + 1 ;\n12 end ;\n13 Retrieve rm, sm from Dr\u2192s using minIndex ;\n14 Resize image I to sm for model training\nStage-II: Fast Seamless Tiled Diffusion (FSTD).\nAlgo-\nrithm 2 outlines the step-by-step procedure of our proposed\nFast Seamless Tiled Diffusion (FSTD) technique, designed\nto efficiently upscale a low-resolution image to a target res-\nolution of H \u00d7 W. Initially, as delineated from line 1 to\nline 7 in Algorithm 2, we partition the input low-resolution\nimage into M non-overlapping tiles, with each being rep-\nresented as a separate latent variable {L1, L2, \u00b7 \u00b7 \u00b7 , LM}.\nThese latent variables collectively form a set Z, as described\nin line 8. In particular, Z is composed of two distinct com-\nponents, as assumed in line 11: the shifted region Zs and\nthe constant region Zc. The former is designated for pro-\ncessing through tiled denoising, while the latter remains ex-\ncluded from the tiling process at the current time step. At\nAlgorithm 2: Procedures for our proposed FSTD.\nInput: an image I \u2208 RH\u00d7W \u00d73\nOutput: an image I\u2032 \u2208 RH\u00d7W \u00d73\n1\n2 \u25b6 Step 1: Tiled Sampling Preparation\n3 Divided the input image I into a set of M disjoint\ntiles: {P h\u00d7w\u00d73\ni\n| 1 \u2264 i \u2264 M} ;\n4 for each tile Pi in {P1, P2, \u00b7 \u00b7 \u00b7 , PM} do\n5\n/* add a random noise to latent */\n6\nLi = E(Pi) + \u03f5i ;\n7 end ;\n8 Hence, we have M disjoint latents\n{L1, L2, \u00b7 \u00b7 \u00b7 , LM} ;\n9\n10 \u25b6 Step 2: Implicit Overlap Tiled Sampling\n11 Suppose that Zs = {L1, L2, \u00b7 \u00b7 \u00b7 , LM} and Zc = \u00d8,\nand Z = Zs \u222a Zc, and the total step is T ;\n12\n13 Initialize Zs\nT = {L1, L2, \u00b7 \u00b7 \u00b7 , LM} ;\n14 Initialize Zc\nT = \u00d8 ;\n15 Initialize ZT = Zs\nT \u222a Zc\nT ;\n16\n17 \u22c6 Implicit Overlaps for Tiles\n18 for each time step t in {T \u2212 1, \u00b7 \u00b7 \u00b7 , 0} do\n19\nSet a random offset (\u2206x, \u2206y);\n20\nfor each tile Li in {L1, L2, \u00b7 \u00b7 \u00b7 , LM} do\n21\nLi,xi,yi = Li,xi+\u2206xi,yi+\u2206yi ;\n22\nend ;\n23\n/* Update Zs and Zc */\n24\nZs\nt = {L1,x1,y1, L2,x2,y2, \u00b7 \u00b7 \u00b7 , LM,xM,yM } and\nthe number of the tiles keeps constant ;\n25\nZc\nt = Zt+1 \\ Zs\nt ;\n26\n/* Denoise updated latents */\n27\nfor each tile Li,xi,yi in Zs\nt do\n28\nApply UNetModel to denoise (Li,xi,yi ) ;\n29\nend ;\n30\nZt = Zs\nt \u222a Zc\nt and Zc\nt \u0338= \u00d8;\n31 end ;\n32\n33 After denoising for T timesteps, Z\u2032 = Z0 ;\n34 Consequently, we have Z\u2032 and I\u2032 = D(Z\u2032).\nthe initial T th time step, as illustrated in lines 13-15, Zs\nis initialized as {L1, L2, \u00b7 \u00b7 \u00b7 , LM}, and Zc is initialized\nas an empty set. This is due to the initial offsets of each\ntile being set to zero. At the heart of the algorithm, as ex-\nhibited from lines 17 to 31, is a novel mechanism termed\n\u2018Implicit Overlap in Tiled Sampling\u2019. This mechanism is\nconceived to significantly reduce both inference time and\nGPU memory consumption compared to the non-tiled sam-\npling method used in stable diffusion processes. For each\ntime step, the algorithm randomly offsets the positions of\nthese latent variables while maintaining their quantities in-\nvariant. This results in an updated shifted region, denoted\nas Zs\nt \u2208 {L1,x1,y1, L2,x2,y2, \u00b7 \u00b7 \u00b7 , LM,xM,yM }, and a novel\nconstant region Zc\nt \u2208 Zt+1 \\ Zs\nt . Notably, from the second\ndenoising step onwards, Zc\nt becomes a non-empty set, with\neach pixel within this constant region retaining the same\nvalue as the corresponding pixel in the preceding denoised\nlatent variables. After T time steps, the iterative procedure\nobtains a new denoised latent set, denoted as Z\u2032. In the final\nstage, the algorithm decodes this latent set Z\u2032 back into an\nimage of the user-defined size H\u00d7W \u00d73, thereby producing\nthe final super-resolved output image.\nImplementation Details\nThis section elaborates on the implementation details asso-\nciated with the multi-aspect ratio training approach, and de-\nlineate configurations integral to the tiled sampling strategy.\nMulti-aspect ratio training.\nWe establish a set of nine\ndistinct aspect ratios, each associated with a corresponding\nsize specification, as enumerated in Table 5. Prior to the\ncommencement of training, images are systematically re-\nsized to a predetermined dimensionality, as prescribed by\nAlgorithm 1. Subsequently, images exhibiting identical as-\npect ratios are agglomerated into a stochastic batch, which\nis then utilized as the input for model training.\nExplicit and implicit overlaps in tiled sampling.\nIn the\ncontext of tiled sampling, we explore two distinct strategies:\nexplicit and implicit overlap configurations. Notably, in both\nstrategies, the dimensions of the input image and tiles can be\nparameterized by the user. For the explicit overlap configura-\ntion, we mathematically formulate the relationship between\nthe number of tiles and their overlaps as follows:\nNtiles = \u230a\nWimage\n(Wtile \u2212 overlap) \u00d7\nHimage\n(Htile \u2212 overlap)\u230b, (4)\nwhere Wimage and Himage represent the width and height of\nthe input image, respectively, while Wtile and Htile denote\nthe corresponding dimensions of each tile. In contrast, the\nimplicit overlap strategy conventionally configures the size\nof each tile to 512\u00d7512 with zero overlaps between adjacent\ntiles. Moreover, the spatial dimensions of latent variables are\n64\u00d764. To introduce a form of overlap, we employ a random\noffset strategy, designed to control the translational shifts of\nthese tiled latent representations, thereby achieving implicit\noverlap of tiles. Specifically on the MM-CelebA-HQ (Xia\net al. 2021) benchmark, we utilize high-resolution images\nwith dimensions of 1024 \u00d7 1024 alongside corresponding\nlow-resolution images sized 256 \u00d7 256.\nTable 5: Dictionary of nine pre-defined ratio-size pairs.\nEach ratio corresponds to a pre-defined image size.\n#\nRatio\nSize\n1\n1.0000\n512 \u00d7 512\n2\n0.7500\n576 \u00d7 768\n3\n1.3330\n768 \u00d7 576\n4\n0.5625\n576 \u00d7 1024\n5\n1.7778\n1024 \u00d7 576\n6\n0.6250\n640 \u00d7 1024\n7\n1.6000\n1024 \u00d7 640\n8\n0.5000\n512 \u00d7 1024\n9\n2.0000\n1024 \u00d7 512\nTable 6: The Effect of different random offset ranges in\nFSTD. We conduct ablation on the MM-CelebA-HD bench-\nmark by upscaling the low-resolution images of sizes 256 x\n256 to 1024x1024. The range of random offset is 16, 32 and\n48. The row in gray is the default setting of our method.\nOffset\nMM-CelebA-HQ\nTime\nrange\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nFID\u2193\nper image\n16\n27.51\n0.76\n0.09\n22.58\n75.39s\n32\n27.53\n0.76\n0.08\n22.25\n75.19s\n48\n27.52\n0.76\n0.08\n22.06\n75.21s\nMore Ablation Study\nIn this section, we present extensive ablation studies to con-\nduct an in-depth analysis of our proposed model. Our analy-\nsis is divided into two primary components: the first compo-\nnent focuses specifically on our Fast Seamless Tiled Diffu-\nsion (FSTD) technique, while the second component encom-\npasses a comprehensive evaluation of the entire pipeline,\nwhich we denote as Any Size Diffusion (ASD).\nImpact of the random offset range in FSTD.\nIn Table 6,\nwe present a detailed analysis, revealing that the range of the\nrandom offset has minimal influence on the super-resolution\nperformance of images within the MM-CelebA-HQ dataset.\nSpecifically, with offset ranges of 16, 32, and 48, the PSNR\nscores exhibit remarkable consistency, recording values of\n27.51, 27.53, and 27.52, respectively. Furthermore, the in-\nference times across these distinct offset ranges remain sim-\nilarly uniform. This observation underscores the robustness\nof our approach, as it performs consistently well under vary-\ning offset parameters, thereby demonstrating its resilience to\nchanges in this aspect of the configuration.\nTable 7: Performance comparison across various num-\nbers of explicit tiles. We conduct ablation on the MM-\nCelebA-HD benchmark by upscaling the low-resolution im-\nages in 256 x 256 to 1024x1024. The number of tiles in total\nwith respect to overlap could be calculated by Eq 4.\nMethod\nMM-CelebA-HQ\nTime\nOverlap\nNtiles\nPSNR\u2191\nSSIM\u2191\nLPIPS\u2193\nFID\u2193\nper image\nw/o\n162\n26.89\n0.76\n0.09\n22.80\n75.1s\n16\n212\n27.50 (+ 0.61)\n0.76\n0.09\n23.21\n148.7s\n32\n322\n27.49\n0.76\n0.09\n24.15\n166.8s\n48\n642\n27.64\n0.76\n0.09\n24.25\n182.6s (+ 33.9)\n10.258\n13.44\n31.692\nOOM\nOOM\nOOM\nOOM\n10.258\n17.454\n27.73\n28.514\n30.802\n31.154\n31.824\n0\n8\n16\n24\n32\n40\nSR-Plus\nOurs ASD\n5122\n10242\n20482\n40962\n81922\n1638422 184322\nCPU Memory Cost (G)\nGenerated Image Resolutions (Pixels)\nFigure 7: Comparison of SR-Plus and ours ASD in terms\nof GPU memory cost (G) vs. image resolution (pixels).\nSR-Plus is the original SD 2.1 model used to generate im-\nages of varying sizes. OOM is the abbreviation of an out-of-\nmemory error. Experiments are conducted on a 32G GPU.\nInfluence on the number of tiles or overlap region in ex-\nplicit overlap tiled sampling.\nTable 7 illustrates the trade-\noff between perceptual quality and computational efficiency\nin relation to tile overlap in image super-resolution. Notably,\nas the number of tiles increases, there is a corresponding\nimprovement in perceptual metrics, although this comes at\nthe cost of increased computational time. For instance, tiles\nwith a 16-pixel overlap exhibit superior perceptual metrics\ncompared to non-overlap tiles, yielding a notable improve-\nment of 0.63 in PSNR score. However, this enhancement\ncomes with a substantial increase in inference time, which\nincreases from 75.1s to 148.7s. Further, compared to the re-\nsults presented in the second row of the table, a tile overlap\nof 48 pixels yields a PSNR score improvement from 27.50 to\n27.64, while incurring an additional inference time of 33.9s.\nPerformance on the generation of images with different\nresolutions.\nFig. 7 highlights the comparative efficiency\nof our proposed Any Size Diffusion (ASD) method relative\nto the baseline SR-Plus model, operating under a 32G GPU\nmemory constraint. Specifically, in the context of generat-\ning images with dimensions of 10242 and 20482, our ASD\nalgorithm consistently exhibits more efficient GPU memory\nusage than the SR-Plus model. For instance, when generat-\ning images of size 10242, ASD consumes 13.44G of mem-\nory compared to SR-Plus\u2019s 17.45G; for 20482 images, the\nconsumption is 27.73G for ASD versus 31.69G for SR-Plus.\nImportantly, while the SR-Plus model is constrained to a\nmaximum resolution of 20482\u2014beyond which it exceeds\navailable GPU memory\u2014our ASD method is developed to\naccommodate image resolutions of up to 184322. This rep-\nresents a significant 9\u00d7 increase over the SR-Plus model\u2019s\nmaximum capacity.\nEmma Watson as a powerful\nmysterious sorceress, casting\nlightning magic, detailed clothing\n(a) SR-Plus\n(b) AR-Plus\nFigure 8: Visual comparison: SR-Plus (original SD 2.1) vs.\nAR-Plus (fine-tuned SD 2.1 with multi-aspect ratio train-\ning). The yellow box means poor composition.\nAchievement of 4K and 8K image-resolutions.\nFig. 8\ndemonstrates the enhanced high-resolution image genera-\ntion capabilities of our ASD method by contrasting it with\nSR-Plus, the original SD 2.1 model (Rombach et al. 2022).\nSR-Plus degrades in composition for resolutions exceed-\ning 512 \u00d7 512 pixels. In comparison, AR-Plus, developed\nthrough multi-aspect ratio training of SD 2.1, addresses this\ndegradation but is bounded to a 2048\u00d72048 pixel output un-\nder a 32GB GPU constraint. A non-overlapping tiled algo-\nrithm improves this limitation but introduces seaming arti-\nfacts, as shown in Fig. 9(a). Our solution, implementing im-\nplicit overlap in tiled sampling, resolves this seaming issue,\nwith results depicted in Fig. 9(b). Thus, our ASD method\neffectively generates high-quality 4K and 8K images.\nReferences\nBar-Tal, O.; Yariv, L.; Lipman, Y.; and Dekel, T. 2023. Mul-\ntiDiffusion: Fusing Diffusion Paths for Controlled Image\nGeneration. In ICML.\nCherti, M.; Beaumont, R.; Wightman, R.; Wortsman, M.;\nIlharco, G.; Gordon, C.; Schuhmann, C.; Schmidt, L.; and\nJitsev, J. 2023. Reproducible Scaling Laws for Contrastive\nLanguage-Image Learning. In CVPR, 2818\u20132829.\nDhariwal, P.; and Nichol, A. 2021. Diffusion Models Beat\nGans on Image Synthesis. NIPS, 34: 8780\u20138794.\nGoodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;\nEmma Watson as a powerful mysterious sorceress, casting lightning magic, detailed clothing \n(a) 4096 x 8192 \n(b) 4096 x 8192 \n(a) 2048 x 4096\n(b) 2048 x 4096\n(a) 2048 x 2048\n(b) 2048 x 2048\n(a) 2048 x 2048\n(b) 2048 x 2048\nFigure 9: Comparative visualization of tiled diffusion techniques: (a) AR-Tile without overlaps vs. (b) our ASD with implicit\noverlaps. Our ASD method enables the generation of 4K and 8K images while effectively avoiding seam issues.\nWarde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.\n2014. Generative Adversarial Nets. In NIPS, volume 27.\nHeusel, M.; Ramsauer, H.; Unterthiner, T.; Nessler, B.; and\nHochreiter, S. 2017. GANs Trained by a Two Time-Scale\nUpdate Rule Converge to a Local Nash Equilibrium. NIPS,\n30.\nHo, J.; Jain, A.; and Abbeel, P. 2020. Denoising Diffusion\nProbabilistic Models. NIPS, 33: 6840\u20136851.\nHu, E. J.; Shen, Y.; Wallis, P.; Allen-Zhu, Z.; Li, Y.; Wang,\nS.; Wang, L.; and Chen, W. 2021.\nLoRA: Low-Rank\nAdaptation of Large Language Models.\narXiv preprint\narXiv:2106.09685.\nKingma, D. P.; and Ba, J. 2014.\nAdam: A Method for\nStochastic Optimization. arXiv preprint arXiv:1412.6980.\nLi, R.; Zhou, Q.; Guo, S.; Zhang, J.; Guo, J.; Jiang, X.; Shen,\nY.; and Han, Z. 2023.\nDissecting Arbitrary-scale Super-\nresolution Capability from Pre-trained Diffusion Generative\nModels. arXiv preprint arXiv:2306.00714.\nLin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ra-\nmanan, D.; Doll\u00b4ar, P.; and Zitnick, C. L. 2014. Microsoft\nCOCO: Common Objects in Context. In ECCV, 740\u2013755.\nMa, Y.; Yang, H.; Yang, W.; Fu, J.; and Liu, J. 2023. Solv-\ning Diffusion ODEs with Optimal Boundary Conditions for\nBetter Image Super-Resolution. arXiv.\nMeng, C.; Rombach, R.; Gao, R.; Kingma, D.; Ermon, S.;\nHo, J.; and Salimans, T. 2023. On Distillation of Guided\nDiffusion Models. In CVPR, 14297\u201314306.\nNichol, A. Q.; and Dhariwal, P. 2021. Improved Denoising\nDiffusion Probabilistic models. In ICML, 8162\u20138171.\nNichol, A. Q.; Dhariwal, P.; Ramesh, A.; Shyam, P.;\nMishkin, P.; Mcgrew, B.; Sutskever, I.; and Chen, M. 2022.\nGLIDE: Towards Photorealistic Image Generation and Edit-\ning with Text-Guided Diffusion Models. In ICML, 16784\u2013\n16804.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. NIPS, 32.\nRadford, A.; Wook Kim, J.; Hallacy, C.; Ramesh, A.; Goh,\nG.; Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark,\nJ.; Krueger, G.; and Sutskever, I. 2021. Learning Transfer-\nable Visual Models From Natural Language Supervision. In\nICML, 8821\u20138831.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen,\nM. 2022. Hierarchical Text-Conditional Image Generation\nwith CLIP Latents. arXiv preprint arXiv:2204.06125.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution Image Synthesis with Latent\nDiffusion Models. In CVPR, 10684\u201310695.\nRuan, L.; Ma, Y.; Yang, H.; He, H.; Liu, B.; Fu, J.; Yuan,\nN. J.; Jin, Q.; and Guo, B. 2023. Mm-diffusion: Learning\nMulti-modal Diffusion Models for Joint Audio and Video\nGeneration. In CVPR, 10219\u201310228.\nRuiz, N.; Li, Y.; Jampani, V.; Pritch, Y.; Rubinstein, M.;\nand Aberman, K. 2023.\nDreamBooth: Fine Tuning Text-\nto-Image Diffusion Models for Subject-Driven Generation.\nIn CVPR, 22500\u201322510.\nSahak, H.; Watson, D.; Saharia, C.; and Fleet, D. 2023.\nDenoising Diffusion Probabilistic Models for Robust Im-\nage Super-Resolution in the Wild.\narXiv preprint\narXiv:2302.07864.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton,\nE. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan,\nB.; Salimans, T.; et al. 2022.\nPhotorealistic Text-to-\nImage Diffusion Models with Deep Language Understand-\ning. NIPS, 35: 36479\u201336494.\nSaharia, C.; Ho, J.; Chan, W.; Salimans, T.; Fleet, D. J.; and\nNorouzi, M. 2023. Image Super-Resolution via Iterative Re-\nfinement. TPAMI, 45(4): 4713\u20134726.\nSalimans, T.; Goodfellow, I.; Zaremba, W.; Cheung, V.; Rad-\nford, A.; and Chen, X. 2016.\nImproved Techniques for\nTraining GANs. NIPS, 29.\nSchuhmann, C. 2022. LAION-AESTHETICS. https://laion.\nai/blog/laion-aesthetics/. Accessed: 2022-8-16.\nSchuhmann, C.; K\u00a8opf, A.; Vencu, R.; Coombes, T.; and\nBeaumont, R. 2022. LAION COCO: 600M Synthetic Cap-\ntions from LAION2B-EN. https://laion.ai/blog/laion-coco/.\nAccessed: 2022-9-15.\nSong, J.; Meng, C.; and Ermon, S. 2020. Denoising Diffu-\nsion Implicit Models. In ICLR.\nSong, Y.; Sohl-Dickstein, J.; Kingma, D. P.; Kumar, A.; Er-\nmon, S.; and Poole, B. 2021. Score-Based Generative Mod-\neling through Stochastic Differential Equations. In ICLR.\nWang, J.; Yue, Z.; Zhou, S.; Chan, K. C.; and Loy, C. C.\n2023.\nExploiting Diffusion Prior for Real-World Image\nSuper-Resolution. arXiv preprint arXiv:2305.07015.\nWang, Z.; Bovik, A.; Sheikh, H.; and Simoncelli, E. 2004.\nImage Quality Assessment: from Error Visibility to Struc-\ntural Similarity. TIP, 13(4): 600\u2013612.\nXia, W.; Yang, Y.; Xue, J.-H.; and Wu, B. 2021. TediGAN:\nText-Guided Diverse Face Image Generation and Manipula-\ntion. In CVPR, 2256\u20132265.\nZhang, R.; Isola, P.; Efros, A. A.; Shechtman, E.; and Wang,\nO. 2018. The Unreasonable Effectiveness of Deep Features\nas a Perceptual Metric. In CVPR, 586\u2013595.\n\u00b4Alvaro Barbero Jim\u00b4enez. 2023.\nMixture of Diffusers for\nScene Composition and High Resolution Image Generation.\narXiv preprint arXiv:2302.02412.\n"
  },
  {
    "title": "Can Programming Languages Boost Each Other via Instruction Tuning?",
    "link": "https://arxiv.org/pdf/2308.16824.pdf",
    "upvote": "9",
    "text": "CAN PROGRAMMING LANGUAGES BOOST EACH OTHER\nVIA INSTRUCTION TUNING?\nTECHNICAL REPORT\nDaoguang Zan\u2020\u2217Ailun Yu\u00a7\u2217Bo Shen\u2021 Jiaxin Zhang\u2021 Taihong Chen\u2021 Bing Geng\u2021 Bei Chen\u00b6\nJichuan Ji\u2021 Yafen Yao\u2021 Yongji Wang\u2020 Qianxiang Wang\u2021\n\u2020Institute of Software, Chinese Academy of Science\n\u00a7Peking University\n\u2021Huawei Co., Ltd.\n\u00b6Independent Researcher\ndaoguang@iscas.ac.cn; yuailun@pku.edu.cn\nABSTRACT\nWhen human programmers have mastered a programming language, it would be easier when they learn\na new programming language. In this report, we focus on exploring whether programming languages\ncan boost each other during the instruction fine-tuning phase of code large language models. We\nconduct extensive experiments of 8 popular programming languages (Python, JavaScript, TypeScript,\nC, C++, Java, Go, HTML) on StarCoder. Results demonstrate that programming languages can\nsignificantly improve each other. For example, CODEM-Python 15B trained on Python is able to\nincrease Java by an absolute 17.95% pass@1 on HumanEval-X. More surprisingly, we found that\nCODEM-HTML 7B trained on the HTML corpus can improve Java by an absolute 15.24% pass@1.\nOur training data is released at https://github.com/NL2Code/CodeM.\nKeywords Large Language Model \u00b7 Code Generation \u00b7 Programming Language \u00b7 Instruction Tuning\n1\nIntroduction\nCode large language models (code LLMs) are blooming recently [Zan et al., 2023]. A lot of code LLMs are released\nin succession, e.g., Codex [Chen et al., 2021], AlphaCode [Li et al., 2022], PaLM-Coder [Chowdhery et al., 2022],\nCodeGen [Nijkamp et al., 2023], CodeGeeX [Zheng et al., 2023], StarCoder [Li et al., 2023], and Code Llama [Rozi\u00e8re\net al., 2023]. Owing to their amazing code generation performance, code LLMs have attracted considerable attention\nfrom both academic and industrial circles. Recent works [Ouyang et al., 2022] have witnessed the instruction tuning\ntechnique that can teach LLMs how to follow instructions. In the realm of code generation, WizardCoder [Luo et al.,\n2023] and PanGu-Coder2 [Shen et al., 2023] also adopt this technique to elicit their code generation capabilities.\nAlthough some code LLMs, such as CodeGen-Multi Nijkamp et al. [2023] and StarCoder-base Li et al. [2023], are\ntrained on corpora spanning multiple programming languages, the interplay among these languages remains unexplored.\nIn programming practice, once a human programmer has mastered a programming language, it would be easier to\nlearn a new one due to the homogeneity between programming languages. Motivated by this, we would like to explore\nwhether different programming languages can boost each other during instruction fine-tuning of code LLMs.\nTo explore this idea, we craft the training corpus for each of 8 popular programming languages (Python, JavaScript,\nTypeScript, C, C++, Java, Go, HTML), where each language includes about 9K programming exercises. We train\nStarCoder 7B using the instruction tuning technique on each programming language corpus separately, and test the\nperformance of each fine-tuned model across every programming language. Our findings reveal that programming\nlanguages can significantly boost each other. Meanwhile, we found that the improvement margin of different program-\nming languages to each other is related to the language similarity between them. For example, CODEM-JavaScript\n7B trained on JavaScript data can yield an absolute 11.80% pass@1 improvement in TypeScript. More interestingly,\n\u2217The first two authors contributed equally to this work.\narXiv:2308.16824v2  [cs.CL]  3 Sep 2023\nTECHNICAL REPORT\n# Instruction\nDesign a web page that displays a message when loaded. The message should be \"Hello, World\". Use HTML code\nto achieve this.\n# Response\n<!DOCTYPE html>\n<html>\n<head>\n<title>Hello World</title>\n</head>\n<body>\n<h1>Hello, World</h1>\n</body>\n</html>\nFigure 1: A HTML training example of our crafted instruction-answer pairs.\nCODEM-HTML 7B trained on the markup language HTML also can achieve an absolute 15.24% pass@1 improvement\nin Java.\nIn a nutshell, our contributions can be listed as follows: (1) Our findings suggest that programming languages can\nsignificantly boost each other during code LLMs\u2019 instruction fine-tuning phase. (2) We glean valuable insights on the\ncorrelation between multiple programming languages, paving the way for future research on code generation. (3) We\nwill make our training data publicly available.\n2\nMethodology\n2.1\nCrafting Training Corpus of Eight Programming Languages\nWe select 8 popular programming languages and construct their training data separately. Our selected languages include\nPython, JavaScript, TypeScript, C, C++, Java, Go, and HTML, covering diverse types such as procedure-oriented,\nobject-oriented, script, and even markup languages. For each programming language, we construct its training data\ncontaining about 9K data pairs. Each pair includes both an instruction describing the programming problem and its\ncorresponding response. One practical example of HTML is shown in Figure 1.\nBased on these selected languages, we construct a series of monolingual datasets. We start from the dataset of\nCodeAlpaca 20K2, and extract those Python-related data to form our seed instruction set. Then for each selected\nprogramming language, we evolve existent instructions in the seed instruction set to get corresponding new ones by\nprompting OpenAI\u2019s GPT-3.53. For all the selected languages except HTML, we adopt an in-depth evolution [Xu et al.,\n2023], by asking GPT-3.5 to rewrite the seed instruction (Python) into a more complicated version relevant to the target\nlanguage (Python, JavaScript, TypeScript, C, C++, Java, or Go). However, for HTML, we adopt in-breadth evolution to\nproduce a brand-new HTML-related instruction, since HTML (markup language) is too different from other languages\n(non-markup languages).\n2.2\nInstruction Tuning\nCode pre-trained models such as Codex [Chen et al., 2021] and StarCoder [Li et al., 2023] store a wealth of code\nknowledge. However, these models only support left-to-right code generation based on context, as they are trained\nsolely on plain code snippets. Of late, the instruction tuning techniques [Ouyang et al., 2022, Luo et al., 2023, Shen\net al., 2023] are proposed, which can enhance the model\u2019s capabilities of following instructions so as to enable chat\nfeatures. During instruction tuning, we train StarCoder using the prompt in Figure 2 to obtain our CODEM. We use\nDeepSpeed to accelerate the training of CODEM with fp16 enabled. Additionally, we set the batch size to 2 per GPU,\nthe learning rate to 2e-5 with a cosine annealing schedule, the gradient accumulation steps to 4, and the warmup steps\nto 30. After instruction tuning, we use the prompt in Figure 3 to do the inference on downstream tasks across various\nprogramming languages. For inference, we adopt the greedy decoding strategy for sampling. Given that CODEM is a\n2https://huggingface.co/datasets/sahil2801/CodeAlpaca-20k\n3https://platform.openai.com/docs/models/gpt-3-5\n2\nTECHNICAL REPORT\nBelow is an instruction that describes a task, paired with an input that provides further context. Write a response that\nappropriately completes the request.\n### Instruction:\n{problem}\n### Response:\n{response}\nFigure 2: Prompt format of instruction tuning. {problem} and {response} refer to the instruction and answer\nobtained in Section 2.1.\nBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nFinish the {language} code for this problem:\n{problem}\n### Response:\n{signature}\nFigure 3: Prompt format of inference. {language}, {problem}, and {signature} represent the downstream\nprogramming language, the given programming problem, and the function header, respectively.\nchat-style model, the responses it generates often contain elements beyond just codes, which typically makes them\nnon-executable. So, we extract the code snippets from the generated response to evaluate the performance of code\ngeneration.\n3\nExperiments\n3.1\nEvaluation Setup\n3.1.1\nBenchmarks and Baselines\nWe use HumanEval-X [Zheng et al., 2023] to evaluate the multilingual abilities of models in Python, JavaScript, C++,\nJava, and Go. HumanEval-X is crafted by adapting HumanEval [Chen et al., 2021] (Python) to other programming\nlanguages. Following the same approach as HumanEval-X, we also create two new versions of HumanEval: HumanEval-\nC and HumanEval-TypeScript. Note that HumanEval can not directly be adapted to markup languages such as HTML,\nso our downstream evaluation languages do not include HTML.\nThe primary baseline for all language versions of CODEM is their base model StarCoder. We analyze whether CODEM\ntrained on language A can improve language B, in which case the baselines are CODEM directly trained on language B.\n3.1.2\nMetrics\nWe adopt pass@1 as our metric to evaluate all the models. Each model generates one answer using the greedy decoding\nstrategy for each programming task, and the answer would be executed upon the given test cases. Only when all the test\ncases are passed, the programming task can be considered solved with the generated code. In this setting, pass@1 can\nbe formulated as |Pc|\n|P | , where |P| denotes the total number of programming tasks in HumanEval and |Pc| represents the\nnumber of solved tasks. In essence, the pass@1 metric we use can be considered as the accuracy.\n3.2\nResults\n3.2.1\nMain Results\nTable 1 shows the performance of CODEM, which are a series of models trained on monolingual datasets of eight\nlanguages respectively, across different language versions of HumanEval. As we can see, all CODEM models outperform\n3\nTECHNICAL REPORT\nTable 1: Pass@1 (Accuracy) of StarCoder 7B and CODEM trained on various programming languages. The numbers in\nred represent the absolute increase compared to StarCoder 7B.\nModel\nHumanEval-Multilingual\nPython\nJavaScript\nTypeScript\nC\nC++\nJava\nGo\nStarCoder 7B\n26.83\n24.39\n28.57\n24.69\n25.61\n23.17\n24.39\nCODEM-Python\n38.4111.58\n34.7610.37\n33.544.97\n29.014.32\n34.158.54\n37.2014.03\n27.443.05\nCODEM-JavaScript\n37.2010.37\n40.2415.85\n40.3711.80\n27.783.09\n32.937.32\n34.7611.59\n26.221.83\nCODEM-TypeScript\n33.546.71\n37.8013.41\n37.278.70\n30.255.56\n30.494.88\n28.054.88\n25.611.22\nCODEM-C\n39.6312.8\n37.2012.81\n32.303.73\n32.107.41\n35.379.76\n38.4115.24\n28.664.27\nCODEM-C++\n34.577.74\n35.3710.98\n32.303.73\n34.579.80\n39.0213.41\n37.2014.03\n28.053.66\nCODEM-Java\n35.378.54\n33.549.15\n32.303.73\n29.634.94\n31.105.49\n37.8014.63\n27.443.05\nCODEM-Go\n35.989.15\n33.549.15\n31.683.11\n30.255.56\n34.158.54\n35.9812.81\n32.327.93\nCODEM-HTML\n31.714.88\n33.549.15\n32.303.73\n25.931.24\n28.663.05\n38.4115.24\n28.053.66\nCODEM-Mixed\n43.2916.46\n37.2012.81\n37.899.32\n32.107.41\n37.8012.19\n39.6316.46\n29.274.88\nTable 2: Pass@1 of StarCoder 15B and CODEM-Python. The numbers in red denote the absolute improvement\ncompared to StarCoder 15B.\nModel\nHumanEval-Multilingual\nPython\nJavaScript\nTypeScript\nC\nC++\nJava\nGo\nStarCoder 15B\n32.93\n30.79\n32.29\n26.99\n31.55\n30.22\n17.61\nCODEM-Python\n64.6331.70\n47.5616.77\n39.757.46\n35.199.20\n43.8012.35\n48.1717.95\n34.7617.15\ntheir base model StarCoder 7B across all programming languages by a large margin. Also, we found that programming\nlanguages can boost each other significantly. For example, CODEM-Python trained solely on Python corpus is able\nto improve HumanEval-Java by an absolute 14.03% pass@1. This finding reveals the inherent commonalities among\ndifferent programming languages. More surprisingly, CODEM-HTML boosts HumanEval-Java by an absolute 15.24%\npass@1, even exceeding CODEM-Java. Similarly, CODEM-C++ beats CODEM-C on HumanEval-C, and CODEM-\nJavaScript beats CODEM-TypeScript on HumanEval-Typescript. Drawing upon these observations, we conjecture that\nthe improvement in multilingual code generation performance is predominantly due to instruction tuning unlocking\nthe model\u2019s inherent potential, such as natural or programming language understanding and following-instruction\ncapabilities, rather than merely incorporating new knowledge. In addition to training CODEM on a monolingual training\ncorpus, we further construct a 9K multilingual training set covering 8 programming languages. Although each language\ncomprises only a small amount (~1.2K) of training instances, experimental findings suggest that CODEM-Mixed excels\nin all languages, even surpassing CODEM-Python on HumanEval-Python and CODEM-Java on HumanEval-Java. This\nsuggests that it is possible to yield superior code generation performance by leveraging multilingual data in instruction\ntuning, without harming the generalization of the model.\nWe also conduct experiments on StarCoder 15B to verify the effectiveness of CODEM. Specifically, we obtain 108K\nPython training data following WizardCoder [Luo et al., 2023], and finetune StarCoder 15B to get CODEM-Python.\nThe results are shown in Table 2. CODEM-Python achieves state-of-the-art performance on HumanEval-Python with\n64.63% pass@1, compared with other models of the same scale. CODEM-Python also gets a tremendous improvement\nin the generation of other programming languages. For instance, it improves Java and JavaScript by an absolute 17.95%\nand 16.77% pass@1, respectively.\n3.2.2\nCloser Analysis\nWe analyze the correlation between different programming languages. As illustrated in Figure 4 (a), the improvement\nof code generation performance is sensitive to training corpus of different programming languages. Moreover, we\nfound that C and C++ can boost each other more significantly, which is the same for JavaScript and TypeScript. It is\nreasonable because these languages are correlated to each other in language design, sharing some common syntax and\ngrammar. Figure 4 (b) shows that training on each programming language can boost the code generation performance\nof all other languages. We can see that the correlation values in Figure 4 (b) are mostly all positive, implying that the\nimprovement trend of different language brought by one monolingual training corpus is relatively similar.\n4\nTECHNICAL REPORT\nPython\nJS\nTS\nC\nC++\nJava\nGo\nPython\nJS\nTS\nC\nC++\nJava\nGo\n1\n0.33\n1\n0.06\n0.87\n1\n0.27\n0.05\n-0.3\n1\n0.5\n0.1\n-0.22\n0.84\n1\n0.26\n-0.5\n-0.63\n-0.01\n0.26\n1\n0.1\n-0.62\n-0.72\n0.2\n0.28\n0.43\n1\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\n(a)\nPython\nJS\nTS\nC\nC++\nJava\nGo\nHTML\nPython\nJS\nTS\nC\nC++\nJava\nGo\nHTML\n1\n0.75\n1\n0.42\n0.87\n1\n0.94\n0.62\n0.34\n1\n0.62\n0.35\n0.18\n0.71\n1\n0.92\n0.66\n0.29\n0.89\n0.57\n1\n0.84\n0.37\n-0.05\n0.82\n0.46\n0.77\n1\n0.73\n0.63\n0.22\n0.65\n0.31\n0.88\n0.68\n1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b)\nFigure 4: Correlations between different programming languages. We regard the data in Table 1 as a matrix, and\nuse \u201cdf.corr()\u201d from the Pandas library to compute the correlation between different programming languages. The\ncorrelation results before and after \u201cdf.T\u201d are presented in (a) and (b), respectively.\n4\nRelated Work\nCodex [Chen et al., 2021] with 12-billion parameters is able to solve Python programming problems automatically.\nThis remarkable success triggered a significant buzz in both the academic and industrial realms. Followed by Codex,\na plenty of code LLMs are proposed, including AlphaCode [Li et al., 2022], PaLM-Coder [Chowdhery et al., 2022],\nCodeGen [Nijkamp et al., 2023], InCoder [Fried et al., 2023], CodeGeeX [Zheng et al., 2023], replit4, CodeT5 [Wang\net al., 2021, 2023], PyCodeGPT [Zan et al., 2022], SantaCoder [Allal et al., 2023], StarCoder [Li et al., 2023],\nCode Llama [Rozi\u00e8re et al., 2023], and phi-1 [Gunasekar et al., 2023]. These above models are trained on a large-\nscale code corpus and achieve impressive code generation performance. During their pre-training, some models are\ntrained on datasets of multilingual programming languages and then fine-tuned on a monolingual dataset to produce a\nmore powerful specialist version. As for the instruction fine-tuning phase, WizardCoder [Luo et al., 2023], PanGu-\nCoder2 [Shen et al., 2023], and Phind-CodeLlama5 are proposed to bolster the capability of following instructions and\nfurther boost the code generation capability. Yet, none of these aforementioned models explore the intricate interplay\nbetween different programming languages. In this report, we therefore would like to investigate whether training code\nLLMs on monolingual data can bolster performance in other programming languages.\n5\nConclusion\nOur findings reveal that a monolingual training corpus can enhance the multilingual code generation capabilities of\ncode LLMs via instruction tuning. This highlights the intrinsic commonality and interconnectedness among multiple\nprogramming languages. In our future work, we plan to delve into the reasons why multiple languages can enhance\neach other. Also, we will explore how to leverage our findings to elevate code generation capabilities for these obscure\nor less-used programming languages by training on data from those popular ones.\nAcknowledgements\nWe would like to thank our colleagues for their valuable feedback and insights. Special thanks to An Fu (Huawei),\nJingyang Zhao (Huawei), and Yuenan Guo (Huawei) for their constructive help throughout this research.\nReferences\nDaoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and Jian-Guang Lou.\nLarge language models meet NL2Code: A survey. In Proceedings of the 61st Annual Meeting of the Association for\n4https://huggingface.co/replit/replit-code-v1-3b\n5https://huggingface.co/Phind/Phind-CodeLlama-34B-v1\n5\nTECHNICAL REPORT\nComputational Linguistics (Volume 1: Long Papers), pages 7443\u20137464, Toronto, Canada, July 2023. Association for\nComputational Linguistics. URL https://aclanthology.org/2023.acl-long.411.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, Jared Kaplan, Harrison Edwards, Yura Burda,\nNicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish\nSastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser,\nMohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert,\nFotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, S. Arun\nBalaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford,\nMatthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam\nMcCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv,\nabs/2107.03374, 2021.\nYujia Li, David H. Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom, Eccles, James\nKeeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de, Masson d\u2019Autume, Igor\nBabuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey, Cherepanov, James Molloy,\nDaniel Jaymin Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de, Freitas, Koray Kavukcuoglu, and\nOriol Vinyals. Competition-level code generation with alphacode. Science, 378:1092 \u2013 1097, 2022.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua\nMaynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam M. Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du,\nBenton C. Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garc\u00eda, Vedant Misra,\nKevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander\nSpiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankara-\nnarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark D\u00edaz, Orhan Firat, Michele Catasta, Jason Wei, Kathleen S.\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. PaLM: Scaling language modeling with\npathways. ArXiv, abs/2204.02311, 2022.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong.\nCodeGen: An open large language model for code with multi-turn program synthesis. In The Eleventh International\nConference on Learning Representations, 2023.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shanshan Wang, Yufei Xue, Zi-Yuan Wang, Lei Shen, Andi Wang,\nYang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for code generation with multilingual\nevaluations on humaneval-x. ArXiv, abs/2303.17568, 2023.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone,\nChristopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier\nDehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade,\nArmel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo\nWang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan\nZhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim\nKunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey\nSchoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan\nDolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz\nFerrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. StarCoder: may the\nsource be with you!, 2023.\nBaptiste Rozi\u00e8re, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal\nRemez, J\u00e9r\u00e9my Rapin, Artyom Kozhevnikov, Ivan Evtimov, Joanna Bitton, Manish Bhatt, Cristian Canton Ferrer,\nAaron Grattafiori, Wenhan Xiong, Alexandre D\u00e9fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin,\nNicolas Usunier, Thomas Scialom, and Gabriel Synnaeve. Code Llama: Open foundation models for code, 2023.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens,\nAmanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to\nfollow instructions with human feedback. ArXiv, abs/2203.02155, 2022.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei\nLin, and Daxin Jiang. WizardCoder: Empowering code large language models with evol-instruct. arXiv preprint\narXiv:2306.08568, 2023.\n6\nTECHNICAL REPORT\nBo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu, Jichuan Ji, Jingyang\nZhao, Yuenan Guo, and Qianxiang Wang. PanGu-Coder2: Boosting large language models for code with ranking\nfeedback, 2023.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. WizardLM:\nEmpowering large language models to follow complex instructions, 2023.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Scott Yih, Luke\nZettlemoyer, and Mike Lewis. InCoder: A generative model for code infilling and synthesis. In The Eleventh\nInternational Conference on Learning Representations, 2023.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. CodeT5: Identifier-aware unified pre-trained encoder-decoder\nmodels for code understanding and generation. In Proceedings of the 2021 Conference on Empirical Methods in\nNatural Language Processing, pages 8696\u20138708, 2021.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi D. Q. Bui, Junnan Li, and Steven C. H. Hoi. Codet5+: Open\ncode large language models for code understanding and generation, 2023.\nDaoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen, and Jian-\nGuang Lou. CERT: Continual pre-training on sketches for library-oriented code generation. In International Joint\nConference on Artificial Intelligence, 2022.\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Mu\u00f1oz Ferrandis, Niklas\nMuennighoff, Mayank Mishra, Alexander Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson,\nYangtian Zi, J. Poirier, Hailey Schoelkopf, Sergey Mikhailovich Troshin, Dmitry Abulkhanov, Manuel Romero,\nMichael Franz Lappert, Francesco De Toni, Bernardo Garc\u2019ia del R\u2019io, Qian Liu, Shamik Bose, Urvashi Bhat-\ntacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen,\nDanish Contractor, Luisa Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Christopher Hughes, Daniel Fried,\nArjun Guha, Harm de Vries, and Leandro von Werra. SantaCoder: don\u2019t reach for the stars! ArXiv, abs/2301.03988,\n2023.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio C\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan\nJavaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin\nWang, S\u00e9bastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. Textbooks are all you\nneed, 2023.\n7\n"
  },
  {
    "title": "BioCoder: A Benchmark for Bioinformatics Code Generation with Contextual Pragmatic Knowledge",
    "link": "https://arxiv.org/pdf/2308.16458.pdf",
    "upvote": "9",
    "text": "BIOCODER: A BENCHMARK FOR BIOINFORMATICS\nCODE GENERATION WITH CONTEXTUAL PRAGMATIC\nKNOWLEDGE\nXiangru Tang\u2020\u2217, Bill Qian\u2020\u2217, Rick Gao\u2020, Jiakang Chen\u2020, Xinyun Chen\u2021, Mark Gerstein\u2020\n\u2020Yale University\n\u2021Google Deepmind\n{xiangru.tang, mark.gerstein}@yale.edu, xinyunchen@google.com\nhttps://BioCoder-benchmark.github.io/\nABSTRACT\nPre-trained large language models have significantly improved code generation. As\nthese models scale up, there is an increasing need for the output to handle more intri-\ncate tasks and to be appropriately specialized to particular domains. Here, we target\nbioinformatics due to the amount of specialized domain knowledge, algorithms,\nand data operations this discipline requires. We present BIOCODER, a benchmark\ndeveloped to evaluate large language models (LLMs) in generating bioinformatics-\nspecific code. BIOCODER spans a broad spectrum of the field and covers cross-file\ndependencies, class declarations, and global variables. It incorporates 1026 Python\nfunctions and 1243 Java methods extracted from GitHub, along with 253 examples\nfrom the Rosalind Project, all pertaining to bioinformatics. Using topic model-\ning we show that overall coverage of the included code is representative of the\nfull spectrum of bioinformatics calculations. BIOCODER incorporates a fuzz-\ntesting framework for evaluation. We have applied it to evaluate many models\nincluding InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+,\nInstructCodeT5+, GPT-3.5, and GPT-4. Furthermore, we finetuned StarCoder,\ndemonstrating how our dataset can effectively enhance the performance of LLMs\non our benchmark (by >15% in terms of Pass@K in certain prompt configurations\nand always >3%). The results highlight two key aspects of successful models:\n(1) Successful models accommodate a long prompt (> 2600 tokens) with full\ncontext, for functional dependencies. (2) They contain specific domain knowledge\nof bioinformatics, beyond just general coding knowledge. This is evident from the\nperformance gain of GPT-3.5/4 compared to the smaller models on the benchmark\n(50% vs up to 25%). Our dataset, benchmark, Docker images, and scripts required\nfor testing are all available at https://github.com/gersteinlab/biocoder.\n1\nINTRODUCTION\nLarge language models (LLMs) have shown great success in code generation (Chen et al., 2021;\nChowdhery et al., 2022; Chen et al., 2023; Barke et al., 2023; Li et al., 2023). The landscape of\nexisting coding benchmarks for large language models is largely populated with simple functions,\noften limited to a handful of lines (Chen et al., 2021; Austin et al., 2021b; Du et al., 2023; Wong\net al., 2023). Combined with a significant lack of closed-domain datasets across diverse fields,\nthis landscape highlights the need for a more robust benchmarking system. Although domain-\nspecific datasets, such as DS1000 (Lai et al., 2022) for data science, have emerged, they fall short of\nadequately addressing specific tasks in fields like bioinformatics. Open-domain alternatives, including\nHumanEval (Chen et al., 2021), MBPP (Austin et al., 2021b), and APPS (Hendrycks et al., 2021),\noffer entry-level programming tasks, but their utility is limited as they lack the ability to test more\nniche, domain-specific code blocks. This shortfall is largely due to a lack of appropriate fine-tuning\nand context (Muennighoff et al., 2023b); therefore, a more comprehensive, encompassing approach\nto benchmarking is clearly needed.\n\u2217Contributed equally.\n1\narXiv:2308.16458v4  [cs.LG]  4 Dec 2023\nCode Parsing Tools\nTools for parsing large-scale projects and\nrepos, including AST (Abstract Syntax Tree) \nparsers for code and utilities to extract code \nattributes.\nProcessed Data\nThe Pragmatic dataset encompasses functional \ncode for bioinformatics repos, comprehensive \nspecifications, and a scalable interface. It has \nundergone rigorous filtering, extensive data \ncleaning, and preprocessing to prevent models \nfrom memorizing.\nModels\nCode generation with large language \nmodels such as SantaCoder, StarCoder, \nCodeGen, in conjunction with APIs based \non OpenAI.\nTesting and Docker\nTesting incorporates a Docker environment, an \nabundance of required dependencies, and a \nmultitude of fuzz test cases. This robust setup \nnot only facilitates testing in realistic project \nscenarios, but also promotes exceptional \nscalability and transferability.\nFigure 1: Overview of our contribution in BIOCODER.\nTo bridge these gaps, we introduce BIOCODER (see Figure 1), a benchmark for code generation\nincorporating 2269 bioinformatics-specific coding problems. Our BIOCODER benchmark mainly\ntargets bioinformatics data analysis, which includes tasks such as managing various biological data\nformats, understanding processing workflows, and utilizing APIs of various packages. This area\nencapsulates the majority of daily tasks a bioinformatician encounters in data analysis. Note, however,\nthat BIOCODER also touches upon parts of writing bioinformatics software: when tool development\nintersects with data analysis (see Appendix o for more details with the topic modeling and statistics\nregarding the overall topic coverage of the dataset). Further expanding the scope of BIOCODER, we\nincluded an additional 253 questions from the Rosalind project. This project specializes in generating\nPython functions addressing key bioinformatics topics such as genetic sequencing and DNA/RNA\nanalysis. BIOCODER assures the inclusion of all potential external packages and code that could\nbe utilized by the generated program. This consideration extends to the recognition that real-world\nfunctions often necessitate managing multiple external function calls and global variable usage;\nhence, we included all potentially required class declarations in the input. Lastly, we performed\nablation studies to determine whether the models are strictly memorizing the solutions rather than\nbeing proficient at generating code (see Appendix n).\nThe key highlights of our work can be outlined as follows:\n(1) We create a new high-quality dataset for code generation, curated from 1,720 bioinformatics\nrepositories referenced in peer-reviewed bioinformatics articles. We meticulously processed the\ndata, rephrasing more detailed text descriptions, as well as associated comments and specifications,\nincluding considerations needed in coding. (2) We provide an extendable parsing tool that can extract\nall pertinent information associated with the target function in expansive projects. (3) We offer a\nlibrary for code LLMs, similar to Bui et al. (2023), furnishing a seamless interface for both training\nand inferencing in code generation tasks. (4) We provide a fuzzer testing tool capable of scaling to\nhandle substantial datasets. Our benchmark results, derived from 1000 iterations, are particularly\nreliable, indicating the Pass@K rate.\n2\nRELATED WORK\nBIOCODER is a code generation benchmark designed for challenging, practical bioinformatics\nscenarios, offering an extensible testing framework for evaluating the performance of LLMs. We\nprovide a brief overview of the related work in both code generation models and benchmarks.\n2.1\nCODE GENERATION WITH LLMS\nLLMs have truly demonstrated astounding performances across various domains (Askell et al.,\n2021; Bai et al., 2022; Biderman et al., 2023; Bommasani et al., 2022; Gao et al., 2022; Patil et al.,\n2023; Xu et al., 2023; Qin et al., 2023; Zhang et al., 2023a). And LLMs trained with code data\nhave shown promising results in generating code, exhibiting impressive zero-shot performance on\nseveral benchmarks (Zhang et al., 2023b; Olausson et al., 2023; Li et al., 2023; Fried et al., 2023;\nWang et al., 2021; Allal et al., 2023). A proven strategy to improve model performance involves\nincreasing both the model parameters and the volume of training data (Radford et al., 2019; Brown\net al., 2020; Mitchell et al., 2023), while a lot of large-scale LLMs have been developed (Chowdhery\net al., 2022; Thoppilan et al., 2022; Hoffmann et al., 2022). These models have proven their code\ngeneration prowess (Brown et al., 2020; Chen et al., 2021; OpenAI, 2023), and the field has also\nseen the release of several open-source code LLMs, such as bilingual GLM-130B (Zeng et al., 2022),\nCodeGeeX-13B (Zheng et al., 2023a), OctoPack (Muennighoff et al., 2023a), WizardCoder (Luo\n2\nBenchmark\nNum\nLanguage\nData Statistics\nScenario\nTest\nP.C.\nP.L.\nC.C.\nC.L.\nHumanEval (2021)\n164\nPython\n7.8\n450.6\n13.7\n180.9\n6.8\nCode Exercise\nMBPP (2021a)\n974\nPython\n3.1\n78.6\n1.0\n181.1\n6.7\nCode Exercise\nAPPS (2021)\n5, 000\nPython\n21.0\n1743.4\n41.6\n473.8\n21.4\nCompetitions\nDS-1000 (2022)\n1, 000\nPython\n1.6\n879.1\n31.6\n137.4\n5.0\nData Science\nHumanEval-X (2023b)\n164\u2217\nMulti.\n7.8\n468.4\n15.5\n264.6\n12.1\nMultilingual\nNumpyEval (2022b)\n101\nPython\n3.5\n222.9\n7.0\n29.9\n1.1\nPublic Library\nTorchDataEval (2022a)\n50\nPython\n1.1\n329.0\n8.6\n50.7\n1.3\nPrivate Library\nBioCoder (public set)\n460\nMulti.\n1000\n10465.6\n243.5\n706.8\n26.2\nBioinformatics\nBioCoder (hidden set)\n2,269\nMulti.\n1000\n12296.7\n298.8\n919.5\n26.2\nBioinformatics\nBioCoder (similar set)\n460\nMulti.\n1000\n9885.6\n240.8\n767.5\n26.8\nBioinformatics\nTable 1: Comparison of the statistics of BIOCODER to previous benchmarks. Num is the benchmark\nsize. Test refers to the average amount of test cases. P.C. and P.L. indicate the average number of\ncharacters and lines in each prompt respectively, and C.C. and C.L. indicate the average number of\ncharacters and lines in the original code solutions. This table is derived from Zan et al. (2023), please\nrefer to Zan et al. (2023) for a more comprehensive survey.\net al., 2023), SantaCoder (Allal et al., 2023), and StarCoder (Li et al., 2023). Salesforce\u2019s CodeGen\n(Nijkamp et al., 2023b;a), Huawei\u2019s PanguCoder (Christopoulou et al., 2022; Shen et al., 2023),\nMeta\u2019s LLaMA (Touvron et al., 2023), and CMU\u2019s InCoder model (Fried et al., 2022) also contribute\nto the field. To adopt code LLMs in real scenarios, researchers have further explored methods to\nintegrate dependencies of relevant code in the prompt (Shrivastava et al., 2023; Zhang et al., 2023a).\n1740 bioinformatics\nGithub repositories\nExtract all functions from Github\nrepositories with an parser\nRemove\nany\ninvalid\nfunctions\nFilter remaining\nfunctions on criteria\nDetermine\nfinal functions\nWrite code description\nand specification\n for each function\nWrite test cases\nfor each function\nWrite code perturbation\nProjects Sort \nDomain knowledge\ncheck\nSpecification check\nRunnable check\nLine number check\nDependency check\nCode Quality check \nHuman Selection\nData cleaning and data inspection\nBenchmark collection\nCreate a context file\nfor each problem.\nAdd all the required\ndependencies to the context file \nFuzzing Test\nEnvironment variables are\nrandomly created by the fuzzing\ntester right before runtime\nAST parser\nSort\nfunctions by\nprogramming\nlanguage\nStandard library imports \nLocal imports\nImporting specific\nmembers from a module\nClass declarations\nGlobal variables\nAdditional function\ndeclarations\nComment\nFunction definition\nIndicates manual human interaction\nGenerated\nfunctions\nProject 1\nProject 2\nProject 3\n...\nExecute\nFigure 2: A diagram of the BIOCODER construction process involving custom GitHub repository\ncleaning, parsing, and function selection, as well as context and test case creation and a massively\ndockerized testing framework.\n2.2\nCODE GENERATION DATASETS AND BENCHMARKS\nEarly work on code generation benchmarks used lexical exact match, data flow, and abstract syntax\ntree (AST) methods. However, these measures proved to be unreliable due to their sensitivity to\ninconsequential differences in the generated code. In response, execution-based evaluation approaches\nhave become more prevalent (Chen et al., 2021; Athiwaratkun et al., 2023; Li et al., 2022; Wang et al.,\n2022b; Lai et al., 2022; Khlaaf et al., 2022). These approaches execute tests on the generated code\nto verify its functional correctness, ensuring unbiased evaluations irrespective of implementation\nmethod or style variations.\nAs a result, the field of code generation has seen a burgeoning number of execution-based benchmarks\n(Table 1) (Yuan et al., 2023; Lee et al., 2023; Pan et al., 2023; Wong et al., 2023; Zan et al., 2023), each\npresenting unique properties in terms of size, language coverage (Orlanski et al., 2023), complexity\n(Du et al., 2023; Zhuo, 2023), and practical applicability (Yu et al., 2023). For instance, HumanEval\n(Chen et al., 2021) and MBPP (Austin et al., 2021b) are frequently used code generation benchmarks\nthat consist of 164 and 974 simple Python functions respectively, representing a small sample size.\nThese benchmarks also overlook the multi-language coding scenarios gap, which is partially bridged\nby benchmarks like HumanEval-X (Zheng et al., 2023b) and MCoNaLa (Wang et al., 2023b). See\nZan et al. (2023) for a more comprehensive survey on the previous benchmarks of code generation.\n3\nPublic\nHidden\nSimilar\nPY\nJAVA\nOverall\nPY\nJAVA\nOverall\nPY\nJAVA\nOverall\nAvg. Comment Lines\n4.96\n2.66\n4.40\n8.77\n4.90\n6.65\n5.75\n3.14\n5.12\nAvg. Tokens of G.T.\n189.25\n106.54\n169.28\n353.67\n107.88\n219.02\n216.62\n100.92\n188.68\nAvg. Lines of G.T.\n24.30\n11.10\n21.11\n43.28\n12.19\n26.25\n26.50\n10.32\n22.59\nAvg. Parameters of G.T.\n2.39\n1.70\n2.23\n2.92\n1.25\n2.00\n2.48\n1.10\n2.15\nAvg. Classes/Function Decl.\n20.25\n2.52\n15.97\n19.45\n32.96\n26.85\n20.20\n1.16\n15.60\nAvg. Global Variables\n1.90\n-\n-\n2.26\n-\n-\n1.87\n-\n-\nAvg. Imports\n11.91\n1.52\n9.40\n10.37\n5.00\n7.43\n11.63\n1.16\n9.10\nTable 2: Summary statistics for the BIOCODER dataset. G.T. stands for the ground truth function.\n\u201cPublic data\u201d represents datasets with test cases. \u201cHidden data\u201d encompasses a wider array of intricate\nissues. \u201cSimilar data\u201d is a subset of the hidden data, mimicing the distribution of the public data\n(Appendix z).\nHowever, all datasets discussed above share the same shortcoming of only benchmarking generic\nfunctions, rather than domain-specific ones. DS-1000 (Lai et al., 2022) represents a more domain-\nspecific dataset, featuring 1,000 data science workflows extracted from Python functions. Li et al.\n(2023) reported that the performance on HumanEval and MBPP benchmarks do not invariably align\nwith those on DS-1000 benchmark. This discrepancy underscores the need for benchmarks that more\naccurately emulate real-world, domain-specific code generation.\nIn addition, the context supplied greatly influences the performance of existing LLMs (Wang et al.,\n2022a). While DS-1000 includes eight packages, it fails to fully reflect a typical coding environment.\nThis gap is partially bridged through benchmarks such as CoderEval (Yu et al., 2023), which\nincorporate some dependencies and function calls; however, these benchmarks are rudimentary in\nnature, and once again consist primarily of domain-agnostic functions. As LLMs continue to develop,\nhowever, we are now beginning to see repository-level benchmarks that provide a high amount of\ncontext, but these remain new and untried, such as RepoBench (Liu et al., 2023).\nOur work shares common ground with CoderEval. Both our approach and CoderEval can evaluate\nmodels beyond the simple generation of standalone functions. Given the necessity to handle context-\ndependent code, both methodologies employ Docker-based testing. However, our approach contrasts\nwith that of CoderEval by placing a specific emphasis on bioinformatics. We ensure each function\ndemands a certain level of domain expertise in bioinformatics by a combination of automatic filtering,\nGPT-assisted filtering, and manual inspection (Appendix t). Moreover, our dataset surpasses the\nscale of CoderEval, which only consists of 230 functions from 43 Python projects and 230 methods\nfrom 10 Java projects. In contrast, we source 2,522 functions from over two thousand repositories,\noffering a broader and more challenging context for code generation tasks. We further compare our\nbenchmark to CoderEval in Appendix h.\n3\nTHE BIOCODER BENCHMARK\n3.1\nDATASET FILTERING\nOur dataset begins with an initial web scrape of 1,743 bioinformatics-adjacent GitHub repositories\n(see Figure 2). Specifically, we used the list of 1740 bioinformatics-adjacent repositories in Russell\net al. (2018) as the initial base for BIOCODER, which contains a curated list of 1720 bioinformatics\nrepositories from the literature. The collection includes code in languages such as C, C++, PHP,\nPython, R, Ruby, SQL, Perl, Java, Matlab, and C#, although for now, we only explore Python and\nJava, with plans to scale up to other languages in the future. Our decision to include Java and Python\nwas based on an empirical investigation into the prevalence of different programming languages\nacross bioinformatics repositories, for a more detailed discussion, please refer to Appendix q.\nThose repositories were then filtered based on popularity and community ratings, as well as a manual\nround of review, resulting in 28 high-quality, highly domain-specific repositories that are commonly\nused in the field of bioinformatics. After determining the set of 28 high-quality, highly domain-\nspecific repositories, we then wrote separate custom Python and Java parsers to automatically parse\nall the selected GitHub repositories. These parsers generated an AST of each code file in each\nrepository and then scraped all the relevant data, including function content, function signature,\nimportant imports, and cross-file dependencies for each function in each code file. After parsing\nall repositories, we were left with a large set of over 20,000 Python functions and over 50,000 Java\nfunctions. Given the large baseline of functions, we initiated two rounds of automatic filtering (see\n4\nHere are the imports: \nimport\norg.apache.commons.math3.distribution.HypergeometricDistribution; \nimport java.text.DecimalFormat; \nimport java.util.*; import java.util.function.Function; import \nhtsjdk.samtools.util.SequenceUtil; import java.util.Objects;\nHere are the class declarations: \nclass KmerOperations {\n    public static long[] rightNeighbours(long kmer, int k) {\n        long mask = (1L << (2 * k)) - 1;\n        long[] ans = new long[] {(kmer << 2) & mask,\n                ((kmer << 2) & mask) | 1,\n                ((kmer << 2) & mask) | 2,\n                ((kmer << 2) & mask) | 3};\n        for (int i = 0; i < ans.length; i++) {\n            long rc = rc(ans[i], k);\n            if (rc < ans[i]) {\n                ans[i] = rc;\n            }\n        }\n        return ans;\n    }\n}\nsummary: Reverses and complement a kmer sequence.\nparam: kmer (long) - the kmer sequence to be reversed and \ncomplemented.\nparam: k (long) - the length of the kmer sequence.\nreturn: (long) - the reversed and complemented kmer sequence.\nThe function is located in the class KmerOperations\npublic static long rc(long kmer, long k)  \nHere are the global variables:\ntrans_dict = defaultdict(lambda : 5, A=0, C=1, G=2, T=3)\ntrans_dict['-'] = 4\nHere are the class declarations:\nclass Sequence(object):\n \nattributes: \nself.label,self.sequence,self.length,self.unaligned_length,self.frequency,self.np_seque\nnce\n \nmethods:\n \ndef __init__(self, label, sequence):\n \n \nsummary: Initializes a class instance with the specified label and sequence \ninformation. \n \n \nparam: label (str) - the label of the sequence.\n \n \nparam: sequence (str) - the nucleotide sequence.\n \n \nreturn: None - the function does not return any value.\n \ndef __eq__(self, other):\n \n \nParameters: \n \n \n- self (object) - the first object to be compared\n \n \n- other (object) - the second object to be compared\n \n \nReturn:\n \n \n- (bool) - returns True if the objects are equal and False if they are not \nequal.\nsummary: Returns a string with the sequence in fasta format param: None return: str - \nThe FASTA representation of the sequence The function is located in the class Sequence\ndef to_fasta(self):\nHere are the imports:\nfrom collections import defaultdict\nimport re\nimport numpy as np\nArea where imports for \nfunction is defined\nArea where global\nvariabls for function is \ndefined (Python only)\nArea where external \nclasses for function is \ndefined\nArea where summary for \nfunction is defined\nArea where function \nsignature is defined\nFigure 3: Sample prompts for code generation. Our prompts follow the same general outline. First,\nimports are declared at the top of the prompt, then global variables (if there are any), then function\ndeclarations, then class dependencies, and finally, our actual instruction regarding the function to be\ngenerated.\nAppendix t), resulting in a final count of 1,026 Python functions and 1,243 Java functions (Table 2).\nMore details on the filtering process are in Appendix t.\n3.2\nBENCHMARK CONSTRUCTION\nBIOCODER-PY and BIOCODER-JAVA\nFor each function that passes all rounds of filtering in\nSection 3.1, we manually wrote custom code context, inclusive of necessary imports, cross-file\ndependencies, and pertinent fuzzer test cases (explained in more detail in Section 3.4). We then\ncrafted custom prompts based on the parsed function data and function summaries, ensuring the\ninclusion of any necessary imports and cross-file dependencies (see Appendix r). Imports and classes\nare predefined and included in the context because, as we are testing function-level code generation,\nwe are not prompting the model nor expecting the model to generate the classes it needs to pass the\ntests. Instead, we are testing the model\u2019s ability to extract the pertinent imports and classes from the\ncontext to use in the generated function. More prompt statistics can be found in Table 7. Finally,\nwe presented the model with a prompt to generate the function, offering the function signature as a\nstarting point. Examples of the different prompt types can be found in Appendix c. Prompts were\ngenerated partly using GPT-3.5, as GPT-3.5 was used to generate function summaries for all the\nfunctions in the public dataset. These function summaries were used as part of the prompt in order to\nefficiently describe the functions. More details about this method are in the Appendix f. An example\nof the resulting prompt is in Figure 3.\nBIOCODER-ROSALIND.\nTo compile the Rosalind portion of the benchmark, we began by scraping\nthe problem descriptions from the Rosalind website, identifying problems with available solutions,\nand gathering all possible solutions. Subsequently, we developed a custom scraper to assemble ten\ntest cases for each Rosalind problem. Using these test cases, we crafted a script to automatically\nassess whether the available solutions successfully ran against the collected test cases.\nSolutions that were successfully executed against all test cases formed the \u2018golden code\u2019 (producing\nthe correct outputs when run with the test cases) section of the Rosalind benchmark. Each Rosalind\nbenchmark context is custom-made, incorporating the scraped test cases and injecting them into the\ngenerated code. The prompts for the Rosalind problems are constructed using the scraped problem\ndescriptions, supplemented with a brief section outlining the context into which the generated code\nwould be integrated. This rigorous filtering process resulted in 253 functions meeting all our criteria.\nSelected examples for the Rosalind dataset are shown in Appendix d. Statistics of token counts,\ncomment lines per function, and parameters per function can be found in Appendix b.\n3.3\nMETRIC\nWe use the Pass@K metric to measure the functional accuracy (Chen et al., 2021; 2022; Cassano et al.,\n2023). The metric Pass@K evaluates the efficiency of a model\u2019s code generation ability. Specifically,\nfor a given model, this metric quantifies the probability that the model can solve a particular problem.\nA problem is deemed \u201csolve\u201d if, among the k-function samples produced by the model, at least one\nsample passes all the provided test cases. The mathematical estimation of Pass@K for a particular\n5\nproblem is articulated as follows: Pass@K :=\nE\nProblems\n\u0014\n1 \u2212 (\nn\u2212c\nk )\n(\nn\nk)\n\u0015\n, where n is the number of samples\ngenerated by the model, and c is the number of samples that pass all test cases (Chen et al., 2021).\n3.4\nTESTING FRAMEWORK\nOur testing framework starts with a manual review of selected functions, leading to the creation of a\ncontext file and a golden code for each problem (see Appendix r), as discussed in 3.2.\nFor Python and Java functions, in the context file, we employ a custom syntax to indicate the insertion\npoints for custom randomly generated test cases. Through this syntax, we cater to four types of\nrandom generation: integers, floats, strings, and Boolean values. During the runtime, each of these\ninsertion points is replaced with language-specific code to insert a dynamically generated test case.\nThe tester can be run for any number of iterations, depending on the number of fuzzer tests desired.\nFor Rosalind functions, the process is simpler and more efficient as the functions are less complex.\nThe golden code\u2019s output is generated and cached ahead of time. During testing, the tester executes\nthe generated code within the corresponding context, and the output of this execution is compared\nwith the cached golden code output.\nFor every fuzzer test case and Rosalind test case, we ran the golden output against itself, to ensure that\nthe golden output passes each test with one hundred percent reliability. Furthermore, to ensure system\nsecurity and test reliability, we ran our tests in Docker environments. We constructed a system using\nAmazon Web Services, coordinating tasks across multiple nodes to accelerate the process without\ncompromising the validity of the results. After creating a generalized Docker image, equipped with\nall necessary Python requirements, we summarized our testing framework in Appendix l. We also\naddressed potential concerns about testing issues due to changes in packages in Appendix y.\n4\nMODELS AND RESULTS\nTo test BIOCODER, we opted to benchmark StarCoder-15B (Li et al., 2023), StarCoder+-15B (Li\net al., 2023), InCoder (Fried et al., 2023), SantaCoder (Allal et al., 2023), CodeGen (6B-mono and\n16B-mono) (Nijkamp et al., 2023b), CodeGen2-7B (Nijkamp et al., 2023a), InstructCodeT5+ (Wang\net al., 2023a), GPT3.5-Turbo and GPT-4 (OpenAI, 2023) through Azure OpenAI Service. Full details\nof the model context lengths and model sizes can be found in Table 3.\nIn order to target specific performance characteristics, we came up with hundreds of variations of\nthe prompt. We chose three goals: test the performance of models with extraneous context, without\nextraneous context, and without any context. These goals allow us to better analyze failure reasons\nand the effectiveness of our context-driven approach. After careful experimentation, we came up with\nthe prompt type shown in Figure 3, which we call Summary at Bottom. Following the instruction\nparadigm of some of the considered models, we test a version with the summary moved to the top,\nalong with the text \"# Here is an instruction. Complete the function using the required context\".\nTo test without extraneous context, we used human annotators to manually determine the required\ncontext, and use the structure of the Summary at Top prompt. The remaining prompt explanations can\nbe found in Appendix i.\nAiming to accurately represent the performance of the LLM outputs, we implemented basic correction\nmechanisms to rectify minor syntax and style errors that did not impact functionality. For instance,\nall StarCoder outputs were appended with a post-script. Consequently, each LLM output was passed\nthrough these correction mechanisms before being sent to the testing framework for evaluation\n(see Table 4 and 5). Furthermore, to empirically evaluate the hypothesis regarding the efficacy of\nsmaller, specialized LLMs in closed-domain code generation, as opposed to large open-domain\npre-trained models like GPT-3.5 and GPT-4, we also fine-tuned StarCoder and documented the\nresulting performance. Our aim is to use StarCoder as a representative sample of currently popular\nmodels. Due to computing restraints, we are unable to fine-tune all the models but we also encourage\nthe contribution from the broader community. We ran inference on HPC clusters with 8x A100 GPUs.\nThe results in Table 4 and Table 5 align with our initial hypothesis, which proposed that larger\nmodels would likely outperform their smaller counterparts. However, the significant performance\ngap between GPT-3.5, GPT-4, and all other code-generation models was surprising. This stark\ncontrast underscores the crucial role of both the dataset size and parameter size of the base models in\n6\nModel\nContext limit\n# Parameters\nInCoder (Fried et al., 2023)\n2048\n6B\nSantaCoder (Allal et al., 2023)\n2048\n1.1B\nStarCoder (Li et al., 2023)\n8192\n15.5B\nStarCoderPlus (Li et al., 2023)\n8192\n15.5B\nInstructCodeT5+ (Wang et al., 2023a)\n2048\n16B\nCodeGen-6B (Nijkamp et al., 2023b)\n2048\n6B\nCodeGen-16B (Nijkamp et al., 2023b)\n2048\n16B\nCodeGen2 (Nijkamp et al., 2023a)\n2048\n7B*\nGPT-3.5-Turbo\n8192\nUnknown\nGPT-4\n8192\nUnknown\nTable 3: Context length limits and sizes of different code LLMs.\naccomplishing closed-domain code generation prompts. Java performance went up a lot since the\nstructure is a bit more similar between the training set and testing set. This stark contrast underscores\nthe crucial role of both the dataset size and parameter size of the base models in accomplishing closed-\ndomain code generation prompts. Interestingly, despite the rudimentary nature of our fine-tuning on\nStarCoder, the results still highlighted a significant improvement compared with the non-fine-tuned\nmodel. This stark contrast in performance bolsters our original assertion: achieving success in\nclosed-domain tasks can be realized either through large open-domain LLMs, or via fine-tuning\nsmaller models. These smaller models could potentially achieve comparable performance but with\nsignificantly reduced computational and memory requirements. Furthermore, Table 4 demonstrates\nthat the performance of models improves with the inclusion of dependencies in prompts, indicating\nthat including dependencies is an important part of promoting. Without additional training, ChatGPT\nmodels performed notably better than other models. Their performance underscores the crucial role\nof both the dataset scale and model size. That said, the performance of other models (e.g. StarCoder)\ncould be improved by fine-tuning.\n5\nANALYSIS AND DISCUSSION\nLooking more closely at the results in Table 4, it is clear that the larger models with more parameters\ngenerally perform better than the smaller models. GPT-4 model dwarfs the other models in this study\nin both size and performance. However, it is clear that BIOCODER remains a challenge as GPT-3.5\nand GPT-4, the best model, only achieved an accuracy of slightly under 60%.\nLooking at the other models, it is interesting to note that while InstructCodeT5+, CodeGen, and\nCodeGen2 are all larger than InCoder and SantaCoder, they perform far worse. This is likely due to\nthe former being trained for single-line completions rather than function completion. Furthermore,\nInstructCodeT5+, CodeGen, and CodeGen2 have relatively small context limits (Mikolov et al., 2013;\nMOI et al., 2022), which likely hurts their performance. As for the remaining model, SantaCoder\nnotably performs impressively well for being only a roughly 1B parameter model, which is an\nindication of aggressive fine-tuning on Python code.\nWe also note that the context length limit has a very large impact on how different models perform\non different prompts. Except for GPT-3.5 and GPT-4, models performed the best on the Summary\nOnly prompt style, likely because of its shorter length. Summary-only prompts are shortened\nprompts utilized across all our LLM models to ensure that context-limited LLMs still receive all\nthe information necessary to potentially generate functions. Within the summary-only prompts, we\noptimized our prompts to only contain the absolute minimum of necessary information, without\nincluding much of the additional context that provides detail regarding the functionality of other\ndependencies. Looking at Figure 3, which contains the complete prompt structure, summary-only\nprompts would reduce the class declarations to only their declarations and one sentence describing\ntheir output and input. This is especially pronounced for InCoder and SantaCoder, as they both have\nsmall context limits of 2,048 tokens. Their Pass@K performance for Python decreases dramatically\nwhen switching from short Summary Only prompts to longer Summary at Top/Bottom.\n7\nModel\nPrompt\nJava\nPython\nPass@1 Pass@5 Pass@10 Pass@20 Pass@1 Pass@5 Pass@10 Pass@20\nInCoder-6B\nSummary at Top\n0\n0\n0\n0\n0.828\n2.016\n3.006\n4.459\nUncommented\n0\n0\n0\n0\n0.032\n0.159\n0.318\n0.637\nSummary Only\n0\n0\n0\n0\n1.688\n5.320\n8.332\n12.006\nNecessary Only\n0\n0\n0\n0\n0.032\n0.159\n0.318\n0.637\nSantaCoder-1.1B\nSummary at Top\n0\n0\n0\n0\n0.637\n1.338\n1.844\n2.548\nUncommented\n0\n0\n0\n0\n0.287\n0.764\n0.955\n1.274\nSummary Only\n0\n0\n0\n0\n2.965\n9.848\n14.227\n18.181\nNecessary Only\n0\n0\n0\n0\n0.032\n0.159\n0.318\n0.637\nStarCoder-15.5B\nSummary at Top\n0\n0\n0\n0\n3.694\n13.197\n19.359\n24.554\nUncommented\n0\n0\n0\n0\n0.318\n1.062\n1.591\n2.548\nSummary Only\n0\n0\n0\n0\n4.682\n15.225\n21.200\n27.166\nNecessary Only\n0\n0\n0\n0\n0.127\n0.603\n1.123\n1.911\nStarCoder-15.5B\n(finetuned)\nSummary at top\n0\n0\n0\n0\n5.818\n16.562\n21.091\n27.048\nUncommented\n0\n0\n0\n0\n3.312\n9.073\n12.574\n17.536\nSummary Only\n0.200\n1.000\n2.000\n4.000\n7.295\n20.838\n26.143\n39.570\nNecessary Only 3.300\n12.097\n19.545\n30.000\n0.597\n1.173\n1.813\n2.611\nStarCoder+\nSummary at Top\n0\n0\n0\n0\n2.675\n9.133\n14.019\n19.650\nUncommented\n0\n0\n0\n0\n0.510\n0.955\n1.274\n1.911\nSummary Only\n1.300\n5.031\n8.042\n12.000\n2.548\n8.279\n12.864\n18.057\nNecessary Only\n0\n0\n0\n0\n0.127\n0.457\n0.609\n0.637\nInstructCodeT5+\nAll prompt types\n0\n0\n0\n0\n0\n0\n0\n0\nCodeGen-6B-mono\nSummary at Top\n0\n0\n0\n0\n0.637\n0.637\n0.637\n0.637\nUncommented\n0\n0\n0\n0\n0\n0\n0\n0\nSummary Only\n0\n0\n0\n0\n0.637\n0.637\n0.637\n0.637\nNecessary Only\n0\n0\n0\n0\n0\n0\n0\n0\nCodeGen-16B-mono\nSummary at Top\n0\n0\n0\n0\n0.637\n0.637\n0.637\n0.637\nUncommented\n0\n0\n0\n0\n0\n0\n0\n0\nSummary Only\n0\n0\n0\n0\n0.637\n0.637\n0.637\n0.637\nNecessary Only\n0\n0\n0\n0\n0\n0\n0\n0\nCodeGen2-7B\nSummary at Top\n0\n0\n0\n0\n0.637\n0.637\n0.637\n0.637\nUncommented\n0\n0\n0\n0\n0.510\n0.637\n0.637\n0.637\nSummary Only\n0\n0\n0\n0\n0.860\n2.494\n3.962\n6.242\nNecessary Only\n0\n0\n0\n0\n0\n0\n0\n0\nGPT-3.5-Turbo\nSummary at Top 4.100\n7.235\n8.989\n11.600\n22.771 33.461\n36.551\n39.490\nUncommented\n6.300\n11.563\n14.436\n18.000\n11.019 19.075\n21.680\n24.204\nSummary Only 17.400 33.199\n37.878\n42.000\n24.682 33.997\n37.132\n40.127\nNecessary Only 43.500 52.582\n53.995\n55.400\n28.758 39.529\n44.029\n47.771\nGPT-4\nSummary at top 1.100\n5.500\n11.000\n22.000\n10.701 25.500\n32.910\n39.490\nUncommented\n6.367\n11.234\n15.897\n18.562\n12.654 20.129\n24.387\n27.932\nSummary Only 19.483 24.721\n29.634\n2.543\n13.172 24.578\n28.394\n31.938\nNecessary Only 45.011 55.350\n57.616\n60.000\n38.439 48.491\n50.619\n52.229\nTable 4: Zero-shot and finetuned performance with five prompt versions of BIOCODER. For detailed\nexplanations of prompt versions see Appendix i. For all settings, we performed trials twice for\nPass@K. Results are expressed in percentages. We only finetuned StarCoder for 2000 steps, all others\nare zero-shot results. Additional results can be found in Appendix u.\nAs shown by the scatterplots in Appendix k, on models with an average Pass@K score of at least 2%,\nthere is an inverse relationship between the number of tokens in the prompt and the Pass@K score.\nFurthermore, for models such as SantaCoder and GPT models, the performance fell sharply after\naround 500 tokens. Despite this, model performance can not only be attributed to prompt length. We\ncan see that even though the \"Necessary Only prompts are relatively shorter when compared to the\n\"Summary at Top\" or \"Uncommented\" prompts, the Pass@k performance of the \"Uncommented\"\n8\nModel\nRosalind\nP@1\nP@5\nP@10\nP@20\nInCoder\n0.020\n0.099\n0.198\n0.395\nSantaCoder\n0.158\n0.658\n1.075\n1.581\nStarCoder\n0.534\n2.042\n3.228\n4.743\nStarCoderPlus\n0.356\n1.313\n1.978\n2.767\nStarCoder (FT)\n1.623\n3.109\n5.328\n7.036\nModel\nRosalind\nP@1\nP@5\nP@10\nP@20\nInstructCodeT5+\n0.059\n0.296\n0.593\n1.186\nCodeGen\n0.692\n2.088\n3.055\n3.953\nCodeGen2\n0.059\n0.296\n0.593\n1.186\nGPT-3.5 Turbo\n23.671\n31.953\n36.702\n40.725\nGPT-4\n24.308\n39.551\n44.864\n50.198\nTable 5: Performance of Rosalind. P@1 denotes Pass@1, etc. Numbers represent the Pass@K in the\nform of percentages. For all settings, n=20, and all models use the Description prompt.\nFailure/Success\nCount\nPercent (%)\nMismatched output\n8661\n4.567\nInvalid syntax\n117665\n62.038\nRuntime error\n55351\n29.184\nTime out\n4\n0.002\nSuccessfully Passed\n7982\n4.209\nTotal Testcases\n189663\n100\nTable 6: Aggregated Error Distribution\nAcross All Models\nPrompt\nMean\nMedian\nSTDev\nJava\n2278.82\n2599.00\n1331.81\nPython\n2790.75\n2194.00\n2539.79\nRosalind\n564.49\n509.00\n286.47\nOverall\n1510.66\n812.50\n1882.80\nTable 7: Prompt Tokens Distribution\nprompts is in fact worse for many of the models. For more analysis regarding this and prompt\nstructure in general, please refer to Appendix aa.\nFocusing on Java\u2019s performance, it is clear that most of the publicly available LLMs have not been\nfine-tuned for Java, resulting in the near 0% Pass@K values. Finally, Rosalind\u2019s performance results\nin Table 5 are roughly in line with Python\u2019s performance in Table 4.\nTable 6 provides an overview of the error statistics collected from our test runs. For more information\nabout what each error means, see Appendix s. We also have error statistics per model in Appendix\nv Looking at Appendix s, it appears that the models struggle the most at writing code that will\nsuccessfully compile or run. The fact that the number of samples of generated code that produced\nwrong output is relatively small compared to the code that failed to compile or run indicates that\nif the model is able to generate code that is able to be run, then that code is generally accurate and\ndoesn\u2019t produce the wrong output. Therefore, it seems that models have the most trouble generating\nsyntactically correct code rather than understanding the logic required to complete the problems\noutlined in the prompts. Further discussion on the results of each model can be found in Appendix j.\nDespite these challenges, we firmly believe that this dataset holds pivotal importance for benchmark-\ning future models, especially ones with larger context limits, like GPT-4-32k and Claude2.\n6\nCONCLUSIONS AND FUTURE WORK\nOur study underscores the challenges in code generation, emphasizing the shortcomings of current\nmodels in the face of complex tasks. We present a highly challenging natural language to code tasks,\nproviding input rich with dependencies and imports. Existing models struggle to comprehend the\napplication of these imported toolkits or functions contained in other files. Our tasks are marked\nby extensive input and a high level of specialization. These programs are closer to real-world\nscenarios, requiring professional-level code-writing skills, rather than merely catering to beginners.\nThis suggests that the code in question can typically be produced only by professional programmers.\nAs a novel benchmark within the field of bioinformatics, there remains a multitude of areas for future\nexploration. Currently, we have covered most of the existing models (at the time, August 2023).\nAdditionally, we will move beyond function-level code generation as current models do not have\nthe capacity of the token to generate file-sized code. We included only a few well-established bioin-\nformatics repositories; in future work, without a manpower constraint, we could include additional\nrepositories that span more niche sub-studies of bioinformatics and span across other languages.\n9\n7\nETHICS STATEMENT\nWe understand the importance of discussing the potential impacts of our specific work. In the context\nof our benchmark, one potential concern is the accuracy of the benchmark across all data points.\nThere is a risk that the benchmark may optimize incorrect outputs, which users might then use to test\ntheir LLMs. This concern is especially significant in research settings deploying incorrect code could\nlead to inaccurate conclusions, initiating a snowball effect of misinformation. Beyond the immediate\nimplications for LLMs and research outcomes, our benchmark and dataset potentially could be\nmisused. For example, malicious users might use these data to train models that generate harmful\nbiomedical experiments, such as designing dangerous molecules. While our intention is to advance\nknowledge and use it in a beneficial manner, there must be a level of caution and responsibility in\nemploying the benchmark and dataset we provide.\n8\nREPRODUCIBILITY\nOur dataset, benchmark, Docker images, corresponding prompts, and test scripts essential for the\nreproduction of our work can be accessed through the anonymous GitHub https://anonymous.\n4open.science/r/BioCoder-86BD/.\nREFERENCES\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz\nFerrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. Santacoder: don\u2019t\nreach for the stars! arXiv preprint arXiv:2301.03988, 2023.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones,\nNicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernan-\ndez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark,\nSam McCandlish, Chris Olah, and Jared Kaplan. A general language assistant as a laboratory for\nalignment. arXiv preprint arXiv:2112.00861, 2021.\nBen Athiwaratkun, Sanjay Krishna Gouda, Zijian Wang, Xiaopeng Li, Yuchen Tian, Ming Tan,\nWasi Uddin Ahmad, Shiqi Wang, Qing Sun, Mingyue Shang, Sujan Kumar Gonugondla, Hantian\nDing, Varun Kumar, Nathan Fulton, Arash Farahani, Siddhartha Jain, Robert Giaquinto, Haifeng\nQian, Murali Krishna Ramanathan, Ramesh Nallapati, Baishakhi Ray, Parminder Bhatia, Sudipta\nSengupta, Dan Roth, and Bing Xiang. Multi-lingual evaluation of code generation models, 2023.\nJacob Austin, Augustus Odena, Maxwell Nye, and Maarten Bosma. Program synthesis with large\nlanguage models. arXiv preprint, 2021a.\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large\nlanguage models, 2021b.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson\nKernion, Tom Conerly, Sheer El-Showk, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez,\nTristan Hume, Scott Johnston, Shauna Kravec, Liane Lovitt, Neel Nanda, Catherine Olsson, Dario\nAmodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann, and Jared Kaplan.\nTraining a helpful and harmless assistant with reinforcement learning from human feedback. arXiv\npreprint arXiv:2204.05862, 2022.\nShraddha Barke, Michael B James, and Nadia Polikarpova. Grounded copilot: How programmers\ninteract with code-generating models. Proceedings of the ACM on Programming Languages, 7\n(OOPSLA1):85\u2013111, 2023.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan,\nMohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron,\nLintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models\nacross training and scaling, 2023.\n10\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S. Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, Erik Brynjolfsson,\nShyamal Buch, Dallas Card, Rodrigo Castellon, Niladri Chatterji, Annie Chen, Kathleen Creel,\nJared Quincy Davis, Dora Demszky, Chris Donahue, Moussa Doumbouya, Esin Durmus, Stefano\nErmon, John Etchemendy, Kawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor Gale, Lauren\nGillespie, Karan Goel, Noah Goodman, Shelby Grossman, Neel Guha, Tatsunori Hashimoto, Peter\nHenderson, John Hewitt, Daniel E. Ho, Jenny Hong, Kyle Hsu, Jing Huang, Thomas Icard, Saahil\nJain, Dan Jurafsky, Pratyusha Kalluri, Siddharth Karamcheti, Geoff Keeling, Fereshte Khani, Omar\nKhattab, Pang Wei Koh, Mark Krass, Ranjay Krishna, Rohith Kuditipudi, Ananya Kumar, Faisal\nLadhak, Mina Lee, Tony Lee, Jure Leskovec, Isabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu\nMa, Ali Malik, Christopher D. Manning, Suvir Mirchandani, Eric Mitchell, Zanele Munyikwa,\nSuraj Nair, Avanika Narayan, Deepak Narayanan, Ben Newman, Allen Nie, Juan Carlos Niebles,\nHamed Nilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr, Isabel Papadimitriou, Joon Sung\nPark, Chris Piech, Eva Portelance, Christopher Potts, Aditi Raghunathan, Rob Reich, Hongyu\nRen, Frieda Rong, Yusuf Roohani, Camilo Ruiz, Jack Ryan, Christopher R\u00c3l\u2019, Dorsa Sadigh,\nShiori Sagawa, Keshav Santhanam, Andy Shih, Krishnan Srinivasan, Alex Tamkin, Rohan Taori,\nArmin W. Thomas, Florian Tram\u00c3\u00b4lr, Rose E. Wang, William Wang, Bohan Wu, Jiajun Wu,\nYuhuai Wu, Sang Michael Xie, Michihiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael Zhang,\nTianyi Zhang, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. On the\nopportunities and risks of foundation models, 2022.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal,\nAriel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford,\nIlya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\nNghi DQ Bui, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, and Steven CH Hoi. Codetf:\nOne-stop transformer library for state-of-the-art code llm. arXiv preprint arXiv:2306.00029, 2023.\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald\nPinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, Arjun Guha,\nMichael Greenberg, and Abhinav Jangda. MultiPL-E: a scalable and polyglot approach to bench-\nmarking neural code generation. IEEE Transactions on Software Engineering, pp. 1\u201317, 2023. doi:\n10.1109/TSE.2023.3267446. URL https://arxiv.org/abs/2208.08227.\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu Chen.\nCodet: Code generation with generated tests. arXiv preprint arXiv:2207.10397, 2022.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios\nChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino,\nNikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa,\nAlec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob\nMcGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating\nlarge language models trained on code, 2021.\nXinyun Chen, Maxwell Lin, Nathanael Sch\u00c3d\u2019rli, and Denny Zhou. Teaching large language models\nto self-debug, 2023.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n11\nFenia Christopoulou, Gerasimos Lampouras, Milan Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi\nLi, Qi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu, Li Yan, Pingyi Zhou, Xin Wang, Yuchi Ma,\nIgnacio Iacobacci, Yasheng Wang, Guangtai Liang, Jiansheng Wei, Xin Jiang, Qianxiang Wang,\nand Qun Liu. PanGu-Coder: program synthesis with function-level language modeling. arXiv\npreprint arXiv:2207.11280, 2022. doi: 10.48550/ARXIV.2207.11280. URL https://arxiv.\norg/abs/2207.11280.\nXueying Du, Mingwei Liu, Kaixin Wang, Hanlin Wang, Junwei Liu, Yixuan Chen, Jiayi Feng,\nChaofeng Sha, Xin Peng, and Yiling Lou. Classeval: A manually-crafted benchmark for evaluating\nllms on class-level code generation. arXiv preprint arXiv:2308.01861, 2023.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\nWen-tau Yih, Luke Zettlemoyer, and Mike Lewis. InCoder: a generative model for code infilling\nand synthesis. arXiv preprint arXiv:2204.05999, 2022. doi: 10.48550/ARXIV.2204.05999. URL\nhttps://arxiv.org/abs/2204.05999.\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wen\ntau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling and\nsynthesis, 2023.\nLeo Gao, Jonathan Tow, Stella Biderman, Charles Lovering, Jason Phang, Anish Thite, Fazz,\nNiklas Muennighoff, Thomas Wang, sdtblck, tttyuntian, researcher2, Zden\u00c4\u02d9Zk Kasner, Khalid\nAlmubarak, Jeffrey Hsu, Pawan Sasanka Ammanamanchi, Dirk Groeneveld, Eric Tang, Charles\nFoster, kkawamu1, xagi dev, uyhcire, Andy Zou, Ben Wang, Jordan Clive, igor0, Kevin Wang,\nNicholas Kross, Fabrizio Milo, and silentv0x. Eleutherai/lm-evaluation-harness: v0.3.0, December\n2022. URL https://doi.org/10.5281/zenodo.7413426.\nDan Hendrycks, Steven Basart, Saurav Kadavath, and Mantas Mazeika. Measuring coding challenge\ncompetence with apps. NeurIPS, 2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre.\nTraining compute-optimal large language models, 2022.\nHeidy Khlaaf, Pamela Mishkin, Joshua Achiam, Gretchen Krueger, and Miles Brundage. A hazard\nanalysis framework for code synthesis large language models. arXiv preprint arXiv:2207.14157,\n2022.\nMarie-Anne Lachaux, Baptiste Roziere, Lowik Chanussot, and Guillaume Lample. Unsupervised\ntranslation of programming languages, 2020.\nYuhang Lai, Chengxi Li, Yiming Wang, Tianyi Zhang, Ruiqi Zhong, Luke Zettlemoyer, Scott Wen\ntau Yih, Daniel Fried, Sida Wang, and Tao Yu. DS-1000: a natural and reliable benchmark for data\nscience code generation. ArXiv, abs/2211.11501, 2022.\nTaehyun Lee, Seokhee Hong, Jaewoo Ahn, Ilgee Hong, Hwaran Lee, Sangdoo Yun, Jamin Shin,\nand Gunhee Kim. Who wrote this code? watermarking for code generation. arXiv preprint\narXiv:2305.15060, 2023.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou,\nMarc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue\nZhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00c3\u02c7co Monteiro,\nOleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar\nUmapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason\nStillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang,\nNour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas,\nMaxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire\nSchlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson,\nCarolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried,\nDzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00c3\u00b4soz Ferrandis, Sean Hughes, Thomas Wolf, Arjun\nGuha, Leandro von Werra, and Harm de Vries. Starcoder: may the source be with you!, 2023.\n12\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien\nde Masson d\u2019Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal,\nAlexey Cherepanov, James Molloy, Daniel Mankowitz, Esme Sutherland Robson, Pushmeet Kohli,\nNando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. Competition-level code generation with\nalphacode. arXiv preprint arXiv:2203.07814, 2022.\nTianyang Liu, Canwen Xu, and Julian McAuley. Repobench: Benchmarking repository-level code\nauto-completion systems, 2023.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\nMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with\nevol-instruct. arXiv preprint arXiv:2306.08568, 2023.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efficient estimation of word representa-\ntions in vector space, 2013.\nMargaret Mitchell, Alexandra Sasha Luccioni, Nathan Lambert, Marissa Gerchick, Angelina\nMcMillan-Major, Ezinwanne Ozoani, Nazneen Rajani, Tristan Thrush, Yacine Jernite, and Douwe\nKiela. Measuring data, 2023.\nAnthony MOI, Nicolas Patry, Pierric Cistac, Pete, Funtowicz Morgan, Sebastian P\u00c3ijtz, Mishig,\nBjarte Johansen, Thomas Wolf, Sylvain Gugger, Clement, Julien Chaumond, Lysandre Debut,\nFran\u00c3\u02d8gois Garillot, Luc Georges, dctelus, JC Louis, MarcusGrass, Taufiquzzaman Peyash, 0xflotus,\nAlan deLevie, Alexander Mamaev, Arthur, Cameron, Colin Clement, Dagmawi Moges, David\nHewitt, Denis Zolotukhin, and Geoffrey Thomas. huggingface/tokenizers: Rust 0.13.2, November\n2022. URL https://doi.org/10.5281/zenodo.7298413.\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam\nSingh, Xiangru Tang, Leandro von Werra, and Shayne Longpre. Octopack: Instruction tuning code\nlarge language models. arXiv preprint arXiv:2308.07124, 2023a.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le\nScao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir\nRadev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson,\nEdward Raff, and Colin Raffel. Crosslingual generalization through multitask finetuning, 2023b.\nErik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2:\nLessons for training llms on programming and natural languages. arXiv preprint, 2023a.\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\nand Caiming Xiong. CodeGen: an open large language model for code with multi-turn program\nsynthesis. In The Eleventh International Conference on Learning Representations, 2023b. URL\nhttps://openreview.net/forum?id=iaYcJKpY2B_.\nTheo X Olausson, Jeevana Priya Inala, Chenglong Wang, Jianfeng Gao, and Armando Solar-Lezama.\nDemystifying gpt self-repair for code generation. arXiv preprint arXiv:2306.09896, 2023.\nOpenAI. Gpt-4 technical report, 2023.\nGabriel Orlanski, Kefan Xiao, Xavier Garcia, Jeffrey Hui, Joshua Howland, Jonathan Malmaud, Jacob\nAustin, Rishabh Singh, and Michele Catasta. Measuring the impact of programming language\ndistribution. In International Conference on Machine Learning, pp. 26619\u201326645. PMLR, 2023.\nRangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna, Divya Sankar, Lambert Pouguem Wassi, Michele\nMerler, Boris Sobolev, Raju Pavuluri, Saurabh Sinha, and Reyhaneh Jabbarvand. Understanding\nthe effectiveness of large language models in code translation. arXiv preprint arXiv:2308.03109,\n2023.\nShishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez. Gorilla: Large language model\nconnected with massive apis. arXiv preprint arXiv:2305.15334, 2023.\n13\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru\nTang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world\napis. arXiv preprint arXiv:2307.16789, 2023.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nPamela H Russell, Rachel L Johnson, Shreyas Ananthan, Benjamin Harnke, and Nichole E Carlson.\nA large-scale analysis of bioinformatics code on github. PloS one, 13(10):e0205898, 2018.\nBo Shen, Jiaxin Zhang, Taihong Chen, Daoguang Zan, Bing Geng, An Fu, Muhan Zeng, Ailun Yu,\nJichuan Ji, Jingyang Zhao, et al. Pangu-coder2: Boosting large language models for code with\nranking feedback. arXiv preprint arXiv:2307.14936, 2023.\nDisha Shrivastava, Denis Kocetkov, Harm de Vries, Dzmitry Bahdanau, and Torsten Scholak. Repo-\nfusion: Training code models to understand your repository, 2023.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven\nZheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin,\nJames Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent\nZhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh\nSrinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi,\nRenelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron\nCohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi,\nand Quoc Le. Lamda: Language models for dialog applications, 2022.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00c3l\u2019e\nLacroix, Baptiste Rozi\u00c3\u00b4lre, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,\nArmand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: open and efficient foundation\nlanguage models. arXiv preprint arXiv:2302.13971, 2023.\nShiqi Wang, Zheng Li, Haifeng Qian, Chenghao Yang, Zijian Wang, Mingyue Shang, Varun Kumar,\nSamson Tan, Baishakhi Ray, Parminder Bhatia, et al. Recode: Robustness evaluation of code\ngeneration models. arXiv preprint arXiv:2212.10264, 2022a.\nYue Wang, Weishi Wang, Shafiq Joty, and Steven C. H. Hoi. Codet5: Identifier-aware unified\npre-trained encoder-decoder models for code understanding and generation, 2021.\nYue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi.\nCodet5+: Open code large language models for code understanding and generation. arXiv preprint\narXiv:2305.07922, 2023a.\nZhiruo Wang, Shuyan Zhou, Daniel Fried, and Graham Neubig. Execution-based evaluation for\nopen-domain code generation. arXiv preprint arXiv:2212.10481, 2022b.\nZhiruo Wang, Grace Cuenca, Shuyan Zhou, Frank F. Xu, and Graham Neubig. Mconala: A benchmark\nfor code generation from multiple natural languages, 2023b.\nMan-Fai Wong, Shangxin Guo, Ching-Nam Hang, Siu-Wai Ho, and Chee-Wei Tan. Natural language\ngeneration and understanding of big code for ai-assisted programming: A review. Entropy, 25(6):\n888, 2023.\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang. On the tool\nmanipulation capability of open-source large language models. arXiv preprint arXiv:2305.16504,\n2023.\nZhou Yang, Zhipeng Zhao, Chenyu Wang, Jieke Shi, Dongsun Kim, DongGyun Han, and David Lo.\nWhat do code models memorize? an empirical study on large language models of code. arXiv\npreprint arXiv:2308.09932, 2023.\n14\nHao Yu, Bo Shen, Dezhi Ran, Jiaxin Zhang, Qi Zhang, Yuchi Ma, Guangtai Liang, Ying Li, Tao\nXie, and Qianxiang Wang. Codereval: A benchmark of pragmatic code generation with generative\npre-trained models. arXiv preprint, 2023.\nZhiqiang Yuan, Junwei Liu, Qiancheng Zi, Mingwei Liu, Xin Peng, and Yiling Lou. Evaluating\ninstruction-tuned large language models on code comprehension and generation. arXiv preprint\narXiv:2308.01240, 2023.\nDaoguang Zan, Bei Chen, Zeqi Lin, Bei Guan, Yongji Wang, and Jian-Guang Lou. When language\nmodel meets private library. In Conference on Empirical Methods in Natural Language Processing,\n2022a.\nDaoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin, Minsu Kim, Bei Guan, Yongji Wang, Weizhu Chen,\nand Jian-Guang Lou. Cert: Continual pre-training on sketches for library-oriented code generation,\n2022b.\nDaoguang Zan, Bei Chen, Fengji Zhang, Dianjie Lu, Bingchao Wu, Bei Guan, Wang Yongji, and\nJian-Guang Lou. Large language models meet nl2code: A survey. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n7443\u20137464, 2023.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen,\nPeng Zhang, Yuxiao Dong, and Jie Tang. GLM-130B: an open bilingual pre-trained model. arXiv\npreprint arXiv:2210.02414, 2022.\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu\nChen, 2023a.\nShizhuo Dylan Zhang, Curt Tigges, Stella Biderman, Maxim Raginsky, and Talia Ringer. Can\ntransformers learn to solve problems recursively? arXiv preprint arXiv:2305.14699, 2023b.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen,\nAndi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. CodeGeeX: A pre-trained model for\ncode generation with multilingual evaluations on HumanEval-X. arXiv preprint arXiv:2303.17568,\n2023a. doi: 10.48550/arXiv.2303.17568.\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen, Andi\nWang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for code\ngeneration with multilingual evaluations on humaneval-x, 2023b.\nTerry Yue Zhuo. Large language models are state-of-the-art evaluators of code generation. arXiv\npreprint arXiv:2304.14317, 2023.\n15\nA\nHYPERPARAMETER SETTINGS\nFor all models, we used the following hyperparameter settings:\n\u2022 top_k=50\n\u2022 top_p=0.95\n\u2022 temperature=0.7\n\u2022 early_stopping=True\n\u2022 num_return_sequences=1\n\u2022 do_sample=True\nB\nFUNCTION STATISTICS\nBelow are our distributions of comment lines, token lengths, and the number of parameters per\nfunction across our entire dataset. Generally, our number of parameters and comments are left-\nskewed, while our prompt lengths, while left-skewed, had a much more even distribution.\nFigure 4: Comment lines per Function Distribution\nFigure 5: Prompt Token Length Distribution\nBelow is the distribution of the number of output tokens, averaged across either Python or Java. The\nvast concentration of the generations is concentrated at 0-250, as those are the minimum reserved\ntokens for generations if the prompts are too large. Across each language, we utilized nine models to\ngenerate outputs.\n16\nFigure 6: Parameters per Function Distribution\nFigure 7: The distribution of the number of models\u2019 output tokens.\nC\nPYTHON EXAMPLES\nC.1\nPYTHON EXAMPLE 1\nC.1.1\nFULL PROMPT\n1 #This is in python\n2 #Write a function called \"unpipe_name\" that takes in a string parameter\ncalled \"name\". The function takes a string containing multiple gene\nnames separated by pipes , and returns a single gene name devoid of\nduplicates and pipe characters. The function checks if there are any\nduplicates , and removes any meaningless target names. If there are\nmultiple gene names present , the function takes the longest name as\nthe final name. If there are any ambiguous gene names , the function\nlogs a warning and selects the longest name as the final name. The\nfunction should also import \"cnvlib.params \".\n3 #\n4 #def unpipe_name(name):\n5 #\n6 #Here are the imports:\n7 #import numpy as np\n17\n8 #import logging\n9 #from . import params\n10 #from skgenome import tabio\n11 #Here are the global variables:\n12 #MIN_REF_COVERAGE = -5.0\n13 #MAX_REF_SPREAD = 1.0\n14 #NULL_LOG2_COVERAGE = -20.0\n15 #GC_MIN_FRACTION = 0.3\n16 #GC_MAX_FRACTION = 0.7\n17 #INSERT_SIZE = 250\n18 #IGNORE_GENE_NAMES = \u2019-\u2019, \u2019.\u2019, \u2019CGH \u2019\n19 #ANTITARGET_NAME = \u2019Antitarget \u2019\n20 #ANTITARGET_ALIASES = ANTITARGET_NAME , \u2019Background \u2019\n21 #Here are the class declarations:\n22 #Here are the additional function declarations:\n23 #def do_import_picard(fname , too_many_no_coverage):\n24 #\nsummary: Reads a file in \u2019picardhs \u2019 format , processes the data , and\nreturns a modified dataframe.\n25 #\nparam: fname (string) - the file name/path to be read in \u2019picardhs \u2019\nformat.\n26 #\nparam: too_many_no_coverage (int) - if the number of bins with no\ncoverage is greater than this value , a warning message is logged.\nDefault is 100.\n27 #\nreturn: garr (pandas dataframe) - a modified dataframe with added\ncolumns \u2019gene \u2019 and \u2019log2 \u2019 based on the original dataframe read from\nthe input file.\n28 #def unpipe_name(name):\n29 #\nsummary: Remove duplicate gene names and pipe characters from a given\nstring.\n30 #\nparam: name (str) - the string containing the gene names.\n31 #\nreturn: new_name (str) - the cleaned gene name string.\n32 #def do_import_theta(segarr , theta_results_fname , ploidy):\n33 #\nsummary: A function for importing theta results and estimating copy\nnumber and log2 ratios of segments.\n34 #\nparam: segarr (numpy array) - array of segments\n35 #\nparam: theta_results_fname (str) - name of theta results file\n36 #\nparam: ploidy (int) - ploidy of genome (default is 2)\n37 #\nreturn: generator of numpy arrays - array of segments with estimated\ncopy number and log2 ratios.\n38 #def parse_theta_results(fname):\n39 #\nsummary: Parses THetA results into a data structure with NLL , mu , C,\nand p* columns.\n40 #\nparam: fname (str) - name of the file to parse the results from\n41 #\nreturn: (dict) - a dictionary containing the NLL , mu_normal ,\nmu_tumors , C, and p* values\n42 #Here are the comments and the specs:\n43 #Write a function called \"unpipe_name\" that takes in a string parameter\ncalled \"name\". The function takes a string containing multiple gene\nnames separated by pipes , and returns a single gene name devoid of\nduplicates and pipe characters. The function checks if there are any\nduplicates , and removes any meaningless target names. If there are\nmultiple gene names present , the function takes the longest name as\nthe final name. If there are any ambiguous gene names , the function\nlogs a warning and selects the longest name as the final name. The\nfunction should also import \"cnvlib.params \".\n44 #def unpipe_name(name):\nC.1.2\nSMALL PROMPT\n1 Write a function with the following specs:\n2 --specs begin here --\n3 #Write a function called \"unpipe_name\" that takes in a string parameter\ncalled \"name\". The function takes a string containing multiple gene\nnames separated by pipes , and returns a single gene name devoid of\n18\nduplicates and pipe characters. The function checks if there are any\nduplicates , and removes any meaningless target names. If there are\nmultiple gene names present , the function takes the longest name as\nthe final name. If there are any ambiguous gene names , the function\nlogs a warning and selects the longest name as the final name. The\nfunction should also import \"cnvlib.params \".\n4 param: name (str) - the string containing the gene names.\n5 return: new_name (str) - the cleaned gene name string.\n6 --specs end here --\n7 Note the function will be embedded in the following context\n8 --context begins here --\n9 import random\n10 import hashlib\n11 import numpy as np\n12 import skimage\n13 import skimage.measure\n14 import scipy.ndimage\n15 import os\n16 import logging\n17 from functools import wraps\n18 from scipy import stats\n19 import sys\n20 import math\n21 IGNORE_GENE_NAMES = (\"-\", \".\", \"CGH\")\n22 <<insert solution here >>\n23 def main():\n24\nstring1 = <|string|>\n25\nstring2 = \u2019CGH \u2019\n26\nname=f\u2019{ string1 }|{ string2}\u2019\n27\nprint(unpipe_name(name))\n28 // context continues\n29 --context ends here --\n30 Make sure to only generate the function and not any of the context. Make\nsure you are generating valid , runnable code. Begin your solution\nwith:\n31 def unpipe_name(name):\n32 MAKE SURE TO INDENT THE BODY OF YOUR FUNCTION BY A TAB\nC.1.3\nNO COMMENT PROMPT\n1 This is in python\n2 Write a function called \"unpipe_name\" that takes in a string parameter\ncalled \"name\". The function takes a string containing multiple gene\nnames separated by pipes , and returns a single gene name devoid of\nduplicates and pipe characters. The function checks if there are any\nduplicates , and removes any meaningless target names. If there are\nmultiple gene names present , the function takes the longest name as\nthe final name. If there are any ambiguous gene names , the function\nlogs a warning and selects the longest name as the final name. The\nfunction should also import \"cnvlib.params \".\n3\n4 def unpipe_name(name):\n5\n6 Here are the imports:\n7 import numpy as np\n8 import logging\n9 from . import params\n10 from skgenome import tabio\n11 Here are the global variables:\n12 MIN_REF_COVERAGE = -5.0\n13 MAX_REF_SPREAD = 1.0\n14 NULL_LOG2_COVERAGE = -20.0\n15 GC_MIN_FRACTION = 0.3\n16 GC_MAX_FRACTION = 0.7\n19\n17 INSERT_SIZE = 250\n18 IGNORE_GENE_NAMES = \u2019-\u2019, \u2019.\u2019, \u2019CGH \u2019\n19 ANTITARGET_NAME = \u2019Antitarget \u2019\n20 ANTITARGET_ALIASES = ANTITARGET_NAME , \u2019Background \u2019\n21 Here are the class declarations:\n22 Here are the additional function declarations:\n23 def do_import_picard(fname , too_many_no_coverage):\n24\nsummary: Reads a file in \u2019picardhs \u2019 format , processes the data , and\nreturns a modified dataframe.\n25\nparam: fname (string) - the file name/path to be read in \u2019picardhs \u2019\nformat.\n26\nparam: too_many_no_coverage (int) - if the number of bins with no\ncoverage is greater than this value , a warning message is logged.\nDefault is 100.\n27\nreturn: garr (pandas dataframe) - a modified dataframe with added\ncolumns \u2019gene \u2019 and \u2019log2 \u2019 based on the original dataframe read from\nthe input file.\n28 def unpipe_name(name):\n29\nsummary: Remove duplicate gene names and pipe characters from a given\nstring.\n30\nparam: name (str) - the string containing the gene names.\n31\nreturn: new_name (str) - the cleaned gene name string.\n32 def do_import_theta(segarr , theta_results_fname , ploidy):\n33\nsummary: A function for importing theta results and estimating copy\nnumber and log2 ratios of segments.\n34\nparam: segarr (numpy array) - array of segments\n35\nparam: theta_results_fname (str) - name of theta results file\n36\nparam: ploidy (int) - ploidy of genome (default is 2)\n37\nreturn: generator of numpy arrays - array of segments with estimated\ncopy number and log2 ratios.\n38 def parse_theta_results(fname):\n39\nsummary: Parses THetA results into a data structure with NLL , mu , C,\nand p* columns.\n40\nparam: fname (str) - name of the file to parse the results from\n41\nreturn: (dict) - a dictionary containing the NLL , mu_normal ,\nmu_tumors , C, and p* values\n42 Here are the comments and the specs:\n43 Write a function called \"unpipe_name\" that takes in a string parameter\ncalled \"name\". The function takes a string containing multiple gene\nnames separated by pipes , and returns a single gene name devoid of\nduplicates and pipe characters. The function checks if there are any\nduplicates , and removes any meaningless target names. If there are\nmultiple gene names present , the function takes the longest name as\nthe final name. If there are any ambiguous gene names , the function\nlogs a warning and selects the longest name as the final name. The\nfunction should also import \"cnvlib.params \".\n44 def unpipe_name(name):\nC.1.4\nREFERENCE CODE\n1 def unpipe_name(name):\n2\n\"\"\"Fix the duplicated gene names Picard spits out.\n3\n4\nReturn a string containing the single gene name , sans duplications\nand pipe\n5\ncharacters.\n6\n7\nPicard CalculateHsMetrics combines the labels of overlapping\nintervals\n8\nby joining all labels with \u2019|\u2019, e.g. \u2019BRAF|BRAF \u2019 -- no two distinct\n9\ntargeted genes actually overlap , though , so these dupes are redundant\n.\n10\nMeaningless target names are dropped , e.g. \u2019CGH|FOO|-\u2019 resolves as \u2019\nFOO \u2019.\n20\n11\nIn case of ambiguity , the longest name is taken , e.g. \"TERT|TERT\nPromoter\"\n12\nresolves as \"TERT Promoter \".\n13\n\"\"\"\n14\nif \u2019|\u2019 not in name:\n15\nreturn name\n16\ngene_names = set(name.split(\u2019|\u2019))\n17\nif len(gene_names) == 1:\n18\nreturn gene_names.pop()\n19\ncleaned_names = gene_names.difference(IGNORE_GENE_NAMES)\n20\nif cleaned_names:\n21\ngene_names = cleaned_names\n22\nnew_name = sorted(gene_names , key=len , reverse=True)[0]\n23\nif len(gene_names) > 1:\n24\nlogging.warning(\u2019WARNING: Ambiguous gene name %r; using %r\u2019, name\n,\n25\nnew_name)\n26\nreturn new_name\nC.1.5\nANNOTATION PROMPTS\n1 #This is in python\n2 #Write a function called \"unpipe_name\" that takes in a string parameter\ncalled \"name\". The function takes a string containing multiple gene\nnames separated by pipes , and returns a single gene name devoid of\nduplicates and pipe characters. The function checks if there are any\nduplicates , and removes any meaningless target names. If there are\nmultiple gene names present , the function takes the longest name as\nthe final name. If there are any ambiguous gene names , the function\nlogs a warning and selects the longest name as the final name. The\nfunction should also import \"cnvlib.params \".\n3 #\n4 #def unpipe_name(name):\nC.1.6\nCONTEXT\n1 import random\n2 import hashlib\n3 import numpy as np\n4 import skimage\n5 import skimage.measure\n6 import scipy.ndimage\n7 import os\n8 import logging\n9 from functools import wraps\n10 from scipy import stats\n11 import sys\n12 import math\n13 IGNORE_GENE_NAMES = (\"-\", \".\", \"CGH\")\n14 <<insert solution here >>\n15 def main():\n16\nstring1 = <|string|>\n17\nstring2 = \u2019CGH \u2019\n18\nname=f\u2019{ string1 }|{ string2}\u2019\n19\nprint(unpipe_name(name))\n20 if __name__ == \"__main__ \":\n21\nmain()\nC.2\nPYTHON EXAMPLE 2\nC.2.1\nFULL PROMPT\n21\n1 #This is in python\n2 #write a function called \"UnifyLevels\" that takes three parameters:\nbaseLevel (a constant array of integers representing the base levels)\n, addonLevel (a constant array of integers representing the\nadditional levels), and windowSize (an integer representing the\nwindow size). The function should merge the two lists of breakpoints ,\nbut drop addonLevel values that are within windowSize of baseLevel\nvalues. The function should return an array of integers called\njoinedLevel. If addonLevel is an empty array , the function should\nreturn baseLevel. The output should be sorted and of dtype=np.int_.\n3 #\n4 #def UnifyLevels(baseLevel , addonLevel , windowSize):\n5 #\n6 #Here are the imports:\n7 #from scipy import stats\n8 #import math\n9 #import numpy as np\n10 #import logging\n11 #import pandas as pd\n12 #Here are the global variables:\n13 #Here are the class declarations:\n14 #Here are the additional function declarations:\n15 #def segment_haar(cnarr , fdr_q):\n16 #\nsummary: Segment CNVkit data using HaarSeg algorithm\n17 #\nparam: cnarr (CopyNumArray) - binned , normalized copy ratios\n18 #\nparam: fdr_q (float) - false discovery rate q-value\n19 #\nreturn: segarr (CopyNumArray) - CBS data table as a CNVkit object\n20 #def one_chrom(cnarr , fdr_q , chrom):\n21 #\nsummary: This function segments copy number data for a single\nchromosome using the HaarSeg algorithm.\n22 #\nparam: cnarr (pandas.DataFrame) - a dataframe with columns \u2019\nchromosome \u2019, \u2019start \u2019, \u2019end \u2019, \u2019log2 \u2019, and \u2019probes \u2019.\n23 #\nparam: fdr_q (float) - the false discovery rate threshold for\nsegmenting the copy number data.\n24 #\nparam: chrom (str) - the chromosome to segment.\n25 #\nreturn: table (pandas.DataFrame) - a dataframe with columns \u2019\nchromosome \u2019, \u2019start \u2019, \u2019end \u2019, \u2019log2 \u2019, \u2019gene \u2019, and \u2019probes \u2019,\nrepresenting the segmented copy number data.\n26 #def variants_in_segment(varr , segment , fdr_q):\n27 #\nsummary: Generates a table of variant segments based on allele\nfrequencies\n28 #\nparam: varr (object) - variant data\n29 #\nparam: segment (object) - genomic segment data\n30 #\nparam: fdr_q (float) - false discovery rate threshold\n31 #\nreturn: table (object) - pandas DataFrame with segmented data\n32 #def haarSeg(I, breaksFdrQ , W, rawI , haarStartLevel , haarEndLevel):\n33 #\nsummary: Perform segmentation on a 1D array of log -ratio values\naccording to the HaarSeg algorithm.\n34 #\nparam: I (array) - A 1D array of log -ratio values , sorted according\nto their genomic location.\n35 #\nparam: W (array) - Weight matrix , corresponding to quality of\nmeasurement , with values :math : \u20181/(\\\\ sigma ^2) \u2018. Must have the same\nsize as I.\n36 #\nparam: rawI (array) - The minimum between the raw test -sample and\ncontrol -sample coverages (before applying log ratio , but after any\nbackground reduction and/or normalization). These raw red / green\nmeasurments are used to detect low -value probes , which are more\nsensitive to noise. Used for the non -stationary variance compensation\n. Must have the same size as I.\n37 #\nparam: breaksFdrQ (float) - The FDR q parameter. This value should\nlie between 0 and 0.5.\n38 #\nparam: haarStartLevel (int) - The detail subband from which we start\nto detect peaks.\n39 #\nparam: haarEndLevel (int) - The detail subband until which we use to\ndetect peaks.\n22\n40 #\nreturn: dict - Returns a dictionary containing the start and end\npoints of each segment and the mean value of each segment.\n41 #def FDRThres(x, q, stdev):\n42 #\nsummary: Calculates the False Discovery Rate (FDR) threshold.\n43 #\nparam: x (unknown type) - array of values.\n44 #\nparam: q (unknown type) - a floating -point number.\n45 #\nparam: stdev (unknown type) - a floating -point number representing\nthe standard deviation.\n46 #\nreturn: T (unknown type) - a floating -point number representing the\nFDR threshold.\n47 #def SegmentByPeaks(data , peaks , weights):\n48 #\nsummary: Average the values of the probes within each segment.\n49 #\nparam: data (array) - the probe array values\n50 #\nparam: peaks (array) - Positions of copy number breakpoints in the\noriginal array\n51 #\nparam: weights (None or array) - optional array of weights of same\nlength as the data array\n52 #\nreturn: segs (array) - array of segment values obtained by averaging\nthe values of the probes within each segment.\n53 #def HaarConv(signal , weight , stepHalfSize):\n54 #\nsummary: Convolve haar wavelet function with a signal , applying\ncircular padding.\n55 #\nparam: signal (array of floats) - signal to be convolved.\n56 #\nparam: weight (array of floats) - optional weights for the steps of\nthe convolution.\n57 #\nparam: stepHalfSize (int) - half size of the step to be used in the\nconvolution.\n58 #\nreturn: array (array of floats) - of floats , representing the\nconvolved signal.\n59 #def FindLocalPeaks(signal):\n60 #\nsummary: Finds local maxima on positive values , local minima on\nnegative values.\n61 #\nparam: signal (const array of floats): an array of floating point\nnumbers\n62 #\nreturn: peakLoc (array of ints): Locations of extrema in \u2018signal \u2018\n63 #def UnifyLevels(baseLevel , addonLevel , windowSize):\n64 #\nsummary: Merge two lists of breakpoints and drop addonLevel values\nthat are too close to baseLevel values.\n65 #\nparam: baseLevel (const array of ints) - a list of integers\nrepresenting the base level.\n66 #\nparam: addonLevel (const array of ints) - a list of integers\nrepresenting the addon level.\n67 #\nparam: windowSize (int) - an integer representing the maximum\ndistance between addonLevel and baseLevel values.\n68 #\nreturn: joinedLevel (array of ints) - a sorted array of integers\nrepresenting the merged list of breakpoints.\n69 #def PulseConv(signal , pulseSize):\n70 #\nsummary: Convolve a pulse function with a signal applying circular\npadding to the signal for non -stationary variance compensation.\n71 #\nparam: signal (const array of floats) - the signal to be convolved.\n72 #\nparam: pulseSize (int) - the size of the pulse function.\n73 #\nreturn: array of floats - the convolved signal.\n74 #def AdjustBreaks(signal , peakLoc):\n75 #\nsummary: Improve localization of breaks in a signal by adjusting peak\nlocations.\n76 #\nparam: signal (const array of floats) - the signal to adjust the peak\nlocations in.\n77 #\nparam: peakLoc (const array of ints) - the current peak locations in\nthe signal.\n78 #\nreturn: newPeakLoc (array of ints) - the adjusted peak locations in\nthe signal.\n79 #def table2coords(seg_table):\n80 #\nsummary: Returns x and y arrays for plotting with the help of the\ninput segment Table\n23\n81 #\nparam: seg_table (list) - a list of tuples containing start , size ,\nand value.\n82 #\nreturn: x (list) - a list of x-coordinates for plotting.\n83 #\nreturn: y (list) - a list of y-coordinates for plotting.\n84 #Here are the comments and the specs:\n85 #write a function called \"UnifyLevels\" that takes three parameters:\nbaseLevel (a constant array of integers representing the base levels)\n, addonLevel (a constant array of integers representing the\nadditional levels), and windowSize (an integer representing the\nwindow size). The function should merge the two lists of breakpoints ,\nbut drop addonLevel values that are within windowSize of baseLevel\nvalues. The function should return an array of integers called\njoinedLevel. If addonLevel is an empty array , the function should\nreturn baseLevel. The output should be sorted and of dtype=np.int_.\n86 #def UnifyLevels(baseLevel , addonLevel , windowSize):\nC.2.2\nSMALL PROMPT\n1 Write a function with the following specs:\n2 --specs begin here --\n3 #write a function called \"UnifyLevels\" that takes three parameters:\nbaseLevel (a constant array of integers representing the base levels)\n, addonLevel (a constant array of integers representing the\nadditional levels), and windowSize (an integer representing the\nwindow size). The function should merge the two lists of breakpoints ,\nbut drop addonLevel values that are within windowSize of baseLevel\nvalues. The function should return an array of integers called\njoinedLevel. If addonLevel is an empty array , the function should\nreturn baseLevel. The output should be sorted and of dtype=np.int_.\n4 param: baseLevel (const array of ints) - a list of integers representing\nthe base level.\n5 param: addonLevel (const array of ints) - a list of integers representing\nthe addon level.\n6 param: windowSize (int) - an integer representing the maximum distance\nbetween addonLevel and baseLevel values.\n7 return: joinedLevel (array of ints) - a sorted array of integers\nrepresenting the merged list of breakpoints.\n8 --specs end here --\n9 Note the function will be embedded in the following context\n10 --context begins here --\n11 import random\n12 import hashlib\n13 import numpy as np\n14 import skimage\n15 import skimage.measure\n16 import scipy.ndimage\n17 import os\n18 import logging\n19 from functools import wraps\n20 from scipy import stats\n21 import sys\n22 import math\n23 <<insert solution here >>\n24 def main():\n25\nnp.random.seed(<|int;range =0 ,1000| >)\n26\nbaseLevel = np.random.randint (20, size =(10))\n27\nnp.random.seed(<|int;range =0 ,1000| >)\n28\naddonLevel = np.random.randint (20, size =(10))\n29\nprint(UnifyLevels(baseLevel , addonLevel , 3))\n30 if __name__ == \"__main__ \":\n31 // context continues\n32 --context ends here --\n24\n33 Make sure to only generate the function and not any of the context. Make\nsure you are generating valid , runnable code. Begin your solution\nwith:\n34 def UnifyLevels(baseLevel , addonLevel , windowSize):\n35 MAKE SURE TO INDENT THE BODY OF YOUR FUNCTION BY A TAB\nC.2.3\nNO COMMENT PROMPT\n1 This is in python\n2 write a function called \"UnifyLevels\" that takes three parameters:\nbaseLevel (a constant array of integers representing the base levels)\n, addonLevel (a constant array of integers representing the\nadditional levels), and windowSize (an integer representing the\nwindow size). The function should merge the two lists of breakpoints ,\nbut drop addonLevel values that are within windowSize of baseLevel\nvalues. The function should return an array of integers called\njoinedLevel. If addonLevel is an empty array , the function should\nreturn baseLevel. The output should be sorted and of dtype=np.int_.\n3\n4 def UnifyLevels(baseLevel , addonLevel , windowSize):\n5\n6 Here are the imports:\n7 from scipy import stats\n8 import math\n9 import numpy as np\n10 import logging\n11 import pandas as pd\n12 Here are the global variables:\n13 Here are the class declarations:\n14 Here are the additional function declarations:\n15 def segment_haar(cnarr , fdr_q):\n16\nsummary: Segment CNVkit data using HaarSeg algorithm\n17\nparam: cnarr (CopyNumArray) - binned , normalized copy ratios\n18\nparam: fdr_q (float) - false discovery rate q-value\n19\nreturn: segarr (CopyNumArray) - CBS data table as a CNVkit object\n20 def one_chrom(cnarr , fdr_q , chrom):\n21\nsummary: This function segments copy number data for a single\nchromosome using the HaarSeg algorithm.\n22\nparam: cnarr (pandas.DataFrame) - a dataframe with columns \u2019\nchromosome \u2019, \u2019start \u2019, \u2019end \u2019, \u2019log2 \u2019, and \u2019probes \u2019.\n23\nparam: fdr_q (float) - the false discovery rate threshold for\nsegmenting the copy number data.\n24\nparam: chrom (str) - the chromosome to segment.\n25\nreturn: table (pandas.DataFrame) - a dataframe with columns \u2019\nchromosome \u2019, \u2019start \u2019, \u2019end \u2019, \u2019log2 \u2019, \u2019gene \u2019, and \u2019probes \u2019,\nrepresenting the segmented copy number data.\n26 def variants_in_segment(varr , segment , fdr_q):\n27\nsummary: Generates a table of variant segments based on allele\nfrequencies\n28\nparam: varr (object) - variant data\n29\nparam: segment (object) - genomic segment data\n30\nparam: fdr_q (float) - false discovery rate threshold\n31\nreturn: table (object) - pandas DataFrame with segmented data\n32 def haarSeg(I, breaksFdrQ , W, rawI , haarStartLevel , haarEndLevel):\n33\nsummary: Perform segmentation on a 1D array of log -ratio values\naccording to the HaarSeg algorithm.\n34\nparam: I (array) - A 1D array of log -ratio values , sorted according\nto their genomic location.\n35\nparam: W (array) - Weight matrix , corresponding to quality of\nmeasurement , with values :math : \u20181/(\\\\ sigma ^2) \u2018. Must have the same\nsize as I.\n36\nparam: rawI (array) - The minimum between the raw test -sample and\ncontrol -sample coverages (before applying log ratio , but after any\nbackground reduction and/or normalization). These raw red / green\n25\nmeasurments are used to detect low -value probes , which are more\nsensitive to noise. Used for the non -stationary variance compensation\n. Must have the same size as I.\n37\nparam: breaksFdrQ (float) - The FDR q parameter. This value should\nlie between 0 and 0.5.\n38\nparam: haarStartLevel (int) - The detail subband from which we start\nto detect peaks.\n39\nparam: haarEndLevel (int) - The detail subband until which we use to\ndetect peaks.\n40\nreturn: dict - Returns a dictionary containing the start and end\npoints of each segment and the mean value of each segment.\n41 def FDRThres(x, q, stdev):\n42\nsummary: Calculates the False Discovery Rate (FDR) threshold.\n43\nparam: x (unknown type) - array of values.\n44\nparam: q (unknown type) - a floating -point number.\n45\nparam: stdev (unknown type) - a floating -point number representing\nthe standard deviation.\n46\nreturn: T (unknown type) - a floating -point number representing the\nFDR threshold.\n47 def SegmentByPeaks(data , peaks , weights):\n48\nsummary: Average the values of the probes within each segment.\n49\nparam: data (array) - the probe array values\n50\nparam: peaks (array) - Positions of copy number breakpoints in the\noriginal array\n51\nparam: weights (None or array) - optional array of weights of same\nlength as the data array\n52\nreturn: segs (array) - array of segment values obtained by averaging\nthe values of the probes within each segment.\n53 def HaarConv(signal , weight , stepHalfSize):\n54\nsummary: Convolve haar wavelet function with a signal , applying\ncircular padding.\n55\nparam: signal (array of floats) - signal to be convolved.\n56\nparam: weight (array of floats) - optional weights for the steps of\nthe convolution.\n57\nparam: stepHalfSize (int) - half size of the step to be used in the\nconvolution.\n58\nreturn: array (array of floats) - of floats , representing the\nconvolved signal.\n59 def FindLocalPeaks(signal):\n60\nsummary: Finds local maxima on positive values , local minima on\nnegative values.\n61\nparam: signal (const array of floats): an array of floating point\nnumbers\n62\nreturn: peakLoc (array of ints): Locations of extrema in \u2018signal \u2018\n63 def UnifyLevels(baseLevel , addonLevel , windowSize):\n64\nsummary: Merge two lists of breakpoints and drop addonLevel values\nthat are too close to baseLevel values.\n65\nparam: baseLevel (const array of ints) - a list of integers\nrepresenting the base level.\n66\nparam: addonLevel (const array of ints) - a list of integers\nrepresenting the addon level.\n67\nparam: windowSize (int) - an integer representing the maximum\ndistance between addonLevel and baseLevel values.\n68\nreturn: joinedLevel (array of ints) - a sorted array of integers\nrepresenting the merged list of breakpoints.\n69 def PulseConv(signal , pulseSize):\n70\nsummary: Convolve a pulse function with a signal applying circular\npadding to the signal for non -stationary variance compensation.\n71\nparam: signal (const array of floats) - the signal to be convolved.\n72\nparam: pulseSize (int) - the size of the pulse function.\n73\nreturn: array of floats - the convolved signal.\n74 def AdjustBreaks(signal , peakLoc):\n75\nsummary: Improve localization of breaks in a signal by adjusting peak\nlocations.\n26\n76\nparam: signal (const array of floats) - the signal to adjust the peak\nlocations in.\n77\nparam: peakLoc (const array of ints) - the current peak locations in\nthe signal.\n78\nreturn: newPeakLoc (array of ints) - the adjusted peak locations in\nthe signal.\n79 def table2coords(seg_table):\n80\nsummary: Returns x and y arrays for plotting with the help of the\ninput segment Table\n81\nparam: seg_table (list) - a list of tuples containing start , size ,\nand value.\n82\nreturn: x (list) - a list of x-coordinates for plotting.\n83\nreturn: y (list) - a list of y-coordinates for plotting.\n84 Here are the comments and the specs:\n85 write a function called \"UnifyLevels\" that takes three parameters:\nbaseLevel (a constant array of integers representing the base levels)\n, addonLevel (a constant array of integers representing the\nadditional levels), and windowSize (an integer representing the\nwindow size). The function should merge the two lists of breakpoints ,\nbut drop addonLevel values that are within windowSize of baseLevel\nvalues. The function should return an array of integers called\njoinedLevel. If addonLevel is an empty array , the function should\nreturn baseLevel. The output should be sorted and of dtype=np.int_.\n86 def UnifyLevels(baseLevel , addonLevel , windowSize):\nC.2.4\nREFERENCE CODE\n1 def UnifyLevels(baseLevel , addonLevel , windowSize):\n2\n\"\"\" Unify several decomposition levels.\n3\n4\nMerge the two lists of breakpoints , but drop addonLevel values that\nare too\n5\nclose to baseLevel values.\n6\n7\nParameters\n8\n----------\n9\nbaseLevel : const array of ints\n10\naddonLevel : const array of ints\n11\nwindowSize : int\n12\n13\nReturns\n14\n-------\n15\njoinedLevel : array of ints\n16\n17\nSource: HaarSeg.c\n18\n\"\"\"\n19\nif not len(addonLevel):\n20\nreturn baseLevel\n21\njoinedLevel = []\n22\naddon_idx = 0\n23\nfor base_elem in baseLevel:\n24\nwhile addon_idx < len(addonLevel):\n25\naddon_elem = addonLevel[addon_idx]\n26\nif addon_elem < base_elem - windowSize:\n27\njoinedLevel.append(addon_elem)\n28\naddon_idx += 1\n29\nelif base_elem - windowSize <= addon_elem <= base_elem +\nwindowSize:\n30\naddon_idx += 1\n31\nelse:\n32\nassert base_elem + windowSize < addon_elem\n33\nbreak\n34\njoinedLevel.append(base_elem)\n35\nlast_pos = baseLevel [-1] + windowSize if len(baseLevel) else -1\n27\n36\nwhile addon_idx < len(addonLevel) and addonLevel[addon_idx] <=\nlast_pos:\n37\naddon_idx += 1\n38\nif addon_idx < len(addonLevel):\n39\njoinedLevel.extend(addonLevel[addon_idx :])\n40\nreturn np.array(sorted(joinedLevel), dtype=np.int_)\nC.2.5\nCONTEXT\n1 import random\n2 import hashlib\n3 import numpy as np\n4 import skimage\n5 import skimage.measure\n6 import scipy.ndimage\n7 import os\n8 import logging\n9 from functools import wraps\n10 from scipy import stats\n11 import sys\n12 import math\n13 <<insert solution here >>\n14 def main():\n15\nnp.random.seed(<|int;range =0 ,1000| >)\n16\nbaseLevel = np.random.randint (20, size =(10))\n17\nnp.random.seed(<|int;range =0 ,1000| >)\n18\naddonLevel = np.random.randint (20, size =(10))\n19\nprint(UnifyLevels(baseLevel , addonLevel , 3))\n20 if __name__ == \"__main__ \":\n21\nmain()\n28\nD\nROSALIND EXAMPLES\nThe Rosalind dataset consists of 253 problems from the bioinformatics training website\nrosalind.info. For each of the 253 problems that make up the Rosalind portion of BIOCODER, we\ncreated both a prompt that we fed into the LLMs as well as a golden code solution that we either\nwrote ourselves or found on public GitHub repositories.\nFor each problem, to create the prompt, we first scraped the problem\u2019s page on the Rosalind website\nto obtain a problem description. This problem description would then make up the first section of the\nprompt. For instance, for the Rosalind problem ba1a, here is the scraped problem description:\n1 # This is the first problem in a collection of \"code challenges\" to\naccompany Bioinformatics Algorithms: An Active -Learning Approach by\nPhillip Compeau & Pavel Pevzner.\n2 # A k-mer is a string of length k.\n3 # We define Count(Text , Pattern) as the number of times that a k-mer\nPattern appears as a substring of Text.\n4 # For example ,\n5 # $\\textit{Count }(\\ text{ACA}\\ color{green }\\ textbf{ACTAT }\\ color{black }\\text\n{GCAT}\\color{green}\\ textbf{ACTAT }\\ color{black }\\text{CGGGA }\\ color{\ngreen}\\ textbf{ACTAT}\\color{black }\\text{CCT}, {\\ color{green }\\ textbf{\nACTAT }}) = 3$.\n6 # We note that Count($\\text{CG}\\ color{green }\\ textbf{ATATA }\\ color{black }\\\ntext{TCC}\\color{green}\\ textbf{ATA}\\ color{black }\\text{G}$, $\\color{\ngreen}\\ textbf{ATA}$) is equal to 3 (not 2) since we should account\nfor overlapping occurrences of Pattern in Text.\n7 # To compute Count(Text , Pattern), our plan is to \"slide a window\" down\nText , checking whether each k-mer substring of Text matches Pattern.\nWe will therefore refer to the k-mer starting at position i of Text\nas Text(i, k). Throughout this book , we will often use 0-based\nindexing , meaning that we count starting at 0 instead of 1. In this\ncase , Text begins at position 0 and ends at position |Text| - 1 (|\nText| denotes the number of symbols in Text). For example , if Text =\nGACCATACTG ,\n8 # then Text(4, 3) = ATA. Note that the last k-mer of Text begins at\nposition |Text| - k, e.g., the last 3-mer of GACCATACTG starts at\nposition 10 - 3 = 7. This discussion results in the following\npseudocode for computing Count(Text , Pattern).\n9 # PatternCount(Text , Pattern)\n10 # count <- 0\n11 # for i <- 0 to |Text| - |Pattern|\n12 # if Text(i, |Pattern |) = Pattern\n13 # count <-s count + 1\n14 # return count\n15 # Implement PatternCount\n16 # Given: {{DNA strings }} Text and Pattern.\n17 # Return: Count(Text , Pattern).\n18 # Sample Dataset\n19 # GCGCG\n20 # GCG\n21 # Sample Output\n22 # 2\nFrom the sample problem description above you can see that a problem description generally consists\nof an explanation of the problem at hand, as well as a sample test case. The second section of the\nprompt consists of a custom block of text that prompts the LLM to generate its code to fit the context\nthat generated Rosalind code will be run in. For instance, for the same Rosalind problem ba1a, here\nis the custom block of text that makes up the second section of the prompt:\n1\n#write the solve() function to solve the above problem\n2 #Do NOT print anything\n3 #Do NOT make comments\n4 #Do NOT call the main() function.\n5 #Use 4 spaces for tabs.\n6 #input_str: a string\n29\n7 #return output: another string\n8\n9\n10 def main():\n11\nwith open(\"input.txt\", \"r\") as f:\n12\noutput = solve(f.read())\n13\nprint(output)\n14\n15 # Write your solution here\n16 # Begin with: def solve(input_str):\nFrom the example above, you can see that the custom block of text consists of a prompt telling\nthe LLM to generate a solve function as well as the context that the generated code will be run\nin, namely the main function that is included in the custom text. Each prompt consists of these\ntwo sections, i.e., the first section of the prompt will include a problem description consisting of an\nexplanation of the problem as well as a sample test case, and the second section of the prompt will\nconsist of custom prompting text that tells the LLM to generate a solve function and provides the\ncontext in which the Rosalind code will be run.\nFor each of the 253 problems that make up the Rosalind portion of BIOCODER, we also either\nfound online or wrote custom golden code solutions. Each golden code solution is meant to be run\nin the custom context, following the main and solve structure that is described in every Rosalind\nprompt. For instance, here is the golden code solution for the Rosalind problem ba1a:\n1 # Compute the Number of Times a Pattern Appears in a Text\n2\n3 def substrings(text , size):\n4\nfor i in range(len(text) - size + 1):\n5\nyield text[i : i + size]\n6\n7 def pattern_count(text , pattern):\n8\nreturn sum(pattern == x for x in substrings(text , len(pattern)))\n9\n10 def solve(input_str):\n11\ntext , pattern = input_str.splitlines ()\n12\n13\nreturn pattern_count(text , pattern)\nAs you can see, a golden code solution consists of the solve function requested by the corresponding\nprompt, as well as any additional helper functions that may be called (possibly recursively) by the\nsolve function. For more information on the Rosalind portion of the BIOCODER dataset, three\ndetailed examples of Rosalind prompts and golden code are provided below to further illustrate the\ndifferences between the code and the prompt.\nD.1\nEXAMPLE 1\nThe given input is a problem description in the field of bioinformatics, namely the Reverse Com-\nplement Problem. The problem is related to the DNA sequences, which are strings containing the\ncharacters \u2018A\u2019, \u2018T\u2019, \u2018G\u2019, and \u2018C\u2019. Each of these characters represents a nucleotide in the DNA.\nThe problem requires model to find the reverse complement of a given DNA string. The reverse\ncomplement of a DNA string is obtained by replacing each nucleotide in the string with its complement\n(\u2018A\u2019 with \u2018T\u2019, \u2018T\u2019 with \u2018A\u2019, \u2018C\u2019 with \u2018G\u2019, and \u2018G\u2019 with \u2018C\u2019), and then reversing the entire string. For\nexample, the reverse complement of \u201cGTCA\u201d is \u201cTGAC.\u201d\nThe problem description includes a sample dataset (DNA string) and its corresponding output, which\nis its reverse complement. The problem also specifies the format for the solution: a Python function\nnamed \u2018solve\u2019, which takes a DNA string as an input and returns its reverse complement as output.\nThe function should not print anything or include any comments.\n1\n2 Reverse Complement Problem:\n3 In DNA strings , symbols \u2019A\u2019 and \u2019T\u2019 are complements of each other , as are\n\u2019C\u2019 and \u2019G\u2019.\n30\n4 Given a nucleotide p, we denote its complementary nucleotide as p. The\nreverse complement of a DNA string Pattern = p1\u00e2\u0102\u0119pn is the string\nPattern = pn \u00e2\u0102\u0119 p1 formed by taking the complement of each\nnucleotide in Pattern , then reversing the resulting string.\n5\n6 For example , the reverse complement of Pattern = \"GTCA\" is Pattern = \"\nTGAC\".\n7\n8\n9 Problem:\n10 Find the reverse complement of a DNA string.\n11 Given: A DNA string Pattern.\n12 Return: Pattern , the reverse complement of Pattern.\n13\n14 Sample Dataset:\n15 AAAACCCGGT\n16\n17 Sample Output:\n18 ACCGGGTTTT\n19\n20 write the solve() function to solve the above problem.\n21\n22 Do NOT print anything\n23 Do NOT make comments\n24 Do NOT call the main() function.\n25 Use 4 spaces for tabs.\n26\n27 input_str: a string\n28 return output: another string\n29\n30 def main():\n31\nwith open(\"input.txt\", \"r\") as f:\n32\noutput = solve(f.read())\n33\nprint(output)\n34\n35\n36 Write your solution here:\n37 Begin with: def solve(input_str):\nThe given sample solution to the problem is written in Python. It defines two functions: \u2018revcomp\u2019 and\n\u2018main\u2019. The \u2018revcomp\u2019 function computes the reverse complement of a DNA string by first reversing\nthe string and then replacing each nucleotide with its complement using Python\u2019s \u2018str.maketrans\u2019\nmethod. The \u2018main\u2019 function reads a DNA string from a file, computes its reverse complement using\nthe \u2018revcomp\u2019 function, and then prints the result.\nThe ground-truth code is a solution to the problem presented in the \u2018Rosalind\u2019 platform, a platform\nthat provides a collection of bioinformatics problems. This platform is known for helping students\nand researchers learn bioinformatics by solving the computational problems it presents.\n1 Find the Reverse Complement of a String\n2\n3\n4 def revcomp(seq):\n5\nreturn seq [:: -1]. translate(str.maketrans (\"ACGT\", \"TGCA\"))\n6\n7\n8 def main(file):\n9\nseq = open(file).read().splitlines ()[0]\n10\nprint(revcomp(seq))\nD.2\nEXAMPLE 2\nThe input here consists of several parts.\n31\nThe initial portion discusses the Hardy Weinberg Principle and Mendel\u2019s laws, which are used in\ngenetics to predict how genes will distribute throughout a population over time. The Hardy Weinberg\nPrinciple assumes that a population is large and remains in genetic equilibrium, meaning the frequency\nof each allele (a version of a gene) remains constant across generations, under certain conditions.\nThe problem statement is presented: if we know the proportion of homozygous recessive individuals\n(those with two copies of the recessive allele) for each of several genetic factors in a population, can\nwe predict the probability that a randomly selected individual carries at least one copy of the recessive\nallele for each factor? The task is to write a Python function named \u2019solve\u2019 to solve this problem.\nA sample dataset is given, represented as an array of floating-point numbers, each of which indicates\nthe proportion of homozygous recessive individuals for a specific genetic factor in the population.\nThe \u2018solve\u2019 function will receive this array as a string and should return another string representing\nthe solution array.\nThe final portion of the input is a sample Python code that reads the dataset from a file, calls the\n\u2018solve\u2019 function to compute the solution, and then prints the solution. However, the user is instructed\nnot to print anything, not to write comments, and not to call this \u2018main\u2019 function in your solution.\nThe user is also instructed to use four spaces for indentation.\n1 Genetic Drift and the Hardy -Weinberg Principle Mendel \u2019s laws of\nsegregation and independent assortment are excellent for the study of\nindividual organisms and their progeny , but they say nothing about\nhow alleles move through a population over time.\n2\n3 Our first question is: when can we assume that the ratio of an allele in\na population , called the allele frequency , is stable? G. H. Hardy and\nWilhelm Weinberg independently considered this question at the turn\nof the 20th Century , shortly after Mendel \u2019s ideas had been\nrediscovered. They concluded that the percentage of an allele in a\npopulation of individuals is in genetic equilibrium when five\nconditions are satisfied: The population is so large that random\nchanges in the allele frequency are negligible. No new mutations are\naffecting the gene of interest; The gene does not influence survival\nor reproduction , so natural selection is not occurring; Gene flow , or\nthe change in allele frequency due to migration into and out of the\npopulation , is negligible. Mating occurs randomly with respect to the\ngene of interest. The Hardy -Weinberg principle states that if a\npopulation is in genetic equilibrium for a given allele , then its\nfrequency will remain constant and evenly distributed through the\npopulation. Unless the gene in question is important to survival or\nreproduction , Hardy -Weinberg usually offers a reasonable enough model\nof population genetics. One of the many benefits of the Mendelian\ntheory of inheritance and simplifying models like Hardy -Weinberg is\nthat they help us predict the probability with which genetic diseases\nwill be inherited , so as to take appropriate preventative measures.\nGenetic diseases are usually caused by mutations to chromosomes ,\nwhich are passed on to subsequent generations. The simplest and most\nwidespread case of a genetic disease is a single gene disorder , which\nis caused by a single mutated gene. Over 4,000 such human diseases\nhave been identified , including cystic fibrosis and sickle -cell\nanemia. In both of these cases , the individual must possess two\nrecessive alleles for a gene in order to contract the disease. Thus ,\ncarriers can live their entire lives without knowing that they can\npass the disease on to their children. The above introduction to\ngenetic equilibrium leaves us with a basic and yet very practical\nquestion regarding gene disorders: if we know the number of people\nwho have a disease encoded by a recessive allele , can we predict the\nnumber of carriers in the population?\n4\n5 Problem:\n6\n7 To model the Hardy -Weinberg principle , assume that we have a population\nof $N$ diploid individuals. If an allele is in genetic equilibrium ,\nthen because mating is random , we may view the $2N$ chromosomes as\n32\nreceiving their alleles uniformly. In other words , if there are $m$\ndominant alleles , then the probability of a selected chromosome\nexhibiting the dominant allele is simply $p = \\frac{m}{2N}$. Because\nthe first assumption of genetic equilibrium states that the\npopulation is so large as to be ignored , we will assume that $N$ is\ninfinite , so that we only need to concern ourselves with the value of\n$p$.\n8\n9 Given: An array $A$ for which $A[k]$ represents the proportion of\nhomozygous recessive individuals for the $k$ -th Mendelian factor in a\ndiploid population. Assume that the population is in genetic\nequilibrium for all factors.\n10\n11 Return: An array $B$ having the same length as $A$ in which $B[k]$\nrepresents the probability that a randomly selected individual\ncarries at least one copy of the recessive allele for the $k$ -th\nfactor.\n12\n13 Sample Dataset\n14 0.1 0.25 0.5\n15\n16 write the solve() function to solve the above problem\n17\n18 Do NOT print anything\n19 Do NOT make comments\n20 Do NOT call the main() function.\n21\n22 Use 4 spaces for tabs.\n23\n24 input_str: a string\n25 return output: another string\n26\n27 def main():\n28\nwith open(\"input.txt\", \"r\") as f:\n29\noutput = solve(f.read())\n30\nprint(output)\n31\n32 Write your solution here\n33 Begin with: def solve(input_str):\nIn the output portion, a Python function afrq is presented, which takes an array of proportions of\nhomozygous recessive individuals for each genetic factor in a population and returns an array of\nprobabilities that a randomly selected individual carries at least one copy of the recessive allele for\neach factor. The main function uses this afrq function to solve the problem based on the input data.\nIt takes the input data from a file, applies the afrq function to compute the solution, and then prints\nthe solution in the form of a space-separated string of floating-point numbers, each rounded to three\ndecimal places.\n1 # Counting Disease Carriers\n2\n3 from math import sqrt\n4 from .helpers import Parser\n5\n6 def afrq(a):\n7\n\"\"\" Counting Disease Carriers \"\"\"\n8\nreturn [2 * sqrt(x) - x for x in a]\n9\n10 def main(file):\n11\nb = afrq(Parser(file).floats ())\n12\nprint (*[ round(x, 3) for x in b])\n33\nD.3\nEXAMPLE 3\nThis Python script is designed to find the most frequent k-mers (substrings of length k) in a given\nDNA sequence (a string), while allowing for a specified number of mismatches. This problem is\nreferred to as the \u201cFrequent Words with Mismatches Problem\u201d.\nThe input to this problem is a string that contains two distinct parts separated by a newline:\nThe DNA sequence is represented as a string of characters \u2018A\u2019, \u2018C\u2019, \u2018G\u2019, and \u2018T\u2019. Two integers, k and\nd, are separated by a space. The integer k refers to the length of the substrings (k-mers) of interest,\nand d specifies the maximum number of mismatches that are allowed in a k-mer.\n1\n2 We defined a mismatch in \u2018\u2018Compute the Hamming Distance Between Two\nStrings\u00e2\u0102\u0130. We now generalize\n\u00e2 \u0102 F i n d\nthe Most Frequent Words in a\nString\u00e2\u0102\u0130 to incorporate mismatches as well. Given strings Text and\nPattern as well as an integer d, we define Countd(Text , Pattern) as\nthe total number of occurrences of Pattern in Text with at most d\nmismatches. For example , Count1(AACAAGCTGATAAACATTTAAAGAG , AAAAA) = 4\nbecause AAAAA appears four times in this string with at most one\nmismatch: AACAA , ATAAA , AAACA , and AAAGA. Note that two of these\noccurrences overlap.\n3\n4 A most frequent k-mer with up to d mismatches in Text is simply a string\nPattern maximizing Countd(Text , Pattern) among all k-mers. Note that\nPattern does not need to actually appear as a substring of Text; for\nexample , AAAAA is the most frequent 5-mer with 1 mismatch in\nAACAAGCTGATAAACATTTAAAGAG , even though AAAAA does not appear exactly\nin this string. Keep this in mind while solving the following problem\n.\n5\n6 Frequent Words with Mismatches Problem\n7\n8 Find the most frequent k-mers with mismatches in a string. Given: A\nstring Text as well as integers k and d. Return: All most frequent k-\nmers with up to d mismatches in Text.\n9\n10 Sample Dataset\n11 ACGTTGCATGTCGCATGATGCATGAGAGCT\n12 4 1\n13\n14 Sample Output\n15 GATG ATGC ATGT\n16\n17 write the solve() function to solve the above problem\n18 Do NOT print anything\n19 Do NOT make comments\n20 Do NOT call the main() function.\n21 Use 4 spaces for tabs.\n22 input_str: a string\n23 return output: another string\n24\n25 def main():\n26\nwith open(\"input.txt\", \"r\") as f:\n27\noutput = solve(f.read())\n28\nprint(output)\n29\n30 Write your solution here\n31 Begin with: def solve(input_str):\nHere is the corresponding reference code:\n1 # Find the Most Frequent Words with Mismatches in a String\n2\n3 from .ba1g import hamming\n34\n4 from .ba1b import count_kmers , most_frequent\n5 from itertools import product\n6\n7 # Note , the best kmer might not be observed in our sequence. The\nsimplistic\n8 # method here simply checks all possible kmers (which is ~17M for k = 12)\n9\n10 def generate_kmers(k):\n11\nreturn (\"\". join(x) for x in product ([\"A\", \"C\", \"G\", \"T\"], repeat=k))\n12\n13 def count_hamming_kmers(kmers , d, k):\n14\nfor x in generate_kmers(k):\n15\ncount = sum(kmers[y] for y in kmers if hamming(x, y) <= d)\n16\nif count > 0:\n17\nyield [x, count]\n18\n19 def main(file):\n20\nseq , ints = open(file).read().splitlines ()\n21\nk, d = list(map(int , ints.split ()))\n22\nkmers = count_kmers(seq , k)\n23\nhkmers = dict(count_hamming_kmers(kmers , d, k))\n24\nprint(* most_frequent(hkmers))\n35\nE\nADDITIONAL MODEL INFORMATION\nModel\nParameters\nInCoder (Fried et al., 2023)\n6B\nSantaCoder (Allal et al., 2023)\n1.1B\nStarCoder (Li et al., 2023)\n15.5B\nStarCoderPlus (Li et al., 2023)\n15.5B\nInstructCodeT5+ (Wang et al., 2023a)\n16B\nCodeGen (Nijkamp et al., 2023b)\n6B-mono*\nCodeGen (Nijkamp et al., 2023b)\n16B-mono*\nCodeGen2 (Nijkamp et al., 2023a)\n7B*\nGPT-3.5-Turbo\nUnknown\nGPT-4\nUnknown\nTable 8: Size of models.\nWe were not able to run the 16B models due to issues with scaling and sharding on the A100s.\nWe found that the following parameters produced the best results across all models.\n\u2022 top_k=50\n\u2022 top_p=0.95\n\u2022 temperature=0.7\n\u2022 early_stopping=True\n\u2022 num_return_sequences=1\n\u2022 do_sample=True\nWe utilized similar parameters to make model testing consistent across all tested models. This\napproach allowed for a more unbiased comparison of how each model performed on our benchmark.\nWe determined these weights by averaging the recommended parameters for the above models and\nthen used Bayesian optimization to determine the most optimal parameters.\nFurthermore, note that we used the version of GPT-3.5-Turbo hosted on Azure OpenAI Service\n(gpt-35-turbo-0301). There may be some minor differences compared with the OpenAI Platform\nversion.\n36\nF\nUTILIZING CHATGPT FOR PROMPT GENERATION\nChatGPT served as a pivotal component in our prompt generation, enabling us to assemble compre-\nhensive annotations for the ground-truth functions and their dependencies. This eliminates the need\nfor extensive manual labor that would otherwise be spent on annotation. Utilizing GPT-3.5 Turbo\ncalls further ensures consistency across annotations, mitigating the variability that might arise from\nhuman annotators.\nThe prompt was constructed as follows:\n1 Generate a prompt for the following function , such that a programmer\ncould reproduce it based solely on the description. Emphasize crucial\ncomponents like the return statement , complex function calls , and\nambiguous variables. However , avoid line -by -line descriptions; aim to\nprovide a high -level overview. The response should be in plain text ,\nfree from any formatting such as Markdown. Keep the length under 200\nwords or 15 lines , whichever is shorter.\n2 Here is the function for you to describe:\n3 <function >\n4 Here are the additional dependencies that may or may not be used within\nthe function:\n5 <dependencies >\n6 Again , limit your responses to no more than 200 words or 15 lines ,\nwhichever comes first.\n7 Start your prompt with \"write a function \".\n37\nG\nMODEL CONTEXT LIMITS\nModel\nContext limit\nInCoder (Fried et al., 2023)\n2048\nSantaCoder (Allal et al., 2023)\n2048\nStarCoder (Li et al., 2023)\n8192\nStarCoderPlus (Li et al., 2023)\n8192\nInstructCodeT5+ (Wang et al., 2023a)\n2048\nCodeGen (Nijkamp et al., 2023b)\n2048\nCodeGen2 (Nijkamp et al., 2023a)\n2048\nGPT-3.5-Turbo\n8192\nGPT-4\n8192\nTable 9: Length limits of different models.\nSince the limit includes both input and output tokens, we reserved 256 tokens for the output, therefore\nreducing the input length limit by 256. In our best effort to continue testing with these context limits,\ninstead of treating a failing test case immediately, we attempted to truncate the prompts such that\nthey fit barely below the context limit of each respective model. For all models, we also tested a\nversion with the function summary at the top. Note that for all models, the context limit was identical\nregardless of the parameter count.\n38\nH\nCODEREVAL COMPARISON\nTo validate the performance of code LLMs, multiple existing benchmarks are proposed, including\nonly cases of generating a standalone function, i.e., a function that invokes or accesses only built-in\nfunctions and standard libraries. However, standalone functions constitute only about 30% of the\nfunctions from real open-source projects. To assess a model\u2019s performance for pragmatic code\ngeneration (i.e., code generation for real settings of open source or proprietary code), CoderEval (Yu\net al., 2023) proposes a benchmark named CoderEval of pragmatic code generation with generative\npre-trained models.\nIn comparison to CoderEval, our approach is more focused on bioinformatics, as we ensure each\nfunction requires a certain level of domain knowledge in bioinformatics, as opposed to entry-level\ndata science or software engineering tasks.\nMoreover, we employ a more advanced parser, implementing a comprehensive parsing of the\nrespective GitHub repositories. This includes the imported third-party packages and the classes that\nhave been imported from another actual file.\nWe also conduct function testing at the file level, rather than the repository level, manually extracting\nthe context at the end and running it through a framework that catches more errors than CoderEval.\nAdditionally, our dataset is larger than CoderEval\u2019s, which only consists of 230 functions from 43\nPython projects and 230 methods from 10 Java projects, while we have data from more than two\nthousand sources.\nCoderEval classifies tasks into six levels according to the source of dependency outside the function,\nwhile we directly use the most complex dependencies.\nCoderEval employs a human-labeled version description from 13 experienced engineers, while we\nleverage ChatGPT to generate function descriptions. Similarly, for test cases, our use of fuzz testing\nallows us to scale to large data volumes without the need for intensive manual annotation.\nYet, we share some similarities with CoderEval. Both BIOCODER and CoderEval can be used to\nassess the performance of models beyond merely generating standalone functions. Given the need\nto address dependencies with context, we both establish testing based on Docker, cloning GitHub\nrepositories and their associated import dependencies. With \u2018k\u2019 candidate outputs generated by\ndifferent models (e.g., 20), we simulate scenarios in actual IDEs.\n39\nI\nPROMPT EXPLANATIONS\nBelow is an explanation of the prompt types:\n1. Summary Only We begin the prompt from these prompts, where only the summary and the\nfunction signature are contained in the prompt. The summary is added as a comment directly\nabove the signature, which is uncommented. The summary includes nearly complete details\nabout the task; however, it intentionally does not thoroughly explain what the context is.\nTherefore, this result is best treated as a baseline when compared with other prompt types.\nAn example is shown below:\n1 #This is in python\n2 #Write a function named \"planewise_morphology_closing\" that\naccepts two parameters , \"x_data\" and \"structuring_element \".\nInside the function , create a new variable named \"y_data\" that\nwill hold the result of the morphological closing operation.\nLoop through each item in \"x_data\" and perform the same\noperation on each. Do this by using the \"enumerate\" function to\nget both the index and plane for each item. Use the \"closing\"\nfunction from the \"skimage.morphology\" module to perform the\nmorphological closing operation on each plane using the \"\nstructuring_element\" parameter. Finally , return the newly\ncreated \"y_data\" array that holds the result of the operation.\nEnsure that the function uses the \"numpy\" and \"skimage.\nmorphology\" modules , as they are dependencies for the function.\n3 #\n4 #def planewise_morphology_closing(x_data , structuring_element):\n5\n6\n2. Uncommented We add the summary back into all of the parsed context. Therefore, the\nentire prompt contains all imports, global variables, classes, internal class functions, etc.,\nthat were imported into the file originally. For functions over ten lines, we summarize the\nparameters, return type, and logic instead of including the actual function code in order to\nreduce the number of tokens used and eliminate data that the model would not need. Note\nthat, by design, if these extraneous functions were used, the test would have counted as\nfailed, because it would be extremely inefficient code. An example is shown below:\n1 This is in python\n2 Here are the imports:\n3 from cellprofiler.library.modules import closing\n4 from ._help import HELP_FOR_STREL\n5 from cellprofiler_core.module import ImageProcessing\n6 from cellprofiler_core.setting import StructuringElement\n7 Here are the global variables:\n8 Here are the class declarations:\n9 class Closing(ImageProcessing):\n10\nattributes: self.structuring_element ,self.function\n11\nmethods:\n12\ndef create_settings(self):\n13\nsummary: creates settings for the Closing function\n14\nparam: self (unknown) - instance of the Closing function\n15\nreturn: None\n16\ndef settings(self):\n17\nsummary: Returns settings from superclass with self.\nstructuring_element added.\n18\nparam: None\n19\nreturn: list - __settings__ with self.structuring_element\nappended.\n20\ndef visible_settings(self):\n21\nsummary: This function returns the visible settings of a\nClosing object.\n22\nparam: self (Closing) - the Closing object whose settings\nare to be returned.\n40\n23\nreturn: list - the visible settings of the Closing object.\n24\ndef run(self , workspace):\n25\nsummary: Applies morphological closing to an image in a\nworkspace.\n26\nparam: workspace (object) - the workspace containing the\nimage.\n27\nreturn: None\n28 Here are the additional function declarations:\n29 def planewise_morphology_closing(x_data , structuring_element):\n30\nsummary: Applies morphology closing operation to each plane in\nx_data using structuring_element and returns the resulting\ny_data.\n31\nparam: x_data (numpy array) - 3D array containing the input\ndata.\n32\nparam: structuring_element (numpy array) - Structuring element\nused to perform the closing operation.\n33\nreturn: y_data (numpy array) - 3D array containing the result\nof closure operation on each plane in x_data.\n34 Here are the comments and the specs:\n35 Write a function named \"planewise_morphology_closing\" that accepts\ntwo parameters , \"x_data\" and \"structuring_element \". Inside the\nfunction , create a new variable named \"y_data\" that will hold\nthe result of the morphological closing operation. Loop through\neach item in \"x_data\" and perform the same operation on each.\nDo this by using the \"enumerate\" function to get both the index\nand plane for each item. Use the \"closing\" function from the \"\nskimage.morphology\" module to perform the morphological closing\noperation on each plane using the \"structuring_element\"\nparameter. Finally , return the newly created \"y_data\" array\nthat holds the result of the operation. Ensure that the\nfunction uses the \"numpy\" and \"skimage.morphology\" modules , as\nthey are dependencies for the function.\n36 def planewise_morphology_closing(x_data , structuring_element):\n37\n38\nNote that it is nearly certain that each prompt will contain extraneous data that does not need\nto be used for the function. The goal is for the LLM to ensure it uses the correct context for\nthe function. Note that for prompts that exceed the model\u2019s context limit, this prompt likely\nfails.\n3. Summary at Bottom We generate these using the previous prompt (uncommented), and\ncomment out all the context. Note that for prompts that exceed the model\u2019s context limit,\nthis prompt likely fails. In addition, note that there are no results for \"summary at bottom\"\nfor Java due to an incompatibility with Java syntax. We could not generate this type of\nprompt for Java similar to how we generated it for Python.\n1 #Here are the imports:\n2 #from cellprofiler.library.modules import closing\n3 #from ._help import HELP_FOR_STREL\n4 #from cellprofiler_core.module import ImageProcessing\n5 #from cellprofiler_core.setting import StructuringElement\n6 #Here are the global variables:\n7 #Here are the class declarations:\n8 #class Closing(ImageProcessing):\n9 #\nattributes: self.structuring_element ,self.function\n10 #\nmethods:\n11 #\ndef create_settings(self):\n12 #\nsummary: creates settings for the Closing function\n13 #\nparam: self (unknown) - instance of the Closing function\n14 #\nreturn: None\n15 #\ndef settings(self):\n16 #\nsummary: Returns settings from superclass with self.\nstructuring_element added.\n17 #\nparam: None\n41\n18 #\nreturn: list - __settings__ with self.structuring_element\nappended.\n19 #\ndef visible_settings(self):\n20 #\nsummary: This function returns the visible settings of a\nClosing object.\n21 #\nparam: self (Closing) - the Closing object whose settings\nare to be returned.\n22 #\nreturn: list - the visible settings of the Closing object.\n23 #\ndef run(self , workspace):\n24 #\nsummary: Applies morphological closing to an image in a\nworkspace.\n25 #\nparam: workspace (object) - the workspace containing the\nimage.\n26 #\nreturn: None\n27 #Here are the additional function declarations:\n28 #def planewise_morphology_closing(x_data , structuring_element):\n29 #\nsummary: Applies morphology closing operation to each plane in\nx_data using structuring_element and returns the resulting\ny_data.\n30 #\nparam: x_data (numpy array) - 3D array containing the input\ndata.\n31 #\nparam: structuring_element (numpy array) - Structuring element\nused to perform the closing operation.\n32 #\nreturn: y_data (numpy array) - 3D array containing the result\nof closure operation on each plane in x_data.\n33 #Here are the comments and the specs:\n34 #Write a function named \"planewise_morphology_closing\" that\naccepts two parameters , \"x_data\" and \"structuring_element \".\nInside the function , create a new variable named \"y_data\" that\nwill hold the result of the morphological closing operation.\nLoop through each item in \"x_data\" and perform the same\noperation on each. Do this by using the \"enumerate\" function to\nget both the index and plane for each item. Use the \"closing\"\nfunction from the \"skimage.morphology\" module to perform the\nmorphological closing operation on each plane using the \"\nstructuring_element\" parameter. Finally , return the newly\ncreated \"y_data\" array that holds the result of the operation.\nEnsure that the function uses the \"numpy\" and \"skimage.\nmorphology\" modules , as they are dependencies for the function.\n35 def planewise_morphology_closing(x_data , structuring_element):\n36\n4. Summary at Top These prompts are generated from the previous prompts (summary at\nbottom); however, the summary is copied at the top (moved for Java). This is intended for\nmodels with shorter context lengths, as when we truncated the prompt, the summary would\nstill be intact, along with a portion of the context.\n1 #This is in python\n2 #Write a function named \"planewise_morphology_closing\" that\naccepts two parameters , \"x_data\" and \"structuring_element \".\nInside the function , create a new variable named \"y_data\" that\nwill hold the result of the morphological closing operation.\nLoop through each item in \"x_data\" and perform the same\noperation on each. Do this by using the \"enumerate\" function to\nget both the index and plane for each item. Use the \"closing\"\nfunction from the \"skimage.morphology\" module to perform the\nmorphological closing operation on each plane using the \"\nstructuring_element\" parameter. Finally , return the newly\ncreated \"y_data\" array that holds the result of the operation.\nEnsure that the function uses the \"numpy\" and \"skimage.\nmorphology\" modules , as they are dependencies for the function.\n3 #\n4 #def planewise_morphology_closing(x_data , structuring_element):\n5 #\n6 #Here are the imports:\n42\n7 #from cellprofiler.library.modules import closing\n8 #from ._help import HELP_FOR_STREL\n9 #from cellprofiler_core.module import ImageProcessing\n10 #from cellprofiler_core.setting import StructuringElement\n11 #Here are the global variables:\n12 #Here are the class declarations:\n13 #class Closing(ImageProcessing):\n14 #\nattributes: self.structuring_element ,self.function\n15 #\nmethods:\n16 #\ndef create_settings(self):\n17 #\nsummary: creates settings for the Closing function\n18 #\nparam: self (unknown) - instance of the Closing function\n19 #\nreturn: None\n20 #\ndef settings(self):\n21 #\nsummary: Returns settings from superclass with self.\nstructuring_element added.\n22 #\nparam: None\n23 #\nreturn: list - __settings__ with self.structuring_element\nappended.\n24 #\ndef visible_settings(self):\n25 #\nsummary: This function returns the visible settings of a\nClosing object.\n26 #\nparam: self (Closing) - the Closing object whose settings\nare to be returned.\n27 #\nreturn: list - the visible settings of the Closing object.\n28 #\ndef run(self , workspace):\n29 #\nsummary: Applies morphological closing to an image in a\nworkspace.\n30 #\nparam: workspace (object) - the workspace containing the\nimage.\n31 #\nreturn: None\n32 #Here are the additional function declarations:\n33 #def planewise_morphology_closing(x_data , structuring_element):\n34 #\nsummary: Applies morphology closing operation to each plane in\nx_data using structuring_element and returns the resulting\ny_data.\n35 #\nparam: x_data (numpy array) - 3D array containing the input\ndata.\n36 #\nparam: structuring_element (numpy array) - Structuring element\nused to perform the closing operation.\n37 #\nreturn: y_data (numpy array) - 3D array containing the result\nof closure operation on each plane in x_data.\n38 #Here are the comments and the specs:\n39 #Write a function named \"planewise_morphology_closing\" that\naccepts two parameters , \"x_data\" and \"structuring_element \".\nInside the function , create a new variable named \"y_data\" that\nwill hold the result of the morphological closing operation.\nLoop through each item in \"x_data\" and perform the same\noperation on each. Do this by using the \"enumerate\" function to\nget both the index and plane for each item. Use the \"closing\"\nfunction from the \"skimage.morphology\" module to perform the\nmorphological closing operation on each plane using the \"\nstructuring_element\" parameter. Finally , return the newly\ncreated \"y_data\" array that holds the result of the operation.\nEnsure that the function uses the \"numpy\" and \"skimage.\nmorphology\" modules , as they are dependencies for the function.\n40 #def planewise_morphology_closing(x_data , structuring_element):\n41\n5. Necessary Only We use a mixture of our syntax solving algorithm and hand annotation\nto determine exactly which objects of the code are necessary for the function to execute,\nand then use them in place of the original context. Note that this is very similar to the\nenvironment used for testing the functions.\n1 Write a function with the following specs:\n43\n2 --specs begin here --\n3 #Write a function named \"planewise_morphology_closing\" that\naccepts two parameters , \"x_data\" and \"structuring_element \".\nInside the function , create a new variable named \"y_data\" that\nwill hold the result of the morphological closing operation.\nLoop through each item in \"x_data\" and perform the same\noperation on each. Do this by using the \"enumerate\" function to\nget both the index and plane for each item. Use the \"closing\"\nfunction from the \"skimage.morphology\" module to perform the\nmorphological closing operation on each plane using the \"\nstructuring_element\" parameter. Finally , return the newly\ncreated \"y_data\" array that holds the result of the operation.\nEnsure that the function uses the \"numpy\" and \"skimage.\nmorphology\" modules , as they are dependencies for the function.\n4 param: x_data (numpy array) - 3D array containing the input data.\n5 param: structuring_element (numpy array) - Structuring element\nused to perform the closing operation.\n6 return: y_data (numpy array) - 3D array containing the result of\nclosure operation on each plane in x_data.\n7 --specs end here --\n8 Note the function will be embedded in the following context\n9 --context begins here --\n10 import numpy\n11 import skimage.morphology\n12 import os\n13 <<insert solution here >>\n14 def main():\n15\nnumpy.random.seed(<|int;range =0 ,100| >)\n16\npixels = numpy.random.randint(2, size =(10, 10, 10))\n17\nstructuring_element = skimage.morphology.square (3)\n18\nprint(planewise_morphology_closing(pixels , structuring_element\n))\n19 if __name__ == \"__main__ \":\n20\nmain()\n21 --context ends here --\n22 Make sure to only generate the function and not any of the context\n. Make sure you are generating valid , runnable code. Begin your\nsolution with:\n23 def planewise_morphology_closing(x_data , structuring_element):\n24\n44\nJ\nMODEL ABLATION STUDY\nFor the ablation study, we considered two representative functions, one for Java and one for Python,\nand determined how the various models performed in generating the following functions. The Java\nfunction we selected is the following:\n1 public static String getReverseComplementedSequence(SAMRecord record , int\nstartIndex , int length) {\n2\nif (startIndex < 0) {\n3\nstartIndex = record.getReadLength () + startIndex;\n4\n}\n5\nbyte[] rangeBytes = Arrays.copyOfRange(record.getReadBases (),\nstartIndex , startIndex + length);\n6\nSequenceUtil.reverseComplement(rangeBytes);\n7\nreturn new String(rangeBytes);\n8 }\nand the (abridged) context that the function getReverseComplementedSequence had access to is\nthe following:\n1 import org.apache.commons.math3.distribution.HypergeometricDistribution;\n2 import java.text.DecimalFormat;\n3 import java.util .*;\n4 import java.util.function.Function;\n5 import htsjdk.samtools.util.SequenceUtil;\n6 import java.util.Objects;\n7\n8 class SAMRecord {\n9\npublic byte[] arr2;\n10\n11\npublic SAMRecord(byte[] arr) {\n12\narr2 = arr;\n13\n}\n14\n15\npublic byte[] getReadBases () {\n16\nreturn arr2;\n17\n}\n18\n19\npublic int getReadLength () {\n20\nreturn arr2.length;\n21\n}\n22 }\n23\n24 final class Utils {\n25\n/**\n26\n* Method returns reverse complemented sequence for the part of the\nrecord. Can work with 3\u2019 and 5\u2019 ends\n27\n* (if start index < 0, then it will found the index in the end of\nsequence by adding the length of record).\n28\n* @param record read from SAM file to process\n29\n* @param startIndex index where start the sequence\n30\n* @param length length of pert of sequence\n31\n* @return reverse complemented part of record\n32\n*/\n33\n<<insert solution here >>\n34\n35\npublic static String reverse(String string) {\n36\nreturn new StringBuffer(string).reverse ().toString ();\n37\n}\n38\n39\npublic static String complement(String string) {\n40\nfinal byte[] bases = htsjdk.samtools.util.StringUtil.\nstringToBytes(string);\n41\ncomplement(bases);\n42\nreturn htsjdk.samtools.util.StringUtil.bytesToString(bases);\n45\n43\n}\n44 }\nWe chose this as the representative Java function because it is highly bioinformatics related and the\nsolution will require an understanding of the context around it, namely the custom SAMRecord\nclass. Furthermore, the model will have to understand that there are many helpful utility functions\navailable in the Utils class that can be used to help complete the function, such as the reverse and\ncomplement functions.\nThe Python function that we selected for this ablation study is the following:\n1 def absolute_clonal(cnarr , ploidy , purity , is_reference_male ,\nis_sample_female\n2\n):\n3\n\"\"\" Calculate absolute copy number values from segment or bin log2\nratios .\"\"\"\n4\nabsolutes = np.zeros(len(cnarr), dtype=np.float_)\n5\nfor i, row in enumerate(cnarr):\n6\nref_copies , expect_copies = _reference_expect_copies(row.\nchromosome ,\n7\nploidy , is_sample_female , is_reference_male)\n8\nabsolutes[i] = _log2_ratio_to_absolute(row.log2 , ref_copies ,\n9\nexpect_copies , purity)\n10\nreturn absolutes\nand the (abridged) context that the function absolute_clonal had access to is the following:\n1 import random\n2 import hashlib\n3 import numpy as np\n4 import os\n5\n6 def _reference_expect_copies(chrom , ploidy , is_sample_female ,\nis_reference_male\n7\n):\n8\n\"\"\" Determine the number copies of a chromosome expected and in\nreference.\n9\nFor sex chromosomes , these values may not be the same ploidy as the\n10\nautosomes. The \"reference\" number is the chromosome \u2019s ploidy in the\n11\nCNVkit reference , while \"expect\" is the chromosome \u2019s neutral ploidy\nin the\n12\ngiven sample , based on the specified sex of each. E.g., given a\nfemale\n13\nsample and a male reference , on chromosome X the \"reference\" value is\n1 but\n14\n\"expect\" is 2.\n15\nReturns\n16\n-------\n17\ntuple\n18\nA pair of integers: number of copies in the reference , and\nexpected in\n19\nthe sample.\n20\n\"\"\"\n21\nchrom = chrom.lower()\n22\nif chrom in [\u2019chrx \u2019, \u2019x\u2019]:\n23\nref_copies = ploidy // 2 if is_reference_male else ploidy\n24\nexp_copies = ploidy if is_sample_female else ploidy // 2\n25\nelif chrom in [\u2019chry \u2019, \u2019y\u2019]:\n26\nref_copies = ploidy // 2\n27\nexp_copies = 0 if is_sample_female else ploidy // 2\n28\nelse:\n29\nref_copies = exp_copies = ploidy\n30\nreturn ref_copies , exp_copies\n31\n46\n32 def _log2_ratio_to_absolute_pure(log2_ratio , ref_copies):\n33\n\"\"\" Transform a log2 ratio to absolute linear scale (for a pure sample\n).\n34\nPurity adjustment is skipped. This is appropriate if the sample is\ngermline\n35\nor if scaling for tumor heterogeneity was done beforehand.\n36\n.. math :: n = r*2^v\n37\n\"\"\"\n38\nncopies = ref_copies * 2 ** log2_ratio\n39\nreturn ncopies\n40\n41 def _log2_ratio_to_absolute(log2_ratio , ref_copies , expect_copies , purity\n=None\n42\n):\n43\n\"\"\" Transform a log2 ratio to absolute linear scale (for an impure\nsample).\n44\nDoes not round to an integer absolute value here.\n45\nMath::\n46\nlog2_ratio = log2(ncopies / ploidy)\n47\n2^ log2_ratio = ncopies / ploidy\n48\nncopies = ploidy * 2^ log2_ratio\n49\nWith rescaling for purity ::\n50\nlet v = log2 ratio value , p = tumor purity ,\n51\nr = reference ploidy , x = expected ploidy ,\n52\nn = tumor ploidy (\" ncopies\" above);\n53\nv = log_2(p*n/r + (1-p)*x/r)\n54\n2^v = p*n/r + (1-p)*x/r\n55\nn*p/r = 2^v - (1-p)*x/r\n56\nn = (r*2^v - x*(1-p)) / p\n57\nIf purity adjustment is skipped (p=1; e.g. if germline or if scaling\nfor\n58\nheterogeneity was done beforehand)::\n59\nn = r*2^v\n60\n\"\"\"\n61\nif purity and purity < 1.0:\n62\nncopies = (ref_copies * 2 ** log2_ratio - expect_copies * (1 -\npurity)\n63\n) / purity\n64\nelse:\n65\nncopies = _log2_ratio_to_absolute_pure(log2_ratio , ref_copies)\n66\nreturn ncopies\n67\n68 <<insert solution here >>\nWe chose this as the representative Python function because, like the Java function that we chose, it is\nboth highly bioinformatics related and the solution will require an understanding of the context around\nit, namely the custom _reference_expect_copies and _log2_ratio_to_absolute functions, as\nwell as their problem, will require the use of some of the general imports such as the numpy package.\n1. InCoder: Similar to many other models, InCoder has very poor Java performance, due to the\nfact that it was trained and fine-tuned on a Python corpus. Therefore, InCoder achieves 0%\nPass@K for the Java prompts. The main issue that InCoder runs into is that for Java, it seems\nto struggle to understand the objective in the prompt and fails to generate the actual function;\ninstead, it seems to regurgitate the prompt, which will obviously result in a fail. For instance\nhere is a sample InCoder output for the function getReverseComplementedSequence:\n1\n# Do not write any comments in your code.\n2 #This is in java\n3 #Write a function called \"getReverseComplementedSequence\" that\ntakes in a SAMRecord object , an integer called \"startIndex\",\nand an integer called \"length \". The function should check if\nstartIndex is less than 0, and if it is , set startIndex to the\nSAMRecord object \u2019s read length plus startIndex. The function\nshould then create a byte array called \"rangeBytes\" that is a\n47\ncopy of a portion of the SAMRecord object \u2019s read bases starting\nfrom the startIndex and ending at startIndex + length. The\nfunction should then call SequenceUtil.reverseComplement on\nrangeBytes , and finally return a new String created from the\nrangeBytes byte array.\n4 #Here are the imports:\n5 #import org.apache.commons.math3.distribution.\nHypergeometricDistribution;\n6 #import java.text.DecimalFormat;\n7 #import java.util .*;\n8 #import java.util.function.Function;\n9 #import htsjdk.samtools.util.SequenceUtil;\n10 #import java.util.Objects;\n11 #Here are the class declarations:\n12 #class SAMRecord {\n13 #\npublic byte[] arr2;\n14 #\npublic SAMRecord(byte[] arr) {\n15 #\narr2 = arr;\n16 #\n}\n17 #\npublic byte[] getReadBases () {\n18 #\nreturn arr2;\n19 #\n}\n20 #\npublic int getReadLength () {\n21 #\nreturn arr2.length;\n22 #\n}\n23 #}\n24 #class Region {\n25 #\n/**\n26 #\n* Chromosome name\n27 #\n*/\n28 #\npublic final String chr;\n29 #\n/**\n30 #\n* Region start position\n31 #\n*/\n32 #\npublic final int start;\n33 #\n/**\n34 #\n* Region end position\n35 #\n*/\n36\nAlthough InCoder performs comparatively better on Python, it still struggles greatly to\nunderstand the objective of the prompt and actually generate the function. Instead, for\nPython code, the InCoder models seem to simply regurgitate the summary of the Python\nfunction that can be found in the prompt. For instance, here is a sample InCoder output for\nthe function absolute_clonal:\n1\n#\nsummary: Returns the number of copies of a chromosome\nin reference and in sample.\n2 #\nparam: chrom (string) - the chromosome to be evaluated\n3 #\nparam: ploidy (int) - the ploidy of the chromosome\n4 #\nparam: is_reference_male (bool) - indicates if the reference\nis male or not\n5 #\nreturn: tuple - a pair of integers indicating the number of\ncopies in the reference and in the sample.\n6 #def _log2_ratio_to_absolute(cnarr , ref_copies , expect_copies ,\npurity):\n7 #\nsummary: Converts log2 ratio values into absolute copy number\nvalues.\n8 #\nparam: cnarr (array -like) - Copy number segments or bins\n9 #\nparam: ref_copies (array -like) - An array of integers\nindicating the number of copies in the reference for each\nsegment or bin.\n10 #\nparam: expect_copies (array -like) - An array of integers\nindicating the number of copies in the expected for each\nsegment or bin.\n48\n11 #\nparam: purity (float) - estimated tumor pur\n12\nOverall, it seems that InCoder struggles to understand longer and more complex prompts,\nand t struggles to understand the objective of the prompts. Often it simply defaults to\nregurgitating parts of the prompt that it has encountered, without generating the desired\nfunction.\n2. SantaCoder:\nSimilar to many other models, SantaCoder also has poor Java performance, due to the fact\nthat it was trained and fine-tuned on a Python corpus. Therefore, SantaCoder achieves 0%\nPass@K for the Java prompts. The main issue that SantaCoder runs into is that Java seems\nto struggle to understand the objective in the prompt and fails to generate the full function,\nonly generating parts of the function. This may also be due to the smaller context limit of\nSantaCoder, resulting in it being unable to fully generate the functions. For instance, here is\na sample SantaCoder output for the function getReverseComplementSequence:\n1\n#\ndouble x = perc;\n2 #\nint a = (int) x;\n3 #\nint b = (int) (x - a);\n4 #\nint c = (int) (b - a);\n5 #\nif(c==0){\n6 #\nreturn 0;\n7 #\n}else if(c==1){\n8 #\nreturn 1;\n9 #\n}else{\n10 #\nreturn 2;\n11 #\n}\n12 #\n// return getColor(value , perc);\n13\n14 #\nint c1 = (int) ((perc - a)/b);\n15 #\nint c2 = (int) ((perc - a - b)/a);\n16 #\nreturn 1 + c1 * 3 + c2 * 5;\n17\n18 #\nint a1 = (int) (c1 / 2);\n19 #\nint a2 = (int) ((c1 - a1) / 2);\n20 #\nint a3 = (int) (c2 / 2);\n21 #\nint a4 = (int) ((c2 - a3) / 2);\n22 #\nint a5 = (int) (c1 +\n23\nAlthough SantaCoder performs comparatively better on Python, especially considering its\nsmall model size, it still struggles greatly to generate many of the functions. Often due\nto the small context size of the SantaCoder model, thus for more complicated functions,\nSantaCoder is unable to generate the model. Overall, it seems that SantaCoder struggles\nto understand the longer and more complex prompts, and it often struggles to understand\nthe objective of the prompts. Furthermore, the small context limit of SantaCoder limits its\nability to generate more complex and more context-rich code.\n3. StarCoder:\nSimilar to many of the other models, StarCoder has very poor Java performance, due\nto the fact that it was trained and finetuned on a Java corpus. Therefore, StarCoder\nachieves 0% Pass@K for the Java prompts. However, unlike some of the other mod-\nels, it seems to understand the objective of the prompt and uses the context appropriately.\nFor instance, using the reverseComplement helper function correctly when generating the\ngetReverseComplementedSequence function, as can be seen below:\n1\ndef getReverseComplementedSequence(samRecord , startIndex ,\nlength):\n2\nif startIndex < 0:\n3\nstartIndex = samRecord.getReadLength () + startIndex\n4\nrangeBytes = samRecord.getReadBases ().getBytes ()[startIndex :\nstartIndex + length]\n5\nreverseComplemented = SequenceUtil.reverseComplement(\nrangeBytes)\n49\n6\nreturn reverseComplemented.toString ()\n7\nHowever, StarCoder, being finetuned on Python, seems to have a hard time generating\nproper Java syntax, and instead generates a Python function. Although StarCoder performs\ncomparatively better on Python, it still struggles greatly to generate many of the functions.\nPython seems to understand the objective in the prompt and generates proper syntax, however,\nsome of its logic is incorrect, resulting in some failed tests. For instance, here is a sample\nStarCoder output for the function absolute_clonal:\n1\n#Write your function here!\n2 def absolute_clonal(cnarr , ploidy , purity , is_reference_male ,\nis_sample_female):\n3\nabsolutes = np.zeros(len(cnarr), dtype=np.float_)\n4\nfor i, row in enumerate(cnarr):\n5\nchromosome , log2_value = row\n6\nref_copies , expect_copies = _reference_expect_copies(\nchromosome , ploidy , is_sample_female , is_reference_male)\n7\nabsolute_value = _log2_ratio_to_absolute(log2_value ,\nref_copies , expect_copies , purity)\n8\nabsolutes[i] = absolute_value\n9\nreturn absolutes\n10\n4. StarCoder+: Unlike many of the other models, StarCoder+ has nonzero Pass@K perfor-\nmance on Java. This is likely because StarCoder+ is trained and finetuned on a larger corpus\nthat consists of more languages than just Python. It seems to understand the objective of\nthe prompt and uses the context appropriately, for instance using the reverseComplement\nhelper function correctly when generating the getReverseComplementedSequence func-\ntion, as can be seen below:\n1\nimport htsjdk.samtools.util.SequenceUtil;\n2 public class getReverseComplementedSequence\n3 {\n4 public static String getReverseComplementedSequence(SAMRecord\nsamRecord , int startIndex , int length)\n5 {\n6 if(startIndex < 0)\n7 {\n8 startIndex = samRecord.getReadLength () + startIndex;\n9 }\n10 byte[] rangeBytes = new byte[length ];\n11 samRecord.getReadBases(rangeBytes , startIndex , length);\n12 byte[] rcBytes = SequenceUtil.reverseComplement(rangeBytes);\n13 return new String(rcBytes);\n14 }\n15 }\n16\nUnlike StarCoder, it seems to understand and be able to generate Java syntax.\n5. CodeGen: The performance of CodeGen1 was not great. This could be due to a variety of\nfactors, including the fact that CodeGen models are not intended for full multi-line function\ngeneration, or the fact that the dataset used to train CodeGen models does not contain much\nJava code. This is demonstrated in some of the generations, as CodeGen ended up generating\nPython code, despite us mentioning that this prompt is in Java, and using Java code in the\ncontext. Sometimes, it also generates the comment again, such as below:\n1 #Note: This is not a string method.\n2\n6. CodeGen2: CodeGen2 outperformed CodeGen as expected in our normal test cases; how-\never, it was unexpectedly slightly outperformed in the Rosalind benchmark.\n7. GPT-3.5: GPT clearly performs the best out of all models tested, on both Java and Python\nand across all prompt types. The version we used on Azure had an 8k context size, so it had\n50\nan advantage over some of the models due to being able to understand more of the context.\nFurthermore, the fact that some of the function descriptions were generated by GPT might\nmean that it had an easier time generating code from its own description. Furthermore, given\nthe size of the model, it is possible that some of the functions were memorized, although we\ntried to combat this by changing the function signature slightly in some cases. Here is an\nexample of the GPT output:\n1 def FindLocalPeaks(signal):\n2\npeakLoc = []\n3\nmaxSuspect = None\n4\nminSuspect = None\n5\nfor i in range(1, len(signal) -1):\n6\nif signal[i] > 0:\n7\nif signal[i] > signal[i-1] and signal[i] == signal[i\n+1]:\n8\nmaxSuspect = i\n9\nelif signal[i] == signal[i-1] and signal[i] > signal[i\n+1]:\n10\nif maxSuspect is not None:\n11\npeakLoc.append(maxSuspect)\n12\nmaxSuspect = None\n13\nelif signal[i] > signal[i-1] and signal[i] > signal[i\n+1]:\n14\npeakLoc.append(i)\n15\nelif signal[i] < 0:\n16\nif signal[i] < signal[i-1] and signal[i] == signal[i\n+1]:\n17\nminSuspect = i\n18\nelif signal[i] == signal[i-1] and signal[i] < signal[i\n+1]:\n19\nif minSuspect is not None:\n20\npeakLoc.append(minSuspect)\n21\nminSuspect = None\n22\nelif signal[i] < signal[i-1] and signal[i] < signal[i\n+1]:\n23\npeakLoc.append(i)\n24\nreturn np.array(peakLoc , dtype=int)\n25\nIt is also important to note that in every prompt, GPT-3.5 generated usable, cleaner code\nwithout comments as directed. Therefore, most test cases were run without syntax errors,\nbut mostly logic errors.\nNext, we performed a horizontal analysis on a single prompt to compare the outputs of each of the\nmodels with each other in order to understand the differences between the models and gain insight\ninto what parts of the prompts the various models seem to understand. We applied the following\nproblem for the horizontal analysis:\n1 @property\n2 def outlines(self):\n3\n\"\"\"Get a mask of all the points on the border of objects \"\"\"\n4\nif self._outlines is None:\n5\nfor i, labels in enumerate(self.labels):\n6\nif i == 0:\n7\nself._outlines = centrosome.outline.outline(labels) != 0\n8\nelse:\n9\nself._outlines |= centrosome.outline.outline(labels) != 0\n10\nif self.line_width is not None and self.line_width > 1:\n11\nhw = float(self.line_width) / 2\n12\nd = scipy.ndimage.distance_transform_edt (~self._outlines)\n13\ndti , dtj = numpy.where ((d < hw + 0.5) & ~self._outlines)\n14\nself._outlines = self._outlines.astype(numpy.float32)\n15\nself._outlines[dti , dtj] = numpy.minimum(1, hw + 0.5 - d[dti ,\ndtj])\n16\nreturn self._outlines.astype(numpy.float32)\n51\nThe reasons we chose this function for the horizontal analysis are the following:\n1. It is a Python function, and all of the model\u2019s generally seem to be fine-tuned on Python code\nand thus will perform better on Python code; therefore, it would be a more fair comparison\nto use a Python function for this analysis.\n2. Furthermore, this is also a relatively tricky function to generate as it has many moving parts\nand relies on many different levels of understanding of the context. First, it is important\nto note that this has a @property decorator and is a function that is part of a class; thus,\nthe models must understand this and generate the @property decorator accordingly. Fur-\nthermore, the model must understand and correctly use the self._outlines, self._line_width,\nand self.labels instance variables. Finally, the model must understand and use correctly the\nhighly bioinformatics-specific external library that is centrosome.\nThe prompt that we used for the horizontal analysis is the following \u201cSummary Only\u201d prompt:\n1 #This is in python\n2 #Write a function called \"get_border_mask\" that takes in a class object\nas an argument and returns a binary mask of all the points on the\nborder of objects. The function should check if the instance variable\n\"_outlines\" is None and if it is , then use the labels attribute of\nthe class object to generate the outlines mask using \"centrosome.\noutline.outline(labels)\" method. If the \"_outlines\" variable is not\nNone , then it should iterate over \"centrosome.outline.outline(labels)\n\" for each i, and update the \"_outlines\" variable using the bitwise\nOR operator with the new mask. If the line_width attribute of the\nclass object is not None and greater than 1, the function should\ncalculate the distance transform using the \"~self._outlines\" mask and\ncompute the binary mask for borders with the given line width (hw)\nusing numpy.where(). Finally , the function should return the mask as\na float32 numpy array. Make sure to import the necessary dependencies\nincluding \"centrosome.outline\" and \"numpy\" and also initialize the \"\n_outlines\" variable in the class.\n3 #\n4 #The function is located in the class OutlinesMixin\n5 #\n6 #@property\n7 #def get_border_mask(self):\nWe chose the \u201cSummary Only\u201d format of the prompt because generally, the models performed best\nwhen the prompt was written in the \u201cSummary Only\u201d format. The outputs of the various models for\nthis prompt are as follows:\n1. InCoder:\n1 def get_border_mask(self):\n2\n#If outlines is none , generate outlines using centrosome.\noutline.outline(labels) method and set outlines to the\ngenerated mask.\n3\nif self.outlines is None:\n4\nself.outlines = self.outline.outline(labels=self.labels)\n5\n6\n# Get the mask using numpy\n7\nborder_mask = np.where(self.outlines > 0, 1, 0 )\n8\n9\n# Check if line width is greater than 1\n10\nif self.line_width > 1:\n11\n# Get distance transform\n12\ndt = np.abs(self.outlines - border_mask) * 255\n13\ndt[dt==0] = 255\n14\n15\n# Use np.where to create border mask\n16\nborder_mask = np.where(\n17\ndt > 0,\n52\n18\n1,\n19\n0\n20\n)\n21\n22\n# Return border mask as float32 numpy array\n23\nreturn border_mask.astype(np.float32)\n24\n2. SantaCoder:\n1 def get_border_mask(self):\n2\nif self._outlines is None:\n3\nself._outlines = centrosome.outline.outline(self.labels)\n4\noutlines_mask = np.where(self._outlines , self._outlines , self.\nlabels)\n5\n#outlines_mask = np.where(self._outlines , self._outlines , self\n._outlines)\n6\n#outlines_mask = np.where(outlines_mask > 0, outlines_mask , 0)\n7\nreturn outlines_mask\n8\n3. StarCoder:\n1 def get_border_mask(self):\n2\nif self._outlines is None:\n3\nself._outlines = centrosome.outline.outline(self.labels)\n4\nelse:\n5\nfor i in range(self._outlines.shape [0]):\n6\nself._outlines[i] = self._outlines[i] | centrosome.\noutline.outline(self.labels[i])\n7\nif self.line_width is not None and self.line_width > 1:\n8\ndist_trans = ndimage.distance_transform_edt (~self.\n_outlines)\n9\nborder_mask = np.where(dist_trans <= self.line_width , 1,\n0)\n10\nelse:\n11\nborder_mask = self._outlines\n12\nreturn border_mask.astype(np.float32)\n13\n4. StarCoder+:\n1 @property\n2 def outlines(self):\n3\nif not hasattr(self , \u2019_outlines \u2019):\n4\nself._outlines = centrosome.outline.outline(self.labels)\n5\nreturn self._outlines\n6\nelse:\n7\nfor i in range(len(self.labels)):\n8\nself._outlines[i] = centrosome.outline.outline(self.\nlabels)\n9\nself._outlines = self._outlines | self._outlines\n10\nreturn self._outlines\n11\n5. InstructCodeT5+:\n1 def get_border_mask(self ,labels):\n2\n3\nif self._outlines is None:\n4\nself._outlines = outline(labels)\n5\n6\nmask = self._outlines\n7\n8\nif self.line_width is not None:\n9\nhw = self.line_width\n53\n10\n11\nif self.line_width >1:\n12\n6. CodeGen:\n1 # Do not write any comments in your code.\n2 #This is in python\n3 #Write a function called \"get_border_mask\" that takes in a class\nobject as an argument and returns a binary mask of all the\npoints on the border of objects. The function should check if\nthe instance variable \"_outlines\" is None and if it is , then\nuse the labels attribute of the class object to generate the\noutlines mask using \"centrosome.outline.outline(labels)\" method\n. If the \"_outlines\" variable is not None , then it should\niterate over \"centrosome.outline.outline(labels)\" for each i,\nand update the \"_outlines\" variable using the bitwise OR\noperator with the new mask. If the line_width attribute of the\nclass object is not None and greater than 1, the function\nshould calculate the distance transform using the \"~self.\n_outlines\" mask and compute the binary mask for borders with\nthe given line width (hw) using numpy.where (). Finally , the\nfunction should return the mask as a float32 numpy array. Make\nsure to import the necessary dependencies including \"centrosome\n.outline\" and \"numpy\" and also initialize the \"_outlines\"\nvariable in the class.\n4 #\n5 #The function is located in the class OutlinesMixin\n6 #\n7 #@property\n8 #def get_border_mask(self):\n9\n7. CodeGen2:\n1 def get_border_mask(self):\n2\n\"\"\"\n3\nComputes the mask for the outlines of a morphology.\n4\n5\nParameters\n6\n----------\n7\nself : morphology object\n8\n9\nReturns\n10\n-------\n11\nborder_mask :\n12\n8. GPT-3.5:\n1 @property\n2 def get_border_mask(self):\n3\nif self._outlines is None:\n4\nself._outlines = outline(self.labels)\n5\nelse:\n6\nfor i in range(1, np.max(self.labels)):\n7\nself._outlines |= outline(self.labels == i)\n8\n9\nif self.line_width is not None and self.line_width > 1:\n10\ndt = np.array(self._outlines , dtype=np.uint8)\n11\ndt = np.where(dt < 0.5, 1, 0)\n12\ndt = ndimage.distance_transform_edt(dt)\n13\ndt = (dt < self.line_width).astype(float)\n14\nreturn dt\n15\nelse:\n16\nreturn np.array(self._outlines , dtype=np.float32)\n17\n54\n9. GPT-4:\n1 def outlines(self):\n2\nif self._outlines is None:\n3\nself._outlines = centrosome.outline.outline(self.labels)\n4\nelse:\n5\nfor i in range(len(self.labels)):\n6\nself._outlines |= centrosome.outline.outline(self.\nlabels[i])\n7\n8\nif self.line_width is not None and self.line_width > 1:\n9\ndistances = scipy.ndimage.distance_transform_edt (~self.\n_outlines)\n10\nhw = self.line_width / 2\n11\nself._outlines = numpy.where(distances <= hw , 1, 0)\n12\n13\nreturn self._outlines.astype(numpy.float32)\n14\nIn comparing the various model\u2019s outputs to the chosen prompt, it is clear that there are various\nlevels of understanding of the prompt between the models. By far the worst performing models\nare the InstructCodeT5+, CodeGen, and CodeGen2 models. All of these models failed to generate\na full function. This is likely due to these models being trained to do one-line completions rather\nthan full-function generation. InstructCodeT5+ did the best out of the three, as it at least generated\npart of a function and showed an understanding of the self._outlines instance variable unlike both\nthe CodeGen and CodeGen2 models. However, InstructCodeT5+ also got the function signature\nwrong, showing no understanding in the prompt of what the structure of the function it was trying to\ngenerate was. The CodeGen2 model did not generate any function body; however, it did generate the\nfunction signature correctly and generated part of a useful docstring for the function. Out of these\nthree models, the original CodeGen model performed by far the worst, as it simply regurgitated the\nprompt, not generating any new code.\nFor the remaining five models, there were different levels of understanding.\nThe two mod-\nels that demonstrated the deepest levels of understanding of the prompt were the StarCoder+\nand GPT-3.5 models, as they were the only two models to recognize that the get_border_mask\nfunction has an @property function decorator.\nFurthermore, they both showed some under-\nstanding of the external function calls that were necessary to complete the function, with Star-\nCoder+ calling the centrosome.outline.outline function correctly and GPT-3.5 using the\nndimage.distance_transform_edt function, although not completely correctly, as the correct\nfunction call would be scipy.ndimage.distance_transform_edt. However, the logic that each of\nthese models use to perform the get_border_mask function is correct, with GPT-3.5 getting closer to\nthe intended logic. For instance, GPT-3.5 makes the self.line_width is None check.\nThe remaining three models not yet mentioned in this analysis are InCoder, SantaCoder, and Star-\nCoder. These three models generated complete functions unlike InstructCodeT5+, CodeGen, and\nCodeGen2; however, they did not include the @property function decorator, unlike the StarCoder+\nand GPT-3.5 models. Out of these three \u201cmiddle-of-the-pack\" models, StarCoder performs especially\nwell, as it shows understanding of the three necessary instance variables, namely self._outlines,\nself.labels, and self.line_width, and uses both the centrosome.outline.outline and\nndimage.distance_transform_edt external function calls. However, like GPT-3.5, it does not\nquite make the ndimage external call correctly. Furthermore, the structural logic of StarCoder\u2019s\ncode is similar to the golden code, on the same level as GPT-3.5. As for InCoder and SantaCoder,\nalthough they both generated full functions, their functions were somewhat off in their structural\nlogic. SantaCoder performs decently, using the centrosome.outline.outline external package\ncorrectly; however, the function it generated is overly simple and does not meet the prompt descrip-\ntion guidelines. As for InCoder, it uses a nonexistent self.outlines instance variable, instead\nof the intended self._outlines instance variable. Furthermore, it calls the nonexistent function\nself.outline.outline instead of the intended centrosome.outline.outline function. By con-\ntrast, InCoder writes a more involved function than SantaCoder which more closely mirrors the\nguidelines provided by the prompt.\n55\nK\nPROMPT LENGTH STUDY\nAs shown by the following scatterplots, it seems that on models with an average Pass@K score of\nat least 2%, there is an inverse relationship between the number of tokens in the prompt and the\nPass@K score. Furthermore, for models such as SantaCoder and GPT, the performance fell sharply\nafter around 500 tokens. This could be due to the massive amount of context \u201cconfusing\u201d the models.\nWhen we look at the Necessary Only prompts, a similar trend occurs for longer contexts, but the\nphenomenon is not as pronounced, as on average these prompts are significantly shorter.\nFigure 8: The scatterplots of the correlation of Pass@K and tokens of prompt. Statistics for GPT-4.\n56\nFigure 9: Statistics for Starcoderplus\nFigure 10: Statistics for Incoder\n57\nFigure 11: Statistics for CodeGen\nFigure 12: Statistics for CodeGen2\n58\nFigure 13: Statistics for InstructCodeT5+\nFigure 14: Statistics for Starcoder\n59\nFigure 15: Statistics for Santacoder\nFigure 16: Statistics for GPT-3.5\n60\nL\nTESTING FRAMEWORK\nAn outline of our testing frame is as follows:\n1. Preprocessing: Each generated output is cleaned and minor corrections are applied in accordance\nwith the above steps. We append a call to the main() function at the end of the file.\n2. Container Preparation: The output from LLMs, alongside the environment files and sample test\ncases, are copied into the container.\n3. Fuzzing Execution: The test cases are executed with a memory cap of 7GB and a time limit of\n60 seconds. A test is considered passed if the actual output precisely matches the expected output,\nfor all cases tested.\n4. Analysis: We gather and analyze the results from these executions.\nThrough this approach, we provide a secure and efficient testing framework, promising robustness in\nthe evaluation of generated code.\nL.1\nMOTIVATION BEHIND FUZZ TESTING\nWe decided to utilize concepts from fuzz testing, as fuzz testing is widely used in the industry to\ncapture bugs, crashes, security vulnerabilities, etc. in functions. However, in these cases, they do\nnot have access to a \"correct\" version of the function; instead, they are merely creating inputs to\nintentionally try to crash the program, find out of bounds memory accesses, etc. Our situation is\nunique because we have the \"golden code\", or the ground truth version of the code, so given an\ninput, we definitely know what the expected output should be, which is not something that\u2019s usually\navailable in typical fuzz testing frameworks. Therefore, our situation could be considered a mixture\nof both unit testing and fuzz testing.\nGiven this requirement, and the goal of large-scale prompt generation, we decided to implement\nour own framework. We set out to accomplish two things: make the annotation process a lot easier\nfor human editors, and support our feature set that combines both elements from unit testing and\nelements from fuzz testing. We believe that our resulting pipeline is more intuitive than piecing\ntogether other testing frameworks, and in our annotation process it proved to make things efficient,\nenabling larger-scale annotation, as the goal of the paper.\nFurthermore, note that while handwritten test cases would likely target edge cases of a program (eg.\nbranch coverage, conditional coverage), the probability of our fuzz testing framework to hit all of the\nsame edge cases is high given 1000 iterations of randomly generated inputs. This means that we can\nsave a significant amount of time building the dataset, as we only need to write an outline of a test\ncase, and let the framework handle the rest. Therefore, we can think of the framework as thousands\nof \"random unit tests,\" with a high probability that these unit tests would include handwritten test\ncases, if we had written them.\nIn terms of variable generation, we replace the <|var_type;parameter|> syntax with random values\neach iteration, for an unlimited number of iterations. These parameters are modifiable, and we\nimplemented this system to be flexible, so that we can target specific scopes for fuzz testing. We\ncheck correctness by substituting the exact same variables in the original code, and checking if the\noutputs of the two functions match. This indicates identical functionality with the original code.\nHere is an example of random integer and numpy array generation:\n1 import numpy\n2 import skimage.morphology\n3 import os\n4 <<insert solution here >>\n5 def main():\n6\nnumpy.random.seed(<|int;range =0 ,100| >)\n7\nlabels = numpy.random.randint(2, size=(3, 3))\n8\ndiameter = <|int;range =2,10|>\n9\nprint(fill_object_holes(labels , diameter))\n10 if __name__ == \"__main__ \":\n11\nmain()\n61\nExample of random string generation:\n1 import random\n2 [IMPORTS REDACTED FOR CONCISENESS]\n3 import warnings\n4 from textwrap import wrap\n5 import string\n6 import zlib\n7 import io\n8 from os.path import isfile\n9 class GenipeError(Exception):\n10\npass\n11 _CHECK_STRING = b\u2019GENIPE INDEX FILE \u2019\n12 def dosage_from_probs(homo_probs , hetero_probs , scale =2):\n13\n\"\"\" Computes dosage from probability matrix (for the minor allele).\n14\nArgs:\n15\nhomo_probs (numpy.array): the probabilities for the homozygous\ngenotype\n16\nhetero_probs (numpy.array): the probabilities for the\nheterozygous\n17\ngenotype\n18\nscale (int): the scale value\n19\nReturns:\n20\nnumpy.array: the dosage computed from the probabilities\n21\n\"\"\"\n22\nreturn (homo_probs + (hetero_probs / 2)) * scale\n23 <<insert solution here >>\n24 def main():\n25\nnp.random.seed(<|int;range =0 ,100| >)\n26\nprob_matrix = np.random.rand(10, 10)\n27\na1 = <|string|>\n28\na2 = <|string|>\n29\nprint(maf_dosage_from_probs(prob_matrix , a1 , a2))\n30 if __name__ == \"__main__ \":\n31\nmain()\n<|int|> denotes an integer. If left without parameters, then in one iteration of the program, this will be\nreplaced with a random integer between INT_MIN and INT_MAX before compile time (or in this\ncase, before the Python file is executed). There are parameters that can be passed in, that include\nrange, even/odd, etc. Similarly, for <|string|> this generates a random ASCII string of any type. It can\nbe further narrowed down into ASCII strings only, lowercase only, specific characters only, etc. by\npassing in the relevant parameters. These random inserted values can be manipulated to become part\nof a larger data structure, for example, a Numpy array, or a mock Python object.\nWhen these files are executed, we replace \u00abinsert solution here\u00bb with the golden code on one iteration,\nand the error corrected generated code on a corresponding iteration. The fuzzing framework is\ndesigned so that the same inputs will be passed to this pair of iterations, meaning that we should be\ngetting the same output (none of the functions have a non-deterministic component to them). There-\nfore this supports one aspect of the \"secure\" testing framework, as we have created an environment\nwhere all else is equal, except for the generated/golden code.\n62\nM\nPROMPT LENGTH ABLATION STUDY\nWe have conducted additional experiments to illustrate the effect of different prompts.\nAs a result, our study revealed that more detailed text descriptions improve code generation, par-\nticularly for Python, but they have less impact on Java. However, irrespective of the level of detail,\nthe core structure of the generated code remains similar. Hence, ambiguity in descriptions will not\ndramatically influence the outcome.\nSpecifically, in response to concerns about the ambiguity of the Python text descriptions, we conducted\na study to examine the effects of ambiguous prompts on code generation. Using ChatGPT, we\ngenerated both \u201cshort\u201d and \u201clong\u201d summaries, with the former being more concise and potentially\nomitting some details, and the latter providing more detail to reduce ambiguity.\nFor the \u201cshort\u201d summaries, we used the following prompt:\n1 Please write a clear and succinct prompt in one paragraph directing the\nuser to write the following function. In the prompt make sure to\ninclude sufficient details on what the following function does.\nFor the \u201clong\u201d summaries, we used the following prompt:\n1 Please write a highly detailed prompt in one paragraph directing the user\nto write the following function. In the one paragraph prompt make\nsure to include all the details on what the following function does.\nIf you are unsure of some. of the variable and function references\nmake an educated guess.\nPassing the above prompts into ChatGPT, we generated \u201cshort\u201d and \u201clong\u201d summaries. Then, we\nused these summaries to construct \u201cshort\u201d and \u201clong\u201d prompts.\nWe calculated Pass@K results on ChatGPT generations for both \u201cshort\u201d and \u201clong\u201d prompts. The\nresults are as follows:\nPrompt\nJava\nPython\nPass@1\nPass@5\nPass@10\nPass@20\nPass@1\nPass@5\nPass@10\nPass@20\nShort Summary\n34.700\n42.313\n43.000\n43.800\n25.960\n36.371\n39.825\n42.484\nLong Summary\n36.800\n46.412\n49.842\n54.000\n38.567\n52.419\n56.439\n60.510\nBelow are examples of \u201cshort\u201d and \u201clong\u201d prompts:\n1 SHORT PROMPT EXAMPLE\n2 #This is in python\n3 # Write a Python function called \"UnifyLevels\" that takes in three\nparameters: \"baseLevel\" (a constant array of integers), \"addonLevel\"\n(a constant array of integers), and \"windowSize\" (an integer). The\nfunction merges the two lists of breakpoints , but drops addonLevel\nvalues that are too close to baseLevel values. The function then\nreturns the merged list of breakpoints as an array of integers. If\nthe \"addonLevel\" list is empty , the function should return the \"\nbaseLevel\" as is.\n4 # Return only the code in the completion. I don \u2019t want any other comments\n. Don \u2019t say \"here is your code\" or similar remarks.\n5 # def UnifyLevels(baseLevel , addonLevel , windowSize):\n1 LONG PROMPT EXAMPLE\n2 #This is in python\n3 # Write a function called \"UnifyLevels\" that takes in three parameters: \"\nbaseLevel\" (a list of integers), \"addonLevel\" (a list of integers),\nand \"windowSize\" (an integer). The purpose of this function is to\nmerge the two lists of breakpoints (baseLevel and addonLevel), but\ndrop any values from addonLevel that are too close to values in\nbaseLevel. The function should first check if addonLevel is empty. If\nit is, the function should simply return baseLevel. Otherwise , the\n63\nfunction should create an empty list called \"joinedLevel\" and an\ninteger variable called \"addon_idx\" initialized to 0. The function\nshould then iterate over each element in baseLevel using a for loop.\nInside the loop , there should be a while loop that continues as long\nas addon_idx is less than the length of addonLevel. Within the while\nloop , the function should retrieve the element at index addon_idx\nfrom addonLevel and assign it to a variable called \"addon_elem \". Next\n, there should be three cases to handle:\n4 # - If addon_elem is less than base_elem minus windowSize , the function\nshould append addon_elem to joinedLevel , increment addon_idx by 1,\nand continue to the next iteration of the while loop.\n5 # - If addon_elem is between base_elem minus windowSize and base_elem\nplus windowSize (inclusive), the function should increment addon_idx\nby 1 and continue to the next iteration of the while loop.\n6 # - If addon_elem is greater than base_elem plus windowSize , the function\nshould assert that base_elem plus windowSize is less than addon_elem\n, break out of the while loop , and move to the next iteration of the\nfor loop.\n7 After each iteration of the for loop , the function should append\nbase_elem to joinedLevel. Once the for loop is complete , the function\nshould check if there are any remaining elements in addonLevel\nstarting from addon_idx. If there are , the function should extend\njoinedLevel with those elements. Finally , the function should return\na numpy array containing the sorted elements of joinedLevel , with the\ndata type set to np.int_.\n8 # Return only the code in the completion. I don \u2019t want any other comments\n. Don \u2019t say \"here is your code\" or similar remarks.\n9 # def UnifyLevels(baseLevel , addonLevel , windowSize):\nAccording to our study, using more detailed prompts improved the performance, especially for Python\ngenerations. Python Pass@1 performance notably increased from 26.0% to 38.6% with more detailed\nprompts. However, the improvement for Java was less pronounced, with only a minor increase\nfrom 34.7% to 36.8%. Despite the difference in performance, the structural similarity between code\ngenerated from both short and long prompts remained high. Therefore, even if a prompt is ambiguous\nor trivial, it will not significantly affect the fundamental structure of the generated code. Consider the\nfollowing short prompt and its generated example:\n1\n2 SHORT PROMPT\n3\n4\n5 #This is in python\n6 # Write a function called \"FindLocalPeaks\" that takes in a constant array\nof floats called \"signal\" as a parameter. The function should find\nthe locations of local maxima on positive values and local minima on\nnegative values in the \"signal\" array. The first and last index of\nthe array should not be considered as extrema. The function should\niterate through the array and compare each element with its adjacent\nelements to determine if it is a local peak. If a local peak is found\n, its index should be stored in an array called \"peakLoc \". The\nfunction should return the \"peakLoc\" array as the output. Make sure\nto include the necessary import statement for numpy.\n7 # Return only the code in the completion. I don \u2019t want any other comments\n. Don \u2019t say \"here is your code\" or similar remarks.\n8 # def FindLocalPeaks(signal):\n1\n2 SAMPLE GENERATION\n3 import numpy as np\n4 def FindLocalPeaks(signal):\n5\npeakLoc = []\n6\nfor i in range(1, len(signal) -1):\n7\nif signal[i-1] < signal[i] > signal[i+1] and signal[i] > 0:\n8\npeakLoc.append(i)\n64\n9\nelif signal[i-1] > signal[i] < signal[i+1] and signal[i] < 0:\n10\npeakLoc.append(i)\n11\nreturn peakLoc\nand here is the corresponding long prompt and its sample generated example:\n1 LONG PROMPT\n2 #This is in python\n3 # Write a function called \"FindLocalPeaks\" that takes in one parameter , \"\nsignal\", which is a constant array of floats. The function is used to\nfind local maxima on positive values and local minima on negative\nvalues. The function excludes the first and last indices of the \"\nsignal\" array as possible extrema. The function should return an\narray of integers , \"peakLoc\", which represents the locations of these\nextrema in the \"signal\" array. The function should follow the logic\nprovided in the source code provided below , which includes conditions\nfor various scenarios when comparing adjacent elements in the \"\nsignal\" array. When a local maxima or minima is found , its location\nshould be appended to the \"peakLoc\" array. If you encounter equal\nvalues while comparing adjacent elements , there are additional\nconditions that need to be followed. Remember to import the necessary\nmodules and define any necessary variables.\n4 # Return only the code in the completion. I don \u2019t want any other comments\n. Don \u2019t say \"here is your code\" or similar remarks.\n5 # def FindLocalPeaks(signal):\n1\n2 SAMPLE GENERATION\n3 def FindLocalPeaks(signal):\n4\nimport numpy as np\n5\npeakLoc = []\n6\nfor i in range(1, len(signal) -1):\n7\nif signal[i] > 0:\n8\nif signal[i] > signal[i-1] and signal[i] > signal[i+1]:\n9\npeakLoc.append(i)\n10\nelif signal[i] < 0:\n11\nif signal[i] < signal[i-1] and signal[i] < signal[i+1]:\n12\npeakLoc.append(i)\n13\nreturn np.array(peakLoc)\nAs can be seen from the above example, using a more detailed prompt (10 lines of description vs. 7\nlines of description) does not have an outsized effect on the structure of the sample generations, as the\nfunctions generated from the two different prompts are structurally similar. Therefore, the improved\nperformance likely stems from the small details that might be omitted in the concise prompts that are\nrelevant to functional performance.\n65\nN\nOBFUSCATION ABLATION STUDY\nWe conducted a study using \u201cobfuscated prompts\u201d. The framework for this \u201cobfuscated prompts\u201d\nablation study drew inspiration from Lachaux et al. (2020); Yang et al. (2023). The prompts used were\nprimarily \u201cshort\u201d GPT summaries in their original form, with function or parameter names substituted\nwith placeholder terminology deliberate adjustment to hinder a model\u2019s ability to memorize portions\nof the training data.\nOur analysis of the results shows that there is a performance decrease with obfuscated prompts\ncompared to their non-obfuscated counterparts. However, this difference doesn\u2019t reach a magnitude\nto justify attributing ChatGPT\u2019s performance exclusively to the memorization of training data.\nBelow we provide an example showing a comparison between an obfuscated prompt with a non-\nobfuscated prompt.\n1 SAMPLE ORIGINAL PROMPT\n2 #This is in python\n3 # Write a Python function called \"unpipe_name\" that takes in a string\nargument called \"name\" and returns a new string that is the single\ngene name after removing any duplications and pipe characters. This\nfunction is designed to fix the duplicated gene names produced by\nPicard CalculateHsMetrics , which combines the labels of overlapping\nintervals by joining all labels with \u2019|\u2019. If the name only consists\nof a single gene name , then that name is returned. If there are\nduplicated gene names and a meaningless target name , such as \u2019-\u2019 or\nany name in the IGNORE_GENE_NAMES set , then they are removed. If\nthere are still duplicated gene names , then the longest name is taken\n. If there are multiple equally long gene names , then one of them is\narbitrarily selected and a warning is logged.\n4\n5 # Return only the code in the completion. I don \u2019t want any other comments\n. Don \u2019t say \"here is your code\" or similar remarks.\n6 # def unpipe_name(name):\n1 SAMPLE OBFUSCATED PROMPT\n2 # This is in python\n3 # Write a Python function called \"FUNCTION\" that takes in a string\nargument called \"VAR0\" and returns a new string that is the single\ngene VAR0 after removing any duplications and pipe characters. This\nfunction is designed to fix the duplicated gene names produced by\nPicard CalculateHsMetrics , which combines the labels of overlapping\nintervals by joining all labels with \u2019|\u2019. If the VAR0 only consists\nof a single gene VAR0 , then that VAR0 is returned. If there are\nduplicated gene names and a meaningless target VAR0 , such as \u2019-\u2019 or\nany VAR0 in the IGNORE_GENE_NAMES set , then they are removed. If\nthere are still duplicated gene names , then the longest VAR0 is taken\n. If there are multiple equally long gene names , then one of them is\narbitrarily selected and a warning is logged.\n4\n5 # Return only the code in the completion. I don \u2019t want any other comments\n. Don \u2019t say \"here is your code\" or similar remarks.\n6 # def FUNCTION(VAR0):\nNotice that, by replacing specific function names and parameters with generic monikers like FUNC-\nTION and VAR0 in our obfuscated prompts, we sought to limit the potential influence of prior\ntraining data memorization in ChatGPT\u2019s code generation. Overall, this process aimed to increase\nthe difficulty for the model to rely on memorization as a primary means of accurate code generation,\nthus ensuring that the model\u2019s performance is based on interpreting the structure and requirement of\nthe task, not on recollecting learned patterns.\nWe carried out Pass@K tests on the obfuscated prompts. The results were comparable to those\nachieved with non-obfuscated versions, although they were slightly lower, showing that obfuscation\nof the prompts does not significantly impede code generation performance. This implies that GPT\n3.5\u2019s performance on our dataset can not be attributed solely to memorization.\n66\nPrompt\nJava\nPython\nPass@1\nPass@5\nPass@10\nPass@20\nPass@1\nPass@5\nPass@10\nPass@20\nShort Summary\n34.700\n42.313\n43.000\n43.800\n25.960\n36.371\n39.825\n42.484\nObfuscated Summary\n30.700\n40.684\n44.250\n47.900\n22.771\n32.733\n35.872\n38.790\nOur experiment indeed shows that obfuscation of the prompts does not significantly impede code\ngeneration performance. Obfuscated prompts marginally underperformed in comparison to non-\nobfuscated ones, suggesting that memorization might play a role in ChatGPT\u2019s performance. Yet,\nthe proximity of the results demonstrates that the model\u2019s understanding of the task described in the\nprompt is the primary driver of its performance.\nMoreover, the fact that obfuscated and non-obfuscated short prompts yield similar generated examples\nserves as further evidence, testifying to the model\u2019s capacity to grasp the task\u2019s requirements and\ngenerate relevant codes even when specific cues from the prompt, such as function or parameter\nnames, are withheld.\nThis finding supports the view that language models like ChatGPT generate code predominantly\nbased on their understanding and interpretation of the task presented to them and that memorization,\nwhile present to an extent, does not appear to be the dominant factor.\nHere is an example to further bolster our case, again showing that there is no data leakage. This\nexample shows more detail on the generation:\n1 SHORT PROMPT\n2 #This is in python\n3 # Write a function called \u2018_neighbors \u2018 that takes in a two -or -three\ndimensional image and returns the number of neighbor pixels for each\npixel of the image. The function should first pad the image with\nzeros , then create a mask where any pixel value greater than zero is\nconsidered a neighbor. Next , it should compute the response using a\nuniform filter and multiply it by a scaling factor. The labels are\nthen computed by multiplying the response with the mask and slicing\nthe padded image. Finally , the function should return the labels as\nan array of unsigned integers.\n4 # Return only the code in the completion. I don \u2019t want any other comments\n. Don \u2019t say \"here is your code\" or similar remarks.\n5 # def _neighbors(image):\n1\n2\n3 SAMPLE GENERATION with ORIGINAL PROMPT\n4\n5\n6 def _neighbors(image):\n7\nfrom skimage.filters import uniform_filter\n8\nfrom scipy import ndimage\n9\nscaling_factor = 10\n10\nmask = (image > 0).astype(int)\n11\npadded_image = np.pad(image , ((1 ,1), (1,1), (0,0)), \u2019constant \u2019,\nconstant_values =0)\n12\nresponse = uniform_filter(padded_image , size =3)\n13\nlabels = (response * scaling_factor * mask)[1:-1, 1:-1, :]\n14\nlabels = ndimage.label(labels)[0]\n15\nreturn labels.astype(np.uint16)\n16\n17\n1 SAMPLE OBFUSCATED PROMPT\n2\n3\n4 # This is in python\n5 # Write a function called \u2018FUNCTION \u2018 that takes in a two -or -three\ndimensional VAR0 and returns the number of neighbor pixels for each\n67\npixel of the VAR0. The function should first pad the VAR0 with zeros ,\nthen create a mask where any pixel value greater than zero is\nconsidered a neighbor. Next , it should compute the response using a\nuniform filter and multiply it by a scaling factor. The labels are\nthen computed by multiplying the response with the mask and slicing\nthe padded VAR0. Finally , the function should return the labels as an\narray of unsigned integers.\n6 # Return only the code in the completion. I don \u2019t want any other comments\n. Don \u2019t say \"here is your code\" or similar remarks.\n7 # def FUNCTION(VAR0):\n8\n9\n10\n1\n2 SAMPLE GENERATION with OBFUSCATED PROMPT\n3 def FUNCTION(VAR0):\n4\nimport numpy as np\n5\nfrom scipy.ndimage import uniform_filter\n6\n7\nVAR0 = np.pad(VAR0 , ((1, 1), (1, 1)), mode=\u2019constant \u2019)\n8\nmask = VAR0 > 0\n9\nresponse = uniform_filter(mask , size =3)\n10\nscaling_factor = 10\n11\nlabels = response * scaling_factor * mask * VAR0[1:-1, 1:-1]\n12\nreturn labels.astype(np.uint16)\nThe striking similarity between functions generated from both obfuscated and non-obfuscated prompts\nreaffirms that the model\u2019s ability to generate code is primarily based on its understanding and\ninterpretation of the prompt and task, rather than the memorization of the training data.\nImportant to note, however, is the impressive resilience of the model\u2019s performance even after the\nobfuscation of specific functions or parameter names. This resilience indicates that the model does\nnot rely heavily on these specific cues to generate appropriate code and can still produce functionally\ncomparable results even when details are abstracted out.\n68\nO\nTOPIC ANALYSIS\nThe repositories that we used for this study came from a larger set of 1,720 bioinformatics repositories\nthat were constructed in Russell et al. (2018). We manually selected 28 high-quality repositories from\nthis set. Each repository is a codebase of one bioinformatics journal article. We used latent Dirichlet\nallocation (LDA) to infer topics for abstracts of the articles citing each repository in the main dataset.\nFrom the LDA model, we identified terms that were primarily associated with a single topic. We chose\na model with eight topics due to its maximal coherence of concepts within the top topic-specialized\nterms. We manually assigned a label to each of the eight topics that capture a summary of the top\nterms. We then classified each article abstract into one or more topics. The eight topics in the LDA\nmodel are described below.\n1. Cancer and epigenetics: Cancer and epigenetics refer to the study of heritable changes in gene ex-\npression that do not involve changes to the underlying DNA sequence but can lead to the development\nand progression of cancer. These epigenetic modifications, which include DNA methylation, histone\nmodifications, and small RNA-associated gene silencing, can turn genes on or off, influencing cancer\nsusceptibility, initiation, progression, and response to treatment. Understanding these processes can\naid in developing targeted therapies and better diagnostic tools for cancer patients.\n2. Proteomics and microscopy: Proteomics and microscopy refer to the integration of proteomic\nanalyses (the large-scale study of proteins, their structures, and functions) with advanced microscopy\ntechniques. This integration allows scientists to visualize and quantify the spatial and temporal\ndistribution of proteins within cells and tissues. By combining the detailed molecular information\nfrom proteomics with the high-resolution imaging from microscopy, researchers can gain deeper\ninsights into cellular processes, protein-protein interactions, and the structural organization of cellular\ncomponents.\n3. Variant calling: Variant calling is a process in bioinformatics where sequence data (often from\nnext-generation sequencing) are analyzed to identify differences, or variants, between a given sample\nand a reference genome. These variants can include single-nucleotide polymorphisms, insertions,\ndeletions, and more. Identifying these variants is crucial for understanding genetic diversity, disease\nsusceptibility, and personalizing medical treatments based on an individual\u2019s genetic makeup.\n4. Genetics and population analysis: Genetics and population analysis in bioinformatics refers to\nthe study of the genetic composition of populations and how it changes over time. This involves\nanalyzing large datasets of genetic information to understand patterns of genetic variation, inheritance,\nand evolution within and between populations. Such analyses can provide insights into population\nstructure, migration, adaptation, and the history of species or populations, as well as help identify\ngenetic factors associated with diseases or traits in specific populations.\n5. Structure and molecular interaction: Structure and molecular interaction in bioinformatics pertains\nto the study and prediction of the three-dimensional structures of biomolecules (like proteins and\nnucleic acids) and how they interact with one another. By using computational methods to model\nand analyze these structures, scientists can gain insights into the molecular mechanisms underlying\nbiological processes. This understanding can be pivotal for drug design, predicting protein functions,\nand understanding the effects of genetic mutations on molecular interactions and stability.\n6. Web and graphical applications: Web and graphical applications in bioinformatics refer to the\ndevelopment and use of online tools and software with user-friendly graphical interfaces to analyze,\nvisualize, and interpret biological data. These applications allow both experts and non-experts to\ninteract with complex datasets, facilitating tasks like sequence alignment, gene annotation, pathway\nanalysis, and more. By providing accessible platforms, these applications help democratize the\nanalysis and understanding of vast and intricate bioinformatics data.\n7. Assembly and sequence analysis: Assembly and sequence analysis in bioinformatics involves the\nprocess of taking raw sequence data, typically from next-generation sequencing technologies, and\nreconstructing the original DNA or RNA sequences. Assembly might mean piecing together short\nDNA reads into longer sequences or entire genomes. Once assembled, sequence analysis is used\nto identify genes, predict their functions, detect variations, and compare sequences across different\norganisms or individuals. This foundational process is key to many downstream analyses in genomics\nand personalized medicine.\n69\n8. Transcription and RNA-seq: Transcription and RNA-seq (RNA sequencing) in bioinformatics\nrelate to the study of the transcriptome\u00e2 \u02d8A\u02c7Tthe complete set of RNA molecules expressed from\ngenes in a cell or tissue. RNA-seq is a technique that uses next-generation sequencing to capture\nand quantify the RNA in a sample, providing insights into which genes are active (or \u201cexpressed\u201d)\nunder specific conditions. By analyzing RNA-seq data, researchers can uncover patterns of gene\nexpression, identify novel RNA species, and study regulatory pathways, aiding in understanding\ncellular responses, disease mechanisms, and developmental processes.\nAll eight topics were present across our subset of 28 repositories from which we created the benchmark\ndataset. A detailed breakdown of which topics corresponded to which repositories is as follows:\nRepository\nTopic Areas\nAdmiralenOla/Scoary\nGenetics and population analysis, Assembly and\nsequence analysis\nbiocore/deblur\nTranscription and RNA-seq\nCellProfiler/CellProfiler\nCancer and epigenetics, Proteomics and mi-\ncroscopy\nCGATOxford/UMI-tools\nGenetics and population analysis\nchoderalab/ensembler\nStructure and molecular interaction\netal/cnvkit\nVariant calling\ngem-pasteur/macsyfinder\nStructure and molecular interaction, Web and\ngraphical applications\nhangelwen/miR-PREFeR\nTranscription and RNA-seq\njnktsj/DNApi\nWeb and graphical applications, Transcription and\nRNA-seq\njuliema/aTRAM\nGenetics and population analysis, Assembly and\nsequence analysis\nkarel-brinda/rnftools\nVariant calling, Structure and molecular interaction\nmad-lab/transit\nWeb and graphical applications\nmakson96/Dynamics\nStructure and molecular interaction\nMikkelSchubert/paleomix\nVariant calling, Assembly and sequence analysis\nmsproteomicstools/msproteomicstools\nProteomics and microscopy\nODonnell-Lipidomics/LipidFinder\nStructure and molecular interaction\npgxcentre/genipe\nProteomics and microscopy, Genetics and popula-\ntion analysis\nratschlab/spladder\nTranscription and RNA-seq\nSamStudio8/goldilocks\nWeb and graphical applications\nsimonvh/fluff\nWeb and graphical applications\nsjspielman/pyvolve\nStructure and molecular interaction, Web and\ngraphical applications\nSpatialTranscriptomicsResearch/st pipeline\nProteomics and microscopy, Web and graphical\napplications, Transcription and RNA-seq\nursgal/ursgal\nProteomics and microscopy\nvals/umis\nTranscription and RNA-seq\nwilliamgilpin/pypdb\nProteomics and microscopy, Web and graphical\napplications\nzhanglab/psamm\nStructure and molecular interaction, Web and\ngraphical applications\nzstephens/neat-genreads\nVariant calling, Assembly and sequence analysis\nsamtools/htsjdk\nAssembly and sequence analysis, Transcription\nand RNA-seq\n70\nP\nMODEL ERROR DISTRIBUTIONS\nModels\nFailure:\nSyntax\nError\nFailure:\nRuntime\nError\nFailure:\nTimeout\nError\nFailure:\nOutput Dis-\nagreement\nPassed\nTests\nCodeGen-6B-Mono\n11268\n8176\n1\n148\n105\nCodeGen2-7B\n12687\n6718\n0\n211\n79\nGPT-3.5 Turbo\n9231\n10603\n0\n5624\n6643\nInCoder-6B\n11268\n8176\n1\n148\n105\nInstructCodeT5P-16B\n19667\n33\n0\n0\n0\nSantaCoder\n14391\n4601\n1\n555\n139\nStarCoder\n26233\n10688\n0\n1660\n808\n71\nQ\nLANGUAGE ANALYSIS\nOur decision to include Java and Python was based on an empirical investigation into the prevalence\nof different programming languages across bioinformatics repositories. We computed the total byte\nsizes of various languages across all repositories we surveyed. Out of the 13 languages analyzed\n(Python, Bourne Shell, R, Perl, Cpp, C, Java, Bourne_Again_Shell, MATLAB, m4, SQL, Ruby,\nPHP), Java was the most prevalent, with a byte size of 242,318,285 bytes, followed closely by Python\nat 192,324,018 bytes. The presence of Java as the most extensive language indicates that it plays a\ncrucial role in the bioinformatics community, perhaps in the form of underlying tools or frameworks\nthat support high-level bioinformatics operations. A detailed breakdown of the byte size of the various\nlanguages in our repository is as follows:\nProgramming Language\nNumber of Bytes\nJava\n242,318,285\nPython\n192,324,018\nC\n184,967,144\nC++\n184,694,473\nPerl\n129,213,485\nR\n40,708,273\nBourne Shell\n35,495,605\nPHP\n33,876,889\nMATLAB\n28,889,990\nSQL\n15,630,061\nRuby\n8,935,640\nm4\n7,956,980\nBourne Again Shell\n2,851,620\n72\nR\nTEST CASE EXAMPLE\nGenerated\nCode\nimport random\nimport hashlib\nimport numpy\nimport skimage\nimport skimage.measure\nimport scipy.ndimage\nimport os\nimport logging\nF_ANGLE = 'Angle'\nLOGGER = logging.getLogger(__name__)\nclass UntangleWorms:\ndef ncontrol_points(self):\nreturn <|int;range=3,10|>\n<<insert solution here>>\ndef main():  \nprint(UntangleWorms().angle_features())\nif __name__ == \"__main__\":\nmain()\nContext file\ndef angle_features(self):\n\"\"\"Return a list of angle feature names\"\"\"\ntry:\nreturn ['_'.join((F_ANGLE, str(n))) \nfor n in range(1, self.\nncontrol_points() - 1)]\nexcept:\nLOGGER.error(\n''Failed to get # of control \npoints from training file. Unknown number of \nangle measurements''\n, exc_info=True)\nreturn []\nGolden Code\ndef angle_features(self):\n# Define angle feature names\nangle_feature_names = ['Feature1', \n'Feature2', 'Feature3']\nreturn angle_feature_names\nHere are the imports: \nImport\norg.apache.commons.math3.distribution.HypergeometricDistribution; \nimport java.text.DecimalFormat; \nimport java.util.*; import java.util.function.Function; import \nhtsjdk.samtools.util.SequenceUtil; import java.util.Objects;\nHere are the class declarations: \nclass KmerOperations {\npublic static long[] rightNeighbours(long kmer, int k) {\nlong mask = (1L << (2 * k)) - 1;\nlong[] ans = new long[] {(kmer << 2) & mask,\n((kmer << 2) & mask) | 1,\n((kmer << 2) & mask) | 2,\n((kmer << 2) & mask) | 3};\nfor (int i = 0; i < ans.length; i++) {\nlong rc = rc(ans[i], k);\nif (rc < ans[i]) {\nans[i] = rc;\n}\n}\nreturn ans;\n}\n}\nsummary: Reverses and complement a kmer sequence.\nparam: kmer (long) - the kmer sequence to be reversed and \ncomplemented.\nparam: k (long) - the length of the kmer sequence.\nreturn: (long) - the reversed and complemented kmer sequence.\nThe function is located in the class KmerOperations\npublic static long rc(long kmer, long k)  \nHere are the global variables:\ntrans_dict = defaultdict(lambda : 5, A=0, C=1, G=2, T=3)\ntrans_dict['-'] = 4\nHere are the class declarations:\nclass Sequence(object):\nattributes: \nself.label,self.sequence,self.length,self.unaligned_length,self.frequency,self.\nnce\nmethods:\ndef __init__(self, label, sequence):\nsummary: Initializes a class instance with the specified label and \ninformation.\nparam: label (str) - the label of the sequence.\nparam: sequence (str) - the nucleotide sequence.\nreturn: None - the function does not return any value.\ndef __eq__(self, other):\nParameters: \n- self (object) - the first object to be compared\n- other (object) - the second object to be compared\nReturn:\n- (bool) - returns True if the objects are equal and False if they \nequal.\nsummary: Returns a string with the sequence in fasta format param: None return\nThe FASTA representation of the sequence The function is located in the class S\ndef to_fasta(self):\nHere are the imports:\nfrom collections import defaultdict\nimport re\nimport numpy as np\nFigure 17: Test case for UntangleWorms example. The context file includes various import de-\npendencies and a class definition with a method placeholder for the solution. The UntangleWorms\nclass comes from a GitHub repository file (https://github.com/CellProfiler/CellProfiler/\nblob/master/cellprofiler/modules/untangleworms.py) that was scraped in our study. Untan-\ngleWorms is an image analysis tool that was initially part of the paper \u201cAn image analysis toolbox for\nhigh-throughput C. elegans assays.\u201d\n73\nS\nERROR DISTRIBUTION CHART\nThe errors include: \u2018different output\u2019 where the generated code\u2019s output did not match the golden\ncode\u2019s output; \u2018invalid syntax\u2019 where syntax errors in the generated code prevented code execution;\n\u2018function timed out\u2019 where code execution exceeded time limits; and \u2018runtime error\u2019 where the\ngenerated code compiled successfully but failed to run. The vast majority of the generated code\nsamples tested encountered a sort syntax or runtime error without resulting in an output. See Figure\n18 for more detail.\nFigure 18: Error distribution aggregated over all models\n74\nT\nFUNCTION FILTERING PROCESS\nWhen filtering the function, we were given a large baseline of functions. In order to decrease the\nmanual workload of filtering through these functions, we initiated two rounds of automatic filtering.\nIn the first round, every function and its associated comments underwent keyword filtering. The\nkeywords were scraped from bioinformatics Wikipedia articles, with each function requiring at least\n10 keyword matches to be considered relevant to bioinformatics. This round was critical to avoid any\nunnecessary OpenAI API calls, which are expensive. The remaining functions advanced to the second\nround, where we inputted each function into GPT-3.5, which assessed bioinformatics relevance. With\nthe workload significantly reduced, we manually sorted through the remaining functions, leaving us\nwith a final count of 1,026 Python functions and 1,243 Java functions (see Table 2). Note that in\nJava, there are no global variables, because all variables are encapsulated in classes. In Table 2, the\n\u201csimilar data\u201d set includes an additional 157 Python problems and 50 Java problems, maintaining the\nsame 253 Rosalind problem tally, thus reflecting the composition of the public data. The issues in\nthis set were cherry-picked based on their L2 similarity norm of calculated statistics, ensuring close\nalignment to the mean distribution of the public data.\nIndeed, knowledge of bioinformatics was an integral part of constructing this dataset and benchmark.\nWe stressed how we used extensive bioinformatics-specific keywords and relied on the expertise of\nfour students specializing in Computer Science and Computational Biology. Our team manually\nsifted through potential functions to build our final dataset.\nIt is important to note that our selections were made with a focus on bioinformatics-related coding\nproblems. From the bioinformatics-related repositories, we obtained a vast number of functions.\nHowever, among these functions: some were highly specific to bioinformatics (like algorithms),\nwhile others were not directly relevant (like web page scripts).\nTherefore, a key task in our process was to judiciously filter these functions. We had to strike a\nbalance to ensure that our final dataset comprises functions that are truly bioinformatics-focused and\napplicable to our study. This filtering process was thorough and meticulous, undertaken by experts\nwith knowledge in bioinformatics, thereby reaffirming the indispensable role of a bioinformatics\nunderstanding in this work.\nTo ensure relevance, we executed a recursive search in scraping the entire Wikipedia corpus using\n100 seed words that are known to be relevant to bioinformatics. Eventually, we manually filtered all\nthe collected keywords to ensure relevance, which resulted in a dictionary of bioinformatics/adjacent\nwords.\nWe then employed a two-level filtering process: (1) In the first round, each function and correspond-\ning comments passed through a keyword filter, requiring at least 10 keyword matches related to\nbioinformatics. (2) In the following round, we leveraged GPT3.5 to generate associated keywords for\neach function, which underwent manual review. This strategy minimized the manual workload and\nhelped us retain high-quality, relevant functions for our dataset.\nThus, our effort to benchmark code generation, although general in nature, is deeply rooted in the\ncontext of bioinformatics, making use of carefully curated and filtered datasets based on bioinfor-\nmatics problems. While an understanding of bioinformatics and biology may not be essential for\nusing the benchmark, the benchmark, and the dataset were built to reflect the complexity and domain\nspecifics of bioinformatics.\n75\nU\nSUMMARY AT BOTTOM RESULTS\nSummary At Bottom results were omitted from Table 4 due to brevity reasons, and so are included\nhere. More details about how Summary At Bottom prompts are constructed can be found in Appendix\ni.\nModel\nPass@1\nPass@5\nPass@10\nPass@20\nInCoder-6B\n0.610\n2.587\n4.303\n6.274\nSantaCoder-1.1B\n0.510\n1.949\n3.013\n4.459\nStarCoder-15.5B\n6.465\n13.824\n16.746\n19.076\nStarCoder-15.5B (finetuned)\n\\\n\\\n\\\n\\\nStarCoder+\n4.172\n11.772\n14.933\n17.197\nCodeGen-6B-mono\n2.070\n4.535\n5.896\n7.006\nCodeGen-16B-mono\n2.166\n5.137\n6.022\n6.369\nCodeGen2-7B\n0.510\n1.019\n1.207\n1.274\nGPT-3.5-Turbo\n13.439\n20.040\n22.460\n25.478\nGPT-4\n6.316\n15.272\n20.223\n24.823\nTable 11: Performance on \"Summary at Bottom\" rows for Python columns\n76\nV\nERROR DISTRIBUTION GIVEN OVER ALL MODELS\nThe failure modes are similar between the models but not exactly the same.\nModels\nSyntax Error Runtime Error Timeout Error Output Disagreement Passed\nOverall\n117665\n55351\n4\n8661\n7982\nCodeGen\n11268\n8176\n1\n148\n105\nCodeGen2\n12687\n6718\n0\n211\n79\nGPT3.5\n9231\n10603\n0\n5624\n6643\nInCoder\n11268\n8176\n1\n148\n105\nInstructCodeT5P\n19667\n33\n0\n0\n0\nSantaCoder\n14391\n4601\n1\n555\n139\nStarCoder\n26233\n10688\n0\n1660\n808\nTable 12: Error distribution per model.\nBelow is the failure breakdown for each model:\n1\n2 CodeGen -6B-Mono\n3\n4 Failure: Syntax Error = 11268\n5 Failure: Runtime Error = 8176\n6 Failure: Timeout Error = 1\n7 Failure: Output Disagreement = 148\n8 Passed Tests = 105\n9\n10 CodeGen2 -7B\n11\n12 Failure: Syntax Error = 12687\n13 Failure: Runtime Error = 6718\n14 Failure: Output Disagreement = 211\n15 Passed Tests = 79\n16\n17 GPT3.5-Turbo\n18\n19 Failure: Syntax Error = 9231\n20 Failure: Runtime Error = 10603\n21 Failure: Output Disagreement = 5624\n22 Passed Tests = 6643\n23\n24 InCoder -6B\n25\n26 Failure: Syntax Error = 12777\n27 Failure: Runtime Error = 6502\n28 Failure: Timeout Error = 2\n29 Failure: Output Disagreement = 309\n30 Passed Tests = 100\n31\n32 InstructCodeT5P -16B\n33\n34 Failure: Syntax Error = 19667\n35 Failure: Runtime Error = 33\n36\n37 SantaCoder\n38\n39 Failure: Syntax Error = 14391\n40 Failure: Runtime Error = 4604\n41 Failure: Timeout Error = 1\n42 Failure: Output Disagreement = 555\n43 Passed Tests = 139\n44\n45 StarCoder\n77\n46\n47 Failure: Syntax Error = 26233\n48 Failure: Runtime Error = 10688\n49 Failure: Output Disagreement = 1660\n50 Passed Tests = 808\n78\nW\nMORE MODELS\nHere we aim to answer the question of why we didn\u2019t mention ChatGPT/Codex/Github copilot as\nbaselines.\nChatGPT/Codex/Github copilot are all powered by a GPT model. In our paper, we provide results\nfor GPT-3.5 and GPT-4, which was the model that ChatGPT and Github Copilot both used. As for\nCodex, it uses a weaker GPT 3.0 model and was deprecated by OpenAI at the time of testing, so we\ndid not include it in the benchmark.\n79\nX\nIMPORTANCE OF THE CONTEXT\nImports and classes are predefined and are included in the context because, as we are testing function-\nlevel code generation, we are not prompting the model nor expecting the model to generate the classes\nit needs to pass the tests. Instead, we are testing the model\u2019s ability to extract the pertinent imports\nand classes from the context to use in the generated function.\nTo further illustrate how the prompt needs to contain information on the context to correctly generate\nthe golden code, we provide an example below. You\u2019ll observe that the golden-code function uses the\nglobal variable \"IGNORE_GENE_NAMES\". We include this information in the prompt because it\nembeds the task\u2019s necessary foundations. Essentially, we ensure the prompt has enough context for an\ninformed human to reproduce the function, implying that the function generated ought to incorporate\nthe environment\u2019s existing resources (imports & classes) rather than generating new ones.\n1 PROMPT:\n2\n3 This is in python.\n4 Write a function called \"unpipe_name\" that takes in a string parameter\ncalled \"name\". The function takes a string containing multiple gene\nnames separated by pipes , and returns a single gene name devoid of\nduplicates and pipe characters. The function checks if there are any\nduplicates , and removes any meaningless target names. If there are\nmultiple gene names present , the function takes the longest name as\nthe final name. If there are any ambiguous gene names , the function\nlogs a warning and selects the longest name as the final name. The\nfunction should also import \"cnvlib.params \".\n5\n6 def unpipe_name(name):\n7\n8 Here are the imports:\n9 import numpy as np\n10 import logging\n11 from . import params\n12 from skgenome import tabio\n13 Here are the global variables:\n14 MIN_REF_COVERAGE = -5.0\n15 MAX_REF_SPREAD = 1.0\n16 NULL_LOG2_COVERAGE = -20.0\n17 GC_MIN_FRACTION = 0.3\n18 GC_MAX_FRACTION = 0.7\n19 INSERT_SIZE = 250\n20 IGNORE_GENE_NAMES = \u2019-\u2019, \u2019.\u2019, \u2019CGH \u2019\n21 ANTITARGET_NAME = \u2019Antitarget \u2019\n22 ANTITARGET_ALIASES = ANTITARGET_NAME , \u2019Background \u2019\n23 Here are the class declarations:\n24 Here are the additional function declarations:\n25 def do_import_picard(fname , too_many_no_coverage):\n26\nsummary: Reads a file in \u2019picardhs \u2019 format , processes the data , and\nreturns a modified dataframe.\n27\nparam: fname (string) - the file name/path to be read in \u2019picardhs \u2019\nformat.\n28\nparam: too_many_no_coverage (int) - if the number of bins with no\ncoverage is greater than this value , a warning message is logged.\nDefault is 100.\n29\nreturn: garr (pandas dataframe) - a modified dataframe with added\ncolumns \u2019gene \u2019 and \u2019log2 \u2019 based on the original dataframe read from\nthe input file.\n30 def unpipe_name(name):\n31\nsummary: Remove duplicate gene names and pipe characters from a given\nstring.\n32\nparam: name (str) - the string containing the gene names.\n33\nreturn: new_name (str) - the cleaned gene name string.\n34 def do_import_theta(segarr , theta_results_fname , ploidy):\n80\n35\nsummary: A function for importing theta results and estimating copy\nnumber and log2 ratios of segments.\n36\nparam: segarr (numpy array) - array of segments\n37\nparam: theta_results_fname (str) - name of theta results file\n38\nparam: ploidy (int) - ploidy of genome (default is 2)\n39\nreturn: generator of numpy arrays - array of segments with estimated\ncopy number and log2 ratios.\n40 def parse_theta_results(fname):\n41\nsummary: Parses THetA results into a data structure with NLL , mu , C,\nand p* columns.\n42\nparam: fname (str) - name of the file to parse the results from\n43\nreturn: (dict) - a dictionary containing the NLL , mu_normal ,\nmu_tumors , C, and p* values\n44 Here are the comments and the specs:\n45 Write a function called \"unpipe_name\" that takes in a string parameter\ncalled \"name\". The function takes a string containing multiple gene\nnames separated by pipes , and returns a single gene name devoid of\nduplicates and pipe characters. The function checks if there are any\nduplicates , and removes any meaningless target names. If there are\nmultiple gene names present , the function takes the longest name as\nthe final name. If there are any ambiguous gene names , the function\nlogs a warning and selects the longest name as the final name. The\nfunction should also import \"cnvlib.params \".\n46\n47 def unpipe_name(name):\n48 GOLDEN CODE:\n49\n50 def unpipe_name(name):\n51\n\"\"\"Fix the duplicated gene names Picard spits out.\n52\n53\nReturn a string containing the single gene name , sans duplications\nand pipe\n54\ncharacters.\n55\n56\nPicard CalculateHsMetrics combines the labels of overlapping\nintervals\n57\nby joining all labels with \u2019|\u2019, e.g. \u2019BRAF|BRAF \u2019 -- no two distinct\n58\ntargeted genes actually overlap , though , so these dupes are redundant\n.\n59\nMeaningless target names are dropped , e.g. \u2019CGH|FOO|-\u2019 resolves as \u2019\nFOO \u2019.\n60\nIn case of ambiguity , the longest name is taken , e.g. \"TERT|TERT\nPromoter\"\n61\nresolves as \"TERT Promoter \".\n62\n\"\"\"\n63\nif \u2019|\u2019 not in name:\n64\nreturn name\n65\ngene_names = set(name.split(\u2019|\u2019))\n66\nif len(gene_names) == 1:\n67\nreturn gene_names.pop()\n68\ncleaned_names = gene_names.difference(IGNORE_GENE_NAMES)\n69\nif cleaned_names:\n70\ngene_names = cleaned_names\n71\nnew_name = sorted(gene_names , key=len , reverse=True)[0]\n72\nif len(gene_names) > 1:\n73\nlogging.warning(\u2019WARNING: Ambiguous gene name %r; using %r\u2019, name\n,\n74\nnew_name)\n75\nreturn new_name\n81\nY\nPOTENTIAL CONCERNS ABOUT CHANGES IN PACKAGES\nIn addressing potential concerns about code generation issues due to changes in packages, we have\ntaken steps to ensure stability and consistency in our testing environment. Our testing environment\nutilizes standardized and version-locked packages that guarantee stable results for our \"golden\" code\nsamples. As such, we do not need to worry about package updates or feature deprecation.\nFurthermore, when giving prompts, we have taken care to specifically mention the packages that\nneed to be used, which guides the LLM to correctly utilize the expected versions of these packages.\nAny remaining discrepancies will be highlighted in our error analysis for the respective LLMs.\nIn the prompts we provide, we clearly specify which packages and functions should be used, including\nany external dependencies. This way, the LLM knows exactly which packages to use.\nTake, for example, this dataset-generated prompt in Python:\n1\n#This is in python\n2\n3 #write a function called \"binary_erosion\" that takes in two parameters: \"\nx_data\" and \"structuring_element \". The function should first check if\nthe \"structuring_element\" is a 2D array or not. If it is , and \"\nx_data\" is not a 2D array , the function should create a new array\ncalled \"y_data\" that is the same size as \"x_data\" and contains only\nzeros. The function should then loop through each slice of \"x_data\"\nand apply the skimage.morphology.binary_erosion function using the \"\nstructuring_element\" as the structuring element. The result of each\nslice should be added to the corresponding slice of \"y_data \". Finally\n, the function should return \"y_data \". If the \"structuring_element\"\nis not 2D and \"x_data\" is 2D, the function should raise a\nNotImplementedError with the message \"A 3D structuring element cannot\nbe applied to a 2D image .\" Otherwise , the function should simply\napply the skimage.morphology.binary_erosion function to \"x_data\"\nusing \"structuring_element\" as the structuring element and return the\nresult.\n4 #\n5 #def binary_erosion(x_data , structuring_element):\nYou can observe how the prompt meticulously guides the LLM to employ the \"binary_erosion\"\nfunction derived from the \"skimage.morphology package\". Any inaccuracies in using the specified\npackages, likely due to the model training on outdated or more advanced versions, would deem\nits output incorrect, leading us to categorize it as a failed generation. A paramount facet we are\nevaluating is the proficiency of the LLM in accurately implementing specified external packages as\nmentioned in the prompts.\n82\nZ\nDIFFERENCES BETWEEN PUBLIC, HIDDEN, AND SIMILAR SETS\nWhile constructing the datasets, we observed the need for a categorization that simultaneously caters\nto present-day language learning models (LLMs) and future, more capable versions. Consequently,\nwe delineated our public and hidden datasets, which admittedly, resulted in a considerable divergence\nbetween their summary statistics.\nThe public test set encapsulates relatively moderate challenges an LLM might encounter, keeping in\nmind the token limits of current-generation code LLMs, which range from 2,048 to 4,096 tokens.\nThis constraint dictated that the public test set should comprise smaller and simpler functions.\nConversely, the hidden dataset was configured to house a broader, more challenging set of problems,\naiming to benchmark future models, for instance, GPT4 with its anticipated 32K token limit. This\ntwo-tiered framework ensures the longevity of this benchmark as advancements unfold in the field of\nLLMs and facilitates additional investigations into domain-specific generative models.\nMoreover, we have devised a \"Similar Dataset\", which is essentially a subset of the hidden dataset\nharboring statistics comparable to the public dataset. This new dataset ensures direct, fair comparisons\nbetween the public and the hidden tests.\nThe \"public data\" represents datasets with crafted specific contexts and corresponding test cases.\nThe \"hidden data\" encompasses a wider array of intricate issues. Furthermore, the \"similar data\"\nis a subset of the hidden data, curated to mimic the overall distribution of the public data. This\nset includes an additional 157 Python problems and 50 Java problems, maintaining the same 253\nRosalind problem tally, thus reflecting the composition of the public data. The issues in this set were\ncherry-picked based on their L2 similarity norm of calculated statistics, ensuring close alignment to\nthe mean distribution of the public data.\nIn addition, the reason why the number of hidden tests is much larger than public tests is that we aim\nto ensure the integrity of our benchmark. Thus, we divided our dataset into a much larger hidden set\nrather than a public set. We believe that access to too many public test cases might lead to potential\noverfitting while fine-tuning m odels, which would fundamentally undermine the benchmark.\n83\nAA\nPROMPT STRUCTURE ANALYSIS\nAs demonstrated by the scatterplots in Appendix k, we can see that there is a general negative\ncorrelation between the length of prompt and the performance of the model. We can also plot the\nnumber of models that perform the best for each prompt type as follows:\nFigure 19: Number of best performing models over all prompt styles\nJudging from Figure 19, we can see that overall, the prompts with the fewest tokens, i.e. the \"Summary\nOnly\" and \"Necessary Only\" prompt styles, had the most models that performed the best, however\nthere are some models that performed better with the longer \"Summary At Top\" and \"Summary At\nBottom\" prompts. In this section, we will analyze some of the possible reasons for these discrepancies\nand variations in the results by looking at prompt structure. Consider the following \"Summary Only\"\nand \"Necessary Only\" prompts for the following \"binary_erosion\" function:\n1 def binary_erosion(x_data , structuring_element):\n2\nis_strel_2d = structuring_element.ndim == 2\n3\nis_img_2d = x_data.ndim == 2\n4\nif is_strel_2d and not is_img_2d:\n5\ny_data = numpy.zeros_like(x_data)\n6\nfor index , plane in enumerate(x_data):\n7\ny_data[index] = skimage.morphology.binary_erosion(plane ,\n8\nstructuring_element)\n9\nreturn y_data\n10\nif not is_strel_2d and is_img_2d:\n11\nraise NotImplementedError(\n12\n\u2019A 3D structuring element cannot be applied to a 2D image.\u2019)\n13\ny_data = skimage.morphology.binary_erosion(x_data ,\nstructuring_element)\n14\nreturn y_data\nHere is the \"Summary Only\" prompt:\n1 #This is in python\n2 #write a function called \"binary_erosion\" that takes in two parameters: \"\nx_data\" and \"structuring_element \". The function should first check if\nthe \"structuring_element\" is a 2D array or not. If it is , and \"\n84\nx_data\" is not a 2D array , the function should create a new array\ncalled \"y_data\" that is the same size as \"x_data\" and contains only\nzeros. The function should then loop through each slice of \"x_data\"\nand apply the skimage.morphology.binary_erosion function using the \"\nstructuring_element\" as the structuring element. The result of each\nslice should be added to the corresponding slice of \"y_data \". Finally\n, the function should return \"y_data \". If the \"structuring_element\"\nis not 2D and \"x_data\" is 2D, the function should raise a\nNotImplementedError with the message \"A 3D structuring element cannot\nbe applied to a 2D image .\" Otherwise , the function should simply\napply the skimage.morphology.binary_erosion function to \"x_data\"\nusing \"structuring_element\" as the structuring element and return the\nresult.\n3 #\n4 #def binary_erosion(x_data , structuring_element):\nand here is the \"Necessary Only\" prompt\n1 Write a function with the following specs:\n2 --specs begin here --\n3 #write a function called \"binary_erosion\" that takes in two parameters: \"\nx_data\" and \"structuring_element \". The function should first check if\nthe \"structuring_element\" is a 2D array or not. If it is , and \"\nx_data\" is not a 2D array , the function should create a new array\ncalled \"y_data\" that is the same size as \"x_data\" and contains only\nzeros. The function should then loop through each slice of \"x_data\"\nand apply the skimage.morphology.binary_erosion function using the \"\nstructuring_element\" as the structuring element. The result of each\nslice should be added to the corresponding slice of \"y_data \". Finally\n, the function should return \"y_data \". If the \"structuring_element\"\nis not 2D and \"x_data\" is 2D, the function should raise a\nNotImplementedError with the message \"A 3D structuring element cannot\nbe applied to a 2D image .\" Otherwise , the function should simply\napply the skimage.morphology.binary_erosion function to \"x_data\"\nusing \"structuring_element\" as the structuring element and return the\nresult.\n4 param: x_data (numpy.ndarray) - input data to be eroded.\n5 param: structuring_element (numpy.ndarray) - structuring element for\nerosion.\n6 return: y_data (numpy.ndarray) - eroded data.\n7 --specs end here --\n8 Note the function will be embedded in the following context\n9 --context begins here --\n10 import numpy\n11 import skimage.morphology\n12 import os\n13 numpy.random.seed(<|int;range =0 ,100| >)\n14 <<insert solution here >>\n15 def main():\n16\nx_data = numpy.random.randint(2, size =(10, 10))\n17\nstructuring_element = skimage.morphology.square (3)\n18\nprint(binary_erosion(x_data , structuring_element))\n19 if __name__ == \"__main__ \":\n20\nmain()\n21 --context ends here --\n22 Make sure to only generate the function and not any of the context. Make\nsure you are generating valid , runnable code. Begin your solution\nwith:\n23 def binary_erosion(x_data , structuring_element):\n24 MAKE SURE TO INDENT THE BODY OF YOUR FUNCTION BY A TAB\nFrom the structure of these prompts, we can hypothesize why some of the simpler models perform\nbetter with the \"Summary Only\" prompts, while the GPT models perform better generally with\nthe \"Necessary Only\" prompts. Since the \"Necessary Only\" prompts includes the GPT-generated\nsummary of the function embedded inside the prompt, while including additional context, it is\n85\nreasonable to hypothesize that the simpler models such as InCoder, SantaCoder, StarCoder, or\nCodeGen are simply unable to digest the context information embedded inside the \"Necessary Only\"\nprompts, and instead the context information ends up being regarded as noise that just confuses the\nsimpler models resulting in poorer performance. However, the larger models, such as the GPT-3.5\nand GPT-4 models released by OpenAI, are able to properly incorporate the context information\nprovided in the \"Necessary Only\" prompts, and as a result, they perform significantly better than their\nsmaller counterparts with the additional context information.\nIt is also worth looking at the structure of the \"Uncommented\" prompt style, to see why these prompts\nperform so poorly when compared to the commented version of the prompts. For instance here is the\n\"Uncommented\" promppt style for the \"binary_erosion\" problem.\n1 This is in python\n2 write a function called \"binary_erosion\" that takes in two parameters: \"\nx_data\" and \"structuring_element \". The function should first check if\nthe \"structuring_element\" is a 2D array or not. If it is , and \"\nx_data\" is not a 2D array , the function should create a new array\ncalled \"y_data\" that is the same size as \"x_data\" and contains only\nzeros. The function should then loop through each slice of \"x_data\"\nand apply the skimage.morphology.binary_erosion function using the \"\nstructuring_element\" as the structuring element. The result of each\nslice should be added to the corresponding slice of \"y_data \". Finally\n, the function should return \"y_data \". If the \"structuring_element\"\nis not 2D and \"x_data\" is 2D, the function should raise a\nNotImplementedError with the message \"A 3D structuring element cannot\nbe applied to a 2D image .\" Otherwise , the function should simply\napply the skimage.morphology.binary_erosion function to \"x_data\"\nusing \"structuring_element\" as the structuring element and return the\nresult.\n3\n4 def binary_erosion(x_data , structuring_element):\n5\n6 Here are the imports:\n7 import skimage.morphology\n8 import scipy.ndimage\n9 import numpy\n10 Here are the global variables:\n11 Here are the class declarations:\n12 Here are the additional function declarations:\n13 def dilation(x_data , structuring_element):\n14\nsummary: Performs dilation on input image data with a structuring\nelement\n15\nparam: x_data (numpy array) - input image data to perform dilation on\n16\nparam: structuring_element (numpy array) - structuring element for\nthe dilation operation\n17\nreturn: y_data (numpy array) - resulting dilated image data\n18 def erosion(x_data , structuring_element):\n19\nsummary: Performs erosion operation on input data using a structuring\nelement.\n20\nparam: x_data (numpy.ndarray) - input data to apply erosion on.\n21\nparam: structuring_element (numpy.ndarray) - structuring element used\nfor erosion operation.\n22\nreturn: y_data (numpy.ndarray) - erosion result as a 2D or 3D numpy\narray.\n23 def binary_erosion(x_data , structuring_element):\n24\nsummary: Performs binary erosion on input data using a structuring\nelement.\n25\nparam: x_data (numpy.ndarray) - input data to be eroded.\n26\nparam: structuring_element (numpy.ndarray) - structuring element for\nerosion.\n27\nreturn: y_data (numpy.ndarray) - eroded data.\n28 def morphological_gradient(x_data , structuring_element):\n29\nsummary: Computes the morphological gradient of an image.\n30\nparam: x_data (numpy array) - input data.\n86\n31\nparam: structuring_element (numpy array) - structuring element used\nfor the operation.\n32\nreturn: y_data (numpy array) - output data.\n33 Here are the comments and the specs:\n34 write a function called \"binary_erosion\" that takes in two parameters: \"\nx_data\" and \"structuring_element \". The function should first check if\nthe \"structuring_element\" is a 2D array or not. If it is , and \"\nx_data\" is not a 2D array , the function should create a new array\ncalled \"y_data\" that is the same size as \"x_data\" and contains only\nzeros. The function should then loop through each slice of \"x_data\"\nand apply the skimage.morphology.binary_erosion function using the \"\nstructuring_element\" as the structuring element. The result of each\nslice should be added to the corresponding slice of \"y_data \". Finally\n, the function should return \"y_data \". If the \"structuring_element\"\nis not 2D and \"x_data\" is 2D, the function should raise a\nNotImplementedError with the message \"A 3D structuring element cannot\nbe applied to a 2D image .\" Otherwise , the function should simply\napply the skimage.morphology.binary_erosion function to \"x_data\"\nusing \"structuring_element\" as the structuring element and return the\nresult.\n35 def binary_erosion(x_data , structuring_element):\nand here is the corresponding commented version of the prompt:\n1 #This is in python\n2 #write a function called \"binary_erosion\" that takes in two parameters: \"\nx_data\" and \"structuring_element \". The function should first check if\nthe \"structuring_element\" is a 2D array or not. If it is , and \"\nx_data\" is not a 2D array , the function should create a new array\ncalled \"y_data\" that is the same size as \"x_data\" and contains only\nzeros. The function should then loop through each slice of \"x_data\"\nand apply the skimage.morphology.binary_erosion function using the \"\nstructuring_element\" as the structuring element. The result of each\nslice should be added to the corresponding slice of \"y_data \". Finally\n, the function should return \"y_data \". If the \"structuring_element\"\nis not 2D and \"x_data\" is 2D, the function should raise a\nNotImplementedError with the message \"A 3D structuring element cannot\nbe applied to a 2D image .\" Otherwise , the function should simply\napply the skimage.morphology.binary_erosion function to \"x_data\"\nusing \"structuring_element\" as the structuring element and return the\nresult.\n3 #\n4 #def binary_erosion(x_data , structuring_element):\n5 #\n6 #Here are the imports:\n7 #import skimage.morphology\n8 #import scipy.ndimage\n9 #import numpy\n10 #Here are the global variables:\n11 #Here are the class declarations:\n12 #Here are the additional function declarations:\n13 #def dilation(x_data , structuring_element):\n14 #\nsummary: Performs dilation on input image data with a structuring\nelement\n15 #\nparam: x_data (numpy array) - input image data to perform dilation on\n16 #\nparam: structuring_element (numpy array) - structuring element for\nthe dilation operation\n17 #\nreturn: y_data (numpy array) - resulting dilated image data\n18 #def erosion(x_data , structuring_element):\n19 #\nsummary: Performs erosion operation on input data using a structuring\nelement.\n20 #\nparam: x_data (numpy.ndarray) - input data to apply erosion on.\n21 #\nparam: structuring_element (numpy.ndarray) - structuring element used\nfor erosion operation.\n87\n22 #\nreturn: y_data (numpy.ndarray) - erosion result as a 2D or 3D numpy\narray.\n23 #def binary_erosion(x_data , structuring_element):\n24 #\nsummary: Performs binary erosion on input data using a structuring\nelement.\n25 #\nparam: x_data (numpy.ndarray) - input data to be eroded.\n26 #\nparam: structuring_element (numpy.ndarray) - structuring element for\nerosion.\n27 #\nreturn: y_data (numpy.ndarray) - eroded data.\n28 #def morphological_gradient(x_data , structuring_element):\n29 #\nsummary: Computes the morphological gradient of an image.\n30 #\nparam: x_data (numpy array) - input data.\n31 #\nparam: structuring_element (numpy array) - structuring element used\nfor the operation.\n32 #\nreturn: y_data (numpy array) - output data.\n33 #Here are the comments and the specs:\n34 #write a function called \"binary_erosion\" that takes in two parameters: \"\nx_data\" and \"structuring_element \". The function should first check if\nthe \"structuring_element\" is a 2D array or not. If it is , and \"\nx_data\" is not a 2D array , the function should create a new array\ncalled \"y_data\" that is the same size as \"x_data\" and contains only\nzeros. The function should then loop through each slice of \"x_data\"\nand apply the skimage.morphology.binary_erosion function using the \"\nstructuring_element\" as the structuring element. The result of each\nslice should be added to the corresponding slice of \"y_data \". Finally\n, the function should return \"y_data \". If the \"structuring_element\"\nis not 2D and \"x_data\" is 2D, the function should raise a\nNotImplementedError with the message \"A 3D structuring element cannot\nbe applied to a 2D image .\" Otherwise , the function should simply\napply the skimage.morphology.binary_erosion function to \"x_data\"\nusing \"structuring_element\" as the structuring element and return the\nresult.\n35 #def binary_erosion(x_data , structuring_element):\nComparing the two prompt styles, we can see the only difference is the inclusion of the \"#\" symbol\nin front of each line in the commented version of the line. These \"#\" symbols seem to have a large\neffect on LLM code generation. For instance, here is the a sample ChatGPT code generation after\nbeing passsed in a \"Uncommented\" prompt:\n1 Here is the Python function as you described:\n2\n3 \u2018\u2018\u2018Python\n4 import numpy as np\n5 from skimage.morphology import binary_erosion\n6\n7 def binary_erosion(x_data , structuring_element):\n8\n9\n# Check if structuring_element is 2D\n10\nif len(structuring_element.shape) == 2:\n11\n# Check if x_data is not 2D\n12\nif len(x_data.shape) != 2:\n13\n# Create y_data (same shape as x_data , filled with zeros)\n14\ny_data = np.zeros_like(x_data)\n15\n16\n# Apply binary_erosion slice by slice\n17\nfor idx in range(x_data.shape [0]):\n18\ny_data[idx] = binary_erosion(x_data[idx],\nstructuring_element)\n19\n20\nreturn y_data\n21\n22\nelse:\n23\n# Apply binary_erosion directly\n24\nreturn binary_erosion(x_data , structuring_element)\n25\n88\n26\nelse:\n27\n# Check if x_data is 2D\n28\nif len(x_data.shape) == 2:\n29\n# Raise error\n30\nraise NotImplementedError (\"A 3D structuring element cannot be\napplied to a 2D image .\")\n31\n32\nelse:\n33\n# Apply binary_erosion directly\n34\nreturn binary_erosion(x_data , structuring_element)\n35 \u2018\u2018\u2018\n36 If 2D structuring element is provided and x_data isn \u2019t 2D, function\napplies the skimage.morphology.binary_erosion function to each slice\nof x_data separately. Then adds result to y_data. Otherwise , if\nstructuring_element isn \u2019t 2D and x_data is 2D, it raises an error. If\nboth conditions don \u2019t apply , it directly applies the binary_erosion\nfunction to x_data.\nand here is a sample ChatGPT code generation after being passed in a corresponding commented\nprompt:\n1 import skimage.morphology\n2 import numpy as np\n3\n4 def binary_erosion(x_data , structuring_element):\n5\nif len(structuring_element.shape) == 2:\n6\nif len(x_data.shape) != 2:\n7\ny_data = np.zeros_like(x_data)\n8\nfor i in range(x_data.shape [0]):\n9\ny_data[i] = skimage.morphology.binary_erosion(x_data[i],\nstructuring_element)\n10\nreturn y_data\n11\n12\nelif len(structuring_element.shape) != 2 and len(x_data.shape) == 2:\n13\nraise NotImplementedError (\"A 3D structuring element cannot be\napplied to a 2D image .\")\n14\n15\nelse:\n16\nreturn skimage.morphology.binary_erosion(x_data ,\nstructuring_element)\nWe can see that the models respond far better to the commented prompt, only returning runnable\nPython code without wrapping it unnecessary text that may result in runtime errors in the testing\nenvironment. Overall, it is not just prompt length that has a tangible effect on LLM code generation\nperformance. For instance, the slightly longer \"Necessary Only\" prompts can outperform the shorter\n\"Summary Only\" prompts for larger more complex models that can properly digest and understand\nthe given code context. Furthermore, the inclusion of certain symbols, such as the presence of a \"#\"\nat the beginning of every line of the commented prompts, can help prompt models to generate more\ndesirable code outputs.\n89\n"
  },
  {
    "title": "Active Neural Mapping",
    "link": "https://arxiv.org/pdf/2308.16246.pdf",
    "upvote": "8",
    "text": "Active Neural Mapping\nZike Yan1,2\nHaoxiang Yang1,2\nHongbin Zha1,2\n1Key Laboratory of Machine Perception (MOE), School of EECS, Peking University\n2PKU-SenseTime Machine Vision Joint Lab\nzike.yan@pku.edu.cn\nyyyhhhxxx@stu.pku.edu.cn\nzha@cis.pku.edu.cn\nAbstract\nWe address the problem of active mapping with a\ncontinually-learned neural scene representation, namely\nActive Neural Mapping. The key lies in actively finding the\ntarget space to be explored with efficient agent movement,\nthus minimizing the map uncertainty on-the-fly within a pre-\nviously unseen environment. In this paper, we examine the\nweight space of the continually-learned neural field, and\nshow empirically that the neural variability, the prediction\nrobustness against random weight perturbation, can be di-\nrectly utilized to measure the instant uncertainty of the neu-\nral map. Together with the continuous geometric informa-\ntion inherited in the neural map, the agent can be guided to\nfind a traversable path to gradually gain knowledge of the\nenvironment. We present for the first time an active map-\nping system with a coordinate-based implicit neural rep-\nresentation for online scene reconstruction. Experiments\nin the visually-realistic Gibson and Matterport3D environ-\nment demonstrate the efficacy of the proposed method.\n1. Introduction\nHow we represent a 3D environment accurately and effi-\nciently is of tremendous importance for vision, robotics, and\ngraphics communities. Recent advances in implicit neural\nrepresentations (INRs) cast the issue as a low-dimensional\nfunction regression problem. Parameterized by a single net-\nwork \u03b8, the quantity of interest y such as color, occupancy,\nand semantics can be efficiently queried with the spatial\ncoordinates x through a feedforward pass y = f(x; \u03b8).\nUnlike traditional representations that discretize the entire\nspace and explicitly store a set of the input-output sam-\nples {xi, yi}N in manually-designed data structures such\nas voxel grid, surfel, and triangle mesh, the implicit neural\nrepresentation is proved to have great capacity [71, 20, 81]\nthat recovers complex signals at a constant small size, guar-\nanteeing high-fidelity view synthesis [44, 4, 43] and ac-\ncurate geometry reconstruction [67, 38, 3]. Nonetheless,\nthe quality of the learned implicit neural representation is\nPrediction error\nSDF value\nCurrent frustum\nStored keyframes\nTarget viewpoints\nLocal horizon\nTrajectory\n0\n-1\n0.3m\n1m\nFigure 1: An overview of the proposed active neural map-\nping system. Guided by the continually-updated neural map\n(visualized as the SDF values through a forward pass), the\nmobile agent explores the environment actively to minimize\nthe prediction uncertainty (visualized as the prediction error\ngiven the truth surface points).\nhighly data-dependent: as implicit neural representations\nare trained through self-supervision given discrete training\nsamples, insufficient sampling frequency leads to geometric\nand texturing artifacts [63, 80, 81, 62].\nUnlike conventional methods that rely on passive data\nacquisition, we address the problem of active neural map-\nping, where a 3D neural field is constructed on the fly\nwith an actively-exploring mobile agent to best represent\nthe scene. The target is to find efficient agent movement\nwithin the previously-unknown environment to gradually\nminimize the map uncertainty. Similar problems such as\nautonomous exploration and next-best-view planning are\nwell-studied [13, 40, 77, 73, 52] by exploiting discretized\nscene representations to achieve the best coverage and re-\nconstruction accuracy (see Sec. 2 for a detailed discus-\nsion).\nThough the implicit neural representation has its\nown merits, e.g. promising representation power and con-\narXiv:2308.16246v1  [cs.CV]  30 Aug 2023\ntinuous/differentiable properties, this problem setting poses\nnew challenges to the INRs: new knowledge of the environ-\nment is actively captured and constantly distilled to the neu-\nral map, where the neural map is expected to 1) specify the\nuncertain areas to be explored; 2) provide reliable geomet-\nric information for reconstruction; 3) allow for incremental\nupdating given constantly observed data.\nIn this paper, we show for the first time a continual\nlearning perspective of online active mapping based on the\ncoordinate-based implicit neural representation.\nInspired\nby the seminal works of [69, 78], we adopt the incremen-\ntal updating of a continuous neural signed distance field.\nThe key to our active mapping solution lies in a novel un-\ncertainty quantification manner of the learned neural map\nthrough weight perturbation. We show empirically that the\nreplayed buffer during continual learning forces the neural\nnetwork to land in a low-loss basin given previously ob-\nserved data to avoid forgetting, while resulting in a sharp\nridge given erroneously-generated zero-crossings from not-\nwell-explored areas to ensure transferability. That is to say,\nas the weight changes constantly during continual learn-\ning, the robustness of the predicted signed distance values\nexhibit distinguishable behaviors against weight perturba-\ntions for explored and unexplored surface samples. These\nfindings share similar spirits with recent studies in neuro-\nscience [39, 46, 21] and learning theory [76, 66], allowing\nus to explicitly reason the uncertain areas within the neural\nfield and guide the mobile agent for actively capturing new\ninformation.\nOur active mapping system adopts ideas from both\nfrontier-based and sampling-based exploration strategies.\nThe neural variabilities of zero-crossing samples are ex-\namined under random weight perturbations, where samples\nwith high variation are viewed as target areas to be explored.\nAlong with the continually-learned geometric information,\nthe neural map guides the agent to explore the environment\nactively. The key contributions can be summarized as fol-\nlows:\n\u2022 We provide a new perspective of active mapping from\nthe optimization dynamics of map parameters.\n\u2022 We introduce an effective online active mapping sys-\ntem in a continual learning fashion.\n\u2022 We propose a novel uncertainty quantification man-\nner through weight perturbation for goal location identifi-\ncation.\n2. Related Work\nActive mapping.\nActive mapping aims to find the op-\ntimal sensor movements to capture observations that best\nrepresent a scene, thus minimizing the uncertainty of the\nenvironment through exploration. Typical approaches can\nbe categorized into frontier-based and sampling-based ones\nfrom a goal location identification perspective. Frontier-\nbased methods explore by approaching the selected frontier\n(regions on the boundary between the explored free space\nand the unexplored space [77]), aiming to push the bound-\nary of the explored areas until the entire space is observed.\nThe major differences lie in the frontier detection strate-\ngies [85, 19, 65, 84, 6] and the best frontier selection strate-\ngies [14, 12, 27, 79].\nOn the other hand, sampling-based methods adopt ran-\ndom or guided sampling of potential viewpoints in the\nworkspace and incrementally grow a Rapidly-exploring\nRandom Tree (RRT) [35] or a Rapidly-exploring Random\nGraph (RRG) [30] to find the traversable paths. The next\nbest view is repeatedly selected along the best branch in a\nreceding horizon fashion [7] to maximize a given objective\nfunction. Unlike frontier-based methods that focus more on\nthe map coverage, sampling-based methods allow different\nobjectiveness, e.g., localization uncertainty [50], geometric\nuncertainty [59, 57, 24], visual saliency [15], and vehicle\ndynamics [18], to be taken into account.\nTo take advantage of both frontier-based and sampling-\nbased methods, new strategies are employed in a hy-\nbrid or an informed sampling-based fashion. The hybrid\nmethod [60] adopts a sampling-based manner for local plan-\nning, while utilizing a frontier-based method for global\nplanning to handle the dead-end case as sampling-based\nmethods can easily get stuck locally. Meanwhile, as most\ncomputational resources are wasted on the redundant utility\ncomputation of non-selected samples [68, 5], the informed\nsampling based methods [32, 41, 56] are proposed that sam-\nple candidates around frontiers to ensure faster exploration.\nDense metric representations.\nDense metric representa-\ntions play important roles in path planning as they provide\ncomplete geometric information for any queried location\nwithin the workspace. Existing active mapping methods\nmainly rely on the volumetric representation that discretizes\nthe space into voxel grids. Occupancy grid map, for exam-\nple, allows distinguishing between free, occupied, and un-\nknown space. Most occupancy grid based methods are de-\nployed in 2D [77, 10, 25] for tractable computation as a mo-\nbile device typically moves at a constant height [31]. There\nare also 3D extensions [7, 19, 6] that exploit an Octomap\nstructure [28] for recursive updating of the occupancy sta-\ntus. Meanwhile, it is noted that merely occupancy informa-\ntion may be insufficient for certain gradient-based planners,\nCHOMP [87] for instance. Therefore, the Euclidean signed\ndistance field (ESDF) is introduced to be updated incremen-\ntally from a truncated signed distance field (TSDF) [45, 49]\nor a 3D occupancy grid map [26] using Breadth-First Search\n(BFS), allowing online planning on a CPU-only platform.\nRecent advances in implicit neural representations\n(INRs) [44, 51, 42, 11] facilitate multiple robotics-related\ndownstream tasks. By encoding the coordinate-based scene\nproperties in weights of a neural network, INRs are able\nto recover fine-grained scene properties with light-weight\nparameters [67, 71, 20, 81]. Hence, accurate scene geome-\ntry can be recovered with a single network [3, 38]. On the\nother hand, the gradient can be efficiently extracted from\nthe continuous neural field through automatic differentia-\ntion. Together with the geometric information, a smooth\ntrajectory can be optimized for collision avoidance [2, 33].\nRecently, [36, 48]share a similar idea of refining a coarsely-\ntrained NeRF by actively selecting new viewpoints for batch\nretraining. Inspired by the continual learning fashion of on-\nline neural field updating [78, 69, 86, 47], we extend the\nworks to an online active mapping framework, where the\nimplicit neural field is updated on the fly to guide the explo-\nration for complete coverage and constant uncertainty re-\nduction. There are also two concurrent works [55, 82] that\nare most related to ours, tackling the inward view selection\nand path planning for object reconstruction.\n3. Preliminaries\nGiven an indoor environment as the workspace X \u2208 R3\nthat is unknown a prior, we aim to best represent the scene\nproperty of interest1 Y \u2208 Rm with a continuous function\nparameterized by a single MLP \u03b8, establishing the map-\nping f(x; \u03b8) : X \u2192 Y between spatial coordinates x \u2208 X\nand the corresponding scene property y \u2208 Y. To obtain an\noptimal map representation, a mobile agent is deployed to\nactively capture sensory data {xi, yi}t \u223c zt \u2282 D sampled\nfrom the scene surfaces D (depth sequence in our case) with\nself-decided control at at each time, and the map parame-\nters \u03b8 is updated incrementally with incoming observations.\nFrom a global optimum view, the map can be optimized\nthrough empirical risk minimization given a pre-defined\npenalty function L and sufficient samples from the true dis-\ntribution of D as:\n\u03b8\u2217 = arg min E(x,y)\u223cD(L(x, y; \u03b8)).\n(1)\nIn our case of an online setting, the continual learning of\nthe map can be cast as minimizing a cumulative loss [54]\nwithin a time interval [t, t + k] in the following steps as:\n\u03b8t = arg min\nt+k\nX\n\u03c4=t\n\u03bb\u03c4E(xt,yt)\u223cz1:\u03c4 (L(xt, yt; \u03b8t)),\n(2)\nwhere the observation zt is conditioned on past controls\na1:t, and k \u2192 \u221e equals an unending exploration setting.\nFrom Eq. (2), we can see that the overarching goal of\nan optimal map is intractable to be achieved as future ob-\nservations zt:t+k are not available. This issue is formalized\nby Raghavan and Balaprakash [54] from a generalization-\nforgetting perspective. They point out that the penalty L is\n1In this paper, we target a continuous signed distance function to rep-\nresent the scene surfaces.\nneeded to not only prevent catastrophic forgetting of pre-\nvious observations, but improve generalization to new data.\nAs proved in [54], the dynamics of continual learning are af-\nfected by three factors: the cost of prediction error over all\npast observations E(x,y)\u223cz1:t(L(x, y; \u03b8t)), the cost varia-\ntion arising from the data distribution shift \u2206xt, and the\ncost variation arising from the parameter changes \u2206\u03b8t:\nH(\u03b4z, \u03b8t) \u2248 \u03b2L(\u03b8t, z1:t)\n+\nt+k\nX\n\u03c4=t\n(L(\u03b8\u03c4, z1:\u03c4 \u2229 \u03b4z) \u2212 L(\u03b8\u03c4, z1:\u03c4))\n+\nt+k\nX\n\u03c4=t\n(L(\u03b8\u03c4 + \u03b4\u03b8, z1:\u03c4) \u2212 L(\u03b8\u03c4, z1:\u03c4)),\n(3)\nwhere L(\u03b8i, zj) = E(xj,yj)\u223czj(L(xj, yj; \u03b8i)).\nIntuitively, minimal H(\u03b4z, \u03b8t) induced by distribution\nshift \u03b4z and parameter changes \u03b4\u03b8 indicates that the arrival\nof a new observation does not affect the current optimal so-\nlution \u03b8t, thus achieving the global optimum. Even though\nsuch an optimum cannot be guaranteed, a saddle point2 can\nbe found. In [54], the discrepancy between two subsequent\ntasks is maximized, followed by the minimization of forget-\nting under the maximum generalization. This manner lays\nthe theoretic foundation for us to solve the active mapping\nproblem: if we iteratively find the most distribution shift\nof \u03b4z and update the map parameters \u03b8 given a new obser-\nvation, we converge to a local equilibrium point within the\nsmall time interval k according to Eq. (3).\nThe optimization perspective of Eqs. (2) and (3) well dis-\ntinguishes the proposed problem setting, namely active neu-\nral mapping, from previous research. Recent INR-based\npassive SLAM systems [69, 86, 47] or multi-view stere-\nopsis methods [44, 3, 83] merely minimize the first term\nin Eq. (3), while we further take the agent action optimiza-\ntion into account to serve as a local generalization maxi-\nmizer. Consequently, the actively captured training sam-\nples can better mimic the actual distribution D compared to\nthe passive observations z1:t. Compared to traditional ac-\ntive mapping methods, we explicitly conduct map optimiza-\ntion through back-propagation instead of the heuristically-\ndesigned fusion techniques. The goal location is decided\nin a data-driven manner (see Sec. 4 for details) instead of\nthe ad-hoc goal location identification strategies. The ob-\njectiveness of active mapping in Eq. (3) allows for con-\ntinual and lifelong (t \u2192 \u221e) optimization even when the\nagent stops, while INR-based planners [2, 33, 36] target\nnavigating to the specified location in a pre-built or batch-\noptimized map. Finally, compared to recent works of object\n2Given that H(\u03b4z, \u03b8\u2217) \u2264 H(\u03b4z\u2217, \u03b8\u2217) \u2264 H(\u03b4z\u2217, \u03b8), the equilib-\nrium point of {\u03b4z\u2217, \u03b8\u2217} can be found by alternatively updating the data\ndiscrepancy to maximize the generalization, and then optimizing \u03b8 to avoid\nforgetting given the maximum generalization.\n(a) |f(x+; \u03b8)|\n(b) |f(x\u2212; \u03b8)|\nFigure 2: The loss landscape |f(x; \u03b8(u, v))| evaluated on\na true surface point x+ and a false-positive point x\u2212. The\npink dotted lines indicate the actual loss variation along the\ncontinually learned \u03b81:T . It is clear that the landscape of the\ntrue surface point stays in a low-loss basin, while that of the\nfalse-positive point falls along a sharp ridge that reaches the\nlow-loss valley once.\nreconstruction [36, 48] that autonomously refine a pre-built\ncoarse map \u03b80 through inward-facing view selection and\nplanning, we target a more challenging case to incremen-\ntally optimize the map \u03b8 in a scene-level from scratch.\n4. Active Neural Mapping\nAs noticed in Sec. 3, central to our method is the iden-\ntification of the next target view that brings a significant\ndistribution shift \u03b4z\u2217.\nA local planner is then deployed\nas a generalization maximizer that decides the following\nagent movement at:t+k to the target location and captures\nthe corresponding data. Given the locally upper-bounded\nH(\u03b4z\u2217, \u03b8t), the map parameters are optimized with the\nnew observation, thus achieving a local equilibrium point\nof H(\u03b4z\u2217, \u03b8\u2217). The process is iteratively conducted that\ndrives the mobile agent to actively explore the environment.\nIn this section, we begin with an empirical analysis of how\n\u03b4z\u2217 can be found. The implementation of the active neural\nmapping system is introduced afterward.\n4.1. Through the lens of loss landscape\nEq. (3) motivates us to understand the behavior of the\nloss L(\u03b8, z) during continual learning:\nthe equilibrium\npoint of {\u03b4z\u2217, \u03b8\u2217} indicates the requirement of a flat low-\nloss landscape for surface points to avoid forgetting (the\nminimization of the first and third terms in Eq. (3)) and an\nevident loss discrepancy for finding \u03b4z so the generalization\nis maximized. Following [37, 72], we define a hyperplane\nby two orthonormal vectors {u, v},3 where any sample \u03b8 in\n3We choose the initial and the final weights during continual learning\nas \u03b81 and \u03b83, and train another network with the same initialization as \u03b82.\nThe orthonormal vectors can be obtained by orthogonalizing and normal-\nizing the two basis vectors (\u03b82 \u2212 \u03b81, \u03b83 \u2212 \u03b81)\nVariance against weight perturbations\nlow\nhigh\nFigure 3: The functionality changes due to weight pertur-\nbations given the ground truth surface points x \u2208 D. It can\nbe noted that most high-variance regions (with warm col-\nors) locate near the boundaries of space between explored\n(colored point cloud) and unexplored areas.\nthe weight space can be represented by the linear combina-\ntion of the two vectors as \u03b8(u, v) = uu + vv. We can then\nestimate the prediction f(x; \u03b8(u, v)) given any queried lo-\ncation x through a single forward pass and obtain the mag-\nnitude of the loss landscape L(\u03b8, z).\nWe randomly pick a true surface point x+ observed at\nt = 1 and a false-positive zero-crossing point4 x\u2212 gen-\nerated at t = 200 due to the continuous nature of the\nneural map. The prediction over the entire weight space\nis then calculated through forward passes given \u03b8(u, v).\nAs presented in Fig. 2, by projecting the high-dimensional\nweight space onto the hyperplane, we can easily visualize\nthe loss changes along the optimization path. Empirically,\nwe observe evidently-different geometries for the true sur-\nface point and the false-positive one: the loss of the true sur-\nface point will be constrained in a low-loss basin, while the\nloss of the false-positive one stays along a sharp ridge that\nonce jumps over a high-loss ridge into the valley at t = 200\nand then keeps ascending.\nThe reason behind this phenomenon is straightforward.\nDuring continual learning, the parameters of the neural map\nundergo constant changes. The functionality of f(x; \u03b8t)\nwill only remain stable in previously-observed areas with\nconstant self-supervision (as verified in [78, 69, 47] through\na simple experience replay strategy). In not-well-explored\nareas, the functionality can easily change due to a lack of\nconstraints. That is to say, the neural map is more sus-\nceptible to areas where the functionality changes the most\nagainst parameter perturbations:\nx = arg max V \u02c6\u03b8\u223cN(\u03b8,b2I )[f(x; \u02c6\u03b8)].\n(4)\nThe term in Eq. (4) is referred to as the artificial neural\nvariability [76] that shares similar concepts with the neural\nvariability in neuroscience [46, 39, 21]: neuronal activity\n4A free-space point whose instant prediction f(x\u2212; \u03b8200) \u2248 0\nFigure 4: The evolution of the learned signed distance field through active neural mapping in 1000 steps. The proposed system\nis conducted in a receding horizon fashion. The target locations (green dots) are constantly pushed to the not-well-explored\nor not-well-trained regions for reaching a local equilibrium point. See the supplementary video for better visualization.\nfluctuates over time given the same inputs, indicating the\nuncertainty of perceptual inference. By evaluating the pre-\ndiction variability given points on the zero-crossing surfaces\nthrough weight perturbation, the false-positive ones and\nthe true-positive ones can be evidently distinguished due\nto variable behaviors, and observations around the false-\npositive ones indicate high generalization cost (the second\nterm of Eq. (3)) as they land in sharp and unstable minima.\nAs illustrated in Fig. 3, the functional sensitivity\nof Eq. (4) is directly linked with the data distribution of\npast observations that supervise the neural map, and ex-\nplicitly indicates the prediction quality and uncertainty.\nHigh-variance regions are usually around the boundaries of\nspace between explored and unexplored areas (where false-\npositive zero-crossing surfaces are generated). This is in\ncommon with the prevalent concept of the frontier. The\ndifferences lie in that the high-variance regions are natu-\nrally indicated by the neural map in a data-driven fashion\ninstead of the heuristic design. Besides, unlike frontiers\nthat rely mainly on adjacent occupancy status, regions with\nscarce data or with thin structures may also fall into a high-\nvariance region in our case as they struggle to converge.\nHence, all areas that are not accurately represented are taken\ninto account. We refer readers to the supplementary video\nfor a better understanding.\n4.2. An online active neural mapping system\nIn this paper, we target a continuous signed distance\nfunction (SDF) of f(x; \u03b8) : R3 \u2192 R as the scene geom-\netry representation, where the neural map \u03b8 is continually\noptimized and guides the mobile agent to not-well-explored\nareas. The system is implemented as four steps: 1) identify-\ning the target viewpoints; 2) selecting the best target view-\npoint; 3) navigating to the target location; 4) and optimizing\nthe neural map parameters given newly-captured data.\nThe target view identification serves as finding the most\ndistribution shift \u03b4z\u2217. A zero-mean Gaussian perturbation\naround the instant map parameters \u03b8t is performed every\ntime a keyframe is selected, where the variance I is set as\nthe norm of recent parameter changes |\u03b8t \u2212\u03b8t\u22121|. We sam-\nple points on the predicted zero-crossing surface to distin-\nguish between the real surface points and the false-positive\nones. This is close in spirit to the frontier-based method in a\nsample-based fashion. In practice, the top 10% points with\nthe highest variance V \u02c6\u03b8\u223cN(\u03b8,b2I )[f(x; \u02c6\u03b8)] are selected and\nthen clustered based on the geometrical similarity. To make\nthe selected samples in sight, we place the target locations\n(green dots in Fig. 4) at a fixed distance along the surface\nnormal \u2207f(x; \u03b8t), where the continuous and differentiable\nneural representation allows for convenient gradient com-\nputation through auto-differentiation. To determine the best\nview (the red dot in Fig. 4) among the target location candi-\ndates, we evaluate each cluster with three criteria: the max-\nimum variance against parameter perturbations, the number\nof points within the cluster, and the distance between the\ncluster center and the current agent position. As illustrated\nin Fig. 4, the red dot and cyan dot are selected based on dif-\nferent criteria. Besides, a local planning horizon [8, 84, 6]\nis adopted that prioritizes the target viewpoint candidates\nwithin the frustum bounding box. Therefore, the agent (the\norange arrow in Fig. 4) will choose the best candidate in\nsight as the target view.\nWithin each receding horizon loop [t, t + k], the point-\ngoal navigation for deciding the agent actions at:t+k and the\ncontinual learning for updating the map parameters \u03b8t:t+k\nare exactly the optimization process for maximizing gener-\nalization and minimizing forgetting, indicating the dynam-\nics of a and \u03b8 to reach the equilibrium point of {\u03b4z\u2217, \u03b8\u2217}\nwithin a local horizon. For point-goal navigation, we adopt\nthe reinforcement-learning-based DD-PPO [74] to reach the\nnext target viewpoint. For incrementally updating the neu-\nral map, we adopt the experience-replay-based strategy of\niSDF [47] with similar architecture and loss functions. It\nshould be noted that other planner [2, 33, 36] and continual\nlearning strategies [78, 69] can be naturally incorporated as\noptimizers that decide the optimization path to reach the lo-\ncal equilibrium point of \u03b4\u2217 and \u03b8\u2217.\n5. Experiments\nCentral to the paper is a novel target view identification\nmodule through weight perturbations and an online active\nmapping system with a 3D implicit neural representation.\nIn this section, we evaluate the performance of the system\nthrough comprehensive experiments.\n5.1. Experimental Setup\nThe experiments are conducted on a desktop PC with an\nIntel Core i7-8700 (12 cores @ 3.2 GHz), 32GB of RAM,\nand a single NVIDIA GeForce RTX 2080Ti.\nData acquisition.\nOur algorithm is conducted with the\nHabitat simulator [58] and evaluated on the visually-\nrealistic Gibson [75] and Matterport3D datasets [9]. The ex-\nperiments are conducted in 1000/2000 steps depending on\nthe scene scale.5 The system takes posed depth images at the\nresolution of 256\u00d7256 as inputs and outputs discrete action\nat each step. The action space consists of MOVE FORWARD\nby 6.5cm, TURN LEFT and TURN RIGHT by 10\u25e6, and\nSTOP. The mobile agent is randomly initialized in the\ntraversable space at the height of 1.25m. The field of view\n(FOV) is set to 90\u25e6 vertically and horizontally.\nNeural map architecture.\nOur neural map is a single\nmulti-layer perceptron (MLP) with 4 hidden layers and 256\nunits per layer. Following [47], a softplus activation and\na positional embedding are adopted, where the positional\nembedding is concatenated to the third layer of the network.\nThe neural map is optimized using the Adam optimizer with\na learning rate of 0.0013.\n5.2. Evaluation metrics\nWe adopt the following metrics for evaluating the\nincrementally-updated neural map:\nMAD (cm). The mean absolute distance is evaluated by es-\ntimating the distance prediction through a forward pass on\nall vertices from the ground truth mesh. This metric defines\nthe accuracy of the learned 3D neural distance field.\nFPR (%). The false-positive rate is calculated as the per-\ncentage of samples from the reconstructed mesh whose\nnearest distance to the ground truth mesh exceeds 5cm.\nThis metric defines the quality of the mesh extracted from\nthe 3D continuous neural map.\n5A more thorough introduction of the test scenes and per-scene analysis\nare provided in the supplementary material.\nTable 1: The coverage of the actively-captured data. See\nsupplementary material for results on each scene for details.\nGibson\nMP3D\nComp. \u2191\nComp. \u2193\nComp. \u2191\nComp. \u2193\n(%)\n(cm)\n(%)\n(cm)\nRandom\n45.80\n34.48\n45.67\n26.53\nFBE\n68.91\n14.42\n71.18\n9.78\nUPEN\n63.30\n21.09\n69.06\n10.60\nOccAnt\n61.88\n23.25\n71.72\n9.40\nOurs\n80.45\n7.44\n73.15\n9.11\nComp.. The completeness metrics are calculated from the\nground truth vertices to the entire observations that are ac-\ntively captured. By estimating per-vertex nearest distance\nto the past observations z1:t, the percentage of points whose\nnearest distance is within 5cm (Comp. (%)) and the mean\nnearest distance (Comp. (cm)) can be calculated to measure\nthe active exploration coverage in 3D space.\n5.3. Comparisons against other methods\nWe compare the proposed method against three relevant\nmethods: FBE [77] aims to push the boundaries between\nunknown and known space for exploration; OccAnt [57] an-\nticipates the occupancy status in unseen areas and rewards\nthe agent with accurate anticipation; UPEN [24] tries to se-\nlect the most uncertain path via the ensemble of occupancy\nprediction models. As the three methods utilize the 2D grid-\nbased map representation, we evaluate the completeness of\nthe actively-captured observations along the trajectory us-\ning the Comp. (%) and the Comp. (cm) metrics.\nAs presented in Tab. 1, the proposed active mapping\nsystem consistently outperforms the three competitors. It\nshould be noted that FBE and UPEN adopt the same DD-\nPPO planner as ours for target goal navigation.\nThere-\nfore, the efficacy of the proposed goal location identifica-\ntion strategy can be fairly evaluated. FBE relies purely on\nthe voxel-based geometric information for identifying the\nfrontiers, whereas the selection mechanism is manually de-\nsigned that can easily ignore areas that have been explored\nwith insufficient data.\nIn contrast, the proposed method\nwell quantifies the map uncertainty to achieve better perfor-\nmance. In terms of OccAnt, the agent occasionally moves\nback and forth as the goal location identification is trained\nthrough a rewarded mechanism, while the proposed goal\nlocation identification strategy and the local planning hori-\nzon guarantee stable exploration routes. UPEN adopts a\ndeep ensemble based manner [34] to quantify the prediction\nuncertainty, which shares a similar idea with the proposed\nmethod regarding epistemic uncertainty reasoning. Never-\nTable 2: Ablation study of the map quality regarding the\nSDF prediction (MAD), the reconstructed mesh (FPR), and\nthe observation completeness (Comp.).\nMAD \u2193\nFPR \u2193\nComp. \u2191\n(cm)\n(%)\n(%)\nRandom\n8.49\n36.57\n45.80\nGibson\nModule 1\n5.68\n34.65\n79.48\nModule 3\n5.44\n25.09\n76.19\nModule 4\n6.11\n32.86\n73.70\nOurs\n5.10\n28.04\n80.45\nRandom\n8.87\n51.88\n45.67\nMP3D\nModule 1\n4.65\n43.03\n71.41\nModule 3\n6.06\n39.05\n74.63\nModule 4\n4.30\n47.99\n67.75\nOurs\n4.29\n40.07\n73.15\ntheless, UPEN simply generates multiple traversable path\ncandidates towards a pre-defined unreachable goal loca-\ntion with a global RRT planner, where the map uncertainty\nmerely ranks the path candidates to achieve the best infor-\nmation gain, while the proposed method better explores the\nenvironment by explicitly exploiting the neural variability\nfor goal location identification and selection.\n5.4. Ablation study and system performance\nAs mentioned in Sec. 4.2, the proposed active neural\nmapping system allows drop-in substitutes to replace the\nexisting modules. We provide detailed ablation studies to\njustify the reasonable design of each module.\nModule 1: target view identification. We replace the pro-\nposed weight perturbation module with MC-Dropout [22]\n(p=0.05) and a BALD [29] score to quantify the prediction\nuncertainty. The output is sampled five times in our ex-\nperiments. As demonstrated in Tab. 2, the proposed goal\nlocation identification strategy achieves better results com-\npared to the substitute. Although the uncertainty quantifi-\ncation method leads to comparable exploration efficiency,\nthe involvement of Dropout layers leads to noisy and coarse\ngeometry and inefficient inference.\nModule 2: best candidate selection. As illustrated in Fig. 5,\nwe evaluate the performance of the three different selection\ncriteria mentioned in Sec. 4.2. Results on different scenes\nshare a similar conclusion: selecting the highest variance\nregions will lead to the best performance. This result meets\nthe arguments in Sec. 3 and 4 to obtain the equilibrium point\nby maximizing generalization, or in other words, moving to\nthe highest variance areas as Eq. (4).\nCantwell\nDenmark\nEudora\nGreigsville\nPablo\nCluster with most samples\nNearest cluster\nCluster with highest variance\n102\n101\n100\n0\n200\n400\n600\n800\n1000\nComp. (cm)\nSteps\nFigure 5: The effect of different candidate selection criteria\n(Module 2) on the Comp. (cm) \u2193 metric in 1000 steps. Best\nviewed in color to see results on different scenes.\nSteps\nFigure 6: Continual learning of the scene geometry.\nModule 3: local planner. In our final setting (Ours) and\nfor evaluating FBE/UPEN, we choose the DD-PPO+ model\ntrained on Gibson4+ and Matterport3D (train/val/test) for\nevaluating the Gibson validation sequences, and choose the\nDD-PPO\u22c6 model trained on Gibson2+ for evaluating the\nMatterport3D test sets to avoid the over-fitting issue. We\nhere alter the model choice to further evaluate the per-\n0\n200\n400\n600\n800\n1000\n103\n102\n101\n100\nTime/ms\nSteps\nMapping Best view selection Visualization Planning Data processing Meshing Target view identi\ufb01cation\nFigure 7: The runtime for each module. The impulse of the runtime is caused by a keyframe-based or windowed execution.\nformance on Gibson with the DD-PPO\u22c6 and on Matter-\nport3D with DD-PPO+. Without pretrained data from Mat-\nterport3D, DD-PPO\u22c6 results in degradation on large scenes\nand improvement in small ones, while DD-PPO+ leads to\nmore robust and balanced results. We refer readers to the\nsupplementary material for a more detailed per-scene anal-\nysis. The results further verify the efficacy of the proposed\ngoal location identification strategy: a more powerful plan-\nner will bring better exploration results only if the goal lo-\ncation is properly decided.\nModule 4: learning of the neural map. Different network\narchitectures affect the convergence rate and the generation\nof false-positive zero-crossing surfaces. We further evalu-\nate the active neural mapping system with a different net-\nwork architecture: a single MLP with positional encod-\ning [44] and ReLU activations. As the substitute architec-\nture converges slower for high-frequency components [71],\nthe reconstruction accuracy deteriorates compared to our fi-\nnal setting (Ours). Meanwhile, the exploration is slightly\naffected as the prediction in visited areas may still be in-\naccurate. Nevertheless, the system still works effectively\ngiven a different network architecture, suggesting the appli-\ncability to embracing the latest advances in implicit neural\nrepresentations.\nSystem performance.\nIn general, our system achieves\npromising reconstruction accuracy and completeness given\nlimited steps. The computational cost for each module is\nillustrated in Fig. 7, where the runtime per step is 0.33s on\naverage. The system is real-time capable and can be accel-\nerated by reducing the per-frame iteration during continual\nlearning. As illustrated in Fig. 6, the prediction of scene ge-\nometry over previously seen areas is continually improved\nduring exploration, and the coverage of space continues to\ngrow. The continual learning fashion allows for constant\nmap optimization and lifelong learning of the scene.\n6. Conclusion\nIn this paper, we introduce a novel active mapping sys-\ntem based on implicit neural representations. The key to\nthe solution is a goal location identification strategy through\nweight perturbation that drives the mobile agent to the areas\nwith the most distribution discrepancy. The active mapping\nis achieved by alternatively performing action decisions to\nreach the goal location, and map parameter updating given\nincoming observations. The iterative process can be viewed\nas a joint optimization to reach an equilibrium point within\nthe receding local horizon, guaranteeing a promising scene\ngeometry recovery through autonomous exploration. The\nproposed strategy and the overall design of the system are\njustified through experiments and ablation studies.\n6.1. Limitations and future potentials\nThough the weight perturbation provides a convenient\nway to find the next best view, the action decision of the\nsystem is dependent on the map-free local planner, which\nmay occasionally get stuck by objects out of view or in\nnarrow areas. Better exploiting the information inherited\nin the neural map for online navigation and replanning is\none natural extension of the proposed system.\nPossible\nsolutions include optimization-based planners [2, 33] and\nINR-guided reinforcement learning to replace the existing\nplanner trained with a 2D top-down map or raw observa-\ntions. Meanwhile, the target view selection module sim-\nply discards other goal location candidates. Without ex-\nploiting temporal and historical cues, the agent may move\nback to visited areas where the complicated geometry is\nhard to converge. This issue may be handled by a graph\nmodel [79] for candidate organization and assignment or a\ndecomposed and hierarchical representation for object-wise\nor room-wise exploration.\nEnabling the mobile agent to behave autonomously in\nan unknown space is one straight path towards spatial in-\ntelligence [16, 17]. The implicit neural representation has\nshown great potential to distill knowledge from pre-trained\nmodel [61, 53] for a globally consistent and informative\nrepresentation.\nBest exploiting the information inherited\nin the prior and streaming data to construct a decodable\nand task-agnostic scene representation may lead to an in-\nnovative map-centric paradigm for the vision, graphics, and\nrobotics communities.\nAcknowledgements\nWe thank anonymous reviewers for\ntheir fruitful comments and suggestions. This work is sup-\nported by the Joint Funds of the National Natural Science\nFoundation of China (U22A2061) and National Natural Sci-\nence Foundation of China (62176010).\nReferences\n[1] Moloud Abdar, Farhad Pourpanah, Sadiq Hussain, Dana\nRezazadegan, Li Liu, Mohammad Ghavamzadeh, Paul\nFieguth, Xiaochun Cao, Abbas Khosravi, U Rajendra\nAcharya, et al. A review of uncertainty quantification in deep\nlearning: Techniques, applications and challenges. Informa-\ntion Fusion, 76:243\u2013297, 2021.\n[2] Michal Adamkiewicz, Timothy Chen, Adam Caccavale,\nRachel Gardner, Preston Culbertson, Jeannette Bohg, and\nMac Schwager.\nVision-only robot navigation in a neural\nradiance world.\nIEEE Robotics and Automation Letters,\n7(2):4606\u20134613, 2022.\n[3] Dejan Azinovi\u00b4c, Ricardo Martin-Brualla, Dan B Goldman,\nMatthias Nie\u00dfner, and Justus Thies. Neural rgb-d surface\nreconstruction. In IEEE/CVF Conf. on Computer Vision and\nPattern Recognition (CVPR), pages 6290\u20136301, 2022.\n[4] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Intl. Conf. on Computer Vision (ICCV),\npages 5855\u20135864, 2021.\n[5] Ana Batinovic, Antun Ivanovic, Tamara Petrovic, and\nStjepan Bogdan.\nA shadowcasting-based next-best-view\nplanner for autonomous 3d exploration. IEEE Robotics and\nAutomation Letters, 7(2):2969\u20132976, 2022.\n[6] Ana Batinovic, Tamara Petrovic, Antun Ivanovic, Frano Pet-\nric, and Stjepan Bogdan. A multi-resolution frontier-based\nplanner for autonomous 3d exploration. IEEE Robotics and\nAutomation Letters, 6(3):4528\u20134535, 2021.\n[7] Andreas Bircher,\nMina Kamel,\nKostas Alexis,\nHelen\nOleynikova, and Roland Siegwart. Receding horizon \u201dnext-\nbest-view\u201d planner for 3d exploration. In IEEE Intl. Conf. on\nRobotics and Automation (ICRA), pages 1462\u20131468. IEEE,\n2016.\n[8] Chao Cao, Hongbiao Zhu, Howie Choset, and Ji Zhang.\nTare:\nA hierarchical framework for efficiently exploring\ncomplex 3d environments. In Robotics: Science and Systems\n(RSS), 2021.\n[9] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-\nber, Matthias Niebner, Manolis Savva, Shuran Song, Andy\nZeng, and Yinda Zhang. Matterport3d: Learning from rgb-\nd data in indoor environments. In Intl. Conf. on 3D Vision\n(3DV), pages 667\u2013676. IEEE, 2017.\n[10] Devendra Singh Chaplot, Dhiraj Gandhi, Saurabh Gupta,\nAbhinav Gupta, and Ruslan Salakhutdinov. Learning to ex-\nplore using active neural slam. In Intl. Conf. on Learning\nRepresentations (ICLR), 2019.\n[11] Zhiqin Chen and Hao Zhang. Learning implicit fields for\ngenerative shape modeling.\nIn IEEE/CVF Conf. on Com-\nputer Vision and Pattern Recognition (CVPR), 2019.\n[12] Titus Cieslewski, Elia Kaufmann, and Davide Scaramuzza.\nRapid exploration with multi-rotors: A frontier selection\nmethod for high speed flight. In IEEE/RSJ Intl. Conf. on\nIntelligent Robots and Systems (IROS), pages 2135\u20132142.\nIEEE, 2017.\n[13] Cl Connolly. The determination of next best views. In IEEE\nIntl. Conf. on Robotics and Automation (ICRA), volume 2,\npages 432\u2013435. IEEE, 1985.\n[14] Anna Dai,\nSotiris Papatheodorou,\nNils Funk,\nDimos\nTzoumanikas, and Stefan Leutenegger. Fast frontier-based\ninformation-driven autonomous exploration with an mav. In\nIEEE Intl. Conf. on Robotics and Automation (ICRA), pages\n9570\u20139576. IEEE, 2020.\n[15] Tung Dang, Christos Papachristos, and Kostas Alexis. Vi-\nsual saliency-aware receding horizon autonomous explo-\nration with application to aerial robotics.\nIn IEEE Intl.\nConf. on Robotics and Automation (ICRA), pages 2526\u2013\n2533. IEEE, 2018.\n[16] Andrew J Davison.\nFuturemapping:\nThe computa-\ntional structure of spatial ai systems.\narXiv preprint\narXiv:1803.11288, 2018.\n[17] Andrew J Davison and Joseph Ortiz.\nFuturemapping 2:\nGaussian belief propagation for spatial ai.\narXiv preprint\narXiv:1910.14139, 2019.\n[18] Mihir Dharmadhikari, Tung Dang, Lukas Solanka, Johannes\nLoje, Huan Nguyen, Nikhil Khedekar, and Kostas Alexis.\nMotion primitives-based path planning for fast and agile ex-\nploration using aerial robots. In IEEE Intl. Conf. on Robotics\nand Automation (ICRA), pages 179\u2013185. IEEE, 2020.\n[19] Christian Dornhege and Alexander Kleiner. A frontier-void-\nbased approach for autonomous exploration in 3d. Advanced\nRobotics, 27(6):459\u2013468, 2013.\n[20] Rizal Fathony, Anit Kumar Sahu, Devin Willmott, and J Zico\nKolter. Multiplicative filter networks. In Intl. Conf. on Learn-\ning Representations (ICLR), 2020.\n[21] Dylan Festa, Amir Aschner, Aida Davila, Adam Kohn, and\nRuben Coen-Cagli. Neuronal variability reflects probabilis-\ntic inference tuned to natural image statistics. Nature Com-\nmunications, 12(1):1\u201311, 2021.\n[22] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian\napproximation:\nRepresenting model uncertainty in deep\nlearning. In Intl. Conf. on Machine Learning (ICML), pages\n1050\u20131059. PMLR, 2016.\n[23] Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep\nbayesian active learning with image data. In Intl. Conf. on\nMachine Learning (ICML), pages 1183\u20131192. PMLR, 2017.\n[24] Georgios Georgakis, Bernadette Bucher, Anton Arapin,\nKarl Schmeckpeper, Nikolai Matni, and Kostas Daniilidis.\nUncertainty-driven planner for exploration and navigation.\nIn IEEE Intl. Conf. on Robotics and Automation (ICRA),\n2022.\n[25] Saurabh Gupta, James Davidson, Sergey Levine, Rahul Suk-\nthankar, and Jitendra Malik. Cognitive mapping and plan-\nning for visual navigation. In IEEE/CVF Conf. on Computer\nVision and Pattern Recognition (CVPR), pages 2616\u20132625,\n2017.\n[26] Luxin Han, Fei Gao, Boyu Zhou, and Shaojie Shen. Fiesta:\nFast incremental euclidean distance fields for online motion\nplanning of aerial robots. In IEEE/RSJ Intl. Conf. on Intel-\nligent Robots and Systems (IROS), pages 4423\u20134430. IEEE,\n2019.\n[27] Dirk Holz, Nicola Basilico, Francesco Amigoni, and Sven\nBehnke. Evaluating the efficiency of frontier-based explo-\nration strategies. In Intl. Sym. on Robotics (ISR) and German\nConf. on Robotics (ROBOTIK), pages 1\u20138, 2010.\n[28] Armin Hornung, Kai M Wurm, Maren Bennewitz, Cyrill\nStachniss, and Wolfram Burgard.\nOctomap: An efficient\nprobabilistic 3d mapping framework based on octrees. Au-\ntonomous robots, 34(3):189\u2013206, 2013.\n[29] Neil Houlsby, Ferenc Husz\u00b4ar, Zoubin Ghahramani, and M\u00b4at\u00b4e\nLengyel. Bayesian active learning for classification and pref-\nerence learning. arXiv preprint arXiv:1112.5745, 2011.\n[30] Sertac Karaman and Emilio Frazzoli. Incremental sampling-\nbased algorithms for optimal motion planning. In Robotics:\nScience and Systems (RSS), volume 104, 2010.\n[31] Evan Kaufman, Kuya Takami, Zhuming Ai, and Taeyoung\nLee.\nAutonomous quadrotor 3d mapping and exploration\nusing exact occupancy probabilities. In Intl. Conf. on Robotic\nComputing (IRC), pages 49\u201355. IEEE, 2018.\n[32] Yves Kompis, Luca Bartolomei, Ruben Mascaro, Lucas\nTeixeira, and Margarita Chli. Informed sampling exploration\npath planner for 3d reconstruction of large scenes.\nIEEE\nRobotics and Automation Letters, 6(4):7893\u20137900, 2021.\n[33] Mikhail Kurenkov,\nAndrei Potapov,\nAlena Savinykh,\nEvgeny Yudin, Evgeny Kruzhkov, Pavel Karpyshev, and\nDzmitry Tsetserukou.\nNfomp: Neural field for optimal\nmotion planner of differential drive robots with nonholo-\nnomic constraints. IEEE Robotics and Automation Letters,\n7(4):10991\u201310998, 2022.\n[34] Balaji Lakshminarayanan, Alexander Pritzel, and Charles\nBlundell.\nSimple and scalable predictive uncertainty esti-\nmation using deep ensembles. In Advances in Neural Infor-\nmation Processing Systems (NIPS), volume 30, 2017.\n[35] Steven LaValle. Rapidly-exploring random trees: A new tool\nfor path planning. Research Report 9811, 1998.\n[36] Soomin Lee, Chen Le, Wang Jiahao, Alexender Liniger,\nSuryansh Kumar, and Fisher Yu. Uncertainty guided pol-\nicy for active robotic 3d reconstruction using neural radiance\nfields. IEEE Robotics and Automation Letters, 2022.\n[37] Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer, and Tom\nGoldstein. Visualizing the loss landscape of neural nets. In\nAdvances in Neural Information Processing Systems (NIPS),\n2018.\n[38] David B Lindell, Dave Van Veen, Jeong Joon Park, and Gor-\ndon Wetzstein. Bacon: Band-limited coordinate networks\nfor multiscale scene representation. In IEEE/CVF Conf. on\nComputer Vision and Pattern Recognition (CVPR), pages\n16252\u201316262, 2022.\n[39] Wei Ji Ma, Jeffrey M Beck, Peter E Latham, and Alexan-\ndre Pouget. Bayesian inference with probabilistic population\ncodes. Nature neuroscience, 9(11):1432\u20131438, 2006.\n[40] Jasna Maver and Ruzena Bajcsy. Occlusions as a guide for\nplanning the next view. IEEE Trans. Pattern Anal. Machine\nIntell., 15(5):417\u2013433, 1993.\n[41] Zehui Meng, Hailong Qin, Ziyue Chen, Xudong Chen, Hao\nSun, Feng Lin, and Marcelo H Ang. A two-stage optimized\nnext-view planning framework for 3d unknown environment\nexploration, and structural reconstruction.\nIEEE Robotics\nand Automation Letters, 2(3):1680\u20131687, 2017.\n[42] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In IEEE/CVF\nConf. on Computer Vision and Pattern Recognition (CVPR),\n2019.\n[43] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla,\nPratul P Srinivasan, and Jonathan T Barron. Nerf in the dark:\nHigh dynamic range view synthesis from noisy raw images.\nIn IEEE/CVF Conf. on Computer Vision and Pattern Recog-\nnition (CVPR), pages 16190\u201316199, 2022.\n[44] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis.\nIn European Conf. on Computer Vision (ECCV),\npages 405\u2013421, 2020.\n[45] Helen Oleynikova, Zachary Taylor, Marius Fehr, Roland\nSiegwart, and Juan Nieto.\nVoxblox: Incremental 3d eu-\nclidean signed distance fields for on-board mav planning.\nIn IEEE/RSJ Intl. Conf. on Intelligent Robots and Systems\n(IROS), pages 1366\u20131373. IEEE, 2017.\n[46] Gerg\u02ddo Orb\u00b4an, Pietro Berkes, J\u00b4ozsef Fiser, and M\u00b4at\u00b4e Lengyel.\nNeural variability and sampling-based probabilistic repre-\nsentations in the visual cortex.\nNeuron, 92(2):530\u2013543,\n2016.\n[47] Joseph Ortiz, Alexander Clegg, Jing Dong, Edgar Sucar,\nDavid Novotny, Michael Zollhoefer, and Mustafa Mukadam.\nisdf: Real-time neural signed distance fields for robot per-\nception. In Robotics: Science and Systems (RSS), 2022.\n[48] Xuran Pan, Zihang Lai, Shiji Song, and Gao Huang. Ac-\ntivenerf: Learning where to see with uncertainty estimation.\nIn European Conf. on Computer Vision (ECCV), pages 230\u2013\n246. Springer, 2022.\n[49] Yue Pan, Yves Kompis, Luca Bartolomei, Ruben Mas-\ncaro, Cyrill Stachniss, and Margarita Chli. Voxfield: Non-\nprojective signed distance fields for online planning and 3d\nreconstruction. In IEEE/RSJ Intl. Conf. on Intelligent Robots\nand Systems (IROS), 2022.\n[50] Christos Papachristos, Shehryar Khattak, and Kostas Alexis.\nUncertainty-aware receding horizon exploration and map-\nping using aerial robots. In IEEE Intl. Conf. on Robotics\nand Automation (ICRA), pages 4568\u20134575. IEEE, 2017.\n[51] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn IEEE/CVF Conf. on Computer Vision and Pattern Recog-\nnition (CVPR), 2019.\n[52] Julio A Placed, Jared Strader, Henry Carrillo, Nikolay\nAtanasov, Vadim Indelman, Luca Carlone, and Jos\u00b4e A\nCastellanos. A survey on active simultaneous localization\nand mapping: State of the art and new frontiers. IEEE Trans.\nRobotics, 2023.\n[53] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall. Dreamfusion: Text-to-3d using 2d diffusion. In Intl.\nConf. on Learning Representations (ICLR), 2023.\n[54] Krishnan Raghavan and Prasanna Balaprakash. Formaliz-\ning the generalization-forgetting trade-off in continual learn-\ning. In Advances in Neural Information Processing Systems\n(NIPS), 2021.\n[55] Yunlong Ran, Jing Zeng, Shibo He, Jiming Chen, Lincheng\nLi, Yingfeng Chen, Gimhee Lee, and Qi Ye. Neurar: Neural\nuncertainty for autonomous 3d reconstruction with implicit\nneural representations. IEEE Robotics and Automation Let-\nters, 8(2):1125\u20131132, 2023.\n[56] Victor Massagu\u00b4e Respall, Dmitry Devitt, Roman Fedorenko,\nand Alexandr Klimchik. Fast sampling-based next-best-view\nexploration algorithm for a mav.\nIn IEEE Intl. Conf. on\nRobotics and Automation (ICRA), pages 89\u201395. IEEE, 2021.\n[57] Ziad Al-Halah Santhosh Kumar Ramakrishnan and Kristen\nGrauman. Occupancy anticipation for efficient exploration\nand navigation.\nIn European Conf. on Computer Vision\n(ECCV), 2020.\n[58] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv\nBatra. Habitat: A platform for embodied AI research. In Intl.\nConf. on Computer Vision (ICCV), 2019.\n[59] Lukas Schmid, Michael Pantic, Raghav Khanna, Lionel Ott,\nRoland Siegwart, and Juan Nieto.\nAn efficient sampling-\nbased method for online informative path planning in un-\nknown environments. IEEE Robotics and Automation Let-\nters, 5(2):1500\u20131507, 2020.\n[60] Magnus Selin, Mattias Tiger, Daniel Duberg, Fredrik Heintz,\nand Patric Jensfelt. Efficient autonomous exploration plan-\nning of large-scale 3d environments. IEEE Robotics and Au-\ntomation Letters, 4(2):1699\u20131706, 2019.\n[61] Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel\nPinto, Soumith Chintala, and Arthur Szlam.\nClip-fields:\nWeakly supervised semantic fields for robotic memory.\narXiv preprint arXiv: Arxiv-2210.05663, 2022.\n[62] Jianxiong Shen, Antonio Agudo, Francesc Moreno-Noguer,\nand Adria Ruiz. Conditional-flow nerf: Accurate 3d mod-\nelling with reliable uncertainty quantification. In European\nConf. on Computer Vision (ECCV), 2022.\n[63] Jianxiong Shen, Adria Ruiz, Antonio Agudo, and Francesc\nMoreno-Noguer. Stochastic neural radiance fields: Quanti-\nfying uncertainty in implicit 3d representations. In Intl. Conf.\non 3D Vision (3DV), pages 972\u2013981. IEEE, 2021.\n[64] Jianxiong Shen, Adria Ruiz, Antonio Agudo, and Francesc\nMoreno-Noguer. Stochastic neural radiance fields: Quanti-\nfying uncertainty in implicit 3d representations. In Intl. Conf.\non 3D Vision (3DV), pages 972\u2013981. IEEE, 2021.\n[65] Shaojie Shen, Nathan Michael, and Vijay Kumar.\nAu-\ntonomous indoor 3d exploration with a micro-aerial vehi-\ncle. In IEEE Intl. Conf. on Robotics and Automation (ICRA),\npages 9\u201315, 2012.\n[66] Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming\nZhan, and Xiao-Ming Wu. Overcoming catastrophic forget-\nting in incremental few-shot learning by finding flat minima.\nIn 34, pages 6747\u20136761, 2021.\n[67] Vincent Sitzmann, Julien Martel, Alexander Bergman, David\nLindell, and Gordon Wetzstein.\nImplicit neural represen-\ntations with periodic activation functions. In Advances in\nNeural Information Processing Systems (NIPS), volume 33,\npages 7462\u20137473, 2020.\n[68] Soohwan Song and Sungho Jo. Surface-based exploration\nfor autonomous 3d modeling.\nIn IEEE Intl. Conf. on\nRobotics and Automation (ICRA), pages 4319\u20134326. IEEE,\n2018.\n[69] Edgar Sucar, Shikun Liu, Joseph Ortiz, and Andrew J Davi-\nson. imap: Implicit mapping and positioning in real-time.\nIn Intl. Conf. on Computer Vision (ICCV), pages 6229\u20136238,\n2021.\n[70] Niko S\u00a8underhauf, Jad Abou-Chakra, and Dimity Miller.\nDensity-aware nerf ensembles:\nQuantifying predictive\nuncertainty in neural radiance fields.\narXiv preprint\narXiv:2209.08718, 2022.\n[71] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. In Advances in Neural Information Process-\ning Systems (NIPS), volume 33, pages 7537\u20137547, 2020.\n[72] Eli Verwimp, Matthias De Lange, and Tinne Tuytelaars. Re-\nhearsal revealed: The limits and merits of revisiting sam-\nples in continual learning. In Intl. Conf. on Computer Vision\n(ICCV), 2021.\n[73] Peter Whaite and Frank P Ferrie. Autonomous exploration:\nDriven by uncertainty. IEEE Trans. Pattern Anal. Machine\nIntell., 19(3):193\u2013205, 1997.\n[74] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee,\nIrfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra.\nDd-ppo: Learning near-perfect pointgoal navigators from 2.5\nbillion frames. In Intl. Conf. on Learning Representations\n(ICLR), 2019.\n[75] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra\nMalik, and Silvio Savarese. Gibson env: Real-world percep-\ntion for embodied agents. In IEEE/CVF Conf. on Computer\nVision and Pattern Recognition (CVPR), pages 9068\u20139079,\n2018.\n[76] Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, Dacheng\nTao, and Masashi Sugiyama.\nArtificial neural variability\nfor deep learning: On overfitting, noise memorization, and\ncatastrophic forgetting. Neural Computation, 33(8):2163\u2013\n2192, 2021.\n[77] Brian Yamauchi. A frontier-based approach for autonomous\nexploration. In IEEE Intl. Sym. on Computational Intelli-\ngence in Robotics and Automation (CIRA), pages 146\u2013151,\n1997.\n[78] Zike Yan, Yuxin Tian, Xuesong Shi, Ping Guo, Peng Wang,\nand Hongbin Zha. Continual neural mapping: Learning an\nimplicit scene representation from sequential observations.\nIn IEEE/CVF Conf. on Computer Vision and Pattern Recog-\nnition (CVPR), pages 15782\u201315792, 2021.\n[79] Kai Ye, Siyan Dong, Qingnan Fan, He Wang, Li Yi, Fei\nXia, Jue Wang, and Baoquan Chen. Multi-robot active map-\nping via neural bipartite graph matching. In IEEE/CVF Conf.\non Computer Vision and Pattern Recognition (CVPR), pages\n14839\u201314848, 2022.\n[80] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\nPixelnerf: Neural radiance fields from one or few images. In\nIEEE/CVF Conf. on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 4578\u20134587, 2021.\n[81] Gizem Y\u00a8uce, Guillermo Ortiz-Jim\u00b4enez, Beril Besbinar, and\nPascal Frossard. A structured dictionary perspective on im-\nplicit neural representations. In IEEE/CVF Conf. on Com-\nputer Vision and Pattern Recognition (CVPR), pages 19228\u2013\n19238, 2022.\n[82] Jing Zeng, Yanxu Li, Yunlong Ran, Shuo Li, Fei Gao,\nLincheng Li, Shibo He, Jiming Chen, and Qi Ye. Efficient\nview path planning for autonomous implicit reconstruction.\nIn IEEE Intl. Conf. on Robotics and Automation (ICRA),\npages 4063\u20134069. IEEE, 2023.\n[83] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and An-\ndrew J Davison. In-place scene labelling and understand-\ning with implicit scene representation. In IEEE/CVF Conf.\non Computer Vision and Pattern Recognition (CVPR), pages\n15838\u201315847, 2021.\n[84] Boyu Zhou, Yichen Zhang, Xinyi Chen, and Shaojie Shen.\nFuel: Fast uav exploration using incremental frontier struc-\nture and hierarchical planning. IEEE Robotics and Automa-\ntion Letters, 6(2):779\u2013786, 2021.\n[85] Cheng Zhu, Rong Ding, Mengxiang Lin, and Yuanyuan Wu.\nA 3d frontier-based exploration tool for mavs. In IEEE Intl.\nConf. on Tools with Artificial Intelligence (ICTAI), pages\n348\u2013352, 2015.\n[86] Zihan Zhu, Songyou Peng, Viktor Larsson, Weiwei Xu, Hu-\njun Bao, Zhaopeng Cui, Martin R Oswald, and Marc Polle-\nfeys. Nice-slam: Neural implicit scalable encoding for slam.\nIn IEEE/CVF Conf. on Computer Vision and Pattern Recog-\nnition (CVPR), pages 12786\u201312796, 2022.\n[87] Matt Zucker, Nathan Ratliff, Anca D Dragan, Mihail Piv-\ntoraiko, Matthew Klingensmith, Christopher M Dellin, J An-\ndrew Bagnell, and Siddhartha S Srinivasa. Chomp: Covari-\nant hamiltonian optimization for motion planning. Intl. J. of\nRobotics Research, 32(9-10):1164\u20131193, 2013.\nSupplementary Materials\nI. Runtime\nSupplementary to Fig. 7 of the main paper, we estimate\nthe average portion of the computational cost induced by\neach module in Fig. S1. The majority of the runtime is dis-\ntributed to the continual learning of the neural map (71%),\nwhile best view selection and identification, visualization,\nand meshing take most of the rest computational resources.\nII. Discussion on uncertainty quantification\nWe refer readers to [1] for a more comprehensive re-\nview of uncertainty quantification methods. For active map-\nping, we are interested in the epistemic uncertainty that\ncharacterizes the incomplete knowledge of D given exist-\ning training samples. In the field of implicit neural rep-\nresentation and learning-based active mapping, a few un-\ncertainty quantification methods are adopted, e.g., varia-\ntional inference [64, 62], distribution analysis of the output\nspace [36, 48], or ensemble technique [24, 70]. Variational\ninference based approaches reformulate the architecture and\nthe training process, hence limiting its application to exist-\ning neural representations. Ensemble methods require ran-\ndom initialization for each ensemble to induce prediction\nvariance, which is impractical for our continual learning\nsetting. In contrast to the distribution analysis of the out-\nput space, we examine the parameter space of the implicit\nneural representations and formulate active mapping from\na continual learning perspective. The proposed method is\nsimple and can be easily applied to different architectures\nand training paradigms. The proposed problem setting is\nalso similar to Bayesian active learning [23]. Hence, we\nalso adopt the MC-Dropout layers with a BALD score as\nthe substitute for Module 1.\nCompared to the proposed\nmethod, the involvement of Dropout leads to slow inference\nas a single forward pass leads to noisy prediction. Even\nthough the exploration efficiency is comparable (Tab. S4),\nthe reconstructed mesh from averaged SDF prediction is\nstill noisy (Fig. S4a) even after 5 runs. Note that most mod-\nules with 98% of the computational time in Fig. S1 require\na noise-free prediction. Increasing the number of feedfor-\nward passes would be computationally burdensome for an\nonline system.\n5%\n9%\n2%\n7%\n6%\n71%\nMapping\nBest view selection\nVisualization\nPlanning\nData processing\nMeshing\nTarget view identi\ufb01cation\nFigure S1: The runtime distribution of each module.\nIII. More experimental results\nIn this section, more results on the Gibson [75] and Mat-\nterport3D datasets [9] are provided as supplementary eval-\nuations to Tab. 1 and 2 of the main paper.\nIII.1. Per-scene evaluation\nWe conduct experiments on single-floor scenes with less\nthan 10 rooms. Scenes in Matterport3D are consistently\nlarger and more complex than in Gibson (see the number of\nkeyframes (Kfs) in Tab. S1). The keyframes are automati-\ncally selected as frames with high prediction losses. Both\na large scene that requires a long trajectory for exploration\nand a complex scene that can not be easily recovered lead\nto a large number of keyframes, hence indicating the scene\ncomplexity. For small scenes (num room <= 5), the active\nmapping is conducted in 1000 steps; While for large scenes\n(num room > 5), we run 2000 steps for better coverage. <\n5cm (%) is defined as the ratio of ground truth mesh ver-\ntices whose predicted distance value is within 5cm. Acc. is\ndefined as the mesh-wise mean distance between the esti-\nmated one and the ground truth.\nFrom the per-scene evaluation, we can obtain a better\n(a) Module 1 (Dropout)\n(b) Module 3 (ReLU)\n(c) Final\n(d) GT\nFigure S2: Extracted meshes from different baselines.\n(a) Module 1\n(b) Module 3\n(c) Final\nFigure S3: Learned SDF from different baselines.\nunderstanding of the proposed method. When comparing to\nthe relevant methods in Tab. S1, we can find a typical failure\ncase of HxpkQ, which will be further explained in Sec. III.2.\nThe major reason for the failure is the local planner that gets\nstuck in the narrow area. Meanwhile, the evaluations on\ndifferent module substitutes in Tab. S2-S4 uncover several\nimportant insights:\n\u2022 Trade-offs between mapping accuracy and area coverage.\nAs a surface-based exploration method, the choice of differ-\nent network architectures and penalty functions may lead\nto diverse performance.\nFor instance, the ReLU activa-\ntions without skip-connection lead to inferior accuracy in\nhigh-frequency areas (see Fig. S2b), hence the target view\nidentification module may guide the agent to visited areas\nas the high-frequency geometric details are not recovered\nwell. Consequently, the mesh quality and the exploration\ncompleteness decline (See Tab. S3 and S4).\n\u2022 Generalization ability matters.\nMeanwhile, there are\nalso different prediction behaviors in unobserved ar-\neas when deployed under different architectures/penalties,\nleading to different generalization abilities.\nAs illus-\ntrated in Fig. S3, the predicted distance values in un-\nexplored areas (within the green bounding box) are\nsmaller for module 3 baseline (PosEnc+ReLU) than the\nmodule 1 (PosEnc+SoftPlus+Dropout) and final baselines\n(PosEnc+SoftPlus). This may affect the generation of false-\npositive zero-crossing surfaces and the convergence rate\nwhen new data come, thus affecting the active mapping per-\nformance. That\u2019s also the reason why module 4 achieves\nhigher <5cm (%) in Tab. S2 even if it explores fewer areas\n(Tab. S4) with a worse mesh model (Tab. S3).\nIII.2. Limitations and typical challenges\nAs illustrated in Fig. S4, the proposed method can re-\ncover high-fidelity scene geometry through autonomous ex-\nploration. All rooms are well-explored, thus achieving high\nComp. in Tab. S1. Compared to other baselines, the pro-\nposed method achieves much better coverage. It should be\nnoted that the Comp. metrics are only affected by the ac-\ntion (trajectory). The results clearly verify the efficacy of\nthe proposed method.\nWe here emphasize the typical challenges that lead to\nunsatisfactory results. As illustrated in Fig. S5a, the bad\nSDF prediction results mainly contribute to incomplete ex-\nploration. On the other hand, the mesh quality is highly\nrelevant to the complexity of the scene geometry. As illus-\ntrated in Fig. S5c, although a straight path leads to efficient\nexploration, the church scenario is full of occlusions that\nmake accurate reconstruction non-trivial. Finally, as illus-\ntrated in Fig. S5b, as the agent gets stuck all the time, all\nmetrics are unpleasant for HxpKQ even if the environment\nis open without occlusions.\nWe can also find that the major reason for the fail-\nure cases is the unsatisfactory trajectory/navigation. Even\nthough target location candidates exist in the unexplored\nrooms (see Fig. S5a), entering a narrow door/corridor or\neven going upstairs brings little information gain sometimes\ndue to a limited field of view. If the agent gets stuck easily\nin nearby areas, the target candidate may never be chosen\nas the optimal one. For future extensions, better local plan-\nning strategies should be deployed to handle the challenges\nas discussed in Sec. 6.1 of the main paper.\nTable S1: Comparison against relevant methods regarding the completeness of actively-captured observations.\nComp. (%) \u2191\nRooms\nKfs\nRandom\nFBE\nUPEN\nOccAnt\nOurs\nGibson-Cantwell*\n8\n38\n24.43\n40.93\n39.42\n37.96\n61.36\nGibson-Denmark\n2\n18\n27.83\n70.28\n66.41\n65.07\n85.86\nGibson-Eastville*\n6\n34\n14.32\n58.49\n51.51\n27.03\n74.21\nGibson-Elmira\n3\n14\n66.29\n72.69\n82.14\n84.37\n91.65\nGibson-Eudora\n3\n13\n53.89\n76.65\n75.74\n74.07\n90.12\nGibson-Greigsville\n2\n23\n75.44\n90.34\n73.72\n88.27\n92.47\nGibson-Pablo\n4\n14\n46.87\n76.06\n54.16\n64.50\n72.88\nGibson-Ribera\n3\n14\n44.29\n79.26\n81.21\n66.97\n88.62\nGibson-Swormville*\n7\n31\n58.81\n55.46\n45.43\n48.64\n66.86\nGibson-mean\n4\n22\n45.80\n68.30\n63.30\n61.88\n80.45\nMP3D-GdvgF*\n6\n32\n68.45\n81.78\n82.39\n80.24\n80.99\nMP3D-gZ6f7\n1\n48\n29.81\n81.01\n82.96\n82.02\n80.68\nMP3D-HxpKQ*\n8\n32\n46.93\n58.71\n52.70\n60.50\n48.34\nMP3D-pLe4w\n2\n52\n32.92\n66.09\n66.76\n67.13\n76.41\nMP3D-YmJkq\n4\n68\n50.26\n68.32\n60.47\n68.70\n79.35\nMP3D-mean\n4\n46\n45.67\n68.53\n69.09\n71.72\n73.15\nComp. (cm)\u2193\nRandom\nFBE \u2191\nUPEN\nOccAnt\nOurs\n59.59\n37.03\n42.12\n43.27\n17.67\n50.42\n12.40\n17.34\n16.96\n3.78\n72.39\n24.08\n28.16\n60.67\n11.36\n11.63\n10.40\n5.35\n4.35\n2.57\n23.24\n8.11\n9.18\n9.27\n2.27\n6.97\n2.62\n16.34\n3.25\n1.78\n34.70\n6.38\n31.81\n20.49\n9.96\n33.27\n6.53\n5.74\n18.67\n4.13\n18.10\n22.19\n33.78\n32.34\n13.43\n34.48\n14.42\n21.09\n23.25\n7.44\n11.67\n5.48\n5.14\n5.66\n5.69\n46.48\n7.06\n6.14\n6.19\n7.43\n19.10\n11.75\n14.11\n11.75\n15.96\n30.79\n12.78\n11.82\n11.51\n8.03\n24.61\n11.85\n15.77\n11.90\n8.46\n26.53\n9.78\n10.60\n9.40\n9.11\n(a) Edgemere\n(b) Elmira\n(c) Eudora\n(d) Greigsville\nFigure S4: Examples of well-explored scenes. The ground truth mesh is in grey while the estimated one is in blue.\n(a) Gibson-Swormville\n(b) MP3D-HxpKQ\n(c) MP3D-YmJkq\nFigure S5: Typical challenges: (a) low <5cm(%) due to\nincomplete exploration; (b) low Comp. as the agent gets\nstuck; (c) high FPR due to extreme occlusions.\nTable S2: Comparison against baselines regarding the SDF prediction accuracy of the zero-crossing surface.\nMAD (cm) \u2193\nRooms\nKfs\nRandom\nModule 1\nModule 3\nModule 4\nFinal\nGibson-Cantwell*\n8\n38\n8.83\n6.10\n8.12\n8.28\n5.20\nGibson-Denmark\n2\n18\n9.06\n4.59\n4.74\n4.92\n4.32\nGibson-Eastville*\n6\n34\n16.66\n6.65\n5.19\n6.96\n6.64\nGibson-Elmira\n3\n14\n4.72\n5.01\n3.89\n3.89\n4.01\nGibson-Eudora\n3\n13\n8.03\n4.22\n4.62\n7.08\n4.02\nGibson-Greigsville\n2\n23\n4.55\n4.10\n4.35\n3.87\n4.18\nGibson-Pablo\n4\n14\n7.48\n4.87\n6.66\n7.96\n5.23\nGibson-Ribera\n3\n14\n6.29\n5.27\n4.95\n5.79\n5.13\nGibson-Swormville*\n7\n31\n10.80\n10.29\n6.46\n6.22\n7.17\nGibson-mean\n4\n22\n8.49\n5.68\n5.42\n6.10\n5.10\nMP3D-GdvgF*\n6\n32\n6.78\n6.67\n7.53\n5.97\n3.77\nMP3D-gZ6f7\n1\n48\n23.12\n4.12\n5.37\n4.46\n3.18\nMP3D-HxpKQ*\n8\n32\n4.60\n5.69\n7.31\n4.99\n7.03\nMP3D-pLe4w\n2\n52\n6.22\n3.14\n4.48\n2.83\n3.25\nMP3D-YmJkq\n4\n68\n3.65\n3.61\n5.60\n3.25\n4.22\nMP3D-mean\n4\n46\n8.87\n4.65\n6.06\n4.30\n4.29\n<5cm (%) \u2191\nRandom\nModule 1\nModule 3\nModule 4\nFinal\n39.07\n51.23\n46.52\n45.95\n59.93\n45.68\n68.47\n66.98\n65.22\n70.14\n15.50\n46.72\n56.95\n42.95\n51.33\n64.65\n63.04\n70.84\n71.42\n72.27\n51.20\n66.48\n66.88\n55.68\n71.98\n66.52\n71.61\n68.38\n71.10\n74.95\n50.49\n62.16\n45.36\n55.35\n60.41\n59.59\n58.42\n63.51\n58.66\n65.37\n28.40\n32.60\n49.16\n53.29\n44.16\n46.79\n57.86\n59.40\n57.74\n63.39\n67.27\n70.23\n63.39\n76.74\n77.05\n28.01\n77.59\n64.42\n75.42\n75.03\n67.80\n61.47\n49.52\n66.97\n53.72\n63.60\n78.79\n64.47\n82.78\n75.22\n72.44\n74.59\n55.56\n82.12\n72.67\n59.82\n72.53\n59.47\n76.81\n70.74\nTable S3: Comparison against baselines regarding the reconstructed mesh quality.\nFPR (%) \u2193\nRooms\nKfs\nRandom\nModule 1\nModule 3\nModule 4\nFinal\nGibson-Cantwell*\n8\n38\n51.32\n60.80\n37.37\n51.95\n45.41\nGibson-Denmark\n2\n18\n40.43\n17.13\n15.83\n22.96\n22.31\nGibson-Eastville*\n6\n34\n68.04\n54.89\n44.86\n61.70\n46.33\nGibson-Elmira\n3\n14\n25.02\n33.54\n16.71\n24.54\n15.94\nGibson-Eudora\n3\n13\n22.47\n17.69\n17.56\n23.57\n18.06\nGibson-Greigsville\n2\n23\n26.63\n27.80\n24.18\n26.12\n23.07\nGibson-Pablo\n4\n14\n28.82\n24.43\n21.99\n20.31\n22.51\nGibson-Ribera\n3\n14\n27.25\n20.53\n23.08\n28.88\n34.51\nGibson-Swormville*\n7\n31\n39.17\n55.04\n24.21\n35.72\n24.26\nGibson-mean\n4\n22\n36.57\n34.65\n25.09\n32.86\n28.04\nMP3D-GdvgF*\n6\n32\n25.55\n27.32\n22.74\n32.09\n22.68\nMP3D-gZ6f7\n1\n48\n69.45\n33.12\n36.68\n40.37\n33.53\nMP3D-HxpKQ*\n8\n32\n48.67\n46.31\n47.78\n60.29\n45.83\nMP3D-pLe4w\n2\n52\n50.02\n36.54\n31.69\n35.00\n35.76\nMP3D-YmJkq\n4\n68\n65.70\n71.86\n56.34\n72.20\n62.54\nMP3D-mean\n4\n46\n51.88\n43.03\n39.05\n47.99\n40.07\nAcc. (cm)\u2193\nRandom\nModule 1\nModule 3\nModule 4\nFinal\n13.83\n17.24\n7.97\n13.67\n9.70\n8.37\n3.63\n3.91\n4.56\n4.76\n34.60\n29.33\n24.54\n34.96\n20.39\n5.19\n6.56\n3.77\n5.27\n3.74\n4.67\n3.58\n3.84\n5.26\n3.99\n12.44\n14.09\n8.50\n6.35\n9.63\n8.39\n5.86\n4.78\n4.71\n6.34\n6.02\n5.52\n5.77\n11.26\n11.52\n10.69\n29.16\n6.11\n15.73\n5.10\n11.58\n12.77\n7.69\n11.31\n8.35\n6.55\n6.45\n5.93\n8.12\n5.09\n19.57\n7.89\n10.00\n9.13\n4.15\n12.43\n16.13\n12.28\n16.70\n15.60\n11.15\n8.01\n6.66\n7.39\n5.56\n37.58\n42.12\n28.28\n53.24\n8.61\n17.46\n16.12\n12.63\n18.92\n7.80\nTable S4: Comparison against baselines regarding the completeness of actively-captured observations.\nComp. (cm) \u2193\nRooms\nKfs\nRandom\nModule 1\nModule 3\nModule 4\nFinal\nGibson-Cantwell*\n8\n38\n59.59\n13.96\n66.23\n41.84\n17.67\nGibson-Denmark\n2\n18\n50.42\n1.86\n2.70\n3.00\n3.78\nGibson-Eastville*\n6\n34\n72.39\n14.32\n21.44\n11.28\n11.36\nGibson-Elmira\n3\n14\n11.63\n2.87\n1.38\n3.97\n2.57\nGibson-Eudora\n3\n13\n23.24\n2.05\n2.62\n2.43\n2.27\nGibson-Greigsville\n2\n23\n6.97\n1.87\n0.86\n1.37\n1.78\nGibson-Pablo\n4\n14\n34.70\n5.47\n13.92\n22.16\n9.96\nGibson-Ribera\n3\n14\n33.27\n4.97\n4.32\n15.99\n4.13\nGibson-Swormville*\n7\n31\n18.10\n20.92\n12.85\n17.50\n13.43\nGibson-mean\n4\n22\n34.48\n7.59\n14.04\n13.28\n7.44\nMP3D-GdvgF*\n6\n32\n11.67\n5.13\n4.92\n7.67\n5.69\nMP3D-gZ6f7\n1\n48\n46.48\n7.53\n8.94\n8.63\n7.43\nMP3D-HxpKQ*\n8\n32\n19.10\n13.85\n14.69\n30.37\n15.96\nMP3D-pLe4w\n2\n52\n30.79\n7.51\n6.14\n5.15\n8.03\nMP3D-YmJkq\n4\n68\n24.61\n15.05\n8.41\n12.01\n8.46\nMP3D-mean\n4\n46\n26.53\n9.78\n8.62\n12.77\n9.11\nComp. (%) \u2191\nRandom\nModule 1\nModule 3\nModule 4\nFinal\n24.43\n68.77\n23.07\n39.19\n61.36\n27.83\n93.46\n88.90\n88.00\n85.86\n14.32\n57.67\n66.21\n73.67\n74.21\n66.29\n89.03\n95.04\n86.04\n91.65\n53.89\n91.57\n90.29\n90.06\n90.12\n75.44\n91.32\n97.79\n94.70\n92.47\n46.87\n79.74\n70.50\n63.97\n72.88\n44.29\n85.86\n87.01\n67.98\n88.62\n58.81\n57.92\n66.88\n59.73\n66.86\n45.80\n79.48\n76.19\n73.70\n80.45\n68.45\n82.14\n82.51\n76.75\n80.99\n29.81\n80.99\n79.06\n76.75\n80.68\n46.93\n49.71\n51.26\n34.32\n48.34\n32.92\n78.91\n83.02\n86.10\n76.41\n50.26\n65.32\n77.29\n64.85\n79.35\n45.67\n71.41\n74.63\n67.75\n73.15\n"
  },
  {
    "title": "GNFactor: Multi-Task Real Robot Learning with Generalizable Neural Feature Fields",
    "link": "https://arxiv.org/pdf/2308.16891.pdf",
    "upvote": "8",
    "text": "GNFactor: Multi-Task Real Robot Learning with\nGeneralizable Neural Feature Fields\nYanjie Ze1\u2217\nGe Yan2\u2217\nYueh-Hua Wu2\u2217\nAnnabella Macaluso2\nYuying Ge3\nJianglong Ye2\nNicklas Hansen2\nLi Erran Li4\nXiaolong Wang2\n1Shanghai Jiao Tong University 2UC San Diego\n3University of Hong Kong 4AWS AI, Amazon\n\u2217Equal Contribution\nyanjieze.com/GNFactor\nAbstract: It is a long-standing problem in robotics to develop agents capable\nof executing diverse manipulation tasks from visual observations in unstructured\nreal-world environments. To achieve this goal, the robot needs to have a compre-\nhensive understanding of the 3D structure and semantics of the scene. In this work,\nwe present GNFactor, a visual behavior cloning agent for multi-task robotic ma-\nnipulation with Generalizable Neural feature Fields. GNFactor jointly optimizes\na generalizable neural field (GNF) as a reconstruction module and a Perceiver\nTransformer as a decision-making module, leveraging a shared deep 3D voxel\nrepresentation. To incorporate semantics in 3D, the reconstruction module uti-\nlizes a vision-language foundation model (e.g., Stable Diffusion) to distill rich\nsemantic information into the deep 3D voxel. We evaluate GNFactor on 3 real\nrobot tasks and perform detailed ablations on 10 RLBench tasks with a limited\nnumber of demonstrations. We observe a substantial improvement of GNFactor\nover current state-of-the-art methods in seen and unseen tasks, demonstrating the\nstrong generalization ability of GNFactor.\nKeywords: Robotic Manipulation, Neural Radiance Field, Behavior Cloning\n1\nIntroduction\nOne major goal of introducing learning into robotic manipulation is to enable the robot to effectively\nhandle unseen objects and successfully tackle various tasks in new environments. In this paper, we\nfocus on using imitation learning with a few demonstrations for multi-task manipulation. Using imi-\ntation learning helps avoid complex reward design and training can be directly conducted on the real\nrobot without creating its digital twin in simulation [1, 2, 3, 4]. This enables policy learning on di-\nverse tasks in complex environments, based on users\u2019 instructions (see Figure 1). However, working\nwith a limited number of demonstrations presents great challenges in terms of generalization. Most\nof these challenges arise from the need to comprehend the 3D structure of the scene, understand the\nsemantics and functionality of objects, and effectively follow task instructions based on visual cues.\nTherefore, a comprehensive and informative visual representation of the robot\u2019s observations serves\nas a crucial foundation for generalization.\nThe development of visual representation for robot learning has mainly focused on learning within\na 2D plane. Self-supervised objectives are leveraged to pre-train the representation from the 2D\nimage observation [6, 7, 8] or jointly optimized with the policy gradients [9, 10, 11]. While these\napproaches improve sample efficiency and lead to more robust policies, they are mostly applied to\nrelatively simple manipulation tasks. To tackle more complex tasks requiring geometric understand-\ning (e.g., object shape and pose) and with occlusions, 3D visual representation learning has been\nrecently adopted with robot learning [11, 12]. For example, Driess et al. [12] train the 3D scene\nrepresentation by using NeRF and view synthesis to provide supervision. While it shows effective-\nness over tasks requiring geometric reasoning such as hanging a cup, it only handles the simple\n7th Conference on Robot Learning (CoRL 2023), Atlanta, USA.\narXiv:2308.16891v2  [cs.RO]  1 Sep 2023\n\u201cTurn the Faucet\u201d\n\u201cOpen the Top Microwave Door\u201d \u201cPlace the Tea Pot on  the Stove\u201d\nKitchen 1\nKitchen 2\nFront\nLeft\nDiffusion Feature Field\nFigure 1: Left: Three camera views used in the real robot setup to reconstruct the feature field generated\nby Stable Diffusion [5]. We segment the foreground feature for better illustration. Right: Three language-\nconditioned real robot tasks across two different kitchens.\nscene structure with heavy masking in a single-task setting. More importantly, without a semantic\nunderstanding of the scene, it would be very challenging for the robot to follow the user\u2019s language\ninstructions.\nIn this paper, we introduce learning a language-conditioned policy using a novel representation\nleveraging both 3D and semantic information for multi-task manipulation. We train Generalizable\nNeural Feature Fields (GNF) which distills pre-trained semantic features from 2D foundation mod-\nels into the Neural Radiance Fields (NeRFs). We conduct policy learning upon this representation,\nleading to our model GNFactor. It is important to note that GNFactor learns an encoder to extract\nscene features in a feed-forward manner, instead of performing per-scene optimization in NeRF.\nGiven a single RGB-D image observation, our model encodes it into a 3D semantic volumetric fea-\nture, which is then processed by a Perceiver Transformer [13] architecture for action prediction.\nTo conduct multi-task learning, the Perceiver Transformer takes in language instructions to get task\nembedding, and reason the relations between the language and visual semantics for manipulation.\nThere are two branches of training in our framework (see Figure 3): (i) GNF training. Given the\ncollected demonstrations, we train the Generalizable Neural Feature Fields using view synthesis\nwith volumetric rendering. Besides rendering the RGB pixels, we also render the features of the\nfoundation models in 2D space. The GNF learns from both pixel and feature reconstruction at\nthe same time. To provide supervision for feature reconstruction, we apply a vision foundation\nmodel (e.g., pre-trained Stable Diffusion model [5]) to extract the 2D feature from the input view\nas the ground truth. In this way, we can distill the semantic features into the 3D space in GNF. (ii)\nGNFactor joint training. Building on the 3D volumetric feature jointly optimized by the learning\nobjectives of GNF, we conduct behavior cloning to train the whole model end-to-end.\nFor evaluation, we conduct real-robot experiments on three distinct tasks across two different\nkitchens (see Figure 1). We successfully train a single policy that effectively addresses these tasks\nin different scenes, yielding significant improvements over the baseline method PerAct [3]. We also\nconduct comprehensive evaluations using 10 RLBench simulated tasks [14] and 6 designed general-\nization tasks. We observe that GNFactor outperforms PerAct with an average improvement of 1.55x\nand 1.57x, consistent with the significant margin observed in the real-robot experiments.\n2\nRelated Work\nMulti-Task Robotic Manipulation. Recent works in multi-task robotic manipulation have led to\nsignificant progress in the execution of complex tasks and the ability to generalize to new scenar-\nios [15, 2, 1, 16, 17, 3, 18, 19]. Notable methods often involve the use of extensive interaction data\nto train multi-task models [2, 1, 16, 17]. For example, RT-1 [1] underscores the benefits of task-\nagnostic training, demonstrating superior performance in real-world robotic tasks across a variety of\ndatasets. To reduce the need for extensive demonstrations, methods that utilize keyframes \u2013 which\nencode the initiation of movement \u2013 have proven to be effective [20, 21, 22, 23, 24]. PerAct [3]\nemploys the Perceiver Transformer [13] to encode language goals and voxel observations and shows\nits effectiveness in real robot experiments. In this work, we utilize the same action prediction frame-\n2\n(a) RGB observations for 10 RLBench tasks.\n(c) Real robot setup.\n(b) Sampled views  for GNF training in simulation.\nFigure 2: Simulation environments and the real robot setup. We show the RGB observations for our 10\nRLBench tasks in Figure (a), the sampled views for GNF in Figure (b), and the real robot setup in Figure (c).\nwork as PerAct while we focus on improving the generalization ability of this framework by learning\na generalizable volumetric representation under limited data.\n3D Representations for Reinforcement/Imitation Learning (RL/IL). To improve manipulation\npolicies by leveraging visual information, numerous studies have concentrated on enhancing 2D vi-\nsual representations [8, 7, 6, 25], while for addressing more complex tasks, the utilization of 3D\nrepresentations becomes crucial. Ze et al. [11] incorporates a deep voxel-based 3D autoencoder in\nmotor control, demonstrating improved sample efficiency compared to 2D representation learning\nmethods. Driess et al. [12] proposes to first learn a state representation by NeRF and then use the\nfrozen state for downstream RL tasks. While this work shows the initial success of utilizing NeRF\nin RL, its applicability in real-world scenarios is constrained due to various limitations: e.g., the\nrequirement of object masks, the absence of a robot arm, and the lack of scene structure. The work\nclosest to ours is SNeRL [26], which also utilizes a vision foundation model in NeRF. However, sim-\nilar to NeRF-RL [12], SNeRL masks the scene structure to ensure functionality and the requirement\nfor object masks persists, posing challenges for its application in real robot scenarios. Our proposed\nGNFactor, instead, handles challenging muti-task real-world scenarios, demonstrating the potential\nfor real robot applications.\nNeural Radiance Fields (NeRFs). Neural fields have achieved great success in novel view synthesis\nand scene representation learning these years [27, 28, 29, 30, 31, 32], and recent works also start to\nincorporate neural fields into robotics [33, 34, 35, 12, 26]. NeRF [29] stands out for achieving pho-\ntorealistic view synthesis by learning an implicit function of the scene, while it requires per-scene\noptimization and is thus hard to generalize. Many following methods [36, 37, 38, 39, 40, 41, 42]\npropose more generalizable NeRFs. PixelNeRF [43] and CodeNeRF [37] encode 2D images as the\ninput of NeRFs, while TransINR [36] leverages a vision transformer to directly infer NeRF parame-\nters. A line of recent works [44, 45, 46, 47, 48, 49] utilize pre-trained vision foundation models such\nas DINO [50] and CLIP [51] as supervision besides the RGB image, which thus enables the NeRF\nto learn generalizable features. In this work, we incorporate generalizable NeRF to reconstruct\ndifferent views in RGB and embeddings from a pretrained Stable Diffusion model [5].\n3\nMethod\nIn this section, we detail the proposed GNFactor, a multi-task agent with a 3D volumetric represen-\ntation for real-world robotic manipulation. GNFactor is composed of a volumetric rendering module\nand a 3D policy module, sharing the same deep volumetric representation. The volumetric render-\ning module learns a Generalizable Neural Feature Field (GNF), to reconstruct the RGB image from\ncameras and the embedding from a vision-language foundation model, e.g., Stable Diffusion [5].\nThe task-agnostic nature of the vision-language embedding enables the volumetric representation to\nlearn generalizable features via neural rendering and thus helps the 3D policy module better handle\nmulti-task robotic manipulation. The task description is encoded with CLIP [51] to obtain the task\nembedding T. An overview of GNFactor is shown in Figure 3.\n3\nDeep 3D Volume\nRGB-D\n(Front View)\nVoxel\nEncoder\nVoxelize\n\u201cPlace the Teapot\non the Stove\u201d\nLanguage\nEncoder\nTokenize\nPerceiver\n\u2026\nPatchify\nRobot State\n\ud835\udc44trans\n\ud835\udc44rot\n\ud835\udc44collide\n\ud835\udc44open\nRenderer\nFigure 3: Overview of GNFactor. GNFactor takes an RGB-D image as input and encodes it using a voxel\nencoder to transform it into a feature in deep 3D volume. This volume is then shared by two modules: vol-\numetric rendering (Renderer) and robot action prediction (Perceiver). These two modules are jointly trained,\nwhich optimizes the shared features to not only reconstruct vision-language embeddings (Diffusion Feature)\nand other views (RGB), but also to estimate accurate Q-values (Qtrans, Qrot, Qcollide, Qopen).\n3.1\nProblem Definition\nTo effectively address complex real-world robotic problems, we structure the observation space as\na 3D voxel space O \u2208 R1003\u00d73, as opposed to the commonly used 2D images [1, 2, 7, 8]. The 3D\nvoxel observation originates from an RGB-D image captured by a single front camera with known\nextrinsic and intrinsic parameters, ensuring our method\u2019s practical applicability in the real world.\nIn addition to the front camera view used for policy training, we also gather additional k views for\ntraining the GNF. We collect only RGB images for these additional views instead of RGB-D images.\nIn real-world scenarios, we use k = 2, while in simulated environments, we set k = 19.\nThe action of the robot arm with a gripper is represented by translation atrans \u2208 R3, rotation arot \u2208\nR(360/5)\u00d73, gripper openness aopen \u2208 [0, 1], and collision avoidance acollision \u2208 [0, 1]. For the rotation\narot, each rotation axis is discretized into R = 5 bins. The collision avoidance parameter acollision\ninstructs the motion planner regarding the necessity to avoid collisions, which is crucial as our tasks\nencompasses both contact-based and non-contact-based motions.\nDue to the inefficiency of continuous action prediction and the extensive data requirements that come\nwith it, we reformulate the behavior cloning problem as a keyframe-prediction problem [3, 52]. We\nfirst extract keyframes from expert demonstrations using the following metric: a frame in the tra-\njectory is a keyframe when joint velocities approach zero and the gripper\u2019s open state remains con-\nstant. The model is then trained to predict the subsequent keyframe based on current observations.\nThis formulation effectively transforms the continuous control problem into a discretized keyframe-\nprediction problem, delegating the internal procedures to the RRT-connect motion planner [53] in\nsimulation and Linear motion planner in real-world xArm7 robot.\n3.2\nLearning Volumetric Representations with Generalizable Neural Feature Fields\nIn our initial step, we transform the RGB-D image into a 1003 voxel. Then the 3D voxel encoder\nencodes this 3D voxel and outputs our volumetric representation v \u2208 R1003\u00d7128. To enhance the\nvolumetric representation v with structural knowledge and language semantics, we learn a General-\nizable Neural Feature Field (GNF) that takes the deep volume v as the scene representation and the\nmodel is learned by reconstructing the additional views and the features predicted by a 2D vision-\nlanguage foundation model [5]. The entire neural rendering process is described as follows.\nWe denote vx \u2208 R128 as the sampled 3D feature for the 3D point x using the volumetric represen-\ntation v. vx is formed with trilinear interpolation due to the discretized nature of the volume v. Our\nGNF primarily consists of three functions: (i) one density function \u03c3(x, vx) : R3+128 7\u2192 R+ that\n4\nmaps the 3D point x and the 3D feature vx to the density \u03c3, (ii) one RGB function c(x, d, vx) :\nR3+3+128 7\u2192 R3 that maps the 3D point x, the view direction d, and the 3D feature vx to color,\nand (iii) one vision-language embedding function f(x, d, vx) : R3+3+128 7\u2192 R512 that maps the 3D\npoint x, the view direction d, and the 3D feature vx to the vision-language embedding. In Figure 3,\nthe corresponding components of these three functions are illustrated. Given a pixel\u2019s camera ray\nr(t) = o + td, which is defined by the camera origin o \u2208 R3, view direction d and depth t with\nbounds [tn, tf], the estimated color and embedding of the ray can be calculated by:\n\u02c6C(r, v) =\nZ tf\ntn\nT(t)\u03c3(r(t), vx(t))c(r(t), d, vx(t))dt ,\n\u02c6F(r, v) =\nZ tf\ntn\nT(t)\u03c3(r(t), vx(t))f(r(t), d, vx(t))dt ,\n(1)\nwhere T(t) = exp\n\u0010\n\u2212\nR t\ntn \u03c3(s)ds\n\u0011\n. The integral is approximated with numerical quadrature in the\nimplementation. Our GNF is then optimized to reconstruct the RGB image and the vision-language\nembedding from multiple views and diverse scenes by minimizing the following loss:\nLrecon =\nX\nr\u2208R\n\u2225C(r) \u2212 \u02c6C(r)\u22252\n2 + \u03bbfeat\u2225F(r) \u2212 \u02c6F(r)\u22252\n2 ,\n(2)\nwhere C(r) is the ground truth color, F(r) is the ground truth vision-language embedding generated\nby Stable Diffusion, R is the set of rays generated from camera poses, and \u03bbfeat is the weight for the\nembedding reconstruction loss. For efficiency, we sample bray rays given one target view, instead\nof reconstructing the entire image. To help the GNF training, we use a coarse-to-fine hierarchical\nstructure as the original NeRF [29] and apply depth-guided sampling [54] in the \u201cfine\u201d network.\n3.3\nAction Prediction with Volumetric Representations\nThe volumetric representation v is optimized not only to achieve reconstruction of the GNF module,\nbut also to predict the desired action for accomplishing manipulation tasks within the 3D policy. As\nsuch, we jointly train the representation v to satisfy the objectives of both the GNF and the 3D policy\nmodule. In this section, we elaborate the training objective and the architecture of the 3D policy.\nWe employ a Perceiver Transformer [3] to handle the high-dimensional multi-modal input, i.e., the\n3D volume, the robot\u2019s proprioception, and the language feature. We first condense the shared\nvolumetric representation v into a volume of size 203 \u00d7 128 using a 3D convolution layer with a\nkernel size and stride of 5, followed by a ReLU function, and flatten the 3D volume into a sequence\nof small cubes of size 8000 \u00d7 128. The robot\u2019s proprioception is projected into a 128-dimensional\nspace and concatenated with the volume sequence for each cube, resulting in a sequence of size\n8000\u00d7256. We then project the language token features from CLIP into the same dimensions (77\u00d7\n256) and concatenate these features with a combination of the 3D volume, the robot\u2019s proprioception\nstate, and the CLIP token embedding. The result is a sequence with dimensions of 8077 \u00d7 256.\nThis sequence is combined with a learnable positional encoding and passed through the Perceiver\nTransformer, which outputs a sequence of the same size. We remove the last 77 features for the ease\nof voxelization [3] and reshape the sequence back to a voxel of size 203 \u00d7 256. This voxel is then\nupscaled to 1003 \u00d7 128 with trilinear interpolation and referred to as vPT. vPT is shared across three\naction prediction heads (Qopen, Qtrans, Qrot, Qcollide in Figure 3) to determine the final robot actions\nat the same scale as the observation space. To retain the learned features from GNF training, we\ncreate a skip connection between our volumetric representation v and vPT. The combined volume\nfeature (v, vPT) is used to predict a 3D Q-function Qtrans for translation, as well as Q-functions\nfor other robot operations like gripper openness (Qopen), rotation (Qrot), and collision avoidance\n(Qcollide). The Q-function here represents the action values of one timestep, differing from the\ntraditional Q-function in RL that is for multiple timesteps. For example, in each timestep, the 3D\nQtrans-value would be equal to 1 for the most possible next voxel and 0 for other voxels. The model\nthen optimizes the cross-entropy loss like a classifier,\nLaction = \u2212EYtrans [log Vtrans ] \u2212 EYrot [log Vrot ] \u2212 EYopen [log Vopen ] \u2212 EYcollide [log Vcollide ] ,\n(3)\n5\n0\n20\n40\nSuccess Rates\n20.4\n31.7\nMulti-Task (10)\n0\n20\n40\n18.0\n28.3\nGeneralization (6)\n0\n20\n40\n60\n28.3\n54.8\nMulti-Task (6)\n0\n20\n40\n16.7\n33.3\nGeneralization (6)\nPerAct\nGNFactor (ours)\nRLBench                                                  Real Robot\nFigure 4: Main experiment results. We present the average success rates in both the multi-task and gener-\nalization settings across RLBench tasks and real robot tasks. The error bar represents one standard deviation.\nThe number in the bracket denotes the number of tasks.\nwhere Vi = softmax(Qi) for Qi \u2208 [Qtrans, Qopen, Qrot, Qcollide] and Yi \u2208 [Ytrans, Yrot, Yopen, Ycollide]\nis the ground truth one-hot encoding. The overall learning objective for GNFactor is as follows:\nLGNFactor = Laction + \u03bbreconLrecon ,\n(4)\nwhere \u03bbrecon is the weight for the reconstruction loss to balance the scale of different objectives. To\ntrain the GNFactor, we employ a joint training approach in which the GNF and 3D policy module are\noptimized jointly, without any pre-training. From our empirical observation, this approach allows\nfor better fusion of information from the two modules when learning the shared features.\n4\nExperiments\nIn this section, we conduct experiments to answer the following questions: (i) Can GNFactor surpass\nthe baseline model in simulated environments? (ii) Can GNFactor generalize to novel scenes in\nsimulation? (iii) Does GNFactor learn a superior policy that handles real robot tasks in two different\nkitchens with noisy and limited real-world data? (iv) What are the crucial factors in GNFactor to\nensure the functionality of the entire system? Our concluded results are given in Figure 4.\n4.1\nExperiment Setup\nFor the sake of reproducibility and benchmarking, we conduct our primary experiments in RLBench\nsimulated tasks. Furthermore, to show the potential of GNFactor in the real world, we design a set\nof real robot experiments across two kitchens. We compare our GNFactor with the strong language-\nconditioned multi-task agent PerAct [3] in both simulation and the real world, emphasizing the\nuniversal functionality of GNFactor. Both GNFactor and PerAct use the single RGB-D image from\nthe front camera as input to construct the voxel grid. In the multi-task simulation experiments, we\nalso create a stronger version of PerAct by adding more camera views as input to fully cover the\nscene (visualized in Figure 10). Figure 2 shows our simulation tasks and the real robot setup. We\nbriefly describe the tasks and details are left in Appendix A and Appendix B.\nSimulation. We select 10 challenging language-conditioned manipulation tasks from the RLBench\ntasksuites [14]. Each task has at least two variations, totaling 166 variations. These variations\nencompass several types, such as variations in shape and color. Therefore, to achieve high success\nrates with very limited demonstrations, the agent needs to learn generalizable knowledge about\nmanipulation instead of merely overfitting to the given demonstrations. We use the RGB-D image\nof size 128 \u00d7 128 \u00d7 3 from the single front camera as the observation. To train the GNF, we also\nadd additional 19 camera views to provide RGB images as supervision.\nReal robot. We use the xArm7 robot with a parallel gripper in real robot experiments. We set up\ntwo toy kitchen environments to make the agent generalize manipulation skills across the scenes and\ndesigned three manipulation tasks, including open the microwave door, turn the faucet, and relocate\nthe teapot, as shown in Figure 1. We set up three RealSense cameras around the robot. Among\nthe three cameras, the front one captures the RGB-D observations for the policy training and the\nleft/right one provides the RGB supervision for the GNF training.\n6\nTable 1: Multi-task test results on RLBench. We evaluate 25 episodes for each checkpoint on 10 tasks\nacross 3 seeds and report the success rates (%) of the final checkpoints. Our method outperforms the most\ncompetitive baseline PerAct [3] with an average improvement of 1.55x and even still largely surpasses PerAct\nwith 4 cameras as input. The additional camera views are visualized in Figure 10.\nMethod / Task\nclose jar\nopen drawer\nsweep to dustpan\nmeat off grill\nturn tap\nAverage\nPerAct\n18.7\u00b18.2\n54.7\u00b118.6\n0.0\u00b10.0\n40.0\u00b117.0\n38.7\u00b16.8\nPerAct (4 Cameras)\n21.3\u00b17.5\n44.0\u00b111.3\n0.0\u00b10.0\n65.3\u00b113.2\n46.7\u00b13.8\nGNFactor\n25.3\u00b16.8\n76.0\u00b15.7\n28.0\u00b115.0\n57.3\u00b118.9\n50.7\u00b18.2\nMethod / Task\nslide block\nput in drawer\ndrag stick\npush buttons\nstack blocks\nPerAct\n18.7\u00b113.6\n2.7\u00b13.3\n5.3\u00b15.0\n18.7\u00b112.4\n6.7\u00b11.9\n20.4\nPerAct (4 Cameras)\n16.0\u00b114.2\n6.7\u00b16.8\n12.0\u00b13.3\n9.3\u00b11.9\n5.3\u00b11.9\n22.7\nGNFactor\n20.0\u00b115.0\n0.0\u00b10.0\n37.3\u00b113.2\n18.7\u00b110.0\n4.0\u00b13.3\n31.7\nTable 2: Generalization to unseen tasks on RLBench. We evaluate 20 episodes for each task with the final\ncheckpoint across 3 seeds. We denote \u201cL\u201d as a larger object, \u201cS\u201d as a smaller object, \u201cN\u201d as a new position, and\n\u201cD\u201d as adding a distractor. Our method outperforms PerAct with an average improvement of 1.57x.\nMethod / Task\ndrag (D)\nslide (L)\nslide (S)\nopen (n)\nturn (N)\npush (D)\nAverage\nPerAct\n6.6\u00b14.7\n33.3\u00b14.7\n5.0\u00b14.1\n25.0\u00b110.8\n18.3\u00b16.2\n20.0\u00b17.1\n18.0\nGNFactor\n46.7\u00b130.6\n25.0\u00b14.1\n6.7\u00b16.2\n31.7\u00b16.2\n28.3\u00b12.4\n31.7\u00b12.4\n28.3\nExpert Demonstrations. We collect 20 demonstrations for each RLBench task with the motion\nplanner. The task variation is uniformly sampled. We collect 5 demonstrations for each real robot\ntask using a VR controller. Details for collection remain in Appendix C.\nGeneralization tasks. To further show the generalization ability of GNFactor, we design additional\n6 simulated tasks and 3 real robot tasks based on the original training tasks and add task distractors.\nTraining details. One agent is trained with two NVIDIA RTX3090 GPU for 2 days (100k iterations)\nwith a batch size of 2. The shared voxel encoder of GNFactor is implemented as a lightweight 3D\nUNet with only 0.3M parameters. The Perceiver Transformer keeps the same number of parameters\nas PerAct [3] (25.2M parameters), making our comparison with PerAct fair.\n4.2\nSimulation Results\nWe report the success rates for multi-task tests on RLBench in Table 1 and for generalization to new\nenvironments in Table 2. We conclude our observations as follows:\nDominance of GNFactor over PerAct for multi-task learning. As shown by Table 1 and Fig-\nure 4, GNFactor achieves higher success rates across various tasks compared to PerAct, particularly\nexcelling in challenging long-horizon tasks. For example, in sweep to dustpan task, the robot\nneeds to first pick up the broom and use the broom to sweep the dust into the dustpan. We find that\nGNFactor achieves a success rate of 28.0%, while PerAct could not succeed at all. In simpler tasks\nlike open drawer where the robot only pulls the drawer out, both GNFactor and PerAct perform\nreasonably well, with success rates of 76.0% and 54.7% respectively. Furthermore, we observe\nthat enhancing PerAct with extra camera views does not result in significant improvements. This\nunderscores the importance of efficiently utilizing the available camera views.\nGeneralization ability of GNFactor to new tasks. In Table 2, we observe that the change made on\nthe environments such as distractors impacts all the agents negatively, while GNFactor shows better\ncapability of generalization on 5 out of 6 tasks compared to PerAct. We also find that for some\nchallenging variations such as the smaller block in the task slide (S), both GNFactor and PerAct\nstruggle to handle. This further emphasizes the importance of robust generalization skills.\nAblations. We summarize the key components in GNFactor that contribute to the success of the\nvolumetric representation in Table 4. From the ablation study, we gained several insights:\n(i) Our GNF reconstruction module plays a crucial role in multi-task robot learning. Moreover, the\nRGB loss is essential for learning a consistent 3D feature in addition to the feature loss, especially\nsince the features derived from foundation models are not inherently 3D consistent.\n(ii) The volumetric representation benefits from Diffusion features and depth-guided sampling,\nwhere the depth prior is utilized to enhance the sampling quality in neural rendering.\nAn\n7\nTable 3: Multi-task test results on real robot. We evaluate 10 episodes for each task and report the resulting\nsuccess rate (%). We denote \u201cdoor\u201d as \u201copen door\u201d, \u201cfaucet\u201d as \u201cturn faucet\u201d, and \u201cteapot\u201d as \u201crelocate teapot\u201d.\nThe number in the parenthesis suggests the kitchen ID and \u201cd\u201d suggests testing with distractors.\nMethod / Task\ndoor (1)\nfaucet (1)\nteapot (1)\ndoor (1,d)\nfaucet (1,d)\nteapot (1,d)\nAverage\nPerAct\n30\n80\n0\n10\n50\n0\nGNFactor\n40\n80\n40\n30\n50\n30\nMethod / Task\ndoor (2)\nfaucet (2)\nteapot (2)\ndoor (2,d)\nfaucet (2,d)\nteapot (2,d)\nPerAct\n10\n50\n0\n10\n30\n0\n22.5\nGNFactor\n50\n70\n40\n20\n40\n30\n43.3\nintuitive explanation is that GNF, when combined with DGS, becomes more adept at learn-\ning depth and 3D structure information.\nThis enhanced understanding allows the 3D rep-\nresentation to better concentrate on the surfaces of objects rather than the entire volume.\nTable 4: Ablations. We report the\naveraged success rates on 10 RL-\nBench tasks.\n\u201cDGS\u201d is short for\ndepth-guided sampling. \u201c\u2192\u201d means\nreplacing.\nAblation\nSuccess Rate (%)\nGNFactor\n36.8\nw/o. GNF objective\n24.2\nw/o. RGB objective\n27.2\nw/o. Diffusion\n30.0\nDiffusion \u2192 DINO\n30.4\nDiffusion \u2192 CLIP\n32.0\nw/o. DGS\n29.2\nw/o. skip connection\n27.6\nk = 19 \u2192 9\n33.2\n\u03bbfeat = 0.01 \u2192 1.0\n35.2\n\u03bbrecon = 0.01 \u2192 1.0\n35.2\nMoreover, replacing Stable Diffusion with DINO [50] or\nCLIP [51] would not result in similar improvements easily, indi-\ncating the importance of our vision-language feature.\n(iii) While the use of skip connection is not a new story and we\nmerely followed the structure of PerAct, the result of removing\nthe skip connection suggests that our voxel representation, which\ndistills features from the foundation model, plays a critical role\nin predicting the final action.\n(iv) Striking a careful balance between the neural rendering loss\nand the action prediction loss is critical for optimal performance\nand utilizing information from multiple views by our GNF mod-\nule proves to be beneficial for the single-view decision module.\nFurthermore, we provide the view synthesis in the real world, generated by GNFactor in Figure 5\nand Figure 6. We also give the quantitative evaluation measured by PSNR [29]. We observe that\nthe rendered views are somewhat blurred since the volumetric presentation learned by GNFactor is\noptimized to minimize both the neural rendering loss as well as the action prediction loss, and the\nrendering quality is largely improved when the behavior cloning loss is removed and only the GNF is\ntrained. Notably, for the view synthesis in the real world, we do not have access to ground-truth point\nclouds for either training or testing. Instead, the point clouds are sourced from RealSense cameras\nand are therefore imperfect. Despite the limitations in achieving accurate pixel-level reconstruction\nresults, we focus on learning semantic understanding of the whole scene from distilling Diffusion\nfeatures, which is more important for policy learning.\nPSNR=15.01\nPSNR=13.88\nPSNR=14.67\nPSNR=21.51\nPSNR=21.28\nPSNR=19.56\nGround Truth RGB \nWith Action Loss\nWithout Action Loss\nRGB\nFeature\nRGB\nFeature\nLeft\nFront\nRight\nFigure 5: View synthesis of GNFactor in the real world. PSNR is computed for quantitative evaluation.\nThe visualization with the action loss is relatively blurred compared to that without the action loss. The noisy\nrendering is mainly because, in inference, we do not optimize per-step for rendering but just perform one\nfeedforward to obtain the feature.\n8\nFeature\nRGB\nFigure 6: More novel view synthesis results. Both RGB and features are synthesized. We remove the action\nloss here for a better rendering quality. Videos are available on yanjieze.com/GNFactor.\nInput RGB-D Image\nInput RGB-D Image\n3D Grad-CAM\n3D Grad-CAM\nFigure 7: Visualize the 3D policy module by Grad-CAM [55]. Though the supervision signal is only the\nQ-value for a single voxel during the training process, we observe in visualizations that the target objects are\nclearly attended by our policy. Videos are available on yanjieze.com/GNFactor.\n4.3\nReal Robot Experiments\nWe summarize the results of our real robot experiment in Table 3. From the experiments, GNFactor\noutperforms the PerAct baseline on almost all tasks. Notably, in the teapot task where the agent\nis required to accurately determine the grasp location and handle the teapot from a correct angle,\nPerAct fails to accomplish the task and obtains a zero success rate across two kitchens. We observed\nthat it is indeed challenging to learn a delicate policy from only 5 demonstrations. However, by\nincorporating the representation from the embedding of a vision-language model, GNFactor gains\nan understanding of objects. As such, GNFactor does not simply overfit to the given demonstrations.\nThe second kitchen (Figure 1) presents more challenges due to its smaller size compared to the\nfirst kitchen. This requires higher accuracy to manipulate the objects effectively. The performance\ngap between GNFactor and the baseline PerAct becomes more significant in the second kitchen.\nImportantly, our method does not suffer the same performance drop transitioning from the first\nkitchen to the second, unlike the baseline.\nWe also visualize our 3D policy module by Grad-CAM [55], as shown in Figure 7. We use the\ngradients and the 3D feature map from the 3D convolution layer after the Perceiver Transformer to\ncompute Grad-CAM. We observe that the target objects are clearly attended by our policy, though\nthe training signal is only the Q-value for a single voxel.\n5\nConclusion and Limitations\nIn this work, we propose GNFactor, a visual behavior cloning agent for real-world multi-task robotic\nmanipulation. GNFactor utilizes a Generalizable Neural Feature Field (GNF) to learn a 3D volu-\nmetric representation, which is also used by the action prediction module. We employ the vision-\n9\nlanguage feature from the foundation model Stable Diffusion besides the RGB feature to supervise\nthe GNF training and observe that the volumetric representation enhanced by the GNF is helpful for\ndecision-making. GNFactor achieves strong results in both simulation and the real world, across 10\nRLBench tasks and 3 real robot tasks, showcasing the potential of GNFactor in real-world scenarios.\nOne major limitation of GNFactor is the requirement of multiple views for the GNF training, which\ncan be challenging to scale up in the real world. Currently, we use three fixed cameras for GNFactor,\nbut it would be interesting to explore using a cell phone to randomly collect camera views, where\nthe estimation of the camera poses would be a challenge.\nReferences\n[1] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Haus-\nman, A. Herzog, J. Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv,\n2022.\n[2] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert, C. Lynch, S. Levine, and C. Finn. Bc-z:\nZero-shot task generalization with robotic imitation learning. In CoRL, 2022.\n[3] M. Shridhar, L. Manuelli, and D. Fox. Perceiver-actor: A multi-task transformer for robotic\nmanipulation. In CoRL, 2023.\n[4] M. Dalal, A. Mandlekar, C. Garrett, A. Handa, R. Salakhutdinov, and D. Fox. Imitating task\nand motion planning with visuomotor transformers. arXiv, 2023.\n[5] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022.\n[6] S. Parisi, A. Rajeswaran, S. Purushwalkam, and A. Gupta. The unsurprising effectiveness of\npre-trained vision models for control. In ICML, 2022.\n[7] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: A universal visual represen-\ntation for robot manipulation. arXiv, 2022.\n[8] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell.\nReal-world robot\nlearning with masked visual pre-training. In CoRL, 2023.\n[9] M. Laskin, A. Srinivas, and P. Abbeel. Curl: Contrastive unsupervised representations for\nreinforcement learning. In ICML, 2020.\n[10] N. Hansen and X. Wang. Generalization in reinforcement learning by soft data augmentation.\nIn ICRA, 2021.\n[11] Y. Ze, N. Hansen, Y. Chen, M. Jain, and X. Wang. Visual reinforcement learning with self-\nsupervised 3d representations. RA-L, 2023.\n[12] D. Driess, I. Schubert, P. Florence, Y. Li, and M. Toussaint. Reinforcement learning with\nneural radiance fields. NeurIPS, 2022.\n[13] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira. Perceiver: General\nperception with iterative attention. In ICML, 2021.\n[14] S. James, Z. Ma, D. R. Arrojo, and A. J. Davison. Rlbench: The robot learning benchmark &\nlearning environment. RA-L, 2020.\n[15] R. Rahmatizadeh, P. Abolghasemi, L. B\u00a8ol\u00a8oni, and S. Levine. Vision-based multi-task manip-\nulation for inexpensive robots using end-to-end learning from demonstration. In 2018 IEEE\ninternational conference on robotics and automation (ICRA), pages 3758\u20133765. IEEE, 2018.\n10\n[16] M. Shridhar, L. Manuelli, and D. Fox. Cliport: What and where pathways for robotic manipu-\nlation. In CoRL, 2022.\n[17] D. Kalashnikov, A. Irpan, P. Pastor, J. Ibarz, A. Herzog, E. Jang, D. Quillen, E. Holly,\nM. Kalakrishnan, V. Vanhoucke, et al. Qt-opt: Scalable deep reinforcement learning for vision-\nbased robotic manipulation. arXiv, 2018.\n[18] T. Yu, D. Quillen, Z. He, R. Julian, K. Hausman, C. Finn, and S. Levine. Meta-world: A\nbenchmark and evaluation for multi-task and meta reinforcement learning. In Conference on\nrobot learning, pages 1094\u20131100. PMLR, 2020.\n[19] R. Yang, H. Xu, Y. Wu, and X. Wang. Multi-task reinforcement learning with soft modular-\nization. Advances in Neural Information Processing Systems, 33:4767\u20134777, 2020.\n[20] S. Song, A. Zeng, J. Lee, and T. Funkhouser. Grasping in the wild: Learning 6dof closed-\nloop grasping from low-cost demonstrations. IEEE Robotics and Automation Letters, 5(3):\n4978\u20134985, 2020.\n[21] A. Murali, A. Mousavian, C. Eppner, C. Paxton, and D. Fox. 6-dof grasping for target-driven\nobject manipulation in clutter. In 2020 IEEE International Conference on Robotics and Au-\ntomation (ICRA), pages 6232\u20136238. IEEE, 2020.\n[22] A. Mousavian, C. Eppner, and D. Fox. 6-dof graspnet: Variational grasp generation for object\nmanipulation. In ICCV, 2019.\n[23] Z. Xu, Z. He, and S. Song. Universal manipulation policy network for articulated objects.\nRA-L, 2022.\n[24] Y. Li, S. Agrawal, J.-S. Liu, S. K. Feiner, and S. Song. Scene editing as teleoperation: A case\nstudy in 6dof kit assembly. In IROS, 2022.\n[25] N. Hansen, Z. Yuan, Y. Ze, T. Mu, A. Rajeswaran, H. Su, H. Xu, and X. Wang. On pre-training\nfor visuo-motor control: Revisiting a learning-from-scratch baseline. In ICML, 2022.\n[26] D. Shim, S. Lee, and H. J. Kim. Snerl: Semantic-aware neural radiance fields for reinforcement\nlearning. ICML, 2023.\n[27] Y. Chen, S. Liu, and X. Wang. Learning continuous image representation with local implicit\nimage function. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8628\u20138638, 2021.\n[28] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. Geiger. Occupancy networks:\nLearning 3d reconstruction in function space. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 4460\u20134470, 2019.\n[29] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthesis. Communications of the ACM,\n65(1):99\u2013106, 2021.\n[30] M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger. Occupancy flow: 4d reconstruction\nby learning particle dynamics. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 5379\u20135389, 2019.\n[31] J. J. Park, P. Florence, J. Straub, R. Newcombe, and S. Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pages 165\u2013174, 2019.\n[32] V. Sitzmann, M. Zollh\u00a8ofer, and G. Wetzstein. Scene representation networks: Continuous\n3d-structure-aware neural scene representations. Advances in Neural Information Processing\nSystems, 32, 2019.\n11\n[33] Z. Jiang, Y. Zhu, M. Svetlik, K. Fang, and Y. Zhu. Synergies between affordance and geometry:\n6-dof grasp detection via implicit representations. arXiv preprint arXiv:2104.01542, 2021.\n[34] Y.-C. Lin, P. Florence, A. Zeng, J. T. Barron, Y. Du, W.-C. Ma, A. Simeonov, A. R. Garcia,\nand P. Isola. Mira: Mental imagery for robotic affordances. In Conference on Robot Learning,\npages 1916\u20131927. PMLR, 2023.\n[35] Y. Li, S. Li, V. Sitzmann, P. Agrawal, and A. Torralba. 3d neural scene representations for\nvisuomotor control. In Conference on Robot Learning, pages 112\u2013123. PMLR, 2022.\n[36] Y. Chen and X. Wang. Transformers as meta-learners for implicit neural representations. In\nComputer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327,\n2022, Proceedings, Part XVII, pages 170\u2013187. Springer, 2022.\n[37] W. Jang and L. Agapito. Codenerf: Disentangled neural radiance fields for object categories.\nIn Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12949\u2013\n12958, 2021.\n[38] K.-E. Lin, Y.-C. Lin, W.-S. Lai, T.-Y. Lin, Y.-C. Shih, and R. Ramamoorthi. Vision transformer\nfor nerf-based view synthesis from a single input image. In Proceedings of the IEEE/CVF\nWinter Conference on Applications of Computer Vision, pages 806\u2013815, 2023.\n[39] J. Reizenstein, R. Shapovalov, P. Henzler, L. Sbordone, P. Labatut, and D. Novotny. Common\nobjects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 10901\u2013\n10911, 2021.\n[40] K. Rematas, R. Martin-Brualla, and V. Ferrari. Sharf: Shape-conditioned radiance fields from\na single view. arXiv preprint arXiv:2102.08860, 2021.\n[41] A. Trevithick and B. Yang. Grf: Learning a general radiance field for 3d representation and\nrendering. In Proceedings of the IEEE/CVF International Conference on Computer Vision,\npages 15182\u201315192, 2021.\n[42] Q. Wang, Z. Wang, K. Genova, P. P. Srinivasan, H. Zhou, J. T. Barron, R. Martin-Brualla,\nN. Snavely, and T. Funkhouser. Ibrnet: Learning multi-view image-based rendering. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n4690\u20134699, 2021.\n[43] A. Yu, V. Ye, M. Tancik, and A. Kanazawa. pixelnerf: Neural radiance fields from one or\nfew images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4578\u20134587, 2021.\n[44] V. Tschernezki, I. Laina, D. Larlus, and A. Vedaldi. Neural feature fusion fields: 3d distillation\nof self-supervised 2d image representations. arXiv, 2022.\n[45] S. Kobayashi, E. Matsumoto, and V. Sitzmann. Decomposing nerf for editing via feature field\ndistillation. NeurIPS, 2022.\n[46] J. Ye, N. Wang, and X. Wang. Featurenerf: Learning generalizable nerfs by distilling founda-\ntion models. arXiv, 2023.\n[47] J. Kerr, C. M. Kim, K. Goldberg, A. Kanazawa, and M. Tancik. Lerf: Language embedded\nradiance fields. arXiv preprint arXiv:2303.09553, 2023.\n[48] K. M. Jatavallabhula, A. Kuwajerwala, Q. Gu, M. Omama, T. Chen, S. Li, G. Iyer, S. Saryazdi,\nN. Keetha, A. Tewari, et al. Conceptfusion: Open-set multimodal 3d mapping. arXiv preprint\narXiv:2302.07241, 2023.\n12\n[49] K. Blomqvist, F. Milano, J. J. Chung, L. Ott, and R. Siegwart. Neural implicit vision-language\nfeature fields. arXiv preprint arXiv:2303.10962, 2023.\n[50] M. Caron, H. Touvron, I. Misra, H. J\u00b4egou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging\nproperties in self-supervised vision transformers. In ICCV, 2021.\n[51] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,\nP. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervi-\nsion. In ICML, 2021.\n[52] S. James, K. Wada, T. Laidlow, and A. J. Davison. Coarse-to-fine q-attention: Efficient learning\nfor visual robotic manipulation via discretisation. In CVPR, 2022.\n[53] S. Klemm, J. Oberl\u00a8ander, A. Hermann, A. Roennau, T. Schamm, J. M. Zollner, and R. Dill-\nmann. Rrt*-connect: Faster, asymptotically optimal motion planning. In 2015 IEEE interna-\ntional conference on robotics and biomimetics (ROBIO), pages 1670\u20131677. IEEE, 2015.\n[54] H. Lin, S. Peng, Z. Xu, Y. Yan, Q. Shuai, H. Bao, and X. Zhou. Efficient neural radiance fields\nfor interactive free-viewpoint video. In SIGGRAPH Asia, 2022.\n[55] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra. Grad-cam:\nVisual explanations from deep networks via gradient-based localization. In Proceedings of the\nIEEE international conference on computer vision, pages 618\u2013626, 2017.\n[56] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song, J. Demmel, K. Keutzer,\nand C.-J. Hsieh. Large batch optimization for deep learning: Training bert in 76 minutes.\narXiv, 2019.\n13\nA\nTask Descriptions\nSimulated tasks. We select 10 language-conditioned tasks from RLBench [14], all of which involve\nat least 2 variations. See Table 5 for an overview. Our task variations include randomly sampled\ncolors, sizes, counts, placements, and categories of objects, totaling 166 different variations. The set\nof colors have 20 instances: red, maroon, lime, green, blue, navy, yellow, cyan, magenta, silver, gray,\norange, olive, purple, teal, azure, violet, rose, black, and white. The set of sizes includes 2 types:\nshort and tall. The set of counts has 3 instances: 1, 2, 3. The placements and object categories\nare specific to each task. For example, open drawer has 3 placement locations: top, middle and\nbottom. In addition to these semantic variations, objects are placed on the tabletop at random poses\nwithin a limited range.\nTable 5: Language-conditioned tasks in RLBench [14].\nTask\nVariation Type\n# of Variations\nAvg. Keyframs\nLanguage Template\nclose jar\ncolor\n20\n6.0\n\u201cclose the \u2014 jar\u201d\nopen drawer\nplacement\n3\n3.0\n\u201copen the \u2014 drawer\nsweep to dustpan\nsize\n2\n4.6\n\u201csweep dirt to the \u2014 dustpan\u201d\nmeat off grill\ncategory\n2\n5.0\n\u201ctake the \u2014 off the grill\u201d\nturn tap\nplacement\n2\n2.0\n\u201cturn \u2014 tap\u201d\nslide block\ncolor\n4\n4.7\n\u201cslide the block to \u2014 target\u201d\nput in drawer\nplacement\n3\n12.0\n\u201cput the item in the \u2014 drawer\u201d\ndrag stick\ncolor\n20\n6.0\n\u201cuse the stick to drag the cube onto the \u2014 \u2014 target\u201d\npush buttons\ncolor\n50\n3.8\n\u201cpush the \u2014 button, [then the \u2014 button]\u201d\nstack blocks\ncolor, count\n60\n14.6\n\u201cstack \u2014 \u2014 blocks\u201d\nGeneralization tasks in simulation. We design 6 additional tasks where the scene is changed based\non the original training environment, to test the generalization ability of GNFactor. Table 6 gives an\noverview of these tasks. Videos are also available on yanjieze.com/GNFactor.\nTable 6: Generalization tasks based on RLBench.\nTask\nBase\nChange\ndrag (D)\ndrag stick\nadd two colorful buttons on the table\nslide (L)\nslide block\nchange the block size to a larger one\nslide (S)\nslide block\nchange the block size to a smaller one\nopen (n)\nopen drawer\nchange the position of the drawer\nturn (N)\nturn tap\nchange the position of the tap\npush (D)\npush buttons\nadd two colorful jar on the table\nReal robot tasks. In the experiments, we perform three tasks along with three additional tasks where\ndistracting objects are present. The door task requires the agent to open the door on an mircowave,\na task which poses challenges due to the precise coordination required. The faucet task requires\nthe agent to rotate the faucet back to center position, which involves intricate motor control. Lastly,\nthe teapot task requires the agent to locate the randomly placed teapot in the kitchen and move it\non top of the stove with the correct pose. Among the three, the teapot task is considered the most\nchallenging due to the random placement and the need for accurate location and rotation of the\ngripper. All 6 tasks are set up in two different kitchens, as visualized in Figure 8. The keyframes\nused in real robot tasks are given in Figure 9.\nB\nImplementation Details\nVoxel encoder. We use a lightweight 3D UNet (only 0.3M parameters) to encode the input voxel\n1003 \u00d7 10 (RGB features, coordinates, indices, and occupancy) into our deep 3D volumetric rep-\nresentation of size 1003 \u00d7 128. Due to the cluttered output from directly printing the network, we\n14\n(a) Kitchen 1.\n(b) Kitchen 2.\nFigure 8: Kitchens. We give a closer view of our two kitchens for real robot experiments. The\nfigures are captured in almost the same position to display the size difference between the two.\ntime\nTurn Faucet\nOpen Oven\nRelocate Teapot\nFigure 9: Keyframes for real robot tasks. We give the keyframes used in our 3 real robot tasks\nacross 2 kitchens.\n15\nprovide the PyTorch-Style pseudo-code for the forward process as follows. For each block, we use\na cascading of one Convolutional Layer, one BatchNorm Layer, and one LeakyReLU layer, which\nis common practice in the vision community.\ndef forward(self, x):\nconv0 = self.conv0(x) # 100^3x8\nconv2 = self.conv2(self.conv1(conv0)) # 50^3x16\nconv4 = self.conv4(self.conv3(conv2)) # 25^3x32\nx = self.conv6(self.conv5(conv4)) # 13^3x64\nx = conv4 + self.conv7(x) # 25^3x32\nx = conv2 + self.conv9(x) # 50^3x16\nx = self.conv_out(conv0 + self.conv11(x)) # 100^3x128\nreturn x\nGeneralizable Neural Field (GNF). The overall network architecture of our GNF is close to the\noriginal NeRF [29] implementation. We use the same positional encoding as NeRF and the encoding\nfunction is formally\n\u03b3(p) =\n\u0000sin\n\u000020\u03c0p\n\u0001\n, cos\n\u000020\u03c0p\n\u0001\n, \u00b7 \u00b7 \u00b7 , sin\n\u00002L\u22121\u03c0p\n\u0001\n, cos\n\u00002L\u22121\u03c0p\n\u0001\u0001\n.\n(5)\nThis function is applied to each of the three coordinate values and we set L = 6 in our experiments.\nThe overall position encoding is then 36-dimensional. The input of GNF is thus a concatenation of\nthe original coordinates (R3), the position encoding (R36), the view directions (R3), and the voxel\nfeature (R128), totaling 170 dimensions. Our GNF mainly consists of 5 ResnetFCBlocks, in which\na skip connection is used. The input feature is first projected to 512 with a linear layer and fed\ninto these blocks, and then projected to the output dimension 516 (RGB, density, and Diffusion\nfeature) with a cascading of one ReLU function and one linear layer. We provide the PyTorch-Style\npseudo-code for the networks as follows.\nGNF(\nLinear(in_features=170, out_features=512, bias=True),\n(0-4): 5 x ResnetFCBlocks(\n(fc_0): Linear(in_features=512, out_features=512, bias=True)\n(fc_1): Linear(in_features=512, out_features=512, bias=True)\n(activation): ReLU()\n),\nReLU(),\nLinear(in_features=512, out_features=516, bias=True)\n)\nPercevier Transformer. Our usage of Percevier Transformer is close to PerAct [3]. We use 6\nattention blocks to process the sequence from multi-modalities (3D volume, language token, and\nrobot proprioception) and output a sequence also. The usage of Perceiver Transformer enables\nus to process the long sequence with computational efficiency, by only utilizing a small set of\nlatents to attend the input. The output sequence is then reshaped back to a voxel to predict the\nrobot action. The Q-function for translation is predicted by a 3D convolutional layer, and for the\nprediction of openness, collision avoidance, and rotation, we use global max pooling and spatial\nsoftmax operation to aggregate 3D volume features and project the resulting feature to the output\ndimension with a multi-layer perception. We could clarify that the design for the policy module is\nnot our contribution; for more details please refer to PerAct [3] and its official implementation on\nhttps://github.com/peract/peract.\nC\nDemonstration Collection for Real Robot Tasks\nFor the collection of real robot demonstrations, we utilize the HTC VIVE controller and bases-\ntation to track the 6-DOF poses of human hand movements. We then use triad-openvr package\n(https://github.com/TriadSemi/triad_openvr) to employ SteamVR and accurately map\nhuman operations onto the xArm robot, enabling it to interact with objects in the real kitchen.\n16\nWe record the real-time pose of xArm and 640 \u00d7 480 RGB-D observations with the pyrealsense2\n(https://pypi.org/project/pyrealsense2/). Though the image size is different from our\nsimulation setup, we use the same shape of the input voxel, thus ensuring the same algorithm is\nused across the simulation and the real world. The downscaled images (80 \u00d7 60) are used for neural\nrendering.\nD\nDetailed Data\nBesides reporting the final success rates in our main paper, we give the success rates for the best sin-\ngle checkpoint (i.e., evaluating all saved checkpoints and selecting the one with the highest success\nrates), as shown in Table 7. Under this setting GNFactor outperforms PerAct with a larger margin.\nHowever, we do not use the best checkpoint in the main results for fairness.\nWe also give the detailed number of success in Table 8 for reference in addition to the success rates\ncomputed in Table 2.\nTable 7: Multi-task test results on RLBench. We report the success rates for the best single checkpoint for\nreference. We could observe GNFactor surpasses PerAct by a large margin.\nMethod / Task\nclose jar\nopen drawer\nsweep to dustpan\nmeat off grill\nturn tap\nAverage\nPerAct\n22.7\u00b15.0\n62.7\u00b113.2\n0.0\u00b10.0\n46.7\u00b114.7\n36.0\u00b19.8\nGNFactor\n40.0\u00b15.7\n77.3\u00b17.5\n40.0\u00b111.8\n66.7\u00b18.2\n45.3\u00b13.8\nMethod / Task\nslide block\nput in drawer\ndrag stick\npush buttons\nstack blocks\nPerAct\n22.7\u00b16.8\n9.3\u00b15.0\n12.0\u00b16.5\n18.7\u00b16.8\n5.3\u00b11.9\n23.6\nGNFactor\n18.7\u00b110.5\n10.7\u00b112.4\n73.3\u00b113.6\n20.0\u00b13.3\n8.0\u00b10.0\n40.0\nTable 8: Detailed data for generalization to novel tasks. We evaluate 20 episodes, each across 3\nseeds, for the final checkpoint and report the number of successful trajectories here.\nGeneralization\nPerAct\nGNFactor w/o. Diffusion\nGNFactor\ndrag (D)\n2, 0, 2\n15, 2, 5\n18, 5, 5\nslide (L)\n6, 6, 8\n1, 10, 10\n6, 5, 4\nslide (S)\n0, 2, 1\n6, 1, 5\n0, 3, 1\npush (D)\n6, 3, 3\n4, 4, 5\n7, 6, 6\nopen (N)\n6, 2, 7\n5, 2, 9\n8, 5, 6\nturn (N)\n4, 5, 2\n2, 7, 2\n6, 6, 5\nE\nStronger Baseline\nTo make the comparison between our GNFactor and PerAct fairer, we enhance Peract\u2019s input by\nusing 4 camera views, as visualized in Figure 10. These views ensure that the scene is fully covered.\nIt is observed in our experiment results (Table 1) that GNFactor which takes the single view as input\nstill outperforms PerAct with more views.\nF\nHyperparameters\nWe give the hyperparameters used in GNFactor as shown in Table 9. For the GNF training, we use\na ray batch size bray = 512, corresponding to 512 pixels to reconstruct, and use \u03bbfeat = 0.01 and\n\u03bbrecon = 0.01 to maintain major focus on the action prediction. For real-world experiment, we set\nthe weight of the reconstruction loss to 1.0 and the weight of action loss to 0.1. This choice was\nbased on our observation that reducing the weight of the action loss and increasing the weight of\nthe reconstruction loss did not significantly affect convergence but did help prevent overfitting to\n17\nFront\nWrist\nLeft\nRight\nFigure 10: Visualization of 4 cameras used for the stronger PerAct baseline. To enhance the\nPerAct baseline, we add more views as the input of PerAct. These views are pre-defined in RLBench,\nmaking sure the observation covers the entire scene.\na limited number of real-world demonstrations. We uniformly sample 64 points along the ray for\nthe \u201ccoarse\u201d network and sample 32 points with depth-guided sampling and 32 points with uniform\nsampling for the \u201cfine\u201d network.\nTable 9: Hyperparameters used in GNFactor.\nVariable Name\nValue\ntraining iteration\n100k\nimage size\n128 \u00d7 128 \u00d7 3\ninput voxel size\n100 \u00d7 100 \u00d7 100\nbatch size\n2\noptimizer\nLAMB [56]\nlearning rate\n0.0005\nray batch size bray\n512\nweight for reconstruction loss \u03bbrecon\n0.01\nweight for embedding loss \u03bbfeat\n0.01\nnumber of transformer blocks\n6\nnumber of sampled points for GNF\n64\nnumber of latents in Perceiver Transformer\n2048\ndimension of Stable Diffusion features\n512\ndimension of CLIP language features\n512\nhidden dimension of NeRF blocks\n512\n18\n"
  },
  {
    "title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants",
    "link": "https://arxiv.org/pdf/2308.16884.pdf",
    "upvote": "8",
    "text": "The BELEBELE Benchmark:\na Parallel Reading Comprehension Dataset in 122 Language Variants\nLucas Bandarkar*, Davis Liang*\u2020, Benjamin Muller*,\nMikel Artetxe*\u2021, Satya Narayan Shukla*, Donald Husa*, Naman Goyal*,\nAbhinandan Krishnan*, Luke Zettlemoyer*, Madian Khabsa*\nMeta AI*\nAbridge AI\u2020\nReka AI \u2021\nAbstract\nWe present BELEBELE, a multiple-choice ma-\nchine reading comprehension (MRC) dataset\nspanning 122 language variants. Significantly\nexpanding the language coverage of natural lan-\nguage understanding (NLU) benchmarks, this\ndataset enables the evaluation of text models in\nhigh-, medium-, and low-resource languages.\nEach question is based on a short passage from\nthe FLORES-200 dataset and has four multiple-\nchoice answers. The questions were carefully\ncurated to discriminate between models with\ndifferent levels of general language comprehen-\nsion. The English dataset on its own proves\ndifficult enough to challenge state-of-the-art\nlanguage models. Being fully parallel, this\ndataset enables direct comparison of model per-\nformance across all languages. We use this\ndataset to evaluate the capabilities of multi-\nlingual masked language models (MLMs) and\nlarge language models (LLMs). We present ex-\ntensive results and find that despite significant\ncross-lingual transfer in English-centric LLMs,\nmuch smaller MLMs pretrained on balanced\nmultilingual data still understand far more lan-\nguages. We also observe that larger vocabulary\nsize and conscious vocabulary construction cor-\nrelate with better performance on low-resource\nlanguages. Overall, BELEBELE opens up new\navenues for evaluating and analyzing the multi-\nlingual capabilities of NLP systems.\n1\nIntroduction\nThe absence of high-quality, parallel evaluation bench-\nmarks is a major obstacle in assessing the text com-\nprehension capabilities of multilingual models. NLP\ndatasets with high language coverage do exist, such\nas FLORES-200 (Team et al., 2022), but they primar-\nily focus on machine translation. Popular multilingual\nevaluation benchmarks, such as multilingual question\nanswering (Lewis et al., 2020; Clark et al., 2020), nat-\nural language inference (NLI) (Conneau et al., 2018),\nsummarization (Ladhak et al., 2020; Hasan et al., 2021),\nand reasoning datasets (Ponti et al., 2020; Lin et al.,\n2021) altogether only cover around 30 languages. And\nwhile understanding and generative text services are\nused across the globe in 100+ languages, the lack of\nlabeled data provides a major obstacle to building func-\ntional systems in most languages. To date, there exists\nno massively-multilingual evaluation dataset for natural\nlanguage understanding (NLU).\nSimultaneously, large language models (LLMs) have\nbecome increasingly popular.\nCertain LLMs, like\nBLOOM (Scao et al., 2022), are trained on multilin-\ngual data and tout their innate multilingual capabilities.\nOthers like GPT-3 (Brown et al., 2020) and LLAMA\n(Touvron et al., 2023a) have demonstrated multilingual\ncompetence despite their training data being predomi-\nnantly English-centric.\nSystems are often built first for high-resource lan-\nguages due to data availability, existing academic work,\norganizational priorities, and even typologically simi-\nlarity to English (Joshi et al., 2020; Team et al., 2022).\nEven so, LLMs benefit from pretraining data that is lin-\nguistically diverse, intentionally or not, as well as from\ncross-lingual transfer (Zoph et al., 2016; Artetxe et al.,\n2020; Muller et al., 2021b). But how multilingual are\nthese models really? Beyond LLMs, significant scien-\ntific progress needs to be made before NLP systems can\nbe built effectively and efficiently in low-resource lan-\nguages. Many modeling techniques are being presented\nas language-agnostic but have only truly been evaluated\nin a small number of languages (Bender, 2011), risking\nnot being applicable to diverse typologically phenomena\n(Bender, 2009). We believe that a large-scale, parallel,\nand discriminative NLU dataset is crucial for studying\nthe multilingual capabilities of such models and under-\nstanding how the technological disparity between high-\nand low-resource languages is evolving.\nIn this paper, we present a fundamental natural lan-\nguage understanding benchmark to evaluate language\nmodels across 122 language variants from around the\nworld, called BELEBELE1. The dataset contains 900\nunique multiple-choice reading comprehension ques-\ntions, each associated with one of 488 distinct passages.\nThe questions have been carefully crafted to discrimi-\nnate between models with different competence in lan-\nguage comprehension. While the questions do not nec-\nessarily require higher levels of knowledge or reasoning,\nthey favor generalizable NLU models and deliberately\npunish biased models. The English questions on their\n1Bambara word meaning \"big, large, fat, great\".\narXiv:2308.16884v1  [cs.CL]  31 Aug 2023\nBELEBELE Statistics\nLanguages\nPassage statistics\nQuestion statistics\nTotal Number\n122\nDistinct Passages\n488\nDistinct Questions\n900\nDistinct Languages (ignoring script)\n115\nQuestions per passage\n1-2\nMultiple-choice answers (num correct) per question\n4 (1)\nLanguage Families\n27\nAvg. words per passage (std)\n79.1 (26.2)\nAvg. words per question (std)\n12.9 (4.0)\nScripts\n29\nAvg. sentences per passage (std)\n4.1 (1.4)\nAvg. words per answer (std)\n4.2 (2.9)\nTable 1: Language and Text Information for BELEBELE. Average length statistics are computed on the English split.\nown present a significant challenge to numerous models,\nwhile humans are capable of answering the questions\nwith near-perfect accuracy. The wide-ranging model\nresults makes this a discriminative NLU task similar\nto popular LLM benchmarks like MMLU (Hendrycks\net al., 2021).\nThe first of its scale, BELEBELE is parallel across\nall languages, facilitating a direct comparison of model\nperformance across all languages. The dataset covers ty-\npologically diverse high-, moderate-, and low-resource\nlanguages across 29 scripts and 27 language families. In\naddition, seven languages are included in two separate\nscripts, resulting in one of the first NLP benchmarks for\nthe romanized variant of Hindi, Urdu, Bengali, Nepali,\nand Sinhala. We further detail our data collection pro-\ncess and the resulting corpus in Section 3.\nThe dataset enables evaluation of monolingual and\nmultilingual models, but the parallel nature also enables\nthe evaluation cross-lingual textual representations in a\nnumber of cross-lingual settings. The task can be eval-\nuated via full fine-tuning by assembling a training set\nfrom related QA datasets. We demonstrate this in Sec-\ntion 4 with several masked language models (MLMs)\non both cross-lingual transfer from English fine-tuning\nand translate-train-all. For LLMs, we evaluate sev-\neral models using five-shot in-context learning and also\ninstruction-tuned models via zero-shot (in-language and\ntranslate-test). We discuss our results in Section 5.\nThis paper\u2019s contributions are the following:\n\u2022 We release BELEBELE, the first parallel Reading\nComprehension evaluation benchmark for 122 lan-\nguages\n\u2022 We present baseline results on BELEBELE across\nthe 122 languages for MLMs and LLMs in numer-\nous settings.\n\u2022 Thanks to BELEBELE, we find that while large vo-\ncabulary size and balanced pretraining data corre-\nlates with highest model performance on medium-\nand low-resource languages, even English-centric\nLLMs can go a long way and generalize to over 30\nlanguages.\n2\nBackground\n2.1\nCross-Lingual Evaluation Benchmarks\nThere are several cross-lingual evaluation benchmarks\nfor NLU that are parallel across numerous languages\nand enable monolingual, multilingual, or cross-lingual\nevaluation, such as XNLI (Conneau et al., 2018),\nXQUAD (Artetxe et al., 2020), or MLQA (Lewis et al.,\n2020). MINTAKA (Sen et al., 2022) is designed with\nLLMs in mind, presenting a more difficult QA task in 9\nlanguages. Beyond QA, XL-SUM (Hasan et al., 2021)\nis an analogous dataset in the domain of abstractive sum-\nmarization. However, all these datasets together cover\nunder 30 languages, most of which are high- or medium-\nresource. MASSIVE (FitzGerald et al., 2023) is a large\nNLU dataset covering 51 languages, but in the domain\nof spoken conversational agents. Our work undertakes\nthe challenge of expanding existing cross-lingual eval-\nuations to 122 languages, for many of which no NLU\nbenchmark currently exists.\nCCQA Huber et al. (2022) is the most multilingual\nQA dataset available, generating questions from Com-\nmon Crawl at scale. However, this dataset is intended to\nbe used for in-domain pretraining and is not a parallel\nevaluation benchmark. Similarly, NER (Pan et al., 2017)\nhas broad language coverage and TYDIQA (Clark et al.,\n2020) is a commonly used evaluation benchmark but\nneither are parallel.\n2.2\nNon-English Machine Reading\nComprehension\nWhile the question-answering portion varies, machine\nreading comprehension (MRC) tasks are defined by the\nclosed-book passage provided to answer each question.\nNaturally, a big majority of MRC datasets are in English,\nsuch as TRIVIAQA (Joshi et al., 2017) and the collection\nof BABI tasks (Weston et al., 2016).\nHowever, there has been a rise in non-English\nMRC datasets in recent years. Monolingual closed-\nbook MRC datasets now exist in Arabic (Mozannar\net al., 2019), Bulgarian (Hardalov et al., 2019), French\n(d\u2019Hoffschmidt et al., 2020), German (M\u00f6ller et al.,\n2021), Hindi (Anuranjana et al., 2019; Gupta et al.,\n2018), Italian (Croce et al., 2018), Russian (Efimov\net al., 2020; Shavrina et al., 2020), and Tibetan (Sun\net al., 2021), amongst others. Many were created us-\ning translation and so are parallel with an English QA\ndataset, often SQUAD (Rajpurkar et al., 2016). How-\never, BELEBELE covers all these languages at once and\nmany more.\n2.3\nMultiple Choice QA\nCompared to extractive QA, multiple-choice question-\nanswering is a less common form of MRC. The most\nsimilar English datasets to BELEBELE are those with\nmultiple-choice question based on a provided paragraph\nin a particular domain. RACE (Lai et al., 2017) and\nFigure 1: A sample passage from the dataset in 4 different languages, displayed alongside its two questions.\nCLEF Entrance Exams (Pe\u00f1as et al., 2014) are large\nfitting datasets made from exam questions for English\nlearners. MCTEST (Richardson et al., 2013) was built\nspecifically for ML systems, but is intended to simi-\nlarly include diverse reasoning phenomena. MULTIRC\n(Khashabi et al., 2018) emphasizes multi-sentence rea-\nsoning for its questions, and provides a multiple-choice\ndataset with any number of correct answers. Agar-\nwal and Mannem (2011) provides a system to gener-\nate fill-in-the-blank multiple-choice questions for a text\ncorpus. MOVIEQA (Tapaswi et al., 2015) and MC-\nSCRIPT2.0 (Ostermann et al., 2019) contain theater or\nmovie scripts. SCIQ (Welbl et al., 2017) and OPEN-\nBOOKQA (Mihaylov et al., 2018) are open-book multi-\nple choice datasets, that also have associations to pas-\nsages. In comparison to more straightforward MRC\ntasks, COPA (Roemmele et al., 2011), SWAG (Zellers\net al., 2018), and RECLOR (Yu et al., 2020) consist\nof multiple-choice questions that require higher-level\ncommonsense reasoning to answer.\nEXAMS (Hardalov et al., 2020) is a parallel multiple-\nchoice QA dataset in 28 languages. However, it differs\nfrom our dataset in that passages are not provided and\nanswering questions requires multilingual knowledge\ntransfer and reasoning.\n2.4\nFLORES-200\nThe FLORES-200 Machine Translation Benchmark\n(Goyal et al., 2022; Team et al., 2022) is a dataset paral-\nlel across 200 languages. The dataset was constructed\nby sourcing English passages from Wikinews, Wikiju-\nnior, and WikiVoyage. The translations were performed\nby native speakers with high English fluency and transla-\ntion experience. Translators were instructed to maintain\ninformative and standardized content while handling\nnamed entities, abbreviations, idiomatic expressions,\nand pronouns appropriately. The passages in the BELE-\nBELE corpus are directly sourced from FLORES.\n3\nThe BELEBELE Dataset\nWe opted to create multiple-choice questions and an-\nswers in English and then translate, as opposed to cre-\nating resources natively in each language. Many of\nthe advantages to this approach outlined in Conneau\net al. (2018) remain. Most importantly, this leads to\nsignificantly more similar sets of samples across lan-\nguages, enabling direct score comparison. The process\nfor creating the dataset is summarized in Figure 2.\n3.1\nCreation of Multiple Choice Questions &\nAnswers\nTo create the BELEBELE dataset, we first construct a\nquestion-answering dataset in English.\nAmongst machine reading comprehension tasks, we\nselect multiple-choice questions (MCQs) because it\nwould lead to the fairest evaluation across languages. In\ncomparison to span extraction, which is more sensitive\nto morphological differences, MCQs enable the ability\nto scale to many languages when translating from En-\nglish. In addition, MCQs enable us to better center the\nquestions on information explicitly stated in the passage,\nas yes/no or entailment (NLI) questions can be easier\nto answer with external knowledge held in pretrained\nmodels. In order for the questions to discriminate solely\nFigure 2: Flowchart illustrating the dataset creation process starting from the FLORES-200 passages via Language\nService Provider (LSP) annotation\nbetween different levels of language understanding, we\nintentionally create questions that do not require higher\nlevels of information processing, such as multi-hop or\ncommonsense reasoning.\nConstructing high quality MCQs depends most im-\nportantly on creating strong negatives that are neither\nobviously wrong nor possibly correct (Agarwal and\nMannem, 2011; Richardson et al., 2013). We do not\nwant the dataset to be easy enough for biased mod-\nels (e.g. models that use shortcuts or pattern-match)\n(Boyd-Graber and B\u00f6rschinger, 2020). In setting up\nthis annotation, we consider the protocols proposed in\nBowman et al. (2020) and the warnings of annotation\nshortcomings mentioned in Malaviya et al. (2022). In\na process similar to what Nangia et al. (2021) advo-\ncates for, we implement an iterative procedure with the\nLanguage Service Provider (LSP) for this involved data\ncollection task. We engaged in 5 total iterations, pro-\nviding and receiving feedback each time. Annotators\nwere instructed on the similarities and differences on\nhow ML models approach QA datasets versus humans,\nwhich we felt substantially improved the quality of the\ndata.\nOur final guidelines include both important points\nsuch as having the correct response being unambiguous,\nas well as particularized rules such as no double nega-\ntives (Mihaylov et al., 2018). For each rule we provided\nannotators with a good and bad example to illustrate.\nAn abridged version of our guidelines can be found in\nthe Appendix A.2.1.\n3.2\nQuality Assurance\nAt each iteration, we evaluate whether or not returned\nsamples satisfy the minimum quality bar through a mix\nof manual inspection and automatic inspection. At ev-\nery step, we manually verified a sample of questions to\nunderstand how well the annotators were on the same\npage with us about the guidelines. While time consum-\ning, manual verification was the most assured way to\nprovide proper feedback to the annotators, notably on\nthe difficulty of the questions created.\nTo complement the manual inspection of a subset\nof questions, we use programmatic methods to evalu-\nate all questions. Based on the findings in Malaviya\net al. (2022), we create low-level features to identify\noverly easy questions or low-effort strategies employed\nby annotators. For example, we evaluate the lexical\noverlap (n-gram precision) between different combina-\ntions of the texts associated with a question to evaluate\nwhether the question is answerable by a biased model.\nThis allows us to see if the question can be answered\nwithout the passage, without the question, or with only\none sentence in the passage. We also identified patterns\nassociated with heuristic solvability, such as only one of\nthe answers being extracted directly from the passage,\nenabling easy answering due to lack of plausibility for\nwrong answers.\nThese low-level features allow us to (1) determine\nwhether an annotation iteration was up to par, (2) filter\nout questions that failed these heuristic checks (for the\nfinal iteration, about 20% were filtered out), and (3)\ncompare to other MCQ datasets. We run statistical\nt-tests to ensure the distribution of these features for\ncorrect answers is no different than for wrong answers.\nThe final collection has p-value 0.81, in comparison\nto MCTEST which largely fails this t-test (p-value <\n0.01). We also train a na\u00efve logistic regression model to\nanswer using only these low-level features and find that\nfor MCTEST, it can achieve accuracy up to 0.44. On our\n900 questions, the best the na\u00efve model could achieve\nwas 0.28, only slightly better than random (0.25).\n3.3\nTranslating the Corpus\nBELEBELE was created end-to-end without the use of\nmachine translation technology, relying solely on ex-\nperts fluent in English and the target language.\nFor all languages included in the corpus, the context\npassages were taken directly from the FLORES-200\ndataset, with the exception of Hindi, Bengali, Urdu,\nNepali, and Sinhala in the Latin script. For these 5 Indo-\nAryan languages, their romanization is not included in\nFLORES-200 while being very prevalent on the modern\nInternet. We thus additionally transliterate from the na-\ntive to Latin script using IndicXlit (Madhani et al., 2022)\nand have annotators proofread. As a result, much like\nModern Standard Arabic, these languages are present in\ntwo forms in the BELEBELE corpus.\nIn order for the questions and answers to properly pair\nthe translated FLORES passages, the latter was provided\nfor the annotators. We specifically instructed annotators\nto align potentially ambiguous translations with the orig-\ninal passages. While Clark et al. (2020) warns that this\nforced alignment could increase \u2018translationese\u2019, it is\nnecessary to ensure equivalent question difficulty across\nlanguages. The modifications to the translation guide-\nlines can be found in Appendix A.2.2. All translations\nwere proofread and edited by an additional annotator.\nAnnotators raised several quality issues or inconsisten-\ncies in the original translated passages. For each, we\ndeliberated with the LSP to establish acceptable transla-\ntions of the question, while still maximizing alignment\nwith the passage.\nFollowing the translations of the questions and an-\nswers to all languages, we form the final BELEBELE\ncorpus by combining the passages, questions, and an-\nswers in each language (including English).\n3.4\nThe BELEBELE Dataset in Summary\nBELEBELE contains 900 questions, each with exactly 4\nmultiple-choice answers and one correct answer. Most\npassages have two associated questions, but some have\nonly one. In total, there are 488 distinct passages, none\nbelonging to the hidden FLORES test set. Parallel across\n122 languages, the corpus therefore contains a total of\n109,800 rows. We display a sample passage in four\nlanguages in Fig. 1. We also provide a training and\ndevelopment set (see Section 4.2).\nBecause of the careful annotation procedure and qual-\nity checks, the MCQs discriminate text comprehension\ncompetence. It often includes paraphrasing and strong\nnegatives in order to allude pattern-matching models\nseeking giveaways. Questions often additionally require\nunderstanding multiple sentences. However, answering\ndoes not require presumptions or external knowledge\nas is required in more difficult reasoning datasets. For\nexample, Q1 in Fig. 1 is unambiguous. Food, mates,\nand flying are all mentioned in the passage, but a careful\nread reveals the wings folding back is only associated\nwith hiding spaces. To confidently rule out the other\ncandidate answers, it is required to understand three\nsentences. In general, we find all questions to be an-\nswerable by humans fluent in the target language, but\nnot without focused reading (see Section 5.1).\nAs can be seen in Fig. 1, the passages, questions, and\nanswers are aligned in semantic meaning and formal-\nity. This therefore poses an equivalent challenge in all\nlanguages. It also enables models with semantic repre-\nsentations aligned across languages to answer questions\nwhen passage, question, and answer are presented in\ndifferent languages. Since FLORES includes passages\nin 83 additional languages, we can even evaluate read-\ning comprehension in these languages by asking the\nquestions in English.\n4\nExperiments\nThanks to BELEBELE, we are able to evaluate numerous\nmodels and establish baseline performances across 122\nlanguage variants. We compare performance between\npopular multilingual MLMs and LLMs in several set-\ntings such as fine-tuning, few-shot in-context learning\nand zero-shot prompting. For all, accuracy is the central\nmetric. With 4 candidate answers for each question, the\nexpected accuracy for sequence classification models\nthat guess randomly is 0.25. However, the accuracy can\nbe lower than this for sequence-to-sequence models (e.g.\ninstructed models) that are evaluated in exact-match sce-\nnarios.\n4.1\nEvaluated Models\nMasked Language Models (MLMs)\nWe evaluate\nthree different models, XLM-V (Liang et al., 2023), IN-\nFOXLM (Chi et al., 2021), and XLM-R (Conneau et al.,\n2020a). All the evaluated MLMs have been pretrained\non data specifically designed to include multilingual text\nin about 100 languages. The pretraining data in high-\nresource languages is typically down-sampled while\nlow-resource languages are up-sampled in order to favor\nmultilingual performance (Conneau et al., 2020a). In\naddition, the subword tokenizers (Kudo and Richardson,\n2018) of all these models are trained on the multilingual\ncorpora making them better suited for multilingual text.\nLarge Language Models\nWe evaluate GPT3.5-\nTURBO, FALCON and LLAMA (1 and 2). GPT3.5-\nTURBO is a model optimized for chat based on GPT-3\n(Brown et al., 2020) available through OpenAI APIs2.\nLimited details have been disclosed about the pretrain-\ning and fine-tuning data and its language distribution.\nLLAMA 1 (Touvron et al., 2023a) is a collection of\ndecoder-only transformers model trained on 1.4T to-\nkens of Web-Crawled data.3 LLAMA 2 (Touvron et al.,\n2023b) improved upon LLAMA 1 models by pretraining\non about 2T tokens of web-crawled data. We evaluate\nthe 7B, 13B, 30B, and 65B pretrained checkpoints for\nLLAMA 1. We evaluate LLAMA 2 70B in both its pre-\ntrained version and its chat version that was instruction-\nfine-tuned (Ouyang et al., 2022) for safe dialog purposes\n(a.k.a. LLAMA-2-CHAT). FALCON is pretrained on\none trillion extensively filtered web-crawled samples\n(Penedo et al., 2023). We evaluate the 40B variant.\nLLAMA 1 was reportedly trained in English data and\n19 languages written in the Latin and Cyrillic scripts.\nNon-English text accounts for less than 4.5% of the\npretraining corpus (Touvron et al., 2023a). LLAMA\n2 pretraining data is made of 89.7% of English data\n(Touvron et al., 2023b). The rest corresponds to text\n2https://platform.openai.com/docs/models/\noverview\n3The 7B and 13B checkpoints have been pretrained for\nonly 1T tokens.\nFigure 3: BELEBELE Results in 122 languages. We compare four models in two settings and see the difference\nbetween intentionally multilingual models and models with English-centric data. GPT3.5-TURBO performs the\nbest on the top 20 languages, but after 40-50, its performance falls far behind INFOXLM and XLM-V. Similarly,\nINFOXLM outperforms XLM-V in the first 40 languages, but XLM-V proves more capable on the long tail of\nlanguages. Note that the language order can change the plot considerably, here we choose median accuracy.\nidentified as belonging to 26 languages (e.g. German,\nVietnamese, Indonesian, etc.) and 8.4% of unidentified\ndata.4 Both series use the same BPE-based tokenizers\n(Kudo and Richardson, 2018). We note that unknown\nUnicode characters are split into bytes so LLAMA mod-\nels avoid out-of-vocabulary errors.\nWe also experimented with other multilingual\ndecoder-only language models such as XGLM (Lin\net al., 2022) and BLOOM (7B) (Scao et al., 2022).\nStill, these models do not perform better than chance on\nBELEBELE, so we do not report their performance.\n4.2\nFine-tuning, Few-shot and Zero-shot\nEvaluation\nEnglish Training data\nThe BELEBELE dataset is in-\ntended to be used only as a test set, and not for train-\ning. Therefore, for models that require additional task\nfinetuning, we instead propose using an assembled\ntraining set consisting of samples from pre-existing\nmultiple-choice QA datasets in English. We consid-\nered diverse datasets, and determine the most compati-\nble to be RACE (Lai et al., 2017), SCIQ (Welbl et al.,\n2017), MULTIRC (Khashabi et al., 2018), MCTEST\n(Richardson et al., 2013), MCSCRIPT2.0 (Ostermann\net al., 2019), and RECLOR (Yu et al., 2020). For each\nof the 6 datasets, we unpack and restructure the pas-\nsages and questions from their respective formats. We\nthen filter out less suitable samples (e.g. questions with\nmultiple correct answers) and experiment with different\n4See Table 10 in Touvron et al. (2023b) for a full list of\nthe identified languages\nstrata to train the best ROBERTA-base model (Liu et al.,\n2019) evaluated on the English set.\nIn the end, the dataset comprises 67.5k training sam-\nples and 3.7k development samples, more than half of\nwhich are from RACE. We provide a script5 to recon-\nstruct this dataset for anyone to perform task finetuning.\nFine-tuning\nin\nCross-Lingual\nTransfer\nand\nTranslate-Train Settings\nFor evaluating all three\nMLMs, we add a multiple-choice classification head\nand fine-tune the entire model. We finetune in two\nsettings, (1) in English and evaluate cross-lingual\ntransfer and (2) on machine-translated samples of the\ntraining set to all the target languages and evaluate\neach language (translate-train-all). We use machine\ntranslation on passages,\nquestions,\nand answers\nseparately.\nFor each training run, we only do one\nepoch and limit the training and validation sample to\n650k. For both settings, the development set is used for\nhyperparameter search and we evaluate the two best\ntraining runs on BELEBELE.\nFive-shots In-Context Learning\nWe evaluate the pre-\ntrained LLAMA 1 and 2 as well as FALCON 40B in the\nfive-shots setting. Examples are sampled from the En-\nglish training set and prompted to the model (following\nthe template P: <passage> \\n Q: <question> \\n\nA: <mc answer 1> \\n B: <mc answer 2> \\n C:\n<mc answer 3> \\n D: <mc answer 4> \\n Answer:\n<Correct answer letter>). We report the average\nscores over 3-runs. In this setting, we perform predic-\n5https://github.com/facebookresearch/belebele\nModel\nSize/Variant\nVocab size\nAVG\n% \u2265 50\n% \u2265 70\neng_Latn\nnon-Eng AVG\n5-Shot In-Context Learning (examples in English)\nLLAMA 1\n7B\n32K\n27.7\n0.0%\n0.0%\n37.3\n27.6\nLLAMA 1\n13B\n32K\n30.4\n0.8%\n0.0%\n53.3\n30.2\nLLAMA 1\n30B\n32K\n36.2\n18.0%\n0.8%\n73.1\n35.9\nLLAMA 1\n70B\n32K\n40.9\n25.4%\n12.3%\n82.5\n40.5\nLLAMA 2 base\n70B\n32K\n48.0\n38.5%\n26.2%\n90.9\n47.7\nFALCON\n40B\n65K\n37.3\n16.4%\n1.6%\n77.2\n36.9\nZero-Shot for Instructed Models (English instructions)\nLLAMA-2-CHAT\n70B\n32K\n41.5\n27.0%\n2.5%\n78.8\n41.2\nGPT3.5-TURBO\nunk\n100K\n51.1\n44.2%\n29.2%\n87.7\n50.7\nFull Finetuning in English\nXLM-R\nlarge (550M)\n250K\n54.0\n64.8%\n15.6%\n76.2\n53.8\nXLM-V\nlarge (1.2B)\n902K\n55.6\n69.7%\n21.2%\n76.2\n54.9\nINFOXLM\nlarge (550M)\n250K\n56.2\n67.2%\n28.7%\n79.3\n56.0\nTranslate-Train-All\nXLM-R\nlarge (550M)\n250K\n58.9\n69.7%\n36.1%\n78.7\n58.8\nXLM-V\nlarge (1.2B)\n902K\n60.2\n76.2%\n32.8%\n77.8\n60.1\nINFOXLM\nlarge (550M)\n250K\n60.0\n70.5%\n36.9%\n81.2\n59.8\nTable 2: Summary of results on BELEBELE across models and evaluation settings. % \u2265 50/70 refers to the proportion\nof languages for which a given model performs above 50/70%. We additionally report LLAMA-2-CHAT zero-shot\nresults leveraging translation in Table 3.\ntion by picking the answer within {A, B, C, D} that has\nthe highest probability relatively to the others.\nZero-shot Evaluation\nWe evaluate both GPT3.5 and\nLLAMA-2-CHAT in the zero-shot setting by describing\nthe task in natural language. We present the passage,\nquestion, and four possible answers, and instruct the\nmodel to provide the letter \u201cA\u201d, \u201cB\u201d, \u201cC\u201d or \u201cD \u201das\nthe answer. The instructions are given in English for\nall languages. We perform post-processing steps and\naccept answers predicted as e.g. \u201c(A)\u201d instead of \u201cA\u201d.6\nIn addition, we evaluate prompting LLAMA-2-CHAT\n(70B) with instructions that are machine translated to\nthe target language from English. Conversely, we eval-\nuate the model in the translate-test setting, where the\npassages, questions, and answers are machine translated\nto English and prompted to the model. This setting al-\nlows us to compare in-language comprehension to using\nmachine translation, as is common in many multilingual\nsystems.\n5\nResults\nWe provide a summary table of results in Table 2 and\nall results in Appendix A.3.\n5.1\nHow difficult is BELEBELE?\nAs discussed in Section 3, the questions in BELEBELE\nare intentionally difficult. While the primary challenge\nof this dataset is its multilinguality, we see that empiri-\ncally, the English questions are able to shed light on the\nvarying NLU capabilites of models. With full finetuning,\n6For LLAMA-2-CHAT we additionally remove the prefix\n\u201cThe correct answer is \u201d.\nwe achieved a maximum accuracy of 71.7 in English\nwith ROBERTA-base model, significantly less than the\n90.9 achieved by LLAMA 2 70B in five-shot. Between\nLLAMA 1 models of different size, we see a significant\nrange of results, as the 7B model only achieves a score\nof 37.3 (See Section 5.2).\nTo establish human performance, 4 authors each ran-\ndomly sampled around 30 English MCQs and answered\nwith focus in a blind test, achieving mean 97.6 accu-\nracy7. This is significantly higher than any of the mod-\nels evaluated, implying the task presents a particular\nchallenge for models and there is room to improve. In\ncomparison, Nangia and Bowman (2019) conservatively\nestimate human performance on XNLI to be 92.8 on the\nEnglish questions (i.e. MNLI (Williams et al., 2018)).\nWhen comparing model performance on BELEBELE\nwith XNLI, we find very high correlation.\nIn the\ntranslate-train-all setting, XLM-V, INFOXLM, and\nXLM-R all perform about 10 accuracy points lower\non BELEBELE than the translate-train8 accuracies on\nXNLI reported in their respective papers (Liang et al.,\n2023; Chi et al., 2021). But overall, across the 15 lan-\nguages and all three models, we find a correlation in\naccuracy of r = 0.85.\n5.2\nMultilingual Generalization of MLMs and\nLLMs on BELEBELE\nMultilingual generalization of MLMs and LLMs is com-\nplex to understand and anticipate. Schematically, the\nperformance of a language model in a given language\n795% CI for all 900 questions = [93.1, 99.5]\n8In traditional translate-train, the model is finetuned on\ntranslated training inputs for each language individually.\nModel\nVariant\nEval Setting\nAVG\n% \u2265 50\n% \u2265 70\neng_Latn\nTranslate-Test (English) on 91 non-English languages in Zero-Shot\nLLAMA-2-CHAT\n70B\nTranslate-Test\n57.1\n78.0%\n2.2%\n78.8\nLLAMA-2-CHAT\n70B\nIn-Language\n44.1\n35.2%\n2.2%\n78.8\nTranslated Instructions in 89 non-English languages Zero-Shot\nLLAMA-2-CHAT\n70B\nIn-Language Translated Instructions\n38.7\n36.0%\n7.9%\n78.8\nLLAMA-2-CHAT\n70B\nEnglish Instructions\n44.9\n37.1%\n3.4%\n78.8\nTable 3: Results of LLAMA-2-CHAT in zero-shot in two machine translation-based evaluation settings. translate-test\n(passages, questions, answers machine-translated back to English) and target language evaluations with the English\nzero-shot translations machine-translated to the target language. The traditional setting on the same languages is\nprovided for comparison. % \u2265 50/70 refers to the proportion of languages for which a given model performs above\n50/70%\nis related to two key factors. (i) First, the amount of\npretraining data in the target language. As predicted\nby the scaling laws (Kaplan et al., 2020), performance\nin a language increases monotonically with the amount\nof tokens the model is pretrained on. (ii) Second, the\ncross-lingual transfer happening between languages in\nthe pretraining data (potentially 100+ languages, e.g.,\nwith XLM-R) and the target language at inference time\n(Conneau et al., 2020a,b). This transfer is impacted by\na combination of typological similarities, token-overlap,\nand script similarity between the pretraining languages\nand the target language (Muller et al., 2021a, 2023). In\nthe case of LLMs and MLMs, these two factors are hard\nto disentangle due to the scale (up to \u223c1T tokens) and\nthe potential language leaks of large-scale pretraining\ncorpora (Kreutzer et al., 2022). However, our results\non BELEBELE provide detailed evidence of both these\nfactors impacting the multilingual generalization of the\nmodels.\nImpact of Pretraining Language Distribution\nOne\nof the key differences between the MLMs and LLMs\nevaluated is the pretraining data distribution. All the\nMLMs we evaluate have a balanced distribution of about\n100 languages. In comparison, the LLAMA models\nand FALCON are pretrained mainly in English (see Sec-\ntion 4.1). This explains the large performance differ-\nences between MLMs and LLMs. For instance, XLM-R\nreaches 58.8 accuracy on average across all the non-\nEnglish languages. LLAMA-2-CHAT (evaluated in the\nzero-shot setting) only reaches 41.2. In comparison\nLLAMA-2-CHAT outperforms XLM-R in English. This\ndifference between the MLMs and LLMs evaluated is\nillustrated in Fig. 3.\nHowever, despite this gap, LLAMA and FALCON\ncheckpoints perform surprisingly well on a large number\nof languages. For instance, LLAMA-2-CHAT is above\n35% accuracy (i.e. 10 points above the random base-\nline) for 59/122 languages and above 50% accuracy for\n33 languages. This shows that English-centric LLMs\npretrained model are a promising starting point to build\nmultilingual models.\nMachine Translation for Zero-Shot\nOur translate-\ntest evaluations show that using machine translation into\nEnglish strongly outperforms LLAMA-2-CHAT (70B)\nperformance in the original target language. Across 91\nevaluated languages, only 2 are non-trivially better in-\nlanguage (German and Italian), 21 languages are about\nthe same, and translating to English shows better results\nfor 68, none of which are considered high-resource.\nCompared to LLAMA-2-CHAT having zero-shot accu-\nracy above 50% for 33 languages, it has 71 in translate-\ntest (see Appendix A.3.4).\nIn addition, we evaluate machine-translating the task\ninstructions to the target language. Out of 89 languages\nevaluated, there are around 25 where the translated in-\nstructions were not well understood (i.e. accuracy less\nthan random), correlating strongly with languages that\nhad low scores to begin with. For the rest, the perfor-\nmance relative to receiving the instructions in English\nis quite split, preventing definitive conclusions. How-\never, languages with the largest accuracy boost from in-\nlanguage instructions are generally those on the higher\nend to begin with (e.g. Catalan and Portuguese).\nImpact of Sub-Word Tokenization\nWe reaffirm a\ncorrelation between increasing vocabulary size (and\nproper multilingual vocabulary construction methods\nsuch as clustering and capacity assignment) and perfor-\nmance on lower resource languages (Liang et al., 2023).\nIn particular, XLM-V has a vocabulary of 900k tokens\nthat is built by de-emphasizing token sharing between\nlanguages with little lexical overlap and proper vocab-\nulary capacity allocation for each individual language.\nXLM-V outperforms XLM-R and INFOXLM (250k vo-\ncabulary size) on low-resource languages even though\nthey all have the same architecture and are trained on the\nsame dataset (CC-100). GPT3.5-TURBO (100k vocabu-\nlary size),9 FALCON (65k vocabulary size), and LLAMA\n2 (32k vocabulary size) all fall off abruptly for medium-\nand low- resource languages. Larger vocabulary size\nmay explain why FALCON 40B performs equivalent to\nLLAMA 1 30B despite having been pretrained on fewer\nnon-English tokens.\n9According to https://github.com/openai/tiktoken.\nLlama 1 Model Size in Billions of Parameters\nAccuracy\n20\n45\n70\n95\n10\n20\n30\n40\n50\n60\n70\nEnglish \nRomance (#6)\nGermanic (#8)\nJaponic (#1)\nHellenic (#1)\nAustronesian (#10)\nDravidian (#4)\nRandom Baseline\nFigure 4: Impact of Models\u2019 scale (from 7B to 65B parameters of LLAMA 1) on the performance on BELEBELE\nfor 6 language families and English. The number of languages in a given family is indicated as (#N). LLAMA 1 is\nevaluated in the 5-shot settings with examples sampled from the training data in English. Scores are average over 3\nruns.\nScaling effect on Multilingual Generalization\nWe\nreport in Fig. 4 the impact of model sizes on perfor-\nmance on the BELEBELE benchmark across six lan-\nguage families in addition to English. We find that scale\nis critical for LLAMA to perform reading comprehen-\nsion. The 7B checkpoint performs slightly above chance\nin English and poorly for most languages\u2014however,\nthe performance increase significantly with the 13B and\neven more for 30B parameters.\nSurprisingly, we find that LLAMA performs non-\ntrivially in Japanese and Greek (cf. Japonic and Hellenic\nlanguage families in Fig. 4) while neither is reported to\nbe in the pretraining corpus. However, unlike other lan-\nguage families such as Romance and Germanic families,\nthe performance becomes non-trivial only with the 30B\nand 65B checkpoints. This suggests that generalizing\nto distant languages, in reference to English-centered\npretraining, requires more parameters.\nImpact of Script\nComparing the Romanized versions\nwith the original scripts for Hindi, Urdu, Bengali, Sin-\nhala, Nepali, and Modern Standard Arabic, we find that\nall models except FALCON perform stronger in the na-\ntive script rather than in the Latin script, as can be seen\nin Table 7. These scripts are allegedly not present in\nthe pretraining data for LLAMA 2 and FALCON (Tou-\nvron et al., 2023b; Penedo et al., 2023).10 For the Indo-\n10We note that thanks to bytes-fall back, all UTF-8 charac-\nters are supported by the LLAMA tokenizer.\nAryan languages (i.e. Bengali, Hindi, Nepali, Urdu),\nwe hypothesized cross-lingual transfer would be higher\nfor these languages in the Latin variant since the tok-\nenization will be more suitable and there is opportunity\nfor shared subwords (anchor points) (Conneau et al.,\n2020b). However, this only seems to be the case for\nFALCON. In the case of LLAMA-2, the results sug-\ngest the model pretrained on some samples with native\nscript (perhaps due to code-switching sentences or poor\nlanguage identification). Meanwhile, the \"refined\" FAL-\nCON pretraining dataset may have been less impacted\nby leaks, resulting in cross-lingual transfer to the Latin\nscript eclipsing the limited grasp of the native script.\nFurther analysis of the language distributions of the pre-\ntraining corpora of these models are needed to provide\nstronger evidence of these interpretations.\n6\nDiscussion\nOur goal in introducing BELEBELE is to foster the de-\nvelopment of LLMs and NLP systems beyond high-\nresource languages. However, modern LLMs require\ntrillions of tokens in pretraining to achieve their pow-\nerful natural language understanding, fluency, and in-\ncontext learning abilities (Brown et al., 2020; Chowdh-\nery et al., 2022; Touvron et al., 2023a). Building LLMs\nfor mid- to low-resource languages as capable as in\nEnglish will therefore require scaling corpus sizes in\nmore languages when feasible (Abadji et al., 2022) or\ndesigning competitive data-efficient training techniques\nsuch as cross-lingual transfer (Artetxe et al., 2020; Pfeif-\nfer et al., 2020; Muller et al., 2020, 2021a). To enable\nthis progress, we point to two critical research direc-\ntions. First, (i) better language identification systems:\npopular language identification models are trained on a\nrestricted number of languages and domains and only\nwork at the sentence level (Bojanowski et al., 2017), lim-\niting their abilities to track languages in code-switched\ndata and embedded text. Second, (ii) we encourage\nLLM developers to improve reporting on pretraining\nlanguage distribution. This is necessary for the research\ncommunity to understand the cross-lingual transfer ca-\npabilities of LLMs and to improve NLP system design\nfor low-resource languages.\n7\nConclusion\nA fundamental limitation to conducting sound evalua-\ntions of the capabilities of language models in low-, or\neven moderate-, resource languages is the availability\nof annotated benchmarks. This paper presents a mas-\nsive dataset, BELEBELE, consisting of passages and\nmultiple-choice questions evaluating reading compre-\nhension in 122 languages. This benchmark enables\ncritical evaluation of reading comprehension capabili-\nties of LLMs in English and top languages. In addition,\nthe dataset is the first of its kind in many medium- and\nlow-resource languages, enabling unprecedented insight\ninto the multilingual capabilities of language models.\nWith all the evaluations and experiments this dataset en-\nables, we hope future work will take a deeper dive into\ncurrent language models. We hope that BELEBELE will\neventually lead to discoveries into how current model ar-\nchitectures and training methods deal with multilingual\ndata and subsequently how they can be improved.\nLimitations\nEven with our extensive quality assurance, we warn that\n\"translationese\" may cause accuracy on non-English lan-\nguages to not be directly comparable to English. Often,\nthe perfect translation does not exist.\nIn addition, BELEBELE was designed to measure the\nreading comprehension abilities of NLP systems across\n122 languages. We specifically align as much as pos-\nsible with translation choices made in the creation of\nFLORES. Therefore, by-design the samples collected\ndo not capture language- and culture-specific phenom-\nena such as formality (Ersoy et al., 2023), values (Kova\u02c7c\net al., 2023), and aboutness (Hershcovich et al., 2022).\nFollowing BELEBELE, building NLP systems inclusive\nof all cultures and languages will require the release of\nbenchmarks that capture these phenomena.\nAs briefly mentioned in Section 3.3, annotators dis-\ncovered a few quality issues with FLORES. Some of\nthem are likely due to style/dialect differences between\nannotators, but many seem to not be. It\u2019s rare enough,\nthanks to the extensive quality-assurance loops imple-\nmented by the NLLB team and the LSP. However, over\nthe scale of 122 languages a fair number of issues have\narisen, especially in lower-resource languages. Since\nupdating the FLORES dataset is not in scope for this\nproject, we deliberated on each with the LSP to maxi-\nmize both appropriateness of the question/answers trans-\nlations and cross-language consistency.\n8\nAcknowledgements\nThe authors would like to acknowledge the annotators\nwho created the questions and translated the dataset to\n122 languages. Notably, we acknowledge the contri-\nbutions to the annotation process from Adam Hakimi,\nBrian Bui, Cynthia Gao, Michal Kolestik, Michaela\nFiolekov\u00e1, Pavl\u00edna Luke\u0161ov\u00e1, and Mirek Driml. In ad-\ndition, the authors want to acknowledge Patrick Lewis,\nParikshit Iyengar, and Waqar Malik for their support.\nReferences\nJulien Abadji, Pedro Ortiz Suarez, Laurent Romary, and\nBeno\u00eet Sagot. 2022. Towards a cleaner document-\noriented multilingual crawled corpus. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 4344\u20134355, Marseille, France.\nEuropean Language Resources Association.\nManish Agarwal and Prashanth Mannem. 2011. Auto-\nmatic gap-fill question generation from text books.\nIn Proceedings of the Sixth Workshop on Innovative\nUse of NLP for Building Educational Applications,\npages 56\u201364, Portland, Oregon. Association for Com-\nputational Linguistics.\nKaveri Anuranjana, Vijjini Anvesh Rao, and Radhika\nMamidi. 2019. Hindirc: A dataset for reading com-\nprehension in hindi. In 20th International Conference\non Computational Linguistics and Intelligent Text.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 4623\u20134637, Online. Association\nfor Computational Linguistics.\nEmily M. Bender. 2009. Linguistically na\u00efve != lan-\nguage independent: Why NLP needs linguistic typol-\nogy. In Proceedings of the EACL 2009 Workshop\non the Interaction between Linguistics and Compu-\ntational Linguistics: Virtuous, Vicious or Vacuous?,\npages 26\u201332, Athens, Greece. Association for Com-\nputational Linguistics.\nEmily M. Bender. 2011. On achieving and evaluating\nlanguage-independence in nlp. Linguistic Issues in\nLanguage Technology, 6.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135\u2013146.\nSamuel R. Bowman, Jennimaria Palomaki, Livio Bal-\ndini Soares, and Emily Pitler. 2020. New protocols\nand negative results for textual entailment data col-\nlection. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 8203\u20138214, Online. Association for\nComputational Linguistics.\nJordan Boyd-Graber and Benjamin B\u00f6rschinger. 2020.\nWhat question answering can learn from trivia nerds.\nIn Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, pages 7422\u2013\n7435, Online. Association for Computational Linguis-\ntics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham\nSinghal, Wenhui Wang, Xia Song, Xian-Ling Mao,\nHeyan Huang, and Ming Zhou. 2021. InfoXLM:\nAn information-theoretic framework for cross-lingual\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3576\u20133588, On-\nline. Association for Computational Linguistics.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A benchmark\nfor information-seeking question answering in typo-\nlogically diverse languages. Transactions of the As-\nsociation for Computational Linguistics, 8:454\u2013470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 8440\u20138451,\nOnline. Association for Computational Linguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2475\u20132485, Brussels,\nBelgium. Association for Computational Linguistics.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b.\nEmerging\ncross-lingual structure in pretrained language mod-\nels. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6022\u20136034, Online. Association for Computational\nLinguistics.\nDanilo Croce, Alexandra Zelenanska, and Roberto\nBasili. 2018. Neural learning for question answering\nin italian. In International Conference of the Italian\nAssociation for Artificial Intelligence.\nMartin d\u2019Hoffschmidt, Wacim Belblidia, Quentin\nHeinrich, Tom Brendl\u00e9, and Maxime Vidal. 2020.\nFQuAD: French question answering dataset. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1193\u20131208, Online. Association\nfor Computational Linguistics.\nPavel Efimov, Andrey Chertok, Leonid Boytsov, and\nPavel Braslavski. 2020. SberQuAD \u2013 russian read-\ning comprehension dataset: Description and analysis.\nIn Lecture Notes in Computer Science, pages 3\u201315.\nSpringer International Publishing.\nAsim Ersoy, Gerson Vizcarra, Tasmiah Tahsin Mayee-\nsha, and Benjamin Muller. 2023. In what languages\nare generative language models the most formal? ana-\nlyzing formality distribution across languages. ArXiv,\nabs/2302.12299.\nJack FitzGerald, Christopher Hench, Charith Peris,\nScott Mackie, Kay Rottmann, Ana Sanchez, Aaron\nNash, Liam Urbach, Vishesh Kakarala, Richa Singh,\nSwetha Ranganath, Laurie Crist, Misha Britan,\nWouter Leeuwis, Gokhan Tur, and Prem Natara-\njan. 2023.\nMASSIVE: A 1M-example multilin-\ngual natural language understanding dataset with\n51 typologically-diverse languages. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 4277\u20134302, Toronto, Canada. Association for\nComputational Linguistics.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc\u2019Aurelio Ranzato, Francisco Guzm\u00e1n,\nand Angela Fan. 2022. The Flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association\nfor Computational Linguistics, 10:522\u2013538.\nDeepak Gupta, Surabhi Kumari, Asif Ekbal, and Push-\npak Bhattacharyya. 2018. MMQA: A multi-domain\nmulti-lingual question-answering framework for En-\nglish and Hindi. In Proceedings of the Eleventh In-\nternational Conference on Language Resources and\nEvaluation (LREC 2018), Miyazaki, Japan. European\nLanguage Resources Association (ELRA).\nMomchil Hardalov, Ivan Koychev, and Preslav Nakov.\n2019. Beyond English-only reading comprehension:\nExperiments in zero-shot multilingual transfer for\nBulgarian. In Proceedings of the International Con-\nference on Recent Advances in Natural Language\nProcessing (RANLP 2019), pages 447\u2013459, Varna,\nBulgaria. INCOMA Ltd.\nMomchil\nHardalov,\nTodor\nMihaylov,\nDimitrina\nZlatkova, Yoan Dinkov, Ivan Koychev, and Preslav\nNakov. 2020. EXAMS: A multi-subject high school\nexaminations dataset for cross-lingual and multilin-\ngual question answering. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5427\u20135444, On-\nline. Association for Computational Linguistics.\nTahmid Hasan, Abhik Bhattacharjee, Md. Saiful Is-\nlam, Kazi Mubasshir, Yuan-Fang Li, Yong-Bin Kang,\nM. Sohel Rahman, and Rifat Shahriyar. 2021. XL-\nsum: Large-scale multilingual abstractive summariza-\ntion for 44 languages. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4693\u20134703, Online. Association for Computa-\ntional Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In International Conference on Learning\nRepresentations.\nDaniel Hershcovich, Stella Frank, Heather Lent,\nMiryam de Lhoneux, Mostafa Abdou, Stephanie\nBrandl, Emanuele Bugliarello, Laura Cabello Pi-\nqueras, Ilias Chalkidis, Ruixiang Cui, Constanza\nFierro, Katerina Margatina, Phillip Rust, and An-\nders S\u00f8gaard. 2022. Challenges and strategies in\ncross-cultural NLP. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 6997\u2013\n7013, Dublin, Ireland. Association for Computational\nLinguistics.\nPatrick Huber, Armen Aghajanyan, Barlas Oguz,\nDmytro Okhonko, Scott Yih, Sonal Gupta, and Xilun\nChen. 2022.\nCCQA: A new web-scale question\nanswering dataset for model pre-training. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2022, pages 2402\u20132420, Seattle, United\nStates. Association for Computational Linguistics.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601\u20131611, Vancouver,\nCanada. Association for Computational Linguistics.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6282\u20136293, Online. Association for Computational\nLinguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252\u2013262, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nGrgur Kova\u02c7c, Masataka Sawayama, R\u00e9my Portelas, C\u00e9-\ndric Colas, Peter Ford Dominey, and Pierre-Yves\nOudeyer. 2023. Large language models as super-\npositions of cultural perspectives.\narXiv preprint\narXiv:2307.07870.\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab,\nDaan van Esch, Nasanbayar Ulzii-Orshikh, Allah-\nsera Tapo, Nishant Subramani, Artem Sokolov, Clay-\ntone Sikasote, Monang Setyawan, Supheakmungkol\nSarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera, An-\nnette Rios, Isabel Papadimitriou, Salomey Osei, Pe-\ndro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, An-\ndre Niyongabo Rubungo, Toan Q. Nguyen, Math-\nias M\u00fcller, Andr\u00e9 M\u00fcller, Shamsuddeen Hassan\nMuhammad, Nanda Muhammad, Ayanda Mnyak-\neni, Jamshidbek Mirzakhalov, Tapiwanashe Matan-\ngira, Colin Leong, Nze Lawson, Sneha Kudugunta,\nYacine Jernite, Mathias Jenny, Orhan Firat, Bonaven-\nture F. P. Dossou, Sakhile Dlamini, Nisansa de Silva,\nSakine \u00c7abuk Ball\u0131, Stella Biderman, Alessia Bat-\ntisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar,\nIsrael Abebe Azime, Ayodele Awokoya, Duygu Ata-\nman, Orevaoghene Ahia, Oghenefego Ahia, Sweta\nAgrawal, and Mofetoluwa Adeyemi. 2022. Quality\nat a glance: An audit of web-crawled multilingual\ndatasets. Transactions of the Association for Compu-\ntational Linguistics, 10:50\u201372.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66\u201371, Brussels, Belgium. As-\nsociation for Computational Linguistics.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen McKeown. 2020. WikiLingua: A new bench-\nmark dataset for cross-lingual abstractive summariza-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 4034\u20134048,\nOnline. Association for Computational Linguistics.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. RACE: Large-scale ReAd-\ning comprehension dataset from examinations. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pages 785\u2013\n794, Copenhagen, Denmark. Association for Compu-\ntational Linguistics.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 7315\u2013\n7330, Online. Association for Computational Linguis-\ntics.\nDavis Liang, Hila Gonen, Yuning Mao, Rui Hou, Na-\nman Goyal, Marjan Ghazvininejad, Luke Zettle-\nmoyer, and Madian Khabsa. 2023. Xlm-v: Overcom-\ning the vocabulary bottleneck in multilingual masked\nlanguage models. arXiv preprint arXiv:2301.10472.\nBill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, and Xi-\nang Ren. 2021. Common sense beyond English: Eval-\nuating and improving multilingual language models\nfor commonsense reasoning. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume\n1: Long Papers), pages 1274\u20131287, Online. Associa-\ntion for Computational Linguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth\nPasunuru, Sam Shleifer, Punit Singh Koura, Vishrav\nChaudhary, Brian O\u2019Horo, Jeff Wang, Luke Zettle-\nmoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoy-\nanov, and Xian Li. 2022. Few-shot learning with\nmultilingual generative language models. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 9019\u20139052,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta:\nA robustly optimized bert pretraining approach.\nYash Madhani, Sushane Parthan, Priyanka Bedekar,\nRuchi Khapra, Vivek Seshadri, Anoop Kunchukuttan,\nPratyush Kumar, and Mitesh M. Khapra. 2022. Ak-\nsharantar: Towards building open transliteration tools\nfor the next billion users. CoRR, abs/2205.03018.\nChaitanya Malaviya, Sudeep Bhatia, and Mark Yatskar.\n2022. Cascading biases: Investigating the effect of\nheuristic annotation strategies on data and models.\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6525\u20136540, Abu Dhabi, United Arab Emirates. Asso-\nciation for Computational Linguistics.\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish\nSabharwal. 2018. Can a suit of armor conduct elec-\ntricity? a new dataset for open book question an-\nswering. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2381\u20132391, Brussels, Belgium. Association for\nComputational Linguistics.\nTimo M\u00f6ller, Julian Risch, and Malte Pietsch. 2021.\nGermanQuAD and GermanDPR: Improving non-\nEnglish question answering and passage retrieval.\nIn Proceedings of the 3rd Workshop on Machine\nReading for Question Answering, pages 42\u201350, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nHussein Mozannar, Elie Maamary, Karl El Hajal, and\nHazem Hajj. 2019. Neural Arabic question answer-\ning. In Proceedings of the Fourth Arabic Natural\nLanguage Processing Workshop, pages 108\u2013118, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nBenjamin Muller, Antonios Anastasopoulos, Beno\u00eet\nSagot, and Djam\u00e9 Seddah. 2021a. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 448\u2013462, Online. Association for Computa-\ntional Linguistics.\nBenjamin Muller, Yanai Elazar, Beno\u00eet Sagot, and\nDjam\u00e9 Seddah. 2021b. First align, then predict: Un-\nderstanding the cross-lingual ability of multilingual\nBERT. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 2214\u20132231,\nOnline. Association for Computational Linguistics.\nBenjamin Muller, Deepanshu Gupta, Jean-Philippe Fau-\nconnier, Siddharth Patwardhan, David Vandyke, and\nSachin Agarwal. 2023.\nLanguages you know in-\nfluence those you learn: Impact of language char-\nacteristics on multi-lingual text-to-text transfer. In\nProceedings of The 1st Transfer Learning for Natu-\nral Language Processing Workshop, volume 203 of\nProceedings of Machine Learning Research, pages\n88\u2013102. PMLR.\nBenjamin Muller, Beno\u00eet Sagot, and Djam\u00e9 Seddah.\n2020. Can multilingual language models transfer\nto an unseen dialect? a case study on north african\narabizi. ArXiv, abs/2005.00318.\nNikita Nangia and Samuel R. Bowman. 2019. Human\nvs. muppet: A conservative estimate of human perfor-\nmance on the GLUE benchmark. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4566\u20134575, Florence,\nItaly. Association for Computational Linguistics.\nNikita Nangia, Saku Sugawara, Harsh Trivedi, Alex\nWarstadt, Clara Vania, and Samuel R. Bowman. 2021.\nWhat ingredients make for an effective crowdsourc-\ning protocol for difficult NLU data collection tasks?\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1221\u20131235, Online. Association for Computational\nLinguistics.\nSimon Ostermann, Michael Roth, and Manfred Pinkal.\n2019. MCScript2.0: A machine comprehension cor-\npus focused on script events and participants. In Pro-\nceedings of the Eighth Joint Conference on Lexical\nand Computational Semantics (*SEM 2019), pages\n103\u2013117, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural In-\nformation Processing Systems, 35:27730\u201327744.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel Noth-\nman, Kevin Knight, and Heng Ji. 2017. Cross-lingual\nname tagging and linking for 282 languages. In Pro-\nceedings of the 55th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1946\u20131958, Vancouver, Canada. As-\nsociation for Computational Linguistics.\nAnselmo Pe\u00f1as, Yusuke Miyao, \u00c1lvaro Rodrigo, Ed-\nuard H. Hovy, and N. Kando. 2014. Overview of\nclef qa entrance exams task 2014. In Conference and\nLabs of the Evaluation Forum.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The refinedweb dataset\nfor falcon llm: Outperforming curated corpora with\nweb data, and web data only.\nJonas Pfeiffer, Ivan Vuli\u00b4c, Iryna Gurevych, and Se-\nbastian Ruder. 2020. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654\u20137673, Online. Association for Computa-\ntional Linguistics.\nEdoardo Maria Ponti, Goran Glava\u0161, Olga Majewska,\nQianchu Liu, Ivan Vuli\u00b4c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2362\u20132376, Online. As-\nsociation for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383\u20132392, Austin,\nTexas. Association for Computational Linguistics.\nMatthew Richardson, Christopher J.C. Burges, and Erin\nRenshaw. 2013. MCTest: A challenge dataset for\nthe open-domain machine comprehension of text. In\nProceedings of the 2013 Conference on Empirical\nMethods in Natural Language Processing, pages 193\u2013\n203, Seattle, Washington, USA. Association for Com-\nputational Linguistics.\nMelissa Roemmele, Cosmin Bejan, and Andrew Gordon.\n2011. Choice of plausible alternatives: An evaluation\nof commonsense causal reasoning. In AAAI Spring\nSymposium - Technical Report.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nPriyanka Sen, Alham Fikri Aji, and Amir Saffari.\n2022. Mintaka: A complex, natural, and multilin-\ngual dataset for end-to-end question answering. In\nProceedings of the 29th International Conference\non Computational Linguistics, pages 1604\u20131619,\nGyeongju, Republic of Korea. International Commit-\ntee on Computational Linguistics.\nTatiana Shavrina, Alena Fenogenova, Emelyanov An-\nton, Denis Shevelev, Ekaterina Artemova, Valentin\nMalykh, Vladislav Mikhailov, Maria Tikhonova, An-\ndrey Chertok, and Andrey Evlampiev. 2020. Rus-\nsianSuperGLUE: A Russian language understanding\nevaluation benchmark. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4717\u20134726, On-\nline. Association for Computational Linguistics.\nYuan Sun, Sisi Liu, Chaofan Chen, Zhengcuo Dan, and\nXiaobing Zhao. 2021. Construction of high-quality\nTibetan dataset for machine reading comprehension.\nIn Proceedings of the 20th Chinese National Confer-\nence on Computational Linguistics, pages 208\u2013218,\nHuhhot, China. Chinese Information Processing So-\nciety of China.\nMakarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen,\nAntonio Torralba, Raquel Urtasun, and Sanja Fidler.\n2015. Movieqa: Understanding stories in movies\nthrough question-answering. 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR),\npages 4631\u20134640.\nNLLB Team, Marta R. Costa-juss\u00e0, James Cross, Onur\n\u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Hef-\nfernan, Elahe Kalbassi, Janice Lam, Daniel Licht,\nJean Maillard, Anna Sun, Skyler Wang, Guillaume\nWenzek, Al Youngblood, Bapi Akula, Loic Bar-\nrault, Gabriel Mejia Gonzalez, Prangthip Hansanti,\nJohn Hoffman, Semarley Jarrett, Kaushik Ram\nSadagopan, Dirk Rowe, Shannon Spruit, Chau\nTran, Pierre Andrews, Necip Fazil Ayan, Shruti\nBhosale, Sergey Edunov, Angela Fan, Cynthia\nGao, Vedanuj Goswami, Francisco Guzm\u00e1n, Philipp\nKoehn, Alexandre Mourachko, Christophe Ropers,\nSafiyyah Saleem, Holger Schwenk, and Jeff Wang.\n2022.\nNo language left behind: Scaling human-\ncentered machine translation. Meta Research.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurelien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023b. Llama 2: Open foundation and fine-\ntuned chat models.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions. In\nProceedings of the 3rd Workshop on Noisy User-\ngenerated Text, pages 94\u2013106, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nJason Weston, Antoine Bordes, Sumit Chopra, and\nTom\u00e1s Mikolov. 2016. Towards ai-complete ques-\ntion answering: A set of prerequisite toy tasks. In\n4th International Conference on Learning Represen-\ntations, ICLR 2016, San Juan, Puerto Rico, May 2-4,\n2016, Conference Track Proceedings.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112\u20131122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nWeihao Yu, Zihang Jiang, Yanfei Dong, and Jiashi Feng.\n2020. Reclor: A reading comprehension dataset re-\nquiring logical reasoning. In International Confer-\nence on Learning Representations.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. SWAG: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods in\nNatural Language Processing, pages 93\u2013104, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nBarret Zoph, Deniz Yuret, Jonathan May, and Kevin\nKnight. 2016. Transfer learning for low-resource neu-\nral machine translation. In Proceedings of the 2016\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1568\u20131575, Austin, Texas.\nAssociation for Computational Linguistics.\nA\nAppendix\nA.1\nLanguages and Variants\nLanguage Code\nName in English\nScript\nFamily\nacm_Arab\nMesopotamian Arabic\nArab\nAfro-Asiatic\nafr_Latn\nAfrikaans\nLatn\nGermanic\nals_Latn\nTosk Albanian\nLatn\nPaleo-Balkanic\namh_Ethi\nAmharic\nEthi\nAfro-Asiatic\napc_Arab\nNorth Levantine Arabic\nArab\nAfro-Asiatic\narb_Arab\nModern Standard Arabic\nArab\nAfro-Asiatic\narb_Latn\nModern Standard Arabic (Romanized)\nLatn\nAfro-Asiatic\nars_Arab\nNajdi Arabic\nArab\nAfro-Asiatic\nary_arab\nMoroccan Arabic\nArab\nAfro-Asiatic\narz_Arab\nEgyptian Arabic\nArab\nAfro-Asiatic\nasm_Beng\nAssamese\nBeng\nIndo-Aryan\nazj_Latn\nNorth Azerbaijani\nLatn\nTurkic\nbam_Latn\nBambara\nLatn\nMande\nben_Beng\nBengali\nBeng\nIndo-Aryan\nben_Latn\nBengali (Romanized)\nLatn\nIndo-Aryan\nbod_Tibt\nStandard Tibetan\nTibt\nSino-Tibetan\nbul_Cyrl\nBulgarian\nCyrl\nBalto-Slavic\ncat_Latn\nCatalan\nLatn\nRomance\nceb_Latn\nCebuano\nLatn\nAustronesian\nces_Latn\nCzech\nLatn\nBalto-Slavic\nckb_Arab\nCentral Kurdish\nArab\nIranian\ndan_Latn\nDanish\nLatn\nGermanic\ndeu_Latn\nGerman\nLatn\nGermanic\nell_Grek\nGreek\nGrek\nHellenic\neng_Latn\nEnglish\nLatn\nGermanic\nest_Latn\nEstonian\nLatn\nUralic\neus_Latn\nBasque\nLatn\nBasque\nfin_Latn\nFinnish\nLatn\nUralic\nfra_Latn\nFrench\nLatn\nRomance\nfuv_Latn\nNigerian Fulfulde\nLatn\nAtlantic-Congo\ngaz_Latn\nWest Central Oromo\nLatn\nAfro-Asiatic\ngrn_Latn\nGuarani\nLatn\nTupian\nguj_Gujr\nGujarati\nGujr\nIndo-Aryan\nhat_Latn\nHaitian Creole\nLatn\nAtlantic-Congo\nhau_Latn\nHausa\nLatn\nAfro-Asiatic\nheb_Hebr\nHebrew\nHebr\nAfro-Asiatic\nhin_Deva\nHindi\nDeva\nIndo-Aryan\nhin_Latn\nHindi (Romanized)\nLatn\nIndo-Aryan\nhrv_Latn\nCroatian\nLatn\nBalto-Slavic\nhun_Latn\nHungarian\nLatn\nUralic\nhye_Armn\nArmenian\nArmn\nArmenian\nibo_Latn\nIgbo\nLatn\nAtlantic-Congo\nilo_Latn\nIlocano\nLatn\nAustronesian\nind_Latn\nIndonesian\nLatn\nAustronesian\nisl_Latn\nIcelandic\nLatn\nGermanic\nita_Latn\nItalian\nLatn\nRomance\njav_Latn\nJavanese\nLatn\nAustronesian\njpn_Jpan\nJapanese\nJpan\nJaponic\nkac_Latn\nJingpho\nLatn\nSino-Tibetan\nkan_Knda\nKannada\nKnda\nDravidian\nkat_Geor\nGeorgian\nGeor\nkartvelian\nkaz_Cyrl\nKazakh\nCyrl\nTurkic\nkea_Latn\nKabuverdianu\nLatn\nPortuguese Creole\nkhk_Cyrl\nHalh Mongolian\nCyrl\nMongolic\nkhm_Khmr\nKhmer\nKhmr\nAustroasiatic\nkin_Latn\nKinyarwanda\nLatn\nAtlantic-Congo\nkir_Cyrl\nKyrgyz\nCyrl\nTurkic\nkor_Hang\nKorean\nHang\nKoreanic\nlao_Laoo\nLao\nLaoo\nKra-Dai\nlin_Latn\nLingala\nLatn\nAtlantic-Congo\nlit_Latn\nLithuanian\nLatn\nBalto-Slavic\nlug_Latn\nGanda\nLatn\nAtlantic-Congo\nluo_Latn\nLuo\nLatn\nNilo-Saharan\nlvs_Latn\nStandard Latvian\nLatn\nBalto-Slavic\nmal_Mlym\nMalayalam\nMlym\nDravidian\nmar_Deva\nMarathi\nDeva\nIndo-Aryan\nmkd_Cyrl\nMacedonian\nCyrl\nBalto-Slavic\nmlt_Latn\nMaltese\nLatn\nAfro-Asiatic\nmri_Latn\nMaori\nLatn\nAustronesian\nmya_Mymr\nBurmese\nMymr\nSino-Tibetan\nnld_Latn\nDutch\nLatn\nGermanic\nnob_Latn\nNorwegian Bokm\u00e5l\nLatn\nGermanic\nnpi_Deva\nNepali\nDeva\nIndo-Aryan\nnpi_Latn\nNepali (Romanized)\nLatn\nIndo-Aryan\nnso_Latn\nNorthern Sotho\nLatn\nAtlantic-Congo\nnya_Latn\nNyanja\nLatn\nAfro-Asiatic\nory_Orya\nOdia\nOrya\nIndo-Aryan\npan_Guru\nEastern Panjabi\nGuru\nIndo-Aryan\npbt_Arab\nSouthern Pashto\nArab\nIndo-Aryan\npes_Arab\nWestern Persian\nArab\nIranian\nplt_Latn\nPlateau Malagasy\nLatn\nAustronesian\npol_Latn\nPolish\nLatn\nBalto-Slavic\npor_Latn\nPortuguese\nLatn\nRomance\nron_Latn\nRomanian\nLatn\nRomance\nrus_Cyrl\nRussian\nCyrl\nBalto-Slavic\nshn_Mymr\nShan\nMymr\nKra-Dai\nsin_Latn\nSinhala (Romanized)\nLatn\nIndo-Aryan\nsin_Sinh\nSinhala\nSinh\nIndo-Aryan\nslk_Latn\nSlovak\nLatn\nBalto-Slavic\nslv_Latn\nSlovenian\nLatn\nBalto-Slavic\nsna_Latn\nShona\nLatn\nAtlantic-Congo\nsnd_Arab\nSindhi\nArab\nIndo-Aryan\nsom_Latn\nSomali\nLatn\nAfro-Asiatic\nsot_Latn\nSouthern Sotho\nLatn\nAtlantic-Congo\nspa_Latn\nSpanish\nLatn\nRomance\nsrp_Cyrl\nSerbian\nCyrl\nBalto-Slavic\nssw_Latn\nSwati\nLatn\nAtlantic-Congo\nsun_Latn\nSundanese\nLatn\nAustronesian\nswe_Latn\nSwedish\nLatn\nGermanic\nswh_Latn\nSwahili\nLatn\nAtlantic-Congo\ntam_Taml\nTamil\nTaml\nDravidian\ntel_Telu\nTelugu\nTelu\nDravidian\ntgk_Cyrl\nTajik\nCyrl\nIranian\ntgl_Latn\nTagalog\nLatn\nAustronesian\ntha_Thai\nThai\nThai\nKra-Dai\ntir_Ethi\nTigrinya\nEthi\nAfro-Asiatic\ntsn_Latn\nTswana\nLatn\nAtlantic-Congo\ntso_Latn\nTsonga\nLatn\nAfro-Asiatic\ntur_Latn\nTurkish\nLatn\nTurkic\nukr_Cyrl\nUkrainian\nCyrl\nBalto-Slavic\nurd_Arab\nUrdu\nArab\nIndo-Aryan\nurd_Latn\nUrdu (Romanized)\nLatn\nIndo-Aryan\nuzn_Latn\nNorthern Uzbek\nLatn\nTurkic\nvie_Latn\nVietnamese\nLatn\nAustroasiatic\nwar_Latn\nWaray\nLatn\nAustronesian\nwol_Latn\nWolof\nLatn\nAtlantic-Congo\nxho_Latn\nXhosa\nLatn\nAtlantic-Congo\nyor_Latn\nYoruba\nLatn\nAtlantic-Congo\nzho_Hans\nChinese (Simplified)\nHans\nSino-Tibetan\nzho_Hant\nChinese (Traditional)\nHant\nSino-Tibetan\nzsm_Latn\nStandard Malay\nLatn\nAustronesian\nzul_Latn\nZulu\nLatn\nAtlantic-Congo\nTable 4: The 122 Languages & Scripts in BELEBELE.\nAs mentioned in Section 3, Bengali, Hindi, Sinhala, Nepali, Urdu, and Modern Standard Arabic are present twice,\nonce in their respective native scripts and once in the Latin script. Chinese is also present twice, in Simplified and\nTraditional characters. There are 50 Indo-European languages, which we decide to display in smaller language\nfamilies. Even so, Indo-Aryan is the most common language family (17), followed by Atlantic-Congo (16) and\nAfro-Asiatic (16).\nNote that the FLORES-200 code isn\u2019t exactly the same as the older FLORES-101 code, see the FLORES website\nfor details.\nA.2\nAnnotation Guidelines\nA.2.1\nMCQA Annotation Guidelines\nThe following is an abridged version of the particularized instructions provided to annotators for the task of creating\na multiple-choice question-answering dataset. As mentioned, we additionally provided a positive and negative\nexample for each guidelines to the annotators.\n1. Ensure that all answers, if not most, are decently plausible to require the test-taker to fully read and understand\nthe passage.\n2. In order to make the questions not overly easy, ensure that if the correct answer is word-for-word from the\npassage, at least some of the wrong answers are as well. This is to ensure that the person answering can\u2019t just\nguess the right answer based off identifying the answer in the passage through skimming.\n3. Make the questions as specific as possible, leave nothing to ambiguity.\n4. It should not be possible to answer the question without having read the passage, but the question must\nbe answerable using just the passage (no external knowledge). We encourage the use of phrases such as\n\u201cAccording to the passage...\u201d in the questions if there may be ambiguity.\n5. Try to sometimes have the answers be word-for-word in the passage and for other questions, the answers be in\nyour own words. We would like a balance of these two categories of question.\n6. Don\u2019t write questions with double negatives that may trivially fool the person answering.\n7. Try to ask questions in your own words and don\u2019t copy and paste entire phrases from the paragraph. This\nallows us to truly evaluate the comprehension as opposed to recognizing patterns in the way the question is\nextracted from the text. That being said, making the questions comprehensible and including details is very\nimportant, as mentioned above.\n8. Avoid using ambiguous qualifiers (for example, \u201cvery\u201d or \u201creally\u201d) or having the question centered around\nan extreme (for example, \u201cthe greatest\u201d or \u201cthe most\u201d). These phrases may leave room for ambiguity and\nsubjectivity. In addition, qualified statements change with time and may not be valid in the future.\nA.2.2\nTranslation Specifications\nTo align the dialect or langauge variant to FLORES, we instructed the LSP to use the same localization as in the\nFLORES-200 creation a few years prior. To align style, formality, and wording, we supplemented the traditional\ntranslation guidelines with the following:\nGiven that the associated translated passage is already established (and not subject to change as it is in\na published dataset), the translations of the questions and answers have to be fully compatible with it.\nThis means that for dates, proper nouns, units of measure, etc. where there is potential ambiguity, the\ntranslators have to follow what was done for the passage, even if they disagree that it is the more correct\ntranslation.\nFor example,\nHungarian Translated passage: \u201cJesus k\u00e9szen \u00e1ll a vit\u00e1ra...\u201d\nEnglish question: \u201cWhat was Jesus working on when...\u201d\nTherefore, in the questions and answers Jesus must be translated as \u201cJesus\u201d and not \u201cJ\u00e9zus\u201d.\nA.3\nDetailed Results Tables\nA.3.1\nCross-Lingual MLMs\nFull Results for Cross-Lingual MLMs\nEvaluation\nFinetune in English\nTranslate-Train-All\nModel Name\nXLM-V-large\nINFOXLM-large\nXLM-R-large\nXLM-V-large\nINFOXLM-large\nXLM-R-large\nAVG\n55.6\n56.2\n54.0\n60.2\n60.0\n58.9\nPCT Above 50\n69.7%\n67.2%\n64.8%\n76.2%\n70.5%\n69.7%\nPCT Above 70\n21.9%\n28.9%\n15.7%\n33.1%\n37.2%\n36.4%\neng_Latn\n76.2\n79.3\n76.2\n77.8\n81.2\n78.7\nacm_Arab\n51.2\n57.3\n55.4\n55.3\n57.6\n59.2\nafr_Latn\n69.3\n72.7\n69.1\n72.3\n75.1\n74.3\nals_Latn\n68.4\n68.9\n64.9\n70.8\n72.2\n71.4\namh_Ethi\n53.1\n52.9\n52.6\n61.6\n60.0\n60.7\napc_Arab\n56.1\n58.8\n57.9\n57.7\n60.6\n61.9\narb_Arab\n67.2\n71.0\n69.8\n70.6\n75.0\n74.3\narb_Latn\n29.3\n32.2\n27.6\n31.6\n33.4\n30.6\nars_Arab\n55.6\n59.9\n58.9\n61.1\n65.8\n65.9\nary_Arab\n43.8\n48.7\n44.0\n48.0\n52.8\n52.6\narz_Arab\n56.9\n60.2\n57.6\n61.4\n64.9\n66.1\nasm_Beng\n53.7\n53.6\n49.3\n58.6\n58.8\n56.9\nazj_Latn\n59.7\n61.3\n59.0\n65.0\n65.6\n65.1\nbam_Latn\n34.2\n34.9\n33.2\n39.2\n39.1\n36.9\nben_Beng\n60.0\n63.4\n59.6\n65.6\n69.6\n63.7\nben_Latn\n46.8\n36.9\n38.8\n53.0\n42.7\n48.1\nbod_Tibt\n24.0\n24.9\n23.7\n24.8\n23.3\n36.9\nbul_Cyrl\n72.6\n72.0\n70.1\n74.0\n75.3\n74.2\ncat_Latn\n71.6\n74.4\n72.0\n75.7\n78.1\n74.7\nceb_Latn\n45.4\n44.1\n42.3\n52.0\n52.6\n50.7\nces_Latn\n69.9\n72.3\n69.9\n72.3\n76.2\n74.4\nckb_Arab\n29.7\n52.3\n30.3\n36.9\n58.0\n36.9\ndan_Latn\n70.8\n74.1\n72.9\n73.0\n76.3\n74.7\ndeu_Latn\n72.6\n75.7\n72.9\n74.1\n78.7\n76.7\nell_Grek\n70.3\n72.3\n70.3\n73.1\n74.9\n73.0\nest_Latn\n63.2\n67.2\n64.8\n68.7\n70.7\n70.4\neus_Latn\n63.6\n66.1\n64.8\n68.2\n70.8\n70.3\nfin_Latn\n69.1\n72.4\n72.2\n73.0\n75.2\n74.9\nfra_Latn\n73.1\n74.2\n72.1\n74.6\n76.8\n75.6\nfuv_Latn\n29.7\n27.7\n26.4\n32.8\n30.7\n31.1\ngaz_Latn\n48.8\n33.8\n36.4\n52.6\n36.0\n43.3\ngrn_Latn\n53.9\n37.8\n37.9\n59.6\n40.6\n41.9\nguj_Gujr\n58.7\n57.0\n54.1\n63.3\n65.9\n63.1\nhat_Latn\n57.1\n39.6\n35.2\n63.2\n44.1\n39.8\nhau_Latn\n51.0\n41.1\n48.2\n53.4\n48.1\n53.0\nheb_Hebr\n67.2\n68.2\n64.8\n69.3\n72.3\n70.6\nhin_Deva\n57.9\n60.2\n57.4\n63.8\n64.3\n63.4\nhin_Latn\n53.1\n49.7\n46.8\n57.6\n55.4\n58.9\nhrv_Latn\n70.0\n72.4\n69.9\n71.2\n75.3\n74.0\nhun_Latn\n69.7\n70.8\n70.0\n73.1\n74.2\n72.8\nhye_Armn\n59.4\n61.0\n58.9\n65.9\n66.1\n64.7\nibo_Latn\n40.1\n32.2\n31.2\n46.8\n32.0\n32.2\nilo_Latn\n37.4\n36.3\n33.8\n38.1\n40.6\n39.7\nind_Latn\n68.9\n70.7\n68.0\n71.3\n73.1\n70.4\nisl_Latn\n67.3\n66.0\n63.8\n70.1\n68.9\n69.0\nita_Latn\n70.6\n72.8\n70.0\n71.8\n76.4\n73.3\njav_Latn\n64.2\n59.8\n60.8\n67.2\n63.3\n66.8\njpn_Jpan\n66.4\n70.1\n67.6\n71.3\n71.8\n71.0\nkac_Latn\n32.0\n29.1\n32.1\n33.8\n34.0\n33.3\nkan_Knda\n61.1\n62.0\n59.7\n66.6\n68.4\n69.1\nkat_Geor\n64.7\n64.8\n63.6\n68.0\n68.9\n67.4\nkaz_Cyrl\n60.1\n61.6\n56.8\n64.9\n65.3\n64.7\nkea_Latn\n44.0\n45.2\n44.9\n48.7\n47.7\n48.1\nkhk_Cyrl\n56.7\n58.8\n57.8\n61.1\n64.6\n64.2\nkhm_Khmr\n60.0\n59.0\n57.7\n63.0\n64.2\n63.8\nkin_Latn\n35.9\n33.6\n34.3\n39.1\n39.1\n38.6\nkir_Cyrl\n65.4\n63.4\n61.8\n68.3\n68.2\n67.7\nkor_Hang\n70.1\n71.4\n68.7\n72.9\n74.6\n74.8\nlao_Laoo\n55.8\n57.6\n53.0\n63.2\n63.6\n63.0\nlin_Latn\n44.7\n33.2\n30.6\n50.9\n35.3\n34.4\nlit_Latn\n68.3\n69.4\n67.2\n71.7\n72.9\n72.0\nlug_Latn\n39.9\n29.4\n31.6\n47.8\n34.7\n34.7\nluo_Latn\n30.3\n30.9\n30.8\n33.7\n34.9\n33.2\nlvs_Latn\n70.1\n71.3\n68.7\n74.1\n75.6\n73.0\nmal_Mlym\n62.0\n65.0\n62.7\n69.1\n68.3\n67.1\nmar_Deva\n62.6\n65.2\n60.8\n69.2\n68.8\n67.2\nmkd_Cyrl\n67.8\n69.3\n65.7\n71.0\n73.8\n72.8\nmlt_Latn\n37.9\n57.1\n38.1\n40.2\n63.7\n42.7\nmri_Latn\n32.0\n30.6\n32.2\n33.0\n35.7\n34.0\nmya_Mymr\n56.6\n59.1\n53.6\n62.2\n65.1\n62.9\nnld_Latn\n68.4\n71.7\n71.0\n68.6\n74.0\n72.8\nnob_Latn\n71.8\n73.6\n70.7\n72.8\n75.4\n74.2\nnpi_Deva\n58.4\n60.7\n55.7\n64.4\n65.8\n62.7\nnpi_Latn\n38.3\n35.8\n33.8\n37.4\n36.4\n34.8\nnso_Latn\n45.9\n31.3\n30.0\n53.2\n34.1\n34.7\nnya_Latn\n31.0\n29.2\n29.8\n34.2\n33.0\n30.8\nory_Orya\n60.8\n62.1\n58.6\n65.6\n65.4\n63.9\npan_Guru\n58.1\n59.2\n57.8\n63.1\n62.6\n62.0\npbt_Arab\n55.4\n56.0\n51.0\n60.6\n62.6\n61.1\npes_Arab\n68.3\n69.1\n68.2\n70.8\n73.6\n72.0\nplt_Latn\n55.7\n45.6\n52.7\n61.7\n53.4\n58.1\npol_Latn\n69.0\n70.4\n67.4\n72.1\n73.7\n72.7\npor_Latn\n70.9\n74.3\n70.6\n73.8\n77.1\n74.0\nron_Latn\n72.3\n72.9\n71.3\n74.0\n76.2\n74.8\nrus_Cyrl\n71.9\n73.8\n72.2\n75.4\n76.8\n77.1\nshn_Mymr\n26.9\n25.2\n26.3\n25.0\n26.4\n27.0\nsin_Latn\n24.9\n34.2\n30.7\n41.7\n38.3\n37.3\nsin_Sinh\n64.4\n67.2\n62.7\n69.8\n70.2\n68.6\nslk_Latn\n69.3\n71.9\n70.2\n72.6\n76.7\n73.0\nslv_Latn\n69.7\n72.2\n68.6\n71.8\n75.4\n73.9\nsna_Latn\n34.8\n37.2\n33.2\n37.1\n38.6\n35.9\nsnd_Arab\n55.2\n56.6\n51.9\n60.0\n61.3\n61.3\nsom_Latn\n46.0\n39.1\n42.6\n50.7\n46.3\n50.7\nsot_Latn\n46.8\n29.3\n31.3\n52.0\n31.9\n32.7\nspa_Latn\n71.0\n73.3\n71.4\n72.7\n75.3\n76.4\nsrp_Cyrl\n71.0\n70.9\n71.1\n73.6\n76.1\n75.9\nssw_Latn\n39.8\n30.6\n34.3\n47.1\n34.3\n38.9\nsun_Latn\n60.9\n50.7\n55.3\n64.2\n55.8\n59.4\nswe_Latn\n73.0\n75.0\n74.2\n74.2\n76.9\n75.1\nswh_Latn\n64.9\n65.3\n62.8\n69.3\n69.2\n68.7\ntam_Taml\n61.8\n64.6\n61.7\n67.4\n69.4\n65.3\ntel_Telu\n55.6\n57.8\n53.6\n62.1\n63.2\n61.1\ntgk_Cyrl\n38.2\n58.6\n33.8\n39.2\n64.3\n39.6\ntgl_Latn\n69.2\n67.4\n64.7\n72.0\n70.4\n70.0\ntha_Thai\n63.8\n68.1\n65.8\n69.0\n68.9\n70.1\ntir_Ethi\n33.3\n36.7\n33.8\n39.9\n42.1\n37.7\ntsn_Latn\n49.0\n35.0\n30.8\n49.8\n35.7\n34.3\ntso_Latn\n37.9\n36.3\n34.2\n41.7\n39.7\n37.1\ntur_Latn\n66.7\n70.2\n66.8\n70.6\n72.0\n72.0\nukr_Cyrl\n70.4\n70.9\n71.0\n72.3\n74.9\n75.0\nurd_Arab\n61.6\n63.8\n59.3\n65.6\n68.6\n66.3\nurd_Latn\n42.2\n42.6\n40.8\n49.4\n48.9\n48.4\nuzn_Latn\n65.2\n66.9\n64.4\n69.1\n70.6\n70.2\nvie_Latn\n69.6\n71.1\n69.4\n73.7\n72.9\n71.4\nwar_Latn\n46.4\n44.7\n43.7\n47.6\n49.3\n46.6\nwol_Latn\n36.8\n32.2\n30.4\n40.6\n32.3\n32.2\nxho_Latn\n48.7\n36.1\n39.0\n54.4\n40.2\n45.4\nyor_Latn\n35.0\n29.3\n28.7\n38.6\n32.0\n27.9\nzho_Hans\n69.8\n74.6\n71.0\n73.7\n76.2\n74.8\nzho_Hant\n69.2\n72.4\n67.1\n73.1\n74.3\n71.3\nzsm_Latn\n69.1\n72.6\n69.9\n72.4\n73.3\n72.2\nzul_Latn\n46.9\n36.4\n39.0\n54.2\n39.8\n44.1\nTable 5: Results of Cross-Lingual MLMs in the two settings described in Section 4.\nFor all, the large version was used which is the same architecture across all three. XLM-V has a significantly\nlarger vocabulary size, leading to more total parameters. We find that in general, INFOXLM and XLM-V are\nvery similar and both out-perform XLM-R across the board. INFOXLM outperforms XLM-V in higher- and\nmedium-resource languages, while XLM-V performs better on the lowest-resource languages. As a result, XLM-V\nhas the most scores above 50, but INFOXLM has more scores above 70.\nA.3.2\nLLMs\nFull Results for Large Language Models\nEvaluation\nZero-Shot for Instructed Models\n5-shot In-Context Learning\nTranslate-Train-All\nModel Name\nGPT3.5-TURBO\nLLAMA-2-CHAT\nLLAMA 2\nLLAMA 1\nFALCON\nXLM-V\nSize/Variant\n70B\n70B\n65B\n40B\nlarge\nAVG\n50.6\n41.5\n48.0\n40.9\n37.3\n60.2\nPCT Above 50\n43.4%\n27.1%\n38.5%\n25.4%\n16.4%\n76.2%\nPCT Above 70\n28.9%\n2.5%\n26.2%\n12.3%\n1.6%\n33.1%\neng_Latn\n87.7\n78.8\n90.9\n82.5\n77.2\n77.8\nacm_Arab\n51.6\n35.9\n47.9\n37.9\n37.6\n55.3\nafr_Latn\n78.3\n57.9\n75.9\n60.7\n53.4\n72.3\nals_Latn\n67.1\n36.0\n45.4\n34.9\n36.6\n70.8\namh_Ethi\n28.7\n28.9\n27.5\n27.8\n24.8\n61.6\napc_Arab\n55.6\n38.8\n51.2\n39.6\n36.3\n57.7\narb_Arab\n69.3\n42.3\n61.7\n44.1\n38.3\n70.6\narb_Latn\n31.1\n30.2\n26.8\n28.0\n26.3\n31.6\nars_Arab\n55.1\n37.4\n50.2\n40.7\n32.1\n61.1\nary_Arab\n45.7\n32.6\n40.6\n33.1\n32.3\n48.0\narz_Arab\n56.7\n37.3\n50.7\n37.4\n33.0\n61.4\nasm_Beng\n36.0\n35.7\n32.3\n28.9\n22.4\n58.6\nazj_Latn\n54.9\n33.4\n42.2\n33.6\n34.1\n65.0\nbam_Latn\n31.7\n29.4\n30.3\n28.4\n29.7\n39.2\nben_Beng\n43.6\n34.9\n39.1\n33.4\n22.6\n65.6\nben_Latn\n34.6\n30.4\n29.6\n29.2\n32.1\n53.0\nbod_Tibt\n26.6\n28.3\n25.7\n24.9\n26.8\n24.8\nbul_Cyrl\n76.0\n65.0\n80.4\n69.3\n41.9\n74.0\ncat_Latn\n78.4\n68.2\n84.6\n76.3\n58.8\n75.7\nceb_Latn\n53.3\n40.6\n50.4\n38.9\n39.2\n52.0\nces_Latn\n76.9\n65.0\n81.1\n70.7\n65.0\n72.3\nckb_Arab\n31.8\n32.8\n28.7\n31.6\n28.9\n36.9\ndan_Latn\n80.7\n66.2\n83.6\n73.6\n56.2\n73.0\ndeu_Latn\n83.3\n69.4\n84.6\n76.0\n70.1\n74.1\nell_Grek\n73.0\n50.7\n64.9\n44.2\n31.2\n73.1\nest_Latn\n73.1\n36.6\n53.0\n36.3\n34.9\n68.7\neus_Latn\n40.9\n31.1\n34.7\n32.8\n38.9\n68.2\nfin_Latn\n77.9\n62.7\n79.3\n55.7\n42.8\n73.0\nfra_Latn\n83.1\n72.2\n86.4\n77.5\n69.7\n74.6\nfuv_Latn\n26.1\n29.8\n24.9\n25.4\n25.1\n32.8\ngaz_Latn\n30.3\n29.3\n27.8\n29.1\n24.9\n52.6\ngrn_Latn\n34.2\n32.2\n32.4\n30.3\n33.8\n59.6\nguj_Gujr\n38.4\n31.1\n27.1\n25.7\n24.7\n63.3\nhat_Latn\n51.6\n34.1\n37.4\n33.7\n36.2\n63.2\nhau_Latn\n32.2\n32.1\n28.0\n26.4\n28.9\n53.4\nheb_Hebr\n64.2\n41.4\n54.9\n41.4\n31.1\n69.3\nhin_Deva\n49.1\n42.0\n52.6\n38.4\n27.1\n63.8\nhin_Latn\n52.3\n39.2\n49.0\n34.2\n40.0\n57.6\nhrv_Latn\n78.4\n64.7\n79.8\n66.9\n48.7\n71.2\nhun_Latn\n74.6\n61.1\n78.8\n66.7\n37.7\n73.1\nhye_Armn\n35.0\n31.9\n34.1\n32.1\n25.4\n65.9\nibo_Latn\n28.4\n30.1\n27.4\n25.3\n30.2\n46.8\nilo_Latn\n37.1\n33.2\n36.6\n32.1\n35.1\n38.1\nind_Latn\n74.2\n61.3\n81.4\n55.7\n52.1\n71.3\nisl_Latn\n62.3\n38.0\n54.3\n42.1\n36.4\n70.1\nita_Latn\n80.0\n68.6\n84.5\n76.1\n66.4\n71.8\njav_Latn\n46.7\n37.0\n40.3\n33.0\n36.8\n67.2\njpn_Jpan\n70.9\n56.6\n77.6\n53.9\n49.6\n71.3\nkac_Latn\n30.9\n30.7\n27.7\n28.6\n27.8\n33.8\nkan_Knda\n40.6\n21.9\n25.7\n24.4\n24.0\n66.6\nkat_Geor\n33.0\n34.6\n37.8\n34.3\n23.4\n68.0\nkaz_Cyrl\n35.0\n32.4\n29.3\n32.4\n32.6\n64.9\nkea_Latn\n46.0\n38.1\n45.4\n38.1\n38.0\n48.7\nkhk_Cyrl\n32.0\n31.1\n29.8\n28.4\n27.4\n61.1\nkhm_Khmr\n30.4\n30.6\n27.0\n28.2\n25.0\n63.0\nkin_Latn\n35.2\n30.6\n29.8\n28.5\n31.9\n39.1\nkir_Cyrl\n37.9\n32.2\n34.6\n32.5\n31.9\n68.3\nkor_Hang\n67.1\n56.3\n77.8\n52.9\n40.2\n72.9\nlao_Laoo\n30.0\n26.5\n24.3\n26.2\n28.1\n63.2\nlin_Latn\n33.8\n31.0\n28.0\n30.4\n29.3\n50.9\nlit_Latn\n72.0\n39.7\n52.1\n39.6\n39.3\n71.7\nlug_Latn\n28.4\n30.9\n29.2\n28.3\n28.9\n47.8\nluo_Latn\n27.1\n31.2\n29.4\n29.3\n29.9\n33.7\nlvs_Latn\n70.8\n41.0\n51.3\n39.0\n37.6\n74.1\nmal_Mlym\n34.9\n30.1\n32.4\n30.0\n21.2\n69.1\nmar_Deva\n38.3\n34.8\n41.2\n32.9\n25.0\n69.2\nmkd_Cyrl\n69.4\n55.7\n72.5\n56.2\n38.1\n71.0\nmlt_Latn\n44.8\n36.2\n44.9\n36.7\n35.4\n40.2\nmri_Latn\n33.3\n31.8\n28.5\n32.0\n29.7\n33.0\nmya_Mymr\n30.3\n31.3\n24.1\n24.2\n22.6\n62.2\nnld_Latn\n80.4\n66.2\n82.2\n73.3\n66.7\n68.6\nnob_Latn\n79.0\n65.7\n81.8\n70.9\n60.8\n72.8\nnpi_Deva\n40.4\n32.9\n40.4\n33.0\n25.4\n64.4\nnpi_Latn\n35.1\n30.4\n30.2\n30.0\n30.9\n37.4\nnso_Latn\n33.6\n30.1\n30.4\n27.4\n29.3\n53.2\nnya_Latn\n33.2\n29.3\n27.3\n28.7\n29.3\n34.2\nory_Orya\n29.2\n24.8\n23.9\n23.7\n65.6\npan_Guru\n39.1\n33.1\n26.3\n27.1\n23.4\n63.1\npbt_Arab\n32.3\n30.2\n30.8\n29.4\n29.4\n60.6\npes_Arab\n61.8\n41.8\n53.9\n41.0\n35.9\n70.8\nplt_Latn\n32.3\n30.5\n29.6\n31.0\n31.4\n61.7\npol_Latn\n74.7\n61.7\n79.2\n67.0\n59.9\n72.1\npor_Latn\n83.0\n70.2\n86.1\n75.4\n68.3\n73.8\nron_Latn\n77.4\n65.6\n83.4\n73.2\n66.6\n74.0\nrus_Cyrl\n78.4\n67.0\n82.7\n73.1\n48.1\n75.4\nshn_Mymr\n28.2\n25.6\n22.7\n24.0\n25.0\nsin_Latn\n30.4\n31.9\n33.8\n27.9\n32.6\n41.7\nsin_Sinh\n32.6\n33.4\n25.2\n29.4\n27.7\n69.8\nslk_Latn\n77.3\n58.8\n75.2\n60.4\n57.0\n72.6\nslv_Latn\n77.4\n62.4\n76.7\n65.6\n43.7\n71.8\nsna_Latn\n35.4\n30.2\n27.4\n28.3\n31.6\n37.1\nsnd_Arab\n34.1\n29.7\n30.9\n28.9\n30.2\n60.0\nsom_Latn\n32.4\n30.3\n27.8\n27.6\n29.9\n50.7\nsot_Latn\n33.9\n30.0\n28.9\n26.8\n29.9\n52.0\nspa_Latn\n79.2\n68.4\n85.0\n74.8\n69.2\n72.7\nsrp_Cyrl\n74.8\n65.1\n81.0\n70.7\n40.2\n73.6\nssw_Latn\n32.0\n30.7\n27.7\n28.0\n30.1\n47.1\nsun_Latn\n38.9\n34.9\n37.8\n30.7\n34.1\n64.2\nswe_Latn\n81.7\n67.4\n82.7\n73.7\n67.3\n74.2\nswh_Latn\n70.3\n35.1\n39.6\n34.4\n36.7\n69.3\ntam_Taml\n32.8\n34.4\n33.2\n31.6\n24.4\n67.4\ntel_Telu\n34.6\n27.5\n25.9\n26.6\n22.4\n62.1\ntgk_Cyrl\n37.7\n32.5\n34.0\n33.1\n32.7\n39.2\ntgl_Latn\n66.7\n49.6\n68.1\n48.3\n47.7\n72.0\ntha_Thai\n55.7\n38.9\n46.2\n35.0\n33.0\n69.0\ntir_Ethi\n28.4\n29.6\n24.5\n23.5\n25.0\n39.9\ntsn_Latn\n31.8\n30.1\n28.5\n24.7\n31.2\n49.8\ntso_Latn\n33.4\n30.0\n30.4\n28.0\n29.7\n41.7\ntur_Latn\n69.9\n47.3\n65.4\n42.1\n39.6\n70.6\nukr_Cyrl\n72.8\n65.7\n80.8\n69.7\n41.9\n72.3\nurd_Arab\n48.3\n37.0\n43.2\n34.7\n31.7\n65.6\nurd_Latn\n40.3\n34.1\n38.0\n30.1\n34.2\n49.4\nuzn_Latn\n44.1\n33.1\n35.1\n30.6\n33.1\n69.1\nvie_Latn\n72.9\n59.6\n78.4\n43.5\n41.4\n73.7\nwar_Latn\n48.9\n39.3\n44.4\n37.4\n38.6\n47.6\nwol_Latn\n29.0\n28.9\n27.6\n26.0\n26.8\n40.6\nxho_Latn\n30.0\n29.9\n28.2\n27.6\n30.2\n54.4\nyor_Latn\n29.1\n30.1\n28.3\n27.7\n27.2\n38.6\nzho_Hans\n77.6\n62.4\n83.7\n64.6\n66.0\n73.7\nzho_Hant\n76.3\n59.3\n82.0\n57.7\n62.2\n73.1\nzsm_Latn\n74.0\n56.4\n76.3\n51.7\n51.3\n72.4\nzul_Latn\n30.4\n30.2\n29.7\n27.1\n30.7\n54.2\nTable 6: Results on LLMs, with comparison to full finetuning on XLM-V\nThe evaluation settings and models are described in more detail in Section 4. We see that none of these models\ncan understand many of the 122 languages, while demonstrating excellent performance on high-resource languages.\nThe 175B-parameter GPT3.5-TURBO outperforms LLAMA-2-CHAT (70B) across the board and has comparable\nresults to LLAMA 2 (70B) even though it is in zero-shot. Note that GPT3.5-TURBO threw errors when processing\ncharacters in Shan (shn_Mymr) and Oriya (ory_Orya) and therefore we could not evaluate the results. For the\npurposes of aggregated scores, we consider this a score of 25.0. For comparison to fully-finetuned multilingual\nmodels, we re-provide the results of XLM-V-large.\nA.3.3\nLanguages in Multiple Scripts\nComparative Results for Languages with Multiple Scripts\nEvaluation\nZero-Shot\nFive-Shot\nFinetune in English\nModel Name\nGPT3.5-TURBO\nLLAMA 2 (70B)\nFALCON (40B)\nINFOXLM-large\nAVG\narb_Arab\n69.3\n61.7\n38.3\n71.0\n60.1\narb_Latn\n31.1\n26.8\n26.3\n32.2\n29.1\nben_Beng\n43.6\n39.1\n22.6\n63.4\n42.2\nben_Latn\n34.6\n29.6\n32.1\n36.9\n33.3\nhin_Deva\n49.1\n52.6\n27.1\n60.2\n47.3\nhin_Latn\n52.3\n49.0\n40.0\n49.7\n47.8\nnpi_Deva\n40.4\n40.4\n25.4\n60.7\n41.7\nnpi_Latn\n35.1\n30.2\n30.9\n35.8\n33.0\nsin_Sinh\n32.6\n25.2\n27.7\n67.2\n38.2\nsin_Latn\n30.4\n33.8\n32.6\n34.2\n32.8\nurd_Arab\n48.3\n43.2\n31.7\n63.8\n46.7\nurd_Latn\n40.3\n38.0\n34.2\n42.6\n38.8\nzho_Hant\n76.3\n82.0\n62.2\n72.4\n73.3\nzho_Hans\n77.6\n83.7\n66.0\n74.6\n75.5\nTable 7: Selected results from 3 models in differing settings comparing languages present in multiple scripts.\nWe find that generally, the performance is higher in the native script than the romanized version, except for\nFALCON which displays the opposite trend on the 5 Indo-Aryan languages. In Chinese, performance on simplified\n& traditional are very similar with simplified being higher across all 4. For INFOXLM, we display the English\nfinetuning score.\nA.3.4\nTranslate-Test\nTranslate-Test Results on 91 languages\nEvaluation\nZero-Shot\nTranslate-Test, Zero-Shot\nTranslate-Train-All\nModel\nLLAMA-2-CHAT (70B)\nXLM-V-large\nAVG\n44.0\n57.1\n64.9\nPCT Above 50\n35.2%\n78.0%\n90.1%\nPCT Above 70\n2.2%\n2.2%\n42.9%\neng_Latn\n87.7\n87.7\n77.8\nfra_Latn\n72.2\n70.6\n73.1\npor_Latn\n70.2\n69.9\n70.9\ndeu_Latn\n69.4\n65.7\n72.6\nita_Latn\n68.6\n66.1\n70.6\nspa_Latn\n68.4\n69.3\n71.0\ncat_Latn\n68.2\n67.0\n71.6\nswe_Latn\n67.4\n66.1\n73.0\nrus_Cyrl\n67.0\n67.3\n71.9\ndan_Latn\n66.2\n66.8\n70.8\nnld_Latn\n66.2\n67.2\n68.4\nnob_Latn\n65.7\n68.3\n71.8\nukr_Cyrl\n65.7\n66.0\n70.4\nron_Latn\n65.6\n67.0\n72.3\nsrp_Cyrl\n65.1\n66.2\n71.0\nbul_Cyrl\n65.0\n67.7\n72.6\nces_Latn\n65.0\n65.6\n69.9\nhrv_Latn\n64.7\n65.3\n70.0\nfin_Latn\n62.7\n61.1\n69.1\nslv_Latn\n62.4\n61.2\n69.7\nzho_Hans\n62.4\n71.2\n69.8\npol_Latn\n61.7\n63.0\n69.0\nind_Latn\n61.3\n64.8\n68.9\nhun_Latn\n61.1\n62.9\n69.7\nvie_Latn\n59.6\n59.4\n69.6\nzho_Hant\n59.3\n65.8\n69.2\nslk_Latn\n58.8\n66.2\n69.3\nafr_Latn\n57.9\n65.0\n69.3\njpn_Jpan\n56.6\n54.8\n66.4\nzsm_Latn\n56.4\n67.0\n69.1\nkor_Hang\n56.3\n56.7\n70.1\nmkd_Cyrl\n55.7\n66.7\n67.8\nell_Grek\n50.7\n67.6\n70.3\ntgl_Latn\n49.6\n62.2\n69.2\ntur_Latn\n47.3\n62.6\n66.7\narb_Arab\n42.3\n60.7\n67.2\nhin_Deva\n42.0\n62.8\n57.9\npes_Arab\n41.8\n59.6\n68.3\nheb_Hebr\n41.4\n62.0\n67.2\nlvs_Latn\n41.0\n60.9\n70.1\nceb_Latn\n40.6\n62.6\n45.4\nlit_Latn\n39.7\n60.8\n68.3\nhin_Latn\n39.2\n52.7\n53.1\ntha_Thai\n38.9\n54.1\n63.8\nisl_Latn\n38.0\n58.1\n67.3\njav_Latn\n37.0\n55.3\n64.2\nurd_Arab\n37.0\n59.4\n61.6\nest_Latn\n36.6\n59.4\n63.2\nals_Latn\n36.0\n63.1\n68.4\nasm_Beng\n35.7\n57.7\n53.7\nswh_Latn\n35.1\n57.8\n64.9\nben_Beng\n34.9\n61.0\n60.0\nsun_Latn\n34.9\n50.8\n60.9\nmar_Deva\n34.8\n60.0\n62.6\nkat_Geor\n34.6\n57.7\n64.7\ntam_Taml\n34.4\n55.9\n61.8\nurd_Latn\n34.1\n43.0\n42.2\nhat_Latn\n34.1\n56.3\n57.1\nazj_Latn\n33.4\n55.6\n59.7\nsin_Sinh\n33.4\n57.7\n64.4\npan_Guru\n33.1\n57.6\n58.1\nnpi_Deva\n32.9\n62.0\n58.4\nckb_Arab\n32.8\n51.3\n29.7\nkaz_Cyrl\n32.4\n53.2\n60.1\nhau_Latn\n32.1\n43.4\n51.0\nhye_Armn\n31.9\n58.0\n59.4\nmya_Mymr\n31.3\n46.6\n56.6\nkhk_Cyrl\n31.1\n52.2\n56.7\nguj_Gujr\n31.1\n59.6\n58.7\nlin_Latn\n31.0\n40.3\n44.7\nlug_Latn\n30.9\n38.7\n39.9\nssw_Latn\n30.7\n43.2\n39.8\nkhm_Khmr\n30.6\n52.8\n60.0\nplt_Latn\n30.5\n46.7\n55.7\nben_Latn\n30.4\n45.1\n46.8\nsom_Latn\n30.3\n40.8\n46.0\npbt_Arab\n30.2\n48.8\n55.4\nzul_Latn\n30.2\n44.4\n46.9\nnso_Latn\n30.1\n43.4\n45.9\ntsn_Latn\n30.1\n40.4\n49.0\nyor_Latn\n30.1\n37.7\n35.0\nibo_Latn\n30.1\n35.3\n40.1\nmal_Mlym\n30.1\n63.0\n62.0\nxho_Latn\n29.9\n49.2\n48.7\nfuv_Latn\n29.8\n29.4\n29.7\ngaz_Latn\n29.3\n37.0\n48.8\nory_Orya\n29.2\n57.8\n60.8\namh_Ethi\n28.9\n50.4\n53.1\nwol_Latn\n28.9\n39.0\n36.8\ntel_Telu\n27.5\n54.3\n55.6\nlao_Laoo\n26.5\n47.4\n55.8\nkan_Knda\n21.9\n62.0\n61.1\nTable 8: Comparing LLAMA-2-CHAT zero-shot performance in-language vs translate-test on 91 languages, with an\naditional comparison to translate-train-all finetuning on XLM-V.\nWe evaluate 91 of the 122 languages in translate-test and find that translate-test performance dominates in-\nlanguage performance on a big majority of languages for LLAMA-2-CHAT (70B) in zero-shot. A few head languages\nsuch as German and Italian have higher scores in the traditional setting, but nearly all medium-resource languages\nare better understood with machine translation. For nearly all low-resource languages, the difference is over 20\naccuracy points. For comparison, we see that machine translation lifts LLAMA-2-CHAT performance in zero-shot\nnot far from fully finetuned XLM-V-large.\nFigure 5: Comparison of LLAMA-2-CHAT (70B) zero-shot performance on translate-test and the standard in-\nlanguage evaluation.\n"
  },
  {
    "title": "SportsSloMo: A New Benchmark and Baselines for Human-centric Video Frame Interpolation",
    "link": "https://arxiv.org/pdf/2308.16876.pdf",
    "upvote": "6",
    "text": "SportsSloMo: A New Benchmark and Baselines for\nHuman-centric Video Frame Interpolation\nJiaben Chen*\nUC San Diego\njic088@ucsd.edu\nHuaizu Jiang\nNortheastern University\nh.jiang@northeastern.edu\nAbstract\nHuman-centric video frame interpolation has great po-\ntential for enhancing entertainment experiences and find-\ning commercial applications in the sports analysis indus-\ntry, e.g., synthesizing slow-motion videos. Although there\nare multiple benchmark datasets available for video frame\ninterpolation in the community, none of them is dedicated\nto human-centric scenarios. To bridge this gap, we intro-\nduce SportsSloMo, a benchmark featuring over 130K high-\nresolution (\u2265720p) slow-motion sports video clips, total-\ning over 1M video frames, sourced from YouTube. We re-\ntrain several state-of-the-art methods on our benchmark,\nand we observed a noticeable decrease in their accuracy\ncompared to other datasets. This highlights the difficulty\nof our benchmark and suggests that it poses significant\nchallenges even for the best-performing methods, as hu-\nman bodies are highly deformable and occlusions are fre-\nquent in sports videos. To tackle these challenges, we pro-\npose human-aware loss terms, where we add auxiliary su-\npervision for human segmentation in panoptic settings and\nkeypoints detection. These loss terms are model-agnostic\nand can be easily plugged into any video frame interpo-\nlation approach. Experimental results validate the effec-\ntiveness of our proposed human-aware loss terms, leading\nto consistent performance improvement over existing mod-\nels. The dataset and code can be found at: https://neu-\nvi.github.io/SportsSlomo/.\n1. Introduction\nVideo frame interpolation (VFI) is a technique that syn-\nthesizes intermediate frames from input images, enhanc-\ning the clarity of content that may be difficult to see oth-\nerwise. This technique finds wide-ranging applications, in-\ncluding slow-motion video generation [27], novel view syn-\nthesis [39], video compression [73], cartoon and rendered\n*Work mainly done when Jiaben Chen was an intern at Northeastern\nUniversity.\n(a)\n(b)\n(c)\n(d)\nFigure 1: Human-centric video frame interpolation re-\nsults. We propose human-aware auxiliary losses to improve\ninterpolation accuracy at motion boundaries. From left to\nright: (a): Overlayed inputs, (b): Ground truth, (c): Inter-\npolation results, (d): Interpolation results with our proposed\nhuman-aware loss terms.\ncontent generation [37, 4], etc. In recent years, we have wit-\nnessed significant advances in this field, due in large part to\nthe development of various benchmarks [1, 65, 57, 66, 79,\n14, 64].\nHumans feature prominently in most contemporary\nvideos. With the widespread use of mobile devices, peo-\nple can easily record and share their daily experiences with\nfamily, friends, and colleagues. Meanwhile, live broadcasts\nof sporting events attract a large audience. Automatically\ngenerated slow-motion videos can create a more immersive\nand engaging experience for users by highlighting details\nand valuable moments of their lives that may be missed in\nreal-time. Therefore, improving video frame interpolation\nresults for human-centric videos has great potential for en-\nhancing user experiences in entertainment. Human-centric\nvideo frame interpolation approaches can also be beneficial\nin various industries. Athletes and coaches, for example,\ncan use slow-motion synthesis to identify flaws in tech-\nniques, highlight areas for improvement, and gain a more\ndetailed understanding of how different factors may con-\ntribute to the success.\nA new benchmark. Despite the availability of various\narXiv:2308.16876v2  [cs.CV]  12 Dec 2023\nHuman Keypoints\nSegmentation Masks\nFigure 2: Visualization of human keypoints [78] and\npanoptic segmentation masks [10].\nbenchmarks for video frame interpolation, a notable gap ex-\nists in datasets specifically tailored for human-centric sce-\nnarios.\nTo bridge this gap and to foster the research in\nthis important direction, we introduce SportsSloMo, a new\ndataset comprising high-resolution (\u2265720p) slow-motion\nsports videos crawled from YouTube under the Common\nCreative Licence. This dataset encompasses a diverse range\nof sports, such as football, basketball, baseball, hockey, etc.\nSince a video may contain advertisement, transition frames,\nchanges of shot, and non-slow-motion content, we carefully\ncurate the data to remove such unwanted content and finally\nsplit each long video into a set of short slow-motion clips\nof 9 frames. The first and last frames are used as input and\nthe rest 7 intermediate frames are reserved as ground truths\nfor training and evaluating VFI models. In total, our bench-\nmark has 130K video clips and more than 1M video frames.\nCompared with existing datasets, as shown in Table 1, our\nproposed SportsSloMo benchmark is the largest one so far,\nwith high resolution and focus on human-centric scenarios.\nWhile primarily designed for human-centric VFI in this\npaper, we believe SportsSloMo dataset may also have the\npotential to aid research in other tasks such as video super-\nresolution [6, 38], group activity recognition [67, 83, 81],\nand dynamic view synthesis [39, 17, 75]. By releasing this\ndataset to the entire community, we hope to encourage tech-\nnical advancement in human-centric video frame interpola-\ntion and empower researchers to explore innovative appli-\ncations in other adjacent fields.\nBenchmarking existing approaches. To facilitate de-\nvelopment and evaluation of human-centric video frame in-\nterpolation methods, we re-train several state-of-the-art ap-\nproaches [27, 36, 23, 25, 29, 28, 82] using their publicly\nreleased code on our SportsSloMo dataset. As the human\nbodies are highly deformable and occlusions are frequent in\nsports videos, the accuracy of all the methods decrease com-\npared to their performance on other datasets. For instance,\nEBME [29] and EMA-VFI [82], two of the top-performing\napproaches on SportsSloMo, produce PSNR scores of 30.15\nand 30.70, respectively - markedly lower than their scores\nTable 1: Comparisons of different benchmark datasets\nfor video frame interpolation. (#inter. frames indicates\nthe number of intermediate frames to synthesize.)\nDataset\n#clips #images #inter.\nresolution human-\nframes\ncentric\nUCF101 [65]\n0.4K\n1K\n1\n256\u00d7256\n\u2713\nAdobe240fps [66] 0.1K\n80K\n7\n1280\u00d7720\n\u2717\nVimeo90K [79]\n70K\n220K\n1\n448\u00d7256\n\u2717\nSNU-FILM [14]\n1.2K\n3.6K\n1\n1280\u00d7720\n\u2717\nX4K1000FPS [64] 4.4K\n286K\n7\n4096\u00d72160\n\u2717\nSportsSloMo\n130K 1183K\n7\n1280\u00d7720\n\u2713\nof 36.19 and 36.64 on Viemo90K [79], as well as 30.64 and\n30.94 on the hard split of the SNU-FILM [14] benchmarks.\nIt highlights the difficulty of our benchmark and suggests\nsignificant challenges need to be addressed.\nEnhancing models for human-centric VFI. To im-\nprove the existing VFI models on our benchmark, we in-\ntroduce human-aware priors to enhance the model train-\ning. Specifically, we propose loss terms based on human\nsegmentation in the panoptic setting [10] and human key-\npoints estimation [78] as extra supervision for intermediate\nframe synthesis. Fig. 2 shows a visualization of detected\nhuman keypoints and segmentation masks in our dataset.\nHuman segmentation masks help delineate human body\nboundaries, which are helpful for reducing ghost effects\naround the motion boundaries. At the same time, human\nkeypoints estimation can also indicate where each body part\nis, enforcing coherent motion trajectories in the synthesized\nvideo frames. Specifically, we compare the output from\npre-trained panoptic segmentation and keypoints detection\nmodels taking input as a synthesized and the ground-truth\nintermediate frame, respectively, and use the consistency as\nsupervision. As shown in Fig. 1, by supervising with our\nproposed human-aware loss terms, we improve the inter-\npolation quality at motion boundaries with less blurry re-\nsults in scenarios with large motion and occlusion. Both\nof these human-aware loss terms are model agnostic and\ncan be easily integrated into any video frame interpolation\napproach. Experimental results show that they can consis-\ntently improve the accuracy of seven existing approaches,\nleading to strong baselines on our benchmark.\nTo sum up, this paper makes the following contributions:\n\u2022 We introduce SportsSloMo, a new benchmark dataset\nconsisting of a large amount of slow-motion sports\nvideos. To the best of our knowledge, this is the first high-\nresolution dataset tailored for human-centric video frame\ninterpolation, supporting synthesis of multiple intermedi-\nate frames.\n\u2022 We benchmark state-of-the-art approaches on the new\nbenchmark, highlighting the challenges of the human-\ncentric video frame interpolation task.\n\u2022 We propose two human-aware loss terms, which take the\nhuman priors into account for video frame interpolation\nand can easily be plugged into existing video frame inter-\npolation approaches. Experimental results validate that\nthey can consistently improve existing models, yielding\nstrong baseline models on our new benchmark.\n2. Related Work\n2.1. Benchmark datasets\nExisting publicly available datasets already provide a\nvaluable resource for developing and evaluating video\nframe interpolation methods. We would briefly introduce\nthese datasets and reveal some limitations in this section.\nTable. 1 shows a comparison between existing datasets and\nour proposed SportsSloMo dataset.\nThe SNU-FILM [14] dataset is a widely-used benchmark\nfor VFI evaluation, containing 1240 frame triplets of 1280\n\u00d7 720 resolution. And it is divided into four different parts,\nnamely, Easy, Medium, Hard, and Extreme according to\nmotion magnitude. The Middlebury benchmark [1] is an-\nother widely used dataset, image resolution in this dataset\nis around 640 \u00d7 480. However, it is commonly only used\nto evaluate VFI methods for 8 sequences. UCF101 [65] is\noriginally a dataset for human action recognition, contain-\ning a variety of human actions. With the test set constructed\nby [43], it is also used to evaluate VFI methods, contain-\ning 379 triplets of 256 \u00d7 256 frame size. Nevertheless,\nits scale is rather small and its resolution is also low. The\nAdobe240fps dataset [66], originally for video deblurring,\nis another widely used dataset for VFI. It is consisted of\nhigh frame-rate videos (240 fps) with a resolution of 1280\n\u00d7 720, yet the videos are from only 118 clips.\nThe mostly used dataset for training and evaluating VFI\nmethods is the Vimeo90K dataset [79]. It contains 73171\nframe triplets from 14777 video clips extracted from real-\nlife video clips with a fps \u2264 30. Nevertheless, one of its\nmain drawback is the low resolution of 448 \u00d7 256 obtained\nby downscaling the original high resolution frames. More-\nover, as VFI methods have been rapidly improving in recent\nyears, their performance on the widely-used Vimeo90K\ndataset has approached saturation. To further advance the\nstate-of-the-art in video frame interpolation, a new dataset\nwith bigger scale, higher resolution and more challenging\nscenarios is necessary. X4K1000FPS is a recently released\nhigh frame rate (1000 fps) with 4K spatial resolution to pro-\nmote the study of VFI for very high resolution videos.\nAs a result, none of existing datasets contains rich\nhuman-centric data with high resolution and big scale. To\nbridge this gap, we introduce the SportsSloMo dataset, a\nhuman-centric VFI dataset with 130K video clips and 1M\nvideo frames at 240 fps with a resolution of 1280 \u00d7 720,\naiming to foster research in human-centric VFI.\n2.2. Video frame interpolation methods\nExisting VFI methods could be generally classified into\nflow-agnostic and flow-based methods.\nFlow-agnostic approaches model VFI without explicit\nintermediate motion representation. Phase-based methods\n[48, 47] directly predict the phase decomposition of the in-\ntermediate frame, but can only handle motion within a lim-\nited range. Kernel-based methods are the mainstream ap-\nproach in this category, which typically aims to estimate\nintermediate frames by learning adaptive kernels to con-\nvolve input frames [53, 54].\nOver the years, numerous\nimprovements are proposed in this field, including using\ndeformable convolution [11, 12], formulating interpolated\nmotion estimation as classification [56], blending deep fea-\ntures [19], introducing dual-frame adversarial loss [36], per-\nforming channel attention [14] and utilizing 3D space-time\nconvolutions [32]. Recently, Shi et al. [63] introduced a\nTransformer-based framework to model long-range depen-\ndencies with the aid of attention mechanisms. By directly\nhallucinating pixel values, these methods tend to generate\nblurry results and artifacts, especially in fast-moving scenes\n[44].\nFlow-based approaches currently serve as a promising\ndirection in VFI. Generally speaking, flow-based method\ntakes a paradigm of a two-stages pipeline: (1) flow esti-\nmation, and (2) frame synthesis. They first estimate op-\ntical flow between input frames, and then synthesize in-\ntermediate frames using image warping [26]. As a repre-\nsentative work, SuperSlomo [27] by Jiang et al. adopted\na skip-connected U-Net to estimate bi-directional optical\nflows under the assumption of linear motion. Quadratic [77]\nand cubic [13, 69] trajectory assumptions have also been\nmade to approximate intermediate motion. Recent work has\nexplored various techniques to improve intermediate flow\nestimation and interpolation accuracy, including forward-\nwarping via softmax splatting [52, 23], voxel flow [43], cy-\ncle consistency loss[61, 41], task-oriented flow distillation\nloss [35], Gram matrix loss [60], implicit neural function\n[9], occlusion mask [3], anchor points alignment [64], priv-\nileged distillation [25], and pyramid recurrent flow estima-\ntion [28]. Considering additional information like contex-\ntual maps [51], depth maps [2] and auxiliary visual informa-\ntion from event cameras [70, 21, 8] can also further improve\ninterpolation accuracy. Park et al. employed symmetric\nbilateral motion field estimation, and further improved in-\ntermediate motion estimation accuracy through asymmetric\nbilateral motion field [55]. Lu et al. [44] leveraged Trans-\nformer architecture [71] to model long-term dependency.\nJin et al. [29] proposed a novel bi-directional motion es-\ntimator in a pyramid structure. Zhang et al. [82] proposed\na novel feature extraction strategy to combine motion and\n(a)\n(b)\n(c)\nFlow Magnitude\nFlow Magnitude\nFigure 3: The SportsSloMo dataset. (a) Sampled frames, covering various sports categories and challenging human-centric\ncontent for VFI; (b) Histogram of flow magnitude of all pixels in the dataset; (c) Histogram of mean flow magnitude of all\nimages in the dataset.\nHockey\nOthers\nSoccer\nAmerican football\nMartial art\nGymnastics\nTable tennis\nTennis\nBadminton\nVolleyball\nBasketball\nBaseball\nFigure 4: Distribution of different sports categories in\nour SportsSloMo benchmark.\nappearance information via a hybrid CNN and Transformer\narchitecture.\nIn a nutshell, although previous methods have proposed\nsuccessful designs to handle complex motion and occlu-\nsion, none of them is carefully designed for human-centric\nscenes.\nAs discussed in Sec.\n1, human-centric VFI is\nconfronted with various challenges including dynamic pose\nvariation, complex human motion patterns and occlusion in\ncrowded scenes. Additionally, accurately synthesizing fine\ndetails such as facial expressions and hand gestures can be\nchallenging. To this end, we propose to consider human-\naware loss terms through incorporating extra supervision\nto both human keypoints detection and segmentation in the\npanoptic setting.\n3. SportsSloMo Benchmark\nA myriad of benchmark datasets for video frame interpo-\nlation are available, including Middlebury [1], GoPro [50],\nUCF101 [65], DAVIS [57], Adobe240fps [66], Vimeo90K\n[79], SNU-FILM [14] and X4K1000FPS [64]. But none\nof these them focuses on human-centric VFI, e.g., in sports\nscenes. This limits the study of VFI methods targeted for\nhuman-centric applications such as enhanced entertainment\nexperiences, commercial deployment, etc.\nTo bridge this gap and to foster future research, we\npropose SportsSloMo, a challenging dataset consisting\nof high-resolution (\u2265 720p) sports videos crawled from\nYouTube under the Common Creative License.\nCareful\ncuration has been taken to remove the unwanted content\nin the videos, including advertisement, transition frames,\nchanges of shot, flashing lights, and non-slow-motion con-\ntent.\nSpecifically, we first conduct human detection on\nall the videos by utilizing Yolov3 [62] and remove video\nframes without detected humans.\nSecond, we remove\nframes with flashing lights by setting a threshold about\nthe brightness change between consecutive frames (large\nbrightness change indicates the existence of a flashing\nlight). Third, we use RAFT [68] to measure motion magni-\ntude and set a threshold to discard non-slow-motion video\nsegments.\nFinally, we carefully curate the the extracted\nclips manually to keep only high-quality self-consistent\nvideos. With such semi-automatic curation, we end up hav-\ning a set of short slow-motion clips of 9 frames, where each\nframe has a spatial resolution of at least 1280 \u00d7 720. The\nfirst and last frames are used as input for a VFI method and\nthe 7 intermediate frames as ground truths for model train-\ning and evaluation, approximately corresponding to con-\nverting a video of 30-fps (frames per second) to 240-fps.\nIn total, we collect 131,464 video clips and 1.2M indi-\nvidual video frames from 259 raw YouTube videos, which\ncovers 22 various sports categories in our dataset with dif-\nferent content and motion patterns, including hockey, base-\nI0\nI1\nVideo Frame \nInterpolation\n0\ud835\udc3ct\nIt\nLbasic\nPanoptic\nSegmentation\nKeypoint\nDetection\nMt\n3\ud835\udc40t\nKt\n3\ud835\udc3et\nLkpt\nLseg\nPlug-in \nHuman-aware\nLoss Module\nKeypoint\nDetection\nPanoptic\nSegmentation\nFigure 5: Illustration of our proposed human-aware loss terms, consisting of losses about human segmentation in the\npanoptic setting and keypoint detection.\nball, skating, basketball, running, volleyball, etc. Fig. 4\nshows the proportion of different sports categories. Visu-\nalizations of randomly sampled video frames are available\nin Fig. 3. We refer the readers to the supplementary material\nfor more results.\nAs shown in Table 1, compared with existing datasets,\nnot only our dataset is tailored for human-centric scenar-\nios, but also surpasses others in terms of scale, resolution,\nand frame rate.\nFig.\n3 demonstrates the histogram of\nflow magnitude of all pixels and mean flow magnitude of\nall images in our dataset (calculated using GMFlow [76]),\nwith a comparison with the widely-used SNU-FILM [14]\ndataset (which has the same spatial resolution as ours) and\nthe recently introduced X4k1000FPS [64] dataset. As can\nbe seen, our SportsSloMo benchmark contains more large-\ndisplacement motion compared to the SNU-FILM dataset.\nFor instance, for the avarege flow magnitude on the image-\nlevel (Fig. 3(c)), our dataset has way more images, whose\nmean flow magnitude is greater than 20 pixels. And we can\nalso observe that both our SportsSloMo and X4K1000FPS\ndatasets contain large motions.\nWe split our proposed SportsSloMo dataset into train\nand test, containing 115,421 and 16,043 video clips, respec-\ntively. For each sports category, videos are split into train\nand test without intersection, so that the test videos are com-\npletely unseen during training.\n4. Human-aware Video Frame Interpolation\n4.1. Overview\nGiven two input frames I0 and I1, the goal of VFI is to\nsynthesize the intermediate frame It at an intermediate time\nstep t \u2208 (0, 1). A VFI method needs to find the pixel-wise\ncorrespondences between I0 and I1 and adaptively fuse cor-\nresponding pixels to synthesize each pixel in It. Flow-based\napproaches, such as SuperSloMo [27] and EBME [29], usu-\nally explicitly model the correspondences as optical flow,\nwhereas flow-agnostic methods, such as AdaCof [36], use\nlearned kernels to process the visual correspondence implic-\nitly.\nTo supervise the network training, the reconstruction er-\nror of the intermediate frame is used as the loss. For in-\nstance, in [29], the weighted sum of the Charbonnier loss [7]\nLchar and census loss [46] Lcen between the ground truth\nintermediate frame It and synthesized frame \u02c6It are adopted\nLbasic = Lchar(It \u2212 \u02c6It) + \u03bbcen \u00b7 Lcen(It, \u02c6It),\n(1)\nwhere Lchar(x) = (x2 + \u03f52)\u03b1, \u03b1 = 0.5, \u03f5 = 10\u22126.\nSuch reconstruction loss, however, may not provide\nenough supervision for human-centric VFI due to the chal-\nlenges of highly deformable human motion and frequent oc-\nclusion. To address these challenges, we propose to incor-\nporate Lseg and Lkpt into the network supervision based\non human segmentation in the panoptic setting and key-\npoint detection, respectively, enforcing the model to pro-\nduce high-quality synthesis results over human boundaries\nand along the keypoint trajectories. The final loss is\nL = Lbasic + \u03bbsegLseg + \u03bbkptLkpt.\n(2)\nIt is worth noting that our proposed human-aware loss terms\nare flexible, which can be plugged into other flow-based\n(e.g., SuperSloMo [27] and EBME [29]) or flow-agnostic\nVFI models (e.g., AdaCoF [36]), as shown in Section 5.\n4.2. Human Segmentation Loss\nIn human-cenrtic scenarios, accurate estimation of hu-\nman body boundaries is crucial. In regions where the body\nmovement is complex or there are heavy occlusions, inaccu-\nracies in the estimated body boundaries may lead to visible\nartifacts in the synthesized video frame. To this end, we pro-\npose to incorporate human segmentation as extra supervi-\nsion to improve the synthesis of intermediate video frames.\nHuman segmentation masks directly tell where human body\nboundaries are, as shown in Fig. 2, helping to reduce ghost\neffects. Though instance and panoptic segmentation could\nboth serve for this purpose, we empirically find that the\npanoptic segmentation models tend to yield better interpo-\nlation results than instance segmentation counterparts.\nSpecifically, we adopt state-of-the-art Mask2Former\n[10] trained on COCO panoptic dataset [33] with the Swin-\nL backbone [42] from Detectron2 [74] to generate panoptic\nsegmentation masks Mt from the ground-truth intermediate\nframe It. During training, the panoptic segmentation re-\nsults \u02c6\nMt of the synthesized intermediate frame \u02c6It obtained\nusing the same Mask2Former model is compared with Mt.\nIdeally, if the synthesized intermediate frame \u02c6It is identical\nto the ground truth It, \u02c6\nMt should be the same as Mt. By\ncomparing their difference, the human segmentation loss\nenhances the consistency of the human body boundaries be-\ntween It and \u02c6It, which helps reduce the ghost effects. We\nfollow the loss function of Mask2Former and use the binary\ncross-entropy loss Lce and the dice loss Ldice [49] for the\nhuman segmentation loss\nLseg = \u03bbceLce( \u02c6\nMt, Mt) + \u03bbdiceLdice( \u02c6\nMt, Mt),\n(3)\nwhere we set \u03bbce = 5.0 and \u03bbdice = 5.0. The panoptic seg-\nmentation model is frozen during training and the gradient\nof this loss will be backpropagated to a VFI model only in a\nway similar to the perceptual loss [31], enforcing the model\nto produce high-quality synthesis results.\n4.3. Human Keypoint Detection Loss\nIn addition to segmentation, we also consider the hu-\nman keypoints estimation as an extra supervision. Accurate\nestimation of human keypoints is also crucial for human-\ncentric VFI, which are indicative about the position of each\nbody part, providing additional cues for motion estima-\ntion.\nHence, by incorporating the supervision of human\nkeypoints, we can better preserve human motions by enforc-\ning coherent motion trajectories and guide the interpolation\nprocess to generate more plausible intermediate frames.\nSpecifically, we adopt the state-of-the-art human pose\nestimation model ViTPose [78] trained on COCO [40], with\nthe ViT-L backbone [15] initialized with MAE [20] pre-\ntrained weights. We first use an off-the-shelf human detec-\ntor YOLOv8 [30] to detect person instances in the ground-\ntruth intermediate frame It. We then employ ViTPose to\nGround Truth\nEBME\nEBME w/ HL\nFigure 6: VFI results with human-aware loss (HL) terms.\nWe can see better human segmentation, keypoint detection,\nand interpolation result can be obtained enhanced by the\nhuman-aware loss.\nestimate the keypoints heatmap Kt containing the locations\nof each joint. Similar to the human segmentaiton loss, we\nestimate the keypoints heatmap \u02c6Kt using the same ViTPose\nmodel on the synthesized intermediate frame \u02c6It. The dif-\nferences between Kt and \u02c6Kt are then used as the supervi-\nsion to improve VFI results. We follow the loss function\nof ViTPose and use the MSE (mean square error) loss over\nheatmaps as the human keypoint loss Lkpt.\nLkpt = MSE(Kt, \u02c6Kt).\n(4)\nThe ViTPose model is frozen during training and the gra-\ndient of this loss will be backpropagated to a VFI model\nto encourage high-quality intermediate frame synthesis so\nthat the estimated human keypoints are close to the ones\nobtained from the true intermediate frame.\n5. Experiments\n5.1. Setup\nVFI models. We benchmark existing VFI approaches by\nre-training several state-of-the-art methods on our SportsS-\nloMo dataset using their publicly available code, including\nSuperSlomo [27], AdaCoF [36], M2MVFI [23], RIFE [25],\nEBME [29], UPR-Net [28], and EMA-VFI [82]. AdaCoF is\na flow-agnostic VFI model and the rest are flow-based. All\nOverlayed input\nGround Truth\nSuperSlomo\nAdaCoF\nM2MVFI\nRIFE\nEBME\nEBME w/ HL\nOverlayed input\nFigure 7: Qualitative comparisons on SportsSloMo dataset. With the enhancement of the human-aware (HL) losses,\nEBME achieves better interpolation results.\nTable 2: Quantitative results of different approaches on\nthe SportsSloMo dataset. As can be seen, our proposed\nhuman-aware loss (HL) terms consistently improve all VFI\nmodels\u2019 performance.\nMethod\nVenue\nPSNR\u2191\nSSIM\u2191\nIE\u2193\nSuperSlomo [27]\nCVPR 2018\n29.77\n0.910\n9.14\nSuperSloMo + HL\n-\n30.24\n0.917\n8.46\nAdaCoF [36]\nCVPR 2020\n28.79\n0.926\n5.84\nAdaCoF + HL\n-\n28.94\n0.926\n5.72\nM2MVFI [23]\nCVPR 2022\n29.03\n0.935\n5.16\nM2MVFI + HL\n-\n29.29\n0.936\n5.07\nRIFE [25]\nECCV 2022\n29.69\n0.931\n5.25\nRIFE + HL\n-\n29.87\n0.933\n5.16\nEBME [29]\nWACV 2023\n30.15\n0.941\n4.66\nEBME + HL\n-\n30.48\n0.944\n4.40\nUPR-Net [28]\nCVPR 2023\n30.25\n0.945\n4.56\nUPR-Net + HL\n-\n30.50\n0.945\n4.38\nEMA-VFI [82]\nCVPR 2023\n30.70\n0.949\n4.28\nEMA-VFI + HL\n-\n30.75\n0.952\n4.19\nthese models are trained from scratch on our SportsSloMo\ndataset.\nWe also incorporate our proposed human-aware loss\nterms into these models to validate the effectiveness of ex-\nplicitly taking human priors into account to improve human-\ncentric VFI.\nEvaluation metrics.\nFollowing previous VFI methods\n[27, 24], we adopt the widely-used signal-to-noise ratio\n(PSNR), structural similarity (SSIM) [72], and interpolation\nerror (IE) [1] to evaluate the interpolation results. For PSNR\nand SSIM, higher indicates better performance. And for IE,\nthe lower the better.\nImplementation details. For the human segmentation and\nkeypoint loss terms, to avoid the influence of Batch Nor-\nmalization (BN) layers on the training phase, we replaced\nthe BN layers with frozen Batch Normalization (frozen BN)\nlayers. We use the default hyperparameters provided by\neach VFI model.\nTraining and evaluation are both con-\nducted on 8 NVIDIA RTX A6000 GPUs.\nDuring training, we randomly sample a frame between\nthe first and the ninth (last) frame for VFI methods support-\ning arbitrary time frame interpolation like SuperSlomo [27],\nM2MVFI [23], RIFE [25], EBME [29], UPR-Net [28], and\nEMA-VFI [82]. For VFI methods that can only synthesize\nthe middle frame like AdaCoF [36], we randomly gener-\nate triplets within the nine frames such that the target frame\nis always in the middle of the input two frames. During\nevaluation, arbitrary-time-supported VFI methods take the\ntime step as input and interpolate every intermediate frame,\nwhile single-frame methods recursively generate each inter-\nmediate frame.\n5.2. Benchmarking Existing Methods\nTable 2 shows quantitative comparisons between exist-\ning VFI methods on our proposed SportsSloMo dataset. We\ncan see that EMA-VFI [82] performs the best interms of all\nthree evaluation metrics. It is worth noting that the per-\nformance for each model decreases compared with results\non popular datasets. For instance, two top-performing ap-\nproaches on SportsSloMo, EBME [29] and EMA-VFI [82]\nproduce PSNR scores of 30.15 and 30.70, respectively,\ncompared with 36.19 and 36.64 on the Viemo90K [79] as\nwell as 30.64 and 30.94 on the hard split of the SNU-\nFILM [14] benchmarks. It highlights the difficulty of our\nTable 3: Effectiveness of different auxiliary loss terms.\nModel\nsegmentation\nkeypoint\nPSNR\u2191\nSSIM\u2191\nIE\u2193\nloss\nloss\nSuperSloMo\n\u2717\n\u2717\n29.77\n0.910\n9.14\n\u2713\n\u2717\n30.19\n0.916\n8.56\n\u2717\n\u2713\n30.22\n0.916\n8.49\n\u2713\n\u2713\n30.24\n0.917\n8.46\nEBME\n\u2717\n\u2717\n30.15\n0.941\n4.66\n\u2713\n\u2717\n30.26\n0.942\n4.61\n\u2717\n\u2713\n30.34\n0.943\n4.46\n\u2713\n\u2713\n30.48\n0.944\n4.40\nbenchmark and suggests significant challenges need to be\naddressed. We provide more details in the supplementary\nmaterial.\n5.3. Enhanced Human-aware VFI\nTable 2 also demonstrates the effectiveness of our pro-\nposed human-aware losses.\nSpecifically, we incorporate\nthe proposed human segmentation and keypoint losses into\neach of the VFI models. As can be seen, they consistently\nimprove the performance of every single VFI method for\nall three evaluation metrics. Specifically, in terms of PSNR,\nour proposed human-aware losses lead to improvement of\n1.6% and 1.1% for SuperSloMo [27] and EBME [29], re-\nspectively. In terms of IE, the improvement for these two\nmethods are 7.4% and 5.6%, respectively. Qualitatively, we\ncan see from Fig. 6 and Fig. 7, our proposed human-aware\nloss terms can improve interpolation results for the highly\ndeformable arms of the athletes and the background under\nocclusions.\n5.4. Ablation Study\nIn this section, we present ablation study on Super-\nSloMo [27] and EBME [29] to analyze the design choices\nof our proposed human-aware losses in Table 3. As can\nbe seen, both human segmentation and keypoint detection\nlosses can successfully improve the performance of Super-\nSloMo and EBME. This justifies our motivation to leverage\nhuman-aware priors to improve the human-centric VFI. By\ncombining these two losses, the largest performance gain\ncan be obtained for both methods. Furthermore, from Fig. 6,\nwe can clearly see that better interpolation results can be ob-\ntained around the elbow and hand.\n5.5. Limitations and Discussions\nWhile we have introduced human-aware loss terms to\nhandle challenging human-centric scenarios in our pro-\nposed dataset, there still remains noteworthy limitations for\nfuture exploration.\nFirst of all, for human-centric scenes, the presence of\nlarge, complex, and highly deformable human motions and\nocclusion by crowds make finding correspondences of hu-\nmans (i.e., optical flow) between input frames a challenging\ntask. Recent studies [25, 35] have demonstrated the effec-\ntiveness of knowledge distillation [22] to improve the inter-\nmediate optical flow estimation.\nWe also explored this loss to incorporate extra supervi-\nsion for flow-based VFI methods (e.g., SuperSloMo [27],\nEBME [29]). Specifically, we use the optical flow results\nobtained from state-of-the-art model GMFlow [76], which\nis pre-trained on the optical flow benchmarks [16, 45, 5],\nas the teacher network to perform flow distillation.\nWe\nfound, however, mixed results on our SportsSloMo bench-\nmark. While it indeed improves the performance for Su-\nperSlomo [27] (PSNR: 29.77 vs.\n30.18), it hurts the\nperformance for EBME [29] (PSNR: 30.15 vs.\n29.85).\nWe also fine-tune the GMFlow model on the human flow\ndataset [59], consisting of synthetic human figures over-\nlaied on top of real world images as background. How-\never, we observed significant domain gap between the syn-\nthetic training images and real-world test images, which\nlead to poor optical flow estimation results. How to im-\nprove human-centric optical flow to improve VFI is still a\nchallenging problem.\nSecond, we only utilize 2D cues of human bodies in our\nproposed loss terms to enhance VFI. Recently, there has\nbeen significant advance made in 3D human body recon-\nstruction from videos, for instance, [18, 80, 58, 34], con-\nsidering both the spatial and temporal information. By lift-\ning 2D videos into the 3D space, occlusions may be bet-\nter solved. More advanced loss terms involving 3D human\nreconstruction is worth exploring. We leave this as future\nwork to explore.\nFinally, in the SportsSloMo dataset, various sports in-\nvolve fast-moving balls and sports equipment. In this paper,\nwe only consider human boundary and body parts for mo-\ntion estimation. Fast-moving objects, however, in these sce-\nnarios are also challenging for VFI. More effort is needed\nfor such a problem.\n6. Conclusion\nIn this paper, we introduced a new benchmark, SportsS-\nloMo, focusing on human-centric video frame interpola-\ntion. Our benchmark contains 130K video clips and more\nthan 1M video frames obtained from high-resolution (\u2265\n720p) slow-motion sports videos crawled from YouTube\nwith careful curation. Due to the complex, highly defor-\nmation human motion and frequent occlusion, this bench-\nmark imposes significant challenges to existing VFI mod-\nels. To enhance their accuracy, we introduce human-aware\nloss terms to improve existing methods, where the super-\nvision of human segmentation and keypoints detection are\nincorporated. Our loss terms are model agnostic and have\nbeen successfully applied to seven existing VFI methods,\nleading to better accuracy consistently.\nOur benchmark\ndataset and code will be publicly released to foster future re-\nsearch in the new exciting direction of human-centric VFI.\nAcknowledgement\nWe thank Thuy-Tien Bui and Devroop Kar for their help\non data collection. This work was partially supported by the\nNational Science Foundation under Award IIS-2310254.\nReferences\n[1] Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth,\nMichael J Black, and Richard Szeliski. A database and eval-\nuation methodology for optical flow. International journal\nof computer vision, 92(1):1\u201331, 2011. 1, 3, 4, 7\n[2] Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang,\nZhiyong Gao, and Ming-Hsuan Yang. Depth-aware video\nframe interpolation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3703\u20133712, 2019. 3\n[3] Wenbo Bao, Wei-Sheng Lai, Xiaoyun Zhang, Zhiyong Gao,\nand Ming-Hsuan Yang. Memc-net: Motion estimation and\nmotion compensation driven neural network for video inter-\npolation and enhancement.\nIEEE transactions on pattern\nanalysis and machine intelligence, 43(3):933\u2013948, 2019. 3\n[4] Karlis Martins Briedis, Abdelaziz Djelouah, Mark Meyer,\nIan McGonigal, Markus H. Gross, and Christopher Schroers.\nNeural frame interpolation for rendered content. ACM Trans.\nGraph., 40(6):239:1\u2013239:13, 2021. 1\n[5] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A\nnaturalistic open source movie for optical flow evaluation.\nIn ECCV, 2012. 8\n[6] Kelvin C.K. Chan, Shangchen Zhou, Xiangyu Xu, and\nChen Change Loy.\nInvestigating tradeoffs in real-world\nvideo super-resolution. In CVPR, 2022. 2\n[7] Pierre Charbonnier, Laure Blanc-Feraud, Gilles Aubert, and\nMichel Barlaud. Two deterministic half-quadratic regular-\nization algorithms for computed imaging. In Proceedings of\n1st international conference on image processing, volume 2,\npages 168\u2013172. IEEE, 1994. 5\n[8] Jiaben Chen, Yichen Zhu, Dongze Lian, Jiaqi Yang, Yifu\nWang, Renrui Zhang, Xinhang Liu, Shenhan Qian, Laurent\nKneip, and Shenghua Gao.\nRevisiting event-based video\nframe interpolation. arXiv preprint arXiv:2307.12558, 2023.\n3\n[9] Zeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu,\nVidit Goel, Zhangyang Wang, Humphrey Shi, and Xiaolong\nWang. Videoinr: Learning video implicit neural representa-\ntion for continuous space-time super-resolution. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 2047\u20132057, 2022. 3\n[10] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexan-\nder Kirillov, and Rohit Girdhar.\nMasked-attention mask\ntransformer for universal image segmentation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 1290\u20131299, 2022. 2, 6\n[11] Xianhang Cheng and Zhenzhong Chen. Video frame interpo-\nlation via deformable separable convolution. In Proceedings\nof the AAAI Conference on Artificial Intelligence, volume 34,\npages 10607\u201310614, 2020. 3\n[12] Xianhang Cheng and Zhenzhong Chen.\nMultiple video\nframe interpolation via enhanced deformable separable con-\nvolution. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 44(10):7029\u20137045, 2021. 3\n[13] Zhixiang Chi, Rasoul Mohammadi Nasiri, Zheng Liu, Juwei\nLu, Jin Tang, and Konstantinos N Plataniotis.\nAll at\nonce: Temporally adaptive multi-frame interpolation with\nadvanced motion modeling.\nIn European Conference on\nComputer Vision, pages 107\u2013123. Springer, 2020. 3\n[14] Myungsub Choi, Heewon Kim, Bohyung Han, Ning Xu, and\nKyoung Mu Lee. Channel attention is all you need for video\nframe interpolation. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 34, pages 10663\u201310671,\n2020. 1, 2, 3, 4, 5, 7\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 6\n[16] Alexey Dosovitskiy, Philipp Fischery, Eddy Ilg, Caner Hazir-\nbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cre-\nmers, Thomas Brox, et al. FlowNet: Learning optical flow\nwith convolutional networks. In ICCV, 2015. 8\n[17] Chen Gao, Ayush Saraf, Johannes Kopf, and Jia-Bin Huang.\nDynamic view synthesis from dynamic monocular video. In\nICCV, 2021. 2\n[18] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran,\nAngjoo Kanazawa*, and Jitendra Malik*. Humans in 4D:\nReconstructing and tracking humans with transformers. In\nInternational Conference on Computer Vision (ICCV), 2023.\n8\n[19] Shurui Gui, Chaoyue Wang, Qihua Chen, and Dacheng Tao.\nFeatureflow:\nRobust video interpolation via structure-to-\ntexture generation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n14004\u201314013, 2020. 3\n[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll\u00b4ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000\u2013\n16009, 2022. 6\n[21] Weihua He, Kaichao You, Zhendong Qiao, Xu Jia, Ziyang\nZhang, Wenhui Wang, Huchuan Lu, Yaoyuan Wang, and\nJianxing Liao.\nTimereplayer: Unlocking the potential of\nevent cameras for video interpolation.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17804\u201317813, 2022. 3\n[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015. 8\n[23] Ping Hu, Simon Niklaus, Stan Sclaroff, and Kate Saenko.\nMany-to-many splatting for efficient video frame interpola-\ntion. In CVPR, 2022. 2, 3, 6, 7\n[24] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi,\nand Shuchang Zhou.\nRife: Real-time intermediate flow\nestimation for video frame interpolation.\narXiv preprint\narXiv:2011.06294, 2020. 7\n[25] Zhewei Huang, Tianyuan Zhang, Wen Heng, Boxin Shi, and\nShuchang Zhou. Real-time intermediate flow estimation for\nvideo frame interpolation. In Computer Vision\u2013ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23\u2013\n27, 2022, Proceedings, Part XIV, pages 624\u2013642. Springer,\n2022. 2, 3, 6, 7, 8\n[26] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. Advances in neural informa-\ntion processing systems, 28, 2015. 3\n[27] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan\nYang, Erik G. Learned-Miller, and Jan Kautz. Super slomo:\nHigh quality estimation of multiple intermediate frames for\nvideo interpolation. In CVPR, 2018. 1, 2, 3, 5, 6, 7, 8\n[28] Xin Jin, Longhai Wu, Jie Chen, Youxin Chen, Jayoon Koo,\nand Cheul-hee Hahm.\nA unified pyramid recurrent net-\nwork for video frame interpolation. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 2023. 2, 3, 6, 7\n[29] Xin Jin, Longhai Wu, Guotao Shen, Youxin Chen, Jie Chen,\nJayoon Koo, and Cheul-hee Hahm. Enhanced bi-directional\nmotion estimation for video frame interpolation. In Proceed-\nings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 5049\u20135057, 2023. 2, 3, 5, 6, 7, 8\n[30] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. YOLO by\nUltralytics, 1 2023. 6\n[31] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual\nlosses for real-time style transfer and super-resolution. In\nECCV, 2016. 6\n[32] Tarun Kalluri, Deepak Pathak, Manmohan Chandraker, and\nDu Tran. Flavr: Flow-agnostic video representations for fast\nframe interpolation. In Proceedings of the IEEE/CVF Win-\nter Conference on Applications of Computer Vision, pages\n2071\u20132082, 2023. 3\n[33] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll\u00b4ar. Panoptic segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 9404\u20139413, 2019. 6\n[34] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges,\nand Michael J. Black. PARE: Part attention regressor for 3D\nhuman body estimation. In ICCV, 2021. 8\n[35] Lingtong Kong, Boyuan Jiang, Donghao Luo, Wenqing Chu,\nXiaoming Huang, Ying Tai, Chengjie Wang, and Jie Yang.\nIfrnet: Intermediate feature refine network for efficient frame\ninterpolation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 1969\u2013\n1978, 2022. 3, 8\n[36] Hyeongmin Lee, Taeoh Kim, Tae-young Chung, Daehyun\nPak, Yuseok Ban, and Sangyoun Lee. Adacof: Adaptive col-\nlaboration of flows for video frame interpolation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5316\u20135325, 2020. 2, 3, 5, 6,\n7\n[37] Siyao Li, Shiyu Zhao, Weijiang Yu, Wenxiu Sun, Dimitris N.\nMetaxas, Chen Change Loy, and Ziwei Liu. Deep animation\nvideo interpolation in the wild. In CVPR, 2021. 1\n[38] Yinxiao Li, Pengchong Jin, Feng Yang, Ce Liu, Ming-\nHsuan Yang, and Peyman Milanfar. Comisr: Compression-\ninformed video super-resolution. In ICCV, 2021. 2\n[39] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of dy-\nnamic scenes. In CVPR, 2021. 1, 2\n[40] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 6\n[41] Yu-Lun Liu, Yi-Tung Liao, Yen-Yu Lin, and Yung-Yu\nChuang. Deep video frame interpolation using cyclic frame\ngeneration. In Proceedings of the AAAI Conference on Arti-\nficial Intelligence, volume 33, pages 8794\u20138802, 2019. 3\n[42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021. 6\n[43] Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and\nAseem Agarwala. Video frame synthesis using deep voxel\nflow. In Proceedings of the IEEE International Conference\non Computer Vision, pages 4463\u20134471, 2017. 3\n[44] Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya\nJia. Video frame interpolation with transformer. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 3532\u20133542, 2022. 3\n[45] Nikolaus Mayer, Eddy Ilg, Philip H\u00a8ausser, Philipp Fischer,\nDaniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A\nlarge dataset to train convolutional networks for disparity,\noptical flow, and scene flow estimation. In CVPR, 2016. 8\n[46] Simon Meister, Junhwa Hur, and Stefan Roth. Unflow: Un-\nsupervised learning of optical flow with a bidirectional cen-\nsus loss. In Proceedings of the AAAI conference on artificial\nintelligence, volume 32, 2018. 5\n[47] Simone Meyer, Abdelaziz Djelouah, Brian McWilliams,\nAlexander Sorkine-Hornung, Markus Gross, and Christo-\npher Schroers. Phasenet for video frame interpolation. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 498\u2013507, 2018. 3\n[48] Simone Meyer, Oliver Wang, Henning Zimmer, Max Grosse,\nand Alexander Sorkine-Hornung. Phase-based frame inter-\npolation for video. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1410\u20131418,\n2015. 3\n[49] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In 2016 fourth international\nconference on 3D vision (3DV), pages 565\u2013571. Ieee, 2016.\n6\n[50] Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep\nmulti-scale convolutional neural network for dynamic scene\ndeblurring.\nIn Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3883\u20133891,\n2017. 4\n[51] Simon Niklaus and Feng Liu. Context-aware synthesis for\nvideo frame interpolation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n1701\u20131710, 2018. 3\n[52] Simon Niklaus and Feng Liu. Softmax splatting for video\nframe interpolation. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n5437\u20135446, 2020. 3\n[53] Simon Niklaus, Long Mai, and Feng Liu. Video frame in-\nterpolation via adaptive convolution. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, pages 670\u2013679, 2017. 3\n[54] Simon Niklaus, Long Mai, and Feng Liu. Video frame in-\nterpolation via adaptive separable convolution. In Proceed-\nings of the IEEE international conference on computer vi-\nsion, pages 261\u2013270, 2017. 3\n[55] Junheum Park, Chul Lee, and Chang-Su Kim. Asymmetric\nbilateral motion estimation for video frame interpolation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 14539\u201314548, 2021. 3\n[56] Tomer Peleg, Pablo Szekely, Doron Sabo, and Omry Sendik.\nIm-net for high resolution video frame interpolation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2398\u20132407, 2019. 3\n[57] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc\nVan Gool, Markus Gross, and Alexander Sorkine-Hornung.\nA benchmark dataset and evaluation methodology for video\nobject segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 724\u2013732,\n2016. 1, 4\n[58] Jathushan\nRajasegaran,\nGeorgios\nPavlakos,\nAngjoo\nKanazawa, Christoph Feichtenhofer, and Jitendra Malik.\nOn the benefits of 3d pose and tracking for human action\nrecognition. In CVPR, 2023. 8\n[59] Anurag Ranjan, David T. Hoffmann, Dimitrios Tzionas, Siyu\nTang, Javier Romero, and Michael J. Black. Learning multi-\nhuman optical flow. Int. J. Comput. Vis., 128(4):873\u2013890,\n2020. 8\n[60] Fitsum Reda, Janne Kontkanen, Eric Tabellion, Deqing Sun,\nCaroline Pantofaru, and Brian Curless. Film: Frame inter-\npolation for large motion. In Computer Vision\u2013ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October 23\u201327,\n2022, Proceedings, Part VII, pages 250\u2013266. Springer, 2022.\n3\n[61] Fitsum A Reda, Deqing Sun, Aysegul Dundar, Mohammad\nShoeybi, Guilin Liu, Kevin J Shih, Andrew Tao, Jan Kautz,\nand Bryan Catanzaro. Unsupervised video interpolation us-\ning cycle consistency. In Proceedings of the IEEE/CVF in-\nternational conference on computer Vision, pages 892\u2013900,\n2019. 3\n[62] Joseph Redmon and Ali Farhadi. Yolov3: An incremental\nimprovement. arXiv preprint arXiv:1804.02767, 2018. 4\n[63] Zhihao Shi, Xiangyu Xu, Xiaohong Liu, Jun Chen, and\nMing-Hsuan Yang. Video frame interpolation transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 17482\u201317491, 2022.\n3\n[64] Hyeonjun Sim, Jihyong Oh, and Munchurl Kim.\nXvfi:\nExtreme video frame interpolation. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 14489\u201314498, 2021. 1, 2, 3, 4, 5\n[65] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah.\nUcf101: A dataset of 101 human actions classes from videos\nin the wild. arXiv preprint arXiv:1212.0402, 2012. 1, 2, 3, 4\n[66] Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo\nSapiro, Wolfgang Heidrich, and Oliver Wang. Deep video\ndeblurring for hand-held cameras.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 1279\u20131288, 2017. 1, 2, 3, 4\n[67] Masato Tamura, Rahul Vishwakarma, and Ravigopal Ven-\nnelakanti. Hunting group clues with transformers for social\ngroup activity recognition. In ECCV, 2022. 2\n[68] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow.\nIn Computer Vision\u2013ECCV\n2020: 16th European Conference, Glasgow, UK, August 23\u2013\n28, 2020, Proceedings, Part II 16, pages 402\u2013419. Springer,\n2020. 4\n[69] Stepan Tulyakov, Alfredo Bochicchio, Daniel Gehrig, Sta-\nmatios Georgoulis, Yuanyou Li, and Davide Scaramuzza.\nTime lens++: Event-based frame interpolation with paramet-\nric non-linear flow and multi-scale fusion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 17755\u201317764, 2022. 3\n[70] Stepan Tulyakov, Daniel Gehrig, Stamatios Georgoulis,\nJulius Erbach, Mathias Gehrig, Yuanyou Li, and Davide\nScaramuzza. Time lens: Event-based video frame interpola-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 16155\u201316164,\n2021. 3\n[71] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 3\n[72] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600\u2013612, 2004. 7\n[73] Chao-Yuan Wu, Nayan Singhal, and Philipp Kr\u00a8ahenb\u00a8uhl.\nVideo compression through image interpolation. In ECCV,\n2018. 1\n[74] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen\nLo, and Ross Girshick. Detectron2. https://github.\ncom/facebookresearch/detectron2, 2019. 6\n[75] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil\nKim. Space-time neural irradiance fields for free-viewpoint\nvideo. In CVPR, 2021. 2\n[76] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and\nDacheng Tao.\nGmflow: Learning optical flow via global\nmatching. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 8121\u20138130,\n2022. 5, 8\n[77] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-\nHsuan Yang. Quadratic video interpolation. Advances in\nNeural Information Processing Systems, 32, 2019. 3\n[78] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vit-\npose: Simple vision transformer baselines for human pose\nestimation. arXiv preprint arXiv:2204.12484, 2022. 2, 6\n[79] Tianfan Xue, Baian Chen, Jiajun Wu, Donglai Wei, and\nWilliam T Freeman.\nVideo enhancement with task-\noriented flow.\nInternational Journal of Computer Vision,\n127(8):1106\u20131125, 2019. 1, 2, 3, 4, 7\n[80] Vickie Ye, Georgios Pavlakos, Jitendra Malik, and Angjoo\nKanazawa.\nDecoupling human and camera motion from\nvideos in the wild. In IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), June 2023. 8\n[81] Hangjie Yuan, Dong Ni, and Mang Wang. Spatio-temporal\ndynamic inference network for group activity recognition. In\nICCV, 2021. 2\n[82] Guozhen Zhang, Yuhan Zhu, Haonan Wang, Youxin Chen,\nGangshan Wu, and Limin Wang. Extracting motion and ap-\npearance via inter-frame attention for efficient video frame\ninterpolation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5682\u2013\n5692, 2023. 2, 3, 6, 7\n[83] Honglu Zhou, Asim Kadav, Aviv Shamsian, Shijie Geng,\nFarley Lai, Long Zhao, Ting Liu, Mubbasir Kapadia, and\nHans Peter Graf.\nComposer: Compositional reasoning of\ngroup activity in videos with keypoint-only modality.\nIn\nECCV, 2022. 2\n"
  }
]