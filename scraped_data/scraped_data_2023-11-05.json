[
  {
    "title": "FlashDecoding++: Faster Large Language Model Inference on GPUs",
    "link": "https://arxiv.org/pdf/2311.01282.pdf",
    "upvote": "30",
    "text": "FLASHDECODING++: FASTER LARGE LANGUAGE MODEL\nINFERENCE ON GPUS\nKe Hong\u2020\nTsinghua University\n& Infinigence-AI\nQiuli Mao\nTsinghua University\n& Infinigence-AI\nKangdi Chen\nInfinigence-AI\nGuohao Dai\u2020\f\nShanghai Jiao Tong University\n& Infinigence-AI\nXiuhong Li\nPeking University\nYuhan Dong\nTsinghua University\nJiaming Xu\u2020\nShanghai Jiao Tong University\n& Infinigence-AI\nJun Liu\nShanghai Jiao Tong University\n& Infinigence-AI\nYu Wang\f\nTsinghua University\n\fdaiguohao@sjtu.edu.cn, daiguohao@infini-ai.com, yu-wang@tsinghua.edu.cn\nABSTRACT\nAs the Large Language Model (LLM) becomes increasingly important in various domains, the\nperformance of LLM inference is crucial to massive LLM applications. However, the following\nchallenges still remain unsolved in accelerating LLM inference: (1) Synchronized partial softmax\nupdate. The softmax operation requires a synchronized update operation among each partial softmax\nresult, leading to \u223c20% overheads for the attention computation in LLMs. (2) Under-utilized\ncomputation of flat GEMM. The shape of matrices performing GEMM in LLM inference is flat,\nleading to under-utilized computation and >50% performance loss after padding zeros in previous\ndesigns (e.g., cuBLAS, CUTLASS, etc.). (3) Performance loss due to static dataflow. Kernel\nperformance in LLM depends on varied input data features, hardware configurations, etc. A single\nand static dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in LLM\ninference.\nWe present FlashDecoding++, a fast LLM inference engine supporting mainstream LLMs and\nhardware back-ends. To tackle the above challenges, FlashDecoding++ creatively proposes: (1)\nAsynchronized softmax with unified max value. FlashDecoding++ introduces a unified max\nvalue technique for different partial softmax computations to avoid synchronization. Based on\nthis, the fine-grained pipelining is proposed.(2) Flat GEMM optimization with double buffering.\nFlashDecoding++ points out that flat GEMMs with different shapes face varied bottlenecks. Then,\ntechniques like double buffering are introduced. (3) Heuristic dataflow with hardware resource\nadaptation. FlashDecoding++ heuristically optimizes dataflow using different hardware resource\n(e.g., Tensor Core or CUDA core) considering input dynamics.Due to the versatility of optimizations\nin FlashDecoding++, FlashDecoding++ can achieve up to 4.86\u00d7 and 3.93\u00d7 speedup on both\nNVIDIA and AMD GPUs compared to Hugging Face implementations. FlashDecoding++ also\nachieves an average speedup of 1.37\u00d7 compared to state-of-the-art LLM inference engines on\nmainstream LLMs.\n\u2020These authors contributed equally to this work.\n\u2021Prof. Guohao Dai is the Chief Scientist at Infinigence-AI, Ke Hong, Jiaming Xu, Qiuli Mao, and Jun Liu are interns at\nInfinigence-AI.\n\fProf. Guohao Dai and Prof. Yu Wang are the corresponding authors of this paper.\narXiv:2311.01282v4  [cs.LG]  5 Jan 2024\nFlashDecoding++: Faster Large Language Model Inference on GPUs\n1\nIntroduction\nAs the Large Language Model (LLM) achieved unprecedented success in various domains [2, 3, 4, 5], the LLM\ninference workload is skyrocketing. For example, OpenAI reports that GPT-4 inference with 8K context length\ncosts $0.03 per 1K input tokens and $0.06 per 1K output tokens [6]. Currently, OpenAI has 180.5 million users and\nreceives over 10 million queries per day [7]. Consequently, the cost to operate OpenAI\u2019s model like ChatGPT is\napproximately $7 million per day for the necessary computing hardware [8]. Thus, optimizations on LLM inference\nperformance will have a huge impact considering massive LLM inference scenarios. Many recent works have proposed\ntechniques to accelerate LLM inference tasks, including DeepSpeed [9], FlexGen [10], vLLM [11], OpenPPL [12],\nFlashDecoding [13], TensorRT-LLM [14], and etc [15, 16, 17, 12].\nThe LLM inference task generates tokens (e.g., words) from the input sequence autoregressively, and can be organized\ninto two typical phases: the prefill phase and the decode phase. The prefill phase generates the first token by processing\nthe input prompt, and previous research (e.g., FlashAttention [18, 19]) optimizes latency for this phase. The decode\nphase generates the following tokens sequentially, and many works [9, 10, 11, 15, 13, 14, 20] focus on improving\nthe throughput of generating tokens (i.e., reducing latency of each token). The prefill phase dominates total time for\nscenarios of long-sequence input or generating short outputs [21, 22], while the decode phase constitutes a significant\nportion of the time when processing long output sequences [23].\nFigure 2 shows the main dataflow of the LLM inference with one transformer layer for both the prefill phase and the\ndecode phase. A transformer layer can be divided into linear GEMM (General Matrix Multiplication) operations (e.g.,\nK, Q, V, O weight projection and the feedforward) and the attention/softmax computation. For the attention computation,\na softmax operation is adopted for a row in the attention matrix. To improve the parallelism, previous designs [18, 13]\ndivide the attention matrices into smaller tiles and rows are also split to compute partial softmax results. A synchronized\nsoftmax operation is adopted to update previous partial softmax results when a new partial softmax result is calculated.\nSuch a synchronized partial softmax update accounts for 18.8% for the attention computation of Llama2-7B inference\naccording to our profiling on NVIDIA Tesla A100 GPU with 1024 input length, resulting in the first challenge for\naccelerating LLM inference. Secondly, the computation resources is under-utilized for the flat GEMM operation\nduring the decode phase. Because the decode phase sequentially generates tokens, the linear GEMM operation tends to\nbe flat-shape (even turning into the GEMV (General Matrix-Vector Multiplication) operation when the batch size is 1).\nFor the small batch size (e.g., 8), previous designs [24, 25] pad the matrix with zeros to perform GEMMs of larger sizes\n(e.g., 64), leading to over 50% computation under-utilization. Thirdly, the performance of LLM inference suffers\nfrom the static dataflow considering input dynamics and hardware configuration. For example, the small batch size\nmakes the decode phase of LLM inference memory-bounded and the large batch size makes it compute-bounded. A\nsingle and static dataflow may lead to 50.25% performance loss for GEMMs of different shapes in LLM inference.\n5\n10\n15\n20\n25\n30\n70\n90\n110\n130\n30\n40\n50\n60\n70\n80\n3200\n3800\n4400\n5000\nSOTA\nw/ FlashDecoding++\n38\n92\n83\n107\nNVIDIA Tesla A100\nAMD MI210\nLLM inference throughput\nToken/s\nfirst token\nlatency/ms\nfirst token\nlatency/ms\neach token\nlatency/ms\neach token\nlatency/ms\ninput length = 1K\ninput length = 32K\nHugging Face/PyTorch\nFlashDecoding\nvllm\nDeepSpeed\nOpenPPL\nFlashDecoding++ (ours)\n\u00d7+\nfaster\nfaster\nFigure 1: Overview of comparison between FlashDecoding++ and state-of-the-art designs. The results in the figure\nare reported with Llama2-7B model [1]. The left is with batch size=1 and input length=1K, and TensorRT-LLM and\nHugging Face are the SOTA baseline for NVIDIA/AMD according to our experimental results. The right shows the\ncomprehensive comparison of both first token latency and each token latency.\n2\nFlashDecoding++: Faster Large Language Model Inference on GPUs\n\u00d7WK\n\u00d7WQ\n\u00d7WV\n\u00d7\nsoftmax\n\u00d7\n\u00d7WK\n\u00d7WQ\n\u00d7WV\n\u00d7\nsoftmax\n\u00d7\n\u00d7WO\nFFN1\nFFN2\nQ, K, V projection\nGEMM\nGEMV/Flat GEMM\nQ \u00d7 K\nGEMM\nGEMV/Flat GEMM\nsoftmax\nsoftmax\nsoftmax\nAttention \u00d7 V\nGEMM\nGEMV/Flat GEMM\nO projection\nGEMM\nGEMV/Flat GEMM\n\u00d7WO\nFFN1\nFFN2\nWhat\nis\nthe\nlargest\nocean\n?\nPacific\nPacific\nOcean\nPrefill phase\nDecode phase\nOperation\nFeedforward\nGEMM\nGEMV/Flat GEMM\n\u2460\n\u2461\n\u2462\n\u2463\n\u2460\n\u2461\n\u2462\n\u2463\n\u2464\n\u2465\n\u2465\n\u2460\n\u2461\n\u2462\n\u2463\n\u2464\n\u2465\npartial attention\n(e.g., FlashAttention)\npartial attention\n(e.g., FlashDecoding)\nautogressively\n\u2464\n\u2465\n\u2465\nP\nK\nQ\nV\nKcache\nVcache\nAttention\nFigure 2: Overview of Large Language Model inference dataflow. We show the dataflow comparison between the prefill\nphase and the decode phase. The prefill phase mainly involves the GEMM operation, while the decode phase mainly\ninvolves the GEMV/Flat GEMM operation.\nTo tackle these challenges and enable a faster Large Language Model (LLM) inference, we present FlashDecoding++\nin this paper. FlashDecoding++ creatively proposes the following contributions:\n\u2022 Asynchronized softmax with unified max value. FlashDecoding++ leverages a unified max value for\ndifferent partial softmax computations. Each partial softmax result can be processed individually without\nsynchronized update.\n\u2022 Flat GEMM optimization with double buffering. FlashDecoding++ only pads the matrix size to 8 rather\nthan 64 in previous designs for flat-shaped GEMM to improve computation utilization. We point out that\nflat GEMMs with different shapes face varied bottlenecks, and further improve the kernel performance with\ntechniques like double buffering.\n\u2022 Heuristic dataflow with hardware resource adaption. FlashDecoding++ takes both input dynamics and\nhardware configurations into consideration and dynamically applies kernel optimization for the LLM inference\ndataflow.\nBecause of the versatility of optimizations, the effectiveness of FlashDecoding++ can be proved on both NVIDIA and\nAMD GPUs. FlashDecoding++ achieves up to 4.86\u00d7 and 3.93\u00d7 speedup on both NVIDIA and AMD GPUs compared\nwith Hugging Face implementations, respectively. Our extensive results show that FlashDecoding++ achieves an\naverage of 1.37\u00d7 speedup compared with FlashDecoding [13], a state-of-the-art LLM inference engine on various\nLLMs (e.g., Llama2, ChatGLM2, etc.).\nThe rest of this paper is organized as follows. Section 2 introduces preliminaries of LLMs and related works on\nLLM inference acceleration. Our three techniques, the asynchronized softmax with unified max value, the flat\nGEMM optimization with double buffering, and the heuristic dataflow with hardware resource adaption are detailed\nin Section 3, 4, and 5, respectively. Section 6 presents the evaluation results. Related works on LLM inference are\nintroduced in Section 7, and Section 8 concludes the paper.\n3\nFlashDecoding++: Faster Large Language Model Inference on GPUs\nSynchronized partial softmax update\nAsynchronized softmax with unified max value\nmul1\nmax\nexp\nsum\nmul2\nAttention \nN-1\nAttention \nN+1\nmul1\nexp\nmul2\nAttention \nN-1\nAttention \nN+1\nsum\nunified max value\nsynchronized update\n(a)\npartial softmax\nasynchronized\nmul1 & mul2 refer to \noperation \u2461&\u2463 in (a)\nUnder-utilized computation of flat GEMM\nFlat GEMM optimization with double buffering\n(b)\nA\nB\n\u00d7\npadding zeros\nA\nzero\nB\n\u00d7\nflat-shape\nGEMM\nor\nload A\ndirectly computing\nA\u00d7B\nload A\u2019\nload A\nA\u00d7B\nload A\u2019\nA\u2019\u00d7B\nload A\u2019\u2019\nA\u2019\u2019\u00d7B\nload A\u2019\u2019\u2019\nA\u2019\u2019\u2019\u00d7B\ncomputation under-utilization\ndouble\nbuffering\nPerformance loss to static dataflow\nHeuristic dataflow with hardware resource adaption\n(c)\nGEMM\nFlat GEMM\nGEMV\nGEMM\u221a Flat GEMM\u00d7 GEMV\u00d7\nGEMM\u00d7 Flat GEMM\u221a GEMV\u221a\nGEMM\nFlat GEMM\nGEMV\nGEMM\u221a Flat GEMM\u221a GEMV\u221a\nstatic dataflow 1\nstatic dataflow 2\nheuristic \ndataflow\nSection 3\nSection 4\nSection 5\nFigure 3: FlashDecoding++ proposes three solutions for corresponding challenges in Large Language Model inference.\n(a) FlashDecoding++ proposes the asynchronized softmax with unified max value technique, avoiding synchronized\nupdate to previous partial attention results. (b) FlashDecoding++ optimizes flat GEMM by improving computation\nutilization. (c) FlashDecoding++ heuristically optimizes dataflow.\n2\nBackground\n2.1\nLLM Inference Dataflow Overview\nThe task of LLM inference is to generate tokens from the input sequence, which can be used to complete a sentence or\nanswer a question. An overview of the LLM inference dataflow is shown in Figure 2. As we can see, the LLM inference\ndataflow can be organized into two typical phases with similar operations: one prefill phase and several decode phases.\nThe prefill phase \u201cunderstands\" the input sequence (i.e., \u201cWhat is the largest ocean?\u201d). Each token (we set one word as\na token in Figure 2 is encoded as an embedding vector, and the input sequence is organized into a matrix. The main\noutput of the prefill phase is a new token, which is predicted to be the next token after the input sequence (i.e., \u201cPacific\"\nin this figure). The decode phase \u201cgenerates\" the output sequence (i.e., \u201cPacific\u201d, \u201cOcean\", etc.) The output token of the\nprefill phase is taken as the input of the decode phase. The decode phase is executed autogressively, and each output\ntoken is used as the input token for the next The decode (e.g., \u201cOcean\" is further used as the input).\n2.2\nOperations in LLM Inference\nThe main operations in LLM inference are depicted as operation \u2460 to \u2465 in Figure 2, including the linear projection\n(\u2460 and \u2464), the attention (\u2461, \u2462, and \u2463), and the feedforward network (\u2465). For simplicity, operations like position\nembedding [26], non-linear activation [27, 28, 29], mask [26], and others are not shown in the figure. Operations in the\nprefill phase and the decode phase are different in the shape of data. Because only one token (batch size=1) or few\ntokens (batch size>1) are processed at one time, input matrices in the decode phase are flat-shape matrices or even\nvectors.\nLinear Projection. The linear projection performs as the fully connected layer, multiplying the input with weight\nmatrices (i.e., WK, WQ, WV , WO, called K, Q, V projection and O projection). For the prefill phase, the K, Q, V\nprojection generates matrices K, Q, V . For the decode phase, the K, Q, V projection generates three corresponding\nvectors and concatenated with K and V (i.e., KVcache, yellow and light blue in Figure 2 in the prefill phase.\nsoftmax(Q \u00d7 KT ) \u00d7 V\n(1)\n4\nFlashDecoding++: Faster Large Language Model Inference on GPUs\nx1\nx2\nx3\nx4\nxd-1\nxd\n\u2026\n\ud835\udc5a \ud835\udc65 = \ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65!\n\ud835\udc53 \ud835\udc65 = \ud835\udc52\"'#$ \" , \ud835\udc52\"(#$ \" , \u2026 , \ud835\udc52\")#$ \"\n\ud835\udc59 \ud835\udc65 = +\n!\n\ud835\udc53! \ud835\udc65\n\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65 = \ud835\udc53 \ud835\udc65\n\ud835\udc59 \ud835\udc65\nCalcutate \ud835\udc5a \ud835\udc65\u2032 , \ud835\udc53 \ud835\udc65\u2032 , \ud835\udc59 \ud835\udc65\u2032 , \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\u2032\nCalcutate \ud835\udc5a \ud835\udc65\u2032\u2032 , \ud835\udc53 \ud835\udc65\u2032\u2032 , \ud835\udc59 \ud835\udc65\u2032\u2032 , \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\u2032\u2032\nUpdate \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\u2032 with \ud835\udc5a \ud835\udc65\u2032 , \ud835\udc53 \ud835\udc65\u2032 , \ud835\udc59 \ud835\udc65\u2032 , \ud835\udc5a \ud835\udc65\u2032\u2032 , \ud835\udc53 \ud835\udc65\u2032\u2032 , \ud835\udc59 \ud835\udc65\u2032\u2032\nUpdate \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\u2032\u2032 with \ud835\udc5a \ud835\udc65\u2032 , \ud835\udc53 \ud835\udc65\u2032 , \ud835\udc59 \ud835\udc65\u2032 , \ud835\udc5a \ud835\udc65\u2032\u2032 , \ud835\udc53 \ud835\udc65\u2032\u2032 , \ud835\udc59 \ud835\udc65\u2032\u2032\nProcess next partial vector\nx1\nx2\nx3\nx4\nxd-1\nxd\n\u2026\nx1\nx2\nx3\nx4\nxd-1\nxd\n\u2026\n\ud835\udc5a \ud835\udc65\u2032 = \ud835\udc5a \ud835\udc65\u2032\u2032 = a unified max value \ud835\udf19\nCalcutate \ud835\udc53 \ud835\udc65\u2032 , \ud835\udc59 \ud835\udc65\u2032\nCalcutate \ud835\udc53 \ud835\udc65\u2032\u2032 , \ud835\udc59 \ud835\udc65%\u2032\nCalcutate \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\n(a) Original softmax\n(b) Partial softmax\n(c) Partial softmax with unified max value\nHigh parallelism \u00d7\nLow memory \u00d7\nSynchronization-free \u221a\n\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\n\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\u2032\n\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\u2032\u2032\n\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\u2032\n\ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65 \ud835\udc65\u2032\u2032\nHigh parallelism \u221a\nLow memory \u221a\nSynchronization-free \u00d7\nHigh parallelism \u221a\nLow memory \u221a\nSynchronization-free \u221a\n\u2026\u2026\nFigure 4: Comparison of different softmax computation schemes. (a) Softmax computation for the whole vector. (b)\nComputing partial softmax for each partial vector, and a synchronized update operation is required for all partial softmax\nresults. (c) Computing partial softmax using a unified max value, and each partial vector is processed individually\nwithout synchronized update.\nAttention. The attention operation is mainly divided into three operations (\u2461 to \u2463 Q \u00d7 K, softmax, Attention \u00d7 V ),\nas shown in Eq. (1). For P = Q \u00d7 KT , the softmax operation is performed for each row of the result matrix of P. The\ndetailed softmax computation is shown in Figure 4(a). The maximum value m(x) is first calculated. The exponent\nof each element divided by em(x), f(x), is then processed. These exponents are normalized to the summation of all\nexponents (i.e., l(x)) to get the softmax result.\nFeedforward Network. The feedforward network primarily comprises two fully connected layers. The first one (\u2465\nFFN1) expands the feature dimensions to enhance the representational capacity. The second one (\u2465 FFN2) restores\nthe feature dimensions and serves as the output layer.\n2.3\nAttention Optimization\nThe softmax operation shown in Figure 4(a) requires all global data to be calculated and stored before it can proceed.\nThis results in high memory consumption and low parallelism. Latter works propose the partial softmax technique to\nreduce memory consumption [18, 19] or improve parallelism [13]. Figure 4(b) shows the diagram of the partial softmax\noperation. The main idea is to divide the vector x into partial vectors (i.e, x\u2032 and x\u2032\u2032). The partial softmax results of x\u2032\nand x\u2032\u2032 are calculated separately according to Figure 4(a), and then synchronously updated by each other. The detailed\ncomputation of this synchronized update is shown in Equation (2). With the implementation of partial softmax, we can\nachieve efficient parallelism of computation while reducing memory cost for attention computation.\nm(x) = max(m(x\u2032), m(x\u2032\u2032))\nf(x\u2032) = em(x\u2032)\u2212m(x)f(x\u2032)\nf(x\u2032\u2032) = em(x\u2032\u2032)\u2212m(x)f(x\u2032\u2032)\nl(x) = f(x\u2032) + f(x\u2032\u2032)\nsoftmax([x\u2032, x\u2032\u2032]) = [f(x\u2032), f(x\u2032\u2032)] \u00f7 l(x)\n(2)\nHowever, since the partial softmax needs to be updated according to other partial softmax results, it unavoidably\nintroduces data synchronization operations. According to our profiling result, such a synchronized update operation\nleads to 18.8% overheads in the attention computation for Llama2-7B inference on NVIDIA Tesla A100 GPU with\n1024 input length.\n3\nAsynchronized Softmax with Unified Maximum Value\nMotivation. The partial softmax operation requires synchronization among different partial vectors, leading to \u223c20%\noverheads of the attention operation. As is shown in Figure 3(a), the synchronization is required after the maximum\nvalue of the partial vector is calculated. The maximum value is used to update previous partial softmax (i.e., recompute\nprevious attention) results. Thus, to reduce synchronization overheads, the key problem to be solved is how to\ncompute each partial softmax result without requiring results from other partial softmax computation.\n5\nFlashDecoding++: Faster Large Language Model Inference on GPUs\n0\n10\n40\n-70\n-20\n-10\nLlama2-7B\n[-16.8]\n[6.5]\n5\n10\n15\n-10\n-5\n0\nChatGLM2-6B\n[-10.5]\n[13.7]\nOPT-6.7B\n-200\n0\n200\n-1400\n-400\n[363.5]\n[-496.8]\n99.99%\n400\n99.99%\n99.99%\nFigure 5: The statistical distribution of xi (elements in the input vectors of softmax) in typical LLMs with different\ninputs.\nChallenge. The reason that synchronization is required lies in that the maximum value of each partial vector is different.\nThe maximum value is used to avoid overflow of the exponent operation (f(x) in Figure 4(a)), and exponents are\nsummed (l(x) in Figure 4(a)) as the denominator of the softmax operation. Such a non-linear operation on each partial\nmaximum value makes the synchronization among each partial softmax computation unavoidable.\nAnalysis and Insights. According to the formula of softmax computation, the maximum value is used as the scaling\nfactor for both the numerator and the denominator (i.e., f(x) and l(x) in Figure 4(a)). Our key insight is, the scaling\nfactor can be an arbitrary number rather than using the maximum value mathematically, shown in Equation (3).\nWhen we set \u03d5 = 0, it becomes the original softmax computation [30].\nsoftmax(x) = [ex1\u2212m(x), ..., exd\u2212m(x)]\nP\ni exi\u2212m(x)\n= [ex1\u2212\u03d5, ..., exd\u2212\u03d5]\nP\ni exi\u2212\u03d5\n, \u2200\u03d5 \u2208 R\n(3)\nHowever, the scaling factor cannot be an arbitrary number considering the overflowing of the exponent computation.\nFor the case where xi \u226b \u03d5, exi\u2212\u03d5 overflows and cannot be represented using a fix-width floating point number (e.g.,\nfloat32 for exponent results in current LLM engines). For another case where xi \u226a \u03d5, exi\u2212\u03d5 \u2192 0, leading to\nprecision loss. Thus, a proper scaling factor \u03d5 should be carefully selected to avoid the two cases above. Figure 5 shows\nthe statistical distribution of xi (elements in the input vectors of softmax) in typical LLMs with different inputs [31].\nOur key insight is, > 99.99% xi are within a certain range. Specifically, for Llama2-7B, we have \u221216.8 < xi < 6.5\nfor > 99.99% xi. Because eb\u2212a and ea\u2212b can be represented by a float32 format, we can set \u03d5 = a in Equation (3).\nFor OPT-6.7B, we do not apply the technique in this section because of the large range in Figure 5.\nApproach: Asynchronization. Based on the insights above, each partial softmax computation shares a unified\nmaximum value, \u03d5. After the softmax operation, an inner product operation is executed between the softmax result\nand a column of V (i.e., v). Assume that the input vector x can be divided into p partial vectors, x = [x(1), ..., x(p)]\n(v = [v(1), ..., v(p)] correspondingly), we have:\n\u27e8softmax(x), v\u27e9 =\nP\ni exi\u2212\u03d5 \u00b7 vi\nP\ni exi\u2212\u03d5\n=\nPp\nj=1\nPd/p\ni=1 ex(j)\ni\n\u2212\u03d5 \u00b7 v(j)\ni\nPp\nj=1\nPd/p\ni=1 ex(j)\ni\n\u2212\u03d5\n(4)\nThe inner accumulation in both the numerator and the denominator only take the partial vectors x(j) and v(j) as input,\nthus they can be processed asynchronously and individually. The outer accumulation is only processed after all partial\nvectors are processed. As we can see in Figure 4(c), each f(x(j)) is calculated individually, and softmax(x) is\ncalculated after all x(j) is calculated.\n6\nFlashDecoding++: Faster Large Language Model Inference on GPUs\nx1=4\nx2=5\nx3=6\nx4=7\nv1\nv2\nv3\nv4\nnumerator += \ne4-6\u00b7v1+e5-6\u00b7v2\ndenominator+=\ne4-6+e5-6\ncalculate \ne6-6, e7-6\nnumerator += \ne4-6\u00b7v1+e5-6\u00b7v2\ndenominator+=\ne4-6+e5-6\nnumerator\u00f7 \ndenominator\ncalculate \ne4-6, e5-6\nget x1, x2\nfrom Q, K\nget x3, x4\nfrom Q, K\nx\nv\ny1=3\ny2=6\ny3=9\ny4=6\nv1\nv2\nv3\nv4\nget y1, y2\nfrom Q, K\nget y3, y4\nfrom Q, K\ny\nv\nnumerator += \ne3-6\u00b7v1+e6-6\u00b7v2\ndenominator+=\ne3-6+e6-6\ncalculate \ne9-6, e7-6\ncalculate \ne3-6, e6-6\ncompute\nsoftmax1\ncompute\nsoftmax2\nupdate\nsoftmax1\nupdate\nsoftmax2\n9-5>2, overflow!\nrecomputation process\n(a) Calculate softmax(x)\u00d7vT\n(b) Calculate softmax(y)\u00d7vT\nFigure 6: Example of asynchronized partial softmax computation. (a) Each partial softmax result is process individually\nwithout the synchronized update. (b) The recomputation process for all parital softmax computation is required when\noverflow happens.\nApproach: Recomputation. Without loss of generality, we assume a < xi \u2212 \u03d5 < b for each xi to ensure precision and\navoid overflow. Then, the partial softmax operation is processed individually. However, when xi \u2212 \u03d5 \u2264 a or xi \u2212 \u03d5 \u2265 b,\nthe asynchronized partial softmax computation is terminated for the vector x where xi belongs to. The softmax is then\nrecomputed using the synchronized partial softmax scheme (used in FlashAttention [18, 19] and FlashDecoding [13])\nshown in Figure 4(b). Such a recomputation scheme avoids overflow while introducing negligible overheads based on\nthe statistical data shown in Figure 5.\nExample. Figure 6 shows an example of the asynchronized softmax scheme. We set a = \u22123, b = 3, \u03d5 = 6. Two\nvectors x and y are calculated from Q \u00d7 KT in Equation (1), and are divided into 2 partial vectors. We omit the process\nfrom Q \u00d7 KT to these partial vectors. For each xi, we have a < xi \u2212 \u03d5 < b, we process ex1\u2212\u03d5 \u00b7 v1 + ex2\u2212\u03d5 \u00b7 v2 and\nex1\u2212\u03d5 + ex2\u2212\u03d5 for the first partial vector of x using two asynchronized threads. Then, each thread moves to the next\npartial vector for the corresponding computation (i.e., ex3\u2212\u03d5 \u00b7 v3 + ex4\u2212\u03d5 \u00b7 v4 and ex3\u2212\u03d5 + ex4\u2212\u03d5). Two threads are\nsynchronized when all partial vectors are processed, and perform the division operation in Equation (4). For y, the first\npartial vector is processed similarly. However, we find that y3 \u2212 \u03d5 > b, then two threads are terminated and the first\nthread recomputes all partial vectors according to the synchronized partial softmax scheme in Figure 4(b).\n4\nFlat GEMM Optimization with Double Buffering\nMotivation. The process of the decode phase is mainly composed of GEMV (batch size=1) or flat GEMM (batch\nsize>1) operation. Without loss of generality, GEMV/GEMM operations can be represented using M, N, K, where\nthe sizes of two multiplied matrices are M \u00d7 K and K \u00d7 N. Previous LLM inference engines utilize Tensor Core\nto accelerate these operations using libraries like cuBLAS [24] and CUTLASS [25]. Although modern Tensor Core\narchitectures [32] process GEMM with M = 8, these libraries usually tile the M\u2212dimension to 64 to hide memory\nlatency. However, for GEMV or flat GEMM operations in the decode phase, we usually have M \u226a 64 and the\nM\u2212dimension is padded to 64 with zeros. The padding leads to under-utilized computation, and the key problem is to\nprocess GEMV or flat GEMM operations with smaller tiles (i.e., padding to 8 corresponding to modern Tensor\nCore architectures) in the M\u2212dimension.\nChallenge. Processing GEMV or flat GEMM operations is non-trivial when the M\u2212dimension is padded to 8. The\ntiling technique in modern libraries like cuBLAS [24] and CUTLASS [25] can only be applied to the N\u2212dimension and\n7\nFlashDecoding++: Faster Large Language Model Inference on GPUs\n32\n64\n128\n256\n512\n1024\n\u221a\n2048\n\u221a\n4096\n\u221a\n8192\n\u221a\n16384\n\u221a\n32768\n\u221a\n65536\n\u221a\n131072\n\u221a\n262144\n\u221a\n32\n64\n128\n256\n512\n1024\n\u221a\n2048\n\u221a\n4096\n\u221a\n8192\n\u221a\n16384\n\u221a\n32768\n\u221a\n65536\n\u221a\n131072\n\u221a\n262144\n\u221a\nBN\nN\nmemory\n-bounded\nparallelism-bounded\nBN\nN\nmemory\n-bounded\nparallelism-bounded\nK=4096\nK=12288\n\u221a\nBN with the best flat GEMM performance for a certain N\nFigure 7: Normalized flat GEMM performance under different N\u2212dimension sizes and N\u2212dimension tiling sizes. We\nset M = 8 and execute GEMM on the NVIDIA Tesla A100 GPU.\nthe K\u2212dimension. Tiles on the K\u2212dimension are processed sequentially in a GPU block to avoid atomic operations\nduring reduction. Tiling on the N\u2212dimension affects both parallelism and computation/memory ratio, which are both\nimportant for GEMV and flat GEMM acceleration.\nAnalysis and Insights. Assume that tiling sizes of the N\u2212dimension and the K\u2212dimension are BN and BK,\nrespectively. The computation of each GEMM tile is 2 \u00d7 M \u00d7 BN \u00d7 BK with total B =\nN\u00d7K\nBN\u00d7BK GEMM tiles. The\ntotal memory access is (M \u00d7 BK + BN \u00d7 BK) \u00d7 B + M \u00d7 N. Thus, the computation/memory ratio is:\n2 \u00d7 M \u00d7 BN \u00d7 BK \u00d7 B\n(M \u00d7 BK + BN \u00d7 BK) \u00d7 B + M \u00d7 N\n=\n2 \u00d7 M \u00d7 K\nK + M\u00d7K\nBN\n+ M\n(5)\nOn the other hand, the parallelism is\nN\nBN . Thus, the computation/memory ratio shows a positive correlation with BN\nwhile the parallelism shows a negative correlation with BN, exposing a contradiction on improving the performance\nof GEMV or flat GEMM. We depict the normalized performance of the flat GEMM in Figure 7 with different N\nand BN. Our key insight is, for the smaller N, the flat GEMM is parallelism-bounded. There are 108 Streaming\nMultiprocessors (SMs) in the NVIDIA Tesla A100.\nN\nBN tends to be a constant (e.g., 128 or 256), which is related\nto the hardware parallelism (number of SMs). Another key insight is, for the larger N, the flat GEMM becomes\nmemory-bounded. The performance of these cases can be improved by hiding memory access latency.\nApproach: Double Buffering. In order to hide memory access latency, we introduce the double buffering technique.\nfor the flat GEMM operation. We allocate two separate buffers in the shared memory. The tile in one buffer performs\nthe GEMM operation, while another buffer loads a new tile for the next GEMM operation. Thus, the computation and\nthe memory access are overlapped. We apply such a technique when N is large in our practice.\nExample. Figure 8 shows the example of our flat GEMM optimization with double buffering. For M < 8, the\nM\u2212dimension is first padded to 8 considering modern Tensor Core architectures. Workloads in the K\u2212dimension\nare processed within one GPU block (e.g., A1, A2, A3, ...), while workloads in the N\u2212dimension are processed in\nparallel using different GPU blocks (e.g., C1, C2, ...). We take GPU Block1 as an example, the first tile for each matrix\nin the K\u2212dimension (i.e., A1 and B1) is loaded to the left buffer in the shared memory. Then, the GEMM operation is\nperformed between A1 and B1. Consequently, A2 and B2 are loaded to the right buffer in the shared memory. The\nfollowing tiles are processed similarly according to the double buffering scheme.\n8\nFlashDecoding++: Faster Large Language Model Inference on GPUs\nB1\nB2\nB3\nA1 A2 A3\nC1\n\u2026\n\u2026\nC\nB\nA\nC1=A1\u00b7B1+A2\u00b7B2+A3\u00b7B3+\u2026\nC2=A1\u00b7B\u20191+A2\u00b7B\u20192+A3\u00b7B\u20193+\u2026\nM\nK\nN\nC2\nB\u20191\nB\u20192\nB\u20193\nTimeline\nBK\nBN\nGPU Block1\nGPU Block2\n\u2026\nA1B1\nidle\nA1B1\nA2B2\nA3B3\nA2B2\nA1B\u20191\nidle\nA1B\u20191\nA2B\u20192\nA3B\u20193\nA2B\u20192\n\u2026\n\u2026\nBuffer in shared memory for loading\nBuffer in shared memory for computing\nFigure 8: Double buffering for flat GEMM when N\u2212dimension is large. The M\u2212 dimension is padded to 8 and not\ntiled.\n5\nHeuristic Dataflow with Hardware Resource Adaption\nMotivation. Although FlashDecoding++ optimizes the flat GEMM operation in Section 4, it does not cover all\noperations (even only for GEMMs) in the LLM inference. As mentioned in Figure 2, the shapes of GEMMs in different\noperations and two phases vary. Thus, the GEMM workload in the LLM inference can be GEMV (batch size=1 for the\ndecode phase), flat GEMM (small batch size for the decode phase and short sequence length for the prefill phase) and\nconventional GEMM (large batch size or long sequence length for the prefill phase). In order to leverage the powerful\ncomputational ability of Tensor Core, current frameworks like FasterTransformer [33] and DeepSpeed [9] tend to\nutilize the highly optimized GEMM implementation from cuBLAS [24] to deal with different workloads. However,\nthe Tensor Core implementation fails with the GEMV workload. The GEMV workload can be optimized by utilizing\nCUDA Core in previous designs like FastGEMV [34]. For a Llama2-7B linear layer in the decode phase, the Tensor\nCore implementation from cuBLAS only achieves 82.15% of the performance of CUDA Core implementation using\nFastGEMV on an NVIDIA A100 GPU. On the other hand, using CUDA Core to do the projection on a batchsize=4\ndecoding input only achieves 49.75% performance compared with the Tensor Core implementation. Thus, in order to\napproach the optimal computation performance, a heuristic dataflow is supposed to be exploited in for different\nworkloads.\nChallenge. Although a heuristic dataflow potentially exists in the implementation of different linear workloads, it\nis challenging to build the mapping from a certain workload to an optimal implementation. In the scenario of LLM\ninference, there are various factors that influence the implementation performance of linear workloads: (a) Input\ndynamics. The variety of the batch size and the input sequence length brings dynamic workloads. (b) Model diversity.\nThe linear workload varies with different model structures and sizes. (c) GPU capacities. The relative performance\nbetween implementations changes with GPU characteristics, such as memory bandwidth, cache size, and computational\nability. (d) Engineering effects. The engineering effort also highly impacts the kernel performance. All these influential\nfactors build a large search space, making it non-trivial to generate an effective mapping between the linear workload\nand the corresponding optimal implementation.\nAnalysis and Insights. Although all influential factors form a large search space, the homogeneity of different layers\nin LLM significantly reduces the search space for operator optimization. Figure 2 shows four linear GEMV/GEMM\noperations in the prefill phase and the decode phase, i.e., K, Q, V projection, O projection, and two feedforward\noperations. Each GEMV/GEMM operation can be can be abstracted as a multiplication between an (M \u00d7 K)-shaped\nmatrix and a (K \u00d7 N)-shaped matrix. Our key insight is, there are only four [K, N] shapes for a certain LLM.\nMoreover, M is only related to the input sequence length and the batch size for the prefill phase, and the batch size for\nthe decode phase. Figure 9(a) shows limited shapes of GEMV/GEMM operations in the LLM inference.\nApproach: Decision flow for inflection points. Because only four [K, N] shapes exist for a certain LLM, we use three\ntypes of implementations for GEMV/GEMM operations when M varies: FastGEMV for the GEMV and flat GEMM\noperations (ImplA), our flat GEMM optimization in Section 4 (ImplB), and the CUTLASS [25] libraries optimized for\nthe conventional GEMM (ImplC). Thus, it is important to decide whether applying ImplA or ImplB for a small M, and\nImplB or ImplC for a large M. Figure 9(b) shows the decision flow. FlashDecoding++ profiles the performance of\nImplA and ImplB for a certain M, and increases M to find an inflection point M1 where the performance of ImplB is\n9\nFlashDecoding++: Faster Large Language Model Inference on GPUs\nOperation\nM\nN\nK\nPrefill\nphase\nK, Q, V projection\nSeqLen*B\nHD*3\nHD\nO projection\nSeqLen*B\nHD\nHD\nFFN1\nSeqLen*B\nFD\nHD\nFFN2\nSeqLen*B\nHD\nFD\nDecode\nphase\nK, Q, V projection\nB\nHD*3\nHD\nO projection\nB\nHD\nHD\nFFN1\nB\nFD\nHD\nFFN2\nB\nHD\nFD\nHD: Hidden dimension size\nFD: Dimension size after the first FFN\nB: Batch size\nSeqLen: Input sequence length\nOnly 4 shapes!\n\u2026\u2026\nM=17\nM=16\n\u2026\u2026\nM=9\nM=8\n\u2026\u2026\nM=3\nM=2\nM=1\nImplA = FastGEMV\nImplB = our flat GEMM\nImplC = CUTLASS\nImpl.B > \nImpl.A?\nImpl.C > \nImpl.B?\nFind \nM1\nM++\nM++\nFind \nM2\nEnd\nM=1\nFor a certain LLM, traverse four [N, K] selections\n(a)  Different shapes of GEMMs in LLM\n(b) Decision flow   \nK, Q, V\nprojection\n[N, K] =\n[12288, 4096]\nO\nprojection\n[N, K] =\n[4096, 4096]\nFFN1\n[N, K] =\n[11008, 4096]\nFFN2\n[N, K] =\n[4096, 11008]\nUsing cuBLAS/CUTLASS\u2026\nUsing our flat GEMM optimization\nUsing GEMV on CUDA Core (e.g., FastGEMV)\n(c) Example of heuristic dataflow with hardware resource adaption\nM1\nM2\nM1\nM2\nM1\nM2\nM1\nM2\nFigure 9: Heuristic dataflow with hardware resource adaption in FlashDecoding++. (a) Only four [N, K] shapes\nexist for a certain LLM. (b) The decision flow. We traverse all [N, K] selections and profile the performance of three\nrepresentative implementations. M is increased to find two inflection points for runtime heuristic dataflow. (c) FlashDe-\ncoding++ heuristically utilizes Tensor Core/CUDA Core with the corresponding GEMV/GEMM implementation by\nreferring to a lookup table.\nbetter than ImplA. Another inflection point M2 is found similarly where the performance of ImplC is better than ImplB.\nNote that each [N, K] gets its individual M1 and M2.\nApproach: Heuristic dataflow. For the runtime LLM inference, FlashDecoding++ adopts ImplA using CUDA Core\nwhen M < M1, and ImplB/ImplC using Tensor Core when M1 \u2264 M < M2/M2 \u2264 M. Note that the decision flow are\nexecuted offline, it does not affect the performance of runtime LLM inference.\nExample. Figure 9(c) shows an example of applying the heuristic dataflow for the Llama2-7B model. Four [N, K]\nshapes are [12288, 4096] for K, Q, V projection, [4096, 4096] for O projection, [11008, 4096] and [4096, 11008] for\nFFN. For each [N, K], the inflection points are found based on the decision flow in Figure 9(c). Then, a lookup table is\nformed, and each GEMV/GEMM operation is executed according to corresponding implementations during runtime. In\nthis example, FastGEMV is adopted for the K, Q, V projection when batch size=1 (M = 1) for the decode phase, and\nour flat GEMM optimization is applied when batch size=1/input sequence length=8 for FFN1 (M = 8).\n6\nEvaluation\n6.1\nExperiments Setup\nWe evaluate the performance of FlashDecoding++ on different GPUs with various Large Language Models. We\ncompare the performance with several state-of-the-art LLM inference engines.\n10\nFlashDecoding++: Faster Large Language Model Inference on GPUs\nTable 1: Hardware Platforms\nNVIDIA\nAMD\nGPU\nTesla A100\nRTX3090\nMI210\nRX7900XTX\n80 GB\n24 GB\n64GB\n24GB\nCUDA 12.2\nCUDA 11.6\nROCm 5.7\nROCm 5.6\nCPU\nIntel Xeon\nIntel Xeon\nAMD EPYC\nIntel Core\nSilver 8358P\nGold 6226R\n7K62\ni9-10940X\n2.60 GHz\n2.90GHz\n2.60GHz\n3.30GHz\nTable 2: Model Configuration\nModel\nDimension\nHeads\nLayers\nContext\nLength\nLlama2-7B\n4096\n32\n32\n4k\nLlama2-13B\n5120\n40\n40\n4k\nOPT-6.7B\n4096\n32\n32\n2k\nChatGLM2-6B\n4096\n32\n32\n32k\n6.1.1\nHardware Platforms\nWe evaluate the performance of FlashDecoding++ and other LLM engines on both NVIDIA and AMD platforms to\nmake a comprehensive comparison. We choose two different GPUs for each platform: Tesla A100 and RTX3090 for\nNVIDIA, MI210 and RX7900XTX for AMD. We show the detailed configuration in Table 6.1.1.\n6.1.2\nLLM Engine Baselines\nWe implement our FlashDecoding++ using the Pytorch-based front-end with the C++ and CUDA backend for NVIDIA\nGPUs while ROCm for AMD GPUs. We compare the inference performance in both prefill phase and decode phase\nwith the following LLM engine baselines: Hugging Face (HF) [35], vLLM [11], DeepSpeed [9], TensorRT-LLM [14],\nOpenPPL [12], and FlashAttention2/FlashDecoding [19, 13]. These baselines are introduced in Section 7.\n6.1.3\nModels\nWe evaluate the performance of FlashDecoding++ with other LLM inference engines on three typical Large Language\nModels: Llama2, OPT, and ChatGLM2. Table 6.1.2 shows the detailed configuration of these models. Note that there\nmay be several models in one LLM (e.g., Llama2-7B, Llama2-13B) with different configurations (e.g., number of heads\nand layers).\n\u2022 Llama2 [1] is a mainstream open-source LLM set released by Meta in 2023. It is a collection of pretrained\nand fine-tuned generative text models ranging in scale from 7B to 70B parameters.\n\u2022 OPT [36], is a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters released\nby Meta AI.\n\u2022 ChatGLM2 [37] is an open-source LLM supporting bilingual (Chinese-English) chat.\n6.2\nComparison with State-of-the-art\nWe compare FlashDecoding++ with state-of-the-art LLM inference engines in Figure 10 and Figure 11 on NVIDIA\nGPUs, Figure 12 and Figure 13 for AMD GPUs. For the decode phase, FlashDecoding++ achieves up to 4.86\u00d7\nspeedup compared with Hugging Face implementations on three LLMs and two GPUs. The average speedup over\nvLLM, DeepSpeed, TensorRT-LLM, OpenPPL, and FlashDecoding is 1.24\u00d7, 1.44\u00d7, 1.13\u00d7, 1.24\u00d7, and 1.21\u00d7 (1.37\u00d7\non Tesla A100 compared with FlashDecoding), respectively. For the prefill phase, FlashDecoding++ achieves up to\n1.40\u00d7 speedup compared with Hugging Face implementations. The average speedup over DeepSpeed, TensorRT-LLM,\nOpenPPL, FlashAttention2 and FlashDecoding is 1.05\u00d7, 1.06\u00d7, 1.08\u00d7, 1.09\u00d7, and 1.08\u00d7, respectively. We also show\nthe decode results on two AMD GPUs. Currently, only the original Hugging Face implementation can be executed\non AMD GPUs as the baseline. FlashDecoding++ achieves up to 2.27\u00d7 and 3.93\u00d7 compared with the baseline on\nRX7900XTX and MI210, respectively.\n11\nFlashDecoding++: Faster Large Language Model Inference on GPUs\n0\n500\n1000\n0\n1\n2\n3\n4\n128\n1k\n8k\n32k 128\n1k\n8k\n32k 128\n1k\n8k\n128\n1k\n8k\nSpeedup\nHF\nvLLM\nDeepSpeed\nTensorRT-LLM\nppl\nFlashDecoding\nOurs\nOurs (token/s)\n0\n500\n1000\n0\n1\n2\n3\n128\n1k\n8k\n32k 128\n1k\n8k\n32k 128\n1k\n8k\n128\n1k\n8k\nSpeedup\n0\n500\n1000\n0\n2\n4\n6\n128\n1k\n8k\n32k 128\n1k\n8k\n32k 128\n1k\n8k\n128\n1k\n8k\nSpeedup\n0\n200\n400\n600\n0\n1\n2\n128\n1k\n2k\n4k\n128\n1k\n2k\n4k\n128\n1k\n2k\n128\n1k\n2k\nSpeedup\n0\n200\n400\n600\n0\n1\n2\n128\n1k\n2k\n4k\n128\n1k\n2k\n4k\n128\n1k\n2k\n128\n1k\n2k\nSpeedup\n0\n200\n400\n600\n0\n1\n2\n3\n128\n1k\n2k\n4k\n128\n1k\n2k\n4k\n128\n1k\n2k\n128\n1k\n2k\nSpeedup\nbatch size =\n1\n2\n4\n8\n(a) Llama2-7B@A100\n(b) OPT-6.7B@A100\n(c) ChatGLM2-6B@A100\n(d) Llama2-7B@3090\n(e) OPT-6.7B@3090\n(f) ChatGLM2-6B@3090\nThroughput\nThroughput\nThroughput\nThroughput\nThroughput\nThroughput\nFigure 10: Speedup of the decode phase on NVIDIA GPUs. Blank bars represent the model cannot be executed (e.g.,\nOpenPPL does not support OPT-6.7B/ChatGLM2-6B, TensorRT-LLM fails to compile the model with > 8K input\nlength, and etc.)\n7\nRelated Works\nLarge language model inference acceleration has gained significant attention in recent research, with several notable\napproaches and techniques emerging in the field. DeepSpeed [9] is a comprehensive engine that optimizes both the\ntraining and inference phases for LLMs. It achieves robust inference performance through kernel fusion and efficient\nGPU memory management, with a particular focus on optimizing memory usage for KVcache. vLLM [11] improves\n12\nFlashDecoding++: Faster Large Language Model Inference on GPUs\n1.E+00\n1.E+02\n1.E+04\n0.0\n1.0\n2.0\n3.0\n4.0\n1k\n8k\n32k\n1k\n8k\n32k\n1k\n8k\n32k\n1k\n8k\nSpeedup\nHF\nTensorRT-LLM\nDeepSpeed\nPPL\nFlashDecoding\nFlashAttention2\nOurs\nOurs-FTL\n1.E+00\n1.E+02\n1.E+04\n0.0\n0.5\n1.0\n1.5\n1k\n8k\n1k\n8k\n1k\n1k\nSpeedup\n(a) Llama2-7B@A100\n(e) ChatGLM2-6B@3090\n1.E+00\n1.E+01\n1.E+02\n1.E+03\n1.E+04\n0.0\n0.5\n1.0\n1.5\n2.0\n1k\n8k\n32k\n1k\n8k\n1k\n8k\n1k\n8k\nSpeedup\n(c) ChatGLM2-6B@A100\n0.E+00\n5.E+03\n1.E+04\n2.E+04\n0.0\n1.0\n2.0\n3.0\n4.0\n1k\n8k\n32k\n1k\n8k\n32k\n1k\n8k\n1k\n8k\nSpeedup\n(b) Llama2-13B@A100\n1.E+00\n1.E+02\n1.E+04\n0.0\n0.5\n1.0\n1.5\n1k\n8k\n1k\n8k\n1k\n1k\nSpeedup\n(d) Llama2-7B@3090\nbatch size =\n1\n2\n4\n8\n*Ours-FTL: Ours (first token latency [ms] )\nLatency\nLatency\nLatency\nLatency\nLatency\nFigure 11: Speedup of the prefill phase on NVIDIA GPUs.\nGPU memory utilization by efficient memory management techniques and the PageAttention method, leading to\nincreased maximum batch sizes and elevating the upper limit of inference performance. FlashAttention [18, 19]\noptimizes the self-attention computation process during the prefill phase through improved parallelism and workload\ndistribution. FlashDecoding [13] is an extension of FlashAttention and enhances the parallelism through spliting K and\nV , supporting efficient self-attention computation for long sequence during the decode phase. FasterTransformer [33]\nand OpenPPL [12] implement large model inference engines using C++ to reduce overhead resulting from kernels\nscheduling, compared to Python implementations. They also employ memory management techniques and kernel fusion\nto achieve efficient LLM inference. TensorRT-LLM [14] is built upon the TensorRT [38] and the FasterTransformer\n[33] engine (C++) and incorporates cutting-edge open-source technologies such as FlashAttention [18, 19]. Additionally,\nit enhances its ease of use by providing the Python API.\n13\nFlashDecoding++: Faster Large Language Model Inference on GPUs\n0\n100\n200\n300\n400\n0\n1\n2\n3\n128\n1k\n2k\n4k\n128\n1k\n2k\n128\n1k\n2k\n128\n1k\nSpeedup\nHuggingFace (PyTorch)\nOurs\nOurs (token/s)\nbatch size=1\nbatch size=2\nbatch size=4\nbatch size=8\n0\n100\n200\n300\n400\n0\n1\n2\n3\n128\n1k\n2k\n4k\n128\n1k\n2k\n128\n1k\n2k\n128\n1k\nSpeedup\n(a) Llama2-7B\n(b) OPT-6.7B\n0\n50\n100\n0\n1\n2\n3\n128\n512\n1k\n2k\n128\n512\n1k\nSpeedup\nHF\nOurs\nOurs (token/s)\n(a) Llama2-7B\n(b) Llama2-13B\nThroughput\nThroughput\nFigure 12: Speedup of the decode phase on AMD RX7900XTX.\n0\n200\n400\n600\n0\n1\n2\n3\n4\n128\n1k\n2k\n8k\n128\n1k\n4k\n128\n1k\n4k\n128\n1k\n4k\nSpeedup\n0\n200\n400\n600\n0\n1\n2\n3\n128\n1k\n2k\n8k\n128\n1k\n4k\n128\n1k\n4k\n128\n1k\n4k\nSpeedup\nHuggingFace (PyTorch)\nOurs\nOurs (token/s)\nbatch size=1\nbatch size=2\nbatch size=4\nbatch size=8\n0\n100\n200\n300\n0\n1\n2\n3\n128\n1k\n2k\n8k\n128\n1k\n4k\n128\n1k\n2k\n128\n1k\n2k\nSpeedup\n(a) Llama2-7B\n(b) Llama2-13B\n0\n50\n100\n0\n1\n2\n3\n128\n512\n1k\n2k\n128\n512\n1k\nSpeedup\nHF\nOurs\nOurs (token/s)\n(a) Llama2-7B\n(b) Llama2-13B\n(c) OPT-6.7B\nThroughput\nThroughput\nThroughput\nFigure 13: Speedup of the decode phase on AMD MI210.\n8\nConclusion\nWe propose FlashDecoding++, a fast Large Language Model inference engine in this paper. FlashDecoding++\naccelerates mainstream LLMs with multiple hardware backend support. FlashDecoding++ proposes three novel\ndesigns: the asynchronized softmax with unified max value, the flat GEMM optimization with double buffering, and the\nheuristic dataflow with hardware resource adaption, achieving up to 4.86\u00d7 and 3.93\u00d7 speedup on NVIDIA and AMD\nGPUs compared with Hugging Face implementations. FlashDecoding++ also achieves an average of 1.37\u00d7 speedup\ncompared with state-of-the-art LLM inference engines, FlashDecoding, on various LLMs.\n14\nFlashDecoding++: Faster Large Language Model Inference on GPUs\nReferences\n[1] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.\narXiv preprint arXiv:2307.09288, 2023.\n[2] Arun James Thirunavukarasu, Darren Shu Jeng Ting, Kabilan Elangovan, Laura Gutierrez, Ting Fang Tan, and\nDaniel Shu Wei Ting. Large language models in medicine. Nature medicine, 29(8):1930\u20131940, 2023.\n[3] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,\nEmanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark, Laurent El Shafey, Yanping Huang,\nKathy Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay,\nKefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul\nBarham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks, Michele Catasta, Yong Cheng, Colin\nCherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, Cl\u00e9ment Crepy, Shachi Dave, Mostafa Dehghani,\nSunipa Dev, Jacob Devlin, Mark D\u00edaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad Fienber,\nMarkus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand, Hadi Hashemi,\nLe Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew\nJagielski, Wenhao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin\nLee, Eric Li, Music Li, Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu,\nMarcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John\nNham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan\nQiao, Emily Reif, Bryan Richter, Parker Riley, Alex Castro Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel,\nRenee Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniel Sohn, Simon Tokumine, Dasha Valter, Vijay\nVasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu,\nKelvin Xu, Yunhan Xu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang\nZhou, Denny Zhou, Slav Petrov, and Yonghui Wu. Palm 2 technical report, 2023.\n[4] Jan Clusmann, Fiona R Kolbinger, Hannah Sophie Muti, Zunamys I Carrero, Jan-Niklas Eckardt, Narmin Ghaffari\nLaleh, Chiara Maria Lavinia L\u00f6ffler, Sophie-Caroline Schwarzkopf, Michaela Unger, Gregory P Veldhuizen, et al.\nThe future landscape of large language models in medicine. Communications Medicine, 3(1):141, 2023.\n[5] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, and Ziran Wang. Receive, reason, and react: Drive as you say with\nlarge language models in autonomous vehicles. arXiv preprint arXiv:2310.08034, 2023.\n[6] OpenAI. Openai pricing. [Online], 2023. https://openai.com/pricing.\n[7] Nerdynav. Up-to-date chatgpt statistics & user numbers [oct 2023]. [Online], 2023. https://nerdynav.com/\nchatgpt-statistics.\n[8] AFZAL AHMAD DYLAN PATEL. The inference cost of search disruption - large language model cost analysis.\n[Online], 2023. https://www.semianalysis.com/p/the-inference-cost-of-search-disruption.\n[9] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji\nRuwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. Deepspeed-inference: enabling efficient inference of\ntransformer models at unprecedented scale. In SC22: International Conference for High Performance Computing,\nNetworking, Storage and Analysis, pages 1\u201315. IEEE, 2022.\n[10] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher\nRe, Ion Stoica, and Ce Zhang. Flexgen: High-throughput generative inference of large language models with a\nsingle gpu. 2023.\n[11] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao\nZhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In\nProceedings of the 29th Symposium on Operating Systems Principles, pages 611\u2013626, 2023.\n[12] Sensetime. Openppl: A high-performance deep learning inference platform. [Online], 2023. https://openppl.\nai/home.\n[13] Tri Dao, Daniel Haziza, Francisco Massa, and Grigory Sizov. Flash-decoding for long-context inference. [Online],\n2023. https://crfm.stanford.edu/2023/10/12/flashdecoding.html.\n[14] Neal Vaidya, Fred Oh, and Nick Comly. Optimizing inference on large language models with nvidia tensorrt-llm,\nnow publicly available. [Online], 2023. https://github.com/NVIDIA/TensorRT-LLM.\n[15] Sensetime. A light and fast inference service for llm. [Online], 2023. https://github.com/ModelTC/\nlightllm.\n15\nFlashDecoding++: Faster Large Language Model Inference on GPUs\n[16] Text generation inference: Fast inference optimize for llms.\n[Online], 2023.\nhttps://github.com/\nhuggingface/text-generation-inference/.\n[17] Mlc llm: Machine learning compilation for large language models. [Online], 2023. https://github.com/\nmlc-ai/mlc-llm.\n[18] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient\nexact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344\u201316359, 2022.\n[19] Tri Dao.\nFlashattention-2: Faster attention with better parallelism and work partitioning.\narXiv preprint\narXiv:2307.08691, 2023.\n[20] Aaron Pham, Chaoyu Yang, Sean Sheng, Shenyang Zhao, Sauyon Lee, Bo Jiang, Fog Dong, Xipeng Guan, and\nFrost Ming. OpenLLM: Operating LLMs in production, June 2023.\n[21] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl:\nAttentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019.\n[22] Z Dong, T Tang, L Li, and WX Zhao. A survey on long text modeling with transformers. arxiv 2023. arXiv\npreprint arXiv:2302.14502.\n[23] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models\nwith attention sinks. arXiv preprint arXiv:2309.17453, 2023.\n[24] NVIDIA. cublas: Basic linear algebra on nvidia gpus. [Online], 2017. https://developer.nvidia.com/\ncublas.\n[25] NVIDIA. Cutlass: Cuda templates for linear algebra subroutines. [Online], 2017. https://github.com/\nNVIDIA/cutlass.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and\nIllia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\n[27] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In Proceedings\nof the 27th international conference on machine learning (ICML-10), pages 807\u2013814, 2010.\n[28] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016.\n[29] Prajit Ramachandran, Barret Zoph, and Quoc V Le.\nSearching for activation functions.\narXiv preprint\narXiv:1710.05941, 2017.\n[30] John Bridle.\nTraining stochastic model recognition algorithms as networks can lead to maximum mutual\ninformation estimation of parameters. Advances in neural information processing systems, 2, 1989.\n[31] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016.\n[32] NVIDIA.\nNvidia tensor core.\n[Online], 2023.\nhttps://www.nvidia.com/en-us/data-center/\ntensor-cores/.\n[33] NVIDIA. Fastertransformer: About transformer related optimization, including bert, gpt. [Online], 2017.\nhttps://github.com/NVIDIA/FasterTransformer.\n[34] Siping Wang. Fastgemv: High-speed gemv kernels. [Online], 2023. https://github.com/wangsiping97/\nFastGEMV.\n[35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine\nJernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander\nRush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October 2020.\nAssociation for Computational Linguistics.\n[36] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,\nMona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig,\nPunit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. Opt: Open pre-trained transformer\nlanguage models, 2022.\n[37] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. Glm: General\nlanguage model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335, 2022.\n[38] NVIDIA. Nvidia tensorrt: An sdk for high-performance deep learning inference. [Online]. https://developer.\nnvidia.com/tensorrt.\n16\n"
  },
  {
    "title": "RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation",
    "link": "https://arxiv.org/pdf/2311.01455.pdf",
    "upvote": "25",
    "text": "ROBOGEN: TOWARDS UNLEASHING INFINITE DATA\nFOR AUTOMATED ROBOT LEARNING VIA GENERATIVE\nSIMULATION\nYufei Wang\u2217,1, Zhou Xian\u2217,1, Feng Chen\u2217,2, Tsun-Hsuan Wang3, Yian Wang4,\nZackory Erickson1, David Held1, Chuang Gan4,5\n1CMU, 2Tsinghua IIIS, 3MIT CSAIL, 4UMass Amherst, 4MIT-IBM AI Lab\nhttps://robogen-ai.github.io/\nABSTRACT\nWe present RoboGen, a generative robotic agent that automatically learns diverse\nrobotic skills at scale via generative simulation. RoboGen leverages the latest\nadvancements in foundation and generative models. Instead of directly using or\nadapting these models to produce policies or low-level actions, we advocate for\na generative scheme, which uses these models to automatically generate diver-\nsified tasks, scenes, and training supervisions, thereby scaling up robotic skill\nlearning with minimal human supervision. Our approach equips a robotic agent\nwith a self-guided propose-generate-learn cycle: the agent first proposes inter-\nesting tasks and skills to develop, and then generates corresponding simulation\nenvironments by populating pertinent objects and assets with proper spatial con-\nfigurations. Afterwards, the agent decomposes the proposed high-level task into\nsub-tasks, selects the optimal learning approach (reinforcement learning, motion\nplanning, or trajectory optimization), generates required training supervision, and\nthen learns policies to acquire the proposed skill. Our work attempts to extract the\nextensive and versatile knowledge embedded in large-scale models and transfer\nthem to the field of robotics. Our fully generative pipeline can be queried repeat-\nedly, producing an endless stream of skill demonstrations associated with diverse\ntasks and environments.\n1\nINTRODUCTION\nThis work is motivated by a long-standing and challenging goal in robotics research: empower-\ning robots with a diverse set of skills, enabling them to operate in various non-factory settings and\nperform a broad range of tasks for humans. Recent years have witnessed impressive progress in\nteaching robots various complex skills: from deformable object and fluid manipulation (Lin et al.,\n2022a; Weng et al., 2022; Xu et al., 2023; Xian et al., 2023c; Wang et al., 2023c; Lin et al., 2020), to\ndynamic and dexterous skills such as object tossing (Zeng et al., 2020), in-hand re-orientation (Chen\net al., 2022), soccer playing (Haarnoja et al., 2023) and even robot parkour (Zhuang et al., 2023).\nHowever, these skills still remain compartmentalized, have relatively short horizons, and necessi-\ntate human-designed task descriptions and training supervision. Notably, due to the expensive and\nlaborious nature of real-world data collection, many of these skills are trained in simulations with\nappropriate domain randomization and then deployed to real-world (Xu et al., 2023; Zhuang et al.,\n2023; Chen et al., 2022).\nIndeed, simulation environments have become a crucial driving force behind diverse robotic skill\nlearning (Lin et al., 2022a; Song et al., 2023; Zhuang et al., 2023). Compared to exploration and\ndata collection in the real-world, skill learning in simulations offers several advantages: 1) simulated\nenvironments provide access to privileged low-level states and unlimited exploration opportunities;\n\u2217Equal contribution. Contribution Statement: Yufei Wang led the implementation of the whole project.\nZhou Xian conceptualized the idea, contributed to the locomotion pipeline, and supervised the direction of the\nproject. Feng Chen was in charge of the soft-body manipulation pipeline.\n1\narXiv:2311.01455v2  [cs.RO]  13 Nov 2023\nFigure 1: RoboGen is a robotic agent that generates tasks and environments, and acquire skills auto-\nmatically. Here we show 25 example tasks generated and corresponding skills learned by RoboGen.\n2) simulation supports massively parallel computation, enabling significantly faster data collection\nwithout reliance on considerable investment in robotic hardware and human labor; 3) exploration in\nsimulation allows robots to develop closed-loop policies and error-recovery capabilities, while real-\nworld demonstrations typically offer only expert trajectories. However, robot learning in simulations\nalso presents its own limitations: while exploration and practicing in simulated environments are\ncost-effective, constructing these environments requires significant labor effort, demanding tedious\nsteps including designing tasks, selecting relevant and semantically meaningful assets, generating\nplausible scene layouts and configurations, and crafting training supervisions such as reward or loss\nfunctions (James et al., 2020; Srivastava et al., 2022; Gu et al., 2023; Li et al., 2023a). The onerous\ntask of creating these components and constructing individualized simulation settings of each one of\nthe countless tasks encountered in our daily life is an overwhelming challenge, which significantly\nconstrains the scalability of robotic skill learning even in simulated worlds.\nIn light of this, we present a paradigm termed Generative Simulation, marrying the advancements in\nsimulated robotic skill learning and the latest progress in foundation and generative models. Lever-\naging the generative capabilities of state-of-the-art foundation models, Generative Simulation aims\nto generate information for all the stages needed for diverse robotic skill learning in simulation:\nfrom high-level task and skill proposals, to task-dependent scene descriptions, assets selections and\ngenerations, policy learning choices, and training supervisions. Thanks to the comprehensive knowl-\nedge encoded in latest foundation models, scene and task data generated this way have potentials to\nclosely resemble the distribution of real-world scenarios. In addition, these models can further pro-\nvide decomposed low-level sub-tasks, which can be seamlessly handled by domain-specific policy\nlearning approaches, thereby producing closed-loop demonstrations for various skills and scenarios.\nA distinct advantage of our proposed paradigm lies in the strategic design of what modes of knowl-\nedge to extract from contemporary foundation models. These models have demonstrated impressive\ncapabilities across various modalities (Touvron et al., 2023; Driess et al., 2023; OpenAI, 2023; Rom-\nbach et al., 2022; Girdhar et al., 2023; Kang et al., 2023), giving rise to autonomous agents capable\nof using a range of tools and solving a variety of tasks in the virtual realm (Sur\u00b4\u0131s et al., 2023; Yang\net al., 2023; Shen et al., 2023). However, due to the absence of training data pertaining to dynamics,\n2\nactuations, and physical interactions, these models are yet to fully grasp understandings of what\u2019s\nessential for robots to effectively execute physical actions and interact with the surrounding envi-\nronments \u2013 from discerning the precise joint torque needed for stable locomotion, to high-frequency\nfinger motor commands needed for dexterous manipulation tasks such as rolling a dough. In contrast\nto recent efforts that employ these foundation models such as Large Language Models (LLMs) for\ndirectly yielding policies or low-level actions (Liang et al., 2022; Huang et al., 2023b; Wang et al.,\n2023b), we advocate for a scheme that extracts information that falls neatly within the capabilities\nand modalities of these models - object semantics, object affordances, common-sense knowledge\nto identify valuable learning tasks, etc. We use these knowledge to construct environmental play-\ngrounds, and then resort to additional help from physics-grounded simulations, for robots to develop\nunderstandings of physical interactions and acquire diverse skills.\nWe first described such a paradigm in a recent white paper (Xian et al., 2023a), sketched as a promis-\ning pathway towards generating diverse data for generalist robot learning. In this paper, we present\nRoboGen, a comprehensive realization of this paradigm. RoboGen is a generative robotic agent that\nself-proposes skills to learn, generates scene components and configurations in simulation, labels\nthe tasks with natural language descriptions, and designs proper training supervisions for subsequent\nskill learning. Our experiments show that RoboGen can deliver a continuous stream of diversified\nskill demonstrations, spanning tasks including rigid and articulated object manipulation, deformable\nobject manipulation, as well as legged locomotion skills (see Figure 1). The diversity of tasks and\nskills generated by RoboGen surpasses previous human-created robotic skill learning datasets, with\nminimal human involvement needed beyond several prompt designs and in-context examples. Our\nwork attempts to extract the extensive and versatile knowledge embedded in large-scale models and\ntransfer them to the field of robotics. When queried endlessly, our system holds the potential for un-\nleashing infinite amount of diversified demonstration data for robot learning, making a step towards\nfully automated large-scale robotic skill training for generalizable robotic systems.\n2\nRELATED WORK\nRobotic skill learning in simulations Various physics-based simulation platforms have been de-\nveloped in the past to accelerate robotics research (Liu & Negrut, 2021). These include rigid-body\nsimulators (Coumans & Bai, 2016; Todorov et al., 2012; Xiang et al., 2020), deformable object sim-\nulators (Macklin et al., 2014; Lin et al., 2020; Xu et al., 2023; Heiden et al., 2021), and environments\nsupporting multi-material and their couplings with robots (Xian et al., 2023c; Gan et al., 2021; Gu\net al., 2023). Such simulation platforms have been heavily employed in the robotics community for\nlearning diverse skills, including table-top manipulation (Gervet et al., 2023; Tung et al., 2020; Xian\net al., 2023b), deformable object manipulation (Lin et al., 2022a; Weng et al., 2022; Wang et al.,\n2023c; Lin et al., 2022b; Wang et al., 2022; Ha & Song, 2022), object cutting (Heiden et al., 2021;\nXu et al., 2023), fluid manipulation (Seita et al., 2023; Xian et al., 2023c), as well as highly dynamic\nand complex skills such as in-hand re-orientation (Chen et al., 2022; Akkaya et al., 2019), object\ntossing (Zeng et al., 2020), acrobatic flight (Kaufmann et al., 2020; Loquercio et al., 2021; Song\net al., 2023), locomotion for legged robots (Cheng et al., 2023; Zhuang et al., 2023; Radosavovic\net al., 2023) and soft robots (Wang et al., 2023a).\nScaling up simulation environments Apart from building physics engines and simulators, a large\nbody of prior work targeted at building large-scale simulation benchmarks, providing platforms for\nscalable skill learning and standardized benchmarking (Li et al., 2023a; Lin et al., 2020; Xian et al.,\n2023c; Yu et al., 2020; James et al., 2020; Mu et al., 2021; Gu et al., 2023; Srivastava et al., 2022).\nNotably, most of these prior simulation benchmarks or skill learning environments are manually\nbuilt with human labeling. Another line of works attempts to scale up tasks and environments us-\ning procedural generation, and generate demonstrations with Task and Motion Planning (TAMP)\n(Dalal et al., 2023; McDonald & Hadfield-Menell, 2021; Murali et al., 2023; Migimatsu & Bohg,\n2019; Toussaint, 2015). These methods primarily build on top of manually-defined rules and plan-\nning domains, limiting the diversity of the generated environments and skills to relatively simple\npick-and-place and object stacking tasks (Dalal et al., 2023; McDonald & Hadfield-Menell, 2021).\nContrary to these works, we extract the common sense knowledge embedded in foundation models\nsuch as LLMs, and use them for generating meaningful tasks, relevant scenes, and skill training\nsupervisions, leading to more diverse and plausible skills.\nFoundation and generative models for robotics Following the rapid advancement in foundation\nand generative models in domains of imagery (Poole et al., 2022; Liu et al., 2023c; Melas-Kyriazi\n3\nFigure 2: RoboGen consists of the following stages: A) task proposal, B) scene generation, C)training super-\nvision generation, and D) skill learning with generated information.\net al., 2023), language (Touvron et al., 2023; Driess et al., 2023; OpenAI, 2023), and other modalities\n(Liu et al., 2023a; Girdhar et al., 2023; Huang et al., 2023a), an active line of works investigate how\nto use these large-scale models for robotics research, via approaches such as code generation (Wu\net al., 2023; Liang et al., 2022), data augmentation (Yu et al., 2023a), visual imagination for skill\nexecution (Du et al., 2023), sub-task planning (Ahn et al., 2022; Huang et al., 2022; Lin et al.,\n2023), concept generalization of learned skills (Brohan et al., 2023), outputting low-level control\nactions (Wang et al., 2023b), and goal specification (Kapelyukh et al., 2023). More related to ours\nare recent methods using LLMs for reward generation (Yu et al., 2023b), and sub-task and trajectory\ngeneration (Ha et al., 2023). In contrast to them, our proposed system aims at a fully automated\npipeline that self-proposes new tasks, generate environments and yields diverse skills.\nGenerative Simulation We first presented the idea of generative simulation in a recent white paper\n(Xian et al., 2023a). We implemented a proof of concept of generative simulation in (Katara et al.,\n2023) by generating assets, task descriptions and decompositions for a Franka arm using image\ngenerative models and LLMs. In this paper, we extend this line of research to support a wider\nrange of robot types and a more diverse set of tasks with photo-realistic visuals, powered by a more\nadvanced rendering and physics engine.\n3\nROBOGEN\nRoboGen is an automated pipeline that utilizes the embedded common sense and generative capabil-\nities of the latest foundation models (OpenAI, 2022; Taori et al., 2023) for automatic task, scene, and\ntraining supervision generation, leading to diverse robotic skill learning at scale. We illustrate the\nwhole pipeline in Figure 2, composed of several integral stages: Task Proposal, Scene Generation,\nTraining Supervision Generation, and Skill Learning. We detail each of them in the following.\n3.1\nTASK PROPOSAL\nRoboGen starts with generating meaningful, diverse, and high-level tasks for robots to learn. Instead\nof directly querying an LLM for task proposal, we initialize the system with a specific robot type\nand an object randomly sampled from a pool. The provided robot and sampled object information\nare then used as input to an LLM to perform task proposal. Such a sampling procedure ensures\nthe diversity of the generated tasks: for instance, a legged robot such as a quadruped is capable\nof acquiring a variety of locomotion skills, while a robotic arm manipulator has the potential to\nexecute a diverse range of manipulation tasks when paired with different sampled objects. This\ninitialization step serves as a seeding stage, providing a basis upon which the LLM can condition\nand subsequently reason and extrapolate to generate a variety of tasks, taking into account both robot\nand object affordances. Apart from object-based initialization, another choice is to employ example-\nbased initialization, where we initialize the query with a provided robot and several example tasks\nsampled from a list 11 pre-defined tasks.\n4\nWe use GPT-4 (OpenAI, 2023) as the LLM to query in our current pipeline, but this backend can\nbe upgraded once better models are available. In the following, we explain details of RoboGen in\nthe context of a robotic arm (e.g., Franka) and tasks generated pertain to object manipulation, using\nobject-based initialization. In this case, the objects used for initialization are sampled from a prede-\nfined list including common articulated and non-articulated objects in household scenarios such as\noven, microwave, dispenser, laptop, dishwasher, etc., extracted from PartNetMobility (Xiang et al.,\n2020) and RLBench (James et al., 2020). Having been trained on extensive internet-scale datasets,\nLLMs like GPT-4 are imbued with a rich understanding of the affordances of these objects, knowl-\nedge of how to interact with them, and what kind of meaningful tasks they can be associated with.\nIn order to generate tasks that involve interaction and manipulation of sampled articulated objects\nby the robot, we construct a query comprising the broad category to which the articulated object be-\nlongs to, its articulation tree derived from the URDF file, and a semantics file providing annotations\nabout the object\u2019s links, e.g., which link corresponds to the door in a sampled microwave. This infor-\nmation is provided with the PartNetMobility dataset. The category of the articulated object informs\nwhat general types of tasks can be performed with this kind of objects, and the articulation tree and\nthe semantics file informs GPT-4 how exactly the object can be articulated, and semantic meanings\nof each joint and link. The query asks GPT-4 to return a number of tasks that can be performed\nwith the sampled object, where each task consists of the task name, a short description of the task,\nif there are any other objects that are required for the task besides the provided articulated object,\nand which joints/links of the articulated object the robot will need to interact with to accomplish\nthe task. Additionally, we include an example input-output pair in the query for GPT-4 to perform\nin-context learning to improve the quality of its response.\nAs a concrete example, given a sampled articulated object being a microwave, where joint 0\nis a revolute joint connecting its door, and joint 1 being another revolute joint controlling a knob\nof the timer, GPT-4 would return a task named as \u201cheat up a bowl of soup\u201d, with a task descrip-\ntion of \u201cThe robot arm places a bowl of soup inside the microwave, closes the door and sets the\nmicrowave timer for an appropriate heating duration\u201d, additional objects needed for the generated\ntask such as \u201cA bowl of soup\u201d, and task-relevant joints and links including joint 0 (for opening\nthe microwave door), joint 1 (for setting the timer), link 0 (the door), and link 1 (the timer\nknob). For the detailed prompt and example responses, please refer to Appendix B. Note that for\ncases where we sample non-articulated objects or use example-based initialization, the sampled ob-\njects and examples are provided only as a hint for task proposal, and the generated tasks will not be\ntied to them. For articulated objects, since PartNetMobility is the only articulated object dataset with\nhigh quality, and already covers diverse range of articulated assets, we will generate tasks dependent\non the sampled asset. For locomotion and soft-body manipulation tasks, we use only example-based\ninitialization, and resort to GPT-4 to populate additional required objects. By repeatedly querying\nwith different sampled objects and examples, we can generate a diverse range of manipulation and\nlocomotion tasks, concerning the relevant object affordances when needed.\n3.2\nSCENE GENERATION\nGiven a proposed task, we proceed to generate a corresponding simulation scene for learning skills\nto accomplish the task. As shown in Figure 2, the scene components and configurations are gener-\nated conditioned on the task description, and the object assets are retrieved or generated to populate\nthe simulation scene subsequently. Specifically, the scene components and configurations are com-\nposed of the following elements: queries for relevant assets to be populated into the scene, their\nphysical parameters such as sizes, configurations such as the initial joint angles, and the overall\nspatial configurations of the assets.\nObtaining queries for relevant assets Aside from the necessary object assets that are needed for\nthe task generated in the previous step of task proposal, in order to increase the complexity and\ndiversity of the generated scene while resembling object distributions of real-world scenarios, we\nquery GPT-4 to return a number of additional queries for objects that are semantically relevant to\nthe task. See Figure 1 for examples of the additional objects returned by GPT-4 for a task, e.g., for\nthe task \u201cOpen storage, put the toy inside and close it\u201d, the generated scene involves additionally a\nliving room mat, a table-top lamp, a book, and an office chair.\nRetrieving or generating assets The generated queries for relevant objects (i.e. their language\ndescriptions) will be used to either search in an existing database (Deitke et al., 2023), or to be used\nas the input to text-to-image (Midjourney, 2022), and then image-to-3d mesh generation models\n5\n(Liu et al., 2023b; Poole et al., 2022) to generate the 3d textured mesh of the asset. Specifically,\nwe use Objaverse (Deitke et al., 2023), a large-scale dataset containing over 800k object assets\n(3d meshes, texture pictures, and etc.) as the main database for the retrieval. For each object in\nObjaverse, we obtain a list of language descriptions of it by combining the default annotations and\na more cleaned version of annotations from (Luo et al., 2023). Given the language description\nof the asset we want to retrieve, we use Sentence-Bert (Reimers & Gurevych, 2019) to get the\nembedding of the description, and retrieve k objects from Objaverse whose language embeddings\nare the most similar to the language embedding of the target asset. Due to noises in the object\nannotations, there can be significant discrepancies between the actual asset and the intended target,\neven when the similarity score in the language embedding space is high. To resolve this, we further\nleverage a vision-language model (VLM) to verify the retrieved assets and filter out the undesired\nones. Specifically, we input an image of the retrieved object to the VLM model, and ask the VLM\nto caption it. The caption, together with the description of the desired asset and the description\nof the task, are fed back into GPT-4 to verify if the retrieved asset is appropriate to be used in the\nproposed task. Since the diverse range of assets from Objaverse is inherently challenging for existing\npre-trained models, in order to improve the robustness of our system, we use both Bard (Google,\n2022) and BLIP-2 (Li et al., 2023b), to cross verify the validity of the retrieved asset, and adopt\nthe asset only when the captions from both of them are deemed suitable for the task by GPT-4. We\nuse k = 10 in our pipeline, and if all assets are rejected, we resort to text-to-image followed by\nimage-to-mesh generation models to generate the needed assets from the language description. We\nuse Midjourney (Midjourney, 2022) as our text-to-image generative model, and Zero-1-to-3 (Liu\net al., 2023b) as our image-to-mesh generative model. For soft-body manipulation tasks, in order\nto obtain a more consistent and controllable target shape for the soft-body under manipulation, we\nask GPT-4 to come up with desired target shape, and only use this text-to-image-to-mesh pipeline\ninstead of database retrieval.\nAsset size Assets from Objaverse (Deitke et al., 2022) or PartNetMobility (Xiang et al., 2020) are\nusually not of physically plausible sizes. To account for this, we query GPT-4 to generate the sizes of\nthe assets such that: 1) the sizes should match real-world object sizes; 2) the relative sizes between\nobjects allow a plausible solution for solving the task, e.g., for the task of \u201cputting a book into the\ndrawer\u201d, the size of the drawer should be larger than the book.\nInitial asset configuration For certain tasks, the articulated object should be initialized with valid\nstates for the robot to learn the skill. For example, for the task of \u201cclose the window\u201d, the window\nshould be initialized in an open state; similarly, for the task of opening the door, the door should be\ninitially closed. Again, we query GPT-4 to set the initial configurations of these articulated objects,\nspecified in joint angles. To allow GPT-4 to reason about the task and the articulated object, the query\nscript contains the task description, the articulation tree and semantic descriptions of the articulated\nobject.\nScene configuration Scene configuration specifying the location and relevant poses of each asset\nin the scene is crucial for both producing plausible environments and allowing valid skill learning.\nE.g., for the task of \u201cretrieving a document from the safe\u201d, the document needs to be initialized\ninside the safe; for the task of \u201cremoving the knife from the chopping board\u201d, the knife needs to\nbe initially placed on the chopping board. RoboGen queries GPT-4 to generate such special spatial\nrelationships with the task description as the input, and also instructs GPT-4 to place objects in a\ncollision-free manner.\nWith the generated scene components and configuration, we populate the scene accordingly. See\nFigure 1 for a collection of example scenes and tasks generated by RoboGen.\n3.3\nTRAINING SUPERVISION GENERATION\nTo acquire the skill for solving the proposed task, supervisions for skill learning are needed. To\nfacilitate the learning process, RoboGen first queries GPT-4 to plan and decompose the generated\ntask, which can be long-horizon, into shorter-horizon sub-tasks. Our key hypothesis is, when the\ntask is decomposed into sufficiently short-horizon sub-tasks, each sub-task can be reliably solved by\nexisting algorithms such as reinforcement learning (Schulman et al., 2017; Haarnoja et al., 2018),\nmotion planning (Karaman & Frazzoli, 2011), or trajectory optimization (Kalakrishnan et al., 2011).\n6\nAfter the decomposition, RoboGen then queries GPT-4 to choose a proper algorithm for solving each\nsub-task. There are three different types of learning algorithms integrated into RoboGen: reinforce-\nment learning, evolutionary strategy, gradient-based trajectory optimization, and action primitive\nwith motion planning. Each of these is suited for different tasks, e.g., gradient-based trajectory op-\ntimization is more suitable for learning fine-grained manipulation tasks involving soft bodies such\nas shaping a dough into a target shape (Xu et al., 2023; Lin et al., 2022a); action primitives coupled\nwith motion planning are more reliable in solving the task such as approaching a target object via a\ncollision-free path; reinforcement learning and evolutionary strategy better suits tasks that are con-\ntact rich and involving continuous interaction with other scene components, e.g., legged locomotion,\nor when the required actions cannot be simply parameterized by discrete end-effector poses, e.g.,\nturning the knob of an oven. We provide examples and let GPT-4 choose online which learning\nalgorithm to use conditioned on the generated sub-task. We consider action primitives including\ngrasping, approaching and releasing a target object. Since parallel jaw gripper can be limited when\ngrasping objects with diverse sizes, we consider a robotic manipulator equipped with a suction cup\nto simplify object grasping. The grasping and approaching primitives are implemented as follows:\nwe first randomly sample a point on the target object or link, compute a gripper pose that aligns with\nthe normal of the sampled point, and then use motion planning to find a collision-free path to reach\nthe target gripper pose. After the pose is reached, we keep moving along the normal direction until\nthere contact is made with the target object.\nFor sub-tasks to be learned using RL or trajectory optimization, we prompt GPT-4 to write the corre-\nsponding reward functions with a few in-context examples. For object manipulation and locomotion\ntasks, the reward functions are based on the low-level simulation states which GPT-4 can query via\na provided list of API calls. Further, we ask GPT-4 to suggest the action space for the learning al-\ngorithm, e.g., delta-translation of the end-effector, or the target location for the end-effector to move\nto. Delta-translation is more appropriate for tasks that involve local movements, e.g., opening a door\nafter it has been grasped; for tasks that involve transferring an object to a different location, directly\nspecifying the target as the action space makes the learning easier. For soft body manipulation tasks,\nthe reward has a fixed form specified as the earth-mover distance between the current and target\nshape of the soft body.\n3.4\nSKILL LEARNING\nOnce we obtained all the required information for the proposed task, including scene components\nand configurations, task decompositions, and training supervisions for the decomposed sub-tasks,\nwe are able to construct the scene in simulation for the robot to learn the required skills for complet-\ning the task.\nAs aforementioned, we use a combination of techniques for skill learning, including reinforcement\nlearning, evolutionary strategy, gradient-based trajectory optimization, and action primitive with\nmotion planning, as each are suitable for different types of tasks. For object manipulation tasks, we\nuse SAC (Haarnoja et al., 2018) as the RL algorithm for learning the skill. The observation space is\nthe low-level state of the objects and robot in the task. The action space of the RL policy includes\nthe delta translation or target location (determined by GPT-4) of the robot end-effector, along with\nits delta rotation. We use BIT\u2217 (Gammell et al., 2015) implemented in the Open Motion Planning\nLibrary (OMPL) (Sucan et al., 2012) as the underlying motion planning algorithm for the action\nprimitives. For long-horizon tasks that involve multiple sub-tasks, we adopt a simple scheme of\nlearning each sub-task sequentially: for each sub-task, we run RL N = 8 times and use the end\nstate with the highest reward as the initial state for the next sub-task. For locomotion tasks, the cross\nentropy method (CEM (De Boer et al., 2005)) is used for skill learning, which we find to be more\nstable and efficient than RL. The ground-truth simulator is used as the dynamcis model in CEM,\nand the actions to be optimized are the joint angle values of the robot. For soft body manipulation\ntasks, we run gradient-based trajectory optimization with Adam (Kingma & Ba, 2014) for learning\nthe skill, where the gradients are provided by the fully differentiable simulator we use. More details\non skill learning can be found in Appendix A.\n4\nEXPERIMENTS\nRoboGen is an automated pipeline that can be queried endlessly, and generate a continuous stream of\nskill demonstrations for diverse tasks. In our experiments, we aim to answer the following questions:\n7\n\u2022 Task Diversity: How diverse are the tasks proposed by RoboGen robotic skill learning?\n\u2022 Scene Validity: Does RoboGen generate valid simulation environments that match the pro-\nposed task descriptions?\n\u2022 Training Supervision Validity: Does RoboGen generate correct task decomposition and train-\ning supervisions for the task that will induce intended robot skills?\n\u2022 Skill Learning: Does integrating different learning algorithms in RoboGen improve the suc-\ncess rate of learning a skill?\n\u2022 System: Combining all the automated stages, can the whole system produce diverse and mean-\ningful robotic skills?\n4.1\nEXPERIMENTAL SETUP\nOur proposed system is generic and agnostic to specific simulation platforms. However, since we\nconsider a wide range of task categories ranging from rigid dynamics to soft body simulation, and\nalso consider skill learning methods such as gradient-based trajectory optimization which necessi-\ntates a differentiable simulation platform, we used Genesis for deploying RoboGen, a simulation\nplatform for robot learning with diverse materials and fully differentiable1. For skill learning, we\nuse SAC (Haarnoja et al., 2018) as the RL algorithm. The policy and Q networks are both Multi-\nlayer Perceptrons (MLP) of size [256, 256, 256], trained with a learning rate of 3e \u2212 4. For each\nsub-task, we train with 1M environment steps. We use BIT\u2217 (Gammell et al., 2015) as the mo-\ntion planning algorithm, and Adam Kingma & Ba (2014) for gradient-based trajectory optimization\nfor soft body manipulation tasks. More implementation details of these learning algorithms can be\nfound in Appendix A.\n4.2\nEVALUATION METRICS AND BASELINES\nWe use the following metrics and baselines for evaluating our system:\nTask Diversity The diversity of the generated tasks can be measured in many aspects, such as the\nsemantic meanings of the tasks, scene configurations of the generated simulation environments, the\nappearances and geometries of the retrieved object assets, and the robot actions required to perform\nthe task. For semantic meanings of the tasks, we perform quantitative evaluations by computing\nthe Self-BLEU and the embedding similarity (Zhu et al., 2018) on the generated task descriptions,\nwhere lower scores indicate better diversity. We compare to established benchmarks, including\nRLBench (James et al., 2020), Maniskill2 (Gu et al., 2023), Meta-World (Yu et al., 2020), and\nBehavior-100 (Srivastava et al., 2022). For object assets and robot actions, we evaluate RoboGen\nqualitatively using the generated simulation environments and visualizations of learned robot skills.\nScene Validity To verify that the retrieved objects match the requirements of the task, we compute\nthe BLIP-2 scores (Li et al., 2023b) between rendered images of the retrieved objects in the simula-\ntion scene, and the text descriptions of the objects. We compare with two ablations of our system. A)\nNo object verification: we do not use a VLM to verify the retrieved object, and just retrieve objects\nbased on text matching. B) No size verification: we do not use object sizes outputted by GPT-4;\ninstead, we use the default size of the asset as provided in Objaverse or PartNetMobility.\nTraining Supervision Validity We perform qualitative evaluations on this by presenting images and\nvideos of the learned skills using the generated decomposition and training supervisions.\nSkill Learning Performance We compare to an ablation where we remove the options of using\nmotion planning-based primitive and gradient-based trajectory optimization, and rely purely on re-\ninforcement learning to learn the skills. Given a task, we run each method with 4 different seeds, and\nreport the mean and std of the task return (we manually verified that the reward functions generated\nby GPT-4 are correct for the evaluated tasks).\nSystem We perform qualitative evaluations of the whole system, by providing videos of over 100\nlearned skills on our websites. In the paper, we show snapshots of representative skills.\n1Genesis is still under development and not yet available to the public. We build our system on top of an\ninternal version, made accessible to us thanks to the courtesy of the development team.\n8\nClose the door\nGrasp the safe door\nOpen the safe door\nRetrieve the gold bar\nMove it to the table\nGrasp the door again\nClose the door\nRotate the knob to lock\nApproach the door\nOpen the door\nGrasp the soup\nPut it in the microwave\nGrasp the timer knob Turn the knob to set timer\n\u201cPut the toy into the storage\u201d\nOpen the door\nGrasp the toy\nMove the toy inside\n\u201cMove the toy car out of the box\u201d\nOpen the box\nRetrieve the toy car\nMove it out of the box\nRelease the toy car\n\u201cRetrieve a gold bar from the safe\u201d\n\u201cHeat up a bowl of soup using the microwave\u201d\nT\nT\nT\nT\nFigure 3: Snapshots of the learned skills on 4 example long-horizon tasks.\n4.3\nRESULTS\nTask Diversity The quantitative evaluation results are presented in Table 1. We compare to a version\nof RoboGen where a total of 106 tasks are generated. As shown, RoboGen achieves the lowest Self-\nBLEU and embedding similarity compared to all previous benchmarks, indicating that the diversity\nof the generated tasks is higher than prior manually established benchmarks under both metrics.\nThis demonstrates that RoboGen can generate a set of tasks whose diversity matches or surpasses\nprior manually crafted skill learning benchmarks and datasets.\nRoboGen\nBehavior-100\nRLbench\nMetaWorld\nManiskill2\nNumber of Tasks\n106\n100\n106\n50\n20\nSelf-BLEU \u2193\n0.284\n0.299\n0.317\n0.322\n0.674\nEmbedding Similarity \u2193\n0.165\n0.210\n0.200\n0.263\n0.194\nTable 1: Task diversity comparison. Note that RoboGen can generate an unbounded number of tasks when\nqueried repeatedly. Here we compare to a version that has a total number of 106 tasks generated, such that\nnumber of tasks is similar to those in prior work.\nScene Validity Figure 4 shows the BLIP-2 score of all compared methods on 7 example generated\ntasks. As shown, removing the size verification leads to drastic decrease in BLIP-2 score. This is\nexpected as the default asset sizes from Objaverse and PartNetMobility can be drastically different\nfrom plausible real-world sizes. The ablation \u201cNo object verification\u201d also has lower BLIP-2 score,\nand with a larger variances, indicating our verification step improves object selection validity. The\nresults demonstrate the importance of using both object and size verification in RoboGen.\nTraining Supervision Validity Figure 3 demonstrates the skills learned with the generated training\nsupervisions from RoboGen, i.e., the task decompositions and reward functions, on 4 example long-\nhorizon tasks. As shown, the robot successfully learns skills to complete the corresponding tasks,\nsuggesting that the automatically generated training supervisions are effective in deriving meaning-\nful and useful skills.\nSkill Learning We evaluate on 4 tasks that involve interactions with articulated objects. The re-\nsults are presented in Table 2. We find that allowing learning algorithm selection is beneficial for\nachieving higher performance for completing the tasks. When only RL is used, the skill learning\ncompletely fails for most tasks.\n9\nFigure 4: We compare the BLIP-2 score of ablations of RoboGen on 7 tasks. Both ablations achieve lower\nscores, showing the importance of both object and size verification.\nSlide Window\nhalfway\nOpen door of\nwashing machine\nOpen\nTrashcan lid\nFlush\nthe toilet\nRoboGen\n\u221231.59 \u00b1 15.05\n\u2212484.80 \u00b1 200.15\n\u2212551.37 \u00b1 128.10\n\u221212.26 \u00b1 2.15\nRoboGen - RL only\n\u2212147.75 \u00b1 1.09\n\u2212926.68 \u00b1 1.44\n\u2212797.46 \u00b1 0.05\n\u2212294.83 \u00b1 1.06\nTable 2: The return decreases drastically if only RL is used for skill learning.\nSystem Figure 1 and 3 visualize some of the generated tasks and learned skills from RoboGen. As\nshown in Figure 1, RoboGen can generate diverse tasks for skill learning spanning from rigid/ar-\nticulated object manipulation, locomotion, and soft body manipulation. Figure 3 further shows that\nRoboGen is able to deliver long-horizon manipulation skills with reasonable decompositions.\nFor extensive qualitative results of proposed tasks and learned skills, please refer to our project site.\n5\nCONCLUSION & LIMITATIONS\nWe introduced RoboGen, a generative agent that automatically proposes and learns diverse robotic\nskills at scale via generative simulation. RoboGen utilizes the latest advancements in foundation\nmodels to automatically generate diverse tasks, scenes, and training supervisions in simulation,\nmaking a foundational step towards scalable robotic skill learning in simulation, while requiring\nminimal human supervision once deployed. Our system is a fully generative pipeline that can be\nqueried endlessly, producing a large number of skill demonstrations associated with diverse tasks\nand environments. RoboGen is agnostic to the backend foundation models, and can be upgraded\ncontinuously using latest models when available.\nOur current system still has several limitations: 1) Large-scale verification of learned skills (i.e.,\nwhether the resultant skill really solves the corresponding task by text descriptions) is still a chal-\nlenge in the current pipeline. This could potentially be addressed by using better multi-modal foun-\ndation models in the future. Contemporary works (Ma et al., 2023) have also explored using envi-\nronment feedback for iterative refinement on the generated supervisions (reward functions), which\nwe hope to integrate into our paradigm in the future. 2) Our paradigm is intrinsically constrained\nby a sim-to-real gap when it comes to real-world deployment. However, with the latest and rapid\nadvancement in physically accurate simulation, as well as techniques such as domain randomization\nand realistic sensory signal rendering, we expect the sim-to-real gap to be narrowed further in the\ncoming future. 3) Our system assumes that with a right reward function, existing policy learning al-\ngorithms are sufficient for learning the proposed skills. For the policy learning algorithms we tested\nin this paper (RL with SAC and an action space of delta end-effector pose, and gradient-based trajec-\ntory optimization), we observe they are still not robust enough and multiple runs are often required\nto produce successful skill demos for certain generated tasks. We leave integration of more power-\nful policy learning algorithms into RoboGen, e.g., those with better action parameterizations (Zeng\net al., 2021; Seita et al., 2023), as future work.\n10\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nIlge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,\nAlex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik\u2019s cube with a\nrobot hand. arXiv preprint arXiv:1910.07113, 2019.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choroman-\nski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action\nmodels transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.\nTao Chen, Jie Xu, and Pulkit Agrawal. A system for general in-hand object re-orientation. In\nConference on Robot Learning, pp. 297\u2013307. PMLR, 2022.\nXuxin Cheng, Ashish Kumar, and Deepak Pathak. Legs as manipulator: Pushing quadrupedal agility\nbeyond locomotion. arXiv preprint arXiv:2303.11330, 2023.\nErwin Coumans and Yunfei Bai.\nPybullet, a python module for physics simulation for games,\nrobotics and machine learning. http://pybullet.org, 2016.\nMurtaza Dalal, Ajay Mandlekar, Caelan Garrett, Ankur Handa, Ruslan Salakhutdinov, and Dieter\nFox. Imitating task and motion planning with visuomotor transformers, 2023.\nPieter-Tjerk De Boer, Dirk P Kroese, Shie Mannor, and Reuven Y Rubinstein. A tutorial on the\ncross-entropy method. Annals of operations research, 134:19\u201367, 2005.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of anno-\ntated 3d objects. arXiv preprint arXiv:2212.08051, 2022.\nMatt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig\nSchmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe of anno-\ntated 3d objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 13142\u201313153, 2023.\nDanny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter,\nAyzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multi-\nmodal language model. arXiv preprint arXiv:2303.03378, 2023.\nYilun Du, Mengjiao Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh Tenenbaum, Dale Schuurmans,\nand Pieter Abbeel. Learning universal policies via text-guided video generation. arXiv preprint\narXiv:2302.00111, 2023.\nJonathan D Gammell, Siddhartha S Srinivasa, and Timothy D Barfoot. Batch informed trees (bit*):\nSampling-based optimal planning via the heuristically guided search of implicit random geo-\nmetric graphs. In 2015 IEEE international conference on robotics and automation (ICRA), pp.\n3067\u20133074. IEEE, 2015.\nChuang Gan, Jeremy Schwartz, Seth Alter, Damian Mrowca, Martin Schrimpf, James Traer, Ju-\nlian De Freitas, Jonas Kubilius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, Kuno Kim,\nElias Wang, Michael Lingelbach, Aidan Curtis, Kevin Feigelis, Daniel M. Bear, Dan Gutfreund,\nDavid Cox, Antonio Torralba, James J. DiCarlo, Joshua B. Tenenbaum, Josh H. McDermott, and\nDaniel L. K. Yamins. Threedworld: A platform for interactive multi-modal physical simulation,\n2021.\nTheophile Gervet, Zhou Xian, Nikolaos Gkanatsios, and Katerina Fragkiadaki. Act3d: Infinite res-\nolution action detection transformer for robotic manipulation. arXiv preprint arXiv:2306.17817,\n2023.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand\nJoulin, and Ishan Misra. Imagebind: One embedding space to bind them all. arXiv preprint\narXiv:2305.05665, 2023.\n11\nGoogle. Bard. https://bard.google.com/, 2022.\nJiayuan Gu, Fanbo Xiang, Xuanlin Li, Zhan Ling, Xiqiang Liu, Tongzhou Mu, Yihe Tang, Stone\nTao, Xinyue Wei, Yunchao Yao, et al. Maniskill2: A unified benchmark for generalizable manip-\nulation skills. arXiv preprint arXiv:2302.04659, 2023.\nHuy Ha and Shuran Song. Flingbot: The unreasonable effectiveness of dynamic manipulation for\ncloth unfolding. In Conference on Robot Learning, pp. 24\u201333. PMLR, 2022.\nHuy Ha, Pete Florence, and Shuran Song. Scaling up and distilling down: Language-guided robot\nskill acquisition. arXiv preprint arXiv:2307.14535, 2023.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International confer-\nence on machine learning, pp. 1861\u20131870. PMLR, 2018.\nTuomas Haarnoja, Ben Moran, Guy Lever, Sandy H Huang, Dhruva Tirumala, Markus Wulfmeier,\nJan Humplik, Saran Tunyasuvunakool, Noah Y Siegel, Roland Hafner, et al. Learning agile soccer\nskills for a bipedal robot with deep reinforcement learning. arXiv preprint arXiv:2304.13653,\n2023.\nEric Heiden, Miles Macklin, Yashraj Narang, Dieter Fox, Animesh Garg, and Fabio Ramos.\nDisect: A differentiable simulation engine for autonomous robotic cutting.\narXiv preprint\narXiv:2105.12244, 2021.\nRongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren, Luping Liu, Mingze Li, Zhenhui Ye, Jinglin\nLiu, Xiang Yin, and Zhou Zhao. Make-an-audio: Text-to-audio generation with prompt-enhanced\ndiffusion models. arXiv preprint arXiv:2301.12661, 2023a.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022.\nWenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li, Jiajun Wu, and Li Fei-Fei. Voxposer:\nComposable 3d value maps for robotic manipulation with language models.\narXiv preprint\narXiv:2307.05973, 2023b.\nStephen James, Zicong Ma, David Rovick Arrojo, and Andrew J Davison. Rlbench: The robot\nlearning benchmark & learning environment. IEEE Robotics and Automation Letters, 5(2):3019\u2013\n3026, 2020.\nMrinal Kalakrishnan, Sachin Chitta, Evangelos Theodorou, Peter Pastor, and Stefan Schaal. Stomp:\nStochastic trajectory optimization for motion planning. In 2011 IEEE international conference\non robotics and automation, pp. 4569\u20134574. IEEE, 2011.\nMinguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung\nPark. Scaling up gans for text-to-image synthesis. arXiv preprint arXiv:2303.05511, 2023.\nIvan Kapelyukh, Vitalis Vosylius, and Edward Johns. Dall-e-bot: Introducing web-scale diffusion\nmodels to robotics. IEEE Robotics and Automation Letters, 2023.\nSertac Karaman and Emilio Frazzoli. Sampling-based algorithms for optimal motion planning. The\ninternational journal of robotics research, 30(7):846\u2013894, 2011.\nPushkal Katara, Zhou Xian, and Katerina Fragkiadaki.\nGen2sim: Scaling up robot learning in\nsimulation with generative models. arXiv preprint arXiv:2310.18308, 2023.\nElia Kaufmann, Antonio Loquercio, Ren\u00b4e Ranftl, Matthias M\u00a8uller, Vladlen Koltun, and Davide\nScaramuzza. Deep drone acrobatics. arXiv preprint arXiv:2006.05768, 2020.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n12\nChengshu Li, Ruohan Zhang, Josiah Wong, Cem Gokmen, Sanjana Srivastava, Roberto Mart\u00b4\u0131n-\nMart\u00b4\u0131n, Chen Wang, Gabrael Levine, Michael Lingelbach, Jiankai Sun, et al. Behavior-1k: A\nbenchmark for embodied ai with 1,000 everyday activities and realistic simulation. In Conference\non Robot Learning, pp. 80\u201393. PMLR, 2023a.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-\nimage pre-training with frozen image encoders and large language models.\narXiv preprint\narXiv:2301.12597, 2023b.\nJacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and\nAndy Zeng. Code as policies: Language model programs for embodied control. arXiv preprint\narXiv:2209.07753, 2022.\nKevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, and Jeannette Bohg. Text2motion:\nFrom natural language instructions to feasible plans. arXiv preprint arXiv:2303.12153, 2023.\nXingyu Lin, Yufei Wang, Jake Olkin, and David Held. Softgym: Benchmarking deep reinforcement\nlearning for deformable object manipulation. arXiv preprint arXiv:2011.07215, 2020.\nXingyu Lin, Zhiao Huang, Yunzhu Li, Joshua B Tenenbaum, David Held, and Chuang Gan. Diff-\nskill: Skill abstraction from differentiable physics for deformable object manipulations with tools.\narXiv preprint arXiv:2203.17275, 2022a.\nXingyu Lin, Yufei Wang, Zixuan Huang, and David Held. Learning visible connectivity dynamics\nfor cloth smoothing. In Conference on Robot Learning, pp. 256\u2013266. PMLR, 2022b.\nC Karen Liu and Dan Negrut. The role of physics-based simulators in robotics. Annual Review of\nControl, Robotics, and Autonomous Systems, 4:35\u201358, 2021.\nHaohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and\nMark D Plumbley.\nAudioldm: Text-to-audio generation with latent diffusion models.\narXiv\npreprint arXiv:2301.12503, 2023a.\nRuoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick.\nZero-1-to-3: Zero-shot one image to 3d object. arXiv preprint arXiv:2303.11328, 2023b.\nZhen Liu, Yao Feng, Michael J Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu.\nMeshdiffusion: Score-based generative 3d mesh modeling. arXiv preprint arXiv:2303.08133,\n2023c.\nAntonio Loquercio, Elia Kaufmann, Ren\u00b4e Ranftl, Matthias M\u00a8uller, Vladlen Koltun, and Davide\nScaramuzza. Learning high-speed flight in the wild. Science Robotics, 6(59):eabg5810, 2021.\nTiange Luo, Chris Rockwell, Honglak Lee, and Justin Johnson. Scalable 3d captioning with pre-\ntrained models. arXiv preprint arXiv:2306.07279, 2023.\nYecheng Jason Ma, William Liang, Guanzhi Wang, De-An Huang, Osbert Bastani, Dinesh Jayara-\nman, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Eureka: Human-level reward design via\ncoding large language models. arXiv preprint arXiv:2310.12931, 2023.\nMiles Macklin, Matthias M\u00a8uller, Nuttapong Chentanez, and Tae-Yong Kim. Unified particle physics\nfor real-time applications. ACM Transactions on Graphics (TOG), 33(4):1\u201312, 2014.\nMichael James McDonald and Dylan Hadfield-Menell. Guided imitation of task and motion plan-\nning, 2021.\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Realfusion: 360 {\\deg}\nreconstruction of any object from a single image. arXiv preprint arXiv:2302.10663, 2023.\nMidjourney. Midjourney. https://www.midjourney.com/, 2022.\nToki Migimatsu and Jeannette Bohg. Object-centric task and motion planning in dynamic environ-\nments. CoRR, abs/1911.04679, 2019. URL http://arxiv.org/abs/1911.04679.\n13\nTongzhou Mu, Zhan Ling, Fanbo Xiang, Derek Yang, Xuanlin Li, Stone Tao, Zhiao Huang, Zhi-\nwei Jia, and Hao Su. Maniskill: Generalizable manipulation skill benchmark with large-scale\ndemonstrations. arXiv preprint arXiv:2107.14483, 2021.\nAdithyavairavan Murali, Arsalan Mousavian, Clemens Eppner, Adam Fishman, and Dieter Fox.\nCabinet: Scaling neural collision detection for object rearrangement with procedural scene gen-\neration, 2023.\nOpenAI. Chatgpt. https://openai.com/blog/chatgpt, 2022.\nOpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023.\nBen Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d\ndiffusion. arXiv preprint arXiv:2209.14988, 2022.\nIlija Radosavovic, Tete Xiao, Bike Zhang, Trevor Darrell, Jitendra Malik, and Koushil Sreenath.\nLearning humanoid locomotion with transformers. arXiv preprint arXiv:2303.03381, 2023.\nNils Reimers and Iryna Gurevych.\nSentence-bert: Sentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:1908.10084, 2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy\noptimization algorithms. arXiv preprint arXiv:1707.06347, 2017.\nDaniel Seita, Yufei Wang, Sarthak J Shetty, Edward Yao Li, Zackory Erickson, and David Held.\nToolflownet: Robotic manipulation with tools via predicting tool flow from point clouds. In\nConference on Robot Learning, pp. 1038\u20131049. PMLR, 2023.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang.\nHugginggpt: Solving ai tasks with chatgpt and its friends in huggingface.\narXiv preprint\narXiv:2303.17580, 2023.\nYunlong Song, Angel Romero, Matthias M\u00a8uller, Vladlen Koltun, and Davide Scaramuzza. Reaching\nthe limit in autonomous racing: Optimal control versus reinforcement learning. Science Robotics,\n8(82):eadg1462, 2023.\nSanjana Srivastava, Chengshu Li, Michael Lingelbach, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, Fei Xia, Kent Elliott\nVainio, Zheng Lian, Cem Gokmen, Shyamal Buch, Karen Liu, et al. Behavior: Benchmark for\neveryday household activities in virtual, interactive, and ecological environments. In Conference\non Robot Learning, pp. 477\u2013490. PMLR, 2022.\nIoan A Sucan, Mark Moll, and Lydia E Kavraki. The open motion planning library. IEEE Robotics\n& Automation Magazine, 19(4):72\u201382, 2012.\nD\u00b4\u0131dac Sur\u00b4\u0131s, Sachit Menon, and Carl Vondrick. Vipergpt: Visual inference via python execution for\nreasoning. arXiv preprint arXiv:2303.08128, 2023.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ international conference on intelligent robots and systems, pp. 5026\u20135033.\nIEEE, 2012.\nMarc Toussaint. Logic-geometric programming: An optimization-based approach to combined task\nand motion planning. In Proceedings of the 24th International Conference on Artificial Intelli-\ngence, IJCAI\u201915, pp. 1930\u20131936. AAAI Press, 2015. ISBN 9781577357384.\n14\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nHsiao-Yu Fish Tung, Zhou Xian, Mihir Prabhudesai, Shamit Lal, and Katerina Fragkiadaki. 3d-oes:\nViewpoint-invariant object-factorized environment simulators. arXiv preprint arXiv:2011.06464,\n2020.\nTsun-Hsuan Wang, Pingchuan Ma, Andrew Everett Spielberg, Zhou Xian, Hao Zhang, Joshua B\nTenenbaum, Daniela Rus, and Chuang Gan. Softzoo: A soft robot co-design benchmark for\nlocomotion in diverse environments. arXiv preprint arXiv:2303.09555, 2023a.\nYen-Jen Wang, Bike Zhang, Jianyu Chen, and Koushil Sreenath. Prompt a robot to walk with large\nlanguage models. arXiv preprint arXiv:2309.09969, 2023b.\nYufei Wang, David Held, and Zackory Erickson. Visual haptic reasoning: Estimating contact forces\nby observing deformable object interactions. IEEE Robotics and Automation Letters, 7(4):11426\u2013\n11433, 2022.\nYufei Wang, Zhanyi Sun, Zackory Erickson, and David Held. One policy to dress them all: Learning\nto dress people with diverse poses and garments. arXiv preprint arXiv:2306.12372, 2023c.\nThomas Weng, Sujay Man Bajracharya, Yufei Wang, Khush Agrawal, and David Held.\nFab-\nricflownet: Bimanual cloth manipulation with a flow-based policy.\nIn Conference on Robot\nLearning, pp. 192\u2013202. PMLR, 2022.\nJimmy Wu, Rika Antonova, Adam Kan, Marion Lepert, Andy Zeng, Shuran Song, Jeannette Bohg,\nSzymon Rusinkiewicz, and Thomas Funkhouser. Tidybot: Personalized robot assistance with\nlarge language models. arXiv preprint arXiv:2305.05658, 2023.\nZhou Xian, Theophile Gervet, Zhenjia Xu, Yi-Ling Qiao, and Tsun-Hsuan Wang. Towards a foun-\ndation model for generalist robots: Diverse skill learning at scale via automated task and scene\ngeneration. arXiv preprint arXiv:2305.10455, 2023a.\nZhou Xian, Nikolaos Gkanatsios, Theophile Gervet, and Katerina Fragkiadaki. Unifying diffusion\nmodels with action detection transformers for multi-task robotic manipulation. In 7th Annual\nConference on Robot Learning, 2023b.\nZhou Xian, Bo Zhu, Zhenjia Xu, Hsiao-Yu Tung, Antonio Torralba, Katerina Fragkiadaki, and\nChuang Gan. Fluidlab: A differentiable environment for benchmarking complex fluid manipula-\ntion. arXiv preprint arXiv:2303.02346, 2023c.\nFanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanx-\niao Jiang, Yifu Yuan, He Wang, et al. Sapien: A simulated part-based interactive environment.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n11097\u201311107, 2020.\nZhenjia Xu, Zhou Xian, Xingyu Lin, Cheng Chi, Zhiao Huang, Chuang Gan, and Shuran Song.\nRoboninja: Learning an adaptive cutting policy for multi-material objects.\narXiv preprint\narXiv:2302.11553, 2023.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng\nLiu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for multimodal\nreasoning and action. arXiv preprint arXiv:2303.11381, 2023.\nTianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey\nLevine. Meta-world: A benchmark and evaluation for multi-task and meta reinforcement learning.\nIn Conference on robot learning, pp. 1094\u20131100. PMLR, 2020.\nTianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson, Anthony Brohan, Su Wang, Jaspiar Singh,\nClayton Tan, Jodilyn Peralta, Brian Ichter, et al. Scaling robot learning with semantically imag-\nined experience. arXiv preprint arXiv:2302.11550, 2023a.\n15\nWenhao Yu, Nimrod Gileadi, Chuyuan Fu, Sean Kirmani, Kuang-Huei Lee, Montse Gonzalez Are-\nnas, Hao-Tien Lewis Chiang, Tom Erez, Leonard Hasenclever, Jan Humplik, et al. Language to\nrewards for robotic skill synthesis. arXiv preprint arXiv:2306.08647, 2023b.\nAndy Zeng, Shuran Song, Johnny Lee, Alberto Rodriguez, and Thomas Funkhouser. Tossingbot:\nLearning to throw arbitrary objects with residual physics. IEEE Transactions on Robotics, 36(4):\n1307\u20131319, 2020.\nAndy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian,\nTravis Armstrong, Ivan Krasin, Dan Duong, Vikas Sindhwani, et al. Transporter networks: Re-\narranging the visual world for robotic manipulation. In Conference on Robot Learning, pp. 726\u2013\n747. PMLR, 2021.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. Texygen:\nA benchmarking platform for text generation models. In The 41st international ACM SIGIR\nconference on research & development in information retrieval, pp. 1097\u20131100, 2018.\nZiwen Zhuang, Zipeng Fu, Jianren Wang, Christopher Atkeson, Soeren Schwertfeger, Chelsea Finn,\nand Hang Zhao. Robot parkour learning. arXiv preprint arXiv:2309.05665, 2023.\n16\nA\nIMPLEMENTATION DETAILS\nFor reinforcement learning, we use SAC (Haarnoja et al., 2018) as the RL algorithm. For both rigid\nobject manipulation and locomotion tasks, the observation space is the low-level state of the objects\nand robot in the task. The policy and Q networks used in SAC are both Multi-layer Perceptrons\n(MLP) of size [256, 256, 256]. We use a learning rate of 3e \u2212 4 for the actor, the critic, and the\nentropy regularizer. The horizon of all manipulation tasks are 100, with a frameskip of 2, and the\nhorizon for all locomotion tasks are 150, with a frameskip of 4. The action of the RL policy is 6d:\nwhere the first 3 elements determines the translation, either as delta translation or target location\n(suggested by GPT-4), and the second 3 elements determines the delta rotation, expressed as delta-\naxis angle in the gripper\u2019s local frame. For each sub-task, we train with 1M environment steps.\nFor action primitives, we use BIT\u2217 (Gammell et al., 2015) implemented in the Open Motion Planning\nLibrary (OMPL) Sucan et al. (2012) as the motion planning algorithm. For the grasping and the\napproaching primitive, we first sample a surface point on the target object or link, then compute a\ngripper pose that aligns the gripper y axis with the normal of the sampled point. The pre-contact\ngripper pose is set to be 0.03m above the surface point along the normal direction. Motion planning\nis then used to find a collision-free path to reach the target gripper pose. After the target gripper\npose is reached, we keep moving the gripper along the normal until contact is made.\nFor soft body manipulation tasks, we use Adam Kingma & Ba (2014) for gradient-based trajectory\noptimization. We run trajectory optimization for 300 gradient steps. We use a learning rate of 0.05\nfor the optimizer. The horizons of all manipulation tasks are either 150 or 200. We use Earth Mover\u2019s\ndistance between object\u2019s current and target shape as the cost function for trajectory optimization.\nFor querying GPT-4, we used a temperature between 0.8 \u2212 1.0 for task proposal to ensure diversity\nin the generated tasks. For all other stages of RoboGen, we use temperature values between 0 \u2212 0.3\nto ensure more robust responses from GPT-4.\nB\nPROMPTS\nPre-defined tasks for example-based initialization of RoboGen.\n\"\"\"\nTask: stack two blocks, with the larger one at the bottom.\nObject: A small block, and a large block.\n\"\"\",\n\"\"\"\nTaks: Put the broccoli on the grill pan\nObjects: a broccoli, a grill pan\n\"\"\",\n\"\"\"\nTask: Put 1 mug on the cup holder\nObjects: A mug, a mug tree holder\n\"\"\",\n\"\"\"\nTask: Pick up the hanger and place it on the clothing rack\nObjects: a cloth hanger, a clothing rack\n\"\"\",\n\"\"\"\nTask: Put 1 book into the bookshelf\nObjects: a book, a bookshelf\n\"\"\",\n\"\"\"\nTaks: Put the knife on the chopping board\nObjects: a kitchen knife, a board\n\"\"\",\n\"\"\"\nTask: Put a old toy in bin\nObjects: A old toy, a rubbish bin\n\"\"\",\n\"\"\"\nTask: Place the dishes and cutlery on the table in preparation for a meal\nObjects: a dish plate, a fork, a spoon, a steak knife\n\"\"\",\n\"\"\"\nTask: Stack one cup on top of the other\n17\nObjects: Two same cups\n\"\"\",\n\"\"\"\nTask: Remove the green pepper from the weighing scales and place it on the floor\nObjects: A green pepper, a weighing scale\n\"\"\",\n\"\"\"\nTask: Put the apple on the weighing scale to weigh it\nObjects: An apple, a weighing scale\n\"\"\",\nIn the following, we show all prompts used for generating an articulated object manipulation task\nusing RoboGen.\nTask Proposal prompt. We show an example where the sampled object is a trashcan for object\nmanipulation task generation.\nI will give you an articulated object, with its articulation tree and semantics. Your goal is to imagine some\ntasks that a robotic arm can perform with this articulated object in household scenarios. You can think\nof the robotic arm as a Franka Panda robot. The task will be built in a simulator for the robot to learn\nit.\nFocus on manipulation or interaction with the object itself. Sometimes the object will have functions, e.g., a\nmicrowave can be used to heat food, in these cases, feel free to include other objects that are needed\nfor the task.\nPlease do not think of tasks that try to assemble or disassemble the object. Do not think of tasks that aim to\nclean the object or check its functionality.\nFor each task you imagined, please write in the following format:\nTask name: the name of the task.\nDescription: some basic descriptions of the tasks.\nAdditional Objects: Additional objects other than the provided articulated object required for completing the\ntask.\nLinks: Links of the articulated objects that are required to perform the task.\n- Link 1: reasons why this link is needed for the task\n- Link 2: reasons why this link is needed for the task\n- ...\nJoints: Joints of the articulated objects that are required to perform the task.\n- Joint 1: reasons why this joint is needed for the task\n- Joint 2: reasons why this joint is needed for the task\n- ...\nExample Input:\n\u2018\u2018\u2018Oven articulation tree\nlinks:\nbase\nlink_0\nlink_1\nlink_2\nlink_3\nlink_4\nlink_5\nlink_6\nlink_7\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_7 child_link: link_0\njoint_name: joint_1 joint_type: continuous parent_link: link_7 child_link: link_1\njoint_name: joint_2 joint_type: continuous parent_link: link_7 child_link: link_2\njoint_name: joint_3 joint_type: continuous parent_link: link_7 child_link: link_3\njoint_name: joint_4 joint_type: continuous parent_link: link_7 child_link: link_4\njoint_name: joint_5 joint_type: continuous parent_link: link_7 child_link: link_5\njoint_name: joint_6 joint_type: continuous parent_link: link_7 child_link: link_6\njoint_name: joint_7 joint_type: fixed parent_link: base child_link: link_7\n\u2018\u2018\u2018\n\u2018\u2018\u2018Oven semantics\nlink_0 hinge door\nlink_1 hinge knob\nlink_2 hinge knob\nlink_3 hinge knob\nlink_4 hinge knob\nlink_5 hinge knob\nlink_6 hinge knob\nlink_7 heavy oven_body\n\u2018\u2018\u2018\nExample output:\nTask Name: Open Oven Door\nDescription: The robotic arm will open the oven door.\nAdditional Objects: None\nLinks:\n- link_0: from the semantics, this is the door of the oven. The robot needs to approach this door in order to\nopen it.\nJoints:\n18\n- joint_0: from the articulation tree, this is the revolute joint that connects link_0. Therefore, the robot\nneeds to actuate this joint for opening the door.\nTask Name: Adjust Oven Temperature\nDescription: The robotic arm will turn one of the oven\u2019s hinge knobs to set a desired temperature.\nAdditional Objects: None\nLinks:\n- link_1: the robot needs to approach link_1, which is assumed to be the temperature knob, to rotate it to set\nthe temperature.\nJoints:\n- joint_1: joint_1 connects link_1 from the articulation tree. The robot needs to actuate it to rotate link_1\nto the desired temperature.\nTask Name: Heat a hamburger Inside Oven\nDescription: The robot arm places a hamburger inside the oven, and sets the oven temperature to be appropriate\nfor heating the hamburger.\nAdditional Objects: hamburger\nLinks:\n- link_0: link_0 is the oven door from the semantics. The robot needs to open the door in order to put the\nhamburger inside the oven.\nlink_1: the robot needs to approach link_1, which is the temperature knob, to rotate it to set the desired\ntemperature.\nJoints:\n- joint_0: from the articulation tree, this is the revolute joint that connects link_0 (the door). Therefore,\nthe robot needs to actuate this joint for opening the door.\n- joint_1: from the articulation tree, joint_1 connects link_1, which is the temperature knob. The robot needs\nto actuate it to rotate link_1 to the desired temperature.\nTask Name: Set Oven Timer\nDescription: The robot arm turns a timer knob to set cooking time for the food.\nAdditional Objects: None.\nLinks:\n- link_2: link_2 is assumed to be the knob for controlling the cooking time. The robot needs to approach\nlink_2 to set the cooking time.\nJoints:\n- joint_2: from the articulation tree, joint_2 connects link_2. The robot needs to actuate joint_2 to rotate\nlink_2 to the desired position, setting the oven timer.\nCan you do the same for the following object:\n\u2018\u2018\u2018TrashCan articulation tree\nlinks:\nbase\nlink_0\nlink_1\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\n\u2018\u2018\u2018\n\u2018\u2018\u2018TrashCan semantics\nlink_0 hinge door\nlink_1 free trashcan_body\n\u2018\u2018\u2018\nAn example response from GPT-4:\nTask Name: Open Trash Can\nDescription: The robotic arm will open the trash can lid.\nAdditional Objects: None\nLinks:\n- link_0: This link represents the lid of the trash can, based on the sematics. The robot needs to approach\nthis lid to open it.\nJoints:\n- joint_0: The joint_0 in the articulation tree connects link_0 (the lid). Thus, the robot needs to actuate\nthis joint to open the lid.\nTask Name: Throw Trash Away\nDescription: The robotic arm places an item of trash inside the trash can.\nAdditional Objects: A pile of trash\nLinks:\n- link_0: Link_0, as the trash can lid, needs to be approached and opened by the robot in order to throw away\nthe trash.\nJoints:\n- joint_0: Joint_0, connecting to link_0, needs to be actuated for the robot to open the trash can lid.\nScene Generation Prompt. Following the above example, we show the scene generation prompt\nfor the task \u201cThrow Trash Away\u201d.\nI need you to describe the initial scene configuration for a given task in the following format, using a yaml\nfile. This yaml file will help build the task in a simulator. The task is for a mobile Franka panda\nrobotic arm to learn a manipulation skill in the simulator. The Franka panda arm is mounted on a floor,\nat location (1, 1, 0). It can move freely on the floor. The z axis is the gravity axis.\n19\nThe format is as follows:\n\u2018\u2018\u2018yaml\n- use_table: whether the task requires using a table. This should be decided based on common sense. If a table\nis used, its location will be fixed at (0, 0, 0). The height of the table will be 0.6m. Usually, if the\nobjects invovled in the task are usually placed on a table (not directly on the ground), then the task\nrequires using a table.\n# for each object involved in the task, we need to specify the following fields for it.\n- type: mesh\nname: name of the object, so it can be referred to in the simulator\nsize: describe the scale of the object mesh using 1 number in meters. The scale should match real everyday\nobjects. E.g., an apple is of scale 0.08m. You can think of the scale to be the longest dimension of\nthe object.\nlang: this should be a language description of the mesh. The language should be a concise description of the\nobejct, such that the language description can be used to search an existing database of objects to\nfind the object.\npath: this can be a string showing the path to the mesh of the object.\non_table: whether the object needs to be placed on the table (if there is a table needed for the task). This\nshould be based on common sense and the requirement of the task. E.g., a microwave is usually placed\non the table.\ncenter: the location of the object center. If there isn\u2019t a table needed for the task or the object does not\nneed to be on the table, this center should be expressed in the world coordinate system. If there is\na table in the task and the object needs to be placed on the table, this center should be expressed in\nterms of the table coordinate, where (0, 0, 0) is the lower corner of the table, and (1, 1, 1) is the\nhigher corner of the table. In either case, you should try to specify a location such that there is\nno collision between objects.\n\u2018\u2018\u2018\nAn example input includes the task names, task descriptions, and objects involved in the task. I will also\nprovide with you the articulation tree and semantics of the articulated object.\nThis can be useful for knowing what parts are already in the articulated object, and thus you do not need to\nrepeat those parts as separate objects in the yaml file.\nYour task includes two parts:\n1. Output the yaml configuration of the task.\n2. Sometimes, the task description / objects involved will refer to generic/placeholder objects, e.g., to\nplace an \"item\" into the drawer, and to heat \"food\" in the microwave. In the generated yaml config, you\nshould change these placeholder objects to be concrete objects in the lang field, e.g., change \"item\" to\nbe a toy or a pencil, and \"food\" to be a hamburger, a bowl of soup, etc.\nExample input:\nTask Name: Insert Bread Slice\nDescription: The robotic arm will insert a bread slice into the toaster.\nObjects involved: Toaster, bread slice. Only the objects specified here should be included in the yaml file.\n\u2018\u2018\u2018Toaster articulation tree\nlinks:\nbase\nlink_0\nlink_1\nlink_2\nlink_3\nlink_4\nlink_5\njoints:\njoint_name: joint_0 joint_type: continuous parent_link: link_5 child_link: link_0\njoint_name: joint_1 joint_type: prismatic parent_link: link_5 child_link: link_1\njoint_name: joint_2 joint_type: prismatic parent_link: link_5 child_link: link_2\njoint_name: joint_3 joint_type: prismatic parent_link: link_5 child_link: link_3\njoint_name: joint_4 joint_type: prismatic parent_link: link_5 child_link: link_4\njoint_name: joint_5 joint_type: fixed parent_link: base child_link: link_5\n\u2018\u2018\u2018\n\u2018\u2018\u2018Toaster semantics\nlink_0 hinge knob\nlink_1 slider slider\nlink_2 slider button\nlink_3 slider button\nlink_4 slider button\nlink_5 free toaster_body\n\u2018\u2018\u2018\nAn example output:\n\u2018\u2018\u2018yaml\n- use_table: True ### Toaster and bread are usually put on a table.\n- type: mesh\nname: \"Toaster\"\non_table: True # Toasters are usually put on a table.\ncenter: (0.1, 0.1, 0) # Remember that when an object is placed on the table, the center is expressed in the\ntable coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.\nHere we put the toaster near the lower corner of the table.\nsize: 0.35 # the size of a toaster is roughly 0.35m\nlang: \"a common toaster\"\npath: \"toaster.urdf\"\n- type: mesh\nname: \"bread slice\"\non_table: True # Bread is usually placed on the table as well.\ncenter: (0.8, 0.7, 0) # Remember that when an object is placed on the table, the center is expressed in the\ntable coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.\nHere we put the bread slice near the higher corner of the table.\nsize: 0.1 # common size of a bread slice\n20\nlang: \"a slice of bread\"\nPath: \"bread_slice.obj\"\n\u2018\u2018\u2018\nAnother example input:\nTask Name: Removing Lid From Pot\nDescription: The robotic arm will remove the lid from the pot.\nObjects involved: KitchenPot. Only the objects specified here should be included in the yaml file.\n\u2018\u2018\u2018KitchenPot articulation tree\nlinks:\nbase\nlink_0\nlink_1\njoints:\njoint_name: joint_0 joint_type: prismatic parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\n\u2018\u2018\u2018\n\u2018\u2018\u2018KitchenPot semantics\nlink_0 slider lid\nlink_1 free pot_body\n\u2018\u2018\u2018\nOutput:\n\u2018\u2018\u2018yaml\n- use_table: True # A kitchen pot is usually placed on the table.\n- type: mesh\nname: \"KitchenPot\"\non_table: True # kitchen pots are usually placed on a table.\ncenter: (0.3, 0.6, 0) # Remember that when an object is placed on the table, the center is expressed in the\ntable coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.\nHere we put the kitchen pot just at a random location on the table.\nsize: 0.28 # the size of a common kitchen pot is roughly 0.28m\nlang: \"a common kitchen pot\"\npath: \"kitchen_pot.urdf\"\n\u2018\u2018\u2018\nNote in this example, the kitchen pot already has a lid from the semantics file. Therefore, you do not need to\ninclude a separate lid in the yaml file.\nOne more example input:\nTask Name: Heat a hamburger in the oven.\nDescription: The robotic arm will put a hamburger in the oven and use the oven to heat it.\nObjects involved: A hamburger, an oven. Only the objects here should be included in the yaml file.\n\u2018\u2018\u2018Oven articulation tree\nlinks:\nbase\nlink_0\nlink_1\nlink_2\nlink_3\nlink_4\nlink_5\nlink_6\nlink_7\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_7 child_link: link_0\njoint_name: joint_1 joint_type: continuous parent_link: link_7 child_link: link_1\njoint_name: joint_2 joint_type: continuous parent_link: link_7 child_link: link_2\njoint_name: joint_3 joint_type: continuous parent_link: link_7 child_link: link_3\njoint_name: joint_4 joint_type: continuous parent_link: link_7 child_link: link_4\njoint_name: joint_5 joint_type: continuous parent_link: link_7 child_link: link_5\njoint_name: joint_6 joint_type: continuous parent_link: link_7 child_link: link_6\njoint_name: joint_7 joint_type: fixed parent_link: base child_link: link_7\n\u2018\u2018\u2018\n\u2018\u2018\u2018Oven semantics\nlink_0 hinge door\nlink_1 hinge knob\nlink_2 hinge knob\nlink_3 hinge knob\nlink_4 hinge knob\nlink_5 hinge knob\nlink_6 hinge knob\nlink_7 heavy oven_body\n\u2018\u2018\u2018\nOutput:\n\u2018\u2018\u2018yaml\n- use_table: True # Although an oven is usually just placed on the ground, a hamburger is usually placed on\nthe table.\n- type: mesh\nname: \"Oven\"\non_table: False # An oven is usually just placed on the floor.\ncenter: (1.0, 0, 0) # Remember that when not on a table, the center is expressed in the world coordinate.\nSince the robot is at (1, 1, 0) and the table is at (0, 0, 0), we place the oven at (1.8, 2, 0) to\navoid collision with the table and the robot.\nsize: 0.9 # the size of an oven is roughly 0.9m\nlang: \"a common oven\"\n21\npath: \"oven.urdf\"\n- type: mesh\nname: \"hamburger\"\non_table: True # a hamburger is usually placed on the table.\ncenter: (0.2, 0.4, 0) # Remember that when an object is placed on the table, the center is expressed in the\ntable coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.\nHere we put the hamburger just at a random location on the table.\nsize: 0.1 # common hamburgers\u2019 diameters are 10 cm.\nlang: \"a common hamburger\"\npath: \"hamburger.obj\"\n\u2018\u2018\u2018\nRules:\n- You do not need to include the robot in the yaml file.\n- The yaml file should only include the objects listed in \"Objects involved\".\n- Sometimes, the task description / objects involved will refer to generic/placeholder objects, e.g., to place\nan \"item\" into the drawer, and to heat \"food\" in the microwave. In the generated yaml config, you\nshould change these placeholder objects to be concrete objects in the lang field, e.g., change \"item\" to\nbe a toy or a pencil, and \"food\" to be a hamburger, a bowl of soup, etc.\nCan you do this for the following task:\nTask Name: Throw Trash Away\nDescription: The robotic arm places an item of trash inside the trash can\nObjects involved: TrashCan, A pile of trash\n\u2018\u2018\u2018TrashCan articulation tree\nlinks:\nbase\nlink_0\nlink_1\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\n\u2018\u2018\u2018\n\u2018\u2018\u2018TrashCan semantics\nlink_0 hinge door\nlink_1 free trashcan_body\n\u2018\u2018\u2018\nAn example response from GPT-4:\nOutput:\n\u2018\u2018\u2018yaml\n- use_table: False # Trash cans are usually placed on the floor.\n- type: mesh\nname: \"TrashCan\"\non_table: False # Trash cans are usually placed on the floor.\ncenter: (1.5, 1.5, 0) # Remember that when not on a table, the center is expressed in the world coordinate.\nSince the robot is at (1, 1, 0) and the table is at (0, 0, 0), we place the trash can at (1.5, 1.5, 0)\nto avoid collision with the table and the robot.\nsize: 0.6 # the size of a common trash can is roughly 0.6m\nlang: \"a common trash can\"\npath: \"trashcan.urdf\"\n- type: mesh\nname: \"Trash\"\non_table: True # Trash is usually picked up from a table or the floor. Here we assume it\u2019s on the table.\ncenter: (0.5, 0.5, 0) # Remember that when an object is placed on the table, the center is expressed in the\ntable coordinate, where (0, 0, 0) is the lower corner and (1, 1, 1) is the higher corner of the table.\nHere we put the trash in the middle of the table.\nsize: 0.1 # common size of a pile of trash\nlang: \"a pile of paper waste\" # Note here, we changed the generic/placeholder \"pile of trash\" object to be a\nmore concrete object: a pile of paper waste.\npath: \"paper_waste.obj\"\n\u2018\u2018\u2018\nMore relevant assets prompt.\nGiven a task, which is for a mobile Franka panda robotic arm to learn a manipulation skill in the simulator,\nyour goal is to add more objects into the task scene such that the scene looks more realistic. The\nFranka panda arm is mounted on a floor, at location (1, 1, 0). It can move freely on the floor. The z\naxis is the gravity axis.\nThe input to you includes the following:\nTask name, task description, the essential objects involved in the task, and a config describing the current\ntask scene, which contains only the essential objects needed for the task. The config is a yaml file in\nthe following format:\n\u2018\u2018\u2018yaml\n- use_table: whether the task requires using a table. This should be decided based on common sense. If a table\nis used, its location will be fixed at (0, 0, 0). The height of the table will be 0.6m.\n# for each object involved in the task, we need to specify the following fields for it.\n- type: mesh\nname: name of the object, so it can be referred to in the simulator\nsize: describe the scale of the object mesh using 1 number in meters. The scale should match real everyday\nobjects. E.g., an apple is of scale 0.08m. You can think of the scale to be the longest dimension of\nthe object.\n22\nlang: this should be a language description of the mesh. The language should be a bit detailed, such that\nthe language description can be used to search an existing database of objects to find the object.\npath: this can be a string showing the path to the mesh of the object.\non_table: whether the object needs to be placed on the table (if there is a table needed for the task). This\nshould be based on common sense and the requirement of the task.\ncenter: the location of the object center. If there isn\u2019t a table needed for the task or the object does not\nneed to be on the table, this center should be expressed in the world coordinate system. If there is\na table in the task and the object needs to be placed on the table, this center should be expressed in\nterms of the table coordinate, where (0, 0, 0) is the lower corner of the table, and (1, 1, 1) is the\nhigher corner of the table. In either case, you should try to specify a location such that there is\nno collision between objects.\n\u2018\u2018\u2018\nYour task is to think about what other distractor objects can be added into the scene to make the scene more\ncomplex and realistic for the robot to learn the task. These distractor objects are not necessary for\nthe task itself, but their existence makes the scene look more interesting and complex. You should\noutput the distractor objects using the same format as the input yaml file. You should try to put these\ndistractor objects at locations such that they don\u2019t collide with objects already in the scene.\nHere is one example:\nInput:\nTask name: Heat up a bowl of soup in the microwave\nTask description: The robot will grab the soup and move it into the microwave, and then set the temperature to\nheat it.\nObjects involved: Microwave, a bowl of soup\nConfig:\n\u2018\u2018\u2018yaml\n-\nuse_table: true\n-\ncenter: (0.3, 0.7, 0)\nlang: A standard microwave with a turntable and digital timer\nname: Microwave\non_table: true\npath: microwave.urdf\nsize: 0.6\ntype: urdf\n-\ncenter: (0.2, 0.2, 0)\nlang: A ceramic bowl full of soup\nname: Bowl of Soup\non_table: true\npath: bowl_soup.obj\nsize: 0.15\ntype: mesh\n\u2018\u2018\u2018\nOutput:\n\u2018\u2018\u2018yaml\n- name: plate # a plate is a common object placed when there is microwave and bowl of soup, in a kitchen setup\nlang: a common kitchen plate\non_table: True\ncenter: (0.8, 0.8, 0)\ntype: mesh\npath: \"plate.obj\"\nsize: 0.15 # a plate is usually of scale 0.15m\n- name: sponge # a sponge is a common object placed when there is microwave and bowl of soup, in a kitchen\nsetup\nlang: a common sponge\non_table: True\ncenter: (0.5, 0.2, 0)\ntype: mesh\npath: \"sponge.obj\"\nsize: 0.1 # a sponge is usually of scale 0.1m\n- name: Oven # a oven is a common object placed when there is microwave and bowl of soup, in a kitchen setup\nlang: a kitchen oven\non_table: False # an oven is usually a standalone object on the floor\ncenter: (1.8, 0.5, 0) # remember robot is at (1, 1, 0) and table is at (0, 0, 0). So the oven is placed at\n(1.8, 0.5, 0) in the world coordinate system to avoid collision with other objects.\ntype: mesh\npath: \"oven.obj\"\nsize: 0.8 # an oven is usually of scale 0.8m\n\u2018\u2018\u2018\nCan you do it for the following task:\nTask name: Throw Trash Away\nTask description: The robotic arm places an item of trash inside the trash can\nInitial config:\n\u2018\u2018\u2018yaml\n- use_table: false\n- center: (1.5, 1.5, 0)\nlang: a common trash can\nname: TrashCan\non_table: false\npath: trashcan.urdf\nsize: 0.6\ntype: urdf\n- center: (0.5, 0.5, 0)\nlang: a pile of paper waste\nname: Trash\non_table: true\npath: paper_waste.obj\nsize: 0.05\n23\ntype: mesh\n\u2018\u2018\u2018\nAn example response from GPT-4:\nSure, here is the output for the given task:\n\u2018\u2018\u2018yaml\n- name: broom # a broom is a common object found near a trash can\nlang: a common broom\non_table: False\ncenter: (2, 1.5, 0)\ntype: mesh\npath: \"broom.obj\"\nsize: 1.2 # a broom is usually of scale 1.2m\n- name: dustpan # a dustpan is a common object found near a trash can\nlang: a common dustpan\non_table: False\ncenter: (2, 2, 0)\ntype: mesh\npath: \"dustpan.obj\"\nsize: 0.3 # a dustpan is usually of scale 0.3m\n- name: recycling_bin # a recycling bin is a common object found near a trash can\nlang: a common recycling bin\non_table: False\ncenter: (1, 2, 0)\ntype: mesh\npath: \"recycling_bin.obj\"\nsize: 0.6 # a recycling bin is usually of scale 0.6m\n- name: soda_can # a soda can is a common object found near a trash can\nlang: a common soda can\non_table: True\ncenter: (0.7, 0.7, 0)\ntype: mesh\npath: \"soda_can.obj\"\nsize: 0.1 # a soda can is usually of scale 0.1m\n\u2018\u2018\u2018\nAsset size adjustment prompt.\nFor each object used in the scene, we query GPT-4 to correct its\nsize such that the size matches the real-world size. We continue the above example of throw trash\naway.\nA robotic arm is trying to manipulate some objects to learn corresponding skills in a simulator. However, the\nsize of the objects might be wrong. Your task is to adjust the size of the objects, such that they match\neach other when interact with each other; and the size should also match what is commonly seen in\neveryday life, in household scenarios.\nNow I will give you the name of the task, the object and their sizes, please correct any unreasonable sizes.\nObjects are represented using a mesh file, you can think of size as the longest dimension of the object.\nI will write in the following format:\n\u2018\u2018\u2018\nTask: task description\nobj1, mesh, size\nobj2, mesh, size\n\u2018\u2018\u2018\nPlease reply in the following format:\nexplanations of why some size is not reasonable.\n\u2018\u2018\u2018yaml\nobj1, mesh, corrected_size\nobj2, mesh, corrected_radius\n\u2018\u2018\u2018\nHere is an example:\nInput:\n\u2018\u2018\u2018\nTask: The robotic arm lowers the toilet seat from an up position to a down position\nToilet, mesh, 0.2\n\u2018\u2018\u2018\nOutput:\nA toilet is usually 0.6 - 0.8m in its back height, so the size is not reasonable -- it is a bit too small.\nBelow is the corrected size.\n\u2018\u2018\u2018yaml\nToilet, mesh, 0.7\n\u2018\u2018\u2018\nAnother example:\nInput:\n\u2018\u2018\u2018\nTask: Fill a cup with water under the faucet\nFaucet, mesh, 0.25\nCup, mesh, 0.3\n\u2018\u2018\u2018\nOutput:\n24\nThe size of the faucet makes senes. However, the size of the cup is too large for 2 reasons: it does not match\nthe size of tha faucet for getting water under the faucet; and it is not a common size of cup in\neveryday life. Below is the corrected size.\n\u2018\u2018\u2018yaml\nFaucet, mesh, 0.25\nCup, mesh, 0.12\n\u2018\u2018\u2018\nOne more example to show that even if no change is needed, you should still reply with the same size.\nInput:\n\u2018\u2018\u2018\nTask: Open Table Drawer The robotic arm will open a table drawer\ntable, mesh, 0.8\n\u2018\u2018\u2018\nOutput:\nThe size of the table is reasonable, so no change is needed.\n\u2018\u2018\u2018yaml\ntable, mesh, 0.8\n\u2018\u2018\u2018\nThis is also a good example to show that sometimes, the task description might include two objects, e.g., a\ntable and a drawer, yet there is only one object size provided (here the table). This is not an error,\nbut that the other object is part of the provided object, i.e., here the drawer is part of the table. It\n\u2019s fine, you should then just reply with the corrected size of the object provided, here, the table, in\nsuch cases.\n\u2018\u2018\u2018\nTask: Throw Trash Away The robotic arm places an item of trash inside the trash can\ntrashcan, mesh, 0.6\ntrash, mesh, 0.1\n\u2018\u2018\u2018\nAn example response from GPT-4:\nThe size of the trashcan is reasonable as it is a common size for a household trashcan. However, the size of\nthe trash is a bit too large. Trash items are usually smaller to fit inside the trashcan. Below is the\ncorrected size.\n\u2018\u2018\u2018yaml\ntrashcan, mesh, 0.6\ntrash, mesh, 0.05\n\u2018\u2018\u2018\nFor scene generation, we also need to specify the asset configuration, and any special scene config-\nuration.\nAdjusting articulated object joint prompt. We continue the example of throwing the trash away.\nYour goal is to set the\njoint angles of some articulated objects to the right value in the initial state,\ngiven a task. The task is for a robot arm to learn the corresponding skills to manipulate the\narticulated object.\nThe input to you will include the task name, a short description of the task, the articulation tree of the\narticulated object, a semantic file of the articulated object, the links and joints of the articulated\nobjects that will be involved in the task, and the substeps for doing the task.\nYou should output for each joint involved in the task, what joint value it should be set to. You should output\na number in the range [0, 1], where 0 corresponds to the lower limit of that joint angle, and 1\ncorresponds to the upper limit of the joint angle. You can also output a string of \"random\", which\nindicates to sample the joint angle within the range.\nBy default, the joints in an object are set to their lower joint limits. You can assume that the lower joint\nlimit corresponds to the natural state of the articulated object. E.g., for a door\u2019s hinge joint, 0\nmeans it is closed, and 1 means it is open. For a lever, 0 means it is unpushed, and 1 means it is\npushed to the limit.\nHere is an example:\nInput:\nTask Name: Close the door\nDescription: The robot arm will close the door after it was opened.\n\u2018\u2018\u2018door articulation tree\nlinks:\nbase\nlink_0\nlink_1\nlink_2\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\njoint_name: joint_2 joint_type: revolute parent_link: link_0 child_link: link_2\n\u2018\u2018\u2018\n\u2018\u2018\u2018door semantics\nlink_0 hinge rotation_door\nlink_1 static door_frame\n25\nlink_2 hinge rotation_door\n\u2018\u2018\u2018\nLinks:\n- link_0: link_0 is the door. This is the part of the door assembly that the robot needs to interact with.\nJoints:\n- joint_0: Joint_0 is the revolute joint connecting link_0 (the door) as per the articulation tree. The robot\nneeds to actuate this joint cautiously to ensure the door is closed.\nsubsteps:\napproach the door\nclose the door\nOutput:\nThe goal is for the robot arm to learn to close the door after it is opened. Therefore, the door needs to be\ninitially opened, thus, we are setting its value to 1, which corresponds to the upper joint limit.\n\u2018\u2018\u2018joint values\njoint_0: 1\n\u2018\u2018\u2018\nAnother example:\nTask Name: Turn Off Faucet\nDescription: The robotic arm will turn the faucet off by manipulating the switch\n\u2018\u2018\u2018Faucet articulation tree\nlinks:\nbase\nlink_0\nlink_1\njoints:\njoint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0\njoint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1\n\u2018\u2018\u2018\n\u2018\u2018\u2018Faucet semantics\nlink_0 static faucet_base\nlink_1 hinge switch\n\u2018\u2018\u2018\nLinks:\n- link_0: link_0 is the door. This is the part of the door assembly that the robot needs to interact with.\nJoints:\n- joint_0: Joint_0 is the revolute joint connecting link_0 (the door) as per the articulation tree. The robot\nneeds to actuate this joint cautiously to ensure the door is closed.\nsubsteps:\ngrasp the faucet switch\nturn off the faucet\nOutput:\nFor the robot to learn to turn off the faucet, it cannot be already off initially. Therefore, joint_1 should\nbe set to its upper joint limit, or any value that is more than half of the joint range, e.g., 0.8.\n\u2018\u2018\u2018joint value\njoint_1: 0.8\n\u2018\u2018\u2018\nOne more example:\nTask Name: Store an item inside the Drawer\nDescription: The robot arm picks up an item and places it inside the drawer of the storage furniture\n\u2018\u2018\u2018StorageFurniture articulation tree\nlinks:\nbase\nlink_0\nlink_1\nlink_2\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\njoint_name: joint_2 joint_type: prismatic parent_link: link_1 child_link: link_2\n\u2018\u2018\u2018\n\u2018\u2018\u2018StorageFurniture semantics\nlink_0 hinge rotation_door\nlink_1 heavy furniture_body\nlink_2 slider drawer\n\u2018\u2018\u2018\nLinks:\n- link_2: link_2 is the drawer link from the semantics. The robot needs to open this drawer to place the item\ninside.\nJoints:\n- joint_2: joint_2, from the articulation tree, connects to link_2 (the drawer). Thus, the robot would need to\nactuate this joint to open the drawer to store the item.\nsubsteps:\ngrasp the drawer\nopen the drawer\ngrasp the item\n26\nput the item into the drawer\ngrasp the drawer again\nclose the drawer\nrelease the grasp\nOutput:\nThis task involves putting one item into the drawer of the storage furniture. As noted in the substeps, the\nrobot needs to first open the drawer, put the item in, and then close it. Since the articulated object\nis initialized with the lower joint limit, i.e., the drawer is initially closed, it aligns with the task\nwhere the robot needs to first learn to open the drawer. Therefore, no particular joint angle needs to\nbe set, and we just output None.\n\u2018\u2018\u2018joint value\nNone\n\u2018\u2018\u2018\nCan you do it for the following task:\nTask Name: Throw Trash Away\nDescription: The robotic arm places an item of trash inside the trash can\n\u2018\u2018\u2018TrashCan articulation tree\nlinks:\nbase\nlink_0\nlink_1\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\n\u2018\u2018\u2018\n\u2018\u2018\u2018TrashCan semantics\nlink_0 hinge door\nlink_1 free trashcan_body\n\u2018\u2018\u2018\nLinks:\nlink_0: Link_0, as the trash can lid, needs to be approached and opened by the robot in order to throw away\nthe trash.\nJoints:\njoint_0: Joint_0, connecting to link_0, needs to be actuated for the robot to open the trash can lid.\nsubsteps:\ngrasp the trash can lid\nopen the trash can lid\ngrasp the trash\nput the trash into the trash can\nrelease the trash\ngrasp the trash can lid again\nclose the trash can lid\nAn example output from GPT-4:\nOutput:\nThe task involves the robot learning to throw trash away. The robot needs to learn to open the trash can lid,\nthrow the trash in, and then close the lid. Therefore, we need to set the initial joint angle such that\nthe trash can lid is not already open. We can set joint_0 to its lower limit, which corresponds to the\ntrash can lid being closed.\n\u2018\u2018\u2018joint value\njoint_0: 0\n\u2018\u2018\u2018\nSpecial spatial relationship prompt. We keep the \u201cThrow away trash\u201d example.\nYour goal is to output any special spatial relationships certain objects should have in the initial state,\ngiven a task. The task is for a robot arm to learn the corresponding skills in household scenarios.\nThe input to you will include\nthe task name,\na short description of the task,\nobjects involved in the task,\nsubsteps for performing the task,\nIf there is an articulated object involved in the task, the articulation tree of the articulated object, the\nsemantic file of the articulated object, and the links and joints of the articulated objects that will\nbe involved in the task.\nWe have the following spatial relationships:\non, obj_A, obj_B: object A is on top of object B, e.g., a fork on the table.\nin, obj_A, obj_B: object A is inside object B, e.g., a gold ring in the safe.\nin, obj_A, obj_B, link_name: object A is inside the link with link_name of object B. For example, a table\nmight have two drawers, represented with link_0, and link_1, and in(pen, table, link_0) would be that a\npen is inside one of the drawers that corresponds to link_0.\n27\nGiven the input to you, you should output any needed spatial relationships of the involved objects.\nHere are some examples:\nInput:\nTask Name:Fetch Item from Refrigerator\nDescription: The robotic arm will open a refrigerator door and reach inside to grab an item and then close the\ndoor.\nObjects involved: refrigerator, item\n\u2018\u2018\u2018refrigerator articulation tree\nlinks:\nbase\nlink_0\nlink_1\nlink_2\njoints:\njoint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0\njoint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1\njoint_name: joint_2 joint_type: revolute parent_link: link_0 child_link: link_2\n\u2018\u2018\u2018\n\u2018\u2018\u2018refrigerator semantics\nlink_0 heavy refrigerator_body\nlink_1 hinge door\nlink_2 hinge door\n\u2018\u2018\u2018\nLinks:\nlink_1: The robot needs to approach and open this link, which represents one of the refrigerator doors, to\nreach for the item inside.\nJoints:\njoint_1: This joint connects link_1, representing one of the doors. The robot needs to actuate this joint to\nopen the door, reach for the item, and close the door.\nsubsteps:\ngrasp the refrigerator door\nopen the refrigerator door\ngrasp the item\nmove the item out of the refrigerator\ngrasp the refrigerator door again\nclose the refrigerator door\nOutput:\nThe goal is for the robot arm to learn to retrieve an item from the refrigerator. Therefore, the item needs to\nbe initially inside the refrigerator. From the refrigerator semantics we know that link_0 is the body\nof the refrigerator, therefore we should have a spatial relationship as the following:\n\u2018\u2018\u2018spatial relationship\nIn, item, refrigerator, link_0\n\u2018\u2018\u2018\nAnother example:\nTask Name: Turn Off Faucet\nDescription: The robotic arm will turn the faucet off by manipulating the switch\nObjects involved: faucet\n\u2018\u2018\u2018Faucet articulation tree\nlinks:\nbase\nlink_0\nlink_1\njoints:\njoint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0\njoint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1\n\u2018\u2018\u2018\n\u2018\u2018\u2018Faucet semantics\nlink_0 static faucet_base\nlink_1 hinge switch\n\u2018\u2018\u2018\nLinks:\nlink_0: link_0 is the door. This is the part of the door assembly that the robot needs to interact with.\nJoints:\njoint_0: Joint_0 is the revolute joint connecting link_0 (the door) as per the articulation tree. The robot\nneeds to actuate this joint cautiously to ensure the door is closed.\nsubsteps:\ngrasp the faucet switch\nturn off the faucet\nOutput:\nThere is only 1 object involved in the task, thus no special spatial relationships are required.\n\u2018\u2018\u2018spatial relationship\nNone\n\u2018\u2018\u2018\n28\nOne more example:\nTask Name: Store an item inside the Drawer\nDescription: The robot arm picks up an item and places it inside the drawer of the storage furniture.\nObjects involved: storage furniture, item\n\u2018\u2018\u2018StorageFurniture articulation tree\nlinks:\nbase\nlink_0\nlink_1\nlink_2\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\njoint_name: joint_2 joint_type: prismatic parent_link: link_1 child_link: link_2\n\u2018\u2018\u2018\n\u2018\u2018\u2018StorageFurniture semantics\nlink_0 hinge rotation_door\nlink_1 heavy furniture_body\nlink_2 slider drawer\n\u2018\u2018\u2018\nLinks:\nlink_2: link_2 is the drawer link from the semantics. The robot needs to open this drawer to place the item\ninside.\nJoints:\njoint_2: joint_2, from the articulation tree, connects to link_2 (the drawer). Thus, the robot would need to\nactuate this joint to open the drawer to store the item.\nsubsteps:\ngrasp the drawer\nopen the drawer\ngrasp the item\nput the item into the drawer\ngrasp the drawer again\nclose the drawer\nrelease the grasp\nOutput:\nThis task involves putting one item into the drawer of the storage furniture. The item should initially be\noutside of the drawer, such that the robot can learn to put it into the drawer. Therefore, no special\nrelationships of in or on are needed. Therefore, no special spatial relationships are needed.\n\u2018\u2018\u2018spatial relationship\nNone\n\u2018\u2018\u2018\nCan you do it for the following task:\nTask Name: Throw Trash Away\nDescription: The robotic arm places an item of trash inside the trash can\nObjects involved: TrashCan, Trash\n\u2018\u2018\u2018TrashCan articulation tree\nlinks:\nbase\nlink_0\nlink_1\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\n\u2018\u2018\u2018\n\u2018\u2018\u2018TrashCan semantics\nlink_0 hinge door\nlink_1 free trashcan_body\n\u2018\u2018\u2018\nLinks:\nlink_0: Link_0, as the trash can lid, needs to be approached and opened by the robot in order to throw away\nthe trash.\nJoints:\njoint_0: Joint_0, connecting to link_0, needs to be actuated for the robot to open the trash can lid.\nsubsteps:\ngrasp the trash can lid\nopen the trash can lid\ngrasp the trash\nput the trash into the trash can\nrelease the trash\ngrasp the trash can lid again\nclose the trash can lid\n29\nAn example output from GPT-4:\nOutput:\nThe task involves the robot arm placing an item of trash into the trash can. Initially, the trash should not\nbe inside the trash can, so the robot can learn to put it in. Therefore, no special spatial\nrelationships are needed.\n\u2018\u2018\u2018spatial relationship\nNone\n\u2018\u2018\u2018\nTraining supervision prompt.\nA robotic arm is trying to solve some household object manipulation tasks to learn corresponding skills in a\nsimulator.\nWe will provide with you the task description, the initial scene configurations of the task, which contains\nthe objects in the task and certain information about them.\nYour goal is to decompose the task into executable sub-steps for the robot, and for each substep, you should\neither call a primitive action that the robot can execute, or design a reward function for the robot to\nlearn, to complete the substep.\nFor each substep, you should also write a function that checks whether the substep has been successfully\ncompleted.\nCommon substeps include moving towards a location, grasping an object, and interacting with the joint of an\narticulated object.\nAn example task:\nTask Name: Fetch item from refrigerator\nDescription: The robotic arm will open a refrigerator door reach inside to grab an item, place it on the table\n, and then close the door\nInitial config:\n\u2018\u2018\u2018yaml\n-\nuse_table: true\n-\ncenter: (1.2, 0, 0)\nlang: a common two-door refrigerator\nname: Refrigerator\non_table: false\npath: refrigerator.urdf\nsize: 1.8\ntype: urdf\n-\ncenter: (1.2, 0, 0.5)\nlang: a can of soda\nname: Item\non_table: false\npath: soda_can.obj\nsize: 0.2\ntype: mesh\n\u2018\u2018\u2018\nI will also give you the articulation tree and semantics file of the articulated object in the task. Such\ninformation will be useful for writing the reward function/the primitive actions, for example, when the\nreward requires accessing the joint value of a joint in the articulated object, or the position of a\nlink in the articulated object, or when the primitive needs to access a name of the object.\n\u2018\u2018\u2018Refrigerator articulation tree\nlinks:\nbase\nlink_0\nlink_1\nlink_2\njoints:\njoint_name: joint_0 joint_type: fixed parent_link: base child_link: link_0\njoint_name: joint_1 joint_type: revolute parent_link: link_0 child_link: link_1\njoint_name: joint_2 joint_type: revolute parent_link: link_0 child_link: link_2\n\u2018\u2018\u2018\n\u2018\u2018\u2018Refrigerator semantics\nlink_0 heavy refrigerator_body\nlink_1 hinge door\nlink_2 hinge door\n\u2018\u2018\u2018\nI will also give you the links and joints of the articulated object that will be used for completing the task:\nLinks:\nlink_1: This link is one of the refrigerator doors, which the robot neesd to reach for the item inside.\nJoints:\njoint_1: This joint connects link_1, representing one of the doors. The robot needs to actuate this joint to\nopen the door, reach for the item, and close the door.\nFor each substep, you should decide whether the substep can be achieved by using the provided list of\nprimitives. If not, you should then write a reward function for the robot to learn to perform this\nsubstep.\nIf you choose to write a reward function for the substep, you should also specify the action space of the\nrobot when learning this reward function.\nThere are 2 options for the action space: \"delta-translation\", where the action is the delta translation of\nthe robot end-effector, suited for local movements; and \"normalized-direct-translation\", where the\naction specifies the target location the robot should move to, suited for moving to a target location.\nFor each substep, you should also write a condition that checks whether the substep has been successfully\ncompleted.\n30\nHere is a list of primitives the robot can do. The robot is equipped with a suction gripper, which makes it\neasy for the robot to grasp an object or a link on an object.\ngrasp_object(self, object_name): the robot arm will grasp the object specified by the argument object name.\ngrasp_object_link(self, object_name, link_name): some object like an articulated object is composed of\nmultiple links. The robot will grasp a link with link_name on the object with object_name.\nrelease_grasp(self): the robot will release the grasped object.\napproach_object(self, object_name): this function is similar to grasp_object, except that the robot only\napproaches the object, without grasping it.\napproach_object_link(self, object_name, link_name): this function is similar to grasp_object_link, except that\nthe robot only approaches the object\u2019s link, without grasping it.\nNote that all primitives will return a tuple (rgbs, final_state) which represents the rgb images of the\nexecution process and the final state of the execution process.\nYou should always call the primitive in the following format:\nrgbs, final_state = some_primitive_function(self, arg1, ..., argn)\nHere is a list of helper functions that you can use for designing the reward function or the success condition\n:\nget_position(self, object_name): get the position of center of mass of object with object_name.\nget_orientation(self, object_name): get the orientation of an object with object_name.\ndetect(self, object_name, object_part): detect the position of a part in object. E.g., the opening of a\ntoaster, or the handle of a door.\nget_joint_state(self, object_name, joint_name): get the joint angle value of a joint in an object.\nget_joint_limit(self, object_name, joint_name): get the lower and upper joint angle limit of a joint in an\nobject, returned as a 2-element tuple.\nget_link_state(self, object_name, link_name): get the position of the center of mass of the link of an object.\nget_eef_pos(self): returns the position, orientation of the robot end-effector as a list.\nget_bounding_box(self, object_name): get the axis-aligned bounding box of an object. It returns the min and\nmax xyz coordinate of the bounding box.\nget_bounding_box_link(self, object_name, link_name): get the axis-aligned bounding box of the link of an\nobject. It returns the min and max xyz coordinate of the bounding box.\nin_bbox(self, pos, bbox_min, bbox_max): check if pos is within the bounding box with the lowest corner at\nbbox_min and the highest corner at bbox_max.\nget_grasped_object_name(self): return the name of the grasped object. If no object is grasped by the robot,\nreturn None. The name is automatically converted to the lower case.\nget_grasped_object_and_link_name(self): return a tuple, the first is the name of the grasped object, and the\nsecond is the name of the grasped link. If no object is grasped by the robot, return (None, None). The\nname is automatically converted to the lower case.\ngripper_close_to_object(self, object_name): return true if the robot gripper is close enough to the object\nspecified by object_name, otherwise false.\ngripper_close_to_object_link(self, object_name, link_name): return true if the robot gripper is close enough\nto the object link, otherwise false.\nYou can assume that for objects, the lower joint limit corresponds to their natural state, e.g., a box is\nclosed with the lid joint being 0, and a lever is unpushed when the joint angle is 0.\nFor the above task \"Fetch item from refrigerator\", it can be decomposed into the following substeps,\nprimitives, and reward functions:\nsubstep 1: grasp the refrigerator door\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object_link(self, \"Refrigerator\", \"link_1\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"Refrigerator\".lower() and grasped_link == \"link_1\".lower())\n\u2018\u2018\u2018\nsubstep 2: open the refrigerator door\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# this reward encourages the end-effector to stay near door to grasp it.\neef_pos = get_eef_pos(self)[0]\ndoor_pos = get_link_state(self, \"Refrigerator\", \"link_1\")\nreward_near = -np.linalg.norm(eef_pos - door_pos)\n# Get the joint state of the door. We know from the semantics and the articulation tree that joint_1\nconnects link_1 and is the joint that controls the rotation of the door.\njoint_angle = get_joint_state(self, \"Refrigerator\", \"joint_1\")\n# The reward is the negative distance between the current joint angle and the joint angle when the door is\nfully open (upper limit).\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"Refrigerator\", \"joint_1\")\ntarget_joint_angle = joint_limit_high\ndiff = np.abs(joint_angle - target_joint_angle)\nreward_joint =\n-diff\nreward = reward_near + 5 * reward_joint\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nIn the last substep the robot already grasps the door, thus only local movements are needed to open it.\nsubstep 3: grasp the item\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object(self, \"Item\")\nsuccess = get_grasped_object_name(self) == \"Item\".lower()\n\u2018\u2018\u2018\n31\nsubstep 4: move the item out of the refrigerator\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# Get the current item position\nitem_position = get_position(self, \"Item\")\n# The first reward encourages the end-effector to stay near the item\neef_pos = get_eef_pos(self)[0]\nreward_near = -np.linalg.norm(eef_pos - item_position)\n# The reward is to encourage the robot to grasp the item and move the item to be on the table.\n# The goal is not to just move the soda can to be at a random location out of the refrigerator. Instead,\nwe need to place it somewhere on the table.\n# This is important for moving an object out of a container style of task.\ntable_bbox_low, table_bbox_high = get_bounding_box(self, \"init_table\") # the table is referred to as \"\ninit_table\" in the simulator.\ntable_bbox_range = table_bbox_high - table_bbox_low\n# target location is to put the item at a random location on the table\ntarget_location = np.zeros(3)\ntarget_location[0] = table_bbox_low[0] + 0.2 * table_bbox_range[0] # 0.2 is a random chosen number, any\nnumber in [0, 1] should work\ntarget_location[1] = table_bbox_low[1] + 0.3 * table_bbox_range[1] # 0.3 is a random chosen number, any\nnumber in [0, 1] should work\ntarget_location[2] = table_bbox_high[2] # the height should be the table height\ndiff = np.linalg.norm(item_position - target_location)\nreward_distance = -diff\nreward = reward_near + 5 * reward_distance\nsuccess = diff < 0.06\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\nnormalized-direct-translation\n\u2018\u2018\u2018\nSince this substep requires moving the item to a target location, we use the normalized-direct-translation.\nsubstep 5: grasp the refrigerator door again\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object_link(self, \"Refrigerator\", \"link_1\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"Refrigerator\".lower() and grasped_link == \"link_1\".lower())\n\u2018\u2018\u2018\nsubstep 6: close the refrigerator door\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# this reward encourages the end-effector to stay near door\neef_pos = get_eef_pos(self)[0]\ndoor_pos = get_link_state(self, \"Refrigerator\", \"link_1\")\nreward_near = -np.linalg.norm(eef_pos - door_pos)\n# Get the joint state of the door. The semantics and the articulation tree show that joint_1 connects\nlink_1 and is the joint that controls the rotation of the door.\njoint_angle = get_joint_state(self, \"Refrigerator\", \"joint_1\")\n# The reward encourages the robot to make joint angle of the door to be the lower limit to clost it.\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"Refrigerator\", \"joint_1\")\ntarget_joint_angle = joint_limit_low\ndiff = np.abs(target_joint_angle - joint_angle)\nreward_joint =\n-diff\nreward = reward_near + 5 * reward_joint\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nI will give some more examples of decomposing the task. Reply yes if you understand the goal.\n=====================================\nYes, I understand the goal. Please proceed with the next example.\n=====================================\nAnother example:\nTask Name: Set oven temperature\nDescription: The robotic arm will turn the knob of an oven to set a desired temperature.\nInitial config:\n\u2018\u2018\u2018yaml\n-\nuse_table: false\n32\n-\ncenter: (1, 0, 0) # when an object is not on the table, the center specifies its location in the world\ncoordinate.\nlang: a freestanding oven\nname: oven\non_table: false\npath: oven.urdf\nsize: 0.85\ntype: urdf\n\u2018\u2018\u2018\n\u2018\u2018\u2018Oven articulation tree:\nlinks:\nbase\nlink_0\nlink_1\nlink_2\nlink_3\nlink_4\njoints:\njoint_name: joint_0 joint_type: continuous parent_link: link_4 child_link: link_0\njoint_name: joint_1 joint_type: continuous parent_link: link_4 child_link: link_1\njoint_name: joint_2 joint_type: continuous parent_link: link_4 child_link: link_2\njoint_name: joint_3 joint_type: continuous parent_link: link_4 child_link: link_3\njoint_name: joint_4 joint_type: fixed parent_link: base child_link: link_4\n\u2018\u2018\u2018\n\u2018\u2018\u2018Oven semantics\nlink_0 hinge knob\nlink_1 hinge knob\nlink_2 hinge knob\nlink_3 hinge knob\nlink_4 heavy oven_body\n\u2018\u2018\u2018\nLinks:\nlink_0: We know from the semantics that link_0 is a hinge knob. It is assumed to be the knob that controls the\ntemperature of the oven. The robot needs to actuate this knob to set the temperature of the oven.\nJoints:\njoint_0: from the articulation tree, joint_0 connects link_0 and is a continuous joint. Therefore, the robot\nneeds to actuate joint_0 to turn link_0, which is the knob.\nThis task can be decomposed as follows:\nsubstep 1: grasp the temperature knob\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object_link(self, \"oven\", \"link_0\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"oven\".lower() and grasped_link == \"link_0\".lower())\n\u2018\u2018\u2018\nsubstep 2: turn the temperature knob to set a desired temperature\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# This reward encourages the end-effector to stay near the knob to grasp it.\neef_pos = get_eef_pos(self)[0]\nknob_pos = get_link_state(self, \"oven\", \"link_0\")\nreward_near = -np.linalg.norm(eef_pos - knob_pos)\njoint_angle = get_joint_state(self, \"oven\", \"joint_0\")\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"oven\", \"joint_0\")\ndesired_temperature = joint_limit_low + (joint_limit_high - joint_limit_low)\n/ 3 # We assume the target\ndesired temperature is one third of the joint angle. It can also be 1/3, or other values between\njoint_limit_low and joint_limit_high.\n# The reward is the negative distance between the current joint angle and the joint angle of the desired\ntemperature.\ndiff = np.abs(joint_angle - desired_temperature)\nreward_joint =\n-diff\nreward = reward_near + 5 * reward_joint\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nI will provide more examples in the following messages. Please reply yes if you understand the goal.\n=====================================\nYes, I understand the goal. Please proceed with the next example.\n=====================================\nHere is another example:\n33\nTask Name:\nPut a toy car inside a box\nDescription: The robotic arm will open a box, grasp the toy car and put it inside the box.\nInitial config:\n\u2018\u2018\u2018yaml\n-\nuse_table: True\n-\ncenter: (0.2, 0.3, 0)\non_table: True\nlang: a box\nname: box\nsize: 0.25\ntype: urdf\n-\ncenter: (0.1, 0.6, 0)\non_table: True\nlang: a toy car\nname: toy_car\nsize: 0.1\ntype: mesh\n\u2018\u2018\u2018\n\u2018\u2018\u2018box articulation tree\nlinks:\nbase\nlink_0\nlink_1\nlink_2\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_2 child_link: link_0\njoint_name: joint_1 joint_type: revolute parent_link: link_2 child_link: link_1\njoint_name: joint_2 joint_type: fixed parent_link: base child_link: link_2\n\u2018\u2018\u2018\n\u2018\u2018\u2018box semantics\nlink_0 hinge rotation_lid\nlink_1 hinge rotation_lid\nlink_2 free box_body\n\u2018\u2018\u2018\nLinks:\nlink_0: To fully open the box, the robot needs to open both box lids. We know from the semantics that link_0\nis one of the lids.\nlink_1: To fully open the box, the robot needs to open both box lids. We know from the semantics that link_1\nis another lid.\nJoints:\njoint_0: from the articulation tree, joint_0 connects link_0 and is a hinge joint. Thus, the robot needs to\nactuate joint_0 to open link_0, which is the lid of the box.\njoint_1: from the articulation tree, joint_1 connects link_1 and is a hinge joint. Thus, the robot needs to\nactuate joint_1 to open link_1, which is the lid of the box.\nThis task can be decomposed as follows:\nsubstep 1: grasp the first lid of the box\n\u2018\u2018\u2018primitive\n# The semantics shows that link_0 and link_1 are the lid links.\nrgbs, final_state = grasp_object_link(self, \"box\", \"link_0\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"box\".lower() and grasped_link == \"link_0\".lower())\n\u2018\u2018\u2018\nsubstep 2: open the first lid of the box\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# This reward encourages the end-effector to stay near the lid to grasp it.\neef_pos = get_eef_pos(self)[0]\nlid_pos = get_link_state(self, \"box\", \"link_0\")\nreward_near = -np.linalg.norm(eef_pos - lid_pos)\n# Get the joint state of the first lid. The semantics and the articulation tree show that joint_0 connects\nlink_0 and is the joint that controls the rotation of the first lid link_0.\njoint_angle = get_joint_state(self, \"box\", \"joint_0\")\n# The reward is the negative distance between the current joint angle and the joint angle when the lid is\nfully open (upper limit).\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"box\", \"joint_0\")\ntarget_joint_angle = joint_limit_high\ndiff = np.abs(joint_angle - target_joint_angle)\nreward_joint =\n-diff\nreward = reward_near + 5 * reward_joint\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nsubstep 3: grasp the second lid of the box\n\u2018\u2018\u2018primitive\n34\n# We know from the semantics that link_0 and link_1 are the lid links.\nrgbs, final_state = grasp_object_link(self, \"box\", \"link_1\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"box\".lower() and grasped_link == \"link_1\".lower())\n\u2018\u2018\u2018\nsubstep 4: open the second lid of the box\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# This reward encourages the end-effector to stay near the lid to grasp it.\neef_pos = get_eef_pos(self)[0]\nlid_pos = get_link_state(self, \"box\", \"link_1\")\nreward_near = -np.linalg.norm(eef_pos - lid_pos)\n# Get the joint state of the second lid. The semantics and the articulation tree show that joint_1\nconnects link_1 and is the joint that controls the rotation of the second lid link_1.\njoint_angle = get_joint_state(self, \"box\", \"joint_1\")\n# The reward is the negative distance between the current joint angle and the joint angle when the lid is\nfully open (upper limit).\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"box\", \"joint_1\")\ntarget_joint_angle = joint_limit_high\ndiff = np.abs(joint_angle - target_joint_angle)\nreward_joint =\n-diff\nreward = reward_near + 5 * reward_joint\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nsubstep 5: grasp the toy car\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object(self, \"toy_car\")\nsuccess = get_grasped_object_name(self) == \"toy_car\".lower()\n\u2018\u2018\u2018\nsubstep 6: put the toy car into the box\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# Get the current car position\ncar_position = get_position(self, \"toy_car\")\n# This reward encourages the end-effector to stay near the car to grasp it.\neef_pos = get_eef_pos(self)[0]\nreward_near = -np.linalg.norm(eef_pos - car_position)\n# Get the box body bounding box\nmin_aabb, max_aabb = get_bounding_box_link(self, \"box\", \"link_4\") # from the semantics, link_4 is the body\nof the box.\ndiff = np.array(max_aabb) - np.array(min_aabb)\nmin_aabb = np.array(min_aabb) + 0.05 * diff\n# shrink the bounding box a bit\nmax_aabb = np.array(max_aabb) - 0.05 * diff\ncenter = (np.array(max_aabb) + np.array(min_aabb)) / 2\n# another reward is one if the car is inside the box bounding box\nreward_in = 0\nif in_bbox(self, car_position, min_aabb, max_aabb): reward_in += 1\n# another reward is to encourage the robot to move the car to be near the box\n# we need this to give a dense reward signal for the robot to learn to perform this task.\nreward_reaching = -np.linalg.norm(center - car_position)\n# The task is considered to be successful if the car is inside the box bounding box\nsuccess = in_bbox(self, car_position, min_aabb, max_aabb)\n# We give more weight to reward_in, which is the major goal of the task.\nreward = 5 * reward_in + reward_reaching + reward_near\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\nnormalized-direct-translation\n\u2018\u2018\u2018\nSince this substep requires moving the item to a target location, we use the normalized-direct-translation.\nPlease decompose the following task into substeps. For each substep, write a primitive/a reward function,\nwrite the success checking function, and the action space if the reward is used.\nThe primitives you can call for the robot to execute:\ngrasp_object(self, object_name): the robot arm will grasp the object specified by the argument object name.\ngrasp_object_link(self, object_name, link_name): some object like an articulated object is composed of\nmultiple links. The robot will grasp a link with link_name on the object with object_name.\nrelease_grasp(self): the robot will release the grasped object.\napproach_object(self, object_name): this function is similar to grasp_object, except that the robot only\napproaches the object, without grasping it.\n35\napproach_object_link(self, object_name, link_name): this function is similar to grasp_object_link, except that\nthe robot only approaches the object\u2019s link, without grasping it.\nNote that all primitives will return a tuple (rgbs, final_state) which represents the rgb images of the\nexecution process and the final state of the execution process.\nYou should always call the primitive in the following format:\nrgbs, final_state = some_primitive_function(self, arg1, ..., argn)\nThe APIs you can use for writing the reward function/success checking function:\nget_position(self, object_name): get the position of center of mass of object with object_name.\nget_orientation(self, object_name): get the orientation of an object with object_name.\nget_joint_state(self, object_name, joint_name): get the joint angle value of a joint in an object.\nget_joint_limit(self, object_name, joint_name): get the lower and upper joint angle limit of a joint in an\nobject, returned as a 2-element tuple.\nget_link_state(self, object_name, link_name): get the position of the center of mass of the link of an object.\nget_eef_pos(self): returns the position, orientation of the robot end-effector as a list.\nget_bounding_box(self, object_name): get the axis-aligned bounding box of an object. It returns the min and\nmax xyz coordinate of the bounding box.\nget_bounding_box_link(self, object_name, link_name): get the axis-aligned bounding box of the link of an\nobject. It returns the min and max xyz coordinate of the bounding box.\nin_bbox(self, pos, bbox_min, bbox_max): check if pos is within the bounding box with the lowest corner at\nbbox_min and the highest corner at bbox_max.\nget_grasped_object_name(self): return the name of the grasped object. If no object is grasped by the robot,\nreturn None. The name is automatically converted to the lower case.\nget_grasped_object_and_link_name(self): return a tuple, the first is the name of the grasped object, and the\nsecond is the name of the grasped link. If no object is grasped by the robot, return (None, None). The\nname is automatically converted to the lower case.\ngripper_close_to_object(self, object_name): return true if the robot gripper is close enough to the object\nspecified by object_name, otherwise false.\ngripper_close_to_object_link(self, object_name, link_name): return true if the robot gripper is close enough\nto the object link, otherwise false.\nThe action space you can use for learning with the reward: delta-translation is better suited for small\nmovements, and normalized-direct-translation is better suited for directly specifying the target\nlocation of the robot end-effector.\nYou can assume that for objects, the lower joint limit corresponds to their natural state, e.g., a box is\nclosed with the lid joint being 0, and a lever is unpushed when the joint angle is 0.\nTask name: Throw Trash Away\nDescription: The robotic arm places an item of trash inside the trash can\nInitial config:\n\u2018\u2018\u2018yaml\n- use_table: false\n- center: (1.5, 1.5, 0)\nlang: a common trash can\nname: TrashCan\non_table: false\npath: trashcan.urdf\nreward_asset_path: \u2019102187\u2019\nsize: 0.6\ntype: urdf\n- center: (0.5, 0.5, 0)\nlang: a pile of paper waste\nname: Trash\non_table: true\npath: paper_waste.obj\nsize: 0.05\ntype: mesh\n\u2018\u2018\u2018\n\u2018\u2018\u2018TrashCan articulation tree\nlinks:\nbase\nlink_0\nlink_1\njoints:\njoint_name: joint_0 joint_type: revolute parent_link: link_1 child_link: link_0\njoint_name: joint_1 joint_type: fixed parent_link: base child_link: link_1\n\u2018\u2018\u2018\n\u2018\u2018\u2018TrashCan semantics\nlink_0 hinge door\nlink_1 free trashcan_body\n\u2018\u2018\u2018\nLinks:\nlink_0: Link_0, as the trash can lid, needs to be approached and opened by the robot in order to throw away\nthe trash.\nJoints:\njoint_0: Joint_0, connecting to link_0, needs to be actuated for the robot to open the trash can lid.\nAn example response for the above task \u201cThrow the trash away\u201d:\nThis task can be decomposed into the following substeps:\nsubstep 1: grasp the trash can lid\n36\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object_link(self, \"TrashCan\", \"link_0\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"TrashCan\".lower() and grasped_link == \"link_0\".lower())\n\u2018\u2018\u2018\nsubstep 2: open the trash can lid\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# This reward encourages the end-effector to stay near the lid to grasp it.\neef_pos = get_eef_pos(self)[0]\nlid_pos = get_link_state(self, \"TrashCan\", \"link_0\")\nreward_near = -np.linalg.norm(eef_pos - lid_pos)\n# Get the joint state of the lid. The semantics and the articulation tree show that joint_0 connects\nlink_0 and is the joint that controls the rotation of the lid.\njoint_angle = get_joint_state(self, \"TrashCan\", \"joint_0\")\n# The reward is the negative distance between the current joint angle and the joint angle when the lid is\nfully open (upper limit).\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"TrashCan\", \"joint_0\")\ntarget_joint_angle = joint_limit_high\ndiff = np.abs(joint_angle - target_joint_angle)\nreward_joint =\n-diff\nreward = reward_near + 5 * reward_joint\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nsubstep 3: grasp the trash\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object(self, \"Trash\")\nsuccess = get_grasped_object_name(self) == \"Trash\".lower()\n\u2018\u2018\u2018\nsubstep 4: put the trash into the trash can\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# Get the current trash position\ntrash_position = get_position(self, \"Trash\")\n# This reward encourages the end-effector to stay near the trash to grasp it.\neef_pos = get_eef_pos(self)[0]\nreward_near = -np.linalg.norm(eef_pos - trash_position)\n# Get the trash can body bounding box\nmin_aabb, max_aabb = get_bounding_box_link(self, \"TrashCan\", \"link_1\") # from the semantics, link_1 is the\nbody of the trash can.\ndiff = np.array(max_aabb) - np.array(min_aabb)\nmin_aabb = np.array(min_aabb) + 0.05 * diff\n# shrink the bounding box a bit\nmax_aabb = np.array(max_aabb) - 0.05 * diff\ncenter = (np.array(max_aabb) + np.array(min_aabb)) / 2\n# another reward is one if the trash is inside the trash can bounding box\nreward_in = 0\nif in_bbox(self, trash_position, min_aabb, max_aabb): reward_in += 1\n# another reward is to encourage the robot to move the trash to be near the trash can\n# we need this to give a dense reward signal for the robot to learn to perform this task.\nreward_reaching = -np.linalg.norm(center - trash_position)\n# The task is considered to be successful if the trash is inside the trash can bounding box\nsuccess = in_bbox(self, trash_position, min_aabb, max_aabb)\n# We give more weight to reward_in, which is the major goal of the task.\nreward = 5 * reward_in + reward_reaching + reward_near\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\nnormalized-direct-translation\n\u2018\u2018\u2018\nSince this substep requires moving the item to a target location, we use the normalized-direct-translation.\nsubstep 5: release the trash\n\u2018\u2018\u2018primitive\nrgbs, final_state = release_grasp(self)\nsuccess = get_grasped_object_name(self) == None\n\u2018\u2018\u2018\nsubstep 6: grasp the trash can lid again\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object_link(self, \"TrashCan\", \"link_0\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"TrashCan\".lower() and grasped_link == \"link_0\".lower())\n37\n\u2018\u2018\u2018\nsubstep 7: close the trash can lid\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# This reward encourages the end-effector to stay near the lid\neef_pos = get_eef_pos(self)[0]\nlid_pos = get_link_state(self, \"TrashCan\", \"link_0\")\nreward_near = -np.linalg.norm(eef_pos - lid_pos)\n# Get the joint state of the lid. The semantics and the articulation tree show that joint_0 connects\nlink_0 and is the joint that controls the rotation of the lid.\njoint_angle = get_joint_state(self, \"TrashCan\", \"joint_0\")\n# The reward encourages the robot to make joint angle of the lid to be the lower limit to close it.\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"TrashCan\", \"joint_0\")\ntarget_joint_angle = joint_limit_low\ndiff = np.abs(target_joint_angle - joint_angle)\nreward_joint =\n-diff\nreward = reward_near + 5 * reward_joint\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nAnother example of heating up a bowl of soup in the microwave:\nThis task can be decomposed as follows:\nsubstep 1: grasp the microwave door\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object_link(self, \"Microwave\", \"link_0\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"Microwave\".lower() and grasped_link == \"link_0\".lower())\n\u2018\u2018\u2018\nsubstep 2: open the microwave door\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# Get the joint state of the door. We know from the semantics and the articulation tree that joint_0\nconnects link_0 and is the joint that controls the rotation of the door.\njoint_angle = get_joint_state(self, \"Microwave\", \"joint_0\")\n# The reward is the negative distance between the current joint angle and the joint angle when the door is\nfully open (upper limit).\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"Microwave\", \"joint_0\")\ntarget_joint_angle = joint_limit_high\ndiff = np.abs(joint_angle - target_joint_angle)\nreward =\n-diff\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nHere from the last substep the robot already grasps the microwave door, thus only local movements are needed\nto open the door.\nsubstep 3: grasp the bowl of soup\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object(self, \"Bowl of soup\")\nsuccess = get_grasped_object_name(self) == \"Bowl of soup\".lower()\n\u2018\u2018\u2018\nsubstep 4: put the bowl of soup into the microwave\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# Get the current soup position\nsoup_position = get_position(self, \"Bowl of soup\")\n# Get the microwave body bounding box\nmin_aabb, max_aabb = get_bounding_box_link(self, \"Microwave\", \"link_3\") # from the semantics, link_3 is\nthe body of the microwave.\ndiff = np.array(max_aabb) - np.array(min_aabb)\nmin_aabb = np.array(min_aabb) + 0.05 * diff\n# shrink the bounding box a bit\nmax_aabb = np.array(max_aabb) - 0.05 * diff\ncenter = (np.array(max_aabb) + np.array(min_aabb)) / 2\n# First reward is one if the soup is inside the microwave bounding box\nreward_in = 0\nif in_bbox(self, soup_position, min_aabb, max_aabb): reward_in += 1\n38\n# Second reward is to encourage the robot to grasp the soup and move the soup to be near the microwave\n# we need this to give a dense reward signal for the robot to learn to perform this task.\nreward_reaching = -np.linalg.norm(center - soup_position)\n# The task is considered to be successful if the soup is inside the microwave bounding box\nsuccess = in_bbox(self, soup_position, min_aabb, max_aabb)\n# We give more weight to the first reward which is putting the soup into the microwave.\nreward = 5 * reward_in + reward_reaching\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\nnormalized-direct-translation\n\u2018\u2018\u2018\nSince this substep requires moving the item to a target location, we use the normalized-direct-translation.\nsubstep 5: grasp the microwave door again\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object_link(self, \"Microwave\", \"link_0\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"Microwave\".lower() and grasped_link == \"link_0\".lower())\n\u2018\u2018\u2018\nsubstep 6: close the microwave door\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# Get the joint state of the door. We know from the semantics and the articulation tree that joint_0\nconnects link_0 and is the joint that controls the rotation of the door.\njoint_angle = get_joint_state(self, \"Microwave\", \"joint_0\")\n# The reward is the negative distance between the current joint angle and the joint angle when the door is\nfully closed (lower limit).\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"Microwave\", \"joint_0\")\ntarget_joint_angle = joint_limit_low\ndiff = np.abs(target_joint_angle - joint_angle)\nreward =\n-diff\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nHere from the last substep the robot already grasps the microwave door, thus only local movements are needed\nto close the door.\nsubstep 7: grasp the microwave timer knob\n\u2018\u2018\u2018primitive\nrgbs, final_state = grasp_object_link(self, \"Microwave\", \"link_1\")\ngrasped_object, grasped_link = get_grasped_object_and_link_name(self)\nsuccess = (grasped_object == \"Microwave\".lower() and grasped_link == \"link_1\".lower())\n\u2018\u2018\u2018\nsubstep 8: turn the microwave timer knob to set a desired heating time\n\u2018\u2018\u2018reward\ndef _compute_reward(self):\n# Get the joint state of the timer knob. We know from the semantics and the articulation tree that joint_1\nconnects link_1 and is the joint that controls the timer knob.\njoint_angle = get_joint_state(self, \"Microwave\", \"joint_1\")\njoint_limit_low, joint_limit_high = get_joint_limit(self, \"Microwave\", \"joint_1\")\ndesired_time = joint_limit_low + (joint_limit_high - joint_limit_low)\n/ 2 # We assume the target desired\ntime is half of the joint angle. It can also be one third, or other values between joint_limit_low\nand joint_limit_high.\n# The reward is the negative distance between the current joint angle and the joint angle of the desired\ntime.\ndiff = np.abs(joint_angle - desired_time)\nreward =\n-diff\n# if the difference is small enough, we consider it a success. Here the threshold value is determined as a\nratio of the joint angle range.\nsuccess = diff < 0.1 * (joint_limit_high - joint_limit_low)\nreturn reward, success\n\u2018\u2018\u2018\n\u2018\u2018\u2018action space\ndelta-translation\n\u2018\u2018\u2018\nSince the robot already gras\nHere unfortunately we exceeded the 8k token limit of GPT-4. But the main body of the task decom-\npositions have been finished, and the response is still good to be used.\n39\n"
  },
  {
    "title": "Idempotent Generative Network",
    "link": "https://arxiv.org/pdf/2311.01462.pdf",
    "upvote": "22",
    "text": "Preprint. Under review.\nIDEMPOTENT GENERATIVE NETWORK\nAssaf Shocher1,2 Amil Dravid1 Yossi Gandelsman1 Inbar Mosseri2 Michael Rubinstein2 Alexei A. Efros1\n1UC Berkeley\n2Google Research\nABSTRACT\nWe propose a new approach for generative modeling based on training a neural\nnetwork to be idempotent. An idempotent operator is one that can be applied\nsequentially without changing the result beyond the initial application, namely\nf(f(z)) = f(z). The proposed model f is trained to map a source distribution\n(e.g, Gaussian noise) to a target distribution (e.g. realistic images) using the fol-\nlowing objectives: (1) Instances from the target distribution should map to them-\nselves, namely f(x) = x. We define the target manifold as the set of all instances\nthat f maps to themselves. (2) Instances that form the source distribution should\nmap onto the defined target manifold. This is achieved by optimizing the idem-\npotence term, f(f(z)) = f(z) which encourages the range of f(z) to be on the\ntarget manifold. Under ideal assumptions such a process provably converges to the\ntarget distribution. This strategy results in a model capable of generating an output\nin one step, maintaining a consistent latent space, while also allowing sequential\napplications for refinement. Additionally, we find that by processing inputs from\nboth target and source distributions, the model adeptly projects corrupted or mod-\nified data back to the target manifold. This work is a first step towards a \u201cglobal\nprojector\u201d that enables projecting any input into a target data distribution.1\n1\nINTRODUCTION\nGEORGE: You\u2019re gonna \u201doverdry\u201d it.\nJERRY: You, you can\u2019t \u201doverdry.\u201d\nGEORGE: Why not?\nJERRY: Same as you can\u2019t \u201doverwet.\u201d You see, once something is wet, it\u2019s wet. Same thing with\ndead: like once you die you\u2019re dead, right? Let\u2019s say you drop dead and I shoot you: you\u2019re not\ngonna die again, you\u2019re already dead. You can\u2019t \u201doverdie,\u201d you can\u2019t \u201doverdry.\u201d\n\u2014 \u201cSeinfeld\u201d, Season 1, Episode 1, NBC 1989\nGenerative models aim to create synthetic samples by drawing from a distribution underlying the\ngiven data. There are various approaches such as GANs (Goodfellow et al., 2014), VAE (Kingma &\nWelling, 2022), diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020), pixel autoregressive\nmethods (van den Oord et al., 2017; 2016b;a) and some recent like consistency models (Song et al.,\n2023) and Bayesian flow networks (Graves et al., 2023). Inputs to these models could range from\nsamples of random noise to specific input images in conditional setups, which are then mapped to\noutputs aligned with a given target distribution, typically the manifold of natural images. However,\neach model is specifically trained to expect a particular type of input. What if we wanted a single\nmodel to be able to take any type of input, be it corrupted instances (e.g., degraded images), an\nalternative distribution (e.g., sketches), or just noise, and project them onto the real image manifold\nin one step, a kind of \u201cMake It Real\u201d button? As a first step toward this ambitious goal, this work\ninvestigates a new generative model based on a generalization of projection \u2014 Idempotence.\nAn idempotent operator is one that can be applied sequentially multiple times without changing\nthe result beyond the initial application, namely f(f(z)) = f(z). Some real-life actions can also\nbe considered idempotent, as humorously pointed out by Jerry Seinfeld (1). One mathematical\nexample is the function mapping z to |z|; applying it repeatedly yields\n\f\f|z|\n\f\f = |z|, leaving the result\nunchanged. In the realm of linear operators, idempotence equates to orthogonal projection. Over\nRn, these are matrices A that satisfy A2 = A, with eigenvalues that are either 0 or 1; they can\n1Project page: https://assafshocher.github.io/IGN/\n1\narXiv:2311.01462v1  [cs.CV]  2 Nov 2023\nPreprint. Under review.\nbe interpreted as geometrically preserving certain components while nullifying others. Lastly, the\nidentity function naturally exhibits idempotent behavior, as applying it multiple times leaves the\ninput unchanged.\nFigure 1:\nThe basic idea behind IGN: real examples (x)\nare invariant to the model f: f(x) = x. other inputs (z)\nare projected onto the manifold of instances that f maps to\nthemselves by optimizing for f(f(z)) = f(z).\nWe propose Idempotent Generative Net-\nworks (IGN), a model based on the idea\nof projection. Given a dataset of examples\n{xi}N\ni=1, Our goal is to \u201cproject\u201d our input\nonto the target distribution Px from which\nxi\u2019s are drawn. Fig. 1 illustrates the basic\nobjectives. We assume that distributions\nPz and Px lie in the same space. Given\nthat, it is valid to apply f to a given ex-\nample x \u223c Px. What should the outcome\nof doing so be then? The natural answer\nto that is \u201cnothing\u201d. Considering the intu-\nition of projection, an instance that already\nlies on the target manifold should just re-\nmain the same- \u201cYou can\u2019t overdry\u201d. The\nfirst objective is then perfectly fulfilled when f(x) = x . We can leverage this notion, and define the\nestimated manifold of the data as the sub-set of all instances that f maps close to themselves.\nNext, we want to map instances from a different distribution onto that estimated manifold. To do so,\nwe want f(z) to be on the manifold for every z \u223c Pz, which is characterized by being mapped to\nitself. This defines our second objective, Idempotence : f(f(z)) = f(z). While the aforementioned\nobjectives ensure both x and f(z) reside on the estimated target manifold, they do not inherently\nconstrain what else populates that manifold. To address this, we introduce a third term, to tighten\nthe manifold, pushing for f(f(z)) \u0338= f(z). The intricacy of reconciling opposing loss terms is\nunraveled in Section 2.1.\nWhile the landscape of generative modeling is rich with diverse methodologies, our Idempotent\nGenerative Network (IGN) features specific advantages that address existing limitations. In contrast\nto autoregressive methods, which require multiple inference steps, IGN produces robust outputs\nin a single step, akin to one-step inference models like GANs. Yet, it also allows for optional\nsequential refinements, reminiscent of the capabilities in diffusion models. Unlike diffusion models,\nhowever, IGN maintains a consistent latent space, facilitating easy manipulations and interpolations.\nThe model shows promise in generalizing beyond trained instances, effectively projecting degraded\nor modified data back onto the estimated manifold of the target distribution. Moreover, the model\u2019s\nability to accept both latent variables and real-world instances as input simplifies the editing process,\neliminating the need for the inversion steps commonly required in other generative approaches. We\ndraw connections to other generative models in Section 5.\n2\nMETHOD\nWe start by presenting our generative model, IGN. It is trained to generate samples from a target\ndistribution Px given input samples from a source distribution Pz. Formally, given a dataset of\nexamples {xi}i\u2208{1,...,n}, with each example drawn from Px, we train a model f to map Pz to Px.\nWe assume both distributions Pz and Px lie in the same space, i.e., their instances have the same\ndimensions. This allows applying f to both types of instances z \u223c Pz and x \u223c Px.\nNext, we describe the optimization objectives, the training procedure of the model, the architecture,\nand practical considerations in the training.\n2.1\nOPTIMIZATION OBJECTIVES\nThe model optimization objectives rely on three main principles. First, each data sample from the\ntarget distribution should be mapped by the model to itself. Second, the model should be idempotent\n- applying it consecutively twice should provide the same results as applying it once. Third, The\nsubset of instances that are mapped to themselves should be as small as possible. Next we explain\nthe objectives and show how these principles are translated to optimization objectives.\n2\nPreprint. Under review.\nReconstruction objective.\nOur first objective, as motivated in the introduction, is the reconstruc-\ntion objective, which is perfectly achieved when each sample x \u223c Px is mapped to itself:\nf(x) = x\n(1)\nGiven a distance metric D (e.g., L2), we define the drift measure of some instance y as:\n\u03b4\u03b8(y) = D\n\u0000y, f\u03b8(y)\n\u0001\n(2)\nWhere \u03b8 are the parameters of a model f\u03b8. We then seek to minimize the drift measure:\nmin\n\u03b8\n\u03b4\u03b8(x) = min\n\u03b8\nD\n\u0000x, f\u03b8(x)\n\u0001\n(3)\nThe fact that real instances are mapped to themselves motivates us to define the ideal estimated\ntarget manifold as the subset of all possible inputs that are mapped to themselves by our model:\nS = {y : f(y) = y} = {y : \u03b4(y) = 0}\n(4)\nIdempotent objective.\nWe desire f to map any instance sampled from the source distribution onto\nthe estimated manifold:\nf(z) \u2208 S\nz \u223c Pz\n(5)\nTaking the definition in Eq. 4 and combining with our goal in 5, we can define our second objective,\nthat when perfectly achieved we get:\nf(f(z)) = f(z)\n(6)\nThis implies that f is idempotent over the domain of all possible z \u223c Pz. This idempotence objective\nis formulated then as follows.\nmin\n\u03b8\n\u03b4\u03b8(f\u03b8(z)) = min\n\u03b8\nD (f\u03b8(z), f\u03b8(f\u03b8(z)))\n(7)\nHowever, we will next see that directly optimizing this formulation of Idemptotnce has a caveat that\nrequires us to split it into two separate terms.\nTightness objective.\nIn the formulation of the objectives so far, there is a missing link. The\nreconstruction objective, if optimized perfectly (eq. 2.1), determines that all given examples are on\nthe estimated manifold. However, it does not imply that other instances are not on that manifold.\nAn extreme example is that if f is identity f(z) = z\n\u2200z, it perfectly satisfies both objectives.\nFurthermore, the idempotent objective as formulated so far, is encouraging the manifold to expand.\nFig. 2 illustrates this problem. There are two distinct pathways of gradients that flow when the\nidempotent objective in eq. 7 is minimized, both of which would contribute when optimizing. The\nfirst pathway, which is the desired one, is by modifying f such that f(z), the first application of f\nto z (red in fig. 2) is better mapped to the currently defined manifold. The second pathway is for a\ngiven f(z) making f(f(z)) closer to that already given f(z) (green in fig. 2). This second way of\noptimizing is effectively expanding the estimated target manifold, as defined in eq. 4.\nFigure 2:\nTwo distinct pathways to enforce Idempotence: By updating f so it maps f(z) into the {f(y) =\ny} area (updating through first instatiation, \u2206f ) or by expanding the {f(y) = y} area so that for a given\ny = f(z), we get f(y) = y (updating through second instantiation \u2206f). If we encourage the red update while\ndiscouraging the green one, we simultaneously map into the estimated manifold while tightening it around the\ndata examples.\n3\nPreprint. Under review.\nIn order to discourage the incentive to expand the manifold, we only optimize w.r.t. the first (inner)\ninstantiation of f, while treating the second (outer) instantiation as a frozen copy of the current state\nof f. We denote by \u03b8\u2032 the parameters of the frozen copy of f. They are equal in value to \u03b8 but they\nare different entities, in the sense that a gradient taken w.r.t \u03b8 will not affect \u03b8\u2032.\nLidem(z; \u03b8, \u03b8\u2032) = \u03b4\u03b8\u2032(f\u03b8(z)) = D (f\u03b8\u2032(f\u03b8(z)), f\u03b8(z))\n(8)\nWe denote the expectation of losses as\nLidem(\u03b8; \u03b8\u2032) = Ez\n\u0002\nLidem(z; \u03b8, \u03b8\u2032)\n\u0003\n(9)\nWhile eq. 8 prevents the encouragement to expand the estimated manifold, it is not enough. We\nare interested in tightening the manifold as much as possible. We therefore maximize the distance\nbetween f(f(z)) and f(z), assuming a given f(z). This is optimizing only the second (outer)\ninstantiation of f, treating the first instantiation as a frozen copy. The term we want to minimze is\nthen\nLtight(z; \u03b8, \u03b8\u2032) = \u2212\u03b4\u03b8(f\u03b8\u2032(z)) = \u2212D (f\u03b8(f\u03b8\u2032(z)), f\u03b8\u2032(z))\n(10)\nNotice that Ltight(z; \u03b8, \u03b8\u2032) = \u2212Lidem(z; \u03b8\u2032, \u03b8). This induces an adversarial fashion for training\nthese two losses together. However, there is no alternating optimization. All gradients are accumu-\nlated on \u03b8 in a single step.\nFinal optimization objective.\nCombining the three optimization terms described above brings us\nto the final loss:\nL\n\u0000\u03b8, \u03b8\u2032\u0001\n= Lrec\n\u0000\u03b8\n\u0001\n+\u03bbiLidem\n\u0000\u03b8; \u03b8\u2032\u0001\n+\u03bbtLtight\n\u0000\u03b8; \u03b8\u2032\u0001\n= Ex,z\nh\n\u03b4\u03b8(x) + \u03bbi\u03b4\u03b8\u2032(f\u03b8(z)) \u2212 \u03bbt\u03b4\u03b8(f\u03b8\u2032(z))\ni\n(11)\nwith Lrec(\u03b8) = Ex\n\u0002\nD(f\u03b8(x), x)\n\u0003\nbeing the reconstruction term and \u03bbi and \u03bbt being the weights of\nthe idempotent and tightening loss terms respectively. Note that while the losses are assigned with\n\u03b8\u2032 = \u03b8, the gradient which is made of partial derivatives is only w.r.t. the original argument \u03b8 of the\nloss Lidem(z; \u03b8, \u03b8\u2032). The general update rule is therefore:\n\u03b8\u2032 \u2190 \u03b8\n\u03b8 \u2190 \u03b8 \u2212 \u03b7\u2207\u03b8L\n\u0000\u03b8, \u03b8\u2032\u0001\n(12)\n2.2\nTRAINING\nFor a single model f that appears multiple times in the calculation, we want to optimize by taking\ngradients of distinct losses w.r.t. different instantiations of f. fig. 2, eq. 13 and fig. 3 all share the\nsame color coding. Red indicates the update of f through its first (inner) instantiation, by minimizing\n\u03b4. Green indicates the update of f through its second (outer) instantiation, by maximizing \u03b4; We\nexamine the gradient of \u03b4\u03b8(f(z))).\n\u2207\u03b8\u03b4(f(z)) = \u2202\u03b4(f(z))\n\u2202f(f(z))\ndf(\u00b7)\nd\u03b8\n\f\f\f\nf(z)\n|\n{z\n}\nLtight: Gradient ascent \u2191\n+\n \n\u2202\u03b4(f(z))\n\u2202f(f(z))\n\u2202f(f(z))\n\u2202f(z)\n+ \u2202\u03b4(f(z))\n\u2202f(z)\n!\ndf(\u00b7)\nd\u03b8\n\f\f\f\nz\n|\n{z\n}\nLidem: Gradient descent \u2193\n(13)\nThe two terms of the gradient exemplify the different optimization goals for the different appear-\nances of f. Optimizing Ltight is done by gradient ascent on the first term while optimizing Lidem\nis done by gradient descent on the second term.\nFor Ltight it is trivial to prevent optimization of the first (inner) instantiation of f. As depicted in\nfig. 3, it can be done by stopping the gradients in the backward process between the two instan-\ntiations of f, treating f(z) as a static input. This method, however, cannot be applied for Lidem.\nEq. 13 shows that the gradient w.r.t the wanted first instantiation of f, \u2202\u03b4(f(z))\n\u2202f(z)\nis calculated with\nchain rule through the second \u2202\u03b4(f(z))\n\u2202f(f(z)). To cope, fig. 3 shows that we employ a copy of the model,\nfcopy. It is updated at every iteration to be identical to f, but as a different entity, we can calculate\nbackpropagation through it without accumulating gradients w.r.t. its parameters.\nfig. 3 and source-code. 2.2 show how the training is performed in practice. For efficiency we first\ncalculate f(z) that can be shared by both the idempotent loss and the tightening loss. In source-\ncode. 2.2 we provide the basic training PyTorch code for IGN. This is the actual code used for\nMNIST experiments, once provided with a model, an optimizer and a data-loader.\n4\nPreprint. Under review.\nFigure 3:\nA diagram of the proposed method. The top depicts the reconstruction term over real data. The\nbottom depicts the Idempotence and tightness terms. The colored arrows depict the gradients. The colors match\nthe colors in eq. 13 and fig. 2. Dashed arrow indicates back propagation without accumulating gradients on the\nparameters it passes through. The final loss is the sum of all the losses.\n1\ndef train(f, f_copy, opt, data_loader, n_epochs):\n2\nfor epoch in range(n_epochs):\n3\nfor x in data_loader:\n4\nz = torch.randn_like(x)\n5\n6\n# apply f to get all needed\n7\nf_copy.load_state_dict(f.state_dict())\n8\nfx = f(x)\n9\nfz = f(z)\n10\nf_z = fz.detach()\n11\nff_z = f(f_z)\n12\nf_fz = f_copy(fz)\n13\n14\n# calculate losses\n15\nloss_rec = (fx - x).pow(2).mean()\n16\nloss_idem = (f_fz - fz).pow(2).mean()\n17\nloss_tight = -(ff_z - f_z).pow(2).mean()\n18\n19\n# optimize for losses\n20\nloss = loss_rec + loss_idem + loss_tight * 0.1\n21\nopt.zero_grad()\n22\nloss.backward()\n23\nopt.step()\nSource Code 1: IGN training routine (PyTorch)\n2.3\nARCHITECTURE AND OPTIMIZATION\nNetwork architecture.\nThe typical model to be used with IGN is built as an autoencoder. One\npossibility is using an existing GAN architecture, \u201cflipping\u201d the order such that the encoder is the\ndiscriminator, with the binary classification head chopped off, and the encoder is the generator.\nTightening loss metric.\nOne undesirable effect caused by Ltight is that it benefits from applying\nbig modifications even to a relatively good generated instance. Moreover, optimized to increase\ndistance between input and output encourages high gradients and instability. To ameliorate these\nissues we modify the distance metric for Ltight and limit its value. We use a smooth clamp by\n5\nPreprint. Under review.\nhyperbolic tangent with the value dependent on the current reconstruction loss for each iteration\nLtight(z) = tanh\n \u02dcLtight(z)\naLrec(z)\n!\naLrec(z)\n(14)\nWith \u02dcLtight the loss as defined before and a \u2265 1 a constant ratio. The rationale is that if at any given\ntime a latent that is mapped far out of the estimated manifold, we have no reason to push it further.\nNoise distribution.\nWe found slight improvement occurs when instead of standard Gaussian noise\nwe sample noise with frequency-statistics as the real data. We apply a Fast Fourier Transform (FFT)\nto a batch of data and take the mean and variance for the real and imaginary parts of each frequency.\nWe then use these statistics to sample and apply inverse FFT to get the noise. Examples of how this\nnoise looks like are shown in fig. 4.\n3\nTHEORETICAL RESULTS\nUnder idealized assumptions, our proposed training paradigm leads to a noteworthy theoretical out-\ncome: After convergence, the distribution of instances generated by the model is aligned with the\ntarget distribution. Moreover, the Idempotence loss describes at each step the probability of a ran-\ndom input z to map onto the manifold estimated by the other losses.\nTheorem 1. Under ideal conditions, IGN converges to the target distribution.\nWe define the generated distribution, represented by P\u03b8(y), as the PDF of y when y = f\u03b8(z) and\nz \u223c Pz. We split the loss into two terms.\nL(\u03b8; \u03b8\u2032) = Lrec(\u03b8) + \u03bbiLtight(\u03b8; \u03b8\u2032)\n|\n{z\n}\nLrt\n+\u03bbtLidem(\u03b8; \u03b8\u2032)\n(15)\nWe assume a large enough model capacity such that both terms obtain a global minimum:\n\u03b8\u2217 = arg min\n\u03b8\nLrt(\u03b8; \u03b8\u2217) = arg min\n\u03b8\nLidem(\u03b8; \u03b8\u2217)\n(16)\nThen, \u2203\u03b8\u2217 : P\u03b8\u2217 = Px and for \u03bbt = 1, this is the only possible P\u03b8\u2217.\nProof. We first demonstrate that Lrt minimizes the drift \u03b4 over the target distribution while maxi-\nmizing it at every other f(z). Next, we show that Lidem maximizes the probability of f to map z to\nminimum drift areas.\nWe first find the global minimum of Lrt given the current parameters \u03b8\u2217:\nLrt(\u03b8; \u03b8\u2217) = Ex\n\u0002\nD(f\u03b8(x), x)\n\u0003\n\u2212 \u03bbtEz\n\u0002\nD(f\u03b8(f\u03b8\u2217(z)), f\u03b8\u2217(z))\n\u0003\n(17)\n=\nZ\n\u03b4\u03b8(x)Px(x)dx \u2212 \u03bbt\nZ\n\u03b4\u03b8(f\u03b8\u2217(z))P\u03b8\u2217(z)dz\n(18)\nWe now change variables. For the left integral, let y := x and for the right integral, let y := f\u03b8\u2217(z).\nLrt(\u03b8; \u03b8\u2217) =\nZ\n\u03b4\u03b8(y)Px(y)dy \u2212 \u03bbt\nZ\n\u03b4\u03b8(y)P\u03b8\u2217(y)dy\n(19)\n=\nZ\n\u03b4\u03b8(y)\n\u0010\nPx(y) \u2212 \u03bbtP\u03b8\u2217(y)\n\u0011\ndy\n(20)\nWe denote M = supy1,y2 D(y1, y2), where the supremum is taken over all possible pairs y1, y2.\nNote that M can be infinity. Since \u03b4\u03b8 is non-negative, the global minimum for Lrt(\u03b8; \u03b8\u2217) is obtained\nwhen:\n\u03b4\u03b8\u2217(y) = M \u00b7 1{Px(y)<\u03bbtP\u03b8\u2217(y)}\n\u2200y\n(21)\nNext, we characterize the global minimum of Lidem given the current parameters \u03b8\u2217:\nLidem(\u03b8, \u03b8\u2217) = Ez\n\u0002\nD (f\u03b8\u2217(f\u03b8(z)), f\u03b8(z))\n\u0003\n= Ez\n\u0002\n\u03b4\u03b8\u2217(f\u03b8(z))\n\u0003\n(22)\nPlugging in Eq. 21 and substituting \u03b8\u2217 with \u03b8 as we examine the minimum of the inner f:\nLidem(\u03b8; \u03b8\u2217) = M \u00b7 Ez\n\u0002\n1{Px(y)<\u03bbtP\u03b8(y)}\n\u0003\n(23)\n6\nPreprint. Under review.\nTo obtain \u03b8\u2217, according to our assumption in Eq. 16, we take arg min\u03b8 of Eq. 23:\n\u03b8\u2217 = M \u00b7 arg min\n\u03b8\nEz\n\u0002\n1{Px(y)<\u03bbtP\u03b8(y)}\n\u0003\n(24)\nThe presence of parameters to be optimized in this formulation is in the notion of the distribution\nP\u03b8(y). If P\u03b8\u2217 = Px and \u03bbt \u2264 1, the loss value will be 0, which is its minimum. If \u03bb = 1,\n\u03b8\u2217 : P\u03b8\u2217 = Px is the only minimizer. This is because the total sum of the probability needs to be 1.\nAny y for which P\u03b8(y) < Px(y) would necessarily imply that \u2203y such that P\u03b8(y) > Px(y), which\nwould increase the loss.\nQualitatively, the value \u03b4\u03b8(y) can be thought of as energy, minimized where the probability Px(y) is\nhigh and maximized where Px(y) is low. Under the ideal assumptions, it is binary, but in practical\nscenarios, it would be continuous.\nInterestingly, Eq. 23 returns 0 if \u03b4\u03b8(y) = 0 which is the definition of being on the estimated mani-\nfold. This indicator essentially describes the event of f\u03b8(z) /\u2208 S\u03b8. Taking the expectation over the\nindicator yields the probability of the event. The loss is the probability of a random z to be mapped\noutside of the manifold. Optimizing idempotence is essentially maximizing the portion of z\u2019s that\nare mapped onto the manifold.\nIn practice, we use \u03bbt < 1. While the theoretical derivation guarantees a single desired optimum\nfor \u03bbt = 1, the practical optimization of a finite capacity neural network suffers undesirable effects\nsuch as instability. The fact that f is continuous makes the optimal theoretical \u03b8\u2217 which produces a\ndiscontinuous \u03b4\u03b8\u2217 unobtainable in practice. This means that Ltight tends to push toward high values\nof \u03b4\u03b8(y) also for y that is in the estimated manifold. Moreover, in general, it is easier to maximize\ndistances than minimize them, just by getting big gradient values.\n4\nEXPERIMENTAL RESULTS\nFollowing the training scheme outlined in Sections 2.2 and 2.3, we train IGN on two datasets -\nCelebA and MNIST. We present qualitative results for the two datasets, as well out-of-distribution\nprojection capabilities and latent space manipulations.\nOur generative outcomes, at this stage, are not competitive with sate-of-the-art models. Our ex-\nperiments currently operate with smaller models and lower-resolution datasets. In our exploration,\nwe primarily focus on a streamlined approach, deferring additional mechanisms to maintain the\npurity of the primary method. It\u2019s worth noting that foundational generative modeling techniques,\nlike GANs Goodfellow et al. (2014) and Diffusion Models Sohl-Dickstein et al. (2015), took con-\nsiderable time to reach their matured, scaled-up performance. We view this as a preliminary step,\nproviding initial evidence of the potential capabilities. Our future work will aim at refining and\nscaling up the approach.\nExperimental settings.\nWe evaluate IGN on MNIST (Deng, 2012), a dataset of grayscale hand-\nwritten digits, and CelebA (Liu et al., 2015), a dataset of face images. We use image resolutions of\n28 \u00d7 28 and 64 \u00d7 64 respectively. We adopt a simple autoencoder architecture, where the encoder\nis a simple five-layer discriminator backbone from DCGAN, and the decoder is the generator. The\ntraining and network hyperparameters are presented in Table 1.\nFigure 4: Examples of MNIST and CelebA IGN generations from input Gaussian noise for IGN. Notice that\nin some cases f(f(z)) corrects for artifacts in f(z).\n7\nPreprint. Under review.\nFigure 5: Projections of images from different distributions using IGN. We demonstrate that IGN can project\nnoisy images x + n (left), grayscale images g(x) (middle), and sketches s(x) (right) onto the learned natural\nimage manifold to perform image-to-image translation. See appendix for details on the degradations.\nGeneration results.\nFigure 4 presents qualitative results for the two datasets after applying the\nmodel once and consecutively twice. As shown, applying IGN once (f(z)) results in coherent\ngeneration results. However, artifacts can be present, such as holes in MNIST digits, or distorted\npixels at the top of the head and hair in the face images. Applying f again (f(f(z))) corrects for\nthese, filling in the holes, or reducing the total variation around noisy patches in the face. Figure 7\nshows additional results, as well as applying f three times. Comparing f(f(f(z))) to f(f(z))\nshows that when the images get closer to the learned manifold, applying f again results in minimal\nchanges as the images are considered to be in distribution. See a large collection of uncurated face\nimages generated by applying IGN a varying number of times in Figures 11-14.\nLatent Space Manipulations.\nWe demonstrate IGN has a consistent latent space by performing\nmanipulations, similarly as shown for GANs (Radford et al., 2015). Latent space interpolation\nvideos can be found in the supplementary material. We sample several random noises, take linear\ninterpolation between them and apply f. In The videos left to right: z, f(z), f(f(z)), f(f(f(z))).\nFig. 6 shows latent space arithmetics. Formally, we consider three inputs zpositive, znegative and z,\nsuch that f(zpositive) has a specific image property that f(znegative) and f(z) do not have (e.g. the\nfaces in the two former images have glasses, while the latter does not have them). The result of\nf(zpositive \u2212 znegative) + z) is an edited version of f(z) that has the property.\nFigure 6:\nInput noise arithmetic.\nSimilar to\nGANs, arithmetic operations can be performed\nin the input noise space to idempotent generative\nnetworks to find interpretable axes of variation.\nOut-of-Distribution Projection.\nWe validate the\npotential for IGN as a \u201cglobal projector\u201d by inputting\nimages from a variety of distributions into the model\nto produce their \u201cnatural image\u201d equivalents (i.e.:\nproject onto IGN\u2019s learned manifold). We demon-\nstrate this by denoising noised images x+n, coloriz-\ning grayscale images g(x), and translating sketches\ns(x) to realistic images in Fig. 5. Although the pro-\njected images are not perfect reconstructions of the\noriginal images x, these inverse tasks are ill-posed.\nIGN is able to create natural-looking projections that\nadhere to the structure of the original images. As\nshown, sequential applications of f can improve the\nimage quality (e.g. it removes dark and smoky arti-\nfacts in the projected sketches). Note that IGN was\nonly trained on natural images and noise, and did\nnot see distributions such as sketches or grayscale\nimages. While other methods explicitly train for this\ntask (Zhu et al., 2017; Isola et al., 2017), this behavior naturally emerges in IGN as a product of its\nprojection objective. Moreover, due to the autoencoding architecture of IGN, we do not need to rely\non inversion for editing. Instead, we rely solely on forward passes through the network.\n8\nPreprint. Under review.\n5\nRELATED WORK\nGenerative Adversarial Networks (GANs).\nIGN incorporates elements of adversarial train-\ning (Goodfellow et al., 2014), evident in the relationship between Lidem and Ltight, which are neg-\natives of each other. One could view \u03b4 as a discriminator trained using Lrec for real examples and\nLtight for generated ones, while f serves as the generator trained by Lidem. Unique to IGN is a\nform of adversarial training we term \u201cself-adversarial\u201d, Put simply, f is both the generator and\nthe discriminator. This streamlined architecture affects the optimization process. Rather than alter-\nnating between two networks, a single model accumulates gradients from both discriminative and\ngenerative perspectives in each step.\nEnergy Based Models (EBMs).\nIn Energy-Based Models (EBMs; Ackley et al. (1985)), a func-\ntion f is explicitly trained to serve as an energy metric, assigning higher values to less desirable\nexamples and lower values to those that fit the model well. IGN introduces a similar, yet distinct\nparadigm: rather than f acting as the energy function, this role is filled by \u03b4(y) = D(f(y), y). The\nmodel trains f to be idempotent, with the objective to minimize \u03b4(f(z)). A successful training pro-\ncedure would align the range of f with the low-energy regions as measured by \u03b4. This reduces the\nneed for separate optimization procedures to find the energy minimum. From another perspective,\nf can be viewed as a transition operator that maps high-energy inputs toward a low-energy domain.\nDiffusion Models.\nIn both diffusion models (Sohl-Dickstein et al., 2015) and IGN, the model can\nbe sequentially applied. Additionally, both methods train the model to transition an input along\na path between a source distribution and a target data manifold. In diffusion models, this path is\ndictated by a predefined noise schedule. At inference, the model takes small, incremental steps,\neffectively performing a form of gradient descent to transition from complete noise\u2014representing\nthe source distribution\u2014to the target data manifold. IGN diverges from this approach. Instead of a\npredefined path dictated by a noise schedule or any other set rule, the trajectory between distributions\nis determined solely by the model\u2019s learning process. Unlike diffusion models, IGN doesn\u2019t employ\nincremental gradient steps toward the data manifold. Instead, it is trained to approximate as closely\nas possible to the target manifold in a single step. It can be reapplied for further refinement if needed.\n6\nLIMITATIONS\nMode collapse.\nSimilar to GANs, our model can experience mode collapse and is not practically\nguaranteed to generate the entire target distribution. Some methods attempt to overcome this failure\nmode in GANs (Mao et al., 2019; Durall et al., 2020). We plan to investigate if these methods are\napplicable to our generative model as well.\nBlurriness.\nSimilar to VAEs and other autoencoders, our model suffers from blurry generated\nsamples. Although repeated applications can fix artifacts to make images appear more natural, they\nmay also smoothen them towards an average-looking image. One possible solution to this problem\nis to replace the naive reconstruction loss with a perceptual loss Johnson et al. (2016). Another\nsolution is to use a two-step approach and apply our model on latents instead of pixels (similar\nto Rombach et al. (2021)). We plan to investigate it in future work.\n7\nCONCLUSION\nIn this paper, we introduced a new type of generative model that is trained to be idempotent. We\npresented a theoretical guarantee of convergence to the target distribution and demonstrated that this\nmodel can perform zero-shot image-to-image tasks. We see this work as a first step towards a model\nthat learns to map any input to a target distribution, a new paradigm for generative modeling. We\nplan to scale IGN up in future work by training a large-scale version of it on much more data to\nrealize its full potential.\n9\nPreprint. Under review.\nACKNOWLEDGEMENTS\nThe authors would like to thank Karttikeya Mangalam, Yannis Siglidis, Konpat Preechakul, Niv\nHaim, Niv Granot and Ben Feinstein for the helpful discussions. Assaf Shocher gratefully acknowl-\nedges financial support for this publication by the Fulbright U.S. Postdoctoral Program, which is\nsponsored by the U.S. Department of State. Its contents are solely the responsibility of the author\nand do not necessarily represent the official views of the Fulbright Program or the Government of\nthe United States. Amil Dravid is funded by the US Department of Energy Computational Sci-\nence Graduate Fellowship. Yossi Gandelsman is funded by the Berkeley Fellowship and the Google\nFellowship. Additional funding came from DARPA MCS and ONR MURI.\nREFERENCES\nDavid H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski.\nA learning algorithm for\nboltzmann machines. Cognitive Science, 9(1):147\u2013169, 1985. ISSN 0364-0213. doi: https:\n//doi.org/10.1016/S0364-0213(85)80012-4.\nURL https://www.sciencedirect.com/\nscience/article/pii/S0364021385800124.\nLi Deng. The mnist database of handwritten digit images for machine learning research. IEEE\nSignal Processing Magazine, 29(6):141\u2013142, 2012.\nRicard Durall, Avraam Chatzimichailidis, Peter Labus, and Janis Keuper. Combating mode collapse\nin GAN training: An empirical analysis using hessian eigenvalues. CoRR, abs/2012.09673, 2020.\nURL https://arxiv.org/abs/2012.09673.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-\nmation processing systems, pp. 2672\u20132680, 2014.\nAlex Graves, Rupesh Kumar Srivastava, Timothy Atkinson, and Faustino Gomez. Bayesian flow\nnetworks. arXiv preprint arXiv:2308.07037, 2023.\nJonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\nral Information Processing Systems, volume 33, pp. 6840\u20136851. Curran Associates, Inc.,\n2020.\nURL https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/4c5bcfec8584af0d967f1ab10179ca4b-Paper.pdf.\nPhillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with\nconditional adversarial networks. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 1125\u20131134, 2017.\nJustin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and\nsuper-resolution, 2016.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes, 2022.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild.\nIn Proceedings of the IEEE international conference on computer vision, pp. 3730\u20133738, 2015.\nQi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking gen-\nerative adversarial networks for diverse image synthesis. CoRR, abs/1903.05628, 2019. URL\nhttp://arxiv.org/abs/1903.05628.\nAlec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep\nconvolutional generative adversarial networks. CoRR, abs/1511.06434, 2015. URL https:\n//api.semanticscholar.org/CorpusID:11758569.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models, 2021.\n10\nPreprint. Under review.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In Francis Bach and David Blei (eds.), Pro-\nceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings\nof Machine Learning Research, pp. 2256\u20132265, Lille, France, 07\u201309 Jul 2015. PMLR. URL\nhttps://proceedings.mlr.press/v37/sohl-dickstein15.html.\nYang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023.\nAaron\nvan\nden\nOord,\nNal\nKalchbrenner,\nLasse\nEspeholt,\nkoray\nkavukcuoglu,\nOriol\nVinyals,\nand Alex Graves.\nConditional image generation with pixelcnn decoders.\nIn\nD.\nLee,\nM.\nSugiyama,\nU.\nLuxburg,\nI.\nGuyon,\nand\nR.\nGarnett\n(eds.),\nAd-\nvances in Neural Information Processing Systems, volume 29. Curran Associates, Inc.,\n2016a.\nURL https://proceedings.neurips.cc/paper_files/paper/2016/\nfile/b1301141feffabac455e1f90a7de2054-Paper.pdf.\nAaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learn-\ning.\nIn I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan,\nand R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Cur-\nran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper_files/\npaper/2017/file/7a98af17e63a0ac09ce2e96d03992fbc-Paper.pdf.\nA\u00a8aron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu.\nPixel recurrent neural net-\nworks.\nIn Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd\nInternational Conference on Machine Learning, volume 48 of Proceedings of Machine Learn-\ning Research, pp. 1747\u20131756, New York, New York, USA, 20\u201322 Jun 2016b. PMLR. URL\nhttps://proceedings.mlr.press/v48/oord16.html.\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation\nusing cycle-consistent adversarial networks. In Proceedings of the IEEE international conference\non computer vision, pp. 2223\u20132232, 2017.\n11\nPreprint. Under review.\nA\nVISUAL COMPARISON OF ITERATIVE APPLICATIONS OF f\nAlso see videos in supplementary material.\nFigure 7: Comparison of iterative applications of f. As the generated images approach the learned manifold,\nsequential applications of f have smaller effects on the outputs.\nFigure 8: Comparison of iterative applications of f on MNIST.\n12\nPreprint. Under review.\nB\nMORE PROJECTIONS\nFigure 9: Projection-based edits. By simply masking out a region of interest and adding noise for\nstochastic variation, we can conduct fine-grained edits, such as closing the mouth, adding hair or\nfacial hair, and putting on glasses.\nFigure 10: Projection-based compositing. Given a reference image f(z), we can use the noise\nspatially corresponding to an attribute of interest, place it on another image x, and project it in order\nto transfer an attribute, such as glasses, facial hair, etc.\n13\nPreprint. Under review.\nC\nIMPLEMENTATION DETAILS\nOperation\nKernel\nStrides\nPadding\nFeature maps\nBN?\nNonlinearity\nEncoder \u2013 3 \u00d7 64 \u00d7 64 input\nConvolution\n4 \u00d7 4\n2 \u00d7 2\n1\n64\n\u00d7\nLeaky ReLU\nConvolution\n4 \u00d7 4\n2 \u00d7 2\n1\n128\n\u221a\nLeaky ReLU\nConvolution\n4 \u00d7 4\n2 \u00d7 2\n1\n256\n\u221a\nLeaky ReLU\nConvolution\n4 \u00d7 4\n2 \u00d7 2\n1\n512\n\u221a\nLeaky ReLU\nConvolution\n4 \u00d7 4\n1 \u00d7 1\n0\n512\n\u00d7\nNone\nDecoder \u2013 512 \u00d7 1 \u00d7 1 input\nTransposed Convolution\n4 \u00d7 4\n1 \u00d7 1\n0\n512\n\u221a\nReLU\nTransposed Convolution\n4 \u00d7 4\n2 \u00d7 2\n1\n256\n\u221a\nReLU\nTransposed Convolution\n4 \u00d7 4\n2 \u00d7 2\n1\n128\n\u221a\nReLU\nTransposed Convolution\n4 \u00d7 4\n2 \u00d7 2\n1\n64\n\u221a\nReLU\nTransposed Convolution\n4 \u00d7 4\n2 \u00d7 2\n1\n3\n\u00d7\nTanh\nLoss metric D L1: D(y1, y2) = ||y1 \u2212 y2||1\nLoss terms weights \u03bbr = 20, \u03bbi = 20, \u03bbt = 2.5\nLthight clamp ratio a = 1.5\nOptimizer Adam (\u03b1 = 0.0001, \u03b21 = 0.5, \u03b22 = 0.999)\nBatch size 256\n# GPUs 8\nIterations 1000\nLeaky ReLU slope 0.2\nWeight, bias initialization Isotropic gaussian (\u00b5 = 0, \u03c3 = 0.02), Constant(0)\nTable 1: CelebA-10 hyperparameters. We train a simple autoencoder architecture with minimal hyperparam-\neter tuning.\nDegradations.\nThe images are scaled to values [\u22121, 1]\n\u2022 Noise: We add Gaussian noise n = N(0, 0.15)\n\u2022 Grayscale: We take the mean of each pixel over the channels and assign to each of the\nthree channels as the model expects three channels.\ng(x) = x.mean(dim = 1, keepdim = True).repeat(1, 3, 1, 1).\n\u2022 Sketch: We divide the pixel values by the Gaussian blurred image pixel values with\nkernel size of 21. The standard deviation is the default w.r.t. kernel size by PyTorch:\n\u03c3 = 0.3 \u00d7 ((kernel size \u2212 1) \u00d7 0.5 \u2212 1) + 0.8. we add and subtract 1 to perform the\ndivision on positive values.\ns(x) = (g(x + 1)/(gaussian blur(g(x + 1), 21)) + 10\u221210) \u2212 1.\n14\nPreprint. Under review.\nD\nUNCURATED VISUAL RESULTS\nFigure 11: Uncurated CelebA samples from applying IGN once: f(z).\n15\nPreprint. Under review.\nFigure 12: Uncurated CelebA samples from applying IGN twice: f(f(z)).\n16\nPreprint. Under review.\nFigure 13: Uncurated CelebA samples from applying IGN three times: f(f(f(z))).\n17\nPreprint. Under review.\nFigure 14: Uncurated CelebA samples from applying IGN three times: f(f(f(f(z)))).\n18\n"
  },
  {
    "title": "E3 TTS: Easy End-to-End Diffusion-based Text to Speech",
    "link": "https://arxiv.org/pdf/2311.00945.pdf",
    "upvote": "10",
    "text": "E3 TTS: EASY END-TO-END DIFFUSION-BASED TEXT TO SPEECH\nYuan Gao, Nobuyuki Morioka, Yu Zhang, Nanxin Chen\nGoogle\n{gaoyua,nmorioka,ngyuzh,nanxinchen}@google.com\nABSTRACT\nWe propose Easy End-to-End Diffusion-based Text to\nSpeech, a simple and efficient end-to-end text-to-speech\nmodel based on diffusion. E3 TTS directly takes plain text\nas input and generates an audio waveform through an iter-\native refinement process. Unlike many prior work, E3 TTS\ndoes not rely on any intermediate representations like spec-\ntrogram features or alignment information. Instead, E3 TTS\nmodels the temporal structure of the waveform through the\ndiffusion process. Without relying on additional conditioning\ninformation, E3 TTS could support flexible latent structure\nwithin the given audio. This enables E3 TTS to be easily\nadapted for zero-shot tasks such as editing without any addi-\ntional training. Experiments show that E3 TTS can generate\nhigh-fidelity audio, approaching the performance of a state-\nof-the-art neural TTS system. Audio samples are available at\nhttps://e3tts.github.io.\nIndex Terms\u2014 text-to-speech, non-autoregressive, diffu-\nsion, diversity\n1. INTRODUCTION\nDiffusion models [1, 2, 3] have demonstrated great perfor-\nmance on a variety of generation tasks, including image [4,\n5, 6] and audio generation [7, 8]. Diffusion models work by\ngradually removing noise from a latent representation of the\ndata until it becomes indistinguishable from real data. Text-\nto-speech (TTS) systems that use diffusion models have been\nshown to produce high-fidelity speech that is comparable with\nstate-of-the-art systems [7, 8, 9, 10].\nMost prior work in this area has relied on a two-stage gen-\neration process. In the first stage, the generator model gener-\nates intermediate representations, typically audio tokens [11,\n12] or spectrogram-based features [13, 14, 15, 16, 17, 18, 19],\nwhich are aligned with the waveform but in lower resolution.\nIn the second stage, a vocoder is introduced to predict the\naudio from the intermediate features. Besides the two-stage\nprocess, most models take some extra neural model or statis-\ntical method to convert the text to some other input units [20],\nsuch as phonemes or graphemes.\nEven though a two-stage TTS pipeline can produce higher\nquality audio, it may also have other concerns, such as rely-\ning on the quality of intermediate features. Additionally, it is\nmore complicated to deploy and set up for different situations.\nEnd-to-end generation of audio from text is elusive, due\nto the difficulty of efficiently modeling strong temporal de-\npendencies in the waveform.\nSample-level autoregressive\nvocoders handle such dependencies by conditioning genera-\ntion of each waveform sample on the whole history. Due to\ntheir highly sequential nature, they are inefficient to sample\nfrom on modern parallel hardware. Some previous work in-\nstead generates a sequence of non-overlapping fixed-length\nblocks autoregressively to speedup the generation [21]. This\nspeeds up the generation process by generating all samples\nwithin the block in parallel.\nA different direction of prior work is to include alignment\ninformation during training. The alignment information pro-\nvides the mapping between each individual input unit, such as\na phoneme, and output samples in the generated audio. It is\nusually extracted using external alignment tools, which pro-\nvide the start time and end time of each individual input unit.\nFastSpeech 2 [22] relies on such alignment or duration infor-\nmation and other properties such as energy and pitch to pre-\ndict the audio. One internal predictor is also trained for each\nindividual property so the predicted results could be utilized\nduring inference. EATS [23] proposes to use a differentiable\nduration predictor and depends on the Dynamic Time Wrap-\ning (DTW) to make sure the prediction is aligned with the\ntarget audio. This avoids the usage of external aligner but\nmakes the training more complicated.\nIn this paper, we propose a Easy End-to-End Text to\nSpeech framework (E3 TTS) that only relies on diffusion to\npreserve temporal structure in waveform. It directly takes text\nas input and uses a pretrained BERT model [24] to extract\ninformation from it. It is followed by a UNet structure [25]\nwhich predicts the audio by attending to the BERT represen-\ntations. The whole model is non-autoregressive and directly\noutputs a waveform.\nOur model achieves comparable re-\nsults to the two-stage framework on proprietary dataset from\nexperiments.\nThe paper is organized as follows. Section 2 gives a brief\noverview of different components used in prior works of TTS\nthat could be optimized. Section 3 introduces the proposed\nsystem which only includes a diffusion model taking BERT\nrepresentations as input. Section 4 starts with experiments\n979-8-3503-0689-7/23/$31.00 \u00a92023 IEEE\narXiv:2311.00945v1  [cs.SD]  2 Nov 2023\non proprietary dataset, comparing with some previous work.\nSection 5 reveals some applications that could be achieved\nwith the proposed method. Section 6 summarizes the system\nand discusses some future work.\n2. COMPLEXITIES OF TTS\nThrough a careful analysis of current text-to-speech (TTS)\nsystems, we have identified several components that greatly\nincrease the complexities of existing systems.\n2.1. Text Normalization\nOne of the challenges in building a text-to-speech (TTS) sys-\ntem is the normalization of input text. This is the process of\nconverting text from its written form into a form that can be\neasily processed by the TTS system. This can be a difficult\ntask, as there are many different ways that text can be writ-\nten [20]. For example, the same word can be written in dif-\nferent ways, such as \u201dcolor\u201d and \u201dcolour\u201d. Additionally, text\ncan contain abbreviations, acronyms, and other non-standard\nforms. A good TTS system must be able to handle all of these\ndifferent variations in order to produce accurate and natural-\nsounding speech.\n2.2. Input Unit\nEven after text normalization, there can still be ambiguities\nin how to pronounce the same word in different contexts.\nFor example, record has different pronunciations depending\nwhether it is a noun or a verb. This is why many TTS sys-\ntems rely on verbalized forms, such as phonemes or prosodic\nfeatures, instead of text directly.\nPhonemes: A phoneme is a unit of sound that is used\nto make up words.\nMany previous work [7, 10] rely on\nphonemes as input. This can be useful for generating speech\nfrom languages that do not have a standard writing system.\nProsodic features: Prosodic features are characteristics\nof speech, such as fundamental frequencies, durations, and\nenergy. Some previous work [13, 22] utilize prosodic fea-\ntures as input. This can be used to control the intonation and\nemphasis of the generated speech.\n2.3. Alignment Modeling\nAnother challenge in building a TTS system is alignment\nmodeling. This is the process of predicting the length of time\nthat each phoneme in a word should be pronounced. This\nis important because it helps to ensure that the generated\nspeech sounds natural and fluent. Alignment modeling can\nbe a difficult task, as there are many factors that can affect the\nlength of time that a phoneme is pronounced. For example,\nthe position of a phoneme in a word can affect its duration.\nAdditionally, the stress of a word can also affect the dura-\ntion of its phonemes. A good TTS system must be able to\nmodel all of these factors in order to produce accurate and\nnatural-sounding speech.\nA typical approach for alignment modeling in end-to-end\nspeech-to-text system is to rely on external aligner which\nprovides the alignment information given transcript and au-\ndio [13, 10].\nDuring model training, a duration predictor\nis learned to predict the information which could be used\nto estimate alignment for inference. For duration predictor,\nNon-Attentive Tacotron framework [15] managed to learn du-\nration implicitly by employing the Variational Auto-Encoder.\nGlow-TTS [26] and Grad-TTS [27] made use of Monotonic\nAlignment Search algorithm (an adoption of Viterbi training\n[28] finding the most likely hidden alignment between two\nsequences). Indeed, we actually solve the quality issue Grad-\nTTS mentioned in paper when they try to conduct end-to-end\nexperiments.\n3. METHOD\nWe propose our solution that addresses challenges presented\nin the last Section to make TTS systems more accessible to the\nwider community. The proposed model includes two modules\nillustrated in Figure 1:\n\u2022 A pretrained BERT model extracts information from\ntext.\n\u2022 An diffusion UNet model attends to the BERT output\nand predicts the raw waveform by refining the noisy\nwaveform iteratively.\n3.1. BERT model\nTo take the advantage of the recent large language model de-\nvelopment, we built our system based on the text representa-\ntions which are given by a pretrained BERT model [24]. The\nBERT model takes the subword as input and it does not rely\non any other presentations of the speech such as phoneme,\ngraphemes, in contrast to some previous work [15, 16, 14,\n13, 22, 21, 29, 30]. This simplifies the process since one\ncould rely on a pretrained text language model which could\nbe trained on multiple languages with only text data available.\n3.2. Diffusion\nOur model is built based on prior work on score matching [3]\nand diffusion probabilistic models [1]. In the case of TTS, the\nscore function is defined as the gradient of the log conditional\ndistribution p(y | x) with respect to the output y as\ns(y | x) = \u2207y log p(y | x)\n(1)\nwhere y is the waveform and x is the conditioning signal.\nFollowing previous work [9], we adopt a special parame-\nterization known as the diffusion model [1]. A score network\nInput\n24kHz\nWaveform\nDBlocks with Adpative Kernel\nDBlock with Attention\nBERT\nUBlock with Attention\nUBlocks with Adpative Kernel\nPredicted\nNoise\nFig. 1. UNet Structure: DBlock for downsampling block, UBlock for upsampling block\ns(\u02dcy | x, \u00af\u03b1) is trained to predict the scaled derivative by min-\nimizing the distance between model prediction and ground\ntruth \u03f5 as\nE\u00af\u03b1,\u03f5\nh\r\r\r\u03f5\u03b8\n\u0010\n\u02dcy, x,\n\u221a\n\u00af\u03b1\n\u0011\n\u2212 \u03f5\n\r\r\r\n2\ni\n(2)\nwhere \u03f5 \u223c N(0, I) is the noise term introduced by apply-\ning the reparameterization trick, \u00af\u03b1 is the noise level and \u02dcy is\nsampled according to\n\u02dcy =\n\u221a\n\u00af\u03b1 y0 +\n\u221a\n1 \u2212 \u00af\u03b1 \u03f5\n(3)\nDuring training, \u00af\u03b1\u2019s are sampled from the intervals [\u00af\u03b1n, \u00af\u03b1n+1]\nbased on a pre-defined linear schedule of \u03b2\u2019s, according to:\n\u00af\u03b1n :=\nn\nY\ns=1\n(1 \u2212 \u03b2s)\n(4)\nIn each iteration, the updated waveform is estimated follow-\ning the following stochastic process\nyn\u22121 =\n1\n\u221a\u03b1n\n\u0012\nyn \u2212\n\u03b2n\n\u221a1 \u2212 \u00af\u03b1n\n\u03f5\u03b8(yn, x, \u221a\u00af\u03b1n)\n\u0013\n+\u03c3nz (5)\nIn this work, to help convergence and to better scale\n\u03f5 loss\u2019s magnitude, we adopt a KL form loss. The model\nalso predicted the variance \u03c9(\u03b1) of the L2 loss according to\ntimestep, and we use a KL loss form to adjust the weight of\nthe loss from different sampled timestep.\nE\u00af\u03b1,\u03f5\n\u0014\n1\n\u03c9(\u00af\u03b1)\n\r\r\r\u03f5\u03b8\n\u0010\n\u02dcy, x,\n\u221a\n\u00af\u03b1\n\u0011\n\u2212 \u03f5\n\r\r\r\n2 + ln(\u03c9(\u00af\u03b1))\n\u0015\n(6)\n3.3. U-Net\nWe deploy a 1D U-Net, following the structure of [4]. The\ngeneral model structure are shown in Figure 1, consists\nof a series of downsampling and upsampling blocks con-\nnected by residual. The detailed structure of each downsam-\npling/upsampling block is shown in Figure 2. Like the typical\napproach in autoregressive TTS [31, 32], we adopt a cross-\nattention to extract information from BERT output in the\ntop downsampling/upsampling blocks. In the low downsam-\npling/upsampling block, following [33], we use an adaptive\nsoftmax CNN kernel whose kernel is determined by timestep\nand speaker. In other layers, speaker and timestep embedding\nare joined using FiLM [34], which comprises a combined\nlayer which predicts channel-wise scaling and bias. Inside\neach block, ublock and dblock the structure closely follow\nthe structure described in [4].\nThe downsampler finally refined the noise information\n(24kHz) to a sequence whose length is similar to the encoded\nBERT output. This has proved important in practice to im-\nprove the quality. The upsampler finally predicts noise whose\nlength is the same as the input waveform.\nIn training, we fixed the length of waveform to be 10.92\nsec, and padding zero to the end of the waveform. When cal-\nculating loss, the padding part are less weighted. In prac-\ntice, we weight each padding frame 1\n10 as non-padding frame.\nIn inference, we fixed the length of output waveform. And\nwe use average magnitude to distinguish the padding part. In\npractice, we calculate average magnitude per 1024 samples\nand cutoff \u2264 0.02 parts.\n4. EXPERIMENT\nWe compare our model with other neural TTS systems. Fol-\nlowing [9], baseline systems were trained on a proprietary\ndataset consisted of 385 hours of high-quality US English\nspeech from 84 professional voice talents. A female speaker\nwas chosen from the training dataset for evaluation. We im-\nplemented our model with parameter size in Table 1. For pre-\ntrained BERT, we take base parameter size model trained on\nEnglish only data 1. In inference, we use 1000 steps DDPM,\n1English only, uncased BERT-Base model provided in https://\ngithub.com/google-research/bert\nCombineEmb\nConv 1D\nGroupNorm + Swish\nConv 1D\nGroupNorm + Swish\nSelf Attention\nCross Attention\nKernel 1\n\u00b7 \u00b7 \u00b7\nKernel K\nFC Layer\nSpeaker Embedding\nTimestamp Embedding\nBert Encoder Output\nDBlock Residual\nFig. 2. UBlock/DBlock Struture: Adaptive Kernel and residual optional.\nnoise scheduling is\n\u03b1n = exp (ln(1e\u22127) \u2217 (1 \u2212 cos(\nn\n1000 \u2217 \u03c0\n2 ))\n3\n2 )\n(7)\nPerformance is measured using subjective listening tests,\nperformed by a pool of native speakers listening with head-\nphones.\nResults are reported as the mean opinion score\n(MOS) which measures the naturalness of generated sample\non a ten-point scale from 1 to 5. Each sample is rated at least\ntwice by two different native speakers.\nWe compared our\nmodel with character based TTS models from [21], the result\nare shown in Table 2.\nBlock index\n0\n1\n2\n3\nBase dimension\n128\n256\n512\n1024\nKernel Size\n[5,5]\n[5,5]\n[5,5]\n[3,3,3,3,3]\nStrides\n[2,2]\n[2,2]\n[4]\n[4,2,2,2,2]\nAdaptive Kernel\n[8,8]\n[4,4]\n[2]\nBlocks\n[2,2]\n[2,2]\n[2]\n[1,1,1,1,1]\nSelf Attention\n[\u00d7,\u00d7]\n[\u00d7,\u00d7]\n[\u00d7]\n[\u2713,\u2713,\u2713,\u2713,\u2713]\nCross Attention\n[\u00d7,\u00d7]\n[\u00d7,\u00d7]\n[\u00d7]\n[\u2713,\u2713,\u2713,\u2713,\u2713]\nAttention Heads\n[8,8,8,8,8]\nTable 1. Model configuration. Empty cell indicates it is not\nused in this block.\nResults suggest the proposed method leads to a better fi-\ndelity than other end-to-end systems. One minor difference\nhere is that the proposed system is based on sub-word instead\nof characters but we believe it should be comparable for TTS\napplication.\nMode\nMOS\nGround truth\n4.56 \u00b1 0.04\nTwo-Stage Models\nTacotron-PN + Griffin-Lim [35] (char)\n3.68 \u00b1 0.08\nTacotron + WaveRNN [36] (char)\n4.36 \u00b1 0.05\nTacotron + Flowcoder [37] (char)\n3.34 \u00b1 0.07\nEnd-to-End Models\nWave-Tacotron [21] (char)\n4.07 \u00b1 0.06\nOur Model\n4.24 \u00b1 0.06\nTable 2. TTS performance on the proprietary single speaker\ndataset, evaluation contains about 600 examples.\n5. APPLICATIONS\nIn this Section, we demonstrate our model could be applied in\ndifferent scenarios. Specifically, we use a base model trained\nwithout any speaker information provided. The speaker is\ndynamically determined during inference.\nTo enlarge the\nspeaker diversity, we train the model on all LibriTTS data,\nmixing clean-100, clean-360, other-500 splits.\n5.1. Zero Shot Learning\nIn the proposed approach, the alignment between the audio\nand text features is dynamically determined during inference.\nThis enables zero-shot learning for a variety of applications.\nWe demonstrate the model\u2019s ability through the following\ntasks. Examples of each task and corresponding audio sam-\nples are displayed on https://e3tts.github.io.\n5.1.1. Waveform Prompt based TTS\nFor this task, for each example, we select two sentences from\nsame speaker from test split of LibriTTS-clean. We concate-\nnate the text of sentences and provide the waveform of first\nsentence as the prompt to the model. The prompt part are\nguaranteed to be longer than 3 seconds, and the part asked to\nbe generated are guaranteed to be longer than 2 seconds, the\ntotal length are guaranteed to be shorter than 9 seconds. Dur-\ning inference, on the prompt part, we replace the predicted\n\u03f5 with actual \u03f5. And on the rest, we keep the predicted \u03f5.\nQuantitative results are shown in the top part of Table 3. We\nreport SQuId score [38] which is an approximation of the\nmean opinion score, and speaker similarity (Speaker Sim).\nSpeaker similarity is estimated based on the speaker embed-\nding given by a LSTM-based speaker model [39]. Results\ndemonstrate that our model could generate high-quality au-\ndio given prompt with similar speaker characteristics.\n5.1.2. Text-based Speech Editing\nTo evaluate the model\u2019s ability to edit speech, we evaluate the\nperformance of text-based speech inpainting, which is a spe-\ncial case of replacement. We select sentences from test split\nof LibriTTS-clean and masked a small fragment (0.5 secs \u223c\n2.5 secs) in waveform. We then provide the sentences, the\nmasked waveform to the model, and ask it to get the whole\nwaveform. Similar to the audio prompt task, we replace the\npredicted \u03f5 with true \u03f5 on the unchanged audio and keep the\nrest. In practice, the length of the masked part is unknown,\nand is usually provided by the user or predicted by some\nother model in a statistical way. To show the ability of dif-\nfusion model, we feed 3 example of same sentence to the\nmodel, with different masked part length(0.8\u00d7, 1.0\u00d7, 1.2\u00d7\nthe ground truth length), and reported their result in Table 3.\nFrom the experiment results, we can conclude that the pro-\nposed model E3 is robust against different lengths of editing\nspan.\nTask\nSplit\nSQuId\nSpeaker Sim\nPrompt TTS\nGround Truth\n3.81\nOur Model\n3.75\n0.95\nText Editing\nGround Truth\n3.91\n0.8\u00d7\n3.84\n0.98\n1.0\u00d7\n3.83\n0.98\n1.2\u00d7\n3.81\n0.97\nBest of 3\n3.85\n0.98\nTable 3. Audio prompt and text-based editing results. Metrics\ninclude SQuId which approximates mean opinion score and\nspeaker similarity. Prompt TTS task contains about 200 ex-\namples. Text Editing task contains about 80 examples. Eval-\nuation data are generated from test split of LibriTTS\n5.1.3. Speaker Similarity\nFor this task, we select sentence from random unseen speak-\ners from test split of LibriTTS-clean.\nFor each example,\nwe select waveform wA from Speaker A and 8 waveform\nw1\nB...w8\nB from random selected speakers (Speaker A must\nbe included).\nWe ask model to predict which speaker\nwA belongs to.\nIn inference, we concatenate the wA and\nwi\nB and get 8 waveform.\nWe random select a timestep\n(0.04 \u2264 \u03b1n \u2264 0.96) and feed the noised waveform to the\nmodel. Similar to [40], we calculated the L2 distance on pre-\ndicted \u03f5 and true \u03f5 and sum them up using a fixed calculated\nweight. To make the result independent to wi\nB\u2019s magnitude\nand length, we only take the \u03f5 part on wA in consideration.\nWe summarize the result from different timestep samples\nusing Monte Carlo method. The result for different sample\ntimes are listed in 4. In general, with more timestep sam-\npled, we observe better speaker accuracy. The result itself is\ninteresting especially since the model is trained without any\nspeaker information.\n#Timesteps\nSpeaker Classification\nAccuracy\n1 sample\n75.50%\n4 samples\n81.00%\n32 samples\n83.20%\nTable 4. Speaker similarity results evaluated on about 1000\nexamples. With more sampled steps, we observe better clas-\nsification accuracy.\n5.2. Sample Diversity\nFr\u00b4echet Speaker Distance\nGround Truth\n8.38\nWave-Tacotron\n26.58\nOur Model\n12.30\nTable 5.\nFr\u00b4echet Speaker Distance results on proprietary\ndataset. Fr\u00b4echet Speaker Distance measures the audio diver-\nsity. FSD of the ground truth is measured by computing the\nscore between non-overlap subsets of ground truth audio.\nDiffusion models can generate samples with higher diver-\nsity and with more authentic distribution. To measure dis-\ntribution, inspired by FID [41] and FAD [42], we introduce\nnew metric called Fr\u00b4echet Speaker Distance (FSD). We use a\nLSTM-based speaker model [39] and take the normalized last\nembedding layer. And calculate the distance using\nFSDA,B = \u2225\u00b5A \u2212 \u00b5B\u22252 + Tr(CA + CB \u2212 2\np\nCA \u2217 CB)\n(8)\nwhere \u00b5 represent the mean of model output speaker embed-\nding among all examples, C represent the covariance.\nWe evaluate our model\u2019s FSD score on proprietary dataset.\nResults in Table 5 reveals the proposed E3 TTS system greatly\nimproves the diversity comparing to previous work. It reaches\nsimilar score as the ground truth.\n6. CONCLUSION\nWe have proposed a novel end-to-end text-to-speech (TTS)\nmodel, E3, that is capable of generating high-fidelity audio\ndirectly from BERT features. E3 is based on the diffusion\nmodel, an iterative refinement process. The alignment be-\ntween the audio and text features is dynamically determined\nduring generation using cross attention. E3 greatly simplifies\nthe design of end-to-end TTS systems and has been shown\nto achieve impressive performance in experiments. We also\ndemonstrate that this simplified architecture enables a variety\nof zero-shot tasks, such as speech editing and prompt-based\ngeneration.\nIn future work, we plan to extend E3 to support multilin-\ngual speech generation by replacing the English-only BERT\nmodel with a multilingual language model.\n7. REFERENCES\n[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel, \u201cDenoising\ndiffusion probabilistic models,\u201d Advances in neural in-\nformation processing systems, vol. 33, pp. 6840\u20136851,\n2020.\n[2] Jascha\nSohl-Dickstein,\nEric\nWeiss,\nNiru\nMah-\neswaranathan, and Surya Ganguli, \u201cDeep unsupervised\nlearning using nonequilibrium thermodynamics,\u201d\nin\nInternational conference on machine learning. PMLR,\n2015, pp. 2256\u20132265.\n[3] Yang\nSong,\nJascha\nSohl-Dickstein,\nDiederik\nP\nKingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole,\n\u201cScore-based generative modeling through\nstochastic differential equations,\u201d\narXiv preprint\narXiv:2011.13456, 2020.\n[4] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour, Raphael Gontijo-Lopes, Burcu Karagol\nAyan, Tim Salimans, Jonathan Ho, David J. Fleet, and\nMohammad Norouzi, \u201cPhotorealistic text-to-image dif-\nfusion models with deep language understanding,\u201d in\nAdvances in Neural Information Processing Systems,\nAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho, Eds., 2022.\n[5] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen,\n\u201cHierarchical text-conditional\nimage generation with clip latents,\u201d\narXiv preprint\narXiv:2204.06125, 2022.\n[6] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans,\n\u201csimple diffusion: End-to-end diffusion for high resolu-\ntion images,\u201d arXiv preprint arXiv:2301.11093, 2023.\n[7] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mo-\nhammad Norouzi, and William Chan, \u201cWavegrad: Esti-\nmating gradients for waveform generation,\u201d in Interna-\ntional Conference on Learning Representations, 2021.\n[8] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and\nBryan Catanzaro,\n\u201cDiffwave: A versatile diffusion\nmodel for audio synthesis,\u201d in International Conference\non Learning Representations, 2020.\n[9] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mo-\nhammad Norouzi, Najim Dehak, and William Chan,\n\u201cWavegrad 2: Iterative refinement for text-to-speech\nsynthesis,\u201d in Proc. Interspeech. 2021, pp. 3765\u20133769,\nISCA.\n[10] Zhijun Liu, Yiwei Guo, and Kai Yu, \u201cDiffvoice: Text-\nto-speech with latent diffusion,\u201d in ICASSP 2023-2023\nIEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\n[11] Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang,\nLong Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu,\nHuaming Wang, Jinyu Li, et al.,\n\u201cNeural codec lan-\nguage models are zero-shot text to speech synthesizers,\u201d\narXiv preprint arXiv:2301.02111, 2023.\n[12] Zal\u00b4an Borsos, Rapha\u00a8el Marinier, Damien Vincent, Eu-\ngene Kharitonov, Olivier Pietquin, Matt Sharifi, Do-\nminik Roblek, Olivier Teboul, David Grangier, Marco\nTagliasacchi, et al., \u201cAudiolm: a language modeling ap-\nproach to audio generation,\u201d IEEE/ACM Transactions\non Audio, Speech, and Language Processing, 2023.\n[13] Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu, \u201cFastspeech: Fast, robust\nand controllable text to speech,\u201d\nAdvances in neural\ninformation processing systems, vol. 32, 2019.\n[14] Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and\nMing Liu, \u201cNeural speech synthesis with transformer\nnetwork,\u201d\nin Proceedings of the AAAI conference on\nartificial intelligence, 2019, vol. 33, pp. 6706\u20136713.\n[15] Jonathan Shen, Ye Jia, Mike Chrzanowski, Yu Zhang,\nIsaac Elias, Heiga Zen, and Yonghui Wu, \u201cNon-attentive\ntacotron:\nRobust and controllable neural tts synthe-\nsis including unsupervised duration modeling,\u201d arXiv\npreprint arXiv:2010.04301, 2020.\n[16] Isaac Elias, Heiga Zen, Jonathan Shen, Yu Zhang,\nYe Jia, Ron J Weiss, and Yonghui Wu,\n\u201cParallel\ntacotron: Non-autoregressive and controllable tts,\u201d in\nICASSP 2021-2021 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 5709\u20135713.\n[17] Heeseung Kim, Sungwon Kim, and Sungroh Yoon,\n\u201cGuided-tts: A diffusion model for text-to-speech via\nclassifier guidance,\u201d 2022.\n[18] Jaesung Tae, Hyeongju Kim, and Taesu Kim, \u201cEditts:\nScore-based editing for controllable text-to-speech,\u201d\n2022.\n[19] Zhifang Guo, Yichong Leng, Yihan Wu, Sheng Zhao,\nand Xu Tan,\n\u201cPrompttts: Controllable text-to-speech\nwith text descriptions,\u201d in ICASSP 2023 - 2023 IEEE\nInternational Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP), 2023, pp. 1\u20135.\n[20] Richard Sproat, \u201cBoring problems are sometimes the\nmost interesting,\u201d Computational Linguistics, vol. 48,\nno. 2, pp. 483\u2013490, 2022.\n[21] Ron J Weiss, RJ Skerry-Ryan, Eric Battenberg, Soroosh\nMariooryad, and Diederik P Kingma, \u201cWave-tacotron:\nSpectrogram-free end-to-end text-to-speech synthesis,\u201d\nin ICASSP 2021-2021 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2021, pp. 5679\u20135683.\n[22] Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao,\nZhou Zhao, and Tie-Yan Liu, \u201cFastspeech 2: Fast and\nhigh-quality end-to-end text to speech,\u201d in International\nConference on Learning Representations, 2021.\n[23] Jeff Donahue, Sander Dieleman, Mikolaj Binkowski,\nErich Elsen, and Karen Simonyan, \u201cEnd-to-end adver-\nsarial text-to-speech,\u201d in International Conference on\nLearning Representations, 2021.\n[24] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova,\n\u201cBert: Pre-training of deep bidirectional\ntransformers for language understanding,\u201d in Proceed-\nings of NAACL-HLT, 2019, pp. 4171\u20134186.\n[25] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,\n\u201cU-net:\nConvolutional networks for biomedical im-\nage segmentation,\u201d in Medical Image Computing and\nComputer-Assisted Intervention\u2013MICCAI 2015: 18th\nInternational Conference, Munich, Germany, October\n5-9, 2015, Proceedings, Part III 18. Springer, 2015, pp.\n234\u2013241.\n[26] Jaehyeon Kim, Sungwon Kim, Jungil Kong, and Sun-\ngroh Yoon, \u201cGlow-tts: A generative flow for text-to-\nspeech via monotonic alignment search,\u201d in Proceed-\nings of the 34th International Conference on Neural In-\nformation Processing Systems, Red Hook, NY, USA,\n2020, NIPS\u201920, Curran Associates Inc.\n[27] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima\nSadekova, and Mikhail Kudinov,\n\u201cGrad-tts: A diffu-\nsion probabilistic model for text-to-speech,\u201d in Proceed-\nings of the 38th International Conference on Machine\nLearning, Marina Meila and Tong Zhang, Eds. 18\u201324\nJul 2021, vol. 139 of Proceedings of Machine Learning\nResearch, pp. 8599\u20138608, PMLR.\n[28] L.R. Rabiner, \u201cA tutorial on hidden markov models and\nselected applications in speech recognition,\u201d Proceed-\nings of the IEEE, vol. 77, no. 2, pp. 257\u2013286, 1989.\n[29] Yuxuan Wang, Daisy Stanton, Yu Zhang, RJ-Skerry\nRyan, Eric Battenberg, Joel Shor, Ying Xiao, Ye Jia,\nFei Ren, and Rif A Saurous, \u201cStyle tokens: Unsuper-\nvised style modeling, control and transfer in end-to-end\nspeech synthesis,\u201d in International conference on ma-\nchine learning. PMLR, 2018, pp. 5180\u20135189.\n[30] RJ Skerry-Ryan, Eric Battenberg, Ying Xiao, Yuxuan\nWang, Daisy Stanton, Joel Shor, Ron Weiss, Rob Clark,\nand Rif A Saurous, \u201cTowards end-to-end prosody trans-\nfer for expressive speech synthesis with tacotron,\u201d in\ninternational conference on machine learning. PMLR,\n2018, pp. 4693\u20134702.\n[31] Wenfu Wang, Shuang Xu, and Bo Xu, \u201cFirst Step To-\nwards End-to-End Parametric TTS Synthesis: Generat-\ning Spectral Parameters with Neural Attention,\u201d in Proc.\nInterspeech 2016, 2016, pp. 2243\u20132247.\n[32] Yuxuan Wang,\nR.J. Skerry-Ryan,\nDaisy Stanton,\nYonghui Wu, Ron Weiss, Navdeep Jaitly, Zongheng\nYang, Ying Xiao, Zhifeng Chen, Samy Bengio, Quoc\nLe, Yannis Agiomyrgiannakis, Rob Clark, and Rif\nSaurous, \u201cTacotron: Towards end-to-end speech syn-\nthesis,\u201d 08 2017, pp. 4006\u20134010.\n[33] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik\nPark, Eli Shechtman, Sylvain Paris, and Taesung Park,\n\u201cScaling up GANs for Text-to-Image Synthesis,\u201d\nin\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2023, pp. 10124\u2013\n10134.\n[34] Ethan Perez, Florian Strub, Harm de Vries, Vincent Du-\nmoulin, and Aaron Courville, \u201cFilm: Visual reasoning\nwith a general conditioning layer,\u201d 2017.\n[35] D. Griffin and Jae Lim, \u201cSignal estimation from mod-\nified short-time fourier transform,\u201d IEEE Transactions\non Acoustics, Speech, and Signal Processing, vol. 32,\nno. 2, pp. 236\u2013243, 1984.\n[36] Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb\nNoury, Norman Casagrande, Edward Lockhart, Florian\nStimberg, Aaron van den Oord, Sander Dieleman, and\nKoray Kavukcuoglu, \u201cEfficient neural audio synthesis,\u201d\nin Proceedings of the 35th International Conference on\nMachine Learning, Jennifer Dy and Andreas Krause,\nEds. 10\u201315 Jul 2018, vol. 80 of Proceedings of Machine\nLearning Research, pp. 2410\u20132419, PMLR.\n[37] Sungwon Kim, Sang-Gil Lee, Jongyoon Song, Jaehyeon\nKim, and Sungroh Yoon, \u201cFloWaveNet : A generative\nflow for raw audio,\u201d in Proceedings of the 36th Inter-\nnational Conference on Machine Learning, Kamalika\nChaudhuri and Ruslan Salakhutdinov, Eds. 09\u201315 Jun\n2019, vol. 97 of Proceedings of Machine Learning Re-\nsearch, pp. 3370\u20133378, PMLR.\n[38] Thibault Sellam, Ankur Bapna, Joshua Camp, Diana\nMackinnon, Ankur P Parikh, and Jason Riesa, \u201cSquid:\nMeasuring speech naturalness in many languages,\u201d in\nICASSP 2023-2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2023, pp. 1\u20135.\n[39] Rajeev Rikhye, Quan Wang, Qiao Liang, Yanzhang He,\nDing Zhao, Arun Narayanan, Ian McGraw, et al., \u201cPer-\nsonalized keyphrase detection using speaker and envi-\nronment information,\u201d in Proc. Interspeech. 2021, pp.\n4204\u20134208, ISCA.\n[40] Alexander C Li, Mihir Prabhudesai, Shivam Duggal,\nEllis Brown, and Deepak Pathak,\n\u201cYour diffusion\nmodel is secretly a zero-shot classifier,\u201d arXiv preprint\narXiv:2303.16203, 2023.\n[41] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter, \u201cGANs Trained\nby a Two Time-Scale Update Rule Converge to a Lo-\ncal Nash Equilibrium,\u201d Advances in neural information\nprocessing systems, vol. 30, 2017.\n[42] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and\nMatthew Sharifi, \u201cFr\u00b4echet audio distance: A metric for\nevaluating music enhancement algorithms,\u201d in Proc. In-\nterspeech. 2019, pp. 2350\u20132354, ISCA.\n"
  },
  {
    "title": "In-Context Prompt Editing For Conditional Audio Generation",
    "link": "https://arxiv.org/pdf/2311.00895.pdf",
    "upvote": "7",
    "text": "IN-CONTEXT PROMPT EDITING FOR CONDITIONAL AUDIO GENERATION\nErnie Chang\u2217\u2660, Pin-Jie Lin\u2217\u2663, Yang Li\u2660, Sidd Srinivasan\u2660, Gael Le Lan\u2660,\nDavid Kant\u2660, Yangyang Shi\u2660, Forrest Iandola\u2660, Vikas Chandra\u2660\n\u2660Meta AI\n\u2663Language Science and Technology, Saarland University\n{erniecyc, yangli1, siddsrinivasan, davidkant}@meta.com\npinjie@lst.uni-saarland.de\nABSTRACT\nDistributional shift is a central challenge in the deployment\nof machine learning models as they can be ill-equipped for\nreal-world data. This is particularly evident in text-to-audio\ngeneration where the encoded representations are easily un-\ndermined by unseen prompts, which leads to the degradation\nof generated audio \u2014 the limited set of the text-audio pairs\nremains inadequate for conditional audio generation in the\nwild as user prompts are under-specified. In particular, we\nobserve a consistent audio quality degradation in generated\naudio samples with user prompts, as opposed to training set\nprompts. To this end, we present a retrieval-based in-context\nprompt editing framework that leverages the training captions\nas demonstrative exemplars to revisit the user prompts. We\nshow that the framework enhanced the audio quality across the\nset of collected user prompts, which were edited with reference\nto the training captions as exemplars.\nIndex Terms\u2014 text-to-audio generation, prompt engineer-\ning, distributional drift\n1. INTRODUCTION\nRecently, there has been notable progress in the task of condi-\ntional text-to-audio (TTA) generation, where audio signals can\nbe synthesized from textual descriptions [1, 2]. In most setups,\ntext encoders model text prompts as priors for audio decoders\nto condition upon, and rely heavily on the amount of parallel\ntext-audio data for generalizability. Consequently, TTA mod-\nels\u2019 adaptability is constrained to the training prompt distri-\nbutions which were accessible during training, and collecting\ndata from all possible prompt distributions is impractical.\nThus, one major limitation remains as the limited ability\nto generalize across the distribution shift. This shift in text\ndistribution in the wild diverges from the training captions that\nthe model has been trained on, resulting in an inadequately\n\u2217 Equal contribution.\nequipped text encoder and leading to inaccuracies in represent-\ning unseen textual inputs. These inaccuracies further cascade\ninto errors during the subsequent decoding inference steps, hin-\ndering the overall synthesis quality. Empirically, we observe a\nmarked audio quality degradation (see Figure 1) when there\nis a distributional shift from the training prompt distribution\nPt(x) to the user prompt distribution Pu(x). The reason is\nthat the learned text representation P(z|x; \u03b8) remains constant\nwhile the acquired prior \u03b8 is unable to adapt to unseen distri-\nbution. Thus, this tendency for models to have better audio\nquality within the training prompt distribution hinders the abil-\nity of the model to be deployed in real-world settings, as it is\nimpossible to train the model on all possible data distributions\nthat it may encounter in the real world.\nIn this paper, we first discuss the distributional shift in\ndeployed conditional audio generation systems (Section 2).\nWe observed that the shift in prompts leads to lower audio\nquality measured in terms of FAD [3] and CLAP [4]. To handle\nthis shift, we propose to edit the user prompts with instruction-\ntuned large language models (LLMs) (i.e., LLaMA 2 [5]).\nHowever, using LLMs as-is results in ill-formulated prompting,\nwhich can lead to sub-par performance [6, 7]. The use of\ndemonstrative exemplars for large language models [8, 5]\nhas recently been shown to bridge the gap between seen and\nunseen prompts. To this end, we introduce a framework for\nLLM-based prompt editing with demonstrative exemplars. To\nvalidate our approach, we conducted extensive experiments\non collected user prompts consisting of a range of free-form\nentered texts.\nWe summarize our contributions as follows:\n\u2022 First, we put forward a way to quantify the distributional\nshift in prompts with feature-based KL divergence re-\nduction. We compute this distributional \u201cprompt diver-\ngence\u201d and establish its correlation to the audio quality\nin terms of FAD scores.\n\u2022 We adopted in-context learning from text-only usages\nto text-conditioned audio generation, and show that in-\ncontext prompt editing enhances the audio quality across\narXiv:2311.00895v1  [cs.SD]  1 Nov 2023\na range of evaluation metrics, including CLAP, Fr\u00e9chet\naudio distance (FAD), and human evaluation.\n\u2022 We improved upon the computationally expensive of\nprompt retrieval from large-scale datasets.\nThis is\nachieved via de-duplication of the training prompts\nwith the minHash algorithm, then using the K-means\nclustering technique to split prompts into groups for fast\nretrieval of relevant exemplars.\n2. PRELIMINARIES ON PROMPT DISTRIBUTION\nTo model two text prompt distributions, we need to first project\nthem into features where additional metrics can be computed.\nHere we first denote the training and user prompt distribution\nas {Pt, Pu} respectively, and formalize the following feature\nextraction process: (1) Given a context (or prompt) xu from\nuser prompt data Du, we retrieve a prompt xt from training\ndistribution Pt, or joint distribution with language models Pt \u222a\nPLM, which we will elaborate in Section 3. (2) Conditioned\non prompt xt, we sample the latent representation z \u223d f(xt)\nfrom trained text encoder f(\u00b7).\nNote that f(\u00b7) is a generalized text encoder, which can be\nany pretrained text encoder such as RoBERTa [9], T5 [10], or\nCLAP [11]. As such, the metric is suitable for any text encoder\nmodels, which makes the approach rather generalized.\nDistributional shift in prompts. We propose to measure the\nKL reduction as it reflects the relative divergence when user\nprompts are fed to the text encoder as opposed to the train-\ning set prompts [12]. Instead of measuring divergence at the\ntext level, we utilize the encoded text\u2019s feature distribution Z.\nHere we define the Kullback\u2013Leibler divergence (KL(P||Q))\nbetween two encoded text features P(X) and Q(X):\nKL(P||Q) =\n1\n|X|\nX\nx\u2208X\nP(x) log P(x)\nQ(x).\n(1)\nwhere each input x consists of normalized scores bounded in\n[0, 1]. The KL value is then averaged across feature channels;\nand the prompt divergence score rdiv is given as:\nrdiv =\n1\n|Z|\nX\n\u02c6ptrain\u2208Z,\nKL(\u02c6ptrain\u2225\u02c6quser) \u2212 KL(\u02c6ptrain\u2225p\u2032\nnew)\n(2)\nwhere \u02c6ptrain is the original prompt feature distribution, \u02c6quser\nis the converted prompt feature distribution, and p\u2032\nnew is the\nempirical feature distribution of the sampled prompt xt. Z is\nthe set of extracted text features (or latent code z) with text\nencoder f(\u00b7). We then examine the relationship between the\nKL reduction induced by a specific prompt editing approach\nto the resulting audio quality in terms of FAD scores in Fig-\nure 1. A smaller rdiv value indicates that the prompt-induced\naudio distribution captures the in-domain audio quality, under\nretrieved prompt distribution Pt.\n0\n0.25\n0.5\n0.75\n1\nAverage KL reduction\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nN(  FAD)\nRetrieval\nRandom\nUser\nFig. 1. Plot of average KL reduction on the n-gram feature space,\ndefined as how much the selected dataset reduces KL divergence to\nthe target distribution over just random sampling. The retrieval uses\nthe data samples from the training prompt distribution, and the user\nspecifies the input from the user prompt distribution. There is a strong\ncorrelation between KL reduction and FAD reduction.\n3. IN-CONTEXT PROMPT EDITING\nInspired by the recent successes in in-context learning [8],\nwe formulate the prompt editing process as follows: Let xu\nbe a query input prompt, written by the user, and consider\nY = {y1, . . . , ym} as the set of refined audio samples. We\nedit xu by incorporating a task instruction I and an in-context\ndemonstration set C = {c1, . . . , ck}, which consists of k\ndemonstration examples. Each ci is a caption retrieved from\ntraining prompt set Dt, and the resultant in-context prompt is\nformulated as \u02dcx = [I, C, xu]. We approximate the likelihood\nof the audio yj being representative using a scoring function f\nparameterized by \u03b8 and applied to the entire input sequence:\nP(yj|xu) \u225c f\u03b8(yj, \u02dcx; \u03b8)\n(3)\nRather than conditioning on an unseen user prompt, we\ndraw the audio signal from a surrogate distribution: \u02c6y =\narg max Pyj\u2208Y (yj|\u02dcx; \u03b8). Given the challenges posed by distri-\nbutional shift arising from disparities between training and real\ndistributions, we present a framework for in-context prompt\nediting. The framework edit user prompts into with demonstra-\ntive exemplars from the training prompt distribution. Primarily,\nthe process of editing in-context prompts based on a collection\nof training prompts Dt consists of two major steps:\n1. De-duplication to improve retrieval efficiency, since the\ndata Dt can be prohibitively large.\n2. Retrieval of demonstrative exemplars for language\nmodel inference.\nIn what follows, we provide the details of these steps.\n3.1. De-duplication\nRetrieving prompts from large-scale datasets can result in\nresource-intensive computations, especially when multiple\nChild\nIn-Context Prompt Editing Framework\nUser Prompt\nencode\nranking by distance\nm nearest exemplars\nRetrieval of demonstrative exemplars\nGenerate an audio description similar to \"Two\ngirls speak followed by gurgling\" , \"A woman\ntalking and boy talking back\" for the user\nprompt \"child\"\nK exemplars augmented\nprompt\nAudio\nEncoder\nDecoder\nTTA\nGenerating\nShift in Distribution\nquery\nedit\nquery exemplars in\nfeature space\nTraining Set\nUser\nFig. 2. Diagram depicting the process of in-context prompt editing for improved audio quality. Training set is first clustered via K-means,\nthen top-M prompts are retrieved based on user queries, of which the most similar prompt is then used as the exemplar for in-context prompt\nediting with LLM. Prior to finding representative centroids, we apply de-duplication to eliminate the nearly identical demonstrative examples in\nthe training set. This enable us to retain sufficient data to represent the data distribution while improve retrieval efficiency.\npairs of similar documents are present in the data. Thus, the\ngoal of de-duplication is to eliminate duplicate or nearly iden-\ntical items from a large sample pool. To do so, we adopt\nMinHash [13] for identifying demonstrative exemplars within\nthe training dataset. MinHash represents each document, de-\nnoted as xi and xj, using sets of n-grams, expressed as di and\ndj respectively. The similarity between these sets is measured\nusing the Jaccard Index [14] to indicate the overlap between\nthe sets. We discard high Jaccard indexes which are highly\nmatched documents for similarity greater than 0.8.\n3.2. Retrieval of demonstrative exemplars\nThe retrieval process begins with K-means clustering using the\nFaiss library [15] which is built for efficient similarity search.\nEach training text prompt is projected into embeddings with\nthe sentence encoder (SBERT [16]). For the sake of ease of\nanalysis, we use AudioCAPS [17] and BBC sounds [2] training\nprompts as exemplars for in-context prompts in clustering.\nFor each user prompt xu, we first perform similarity search\nwith the indexed clusters and obtain the top-M closest neigh-\nbors \u2013 x1, x2, ..., xk \u2013 from the training set using the distances\nwithin the sentence encoder\u2019s embedding space. Utilizing\nthese neighboring exemplars, the in-context demonstration set\nC is constructed, wherein each xi corresponds to ci. We order\nthe neighbors to satisfy d(c1) \u2264 d(ck) when i < j. This\nranking provides a natural hierarchy of sentences within the\ncluster, based on their contextual relevance to the user\u2019s query.\nConsequently, the top-M candidate prompts are selected as\nillustrative examples. We then structured the top candidate as\nin Figure 2.\n4. EXPERIMENTAL SETTINGS\nWe employ AudioLDM [2] to generate realistic speech and\npiano music audio samples. We use LLaMa-70B [18] as the\nprompt editing models, which is a decoder-only language\nmodel.\nWe collected and evaluated our approaches 1525\nfree-form user prompts (Open-prompts) as real-world test\nprompts; and evaluated on AudioCAPS [17] to see if there is\nperformance degradation if more elaborate, expert-annotated\nprompts are used instead. No training was performed except\nfor the instruction-tuning of the large language models. Au-\ndio samples are evaluated with CLAP [4] and FAD [3] for\nautomatic text-audio alignment and distance to clean audios\nrespectively. Human evaluation was performed in terms of sub-\njective (SUB) and objective (OBJ) human evaluation [2, 19]\nfor audio quality assessment by five participants. Both SUB\nand OBJ are rated on a scale of 5; and SUB is focused on\naudio quality and OBJ is measured likewise on a scale of 5\nfor relevance to the edited prompts, where scores are averaged\nover the participants.\n5. RESULTS AND ANALYSIS\nWe first demonstrate that retrieval approach (exemplar) syn-\nthesize better quality audio samples than the original user\nprompt baseline (User) across automatic metrics and human\nevaluation in Table 1, where consistent improvement is ob-\nserved across metrics, while rdiv is reduced with the guidance\nof demonstration. To show that the improvement does not\nsimply come from LLM prompt editing, we also compare\nexemplar (K=100, closest) with LLM, where text-audio align-\nment (CLAP) is increased by +0.011 and distance to clean\naudios (FAD) is further improved by +2.125. Moreover, we\nrevisit the past hypothesis that the most similar exemplars are\nthe best for in-context editing by comparing exemplar (K=100,\nfarthest) and exemplar (K=100, random), where exemplars are\nselected differently from the top-M candidates. The closest\nexemplars are more distinct examples with highest token-type\nratio in Table 2. Overall, we also found higher agreement of\nexemplar-based editing as compared with other editing tech-\nPrompting Approach\n\u2206rdiv \u2191\n\u2206 CLAP \u2191\n\u2206 KL \u2191\n\u2206 FAD \u2191\nSUB \u2191\nOBJ \u2191\nUser\n-\n-\n-\n-\n3.58\n1.54\nRandom\n0.472\n0.003\n0.1013\n0.590\n3.65\n2.56\nLLM-only\n0.044\n0.036\n0.0649\n0.943\n3.61\n2.58\nexemplar (out-domain, K=100, random)\n0.444\n0.019\n0.0660\n1.433\n3.66\n2.61\nexemplar (in-domain, K=100, random)\n0.439\n0.025\n0.0469\n1.803\n3.72\n2.62\nexemplar (in-domain, K=100, farthest)\n0.464\n0.046\n0.0577\n2.203\n3.78\n2.64\nexemplar (in-domain, K=50, closest)\n0.520\n0.042\n0.0813\n2.860\n3.84\n2.69\nexemplar (in-domain, K=100, closest)\n0.594\n0.047\n0.1453\n3.068\n3.832\n2.68\nTable 1. The comparison between retrieval-based approaches and baseline TTA generation models on the collected 1,525 user prompts (User).\nOther approaches include random prompt retrieval from training set using user prompt as queries (Random), and using LLM to edit the user\nprompt directly without exemplars are demonstrations (LLM). The proposed approaches uses in-context editing of prompt with exemplars from\n(1) out-domain text drawn from wiki-103 with fixed length window size of 10, and from (2) in-domain AudioCAPS and BBC sound prompts\nwhich AudioLDM learned from. We fix the retrieved candidate to up to K = 100 and experimented with various settings within K.\nModel\n#T\nTTR(%)\n\u2206rdiv \u2191\nRandom\n148\n9.56\n0.474\nFarthest\n147\n9.61\n0.422\nClosest\n117\n10.38\n0.602\nTable 2. #T denote as the number of tokens and TTR(%) refers to\nthe type token ratios and the prompt divergence (\u2206rdiv).\nniques. The greatest audio improvement comes from the use\nof LLM editing with in-context learning, even out-performing\npure LLM technique by up to +0.23 in subjective icashuman\nevaluation.\n5.1. Correlation of prompt divergence with audio quality\nHere we intend to show the impact of exemplars from the per-\nspective of prompt divergence metric in Table 1 and Figure 3.\nPrimarily, the average KL reduction is linearly proportional to\nthe quality in terms of normalized FAD reduction (N(\u2206 FAD)),\nwhich measures the reduced distance to clean audio samples.\nThis allows us to deduce the usefulness of retrieved prompts\nas exemplars in terms of LLM prompt editing. Further, we\ncompared in- and out-domain prompts in order to show that\nin-domain prompts as demonstrative exemplars are more effec-\ntive in driving up the audio qualities. We show in Table 1 this\ncomparison, and see that the domain relevance does indeed\nhelp in the editing process, exhibiting a +0.37 improvement\nin FAD and the increases in both human evaluations.\n5.2. On the inference efficiency\nWhile in-context retrieval improves audio quality, one major\nconcern remains in terms of its efficiency since the method\nis employed at inference time. In fact, we observe that on\na Intel Xeon CPU with FAISS implementation, the average\nsearch time for K = 100 candidate is 2.13 seconds; and only\nscale approximately linearly with the total number of training\n0\n0.25\n0.5\n0.75\n1\nAverage KL reduction\n0.2\n0.4\n0.6\n0.8\n1.0\nN(  FAD)\nRetrieval+Editing\nEditing\nUser\nFig. 3. Plot of average KL reduction on the n-gram feature space,\ndefined as how much the retrieved prompt sets reduces KL divergence\nto the training distribution. There is a strong correlation between KL\nreduction and the audio quality in terms of FAD reduction.\nsamples. In general, several factors including (1) number of\nclusters: a higher number of cluster corresponds to a better\nperformance, though the difference makes up a minor degree\nin some instances, as the LLM editing is also crucial in this\nprocess. (2) The number of retrieved candidates: since we\nre-compute the similarity of each candidate with user query,\nthe size of the pool will directly (linearly) influence the speed\nof inference. (3) Size of dimension: The size of sentence\nembedding is fixed at 384 due to S-BERT.\n6. CONCLUSIONS\nIn this work, we address the challenge of distributional shift\nwhen the text-to-audio generation models are conditioned on\nunder-specified user prompts. We propose to edit user prompts\nwith demonstrative exemplars, where training captions are\nused as demonstrations for the LLMs to better make the edits.\nWe observed consistent improvement in audio quality as the\ncaptions are now closer in distribution to the training captions.\nOur approach is simple and requires no retraining of models,\nand can be easily adopted to any text-based audio pipelines.\n7. REFERENCES\n[1] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang,\nChao Weng, Yuexian Zou, and Dong Yu, \u201cDiffsound:\nDiscrete diffusion model for text-to-sound generation,\u201d\nIEEE/ACM Transactions on Audio, Speech, and Lan-\nguage Processing, 2023.\n[2] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,\nDanilo Mandic, Wenwu Wang, and Mark D Plumbley,\n\u201cAudioldm: Text-to-audio generation with latent diffusion\nmodels,\u201d arXiv preprint arXiv:2301.12503, 2023.\n[3] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and\nMatthew Sharifi, \u201cFr\u00e9chet audio distance: A metric for\nevaluating music enhancement algorithms,\u201d 2019.\n[4] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is-\nmail, and Huaming Wang, \u201cClap: Learning audio con-\ncepts from natural language supervision,\u201d in ICASSP\n2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2023, pp.\n1\u20135.\n[5] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Bap-\ntiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar,\net al., \u201cLlama: Open and efficient foundation language\nmodels,\u201d arXiv preprint arXiv:2302.13971, 2023.\n[6] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer,\n\u201cRethinking the role of demonstrations: What makes in-\ncontext learning work?,\u201d in Proceedings of the 2022\nConference on Empirical Methods in Natural Language\nProcessing, 2022, pp. 11048\u201311064.\n[7] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.,\n\u201cChain-of-thought prompting elicits reasoning in large\nlanguage models,\u201d Advances in Neural Information Pro-\ncessing Systems, vol. 35, pp. 24824\u201324837, 2022.\n[8] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christo-\npher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin,\nScott Gray, Benjamin Chess, Jack Clark, Christopher\nBerner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei,\n\u201cLanguage models are few-shot\nlearners,\u201d CoRR, vol. abs/2005.14165, 2020.\n[9] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov, \u201cRoberta: A robustly\noptimized bert pretraining approach,\u201d arXiv preprint\narXiv:1907.11692, 2019.\n[10] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu, \u201cExploring the limits of transfer\nlearning with a unified text-to-text transformer,\u201d Journal\nof Machine Learning Research, vol. 21, pp. 1\u201367, 2020.\n[11] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Tay-\nlor Berg-Kirkpatrick, and Shlomo Dubnov, \u201cLarge-scale\ncontrastive language-audio pretraining with feature fu-\nsion and keyword-to-caption augmentation,\u201d in ICASSP\n2023-2023 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2023, pp.\n1\u20135.\n[12] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and\nPercy Liang, \u201cData selection for language models via im-\nportance resampling,\u201d arXiv preprint arXiv:2302.03169,\n2023.\n[13] A. Broder, \u201cOn the resemblance and containment of doc-\numents,\u201d in Proceedings of the Compression and Com-\nplexity of Sequences 1997, USA, 1997, SEQUENCES\n\u201997, p. 21, IEEE Computer Society.\n[14] Paul Jaccard, \u201cThe distribution of the flora in the alpine\nzone. 1,\u201d New phytologist, vol. 11, no. 2, pp. 37\u201350,\n1912.\n[15] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou, \u201cBillion-\nscale similarity search with GPUs,\u201d IEEE Transactions\non Big Data, vol. 7, no. 3, pp. 535\u2013547, 2019.\n[16] Nils Reimers and Iryna Gurevych, \u201cSentence-bert: Sen-\ntence embeddings using siamese bert-networks,\u201d 2019.\n[17] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee,\nand Gunhee Kim, \u201cAudioCaps: Generating captions for\naudios in the wild,\u201d in Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), Minneapo-\nlis, Minnesota, June 2019, pp. 119\u2013132, Association for\nComputational Linguistics.\n[18] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad\nAbdul-Mageed, and Alham Fikri Aji, \u201cLamini-lm: A\ndiverse herd of distilled models from large-scale instruc-\ntions,\u201d arXiv preprint arXiv:2304.14402, 2023.\n[19] Andrea Agostinelli, Timo I Denk, Zal\u00e1n Borsos, Jesse En-\ngel, Mauro Verzetti, Antoine Caillon, Qingqing Huang,\nAren Jansen, Adam Roberts, Marco Tagliasacchi, et al.,\n\u201cMusiclm: Generating music from text,\u201d arXiv preprint\narXiv:2301.11325, 2023.\n"
  },
  {
    "title": "RoboVQA: Multimodal Long-Horizon Reasoning for Robotics",
    "link": "https://arxiv.org/pdf/2311.00899.pdf",
    "upvote": "7",
    "text": "RoboVQA: Multimodal Long-Horizon Reasoning\nfor Robotics\nPierre Sermanet, Tianli Ding, Jeffrey Zhao, Fei Xia, Debidatta Dwibedi, Keerthana Gopalakrishnan,\nChristine Chan, Gabriel Dulac-Arnold, Sharath Maddineni, Nikhil J Joshi, Pete Florence, Wei Han,\nRobert Baruch, Yao Lu, Suvir Mirchandani, Peng Xu, Pannag Sanketi, Karol Hausman,\nIzhak Shafran, Brian Ichter, Yuan Cao\nGoogle DeepMind\nAbstract\u2014 We present a scalable, bottom-up and intrinsically\ndiverse data collection scheme that can be used for high-level\nreasoning with long and medium horizons and that has 2.2x\nhigher throughput compared to traditional narrow top-down\nstep-by-step collection. We collect realistic data by performing\nany user requests within the entirety of 3 office buildings\nand using multiple embodiments (robot, human, human with\ngrasping tool). With this data, we show that models trained\non all embodiments perform better than ones trained on the\nrobot data only, even when evaluated solely on robot episodes.\nWe explore the economics of collection costs and find that for\na fixed budget it is beneficial to take advantage of the cheaper\nhuman collection along with robot collection. We release a large\nand highly diverse (29,520 unique instructions) dataset dubbed\nRoboVQA containing 829,502 (video, text) pairs for robotics-\nfocused visual question answering. We also demonstrate how\nevaluating real robot experiments with an intervention mech-\nanism enables performing tasks to completion, making it\ndeployable with human oversight even if imperfect while also\nproviding a single performance metric. We demonstrate a single\nvideo-conditioned model named RoboVQA-VideoCoCa trained\non our dataset that is capable of performing a variety of\ngrounded high-level reasoning tasks in broad realistic settings\nwith a cognitive intervention rate 46% lower than the zero-\nshot state of the art visual language model (VLM) baseline\nand is able to guide real robots through long-horizon tasks.\nThe performance gap with zero-shot state-of-the-art models\nindicates that a lot of grounded data remains to be collected\nfor real-world deployment, emphasizing the critical need for\nscalable data collection approaches. Finally, we show that video\nVLMs significantly outperform single-image VLMs with an\naverage error rate reduction of 19% across all VQA tasks.\nThanks to video conditioning and dataset diversity, the model\ncan be used as general video value functions (e.g. success and\naffordance) in situations where actions needs to be recognized\nrather than states, expanding capabilities and environment\nunderstanding for robots. Data and videos are available at\nrobovqa.github.io\nI. INTRODUCTION\nThe field of textual high-level reasoning has seen ma-\njor breakthroughs recently with large language models\n(LLMs) [1, 2], while progress has also been made in visual\nlanguage models (VLMs) [3], high-level reasoning that is\ngrounded in the real world remains a challenging task and\ncritical for robotics. Can the state-of-the-art VLMs trained\non available multimodal datasets perform grounded tasks\nwith high accuracy in the real-world? We aim to answer the\nquestion by showing that new large scale data collection are\nstill needed to achieve lower error rates outside of lab envi-\nronments. A major difficulty for VLMs stems from the high-\ndimensionality of the real world which, accordingly requiring\nlarge amounts of multimodal data (video, language, actions)\nfor training. Hence a major contribution of our work is to\nvalidate more efficient data collection approaches than the\ntraditional top-down step-by-step collection [4], by reducing\noverheads such as resets and scene preparations and leverag-\ning the low costs of human embodiment collection. With a\ncrowd-sourced bottom-up approach where long-horizon tasks\nare decided by real users the resulting medium-horizon steps\nare naturally highly diverse, relevant and on-distribution for\nusers. Not only it is a more efficient way to collect medium-\nhorizon steps, we also get long-horizon coherent sequences\nwhich can train models to perform planning tasks. With a\n2.2x throughput increase compared to the traditional method,\nit is preferable to collect data this way even if long-horizon\ntasks are not needed. While we do collect robot actions\nin this dataset, the focus of this paper is on high-level\nreasoning tasks, we can hence train on embodiments which\ndo not come with motor commands and observe transfer of\nknowledge between embodiments. We find in Sec. IX-C that\nfor a fixed collection budget, it is beneficial for high-level\nreasoning to jointly with cheaper human embodiment even\nwhen evaluating on the robot embodiment only.\nOur contributions can be summarized as follows:\n1) We demonstrate a scalable, bottom-up and intrinsically\ndiverse data collection scheme that can be used for\nhigh-level reasoning with long and medium horizons\nand that has 2.2x higher throughput compared to\ntraditional narrow top-down step-by-step collection\nand show additional cheap human embodiment data\nimproves performance.\n2) We release a large and diverse cross-embodiment\ndataset of 829,502 (video, text) pairs for robotics-\nfocused visual question answering.\n3) We demonstrate a single video-conditioned model\ntrained on the dataset that is capable of performing\na variety of tasks with higher accuracy than baselines\nand is able to guide real robots through long-horizon\ntasks.\n4) We establish a robotics VQA benchmark and long-\nhorizon planning benchmark with an intervention\nmechanism on real robots providing a single perfor-\narXiv:2311.00899v1  [cs.RO]  1 Nov 2023\n<random task>\nQ: possible right now? A: no\n<task n>\nQ: satisfied? A: yes\nQ: what action is possible right now?\nA: <task n>\n<task n>\nQ: possible right now? A: yes\nQ: what is likely to happen next? \nA: <task n>\n<long-horizon> \nQ: immediate next step? A: <task n>\n<task n>\ndiscriminative \naffordance\n(negative)\n<task n+1>\n<task n-1>\ndiscriminative \naffordance\n(positive)\ngenerative \naffordance\nsuccess\n(positive)\nplanning\nplanning\nwith context\nplanning\nremaining steps\nfuture prediction\npast description\ncurrent goal is: <long-horizon>. \nsteps so far: 1- <task 1> \u2026 n-1- <task n-1>\nQ: immediate next step? A: <task n>\ncurrent goal is: <long-horizon>. \nsteps so far: 1- <task 1> \u2026 n-1- <task n-1>\nQ: remaining 5 step? A: 1- <task n> \u2026 <task n+4>\nQ: what just happened?\nA: <task n>\nAll medium-horizon tasks \nfrom training set\nmedium-horizon tasks:\n<long-horizon>\n\u201ccan you make me \na coffee please?\u201d\n2- Data collection\n(robot or\nhuman embodiment)\n1- Long-horizon user request\n3- Hindsight labeling\n4- Tasks Augmentation\n(free)\n<task n>\nQ: satisfied? A: no\nsuccess\n(negative)\nFig. 1: Data collection procedure: Given long-horizon user requests, a human operator teleoperates a robot to fulfill the task. Medium-\nhorizon tasks are then labeled in hindsight via crowd-sourcing, with temporal segmentation and task instruction for each segment. Finally,\nfrom a sequence of labeled segments, we automatically generate 10 types of question/answer pairs.\nmance metric and enabling performing tasks to com-\npletion, making it deployable with human oversight\neven when imperfect.\nII. DATA\nCollection & Dataset: In Fig. 1 we describe the collection\nprocess, from user request to VQA tasks generation. We\ncollect episodes from any long-horizon tasks within the en-\ntirety of 3 office buildings and with 3 embodiments (Fig. 3),\nresulting in 238 hours of video (10 days), 5,246 long-horizon\nepisodes and 92,948 medium-horizon episodes. The average\nlong-horizon episode lasts 102 seconds, the medium-horizon\naverage is 14s. Because evaluation of freeform text answers\nare performed by humans in our experiments, we keep the\nvalidation and test sets small on purpose with approximately\n1,000 VQA entries for each (coming from 50 episodes each).\nWhile there can be overlap in scenes between training and\nval/test, there is no overlap in episodes. For more statistics,\nsee Sec. IX-B.\nTask diversity: To ensure that our dataset and benchmark\ndo not overfit to a specific environment, domain or task, we\ncollect examples over a wide range of tasks compared to\nmore traditional collections [5] where a fixed and small list\nof tasks is decided in advance by researchers and engineers\nin a top-down fashion. We opt for a bottom-up approach\nwhere a large number of tasks are crowd-sourced by users\nand tele-operators. This favors breadth and a better alignment\nwith a distribution of requests coming from real users.\nThis results in high tasks diversity (26,798 unique medium-\nhorizon instructions, 2,722 unique long-horizon instructions).\nThroughput and costs: Much of the throughput gains\ncollection speedup:     x 2.2                x 6.9              x 13.8\nFig. 2: Throughput gains compared to the traditional top-down step-\nby-step collection approach. The throughput of our long-horizon\ncollection is 2.2x higher for robot collection and 13.8x higher with\nhuman bodies (compared to the robot used in our experiments).\nreported in Fig. 2 come from collecting medium-horizon\nepisodes in a continuous fashion without needing to reset\nthe scene or the robot. Note that the hindsight labeling\nprocess can be parallelized via crowd-sourcing and does not\nimpact the throughput if performed in parallel, however it\nremains a cost in the collection budget. The VQA tasks\nhowever are generated for free by taking advantage of the\nknown sequence of past and future tasks and positioning the\nquestions in time with respect to different known semantic\npoints (e.g. before or after a medium-horizon task was\nperformed).\nChain-of-Thought: Decomposing high-level goals into\nthe defined tasks allows for robots to manifest its thinking\nprocess when carrying out long-horizon plans. Moreover,\nthese tasks are provided as natural language questions and\nanswers, and can be viewed as a series of Visual Question\n2\nFig. 3: Examples of 3 embodiments in the dataset: robot, human (single) arm, human using a grasping tool.\nError rate (evaluated on human and robot data)\n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\nAll tasks\nPlanning\nPlanning with Context\nRemaining 5 Steps\nSuccess Classification\nDiscriminative \nGenerative Affordance\nPast Description\nFuture Prediction\nSayCan+RoboVQA\nGroundedDecoding+RoboVQA\nPaLM-E-562B 0-shot (1 frame)\nRoboVQA-VideoCoCa (1 frame)\nRoboVQA-VideoCoCa (16 frames)\nFig. 4: VQA Error rates: we evaluate all models on the test set using human raters. We observe that state-of-the-art methods do not\nperform well in realistic settings in zero-shot, thus motivating the need for further scalable data collections. We also observe substantial\ngains when using video (16 frames) vs image conditioning.\nAnswering (VQA) steps. This formulation is similar to chain-\nof-thought for language model prompting [6]. We also note\nconcurrent work [7] which demonstrates that mimicking\nstep-by-step human thought improves planning accuracy.\nIII. MODELS\nA. RoboVQA-VideoCoCa\nWe train a new model called RoboVQA-VideoCoCa de-\nrived from the VideoCoCa model\n[8], which is a video\nlanguage model extending CoCa\n[9]. It uses an encoder-\ndecoder architecture combining contrastive pretraining (like\nCLIP [10]) as well as generative pretraining (like SimVLM\n[11]) between video and text modalities. Unless otherwise\nstated, we use a VideoCoCa base model of 383M parame-\nters with the initial checkpoint trained on image-captioning\ntasks as the original paper did, and fine-tune the model\non the RoboVQA video-text datasets. We choose a video-\nconditioned model to explore the importance of video in\nanswering the visual questions in our dataset and find sub-\nstantial benefits to video conditioning (see Fig. 17 and 16).\nB. Baselines\nTo compare with our finetuned model, we consider the\nfollowing state-of-the-art baselines which have similar ca-\npabilities in visual question answering and planning for\nrobotics.\nPaLM-E [3] is a visual language model built from pre-\ntrained ViT [12] and PaLM [2] LLM models, which projects\nimages into the token embedding space of the pretrained\nLLM. In our experiments we test PaLM-E-562B zero-shot,\nwithout training on RoboVQA dataset. While not finetuning\nis not a head to head comparison of models, the point of\nthis comparison is establish how well state-of-the-art models\ntrained on prior datasets can perform in the real world, and\nmotivate further scalable data collection efforts to address\nthe remaining performance gap.\nPlanning Methods. We experiment with four baseline\nplanning methods: two of which use RoboVQA-VideoCoCa\nand PaLM-E (zero-shot), as end-to-end planning models. As\ntwo other baselines, we adapt the methods of SayCan [5]\nand Grounded Decoding [13], which use a text-only LLM\n(PaLM [2]) in either phrase-level or token-level decoding\nguided by a visual affordance function (using RoboVQA-\nVideoCoCa as a video value function for affordance).\nIV. BENCHMARKS\nA. VQA Benchmark\nWe first evaluate the model performance on individual\ntasks, where each task consists of a video segment and a\nquestion. The inference result is compared using exact match\nagainst prior human evaluation results stored in a central\ndatabase as correct/incorrect for the video-question pair. The\ninference results for which no match is found are then\ncollected for human raters to evaluate. During evaluation,\na human rater is presented with the exact video segment\nand question as presented to the model. The rater is asked\nto either mark the model-generated answer as correct or\nincorrect, in which case the rater can propose a correct\nanswer. All answers are added to the database, with the\ncorrectness of each answer marked accordingly.\n3\nCognitive Model\nPhysical Model Multi-turn Long-Horizon Planning\nIntervention Rate\nEvaluation #1: \npre-recorded\n44.0%\nEvaluation #2:\nlive real-world\n47.67%\nEvaluation #3: \nfully autonomous\nTraining\nInference\n(policy)\nTotal\n(per episode average)\nModel\nprocedure\nSize\ntime\n# frames\n# tasks # steps domain\nbodies\ncognitive\nphysical\naverage\n100 long-horizon multi-turn planning tasks on \n videos (robot and human embodiments)\nSayCan /\nPaLM\nLanguage\npretraining\nonly &\nRoboVQA\nA\ufb00ordance\nModel\n540B\n150h+\n(30k\na\ufb00ordances)\n1\nPre-recorded\nvideo\n100\n854\nBroad\nRobot\n&\nHuman\n(50/50%)\n98.8%\nGrounded\nDecoding /\nPaLM\n~10s\n(8\na\ufb00ordances)\n1\n95.5%\nPaLM-E\n(Zero-Shot)\nFinetuned on\nSayCan/\nFractal\n12B\n1s\n1\n81.4%\nRoboVQA-\nVideoCoCa\n(ours)\nFinetuned on\nRoboVQA\n383M\n1s\n16\n 10 long-horizon multi-turn planning tasks in a \n setting, with human teleoperation as policy\nPaLM-E\n(Zero-Shot)\nFinetuned on\nSayCan/\nFractal\n12B\n1s\n1\nLive\nhuman teleop.\n10\n~60\nBroad\nRobot\n78.2%\n\u00b1 7.6%\nRoboVQA-\nVideoCoCa\n(ours)\nFinetuned on\nRoboVQA\n383M\n1s\n16\n\u00b1 9.1%\n1 long-horizon multi-turn planning tasks in a live real-world setting with a policy X for control (\n)\nRoboVQA-\nVideoCoCa\n(ours)\nFinetuned on\nRoboVQA\n383M\n1s\n16\npolicy X\n1\n5\nNarrow\n/ Easy\nRobot\n40.0%\n0%\n(easy tasks)\n20.0%\n100%\n(teleop.)\n99.4%\n97.8%\n90.7%\n72.0%\n100%\n(teleop.)\n92.8%\n73.8%\nFig. 5: Planning benchmarks with Intervention: evaluation #1 evaluates 854 planning steps on long-horizon episodes from RoboVQA\ndataset, evaluation #2 is performed live on a robot teleoperated by a human, while evaluation #3 is controlled end-to-end by our model\nand a policy. Note that thanks to human intervention in the loop, all tasks are performed to completion even when the model makes\nmistakes.\nWe report the error rate for all models in Fig. 4 and find\nthat there remains a substantial gap in performance for zero-\nshot state-of-the-art models compared to the finetuned model.\nWhile this is not too surprising, it is a valid question to ask\nwhen seeing good qualitative results by recent VLMs. Here\nwe quantitatively prove that further scalable data collection\nefforts are required when deploying in the real world. In this\ngraph we also make the case for video conditioning over\nimage conditioning by presenting substantial gains with the\nformer.\nB. Planning Benchmark with Intervention\nIntervention: In Fig. 5, we propose 3 different evaluations\nof long-horizon planning. Each evaluation is measured by\nintervention rate, which we further decompose into cognitive\nfor the high-level text domain and physical for the low-\nlevel motor command domain. However all progress can be\nmeasured with the single intervention rate which averages the\ncognitive and physical rates. This distinction is useful when\nphysical actions are teleoperated (100% physical interven-\ntion) to decouple high-level evaluations from low-level ones.\nBecause the RoboVQA dataset is very broad and diverse, we\nneed an evaluation procedure that can test that entire breadth.\nCurrent low-level policies however tend to only perform in\nvery narrow domains, this decoupling thus allows us to test\nthe full breadth of tasks in evaluations #1 and #2. See Fig. 6\nfor an example of cognitive intervention in the chat window\nbetween the user, the model and the intervention operator.\nOffline Video Results: In evaluation #1, we run models on\n100 long-horizon episodes (robot and human embodiments)\nfrom the RoboVQA dataset which amounts to 854 planning\nsteps in total. Models are given the long-horizon instruction\nand need to output medium-horizon plans, which are graded\nby humans. Note that the SayCan and Grounded Decoding\nbaselines have slow inference time which makes them im-\npractical to run in a live settings (hence not showing in other\nevaluations). Similarly, the inference time of the PaLM-E\n562B model is too slow for real time ( 30s), so we use a\nsmaller version here. Note that despite being is 30x smaller,\nour model outperforms the state-of-the-art model by 46%.\nLive Real-world Results: In evaluation #2, the high-\nlevel models are given a long-horizon instruction and pro-\nvide medium-horizon plans in real time to a real robot\nteleoperated by a human. In evaluation #3, a policy is\ndeployed instead of a human teleoperator, but the domain\nis a lot narrower given the limited abilities of the policy. See\nvideos of these evaluations at robovqa.github.io. While with\nevaluation #3 we can obtain a much lower intervention rate\nthanks to the policy deployment, the domain is a lot narrower\nand emphasizes the need for a decoupled evaluation for high-\nlevel reasoning in broad domains.\nV. ANALYSIS\nA. Task Augmentation Matters\nIn Fig. 7 we trained models on different following set\nof tasks: planning only, context-planning only, planning +\nsuccess + affordance, context-planning + success + affor-\ndance, or all tasks. Note that when comparing planning vs.\nall tasks, the model trained on planning only sees 38M\nexamples of planning task, while the one trained on all tasks\nsees roughly 1/8 the number of samples for the planning\ntask. We find that the model trained on all tasks is often\nbetter of comparable than the models dedicated to a subset\n4\nFig. 6: Example of grounded chat with cognitive intervention. Our model \u201dBrain\u201d is tasked with the following task at the beginning\nof the chat: \u201dtake the bag and cap on the desk and hang them on the coat rack\u201d in this case. The bottom of the chat shows the most\nrecent messages. The model is ran on an existing long-horizon video from the RoboVQA dataset and produces medium-horizon plans to\nfulfill the long-horizon request. An operator is in the chatroom and validates each plan or provides a correction if incorrect. The user is\nalso able to ask questions at any point in time. Here we see that the operator intervened and the system reported a cognitive intervention\nrate of 12.5% at this point of the episode.\nTasks\nError rate (evaluated on human and robot data)\n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\nAll tasks\nPlanning\nPlanning With Context\nRemaining Steps\nSuccess\nDiscriminative Affordance\nGenerative Affordance\nPast Description\nFuture Prediction\nplanning\ncontext-planning\nplanning + success + affordance\ncontext-planning + success + affordance\nall tasks\nFig. 7: Error rates for models trained with different sets of\ntasks. Each model is trained and evaluated on the (robot + human)\ndataset, but using different subsets of tasks. We find that training on\nall tasks leads to better planning (70.9% error) compared to training\non planning only (77.2% error).\nof tasks, with the exception of the success task. For example\ntraining on all tasks leads to better planning (70.9% error)\ncompared to training on planning only (77.2% error). From\na collection cost perspective, it is interesting to note that\ndespite coming from the exact same set of instructions,\nthe free tasks augmentation yields better results at no extra\ncost, hence task augmentation matters for performance and\ncollection scalability.\nB. Tasks Transfer via Cross-Embodiment Data\nIn Fig. 14, we compare error rates on the test split using\nRoboVQA-VideoCoCa trained on robot embodiment only,\nhuman embodiment only, and their combination. The test\nset contains only robot embodiment data. Despite cross-\nembodiment, we find that errors are below 100% for all\ntasks when training on human data only, indicating human\ndata by itself is useful to acquire a grounded understanding\nof videos with robot embodiment. Furthermore, training on\nboth embodiments performs better than training on robot\ndata only, indicating that extra data with human embodiment\ndoes not hurt performance when evaluating on the robot\nembodiment. We use [5] as a baseline, which uses a small,\nfixed list of 60 tasks and can only be evaluated on the\nplanning task. We also provide the affordance answers from\nRoboVQA as affordance function to SayCan for planning.\nSimilarly, we evaluate on the joint human and robot test split\nin Fig. 15. While it is not surprising that training on both\nembodiments performs best on the robot+human test set, we\nalso shows it is the most general model as it performs better\nin all situations. More analysis is available in Sec. IX-C.\nC. Importance of Video modeling\nWe investigate performance gains from video by training\nour model with (1, 2, 4, 8, 16) frames in 16 and find\nsubstantial error reductions in Fig. 17 between 1 and 16\nframes. As expected, modeling with more frames yields\nbetter results, as it captures longer temporal dynamics for\nmore accurate visual grounding.\nD. Video Value-Functions\nWe evaluate our model as a general grounded value-\nfunction from video and observe that it can provide stable\nbinary detections as shown in Fig. 8. Moreover, when\nfiltering by the confidence of the yes/no tokens, we can\nfurther improve the accuracy of the success detection. These\nvalue functions can be used for closed-loop planning to\nknow when a step is performed. Additionally, thanks to the\n5\nFig. 8: RoboVQA-VideoCoCa used for video success detection.\nIn blue are the raw answers to the question \u201dput purple marker on\nthe table Q: satisfied? A:\u201d, the confidence is shown in red and the\nanswer filted by confidence is shown in green.\ndataset breadth and to video conditioning, the value functions\ncan give richer understanding than traditional image-based\nsuccess or affordance detectors.\nVI. RELATED WORK\nVision-Language Models. Recently many methods [10, 14,\n15, 9, 11, 16, 12] have been proposed that aim to train\nvision-language models (VLMs) on large-scale image-text\npair datasets. We find the features learned by these methods\ngeneralize to robotic datasets. In this work, we also fine-tune\na pre-trained vision language model called VideoCoCa [8]\non conversation data grounded in long-horizon videos. The\nadvantage of this VLM is that it is the encoder can consume\nfull videos which helps in fine-grained temporal reasoning\nrequired to solve the tasks introduced in the RoboVQA\nbenchmark.\nVideo Captioning. Our task is closely related to the task of\nvideo captioning [17, 18, 19, 20, 21] which is a well studied\nproblem in computer vision. In fact, we fine-tune a pre-\ntrained video-captioning model VideoCoCa on these long-\nhorizon videos. Different from the video captioning problem,\nall the videos in our fine-tuning dataset are egocentric. Also,\nwe collect segment labels for a long-horizon task executed\nby either a robot or human. Furthermore, we augment these\nsegments with a variety of question-answer pairs that add\nmore supervision to the model so that an agent can execute\nlong-horizon tasks.\nVideo Datasets with Text Annotations. Recently many\nlarge-scale video datasets have been introduced [22, 23, 24,\n25, 26, 27, 28, 29] that include videos of humans performing\ntasks with text narrations or question-answer annotations.\nEgo4D is the most similar dataset to the RoboVQA dataset\nbecause Ego4D also has egocentric view of daily human\nactivities annotated with dense narrations. However, our\ndataset differs in two key aspects. First, we collect human\nand robot interactions in the same environment. Second,\nour focus is on tasks that a robot is capable of doing.\nWe hope that by lowering the domain gap between the\nhuman and robot videos we can achieve more transfer\nfrom human videos (which are faster to collect) to robot\nvideos. [30] also explores scalable ways to collect language\ndata with unstructured play [31], however they rely on\nan LLM requiring a prompt with a scene description that\nmatches the environment\u2019s state and is limited to 25 medium-\nhorizon instructions. Like RoboVQA, TEACh[32] is another\ndataset that also contains interactive dialogues required to\nsolve household tasks. However, TEACh consists of data\nin simulated environments while our dataset is collected in\nreal kitchen and office environments with both humans and\nrobots.\nLanguage Models for Planning. [33] used a large language\nmodel (LLM) to produce plans for robotic tasks. This has\nbeen followed up by many works that also use LLMs to\nproduce feasible next steps for a robot [5, 3, 34, 35, 36].\nOne advantage of using LLMs to plan is that the output of\nthese models can be used as input to language-conditioned\npolicies [37, 4, 38] that may have been trained independently.\nIntervention Rate. Intervention Rate is a commonly used\nevaluation metric [39, 40, 41] in robotics and self-driving\ncar literature for measuring the performance of policies. In\nthis work, we use it as a metric and as a mean to perform\nall tasks to completion, a necessary condition for real-world\ndeployment.\nChain of Thought Prompting. [42, 43, 6] use the idea of\nprompting a language model with the process or steps to\nperform a reasoning task. The authors observe that prompting\nallows the model to improve performance on symbolic\nreasoning tasks like algebraic problems. Inspired by those\nresults, we also provide rationale or thought supervision to\nthe model by providing the sub-tasks as hindsight labels for\nsuccessfully achieving the long-horizon task.\nVII. LIMITATIONS\nSome long-horizon episodes may be too repetitive and\neasy, thus we have filtered out episodes with more than 5\nidentical medium-horizon steps. Subsequently we observed\ngains in generalization. Additionally we have not com-\npared the effectiveness of the proposed human-and-robot\ndataset/benchmark with human-only dataset/benchmarks like\nEgo4D [29], EpicKitchens [44] etc., which merit careful\nstudy in our future work.\nVIII. CONCLUSION\nWe have shown a long-horizon collection approach with\nhigher throughput and high diversity and breadth and re-\nleased the resulting dataset for the benefit of the robotics\ncommunity. We have demonstrated on real robots a number\nof capabilities learned with this dataset and established\nplanning benchmarks with intervention as a metric and as\na means for deployment.\nACKNOWLEDGMENT\nWe thank Tran Pham, Elio Prado, Gavin Gonzalez, Jodilyn\nPeralta, Joseph Dabis, Alex Luong, Jodexty Therlonge, Rochelle\nDela Cruz, Brianna Zitkovich, Emily Perez, Eric Tran, Huong T\nTran, Dee M, Utsav Malla, Samuel Wan, Justice Carbajal and Scott\nLehrer, Joel Magpantay, Sarah Nguyen, Steve Vega and Jonathan\nVela for their contributions to data collection.\n6\nREFERENCES\n[1] OpenAI, \u201cGpt-4 technical report,\u201d ArXiv, vol. abs/2303.08774,\n2023.\n[2] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\nA. Roberts, P. Barham, H. W. Chung, C. Sutton, S. Gehrmann\net al., \u201cPalm: Scaling language modeling with pathways,\u201d\narXiv preprint arXiv:2204.02311, 2022.\n[3] D. Driess, F. Xia, M. S. M. Sajjadi, C. Lynch, A. Chowdhery,\nB. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu, W. Huang,\nY. Chebotar, P. Sermanet, D. Duckworth, S. Levine, V. Van-\nhoucke, K. Hausman, M. Toussaint, K. Greff, A. Zeng, I. Mor-\ndatch, and P. Florence, \u201cPalm-e: An embodied multimodal\nlanguage model,\u201d in arXiv preprint arXiv:2303.03378, 2023.\n[4] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis,\nC. Finn, K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu,\nJ. Ibarz, B. Ichter, A. Irpan, T. Jackson, S. Jesmonth, N. Joshi,\nR. Julian, D. Kalashnikov, Y. Kuang, I. Leal, K.-H. Lee,\nS. Levine, Y. Lu, U. Malla, D. Manjunath, I. Mordatch,\nO. Nachum, C. Parada, J. Peralta, E. Perez, K. Pertsch,\nJ. Quiambao, K. Rao, M. Ryoo, G. Salazar, P. Sanketi,\nK. Sayed, J. Singh, S. Sontakke, A. Stone, C. Tan, H. Tran,\nV. Vanhoucke, S. Vega, Q. Vuong, F. Xia, T. Xiao, P. Xu,\nS. Xu, T. Yu, and B. Zitkovich, \u201cRt-1: Robotics trans-\nformer for real-world control at scale,\u201d in arXiv preprint\narXiv:2212.06817, 2022.\n[5] M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes,\nB. David, C. Finn, C. Fu, K. Gopalakrishnan, K. Hausman,\nA. Herzog, D. Ho, J. Hsu, J. Ibarz, B. Ichter, A. Irpan,\nE. Jang, R. J. Ruano, K. Jeffrey, S. Jesmonth, N. Joshi,\nR. Julian, D. Kalashnikov, Y. Kuang, K.-H. Lee, S. Levine,\nY. Lu, L. Luu, C. Parada, P. Pastor, J. Quiambao, K. Rao,\nJ. Rettinghouse, D. Reyes, P. Sermanet, N. Sievers, C. Tan,\nA. Toshev, V. Vanhoucke, F. Xia, T. Xiao, P. Xu, S. Xu,\nM. Yan, and A. Zeng, \u201cDo as i can and not as i say:\nGrounding language in robotic affordances,\u201d in arXiv preprint\narXiv:2204.01691, 2022.\n[6] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia,\nE. Chi, Q. Le, and D. Zhou, \u201cChain-of-thought prompting\nelicits reasoning in large language models,\u201d 2023.\n[7] S. Hu and J. Clune, \u201cThought cloning: Learning to think while\nacting by imitating human thinking,\u201d 2023.\n[8] S. Yan, T. Zhu, Z. Wang, Y. Cao, M. Zhang, S. Ghosh, Y. Wu,\nand J. Yu, \u201cVideococa: Video-text modeling with zero-shot\ntransfer from contrastive captioners,\u201d 2023.\n[9] J. Yu, Z. Wang, V. Vasudevan, L. Yeung, M. Seyedhosseini,\nand Y. Wu, \u201cCoca: Contrastive captioners are image-text\nfoundation models,\u201d 2022.\n[10] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al.,\n\u201cLearning transferable visual models from natural language\nsupervision,\u201d in International conference on machine learning.\nPMLR, 2021, pp. 8748\u20138763.\n[11] Z. Wang, J. Yu, A. W. Yu, Z. Dai, Y. Tsvetkov, and Y. Cao,\n\u201cSimvlm: Simple visual language model pretraining with\nweak supervision,\u201d 2022.\n[12] X.\nChen,\nX.\nWang,\nS.\nChangpinyo,\nA.\nPiergiovanni,\nP. Padlewski, D. Salz, S. Goodman, A. Grycner, B. Mustafa,\nL. Beyer, A. Kolesnikov, J. Puigcerver, N. Ding, K. Rong,\nH. Akbari, G. Mishra, L. Xue, A. Thapliyal, J. Bradbury,\nW. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. Riquelme,\nA. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut,\n\u201cPali: A jointly-scaled multilingual language-image model,\u201d\n2023.\n[13] W. Huang, F. Xia, D. Shah, D. Driess, A. Zeng, Y. Lu,\nP. Florence, I. Mordatch, S. Levine, K. Hausman et al.,\n\u201cGrounded decoding: Guiding text generation with grounded\nmodels for robot control,\u201d arXiv preprint arXiv:2303.00855,\n2023.\n[14] C. Jia, Y. Yang, Y. Xia, Y.-T. Chen, Z. Parekh, H. Pham,\nQ. Le, Y.-H. Sung, Z. Li, and T. Duerig, \u201cScaling up visual\nand vision-language representation learning with noisy text su-\npervision,\u201d in International Conference on Machine Learning.\nPMLR, 2021, pp. 4904\u20134916.\n[15] J. Li, D. Li, C. Xiong, and S. Hoi, \u201cBlip: Bootstrapping\nlanguage-image pre-training for unified vision-language un-\nderstanding and generation,\u201d in International Conference on\nMachine Learning.\nPMLR, 2022, pp. 12 888\u201312 900.\n[16] T. Gupta, A. Kamath, A. Kembhavi, and D. Hoiem, \u201cTo-\nwards general purpose vision systems: An end-to-end task-\nagnostic vision-language architecture,\u201d in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 16 399\u201316 409.\n[17] X. Wang, W. Chen, J. Wu, Y.-F. Wang, and W. Y. Wang,\n\u201cVideo captioning via hierarchical reinforcement learning,\u201d in\nProceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 4213\u20134222.\n[18] L. Gao, Z. Guo, H. Zhang, X. Xu, and H. T. Shen, \u201cVideo cap-\ntioning with attention-based lstm and semantic consistency,\u201d\nIEEE Transactions on Multimedia, vol. 19, no. 9, pp. 2045\u2013\n2055, 2017.\n[19] Y. Pan, T. Yao, H. Li, and T. Mei, \u201cVideo captioning with\ntransferred semantic attributes,\u201d in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017,\npp. 6504\u20136512.\n[20] H. Luo, L. Ji, B. Shi, H. Huang, N. Duan, T. Li, J. Li, T. Bharti,\nand M. Zhou, \u201cUnivl: A unified video and language pre-\ntraining model for multimodal understanding and generation,\u201d\narXiv preprint arXiv:2002.06353, 2020.\n[21] K. Lin, L. Li, C.-C. Lin, F. Ahmed, Z. Gan, Z. Liu, Y. Lu, and\nL. Wang, \u201cSwinbert: End-to-end transformers with sparse at-\ntention for video captioning,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\n2022, pp. 17 949\u201317 958.\n[22] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari,\nE. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price\net al., \u201cScaling egocentric vision: The epic-kitchens dataset,\u201d\nin Proceedings of the European Conference on Computer\nVision (ECCV), 2018, pp. 720\u2013736.\n[23] G. A. Sigurdsson, A. Gupta, C. Schmid, A. Farhadi,\nand\nK.\nAlahari,\n\u201cCharades-ego:\nA\nlarge-scale\ndataset\nof paired third and first person videos,\u201d arXiv preprint\narXiv:1804.09626, 2018.\n[24] J. Lei, L. Yu, M. Bansal, and T. L. Berg, \u201cTvqa: Localized,\ncompositional video question answering,\u201d in EMNLP, 2018.\n[25] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao,\n\u201cActivitynet-qa: A dataset for understanding complex web\nvideos via question answering,\u201d in AAAI, 2019, pp. 9127\u2013\n9134.\n[26] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev,\nand J. Sivic, \u201cHowTo100M: Learning a Text-Video Embed-\nding by Watching Hundred Million Narrated Video Clips,\u201d in\nICCV, 2019.\n[27] A. Yang, A. Miech, J. Sivic, I. Laptev, and C. Schmid, \u201cJust\nask: Learning to answer questions from millions of narrated\nvideos,\u201d in Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 2021, pp. 1686\u20131697.\n[28] J. Xiao, X. Shang, A. Yao, and T.-S. Chua, \u201cNext-qa: Next\nphase of question-answering to explaining temporal actions,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), June 2021, pp. 9777\u2013\n9786.\n[29] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari,\nR. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu et al.,\n\u201cEgo4d: Around the world in 3,000 hours of egocentric\nvideo,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 18 995\u2013\n7\n19 012.\n[30] O. Mees, J. Borja-Diaz, and W. Burgard, \u201cGrounding language\nwith visual affordances over unstructured data,\u201d in Proceed-\nings of the IEEE International Conference on Robotics and\nAutomation (ICRA), London, UK, 2023.\n[31] C. Lynch and P. Sermanet, \u201cGrounding language in play,\u201d\narXiv preprint arXiv:2005.07648, 2020. [Online]. Available:\nhttps://arxiv.org/abs/2005.07648\n[32] A. Padmakumar, J. Thomason, A. Shrivastava, P. Lange,\nA. Narayan-Chen, S. Gella, R. Piramuthu, G. Tur, and\nD. Hakkani-Tur, \u201cTEACh: Task-driven Embodied Agents that\nChat,\u201d in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 36, no. 2, 2022, pp. 2017\u20132025.\n[33] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, \u201cLanguage\nmodels\nas\nzero-shot\nplanners:\nExtracting\nactionable\nknowledge for embodied agents,\u201d CoRR, vol. abs/2201.07207,\n2022. [Online]. Available: https://arxiv.org/abs/2201.07207\n[34] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao,\nand Y. Su, \u201cLlm-planner: Few-shot grounded planning for\nembodied agents with large language models,\u201d arXiv preprint\narXiv:2212.04088, 2022.\n[35] T. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar,\nT. Lozano-P\u00b4erez, and L. P. Kaelbling, \u201cPDDL planning\nwith\npretrained\nlarge\nlanguage\nmodels,\u201d\nin\nNeurIPS\n2022 Foundation Models for Decision Making Workshop,\n2022. [Online]. Available: https://openreview.net/forum?id=\n1QMMUB4zfl\n[36] B. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas,\nand P. Stone, \u201cLlm+ p: Empowering large language mod-\nels\nwith\noptimal\nplanning\nproficiency,\u201d\narXiv\npreprint\narXiv:2304.11477, 2023.\n[37] E. Jang, A. Irpan, M. Khansari, D. Kappler, F. Ebert,\nC. Lynch, S. Levine, and C. Finn, \u201cBC-z: Zero-shot task\ngeneralization with robotic imitation learning,\u201d in 5th Annual\nConference on Robot Learning, 2021. [Online]. Available:\nhttps://openreview.net/forum?id=8kbp23tSGYv\n[38] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker,\nR. Baruch, T. Armstrong, and P. Florence, \u201cInteractive\nlanguage: Talking to robots in real time,\u201d arXiv preprint\narXiv:2210.06407, 2022.\n[39] A. Steinfeld, T. Fong, D. Kaber, M. Lewis, J. Scholtz,\nA.\nSchultz,\nand\nM.\nGoodrich,\n\u201cCommon\nmetrics\nfor\nhuman-robot interaction,\u201d in Proceedings of the 1st ACM\nSIGCHI/SIGART conference on Human-robot interaction,\n2006, pp. 33\u201340.\n[40] R. R. Murphy and D. Schreckenghost, \u201cSurvey of metrics\nfor human-robot interaction,\u201d in 2013 8th ACM/IEEE Interna-\ntional Conference on Human-Robot Interaction (HRI). IEEE,\n2013, pp. 197\u2013198.\n[41] D. Riedelbauch, N. H\u00a8ollerich, and D. Henrich, \u201cBenchmarking\nteamwork of humans and cobots\u2013an overview of metrics,\nstrategies, and tasks,\u201d IEEE Access, 2023.\n[42] W. Ling, D. Yogatama, C. Dyer, and P. Blunsom, \u201cProgram\ninduction by rationale generation: Learning to solve and\nexplain algebraic word problems,\u201d in Proceedings of the\n55th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers).\nVancouver, Canada:\nAssociation for Computational Linguistics, Jul. 2017, pp. 158\u2013\n167. [Online]. Available: https://aclanthology.org/P17-1015\n[43] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun,\nL. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano\net al., \u201cTraining verifiers to solve math word problems,\u201d arXiv\npreprint arXiv:2110.14168, 2021.\n[44] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari,\nE. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price,\nand M. Wray, \u201cScaling egocentric vision: The epic-kitchens\ndataset,\u201d\nin\nEuropean\nConference\non\nComputer\nVision\n(ECCV), 2018.\n8\nIX. APPENDIX\nA. Random frames from training set\nFig. 9: Random frames from training set.\nB. Dataset Statistics\nAs reported in Fig. 10, the entire dataset is a collection of 5246\nlong-horizon episodes (5046 for training and 100 for validation).\nEach episode has 1 long-horizon instruction and a varying number\nof medium horizon instructions that are temporally segmented.\nThere are 2638 unique long-horizon instructions in the training\nset. Each unique long-horizon instruction has an average of 2.01\nepisodes collected, median is 1 and maximum is 90. See Fig. 11\nfor the number of training episodes per long-horizon instruction.\nIn Fig. 12 we show the number of training episodes that have the\nsame long-horizon instruction as a test episode. We find that 46%\nof the test episodes do not have a long-horizon match in the training\nset. We show random frames from the training set in Fig. 9 and\nrandom long and short horizon instructions from the training set\nin IX-D. We also provide extensive analysis of the language found\nin the training set in IX-E by automatically breaking down short-\nhorizon instructions by categories (objects, actions, locations and\nattributes) using an LLM. This analysis found 2862 objects (e.g.\n\u201dtissue box\u201d, \u201dpurple color plate\u201d), 680 skills or verbs (e.g. \u201dadd\nsomething into something\u201d or \u201dgo out of a room\u201d), 3322 locations\nor spatial relations (e.g. \u201din the green plate\u201d, \u201dleft trash can\u201d) and\n901 attributes (e.g. shapes, color). Note that these numbers are\nonly indicative as some objects can be redundantly described for\nexample, see IX-E for more details.\nC. Comparing Embodiment Mixtures\nRobot collection throughput will often be a factor of the cost\nincluding time, money, tele-operator training and availability, hard-\nEntire dataset\nTraining set\nValidation set\n% of data\nVQA tasks (8 types)\n# (video, text) pairs\n829,502\n-\n798,429\n18,248\nLong-horizon instructions\n# instructions\n5,246\n-\n5,046\n100\n# unique instructions\n2,722\n-\n2,638\n94\naverage length\n163.4s (2m 7s)\n-\n163.6s\n161.0s\nMedium-horizon instructions\n# instructions\n92,948\n-\n89,227\n1,850\n# unique instructions\n26,798\n-\n25,880\n885\naverage length\n14.2s\n-\n14.2s\n13.5s\nEpisodes\n# episodes\n5,246\n100.0%\n5,046\n100\n# robot episodes\n2,350\n44.8%\n2,274\n41\n# human episodes\n2,896\n55.2%\n2,772\n59\ntotal duration\n238.0 hours\n(~10 days)\n-\n229.3 hours\n(~10 days)\n4.5 hours\naverage # medium-horizon steps per\nepisode with low overlap (<.5)\n9.5\n-\n9.5\n10.0\nLocations (# long-horizon episodes)\nBuilding 1\n3,190\n60.8%\n3,078\n58\nBuilding 2\n1,507\n28.7%\n1,442\n32\nBuilding 3\n485\n9.2%\n464\n10\nUnkown building\n64\n1.2%\n62\n0\nLanguage analysis (approximate)\n# unique objects\n2862\n-\n2773\n254\n# unique verbs\n680\n-\n671\n115\n# unique locations\n3322\n-\n3199\n220\n# unique attributes\n901\n-\n861\n108\nRobot data\n# long-horizon instructions\n2350\n-\n2274\n41\n# medium-horizon instructions\n61153\n-\n58916\n1140\n# unique long-horizon instructions\n1214\n-\n1181\n37\n# unique medium-horizon instructions\n19448\n-\n18772\n597\ntotal duration\n185.3 hours\nHuman data\n# long-horizon instructions\n2896\n-\n2772\n59\n# medium-horizon instructions\n31795\n-\n30311\n710\n# unique long-horizon instructions\n1551\n-\n1499\n57\n# unique medium-horizon instructions\n8786\n-\n8499\n300\ntotal duration\n52.7 hours\nFig. 10: Dataset statistics.\nFig. 11: Number of training episodes per unique instruction: the\nmaximum number of episodes for a unique long-horizon instruction\nis 90, the average 2.01 and the median is 1. There are 3894 training\nepisodes which yield 1939 unique long-horizon instructions.\nware maintenance etc., while humans are already expert of their\nown embodiment, collecting data with much less cost and cycle than\nrobots. When factoring in all of these parameters into a collection\nbudget, we can see that robot-to-human collection cost ratios and\nthroughputs can vary wildly depending on all of these parameters.\nIt is hence a critical question while scaling up data collection to\n9\nFig. 12: Number of training episodes that have the same long-\nhorizon instruction as a test episode. Test episodes were sampled\nrandomly and hence follow a similar distribution as observed in\nFig. 11. Among the 43 episodes in the test set, we find that 23\nof them have at least one episode with the same long-horizon\ninstruction in the training set. For 20 of them (46% of the test\nset), the long-horizon instruction is not present in the training set.\nknow which data mixture for a given budget leads to the lowest\nerror rates.\nWe explore this question in Fig. 13 by looking at the data yields\nfor a fixed collection budget of 500,000 VQA conversations, and\nreport the performance for different configurations in Figure 13-b\nto analyze the trade-offs between different mixtures. We find that\neven if the robot-human ratio is 1.0 and only evaluating on the robot\ntest set, the error rate is comparable when training on the equal\nrobot250k-human250k mixture (62.4%) compared to the full 500k\nrobot dataset (62.7%), while also being significantly lower on the\nhuman test set (53.9% vs 67.0%). Not only there is no downside\nfor the robot performance to mix human data, it also makes the\nmodel more general and usable for other applications that require\nhuman embodiment understanding.\nSimilarly we find that when the robot-human cost ratio is 4.0,\nthe performance of the mixed dataset (robot-62k + human-250k) on\nthe robot test set is similar to the robot-only 125k dataset (65.3% vs\n63.5%) while also being significantly lower on the human test set\n(51.1% vs 68.7%). We also observe that the performance gains seem\nrather small when training on 500k robot samples vs 125k, and that\nperformance on human data degrades slightly when increasing robot\ndata from 62k to 250k. We conclude that this analysis validates\nthe common intuition that human data collection is an efficient\nway to scale up data collection for robots, despite the embodiment\ndifferences.\nD. Instructions Samples\nWe print 50 random instructions from the training set for both\nlong-horizon and short-horizon below to get a sense of what the\ndata looks like.\n50 long-horizon instructions:\n\u2022 please place all of the highlighters into the pen holder\n\u2022 please clean up the spill and put cup back on mouse pad\n\u2022 Please flip the bowls and pickup the yellow, pink and green\ncandies from the floor and place them in bowls. Then restock\nthe chips into the bin.\n\u2022 please grab a small bin from the cart, place it on the table, put\nthe red pens on the table in it, then put it back on the supply\ncart\n\u2022 empty the chips onto the counter\n\u2022 Please flip the bowls and pickup the yellow, pink and green\ncandies from the floor and place them in bowls. Then place\nthe tongs into the bins.\n\u2022 Please flip the bowls and pickup the yellow, pink and green\ncandies from the floor and place them in bowls. Then pick up\nthe tongs from floor and place in bins.\n\u2022 please clean up the pistachios spill on desk\n\u2022 I am feeling a little sick, can you please get me a covid test\nin the cabinet at the end of the building, as well as return it\nback onto my desk.\n\u2022 put fruit on the bookshelf\n\u2022 fill the bowl with apples\n\u2022 prepare a cup of coffee with the espresso machine.\n\u2022 place candies into middle bowl and blue chip bag in left bowl\n\u2022 place items from counter to bin\n\u2022 I don\u2019t want the water anymore. Can you pour the water into\nthe sink and then throw the cup away\n\u2022 move items from table to cart\n\u2022 can you take the wireless mouse box out of the filing cabinet\nand put it on top of the table for me\n\u2022 I am done using the room can you turn off all the lamps.\n\u2022 Tidy up the mk table by straightening out the fruit labels,\nlining up the utensil holders and straightening the honey bottle\nplatform\n\u2022 there is rubbish on the table, please throw them away into the\ncorrect places in the disposal bins on the floor by the door\n\u2022 i\u2019m done writing in my notebook, please close it up and return\nthe pen to the pen holder\n\u2022 please bring my green shopping bag from the coat rack to the\ntable\n\u2022 separate the toys and microfiber cloths into different baskets.\n\u2022 please remove the chips from the bowl and place them in the\ntop draw.\n\u2022 I am done drinking the coffee can you throw it in a trash can\nand get me some laffy taffy from MK kitchen to my desk.\n\u2022 please put the sugar packets in the tray\n\u2022 Can you refill my water cup and replace the cap and straw?\n\u2022 Restock the Numi tea boxes into the correct places\n\u2022 put the chips in the bin.\n\u2022 put all the snacks in the tray.\n\u2022 move the mouse box from the Whitney conference room to\nthe dining booth\n\u2022 Please place the cookie squares into the tray.\n\u2022 please stock caddy for phone room\n\u2022 pick the apple out of the jar and take it to phone room 2a3\n\u2022 place only the green pears in the bowl\n\u2022 Restock the ice packs and bandage rolls\n\u2022 put all the screwdrivers in the cup\n\u2022 please get the colored plastic cups from the top drawer and\nput them on the countertop\n\u2022 empty bin onto the table\n\u2022 open locker 17. then bring bag of chips from desk 2p2a to\nlocker. close locker 17.\n\u2022 throw away the cocunut water\n\u2022 Put the red pens in the cup and bring them to a table in the\nmk, then bring the large postit notes to the table also\n\u2022 make a virtal line of the plants and sort them by hight\n\u2022 please pick up the trash on the table and throw it away into\nthe compost\n\u2022 bring a usb c charger from the bookshelf to the desk in the\nwhitney room\n\u2022 take out duck from plate on counter in a group\n\u2022 put duck into the basket\n\u2022 i\u2019m finished with this hint water, please go recycle it in the\nmicro kitchen for me and then bring me back a bag of lesser\nevil popcorn, cheese flavor\n\u2022 Please flips the bowls then seperate the green, yellow and\npink candy. Then remove the tongs and the forks from bins\nand place them on table.\n\u2022 put the fruits in the basket\n10\nrobot cost = 1x human cost\n(a)\n(b)\nReference budget for \nother mixtures on the right\nrobot cost = 2x human cost\nrobot cost = 4x human cost\n125k\n62k + 250k\n500k\n250k + 250k\nrobot cost = 8x human cost\nFig. 13: Possible embodiment mixtures for a fixed collection budget. This graph illustrates the possible trade-offs in total amounts of\nVQA samples collected for a fixed collecting budget and depending on the collection cost ratios between robot and human embodiments.\nIn (a) we simulate different cost ratios by reducing the dataset size of the robot-embodiment dataset while keeping an equal budget for\neach embodiment. We calibrate this graph with a reference fixed budget that can produce approximately 500,000 VQA conversations at\nhuman collection cost. In (b) we report the error rates of each mixture (average error rate over all tasks). We find that mixing embodiments\nis overall beneficial even when the collection costs are the same and even when evaluating on the robot embodiment data only.\nTasks\nError rate (evaluated on robot data only)\n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\nAll tasks\nPlanning\nPlanning With Context\nRemaining Steps\nSuccess\nDiscriminative Affordance\nGenerative Affordance\nPast Description\nFuture Prediction\nSayCan\nRoboVQA with human data (250k)\nRoboVQA with robot data (625k)\nRoboVQA with robot + human data (875k)\nFig. 14: Error rates on robot-only test set, comparing models trained on robot only, human only or both embodiments. We observed\nthat while it is not trained on robot data, the model trained on human data still performs with less than 100% error. We also find that the\ncross-embodiment training is beneficial even when evaluated on robot data only.\n50 medium-horizon instructions:\n\u2022 Touch the green bag\n\u2022 go away from the table\n\u2022 Grab the tissue\n\u2022 place the banana into the small bowl\n\u2022 drop the cups on the table\n\u2022 place strawberry hint water bottle in the tray\n\u2022 place green marker in the cup\n\u2022 Drop the green candy packet in the container\n\u2022 Place the black book on the table\n\u2022 Pick the bag on the table\n\u2022 Arrange the white packet in tray\n\u2022 open the cap of jar\n\u2022 place the yellow packet in glass\n\u2022 Put the tilted cup up right on the table\n\u2022 Release the orange marker into the left holder\n\u2022 Turn to the right\n\u2022 drop yellow candy into the left bowl\n\u2022 place the cup backward\n\u2022 drop the blue pen on a table\n\u2022 open the white box\n\u2022 Put orange bowl in the box\n11\nTasks\nError rate (evaluated on human and robot data)\n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\nAll tasks\nPlanning\nPlanning With Context\nRemaining Steps\nSuccess\nDiscriminative Affordance\nGenerative Affordance\nPast Description\nFuture Prediction\nTrained on robot data (625k)\nTrained on human data (250k)\nTrained on robot + human data (875k)\nFig. 15: Error rates on robot+human test set. While it is expected that the model trained on both embodiments performs best, this\ngraph illustrates that this model has the most breadth in capabilities and embodiments.\nError rate (evaluated on human and robot \n0.0%\n25.0%\n50.0%\n75.0%\n100.0%\nAll tasks\nPlanning\nPlanning with Context\nRemaining 5 Steps\nSuccess Classification\nDiscriminative Affordance\nGenerative Affordance\nPast Description\nFuture Prediction\n1 frame\n2 frames\n4 frames\n8 frames\n16 frames\nFig. 16: Error rates for video model trained with different number of frames. The model is trained on 875k samples (robot + human)\nand evaluated on the (robot + human) test set. We find that 16 frames yields the best results.\n\u2022 place tissue in the tray\n\u2022 Put the banana on the white table\n\u2022 move away from the rack\n\u2022 place 2 pistachio in the vessel\n\u2022 move away from the hanger\n\u2022 Place the square symbol in the baby pink box\n\u2022 Move your arm towards the right chair\n\u2022 place the lead on the glass\n\u2022 Put the paper bag in the black container\n\u2022 put paper clip in the rectangular stand\n\u2022 move to the orange packet\n\u2022 throw the tissue paper in dustbin\n\u2022 Place the red pen on the table\n\u2022 move towards the apple\n\u2022 Move away from the hint bottle\n\u2022 Go to the right side chair\n\u2022 Place the left indoor plant on the table\n\u2022 draw R on board\n\u2022 put sugar packets in the container\n\u2022 Place the 2 red packets on the table\n\u2022 move to the orange cable on the table\n\u2022 Drop the white pebble in the transparent glass\n\u2022 drop the black container in the box\n\u2022 Draw a diagonal line from left\n\u2022 place the black cart to the corner\n\u2022 Put blue cup on the table\n\u2022 drop the apple on the floor\n\u2022 Place the red can in fridge\n\u2022 pick the sanitizer\n12\nReduction in error rate 1 -> 16 frames\n0%\n10%\n20%\n30%\n40%\n50%\nAll tasks\nPlanning\nPlanning with Context\nRemaining 5 Steps\nSuccess Classification\nDiscriminative Affordance\nGenerative Affordance\nPast Description\nFuture Prediction\nFig. 17: Rate of error reductions when training a model with\n16 frames as input versus 1\nE. Dataset Language Statistics Analysis by LLM\nWe use an LLM to extract different attributes from each short-\nhorizon instruction from the training set and find:\n\u2022 1795 objects, e.g. \u201dtissue box\u201d, \u201dpurple color plate\u201d.\n\u2022 494 actions, e.g. \u201dadd something into something\u201d, \u201dgo out of\na room\u201d.\n\u2022 2064 locations, e.g. \u201din the green plate\u201d, \u201dleft trash can\u201d.\n\u2022 462 attributes, e.g. shapes, color.\nNote that no clustering is performed and these lists contain\nredundant descriptions for each categories, the counts above are\nnot meant to represent unique instances. In subsequent sections we\ndisplay the full lists for each category above along with their parent\ncategories inferred by the LLM.\n13\n"
  }
]