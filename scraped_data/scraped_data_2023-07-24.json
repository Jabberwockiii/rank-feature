[
  {
    "title": "Subject-Diffusion:Open Domain Personalized Text-to-Image Generation without Test-time Fine-tuning",
    "link": "https://arxiv.org/pdf/2307.11410.pdf",
    "upvote": "14",
    "text": "Subject-Diffusion: Open Domain Personalized\nText-to-Image Generation without Test-time\nFine-tuning\nJian Ma\nOPPO Research Institute\nmajian2@oppo.com\nJunhao Liang \u2217\nSouthern University of Science and Technology\n12132342@mail.sustech.edu.cn\nChen Chen\u0000OPPO Research Institute\nchenchen4@oppo.com\nHaonan Lu\u0000OPPO Research Institute\nluhaonan@oppo.com\nAbstract\nRecent progress in personalized image generation using diffusion models has been\nsignificant. However, development in the area of open-domain and non-fine-tuning\npersonalized image generation is proceeding rather slowly. In this paper, we\npropose Subject-Diffusion, a novel open-domain personalized image generation\nmodel that, in addition to not requiring test-time fine-tuning, also only requires\na single reference image to support personalized generation of single- or multi-\nsubject in any domain. Firstly, we construct an automatic data labeling tool and use\nthe LAION-Aesthetics dataset to construct a large-scale dataset consisting of 76M\nimages and their corresponding subject detection bounding boxes, segmentation\nmasks and text descriptions. Secondly, we design a new unified framework that\ncombines text and image semantics by incorporating coarse location and fine-\ngrained reference image control to maximize subject fidelity and generalization.\nFurthermore, we also adopt an attention control mechanism to support multi-\nsubject generation. Extensive qualitative and quantitative results demonstrate that\nour method outperforms other SOTA frameworks in single, multiple, and human\ncustomized image generation. Please refer to our project page.\n1\nIntroduction\nRecently, with the rapid development of diffusion-based generative models [24, 57, 56], many large\nsynthesis models [47, 45, 42, 51, 4, 15] trained on large-scale datasets containing billions of image-\ntext pairs, e.g., LAION-5B [52], have shown amazing text-to-image generation ability with fantastic\nartistry, authenticity and semantic alignment. However, merely textual information is insufficient to\nfully translate users\u2019 intentions. Therefore, integrating textual description and reference images to\ngenerate new customized images is an emerging research direction.\nBased on a pre-trained text-to-image generation model, e.g., Stable Diffusion [47] and Imagen [51],\na group of approaches [16, 49, 31, 58, 3, 22, 55] propose to fine-tune the models on each group of\nthe provided reference images (typically 3-5 images). Although these methods yield satisfactory\nresults, they require specialized training of the network (word embedding space [16], specific layers\nof the UNet [49, 31] or some adding side-branches [55]), which is inefficient for realistic application.\n1*Author did this work during his internship at OPPO Research Institute.\n2\u0000 Corresponding authors.\nPreprint.\narXiv:2307.11410v1  [cs.CV]  21 Jul 2023\nInput Image\nHuman Subject Generation\nruning on water\ncube shaped\npainting by Van \nGogh\nin police outfit\nin the rain\nswimming\nin batman suit\nreading book\nInput Image\nMulti Subject Generation\non dirt road\nwith Eiffel Tower\nin the snow\nInput Image\nSingle Subject Generation\nText-Image Interpolation\nInput Image\nwoman\nman\ncat\ndog\na woman in the rain\na cat on the beach\nFigure 1: Our Subject-Diffusion model is capable of generating high-fidelity subject-driven images\nusing just one reference image, without the need for any fine-tuning. This applies not only to general\nsubjects, but also to human subjects, allowing for the preservation of their identity. Furthermore, our\nmodel supports the generation of multiple subjects within a single model.\nAnother technique roadmap [63, 61, 11, 12] is to re-train the text-to-image generation base model\nby specially-designed network structures or training strategies on a large-scale personalized image\ndataset, but often resulting in inferior fidelity and generalization as compared with test-time fine-\ntuning approaches. Further more, some methods can only achieve personalized image generation\non specific domains, such as portrait [63, 54, 28], cats [54] or dogs [28]. Even though some recent\nproposed algorithms [61, 33, 40] can achieve open-domain customized image generation, they\ncan only handle single-concept issues. With regard to a single reference image, multiple concept\ngeneration, the absence of test-time fine-tuning, and open-domain zero-shot capability, we summarize\na comprehensive list of personalized image generation papers as in Appendix B. According to\nthe statistics, no algorithm is currently available that can fully satisfy the four conditions listed\nabove. As a result, we are motivated to propose the Subject-Diffusion, an open-domain personalized\ntext-to-image generation framework that only needs one reference image and doesn\u2019t require test-time\nfine-tuning.\nA large-scale training dataset with object-level segmentation masks and image-level detailed language\ndescriptions is crucial for zero-shot customized image generation. While for such a laborious\nlabeling task, publicly available datasets, including LVIS [19], ADE20K [67], COCO-stuff [6],\nVisual Genome [30] and Open Images [32], typically have insufficient image volumes ranging from\n10k to 1M or even no text description. To address the data shortage for open-domain personalized\nimage generation, we are motivated to build an automatic data labeling tool. Based on the LAION-\nAesthetics dataset [52], as shown in Fig. 2, our approach employs SAM [29], Grounding DINO [36]\nand BLIP2 [33] to obtain object-level detection boxes and segmentation masks and image-level\n2\ncaptions and tags, finally constructing a Subject-Diffusion Dataset (SDD) with 76M images and\n222M entities (refer to Sec. 3 for detailed information).\nTo achieve controllable personalized image generation in open domains, we propose a novel frame-\nwork that comprises three distinct parts. The first part involves incorporating location control by\nconcatenating the mask images of the main subjects during the noise injection process. The second\npart involves fine-grained reference image control, where we design a combined text-image informa-\ntion module to better integrate the two information granularities. We append the image representation\nto the text embedding using a specific prompt style and perform feature fusion within the text encoder.\nAdditionally, we add adapter modules to the UNet to specifically receive personalized image informa-\ntion, thereby increasing the fidelity of personalized images. For the third part, to further control the\ngeneration of multiple subjects, we introduce attention control during training.\nAs exhibited in Fig. 1, based on the constructed large-scale structured data in an open domain and our\nproposed new model architecture, Subject-Diffusion achieves remarkable fidelity and generalization,\nwhich can perform single, multiple and human subject personalized generation by modifying their\nshape, pose, background, and even style with only one reference image for each subject. In addition,\nSubject-Diffusion can also perform smooth interpolation between customized images and text\ndescriptions by specially designed denoising process. In terms of quantitative comparisons, our\nmodel outperforms or is comparable to other SOTA methods, including test-time fine-tuning and\nnon-fine-tuning approaches on the DreamBench [49] and our proposed larger open-domain test\ndataset. In summary, our contributions are threefold:\n\u2022 We design an automatic dataset construction pipeline and create a sizable and structured training\ndataset that comprises 76 million open-domain images and 222 million entities, which is highly\nadvantageous for the open-domain subject image generation task.\n\u2022 We propose a personalized image generation network based on coarse location and fine-grained\nimage control. To the best of our knowledge, it\u2019s the first work to address the challenge task of\nsimultaneously generating open-domain single- and multi-concept personalized images without\ntest-time fine-tuning, solely relying on a single reference image for each subject.\n\u2022 Both quantitative and qualitative experimental results demonstrate the excellent performance of our\nproposed Subject-Diffusion as compared with other SOTA methods.\n2\nRelated Work\n2.1\nText-to-Image Generation\nThe diffusion model [24, 41, 13, 25] has emerged as a promising direction to generate images with\nhigh fidelity and diversity based on provided textual input. GLIDE [42] utilizes an unclassified\nguide to introduce text conditions into the diffusion process. DALL-E2 [45] uses a diffusion prior\nmodule and cascading diffusion decoders to generate high-resolution images based on the CLIP text\nencoder. Imagen [51] emphasizes language understanding and suggests using a large T5 language\nmodel to better represent semantics. Latent diffusion model [47] uses an autoencoder to project\nimages into latent space and applies the diffusion process to generate latent-level feature maps. Stable\ndiffusion [47], ERNIE-ViLG2.0 [15] and ediffi [4] propose to employ cross-attention mechanism to\ninject textual condition into the diffusion generation process. Our framework is built on the basis of\nStable Diffusion [47] due to its flexible scalability and open-source nature.\n2.2\nSubject-driven Text-to-Image Generation\nCurrently, there are two main frameworks for personalized text-to-image generation from the perspec-\ntive of whether to introduce test-time fine-tuning or not. In terms of test-time fine-tuning strategies,\na group of approaches requires several personalized images containing a specific subject, and then\ndirectly fine-tunes the model using these images [49, 31, 21, 14, 9]. Another group of solutions only\nfine-tunes the token embedding of the subject to adapt to learning visual concepts [16, 20, 64, 59, 1].\nDreamBooth [49] fine-tunes the entire UNet network, while Custom Diffusion [31] only fine-tunes\nthe K and V layers of the cross-attention in the UNet network. The LoRA [27] model is further\nused to efficiently adjust the fine-tuning parameters. On the other hand, Custom Diffusion proposes\npersonalized generation of multiple subjects for the first time. SVDiff [21] constructs training data\n3\nusing cutmix and adds regularization penalties to limit the confusion of multiple subject attention\nmaps. Cones proposes concept neurons and updates only the concept neurons for a single subject\nin the K and V layers of cross-attention. For multiple personalized subject generation, the concept\nneurons of multiple trained personalized models are directly concatenated. Mix-of-Show [18] trains a\nseparate LoRA model for each subject and then performs fusion. Cones 2 [38] generates multi-subject\ncombination images by learning the residual of token embedding and controlling the attention map.\nSince the test-time fine-tuning methods suffer from the notoriously time-consuming problem, another\nresearch route involves constructing a large amount of domain-specific data or using only open-\ndomain image data for training without the need for additional fine-tuning. InstructPix2Pix [5]\ncan follow human instructions to perform various edits, including object replacement, style change,\nenvironment modification, and artistic medium, by simply concatenating the latent of the reference\nimages during the model\u2019s noise injection process. ELITE [61] proposes global and local mapping\ntraining schemes by using the OpenImages testset containing 125k images and 600 object classes as\nthe training data. However, due to the limitations of the model architecture, the text alignment effect\nis relatively medium. UMM-Diffusion presents a novel Unified Multi-Modal Latent Diffusion [40]\nthat takes joint texts and images containing specified subjects as input sequences and generates\ncustomized images with the subjects. Its limitations include not supporting multiple subjects and its\ntraining data being selected from LAION-400M [53], resulting in poor performance in generating\nrare themes. Similarly, Taming Encoder [28], InstantBooth [54], and FastComposer [63] are all\ntrained on domain-specific data. BLIP-Diffusion[33] uses OpenImages data, and due to its two-stage\ntraining scheme, it achieves good fidelity effects but does not support multi-subject generation.\nIn contrast, our model, which is trained on a sizable self-constructed open-domain dataset, performs\nexceptionally well in terms of the trade-off between fidelity and generalization in both single- and\nmulti-subject generation.\n3\nBuild the Dataset\n3.1\nMotivation\nTo endow the diffusion model with the capability of arbitrary subject image generation, a huge\nmultimodal dataset with open-domain capabilities is necessary. However, the existing image datasets\neither have a small number of images, such as COCO-Stuff [6] and OpenImages [32], or lack\nmodalities (segmentation masks and detection bounding boxes) and have inconsistent data quality,\nsuch as LAION-5B [52]. Therefore, we are inspired to utilize LAION-5B, which contains more\nthan 5 billion image-text pairs, as our starting point before filtering and processing it to create a\nhigh-quality, large-scale multimodal dataset that is suitable for our task.\nOur dataset contains 76 million examples, which is about 76 times larger than the amount of annotated\nimages in the famous OpenImages (1M images) [32]. It consists of a variety of different modalities,\nincluding image-text pairs, instance detection boxes, segmentation masks, and corresponding labels.\nFurthermore, it also covers a wide range of variations involving the capture of scenes, entity classes,\nand photography conditions (resolution, illumination, etc.). This great diversity, as well as its large\nscale, offers great potential for learning subject-driven image generation abilities in the open domain,\nwhich is believed to boost the development of generative artificial intelligence. Please refer to\nAppendix A.2 for more dataset statistics and comparisons.\n3.2\nData Collection and Processing\nAs depicted in Fig. 2, we outline the three steps we took to create our training data. The captions for\nimages provided by LAION-5B are of poor quality, often containing irrelevant or nonsensical wording.\nThis can pose a significant challenge for text-to-image tasks that require accurate image captions.\nTo address this issue, researchers have developed a new multimodal model called BLIP-2 [33],\nwhich utilizes LLMs and can perform zero-shot image-to-text generation based on natural language\ninstructions. By using BLIP-2, we can generate more precise captions for each image. However, for\nour subject-driven image generation task, we also need to obtain entities\u2019 masks and labels from the\nimages. In order to accomplish this, we perform part-of-speech analysis on the generated captions\nand treat the nouns as entity tags. Once we have obtained the entity labels, we can use the open-set\ndetection model Grounding DINO [36] to detect the corresponding location of the entity and use the\n4\nGrounding DINO\n+ SAM\nBLIP2\n\u201ca man in a cowboy \nhat and jeans is \nholding an acoustic \nguitar\u201d\njeans \nguitar\nhat\nman \nSpacy\nt\na\ng\nhat\nman\nguitar\njeans\n(c) Compose structured data:\nTraining Data Generation\n(a) Generate caption and tags: \n(b) Generate boxs and masks:\nCaption:\na man in a cowboy hat and \njeans is holding an \nacoustic guitar\nFigure 2: The procedure of training data generation. (a) We first use BLIP2 [33] to generate caption\nof the given image, then we use spaCy [26] to extract tags based on the part of speech of each word\nin the caption sentence. (b) We use the extracted tags as input to Grounding DINO [36] to obtain\ndetection boxes for each object, and these detection boxes are then used as prompt input to SAM [29]\nto obtain their respective object masks. (c) Finally, all the different modalities are combined into\nstructured data as our multimodal dataset.\ndetection box as a cue for the segmentation model SAM [29] to determine the corresponding mask.\nFinally, we combine the image-text pairs, detection boxes, segmentation masks, and corresponding\nlabels for all instances to structure the data.\nBased on the aforementioned pipeline, we apply sophisticated filtering strategies, as detailed in\nAppendix A.1, to form the final high-quality dataset called Subject-Diffusion Dataset (SDD).\n4\nMethodology\nIn this section, we will first briefly review the necessary notations of Stable Diffusion [47] for a more\nconvenient description of our proposed algorithm later on. Then, an overview of the Subject-Diffusion\nframework, followed by an explanation of how we leverage auxiliary information, will be detailed.\nFinally, we will introduce our training and inference strategies, which differ slightly from each other.\n4.1\nPreliminaries\nIn this paper, we implement our method based on an open-source image synthesis algorithm \u2013 Stable\nDiffusion [47], which enhances computational efficiency by performing the diffusion process in\nlow-dimensional latent space with the help of an auto-encoder. Specifically, for an input image x0 \u2208\nRH\u00d7W \u00d73, the encoder E of the auto-encoder transforms it into a latent representation z0 \u2208 Rh\u00d7w\u00d7c,\nwhere f = H/h = W/w is the downsampling factor and c is the latent feature dimension. The\ndiffusion process is then performed on the latent space, where a conditional UNet [48] denoiser \u03f5\u03b8 is\nemployed to predict noise \u03f5 with current timestep t, noisy latent zt and generation condition C. The\n5\nImage\nEncoder\nText\nEncoder\nReplace \nEmbeddings\na dog and a cat are liying \non piles of dirt, the dog is \n[PH_0], the cat is [PH_1] \nSelf \nAttention\nCross \nAttention\nAdapter\nU-Net\n...\nSelf \nAttention\nCross \nAttention\nAdapter\nAugment\nDenoised Image\nCaption\nSubject Segments\nInput Image\nAdd Noise\nEmbed\nDenoise\nFrozen\nTrainable\nConcat\nwv\nwk\nMLP\nwv\nwk\nFigure 3: An overview of the proposed Subject-Diffusion method based on Stable Diffusion structure.\nFor the image latent part, the image mask lm is concatenated to image latent feature zt, in the case of\nmultiple subject, the multi-subject image mask is superimposed. Then the combined latent feature z\u2032\nt\nis used as input of the UNet. As to the text condition part, first construct a special prompts template,\nthen at the embedding layer of the text encoder, the \u201cCLS\u201d embedding of the segmented image is\nused to replace the corresponding token embedding. Additionally, the regular control is applied to\nthe cross attention map of these embeddings and the shape of the actual image segmentation map. In\nthe fusion part, the patch embeddings of segmented image and bounding box coordinate information\nare integrated and trained as a separate layer of the UNet.\ncondition information C is fed into each cross attention block i of the UNet model as\nAttention(Q, K, V ) = softmax\n\u0012QKT\n\u221a\nd\n\u0013\n\u00b7 V\nwhere\nQ = W (i)\nQ \u00b7 \u03c6i(zt), K = W (i)\nK \u00b7 C, V = W (i)\nV\n\u00b7 C.\n(1)\nHere, d denotes the output dimension of key (K) and query (Q) features, \u03c6i(zt) is a flattened interme-\ndiate representation of the noisy latent zt through the UNet implementation \u03f5\u03b8, and W (i)\nQ , W (i)\nK , W (i)\nV\nare learnable projection matrices. In text-to-image scenarios, the condition C = \u03c4\u03b8(y) is produced\nby encoding the text prompts y with a pre-trained CLIP [44] text encoder \u03c4\u03b8. Therefore, the overall\ntraining objective of Stable Diffusion is defined as\nLSD = EE(x0),C,\u03f5\u223cN (0,1),t\nh\n\u2225\u03f5 \u2212 \u03f5\u03b8(zt, t, C)\u22252\n2\ni\n(2)\n4.2\nModel Overview\nThe overall training framework of our proposed Subject-Diffusion method is illustrated in Fig. 3. The\nentire framework can be divided into three parts. The first part is location control, which involves\nsplicing mask information during the noise addition process to enhance the model\u2019s local learning\nwithin the subject position box, thereby increasing image fidelity. The second part is fine-grained\nreference image control, which can be further divided into two components. The first component\nentails effectively integrating segmented reference image information through specially constructed\nprompts. This is achieved by learning the weights of the blended text encoder to simultaneously\nenhance both prompt generalization and image fidelity. For the second component, a new layer\nfor learning is added to the UNet, which receives patch embedding of the segmented image and\ncorresponding position coordinate information. The third part introduces additional control learning\nof the cross-attention map to support multi-subject learning.\n6\n4.3\nExploitation of Auxiliary Information\nThe challenge of generating personalized images in an open domain lies in the trade-off between\nimage fidelity and generalization. These two indicators exhibit a negative correlation. With the current\nstable diffusion model which only relies on textual input, image fidelity is achieved by ensuring\nthe accuracy of the recognized objects without reference to any specific image. However, when\nincorporating new image conditions into the task, the fidelity of the unique reference image must\nbe guaranteed, which will inevitably introduce new adaptations and changes to the model. To better\nadapt to these changes, we propose a new architecture that includes four aspects.\nLocation control. Unlike global conditional input such as segmentation maps, depth maps, sketches,\nand grayscale images, as in [65], personalized generation requires more attention to a specific local\narea of the image. To prevent model learning from collapsing, a location-area control is innovatively\nintroduced to decouple the distribution between different regions. Specifically, as shown in Fig. 3, a\nbinary mask feature map is generated and concatenated to the original image latent feature for a single\nsubject. For multiple subjects, we overlay the binary images of each subject and then concatenate\nthem onto the latent feature. During training, we reset the weights of the convolutional layers at the\nentrance of the U-Net to zero [5, 39]. During inference, the binary image can be specified by the user,\ndetected automatically based on the user\u2019s personalized image, or just randomly generated.\nDense image control. Generating personalized images in an open domain while ensuring the\nfidelity of the subject image with only textual input poses a significant challenge. To address this\nchallenge, we propose to incorporate dense image features as an important input condition, similar\nto the textual input condition. To ensure that the model focuses solely on the subject information\nof the image and disregards the background information, we feed the segmented subject image into\nthe CLIP [44] image encoder to obtain 256-length patch feature tokens. Furthermore, to prevent\nconfusion when generating multiple subjects, we fuse the corresponding image embedding with the\nFourier-transformed coordinate position information of the subject. Subsequently, we feed the fused\ninformation into the UNet framework for learning. To achieve this goal, we adopt a similar approach\nto GLIGEN [34]. In each Transformer block, we introduce a new learnable adapter layer between the\nself-attention layer and the cross-attention layer, which takes the fused information as input and is\ndefined as follows:\nLa := La + \u03b2 \u00b7 tanh(\u03b3) \u00b7 S([La, he]),\n(3)\nwhere La is the output of the self-attention layer, \u03b2 a constant to balance the importance of the\nadapter layer, \u03b3 a learnable scalar that is initialized as 0, S the self-attention operator, and he defined\nas follows:\nhe = MLP([v, Fourier(l)]),\n(4)\nwhere MLP(\u00b7, \u00b7) is a multi-layer perceptron that first concatenates the two inputs across the feature\ndimension: v the visual 256 patch feature tokens of an image, and l the coordinate position information\nof the subject. In the process of training the UNet model, we selectively activate the key and value\nlayers of cross-attention layers and adapter layers while freezing the remaining layers. This approach\nis adopted to enable the model to focus more on learning the adapter layer.\nFusion text encoder. Location control and dense image control both aim at enhancing the fidelity of\nthe image subject to a certain extent. Effectively integrating the conditional information of text and\nimage is equally significant for the final image generation outcome. We take into account information\nfusion from two perspectives with this goal in mind.\nFirstly, we construct a new prompts template similar to BLIP-Diffusion [33]: \u201c[text prompt], the\n[subject label 0] is [PH_0], the [subject label 1] is [PH_1], ...\u201d where \u201ctext prompt\u201d represents the\noriginal text description, \u201csubject label *\u201d represents the category label of the subject, and \u201cPH_*\u201d\nare place holders corresponding to the subject image.\nThen, in contrast to approaches [39, 54, 63, 40], we choose to fuse text and image information before\nthe text encoder. We conduct extensive experiments, showing that fusing text and image information\nbefore the text encoder and then retraining the entire text encoder has stronger self-consistency than\nfusing them later. Specifically, we replace the entity token embedding at the first embedding layer of\nthe text encoder with the image subject \u201cCLS\u201d embedding at the corresponding position, and then\nretrain the entire text encoder.\nCross attention map control. Currently, text-to-image generation models often encounter confusion\nand omissions when generating multiple entities. Most solutions involve controlling the cross-\n7\nattention map during model inference. One direct approach is to modify the region of the cross-\nattention map based on the bounding box of the entity [62, 10, 60, 43, 46, 35, 66]. Another approach\nis to guide the model to refine the cross-attention units to attend to all subject tokens in the text\nprompt and strengthen or excite their activations [8]. The proposed approaches in this study are\nprimarily based on the conclusions drawn from Prompt-to-Prompt [23]. The cross-attention in the\ntext-to-image diffusion models can reflect the positions of each generated object specified by the\ncorresponding text token, which is calculated from:\nCAl(zt, y) = Softmax(Ql(zt) \u00b7 Ll(y)T ),\n(5)\nwhere CAl(zt, y) is the cross-attention map at layer l of the denoising network between the interme-\ndiate feature of the noisy latent zt and the text token y, Ql and Ll are the query and key projections.\nFor each text token, we could get an attention map of size hl \u00d7 wl, where hl and wl are the spatial\ndimensions of the feature zt and the cross-attention mechanism within diffusion models governs the\nlayout of generated images. The scores in cross-attention maps represent the amount of information\nthat flows from a text token to a latent pixel. Similarly, we assume that subject confusion arises\nfrom an unrestricted cross-attention mechanism, as a single latent pixel can attend to all other tokens.\nTherefore, we introduce an additional loss term that encourages the model not only to reconstruct the\npixels associated with learned concepts but also to ensure that each token only attends to the image\nregion occupied by the corresponding concept.\nFor instance, as illustrated in Fig. 3, we introduce an attention map regularization term at the position\nof the entity tokens \u201cdog\u201d , \u201c[cls_0]\u201d, \u201ccat\u201d and \u201c[cls_1]\u201d. Intuitively, the positions within the area\ncontaining the entity e.g., \u201ccat\u201d, should have larger values than other positions, so we optimize zt\ntowards the target that the desired area of the object has large values by penalizing the L1 deviation\nbetween the attention maps and the corresponding segmentation maps of the entities. We choose l to\nbe the layers with hl = wl = 32, 16, 8. Formally, we incorporate the following loss terms into the\ntraining phase:\nLattn = 1\nN\nN\nX\nk=1\n|CAl(zt, y) \u2212 Mk|\n(6)\nwhere Mk is the segmentation mask of the kth object corresponding to its text token.\nObjective function. As shown in Fig. 3, given the original clear image x0 and segmented subject\nimage xs, the detected image mask lm is concatenated to the noisy image latent vector zt to form\na new latent vector z\u2032\nt = concat(zt, lm). After dimension adjustment through a convolution layer,\nthe feature vector \u02dczt = conv_in(z\u2032\nt) is fed into the UNet as the query component. In terms of the\nconditional information, given the text prompt y, C = T\u03b8(vg, ty) is fused by the text encoder T\u03b8\nfrom segmented image global embedding (vg = I\u03b8(xs)) and text token embeddings (ty) which are\nextracted from the fixed CLIP image encoder (I\u03b8) and the text embedding layer, respectively. For\nthe adapters, they receive local image patch features v and bbox coordinates l as the additional\ninformation through a MLP feature fusion. Consequently, the basic Subject-Diffusion training\nobjective is:\nL = EE(x0),y,\u03f5\u223cN (0,1),t\n\u0002\n\u2225 \u03f5 \u2212 \u03f5\u03b8(zt, t, y, xs, l, lm) \u22252\n2\n\u0003\n+ \u03bbattnLattn.\n(7)\nwhere \u03bbattn is a weighting hyper-parameter.\n4.4\nTraining and Inference Strategies\nIn terms of training, to enable the UNet to take the location mask as an additional channel alongside\nthe image latent, we adjust the input conv_in module of the UNet accordingly to accommodate the\nadditional information and update it during learning. Similarly, the adapter control module, which\nensures the fidelity of the subject image, also needs to be updated. Moreover and most importantly,\nas proposed in [44], when updating the mapping from given text to image distribution, only updating\nW (i)\nK , W (i)\nV\nin each cross-attention block i is sufficient since text features are the only input to the key\nand value projection matrix. By carefully choosing the parameters to update, our method effectively\nmaintains the model\u2019s generation performance while updating only 28% of the total parameters,\nsignificantly accelerating model convergence.\nDuring inference, the mask information lm can be directly extracted by the original reference images\nor manually specified by users. In order to enhance the flexibility of prediction, we introduce a degree\n8\nof freedom into the mask by randomly shifting and slightly adjusting the size of the subject image.\nAdditionally, to ensure the generation of images with varying aspect ratios and resolutions, we make\nminor adjustments while controlling the aspect ratio of the segmentation image.\n5\nExperiments\n5.1\nImplementation Details and Evaluation\nTraining Data. The Subject-Diffusion is trained on our proposed open-domain training dataset,\nwhich consists of 76M high-quality images and 222M entities with 162K common object classes, as\ndetailed information is provided in Sec. 3.\nImplementation details. Based on Stable Diffusion v2-base1, Subject-Diffusion consists of VAE,\nUNet (with adapter layer), text encoder, and OpenCLIP-ViT-H/142 vision encoder, comprising 2.5\nbillion parameters, out of which a mere 0.7 billion parameters (text encoder, conv_in module, adapter\nlayer, and projection matrices W (i)\nK , W (i)\nV ) are trainable. The VAE, text encoder, and UNet are\ninitialized from the Stable Diffusion checkpoints and the CLIP image encoder are loaded from the\npretrained OpenCLIP checkpoints. We set the learning rate to 3e-5, the weighting scale hyper-\nparameter \u03bbattn in Eq. (7) to 0.01 and the balance constant in Eq. (3) to 1. The entire model is trained\non 24 A100 GPUs for 300,000 steps with a batch size of 12 per GPU.\nTest benchmark. We follow the benchmark DreamBench proposed in [49] for quantitative and\nqualitative comparison. In order to further validate the model\u2019s generation capability in open domain,\nwe also utilize the validation and test data from OpenImages, which comprises 296 classes with two\ndifferent entity images in each class. In comparison, DreamBench only includes 30 classes.\nEvaluation metrics. We evaluate our method with the image-alignment and text-alignment metrics.\nFor image-alignment, we calculate the CLIP visual similarity (CLIP-I) and DINO [7] similarity\nbetween the generated images and the target concept images. For text-alignment, we calculate the\nCLIP text-image similarity (CLIP-T) between the generated images and given text prompts.\nBaseline methods. We compare several methods for personalized image generation, including\nTextual Inversion [16], DreamBooth [49] and Custom Diffusion [31]. All of these models require test-\ntime fine-tuning on personalized images in a certain category. Additionally, we compare ELITE [61]\nand BLIP-Diffusion [33], both are trained on OpenImages without test-time fine-tuning.\n5.2\nExperimental Results\nGenerating personalized images can be a resource-intensive task, with some methods requiring\nsignificant storage and computing power to fine-tune models based on user-provided photos. However,\nour method and similar ones do not require any test-time fine-tuning and can generate personalized\nimages in a zero-shot manner, making them more efficient and user-friendly. In the following sections,\nwe will present both quantitative and qualitative results of our method as compared with other SOTA\napproaches on personalized image generation in both single- and multi-subject settings.\nComparison results for single-subject. We compare our Subject-Diffusion with methods [16, 12, 49,\n61, 33] for single-subject generation. In Table 1, we follow [49, 33] and generate 6 images for each\ntext prompt provided by DreamBench, amounting in total to 4,500 images for all the subjects. We\nreport the average of the DINO, CLIP-I, and CLIP-T scores over all pairs of real and generated images.\nThe overall results show that our method significantly outperforms other methods in terms of DINO\nscore, with a score of 0.711 compared to DreamBooth\u2019s score of 0.668. Our CLIP-I and CLIP-T\nscores are also slightly higher or on par with other fine-tuning free algorithms, such as Re-Image [12],\nELITE [61] and BLIP-Diffusion [33]. Fig. 4 displays a comparison of the qualitative results of single-\nsubject image generation across various prompts, using different approaches. Excluding Textual\nInversion and ELITE, which exhibit significantly lower subject fidelity, our proposed method\u2019s subject\nfidelity and text consistency are comparable to DreamBooth and CustomDiffusion methods that\nrequire multiple images for finetuning. Furthermore, we conduct experiments on the OpenImages\n1https://huggingface.co/stabilityai/stable-diffusion-2-base\n2https://github.com/mlfoundations/open_clip\n9\na can with a mountain in the \nbackground\na toy on a cobblestone street\na boot in the jungle\nTextual \nInversion\nTextual \nInversion\nDream \nBooth\nDream \nBooth\nCustom \nDiffusion\nCustom \nDiffusion\nELITE\nOurs\nInput \nImages\nInput \nImages\nFigure 4: Qualitative result for single-subject generation. Texture Inversion [16], DreamBooth [49]\nand CustomDiffusion [31] employ all three reference images to fine-tune models, whereas only\nELITE [61] and our Subject-Diffusion can generate personalized images using a single input reference\nimage (corresponding position) without the need for finetuning.\n10\nTable 1: Quantitative single subject results. DB represents DreamBench, OIT represents OpenImage\nTestset. \u2020 indicates that the experimental value is referenced from BLIP-Diffusion [33].\nMethods\nType\nTestset\nDINO\nCLIP-I\nCLIP-T\nReal Images (Oracle)\u2020\n-\n-\n0.774\n0.885\n-\nTextual Inversion [16]\u2020\nFinetune\nDB\n0.569\n0.780\n0.255\nDreamBooth [49]\u2020\nFinetune\nDB\n0.668\n0.803\n0.305\nCustom Diffusion [31]\nFinetune\nDB\n0.643\n0.790\n0.305\nRe-Imagen [12]\u2020\nZero shot\nDB\n0.600\n0.740\n0.270\nELITE [61]\nZero shot\nDB\n0.621\n0.771\n0.293\nBLIP-Diffusion [33]\u2020\nZero shot\nDB\n0.594\n0.779\n0.300\nSubject-Diffusion (ours)\nZero shot\nDB\n0.711\u00b10.001\n0.787\u00b10.002\n0.293\u00b10.000\nOIT\n0.668\u00b10.000\n0.782\u00b10.002\n0.303\u00b10.001\nDream\nBooth\non dirty road\non the beach\non cobblestone street on wooden floor\nfloating on water\nCustom\nDiffusion\nInput \nImages\nOurs\nFigure 5: Qualitative result for multi-subject generation. Our method only utilized one image per\nsubject to generate personalized images, while the other two methods required 4 to 6 images to\nfine-tune their model.\ntestset, which has about 10\u00d7 the number of subjects as DreamBench, and our method still achieve\nhigh DINO, CLIP-I, and CLIP-T scores, revealing its generalization ability.\nComparison result for multi-subject. We conduct a comparison study on our method with two\nother approaches, i.e., DreamBooth [49] and Custom Diffusion [31]. This study involves 30 different\ncombinations of two subjects from DreamBench, details of which can be found in Appendix C. For\neach combination, we generated 6 images per prompt by utilizing 25 text prompts from DreamBench.\nAs depicted in Fig. 5, we present four prompts of generated images. Overall, our method demonstrates\nsuperior performance compared to the other two methods, particularly in maintaining subject fidelity\nin the generated images. On the one hand, images generated by the comparative methods often miss\none subject, as exemplified by DreamBooth\u2019s failure to include entities like \u201con cobblestone street\u201d\nand \u201cfloating on water\u201d, as well as Custom Diffusion\u2019s inability to accurately capture entities in\n\"on dirty road\" and \"on cobblestone street\". On the other hand, while these methods are capable of\ngenerating two subjects, the appearance features between them are noticeably leaking and mixing,\n11\nTable 2: Quantitative two subject results. ZS means zero-shot.\nMethods\nType\nDINO\nCLIP-I\nCLIP-T\nDreamBooth [49]\nFinetune\n0.430\n0.695\n0.308\nCustom Diffusion [31]\nFinetune\n0.464\n0.698\n0.300\nSubject-Diffusion (ours)\nZero shot\n0.506\u00b10.001\n0.696\u00b10.001\n0.310\u00b10.001\nTable 3: Ablation Results.\nIndex\nMethods\nDINO\nCLIP-I\nCLIP-T\n(a)\nSubject-Diffusion\n0.711\n0.787\n0.293\n(b)\ntrained on OpenImage\n0.664\u2193\n0.777\u2193\n0.294\u2191\nSingle\n(c)\nw/o location control\n0.694\u2193\n0.778\u2193\n0.275\u2193\n(d)\nw/o box coordinates\n0.732\u2191\n0.810\u2191\n0.282\u2193\nSubject\n(e)\nw/o adapter layer\n0.534\u2193\n0.731\u2193\n0.291\u2193\n(f)\nw/o attention map control\n0.692\u2193\n0.789\u2191\n0.288\u2193\n(g)\nw/o image cls feature\n0.637\u2193\n0.719\u2193\n0.299\u2191\n(a)\nSubject-Diffusion\n0.506\n0.696\n0.310\n(b)\ntrained on OpenImage\n0.491\u2193\n0.693\u2193\n0.302\u2193\nTwo\n(c)\nw/o location control\n0.477\u2193\n0.666\u2193\n0.281\u2193\n(d)\nw/o box coordinates\n0.464\u2193\n0.687\u2193\n0.305\u2193\nSubjects\n(e)\nw/o adapter layer\n0.411\u2193\n0.649\u2193\n0.307\u2193\n(f)\nw/o attention map control\n0.500\u2193\n0.688\u2193\n0.302\u2193\n(g)\nw/o image cls feature\n0.457\u2193\n0.627\u2193\n0.309\u2193\nleading to lower subject fidelity when compared to the images provided by the user. By contrast,\nthe images generated by our method effectively preserve the user-provided subjects, and each one is\naccurately produced.\nWe also conduct image similarity computations using DINO and CLIP-I, as well as text similarity\ncomputations using CLIP-T on all of the generated images, user-provided images and prompts. To\nobtain the image similarity, we average the calculated similarities between the generated image and\nthe two subjects, and the results are presented in Table 2. As a result, our approach shows obvious\nsuperiority over DreamBooth and Custom Diffusion across DINO and CLIP-T indicators, providing\ncompelling evidence of its ability to capture the subject information of user-provided images more\naccurately and display multiple entities in a single image simultaneously. These results further\nreinforce the efficacy of our method in the generation of high-quality personalized images.\n5.3\nAblation Studies\nThe ablation studies involve examining six main aspects, namely: 1) the impact of our training data;\n2) the impact of location control; 3) the effectiveness of box coordinates; 4) the effectiveness of the\nadapter layer; 5) the impact of attention map control; and 6) the impact of the image \u201cCLS\u201d feature.\nAs shown in Table 3, we present zero-shot evaluation results for both single-subject and two-subject\ncases. We observe that all the ablation settings result in weaker quantitative results than our full\nsetting.\nImpact of our training data.\nThe training data proposed in this paper consists of large-scale,\nrichly annotated images, thereby enabling our model to effectively capture the appearance features\nof any given subject. To further assess the impact of training data, we retrain our model using\nOpenImages [32] training data, limiting the categories to only 600. Our evaluation results (a) and\n(b) demonstrate that this smaller dataset leads to lower image similarity, with the DINO and CLIP-I\nscores both decreasing for single-subject and two-subject cases, which underscores the importance of\nutilizing large-scale training data in generating highly personalized images. However, the results still\nsurpass or are on par with those of ELITE and BLIP-diffusion.\n12\nTable 4: Comparison among our method and baseline approaches on single-subject human image\ngeneration. \u2020 indicates that the experimental value is referenced from FastComposer [63].\nMethod\nReference Images\u2193\nIdentity Preservation\u2191\nPrompt Consistency\u2191\nStableDiffusion\u2020\n0\n0.039\n0.268\nTextual-Inversion\u2020\n5\n0.293\n0.219\nDreamBooth\u2020\n5\n0.273\n0.239\nCustom Diffusion\u2020\n5\n0.434\n0.233\nFastComposer\u2020\n1\n0.514\n0.243\nSubject-Diffusion (ours)\n1\n0.605\n0.228\nImpact of location control.\nRecently, some methods [5, 39] argue that the location control on\ninput latent also plays an important role in preserving image fidelity. Hence, we simply remove the\nlocation control from our model to discuss its impact. The comparison between experiments (a) and\n(c) declares that, if we remove the location control, our model would meet an apparent degeneration\nover all evaluation metrics.\nEffectiveness of box coordinates.\nTo prevent any confusion between patch features from reference\nimages, we incorporate box coordinate information for each subject. Our results from experiments\n(a) and (d) indicate that the introduction of coordinate information leads to significant improvements\non two-subject generation (with the DINO score increasing by 0.042, the CLIP-I score increasing by\n0.09, and the CLIP-T score increasing by 0.005). However, the fidelity of single-subject generation\ndecreased by 0.021 for the DINO score and 0.023 for the CLIP-I score. This decline may be due to\nthe fact that, when generating a single subject, the information becomes overly redundant, making it\nchallenging for the model to grasp the key details of the subject.\nEffectiveness of adapter layer.\nThe high fidelity of our model is primarily attributed to the 256\nimage patch features input to the adapter layer. As demonstrated in experiment (e), removing this\nmodule results in a significant drop in nearly all of the metrics.\nImpact of attention map control.\nTo enhance the model\u2019s focus on semantically relevant subject\nregions within the cross-attention module, we incorporate the attention map control. Our experimental\nresults (f) clearly indicate that this operation delivers a substantial performance improvement for\ntwo-subject generation as well as a slight performance improvement for single-subject generation.\nThis difference is most likely due to the ability of the attention map control mechanism to prevent\nconfusion between different subjects.\nImpact of image \u201cCLS\u201d feature.\nTo explore the impact of image features on the diffusion model\u2019s\ncross-attention, we conduct additional experiments without the use of the image \u201cCLS\u201d feature\nand without implementing the attention map control mechanism. The results of (a), (f), and (g)\nindicate that the absence of the image \u201cCLS\u201d feature led to a significant reduction in the fidelity of\nthe subject, highlighting the significance of the feature in representing the overall image information.\nFurthermore, we observe a slight increase in the CLIP-T score, indicating a trade-off between image\nand text obedience.\n5.4\nHuman Image Generation\nDue to our method\u2019s ability to produce high-fidelity results, it is also well-suited for human image\ngeneration. To evaluate our model\u2019s effectiveness in this area, we use the single-entity evaluation\nmethod employed in FastComposer [63] and compare our model\u2019s performance to that of other\nexisting methods. The experimental results are shown in Table 4. Subject-Diffusion significantly\noutperforms all baseline approaches in identity preservation, with an exceptionally high score that\nsurpasses FastComposer\u2019s model trained on the specific portrait dataset by 0.091. However, in terms\nof prompt consistency, our method was slightly weaker than FastComposer (-0.015). We believe\nthis vulnerability could be due to our method\u2019s tendency to prioritize subject fidelity over more\nchallenging prompt words.\n13\n\uf061 \uf03d1\n2.0\n\uf061 \uf03d\n8.0\n\uf061 \uf03d\n6.0\n\uf061 \uf03d\n4.0\n\uf03d\n\uf061\nman\nwoman\ndog\ncat\nwolf\nlion\nFigure 6: Text-image interpolation. The prompts are followings: A man in the rain, the woman is\n[PH]; A dog in the snow, the cat is [PH]; A wolf plushie on the beach, the lion is [PH].\n5.5\nText-Image Interpolation\nBy utilizing the \u201c[text prompt], the [subject label] is [PH]\u201d prompt template during image generation,\nwe are able to utilize the dual semantics of both text and image to control the generated image output.\nMoreover, we could utilize text and images from distinct categories and perform interpolation of\ngenerated images by controlling the proportion of the diffusion steps.\nTo achieve this, we remove the user input image control once the image layout is generated, retaining\nonly the textual semantic control. Our step-based interpolation method is represented by the following\nformula:\n\u03f5t =\n\u001a\u03f5\u03b8(zt, t, y\u2032, xs, l, lm)\nif t > \u03b1T,\n\u03f5\u03b8(zt, t, y)\notherwise\n(8)\nIn this context, y denotes the use of the original text prompts, while y\u2032 signifies employing a\nconvoluted text template: \"[text prompt]a, the [subject]b is [cls]a\". The visualization result can be\nfound in Figure 6.\n6\nConclusion and Limitation\nConclusion\nTo date, the high cost and scarcity of manual labeling have posed significant obstacles to\nthe practical implementation of personalized image generation models. Inspired by the breakthroughs\nin zero-shot large models, this paper develops an automatic data labeling tool to construct a large-scale\nstructured image dataset. Then, we build a unified framework that combines text and image semantics\nby utilizing different levels of information to maximize subject fidelity and generalization. Our\nexperimental analysis shows that our approach outperforms existing models on the DreamBench data\nand has the potential to be a stepping stone for improving the performance of personalized image\ngeneration models in the open domain.\nLimitation\nAlthough our method is capable of zero-shot generation with any reference image in\nopen domains and can handle multi-subject scenarios, it still has certain limitations. First, our method\nfaces challenges in editing attributes and accessories within user-input images, leading to limitations\nin the scope of the model\u2019s applicability. Secondly, when generating personalized images for more\nthan two subjects, our model will fail to render harmonious images with a high probability. Moreover,\n14\nmulti-concept generation will increase the computational load slightly. In the future, we will conduct\nfurther research to address these shortcomings.\nReferences\n[1] Yuval Alaluf, Elad Richardson, Gal Metzer, and Daniel Cohen-Or.\nA neural space-time\nrepresentation for text-to-image personalization. arXiv preprint arXiv:2305.15391, 2023.\n[2] Moab Arar, Rinon Gal, Yuval Atzmon, Gal Chechik, Daniel Cohen-Or, Ariel Shamir, and\nAmit H Bermano. Domain-agnostic tuning-encoder for fast personalization of text-to-image\nmodels. arXiv preprint arXiv:2307.06925, 2023.\n[3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-Or, and Dani Lischinski. Break-a-\nscene: Extracting multiple concepts from a single image. arXiv preprint arXiv:2305.16311,\n2023.\n[4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika\nAittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models\nwith an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\n[5] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow\nimage editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 18392\u201318402, 2023.\n[6] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in\ncontext. In Proceedings of the IEEE conference on computer vision and pattern recognition,\npages 1209\u20131218, 2018.\n[7] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski,\nand Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings\nof the IEEE/CVF international conference on computer vision, pages 9650\u20139660, 2021.\n[8] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or.\nAttend-and-\nexcite: Attention-based semantic guidance for text-to-image diffusion models. arXiv preprint\narXiv:2301.13826, 2023.\n[9] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Dis-\nenbooth: Identity-preserving disentangled tuning for subject-driven text-to-image generation.\narXiv preprint arXiv:2305.03374, 2023.\n[10] Minghao Chen, Iro Laina, and Andrea Vedaldi. Training-free layout control with cross-attention\nguidance. arXiv preprint arXiv:2304.03373, 2023.\n[11] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, and\nWilliam W Cohen. Subject-driven text-to-image generation via apprenticeship learning. arXiv\npreprint arXiv:2304.00186, 2023.\n[12] Wenhu Chen, Hexiang Hu, Chitwan Saharia, and William W Cohen. Re-imagen: Retrieval-\naugmented text-to-image generator. arXiv preprint arXiv:2209.14491, 2022.\n[13] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis.\nAdvances in Neural Information Processing Systems, 34:8780\u20138794, 2021.\n[14] Zhengcong Fei, Mingyuan Fan, and Junshi Huang. Gradient-free textual inversion. arXiv\npreprint arXiv:2304.05818, 2023.\n[15] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu,\nJiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image\ndiffusion model with knowledge-enhanced mixture-of-denoising-experts. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10135\u201310145,\n2023.\n[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and\nDaniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using\ntextual inversion. arXiv preprint arXiv:2208.01618, 2022.\n[17] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or.\nEncoder-based domain tuning for fast personalization of text-to-image models. arXiv preprint\narXiv:2302.12228, 2023.\n[18] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao,\nRui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized low-rank adaptation\nfor multi-concept customization of diffusion models. arXiv preprint arXiv:2305.18292, 2023.\n15\n[19] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A dataset for large vocabulary instance\nsegmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 5356\u20135364, 2019.\n[20] Inhwa Han, Serin Yang, Taesung Kwon, and Jong Chul Ye. Highly personalized text embedding\nfor image manipulation by stable diffusion. arXiv preprint arXiv:2303.08767, 2023.\n[21] Ligong Han, Yinxiao Li, Han Zhang, Peyman Milanfar, Dimitris Metaxas, and Feng Yang.\nSvdiff: Compact parameter space for diffusion fine-tuning. arXiv preprint arXiv:2303.11305,\n2023.\n[22] Shaozhe Hao, Kai Han, Shihao Zhao, and Kwan-Yee K Wong. Vico: Detail-preserving visual\ncondition for personalized text-to-image generation. arXiv preprint arXiv:2306.00971, 2023.\n[23] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or.\nPrompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626,\n2022.\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances\nin Neural Information Processing Systems, 33:6840\u20136851, 2020.\n[25] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim\nSalimans. Cascaded diffusion models for high fidelity image generation. The Journal of\nMachine Learning Research, 23(1):2249\u20132281, 2022.\n[26] Matthew Honnibal, Ines Montani, Sofie Van Landeghem, Adriane Boyd, et al. spacy: Industrial-\nstrength natural language processing in python. 2020.\n[27] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu\nWang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685, 2021.\n[28] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han Zhang, Boqing Gong, Tingbo Hou,\nHuisheng Wang, and Yu-Chuan Su. Taming encoder for zero fine-tuning image customization\nwith text-to-image diffusion models. arXiv preprint arXiv:2304.02642, 2023.\n[29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al. Segment anything. arXiv\npreprint arXiv:2304.02643, 2023.\n[30] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations. International Journal of\nComputer Vision, 123:32\u201373, 2017.\n[31] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-\nconcept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 1931\u20131941, 2023.\n[32] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset,\nShahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images\ndataset v4: Unified image classification, object detection, and visual relationship detection at\nscale. International Journal of Computer Vision, 128(7):1956\u20131981, 2020.\n[33] Dongxu Li, Junnan Li, and Steven CH Hoi. Blip-diffusion: Pre-trained subject representation\nfor controllable text-to-image generation and editing. arXiv preprint arXiv:2305.14720, 2023.\n[34] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan\nLi, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511\u201322521,\n2023.\n[35] Luping Liu, Zijian Zhang, Yi Ren, Rongjie Huang, Xiang Yin, and Zhou Zhao. Detector\nguidance for multi-object text-to-image generation. arXiv preprint arXiv:2306.02236, 2023.\n[36] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for\nopen-set object detection. arXiv preprint arXiv:2303.05499, 2023.\n[37] Zhiheng Liu, Ruili Feng, Kai Zhu, Yifei Zhang, Kecheng Zheng, Yu Liu, Deli Zhao, Jingren\nZhou, and Yang Cao. Cones: Concept neurons in diffusion models for customized generation.\narXiv preprint arXiv:2303.05125, 2023.\n[38] Zhiheng Liu, Yifei Zhang, Yujun Shen, Kecheng Zheng, Kai Zhu, Ruili Feng, Yu Liu, Deli\nZhao, Jingren Zhou, and Yang Cao. Cones 2: Customizable image synthesis with multiple\nsubjects. arXiv preprint arXiv:2305.19327, 2023.\n16\n[39] Jian Ma, Mingjun Zhao, Chen Chen, Ruichen Wang, Di Niu, Haonan Lu, and Xiaodong Lin.\nGlyphdraw: Learning to draw chinese characters in image synthesis models coherently. arXiv\npreprint arXiv:2303.17870, 2023.\n[40] Yiyang Ma, Huan Yang, Wenjing Wang, Jianlong Fu, and Jiaying Liu. Unified multi-modal\nlatent diffusion for joint subject and text conditional image generation.\narXiv preprint\narXiv:2303.09319, 2023.\n[41] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic\nmodels. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.\n[42] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin,\nBob Mcgrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In International Conference on Machine\nLearning, pages 16784\u201316804. PMLR, 2022.\n[43] Quynh Phung, Songwei Ge, and Jia-Bin Huang. Grounded text-to-image synthesis with attention\nrefocusing. arXiv preprint arXiv:2306.05427, 2023.\n[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning,\npages 8748\u20138763. PMLR, 2021.\n[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical\ntext-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[46] Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik.\nLinguistic binding in diffusion models: Enhancing attribute correspondence through attention\nmap alignment. arXiv preprint arXiv:2306.08877, 2023.\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\n[48] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net: Convolutional networks\nfor biomedical image segmentation. In Medical Image Computing and Computer-Assisted\nIntervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9,\n2015, Proceedings, Part III 18, pages 234\u2013241. Springer, 2015.\n[49] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman.\nDreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n22500\u201322510, 2023.\n[50] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wad-\nhwa, Michael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast\npersonalization of text-to-image models. arXiv preprint arXiv:2307.06949, 2023.\n[51] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton,\nKamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35:36479\u201336494, 2022.\n[52] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,\nMehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-\n5b: An open large-scale dataset for training next generation image-text models. arXiv preprint\narXiv:2210.08402, 2022.\n[53] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton\nMullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open\ndataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[54] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instantbooth: Personalized text-to-image\ngeneration without test-time finetuning. arXiv preprint arXiv:2304.03411, 2023.\n[55] James Seale Smith, Yen-Chang Hsu, Lingyu Zhang, Ting Hua, Zsolt Kira, Yilin Shen, and\nHongxia Jin. Continual diffusion: Continual customization of text-to-image diffusion with\nc-lora. arXiv preprint arXiv:2304.06027, 2023.\n[56] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\n[57] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and\nBen Poole. Score-based generative modeling through stochastic differential equations. arXiv\npreprint arXiv:2011.13456, 2020.\n17\n[58] Yoad Tewel, Rinon Gal, Gal Chechik, and Yuval Atzmon. Key-locked rank one editing for\ntext-to-image personalization. arXiv preprint arXiv:2305.01644, 2023.\n[59] Andrey Voynov, Qinghao Chu, Daniel Cohen-Or, and Kfir Aberman. p+: Extended textual\nconditioning in text-to-image generation. arXiv preprint arXiv:2303.09522, 2023.\n[60] Ruichen Wang, Zekang Chen, Chen Chen, Jian Ma, Haonan Lu, and Xiaodong Lin. Composi-\ntional text-to-image synthesis with attention map control of diffusion models. arXiv preprint\narXiv:2305.13921, 2023.\n[61] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei Zhang, and Wangmeng Zuo. Elite:\nEncoding visual concepts into textual embeddings for customized text-to-image generation.\narXiv preprint arXiv:2302.13848, 2023.\n[62] Qiucheng Wu, Yujian Liu, Handong Zhao, Trung Bui, Zhe Lin, Yang Zhang, and Shiyu Chang.\nHarnessing the spatial-temporal attention of diffusion models for high-fidelity text-to-image\nsynthesis. arXiv preprint arXiv:2304.03869, 2023.\n[63] Guangxuan Xiao, Tianwei Yin, William T Freeman, Fr\u00e9do Durand, and Song Han. Fastcom-\nposer: Tuning-free multi-subject image generation with localized attention. arXiv preprint\narXiv:2305.10431, 2023.\n[64] Jianan Yang, Haobo Wang, Ruixuan Xiao, Sai Wu, Gang Chen, and Junbo Zhao. Controllable\ntextual inversion for personalized text-to-image generation. arXiv preprint arXiv:2304.05265,\n2023.\n[65] Lvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.05543, 2023.\n[66] Zhiyuan Zhang, Zhitong Huang, and Jing Liao. Continuous layout editing of single images\nwith diffusion models. arXiv preprint arXiv:2306.13078, 2023.\n[67] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio\nTorralba. Semantic understanding of scenes through the ade20k dataset. International Journal\nof Computer Vision, 127:302\u2013321, 2019.\n18\nAppendix\nIn this supplementary, we will first present more about the dataset construction process and detail\nstatistics in Appendix A.1. Then we summarize a comprehensive set of related work comparisons\nin Appendixe B. We further provide more information about the test dataset. And finally, more\nvisualization results of our proposed Subject-Diffusion are exhibited in Appendix D.\nA\nSubject-Diffusion Dataset\nA.1\nDataset building strategy\nTo produce our dataset, all of our training images are sampled from the LAION-Aesthetics V2 5+3\nwhich is a subset of LAION-5B with an aesthetic score greater than 5. To keep the diversity of\nimages, we only set the filter conditions for resolution, i.e., keep the images with the small side\ngreater than 1024. However, in order to ensure that the images are suitable for our subject-driven\nimage generation task, we apply several filtering rules: (1) We only keep the bounding boxes with\nan aspect ratio between 0.3 and 3; (2) We only keep images where the subject\u2019s bounding box area\nis between 0.05 and 0.7 of the total image area; (3) We filter out entities with IOU exceeding 0.8;\n(4) We remove entities that appear more than 5 times in a detection box; (5) We filter out entities\nwith detection scores below 0.2; (6) We remove images where the segmentation mask area is less\nthan 60% of the corresponding detection box area; (7) For the OpenImages training set, we filter out\nentities that appear in groups and belong to human body parts. After applying these rules, we keep 22\nmillion images for our SDD and 300,000 images for the OpenImages dataset.\nA.2\nStatistics and Comparison\nStatistics about our training data are illustrated in Fig. 7 and Table. 5. Among them, Fig. 7 presents\na comprehensive analysis of the dataset properties of our training data, which includes a detailed\ndistribution of caption length and bbox number per image. The caption length distribution reveals\nthat the majority of captions fall within a range of 5 to 15 words, with a few outliers exceeding 15\nwords. On the other hand, the bbox number per image distribution shows that most images contain\nbetween 1 and 5 bounding boxes, with a small percentage of images having more than 10 bounding\nboxes. These statistics provide valuable insights into the nature of our training data and can be used\nto inform the design of our machine learning models.\nIn Table. 5, we compare the scale of different well-annotated image datasets with the training data\nused in the study. The number of images in the datasets ranges from 0.028 million to 11 million,\nwhile the number of entities ranges from 0.7 million to 1.1 billion. In Table. 5, we compare the scale\nof different annotated image datasets to the training data used in our study. The number of images\nin these datasets ranges from 28,000 to 11 million, with the entity count ranging from 700,000 to\n1.1 billion. Although SA-1B [29] offers the highest entity count of 1.1 billion, it lacks annotated\nentity categories and tends to include small-sized masks, which is unsuitable for our image generation\npurposes. In contrast, the training dataset employed in this study comprises 76 million images and\n220 million entities, making it the largest-scale dataset available. Furthermore, it is important to\nnote that our study not only provides the number of entity classes, but also highlights the superior\ndiversity of our training data compared to other datasets. This diversity is crucial in enabling our\nmodel to comprehend and identify a wide range of reference objects in the open world. Our training\ndata includes a vast array of entities, i.e. 162K kinds of entities, ranging from common objects such\nas animals and plants to more complex entities such as vehicles and buildings. This comprehensive\ndataset ensures that our model is equipped with the necessary knowledge to accurately identify and\nclassify any reference object it encounters. Additionally, our study also takes into account the varying\ncontexts in which these entities may appear, further enhancing the robustness and adaptability of our\nmodel. Overall, our research provides a comprehensive and diverse training dataset that enables our\nmodel to effectively understand and generate reference objects in the open world.\n3https://huggingface.co/datasets/ChristophSchuhmann/improved_aesthetics_5plus\n19\n\u0000\u0001\n\u0002\u0000\u0002\u0001\n\u0003\u0000\u0003\u0001\n\u0004\u0005\u0006\u0007\b\t\n\u0005\u000b\f\r\t\u0005\u000e\t\n\u000f\u0010\r\u0011\u0005\f\n\u0000\u0012\u0000\u0003\u0012\u0001\n\u0001\u0012\u0000\u0013\u0012\u0001\n\u0002\u0000\u0012\u0000\u0002\u0003\u0012\u0001\n\u0002\u0001\u0012\u0000\u0002\u0013\u0012\u0001\n\u0003\u0000\u0012\u0000\u0000\u0001\u0002\u0003\u0001\u0004\u0005\u0006\u0007\b\u0006\t\n\u000b\f\u0001\r\n\u0000\u0001\n\u0002\n\u0003\n\u0004\n\u0005\n\u0006\n\u0007\n\b\n\u0000\t \u0000\u0000 \u0000\u0001 \u0000\u0002 \u0000\u0003 \u0000\u0004 \u0000\u0005 \u0000\u0006 \u0000\u0007 \u0000\b\n\u0001\t\n\u000b\f\r\u000e\u000f\u0010\u0011\u0012\u0013\u0011\u000e\u0012\f\u0014\u0015\u0016\u0014\u0017\u0011\u000e\u0012\u0018\u000f\u0019\u0011\u001a\u000f\u0010\u0011\u0016\r\u001b\u0017\u000f\n\t\n\u0004\n\u0000\t\n\u0000\u0004\n\u0001\t\n\u0001\u0004\n\u0002\t\n\u0000\u0001\u0002\u0003\u0001\u0004\u0005\u0006\u0007\b\u0006\t\n\u000b\f\u0001\r\nFigure 7: Dataset properties. Left: word count distribution of captions in SDD; Middle: bounding\nbox count distribution of images in SSD; Right: Word cloud diagram of SDD. We can observe that\nthe most frequent entities in our SDD are man, woman, people, table, room, etc.\nTable 5: The comparison between well annotated image dataset and our training data. Image # , entity\n# and class # refer to the number of images, the number of entities and the number of class categories,\nrespectively. SA-1B \u2020 does not provide the class label of instances.\nDataset\nLVIS v1\nCOCO\nADE20K\nOpen Images\nSA-1B \u2020\nSDD (ours)\nImage #\n0.120M\n0.123M\n0.028M\n1M\n11M\n76M\nEntity #\n1.5M\n0.9M\n0.7M\n2.7M\n1.1B\n222M\nClass #\n1200\n91\n2693\n600\nN/A\n162K\nB\nPersonalization baselines comparison\nWe carefully survey the personalized image generation papers published in recent years and compile\na comprehensive comparison table comparing their support for single reference image, multi-subject\ngeneration, no test-time fine-tuning, and open domain generalization. As delineated in Table 6, the\nmain stream of personalized image generation still considers test-time fine-tuning, which suffers from\ninference time-consuming ranging from several seconds to more than one hour [16, 49, 31, 17, 21,\n55, 59, 37, 38, 58, 9, 3, 1, 18, 22, 50, 2]. Only a small portion of papers are dedicated to studying\npersonalized image generation without test-time fine-tuning [28, 54, 63, 11, 12, 40, 61, 33]. But all\nof the pioneering works cannot satisfy the four aforementioned requirements, either by being trained\non specific domains [54, 28, 63], or by supporting only single-concept generation. To the best of our\nknowledge, our Subject-Diffusion is the first open-domain personalized image generation method\nthat supports multi-concept synthesis and requires only a single reference image for each subject.\nC\nTwo-Subject Evaluation Details\nWe utilize all the objects in DreamBench and randomly select 30 pairs of combinations, out of which\n9 pairs belong to live objects. The specific subject pairs are presented in Table 7. For the prompts\nused in generating images with two subjects, we follow the format outlined in DreamBench, with the\ntwo subjects connected using the word \u201cand\u201d.\nD\nMore visualization results\nIn this section, we provide more single-, multi-, and human subject generation visualization examples,\nas in Fig. 8, Fig. 9 and Fig. 10. Notice that we display 10 generated results for each personal image\nwithout carefully cherry-picking, demonstrating the consistent fidelity and generalization ability of\nour proposed Subject-Diffusion.\n20\nTable 6: Survey of recent personalized image generation works in terms of single reference image,\nmulti-subject generation, no test-time fine-tuning and open domain generalization.\nMethod\nSingle image\nMulti-subject\nNo fine-tuning\nOpen domain\nTextual Inversion [16]\n%\n%\n%\n-\nDreambooth [49]\n%\n%\n%\n-\nCustom Diffusion [31]\n%\n!\n%\n-\nE4T [17]\n!\n%\n%\n-\nSVDiff [21]\n!\n!\n%\n-\nContinual Diffusion [55]\n%\n!\n%\n-\nXTI [59]\n%\n%\n%\n-\nCones [37]\n!\n!\n%\n-\nCones 2 [38]\n!\n!\n%\n-\nPerfusion [58]\n%\n!\n%\n-\nDisenBooth [9]\n!\n%\n%\n-\nBreak-A-Scene [3]\n!\n!\n%\n-\nNeTI [1]\n%\n%\n%\n-\nMix-of-Show [18]\n%\n!\n%\n-\nViCo [22]\n%\n%\n%\n-\nHyperDreamBooth [50]\n!\n%\n%\n-\nDomain-Agnostic [2]\n!\n%\n%\n-\nTaming [28]\n!\n%\n!\n%\nInstantBooth [54]\n!\n%\n!\n%\nFastComposer [63]\n!\n!\n!\n%\nSuTI [11]\n%\n%\n!\n!\nRe-Imagen [12]\n!\n%\n!\n!\nUMM-Diffusion [40]\n!\n%\n!\n!\nELITE [61]\n!\n%\n!\n!\nBlip-Diffusion [33]\n!\n%\n!\n!\nOurs(Subject-Diffusion)\n!\n!\n!\n!\nTable 7: Prompts for a dual-subject personalized image generation testset. The first 21 combinations\nare still objects, and the last 9 combinations are animals.\nbackpack-can\nbear_plushie-backpack_dog\nberry_bowl-vase\nduck_toy-can\nfancy_boot-shiny_sneaker\ngrey_sloth_plushie-poop_emoji\nteapot-backpack_dog\nteapot-berry_bowl\nwolf_plushie-backpack_dog\ncan-bear_plushie\ncan-candle\ncan-duck_toy\ncan-shiny_sneaker\nclock-teapot\ncolorful_sneaker-vase\nrobot_toy-backpack\nshiny_sneaker-duck_toy\nshiny_sneaker-poop_emoji\npink_sunglasses-candle\npoop_emoji-clock\npoop_emoji-shiny_sneaker\ncat-dog2\ncat-dog5\ncat2-dog3\ndog2-dog3\ndog5-dog6\ndog6-dog7\ndog6-dog8\ndog7-dog8\ndog8-dog6\n21\na backpack in the jungle\na backpack in the snow\na backpack on the beach\na backpack on a cobblestone \nstreet\na backpack on top of pink fabric\na backpack on top of a wooden \nfloor\na backpack with a city in the \nbackground\na backpack with a mountain in the \nbackground\na backpack with a blue house in \nthe background\na backpack on top of a purple rug \nin the forest\na cat in the jungle\na cat in the snow\na cat on the beach\na cat on a cobblestone street\na cat on top of pink fabric\na cat on top of a wooden floor\na cat with a city in the \nbackground\na cat with a mountain in the \nbackground\na cat with a blue house in the \nbackground\na cat on top of a purple rug in the \nforest\na dog in the jungle\na dog in the snow\na dog on the beach\na dog on a cobblestone street\na dog on top of pink fabric\na dog on top of a wooden floor\na dog with a city in the \nbackground\na dog with a mountain in the \nbackground\na dog with a blue house in the \nbackground\na dog on top of a purple rug in the \nforest\na toy in the jungle\na toy in the snow\na toy on the beach\na toy on a cobblestone street\na toy on top of pink fabric\na toy on top of a wooden floor\na toy with a city in the \nbackground\na toy with a mountain in the \nbackground\na toy with a blue house in the \nbackground\na toy on top of a purple rug in the \nforest\nInput Image\nInput Image\nInput Image\nInput Image\nFigure 8: More qualitative results for single-subject generation.\n22\na stuffed animal and a backpack \nin the jungle\na stuffed animal and a backpack \nin the snow\na stuffed animal and a backpack \non the beach\na stuffed animal and a backpack \non a cobblestone street\na stuffed animal and a backpack \non top of pink fabric\na stuffed animal and a backpack \non top of a wooden floor\na stuffed animal and a backpack \nwith a city in the background\na stuffed animal and a backpack \nwith a mountain in the \nbackground\na stuffed animal and a backpack \nwith a blue house in the \nbackground\na stuffed animal and a backpack \non top of a purple rug in the forest\na cat and a dog in the jungle\na cat and a dog in the snow\na cat and a dog on the beach\na cat and a dog on a cobblestone \nstreet\na cat and a dog on top of pink \nfabric\na cat and a dog on top of a \nwooden floor\na cat and a dog with a city in the \nbackground\na cat and a dog with a mountain \nin the background\na cat and a dog with a blue house \nin the background\na cat and a dog on top of a purple \nrug in the forest\na dog and a dog in the jungle\na dog and a dog in the snow\na dog and a dog on the beach\na dog and a dog on a cobblestone \nstreet\na dog and a dog on top of pink \nfabric\nInput Image\nInput Image\nInput Image\nInput Image\na stuffed animal and a toy on top \nof a wooden floor\na stuffed animal and a toy with a \ncity in the background\na stuffed animal and a toy with a \nmountain in the background\na stuffed animal and a toy with a \nblue house in the background\na stuffed animal and a toy on top \nof a purple rug in the forest\na stuffed animal and a toy on top \nof a wooden floor\na stuffed animal and a toy with a \ncity in the background\na stuffed animal and a toy with a \nmountain in the background\na stuffed animal and a toy with a \nblue house in the background\na stuffed animal and a toy on top \nof a purple rug in the forest\na stuffed animal and toy in the \njungle\na stuffed animal and a toy in the \nsnow\na stuffed animal and a toy on the \nbeach\na stuffed animal and a toy on a \ncobblestone street\na stuffed animal and a toy on top \nof pink fabric\na stuffed animal and toy in the \njungle\na stuffed animal and a toy in the \nsnow\na stuffed animal and a toy on the \nbeach\na stuffed animal and a toy on a \ncobblestone street\na stuffed animal and a toy on top \nof pink fabric\na dog and a dog on top of a \nwooden floor\na dog and a dog with a city in the \nbackground\na dog and a dog with a mountain \nin the background\na dog and a dog with a blue house \nin the background\na dog and a dog on top of a purple \nrug in the forest\na dog and a dog on top of a \nwooden floor\na dog and a dog with a city in the \nbackground\na dog and a dog with a mountain \nin the background\na dog and a dog with a blue house \nin the background\na dog and a dog on top of a purple \nrug in the forest\nFigure 9: More qualitative results for two-subject generation.\n23\na painting of a woman in the style \nof Vincent Van Gogh\na watercolor painting of a woman\na woman in the snow\na woman wearing a rainbow scarf\na woman wearing a red hat\na woman wearing a santa hat\na woman in a chef outfit\na woman in a firefighter outfit\na woman in a police outfit\na woman working out at the gym\na painting of a man in the style of \nVincent Van Gogh\na watercolor painting of a man\na man in the snow\na man wearing a rainbow scarf\na man wearing a red hat\na man wearing a santa hat\na man in a chef outfit\na man in a firefighter outfit\na man in a police outfit\na man working out at the gym\na painting of a man in the style of \nVincent Van Gogh\na watercolor painting of a man\na man in the snow\na man wearing a rainbow scarf\na man wearing a red hat\na man wearing a santa hat\na man in a chef outfit\na man in a firefighter outfit\na man in a police outfit\na man working out at the gym\na painting of a woman in the style \nof Vincent Van Gogh\na watercolor painting of a woman\na woman in the snow\na woman wearing a rainbow scarf\na woman wearing a red hat\na woman wearing a santa hat\na woman in a chef outfit\na woman in a firefighter outfit\na woman in a police outfit\na woman working out at the gym\nInput Image\nInput Image\nInput Image\nInput Image\nFigure 10: More qualitative results for human image generation.\n24\n"
  },
  {
    "title": "CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2307.11526.pdf",
    "upvote": "11",
    "text": "CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields\nZiyuan Luo1,2\nQing Guo3\nKa Chun Cheung2,4\nSimon See2\nRenjie Wan1*\n1Department of Computer Science, Hong Kong Baptist University\n2NVIDIA AI Technology Center, NVIDIA\n3IHPC and CFAR, Agency for Science, Technology and Research, Singapore\n4Department of Mathematics, Hong Kong Baptist University\nziyuanluo@life.hkbu.edu.hk, guo_qing@cfar.a-star.edu.sg, {chcheung, ssee}@nvidia.com,\nrenjiewan@hkbu.edu.hk\nAbstract\nNeural Radiance Fields (NeRF) have the potential to be\na major representation of media. Since training a NeRF\nhas never been an easy task, the protection of its model\ncopyright should be a priority. In this paper, by analyz-\ning the pros and cons of possible copyright protection solu-\ntions, we propose to protect the copyright of NeRF models\nby replacing the original color representation in NeRF with\na watermarked color representation. Then, a distortion-\nresistant rendering scheme is designed to guarantee robust\nmessage extraction in 2D renderings of NeRF. Our pro-\nposed method can directly protect the copyright of NeRF\nmodels while maintaining high rendering quality and bit ac-\ncuracy when compared among optional solutions. Project\npage: https://luo-ziyuan.github.io/copyrnerf.\n1. Introduction\nThough Neural Radiance Fields (NeRF) [23] have the\npotential to be the mainstream for the representation of dig-\nital media, training a NeRF model has never been an easy\ntask. If a NeRF model is stolen by malicious users, how can\nwe identify its intellectual property?\nAs with any digital asset (e.g., 3D model, video, or im-\nage), copyright can be secured by embedding copyright\nmessages into asset, aka digital watermarking, and NeRF\nmodels are no exception. An intuitive solution is to directly\nwatermark rendered samples using an off-the-shelf water-\nmarking approach (e.g., HiDDeN [50] and MBRS [14]).\nHowever, this only protects the copyright of rendered sam-\nples, leaving the core model unprotected. If the core model\nhas been stolen, malicious users may render new samples\n*Corresponding author.\nThis work was done at Renjie\u2019s Research\nGroup at the Department of Computer Science of Hong Kong Baptist Uni-\nversity.\nj\n00100110\u2026\nSecret message\n00100110\u2026\n00100110\u2026\n00100110\u2026\nExtracted\nmessage\nk\nGround truth\nOur CopyRNeRF\nNeRF with message\n32.69/100%\n22.83/69%\nNovel\nviews\nPSNR/Bit Acc.\nReconstruction quality under different settings\nHiDDeN + NeRF\n27.71/51%\nFigure 1: When NeRF models are stolen ( 1 ) by macli-\ncious users, CopyRNeRF can help to claim model owner-\nship by transmitting copyright messages embedded in mod-\nels to rendering samples ( 2 ). We show some comparisons\nwith HiDDeN [50] + NeRF [23], and NeRF [23] with mes-\nsages. PSNR/Bit Accuracy is shown below each example.\nusing different rendering strategies, leaving no room for\nexternal watermarking expected by model creators.\nBe-\nsides, without considering factors necessary for rendering\nduring watermarking, directly watermarking rendered sam-\nples may leave easily detectable trace on areas with low ge-\nometry values.\nThe copyright messages are usually embedded into 3D\nstructure (e.g., meshes) for explicit 3D models [43]. Since\nsuch structures are all implicitly encoded into the weights\nof multilayer perceptron (MLP) for NeRF, its copyright\nprotection should be conducted by watermarking model\nweights. As the information encoded by NeRF can only be\naccessed via 2D renderings of protected models, two com-\nmon standards should be considered during the watermark\nextraction on rendered samples [1, 15, 41, 45]: 1) invisi-\nbility, which requires that no serious visual distortion are\ncaused by embedded messages, and 2) robustness, which\narXiv:2307.11526v2  [cs.CV]  29 Jul 2023\nensures robust message extraction even when various dis-\ntortions are encountered.\nOne option is to create a NeRF model using watermarked\nimages, while the popular invisible watermarks on 2D im-\nages cannot be effectively transmitted into NeRF mod-\nels. As outlined in Figure 1 (HiDDeN [50] + NeRF [23]),\nthough the rendered results are of high quality, the secret\nmessages cannot be robustly extracted. We can also directly\nconcatenate secret messages with input coordinates, which\nproduces higher bit accuracy (NeRF with message in Fig-\nure 1). However, the lower PSNR values of rendered sam-\nples indicate that there is an obvious visual distortion, which\nviolates the standard for invisibility.\nThough invisibility is important for a watermarking sys-\ntem, the higher demand for robustness makes watermarking\nunique [50]. Thus, in addition to invisibility, we focus on\na more robust protection of NeRF models. As opposed to\nembedding messages into the entire models as in the above\nsettings, we create a watermarked color representation for\nrendering based on a subset of models, as displayed in Fig-\nure 2. By keeping the base representation unchanged, this\napproach can produce rendering samples with invisible wa-\ntermarks. By incorporating spatial information into the wa-\ntermarked color representation, the embedded messages can\nremain consistent across different viewpoints rendered from\nNeRF models. We further strengthen the robustness of wa-\ntermark extraction by using distortion-resistant rendering\nduring model optimization. A distortion layer is designed to\nensure robust watermark extraction even when the rendered\nsamples are severely distorted (e.g., blurring, noise, and ro-\ntation). A random sampling strategy is further considered\nto make the protected model robust to different sampling\nstrategy during rendering.\nDistortion-resistant rendering is only needed during the\noptimization of core models. If the core model is stolen,\neven with different rendering schemes and sampling strate-\ngies, the copyright message can still be robustly extracted.\nOur contribution can be summarized as follows:\n\u2022 a method to produce copyright-embedded NeRF mod-\nels.\n\u2022 a watermarked color representation to ensure invisibil-\nity and high rendering quality.\n\u2022 distortion-resistant rendering to ensure robustness\nacross different rendering strategies or 2D distortions.\n2. Related work\nNeural radiance fields.\nVarious neural implicit scene\nrepresentation schemes have been introduced recently [25,\n42, 48]. The Scene Representation Networks (SNR) [32]\nrepresent scenes as a multilayer perceptron (MLP) that\nmaps world coordinates to local features, which can be\ntrained from 2D images and their camera poses. DeepSDF\n[27] and DIST [20] use trained networks to represent a con-\ntinuous signed distance function of a class of shapes. PIFu\n[30] learned two pixel-aligned implicit functions to infer\nsurface and texture of clothed humans respectively from\na single input image. Occupancy Networks [21, 28] are\nproposed as an implicit representation of 3D geometry of\n3D objects or scenes with 3D supervision. NeRF [23, 49]\nin particular directly maps the 3D position and 2D view-\ning direction to color and geometry by a MLP and synthe-\nsize novel views via volume rendering. The improvements\nand applications of this implicit representation have been\nrapidly growing in recent years, including NeRF acceler-\nating [9, 24], sparse reconstruction [44, 6], and generative\nmodels [31, 5]. NeRF models are not easy to train and may\nuse private data, so protecting their copyright becomes cru-\ncial.\nDigital watermarking for 2D.\nEarly 2D watermarking\napproaches encode information in the least significant bits\nof image pixels [35]. Some other methods instead encode\ninformation in the transform domains [17]. Deep-learning\nbased methods for image watermarking have made substan-\ntial progress. HiDDeN [50] was one of the first deep image\nwatermarking methods that achieved superior performance\ncompared to traditional watermarking approaches.\nRed-\nMark [1] introduced residual connections with a strength\nfactor for embedding binary images in the transform do-\nmain. Deep watermarking has since been generalized to\nvideo [37, 46] as well. Modeling more complex and real-\nistic image distortions also broadened the scope in terms\nof application [38, 34]. However, those methods all cannot\nprotect the copyright of 3D models.\nDigital watermarking for 3D.\nTraditional 3D water-\nmarking approaches [26, 29, 39] leveraged Fourier or\nwavelet analysis on triangular or polygonal meshes. Re-\ncently, Hou et al. [11] introduced a 3D watermarking\nmethod using the layering artifacts in 3D printed objects.\nSon et al. [33] used mesh saliency as a perceptual metric\nto minimize vertex distortions. Hamidi et al. [10] further\nextended mesh saliency with wavelet transform to make 3D\nwatermarking robust. Jing et al. [19] studied watermark-\ning for point clouds through analyzing vertex curvatures.\nRecently, a deep-learning based approach [43] successfully\nembeds messages in 3D meshes and extracts them from 2D\nrenderings. However, existing methods are for explicit 3D\nmodels, which cannot be used for NeRF models with im-\nplicit property.\nRay\ncasting\n\u06fb\nMessage\n\u03a3\nNoise\nRotation\nScaling\n\u06fb\u0de1\nExtracted\nmessage\n(a) Building watermarked color representation\nSec 4.1.\nColor\nfeature field\n(b) Distortion-resistant rendering\nSec 4.2.\n(c) Message\nextractor\nSec 4.3.\nMessage Loss\n\u06fb\nMessage\n\u08cc\n\u089e\nContent Loss\n\u08cc\n\u089e : Coordinate\n\u088a: Viewing direction\n\u08cc : Geometry\n\u0889\u0893: Watermarked color\nWatermarked\ncolor\nColor feature\nencoder\nMessage feature\nencoder\nFeature fusion\nmodule\nDistortion layer\nCore model\nRendering\nMessage extraction\nMessage\nfeature field\nFeature\nfusion\n11010\u2026\n11010\u2026\n\u088a\nRandom\nsampling\n\u0889\u0893\nFigure 2: Illustration of our proposed method. (a) A watermarked color representation is obtained with the given secret\nmessage, which is able to produce watermarked color for rendering. (b) During training, a distortion-resistant rendering\nis deployed to map the geometry (\u03c3) and watermarked color representations to image patches with several distortions. (c)\nFinally, the secret message can be revealed by a CNN-based message extractor.\n3. Preliminaries\nNeRF [23] uses MLPs \u0398\u03c3 and \u0398c to map the 3D location\nx \u2208 R3 and viewing direction d \u2208 R2 to a color value\nc \u2208 R3 and a geometric value \u03c3 \u2208 R+:\n[\u03c3, z] = \u0398\u03c3 (\u03b3x(x)) ,\n(1)\nc = \u0398c (z, \u03b3d(d)) ,\n(2)\nwhere \u03b3x and \u03b3d are fixed encoding functions for location\nand viewing direction respectively. The intermediate vari-\nable z is a feature output by the first MLP \u0398\u03c3.\nFor rendering a 2D image from the radiance fields \u0398\u03c3\nand \u0398c, a numerical quadrature is used to approximate the\nvolumetric projection integral.\nFormally, Np points are\nsampled along a camera ray r with color and geometry val-\nues {(ci\nr, \u03c3i\nr)}N\ni=1. The RGB color value \u02c6C(r) is obtained\nusing alpha composition\n\u02c6C(r) =\nNp\nX\ni=1\nT i\nr(1 \u2212 exp\n\u0000\u2212\u03c3i\nr\u03b4i\nr\n\u0001\n)ci\nr,\n(3)\nwhere T i\nr = Qi\u22121\nj=1\n\u0000exp\n\u0000\u2212\u03c3i\nr\u03b4i\nr\n\u0001\u0001\n, and \u03b4i\nr is the distance\nbetween adjacent sample points. The MLPs \u0398\u03c3 and \u0398c are\noptimized by minimizing a reconstruction loss between ob-\nservations C and predictions \u02c6C as\nLrecon = 1\nNr\nNr\nX\nm=1\n\u2225 \u02c6C(rm) \u2212 C(rm)\u22252\n2,\n(4)\nwhere Nr is the number of sampling pixels. Given \u0398\u03c3 and\n\u0398c, novel views can be synthesized by invoking volume\nrendering for each ray.\nConsidering the superior capability of NeRF in rendering\nnovel views and representing various scenes, how can we\nprotect its copyright when it is stolen by malicious users?\n4. Proposed method\nAs outlined in Figure 2, with a collection of 2D images\n{In}N\nn=1 and the binary message M \u2208 {0, 1}Nb with length\nNb, we address the issue raised in Section 3 by building a\nwatermarked color representation during optimization. In\ntraining, a distortion-resistant rendering is further applied\nto improve the robustness when 2D distortions or different\nrendering schemes are encountered. With the above design,\nthe secret messages can be robustly extracted during testing\neven encountering sever distortions or different rendering\nstrategies.\n4.1. Building watermarked color representation\nThe rendering in Equation (3) relies on color and ge-\nometry produced by their corresponding representation in\nNeRF. To ensure the transmission of copyright messages to\nthe rendered results, we propose embedding messages into\ntheir representation. We create a watermarked color rep-\nresentation on the basis of \u0398c defined in Equation (2) to\nguarantee the message invisibility and consistency across\nviewpoints. The representation of geometry is also the po-\ntential for watermarking, but external information on geom-\netry may undermine rendering quality [36, 12, 7]. There-\nfore, the geometry does not become our first option, while\nexperiments are also conducted to verify this setting.\nWe keep the geometry representation in Equation (1) un-\nchanged, and construct the watermarked color representa-\ntion \u0398m to produce the message embedded color cm as fol-\nlows:\ncm = \u0398m (c, \u03b3x(x), \u03b3d(d), M) ,\n(5)\nwhere M denotes the message to be embedded and \u0398m con-\ntains several MLPs to ensure reliable message embedding.\nThe input c is obtained by querying \u0398c using Equation (2).\nSeveral previous methods have pointed out the importance\nof building a 3D feature field when distributed features are\nneeded to characterize composite information [40, 4]. Thus,\ninstead of directly fusing those information, we first con-\nstruct their corresponding feature field and then combine\nthem progressively.\nColor feature field. In this stage, we aim at fusing the spa-\ntial information and color representation to ensure message\nconsistency and robustness across viewpoints. We adopt a\ncolor feature field by considering color, spatial positions,\nand viewing directions simultaneously as follows:\nfc = E\u03be(c, \u03b3x(x), \u03b3d(d)).\n(6)\nGiven a 3D coordinate x and a viewing direction d, we first\nquery the color representation \u0398c (z, \u03b3d(d)) to get c, and\nthen concatenate them with x and d to obtain spatial de-\nscriptor v as the input. Then the color feature encoder E\u03be\ntransforms v to the high-dimensional color feature field fc\nwith dimension Nc. The Fourier feature encoding is applied\nto x and d before the feature extraction.\nMessage feature field. We further construct the message\nfeature field. Specifically, we follow the classical setting in\ndigital watermarking by transforming secret messages into\nhigher dimensions [2, 3]. It ensures more succinctly en-\ncoding of desired messages [2]. As shown in Figure 2, a\nmessage feature encoder is applied to map the messages to\nits corresponding higher dimensions as follows:\nfM = D\u03d5(M).\n(7)\nIn Equation (7), given message M of length Nb, the mes-\nsage feature encoder D\u03d5 applies a MLP to the input mes-\nsage, resulting in a message feature field fM of dimension\nNm.\nThen, the watermarked color can be generated via a fea-\nture fusion module G\u03c8 that integrates both color feature\nfield and message feature field as follows:\ncm = G\u03c8(fc, fM, c).\n(8)\nSpecifically, c is also employed here to make the final re-\nsults more stable.\ncm is with the same dimension to c,\nwhich ensures this representation can easily adapt to cur-\nrent rendering schemes.\n4.2. Distortion-resistant rendering\nDirectly employing the watermarked representation for\nvolume rendering has already been able to guarantee in-\nvisibility and robustness across viewpoints. However, as\ndiscussed in Section 1, the message should be robustly ex-\ntracted even when encountering diverse distortion to the\nrendered 2D images. Besides, for an implicit model relying\non rendering to display its contents, the robustness should\nalso be secured even when different rendering strategies\nare employed. Such requirement for robustness cannot be\nachieved by simply using watermarked representation un-\nder the classical NeRF training framework. For example,\nthe pixel-wise rendering strategy cannot effectively model\nthe distortion (e.g., blurring and cropping) only meaning-\nful in a wider scale. We, therefore, propose a distortion-\nresistant rendering by strengthening the robustness using a\nrandom sampling strategy and distortion layer.\nSince most 2D distortions can only be obviously ob-\nserved in a certain area, we consider the rendering process\nin a patch level [16, 8]. A window with the random posi-\ntion is cropped from the input image with a certain height\nand width, then we uniformly sample the pixels from such\nwindow to form a smaller patch. The center of the patch is\ndenoted by u = (u, v) \u2208 R2, and the size of patch is de-\ntermined by K \u2208 R+. We randomly draw the patch center\nu from a uniform distribution u \u223c U(\u2126) over the image\ndomain \u2126. The patch P(u, K) can be denoted by by a set\nof 2D image coordinates as\nP(u, K) = {(x + u, y + v) | x, y \u2208 {\u2212K\n2 , . . . , K\n2 \u2212 1}}.\n(9)\nSuch a patch-based scheme constitutes the backbone of our\ndistortion-resistant rendering, due to its advantages in cap-\nturing information on a wider scale. Specifically, we em-\nploy a variable patch size to accommodate diverse distor-\ntions during rendering, which can ensure higher robust-\nness in message extraction. This is because small patches\nincrease the robustness against cropping attacks and large\npatches allow higher redundancy in the bit encoding, which\nleads to increased resilience against random noise [8].\nAs the corresponding 3D rays are uniquely determined\nby P(u, K), the camera pose and intrinsics, the image patch\neP can be obtained after points sampling and rendering.\nBased on the sampling points in Section 3, we use a random\nsampling scheme to further improve the model\u2019s robustness,\nwhich is described as follows.\nRandom sampling. During volume rendering, NeRF [23]\nis required to sample 3D points along a ray to calculate\nthe RGB value of a pixel color. However, the sampling\nstrategy may vary as the renderer changes [24, 18].\nTo\nmake our message extraction more robust even under dif-\nferent sampling strategies, we employ a random sampling\nstrategy by adding a shifting value to the sampling points.\nSpecifically, the original Np sampling points along ray r is\ndenoted by a sequence, which can be concluded as X =\n(x1\nr, x2\nr, \u00b7 \u00b7 \u00b7 , xNp\nr ), where xi\nr, i = 1, 2, \u00b7 \u00b7 \u00b7 , Np denotes the\nsampling points during rendering. The randomized sam-\nple sequence Xrandom can be denoted by adding a shifting\nvalue as\nXrandom = (x1\nr + z1, x2\nr + z2, \u00b7 \u00b7 \u00b7 , xNp\nr\n+ zNp),\nzi \u223c N(0, \u03b22), i = 1, 2, \u00b7 \u00b7 \u00b7 , Np,\n(10)\nwhere N(0, \u03b22) is the Gaussian distribution with zero mean\nand standard deviation \u03b2.\nBy querying the watermarked color representation and\ngeometry values at Np points in Xrandom, the rendering\noperator can be then applied to generate the watermarked\ncolor eCm in rendered images:\neCm(r) =\nNp\nX\ni=1\nT i\nr(1 \u2212 exp\n\u0000\u2212\u03c3i\nr\u03b4i\nr\n\u0001\n)ci\nm,\n(11)\nwhere T i\nr and \u03b4i\nr are with the same definitions to their coun-\nterparts in Equation (3).\nAll the colors obtained by coordinates P can form a\nK \u00d7K image patch eP. The content loss Lcontent of the 3D\nrepresentation is calculated between watermarked patch eP\nand the \u02c6P, where \u02c6P is rendered from the non-watermarked\nrepresentation by the same coordinates P. In detail, the con-\ntent loss Lcontent has two components namely pixel-wise\nMSE loss and perceptual loss:\nLcontent = \u2225eP \u2212 \u02c6P\u22252\n2+\u03bb\u2225\u03a8(eP) \u2212 \u03a8(\u02c6P)\u22252\n2,\n(12)\nwhere \u03a8(\u00b7) denotes the feature representation obtained from\na VGG-16 network, and \u03bb is a hyperparameter to balance\nthe loss functions.\nDistortion layer. To make our watermarking system robust\nto 2D distortions, a distortion layer is employed in our wa-\ntermarking training pipeline after the patch eP is rendered.\nSeveral commonly used distortions are considered: 1) ad-\nditive Gaussian noise with mean \u00b5 and standard deviation\n\u03bd; 2) random axis-angle rotation with parameters \u03b1; and 3)\nrandom scaling with a parameter s; 4) Gaussian blur with\nkernel k. Since all these distortions are differentiable, we\ncould train our network end-to-end.\nThe distortion-resistant rendering is only applied during\ntraining. It is not a part of the core model. If the core model\nis stolen, even malicious users use different rendering strat-\negy, the expected robustness can still be secured.\n4.3. Message extractor\nTo retrieve message \u02c6M from the K \u00d7 K rendered patch\nP, a message extractor H\u03c7 is proposed to be trained end-to-\nend:\nH\u03c7 : RK\u00d7K \u2192 RNb, P 7\u2192 \u02c6M,\n(13)\nwhere \u03c7 is a trainable parameter. Specifically, we employ\na sequence of 2D convolutional layers with the batch nor-\nmalization and ReLU functions [13]. An average pooling\nis then performed, following by a final linear layer with a\nfixed output dimension Nb, which is the length of the mes-\nsage, to produce the continuous predicted message \u02c6M. Be-\ncause of the use of average pooling, the message extractor\nis compatible with any patch sizes, which means the net-\nwork structure can remain unchanged when applying size-\nchanging distortions such as random scaling.\nThe message loss Lm is then obtained by calculating the\nbinary cross-entropy error between predicted message \u02c6M\nand the ground truth message M:\nLm = mean[\u2212(M log \u02c6M + (1 \u2212 M) log(1 \u2212 \u02c6M))], (14)\nwhere mean[\u00b7] indicates the mean value over all bits.\nTo evaluate the bit accuracy during testing, the binary\npredicted message \u02c6Mb can be obtained by rounding:\n\u02c6Mb = clamp(sign( \u02c6M), 0, 1),\n(15)\nwhere clamp and sign are of the same definitions in [43]. It\nshould be noted that we use the continuous result \u02c6M in the\ntraining process, while the binary one \u02c6Mb is only adopted\nin testing process.\nTherefore, the overall loss to train the copyright-\nprotected neural radiance fields can be obtained as\nL = \u03b31Lcontent + \u03b32Lm,\n(16)\nwhere \u03b31 and \u03b32 are hyperparameters to balance the loss\nfunctions.\n4.4. Implementation details\nWe implement our method using PyTorch. An eight-\nlayer MLP with 256 channels and the following two MLP\nbranches are used to predict the original colors c and opaci-\nties \u03c3, respectively. We train a \u201ccoarse\u201d network along with\na \u201cfine\u201d network for importance sampling. we sample 32\npoints along each ray in the coarse model and 64 points in\nthe fine model. Next, the patch size is set to 150 \u00d7 150.\nThe hyperparameters in Equation (12) and Equation (16)\nare set as \u03bb1 = 0.01, \u03b31 = 1, and \u03b32 = 5.00. We use the\nAdam optimizer with defaults values \u03b21 = 0.9, \u03b22 = 0.999,\n\u03f5 = 10\u22128, and a learning rate 5 \u00d7 10\u22124 that decays follow-\ning the exponential scheduler during optimization. In our\nexperiments, we set Nm in Equation (7) as 256. We first\noptimize MLPs \u0398\u03c3 and \u0398c using loss function Equation (4)\nfor 200K and 100K iterations for Blender dataset [23] and\nLLFF dataset [22] separately, and then train the models E\u03be,\nD\u03d5, and H\u03c7 on 8 NVIDIA Tesla V100 GPUs. During train-\ning, we have considered messages with different bit lengths\nand forms. If a message has 4 bits, we take into account all\n24 situations during training. The model creator can choose\none message considered in our training as the desired mes-\nsage.\nGroundtruth\nProposed Method\nHiDDeN+NeRF\nNeRF with message\nAccuracy 100% / PSNR 30.28\nCopyRNeRF in geometry\nMBRS+NeRF\nAccuracy 51.04% / PSNR 28.61\nAccuracy 51.38% / PSNR 29.09\nAccuracy 69.28% / PSNR 22.83\nAccuracy 61.50% / PSNR 18.82\nAccuracy 68.00% / PSNR 17.61\nAccuracy 63.19% / PSNR 20.26\nAccuracy 100% / PSNR 32.69\nAccuracy 50.94% / PSNR 27.71\nAccuracy 50.25% / PSNR 27.75\nFigure 3: Visual quality comparisons of each baseline. We show the differences (\u00d710) between the synthesized results and\nthe ground truth next to each method. Our proposed CopyRNeRF can achieve a well balance between the reconstruction\nquality and bit accuracy.\nTable 1: Bit accuracies with different lengths compared with baselines. The results are averaged on all all examples.\n4 bits\n8 bits\n16 bits\n32 bits\n48 bits\nProposed CopyRNeRF\n100%\n100%\n91.16%\n78.08%\n60.06%\nHiDDeN [45]+NeRF[23]\n50.31%\n50.25%\n50.19%\n50.11%\n50.04%\nMBRS [14]+NeRF [23]\n53.25%\n51.38%\n50.53%\n49.80%\n50.14%\nNeRF[23] with message\n72.50%\n63.19%\n52.22%\n50.00%\n51.04%\nCopyRNeRF in geometry\n76.75%\n68.00%\n60.16%\n54.86%\n53.36%\n5. Experiments\n5.1. Experimental settings\nDataset. To evaluate our methods, we train and test our\nmodel on Blender dataset [23] and LLFF dataset [22],\nwhich are common datasets used for NeRF. Blender dataset\ncontains 8 detailed synthetic objects with 100 images taken\nfrom virtual cameras arranged on a hemisphere pointed in-\nward. As in NeRF [23], for each scene we input 100 views\nfor training. LLFF dataset consists of 8 real-world scenes\nthat contain mainly forward-facing images. Each scene con-\ntains 20 to 62 images. The data split for this dataset also fol-\nlows NeRF [23]. For each scene, we select 20 images from\ntheir testing dataset to evaluate the visual quality. For the\nevaluation of bit accuracy, we render 200 views for each\nscene to test whether the message can be effectively ex-\ntracted under different viewpoints. We report average val-\nues across all testing viewpoints in our experiments.\nBaselines.\nTo the best of our knowledge, there is no\nmethod specifically for protecting the copyright of NeRF\nmodels.\nWe, therefore, compare with four strategies to\nguarantee a fair comparison: 1) HiDDeN [50]+NeRF[23]:\nprocessing images with classical 2D watermarking method\nHiDDeN [50] before training the NeRF model;\n2)\nMBRS [14]+NeRF [23]: processing images with state-of-\nthe-art 2D watermarking method MBRS [14] before train-\ning the NeRF model; 3) NeRF with message: concatenat-\ning the message M with location x and viewing direction d\nas the input of NeRF; 4) CopyRNeRF in geometry: chang-\ning our CopyRNeRF by fusing messages with the geometry\nto evaluate whether geometry is a good option for message\nembedding.\nEvaluation methodology.\nWe evaluate the performance\nof our proposed method against other methods by follow-\ning the standard of digital watermarking about the invisibil-\nity, robustness, and capacity. For invisibility, we evaluate\nthe performance by using PSNR, SSIM, and LPIPS [47]\nto compare the visual quality of the rendered results af-\nter message embedding. For robustness, we will investi-\ngate whether the encoded messages can be extracted ef-\nfectively by measuring the bit accuracy on different distor-\ntions. Besides normal situations, we consider the follow-\ning distortions for message extraction: 1) Gaussian noise,\n2) Rotation, 3) Scaling, and 4) Gaussian blur. For capac-\nity, following the setting in previous work for the water-\nmarking of explicit 3D models [43], we investigate the in-\nvisibility and robustness under different message length as\nNb \u2208 {4, 8, 16, 32, 48}, which has been proven effective in\nprotecting 3D models [43]. Since we have included differ-\nent viewpoints in our experiments for each scene, our eval-\nuation can faithfully reflect whether the evaluated method\ncan guarantee its robustness and consistency across view-\npoints.\n5.2. Experimental results\nQualitative results.\nWe first compare the reconstruc-\ntion quality visually against all baselines and the results\nare shown in Figure 3. Actually, all methods except NeRF\nwith message and CopyRNeRF in geometry can achieve\nhigh reconstruction quality. For HiDDeN [50] + NeRF [23]\nTable 2: Bit accuracies and reconstruction qualities compared with our baselines. \u2191 (\u2193)\nmeans higher (lower) is better. We show the results on Nb = 16 bits. The results are\naveraged on all all examples. The best performances are highlighted in bold.\nBit Acc\u2191\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nProposed CopyRNeRF\n91.16%\n26.29\n0.910\n0.038\nHiDDeN [50]+NeRF[23]\n50.19%\n26.53\n0.917\n0.035\nMBRS [14]+NeRF [23]\n50.53%\n28.79\n0.925\n0.022\nNeRF with message\n52.22%\n22.33\n0.773\n0.108\nCopyRNeRF in geometry\n60.16%\n20.24\n0.771\n0.095\nWatermarked by MBRS Residual (X10)\nResult of MBRS+NeRF\nResidual (X10)\nFigure 4: Analysis for failure of\nMBRS [14]+NeRF.\nTable 3: Bit accuracies with different distortion types compared with each baseline and our CopyRNeRF without distortion-\nresistant rendering (DRR). We show the results on Nb = 16 bits. The results are averaged on all all examples.\nNo Distortion\nGaussian noise\nRotation\nScaling\nGaussian blur\n(\u03bd=0.1)\n(\u00b1\u03c0/6)\n(\u2264 25%)\n(deviation = 0.1)\nProposed CopyRNeRF\n91.16%\n90.44%\n88.13%\n89.33%\n90.06%\nHiDDeN [50]+NeRF[23]\n50.19%\n49.84%\n50.12%\n50.09%\n50.16%\nMBRS [14]+NeRF [23]\n50.53%\n51.00%\n51.03%\n50.12%\n50.41%\nNeRF with message\n52.22%\n50.53%\n50.22%\n50.19%\n51.34%\nCopyRNeRF in geometry\n60.16%\n58.00%\n56.94%\n60.09%\n59.38%\nCopyRNeRF W/o DRR\n91.25%\n89.12%\n75.81%\n87.44%\n87.06%\nand MBRS [14]+NeRF [23], although they are efficient ap-\nproaches in 2D watermarking, their bit accuracy values are\nall low for rendered images, which proves that the mes-\nsage are not effectively embedded after NeRF model train-\ning. From the results shown in Figure 4, the view synthesis\nof NeRF changes the information embedded by 2D water-\nmarking methods, leading to their failures. For NeRF with\nmessage, as assumed in our previous discussions, directly\nemploying secret messages as an input change the appear-\nance of the output, which leads to their lower PSNR values.\nBesides, its lower bit accuracy also proves that this is not an\neffective embedding scheme. For CopyRNeRF in geome-\ntry, it achieves the worst visual quality among all methods.\nThe rendered results look blurred, which confirms our as-\nsumption that the geometry is not a good option for message\nembedding.\nBit Accuracy vs.\nMessage Length.\nWe launch 5 ex-\nperiments for each message length and show the relation-\nship between bit accuracy and the length of message in Ta-\nble 1.\nWe could clearly see that the bit accuracy drops\nwhen the number of bits increases. However, our CopyRN-\neRF achieves the best bit accuracy across all settings, which\nproves that the messages can be effectively embedded and\nrobustly extracted. CopyRNeRF in geometry achieves the\nsecond best results among all setting, which shows that em-\nbedding message in geometry should also be a potential op-\ntion for watermarking. However, the higher performance of\nour proposed CopyRNeRF shows that color representation\nis a better choice.\nBit Accuracy vs.\nReconstruction Quality.\nWe con-\nduct more experiments to evaluate the relationship between\nbit accuracy and reconstruction quality.\nThe results are\nshown in Table 21. Our proposed CopyRNeRF achieves a\ngood balance between bit accuracy and error metric values.\nThough the visual quality values are not the highest, the\nbit accuracy is the best among all settings. Though HiD-\nDeN [50] + NeRF [23] and MBRS [14]+NeRF [23] can pro-\nduce better visual quality values, its lower bit accuracy indi-\ncates that the secret messages are not effectively embedded\nand robustly extracted. NeRF with message also achieves\ndegraded performance on bit accuracy, and its visual qual-\nity values are also low. It indicates that the embedded mes-\nsages undermine the quality of reconstruction. Specifically,\nthe lower visual quality values of CopyRNeRF in geometry\nindicates that hiding messages in color may lead to better\nreconstruction quality than hiding messages in geometry.\nModel robustness on 2D distortions. We evaluate the ro-\nbustness of our method by applying several traditional 2D\ndistortions. Specifically, as shown in Table 3, we consider\nseveral types of 2D distortions including noise, rotation,\nscaling, and cropping. We could see that our method is quite\nrobust to different 2D distortions. Specifically, CopyRN-\neRF w/o DRR achieves similar performance to the complete\nCopyRNeRF when no distortion is encountered. However,\n1Results for other lengths of raw bits can be found in the supplementary\nmaterials.\n32 ASP + 32 ISP\n64 ASP\n32 ASP\n64 RSP\n16 ASP\nGroundtruth\nBit Acc.=90.06%\nBit Acc.=89.22%\nBit Acc.=88.06%\nBit Acc.=85.81%\nBit Acc.=88.41%\nFigure 5: Comparisons for different rendering degradadtion in the inference phase. The message length is set to 16. We use\naverage sampling points (ASP), importance sampling points (ISP), and random sampling points (RSP) in different rendering\nstrategies. \u201c32 ASP + 32 ISP\u201d is a strategy employed in the training process, and message extraction also shows the highest\nbit accuracy. When sampling strategies are changed to other ones during inference, the message extraction still shows similar\nperformance, which verifies the effectiveness of our distortion-resistant rendering.\nTable 4: Comparisons for our full model, our model without\nMessage Feature Field (MFF) and our model without Color\nFeature Field (CFF). The last row shows that our method\nachieves consistent performance even when different ren-\ndering scheme (DRS) is applied during testing.\nBit Acc\u2191\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nOurs\n100%\n32.68\n0.948\n0.048\nW/o MFF\n82.69%\n20.46\n0.552\n0.285\nW/o CFF\n80.69%\n21.06\n0.612\n0.187\nDRS\n100%\n32.17\n0.947\n0.052\nwhen it comes to different distortions, its lower bit accura-\ncies demonstrate the effectiveness of our distortion-resistant\nrendering during training.\nAnalysis for feature field. In the section, we further evalu-\nate the effectiveness of color feature field and message fea-\nture field. We first remove the module for building color\nfeature field and directly combine the color representation\nwith the message features. In this case, the model performs\npoorly in preserving the visual quality of the rendered re-\nsults.\nWe further remove the module for building mes-\nsage feature field and combine the message directly with\nthe color feature field. The results in Table 4 indicate that\nthis may result in lower bit accuracy, which proves that mes-\nsages are not embedded effectively.\nModel robustness on rendering. Though we apply a nor-\nmal volume rendering strategy for inference, the messages\ncan also be effectively extracted using a distortion rendering\nutilized in training phase. As shown in the last row of Ta-\nble 4, the quantitative values with the distortion rendering\nare still similar to original results in the first row of Ta-\nble 4, which further confirms the robustness of our proposed\nmethod.\nThe results for different sampling schemes are presented\nin Figure 5. Our distortion-resistant rendering employs 32\naverage sampling points and 32 importance sampling points\nduring training. When different sampling strategies are ap-\nplied in the inference phase, our method can also achieve\nOurs\nNeRF+HiDDeN\nNeRF+MBRS\nNo message\nFigure 6: Comparisons for watermarking after rendering.\nThe patch in the lower left corner shows the augmentation\nresult by simply multiplying a factor 30. We use image in-\nversion for better visualization\nhigh bit accuracy, which can validate the robustness of our\nmethod referring to different sampling strategies.\nComparison with NeRF+HiDDeN/MBRS [50, 14]. We\nalso conduct an experiment to compare our method with\napproaches by directly applying 2D watermarking method\non rendered images, namely NeRF+HiDDeN [50] and\nNeRF+MBRS [14]. Although these methods can reach a\nhigh bit accuracy as reported in their papers, as shown in\nFigure 6, these methods can easily leave detectable traces\nespecially in areas with lower geometry values, as they lack\nthe consideration for 3D information during watermarking.\nBesides, they only consider the media in 2D domain and\ncannot protect the NeRF model weights.\n6. Conclusions\nIn this paper, we propose a framework to create a\ncopyright-embedded 3D implicit representation by embed-\nding messages into model weights. In order to guarantee\nthe invisibility of embedded information, we keep the ge-\nometry unchanged and construct a watermarked color rep-\nresentation to produce the message embedded color. The\nembedded message can be extracted by a CNN-based ex-\ntractor from rendered images from any viewpoints, while\nkeeping high reconstruction quality. Additionally, we intro-\nduce a distortion-resistant rendering scheme to enhance the\nrobustness of our model under different types of distortion,\nincluding classical 2D degradation and different rendering\nstrategies. The proposed method achieves a promising bal-\nance between bit accuracy and high visual quality in exper-\nimental evaluations.\nLimitations. Though our method has shown promising per-\nformance in claiming the ownership of Neural Radiance\nFields, training a NeRF model is time-consuming. We will\nconsider how to speed up the training process in our future\nwork. Besides, though we have considered several designs\nto strengthen the system robustness, this standard may still\nbe undermined when malicious users directly attack model\nweights, i.e., the model weights are corrupted. We conduct\na simple experiment by directly adding Gaussian noise (std\n= 0.01) to the model parameters, and the accuracy slightly\ndecreases to 93.97% (Nb = 8). As this may also affect\nrendering quality, such model weights corruption may not\nbe a priority for malicious users who intend to display the\ncontent. We will still actively consider how to handle such\nattacks in our future work.\nAcknowledgement. Renjie Wan is supported by the Blue\nSky Research Fund of HKBU under Grant No. BSRF/21-\n22/16 and Guangdong Basic and Applied Basic Research\nFoundation under Grant No. 2022A1515110692. Qing Guo\nis supported by the A*STAR Centre for Frontier AI Re-\nsearch and the National Research Foundation, Singapore,\nand DSO National Laboratories under the AI Singapore\nProgramme (AISG Award No: AISG2-GC-2023-008).\nReferences\n[1] Mahdi Ahmadi, Alireza Norouzi, Nader Karimi, Shadrokh\nSamavi, and Ali Emami. Redmark: Framework for resid-\nual diffusion watermarking based on deep networks. Expert\nSystems with Applications, 2020.\n[2] Shumeet Baluja.\nHiding images in plain sight:\nDeep\nsteganography. In Advances in Neural Information Process-\ning Systems, 2017.\n[3] Shumeet Baluja. Hiding images within images. IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence, 2019.\n[4] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3D generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2022.\n[5] Eric R Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu,\nand Gordon Wetzstein. pi-GAN: Periodic implicit genera-\ntive adversarial networks for 3D-aware image synthesis. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 2021.\n[6] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su.\nMVSNeRF: Fast\ngeneralizable radiance field reconstruction from multi-view\nstereo. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 2021.\n[7] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-\nSheng Lai, and Wei-Chen Chiu. Stylizing 3D scene via im-\nplicit representation and hypernetwork. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, 2022.\n[8] Daniel Cotting, Tim Weyrich, Mark Pauly, and Markus\nGross. Robust watermarking of point-sampled geometry. In\nProceedings Shape Modeling Applications, 2004., 2004.\n[9] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022.\n[10] Mohamed Hamidi, Aladine Chetouani, Mohamed El Haziti,\nMohammed El Hassouni, and Hocine Cherifi. Blind robust\n3D mesh watermarking based on mesh saliency and wavelet\ntransform for copyright protection. Information, 2019.\n[11] Jong-Uk Hou, Do-Gon Kim, and Heung-Kyu Lee. Blind 3D\nmesh watermarking for 3D printed model by analyzing lay-\nering artifact. IEEE Transactions on Information Forensics\nand Security, 2017.\n[12] Yi-Hua Huang, Yue He, Yu-Jie Yuan, Yu-Kun Lai, and Lin\nGao. StylizedNeRF: Consistent 3D scene stylization as styl-\nized NeRF via 2D-3D mutual learning. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022.\n[13] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International Conference on Machine Learn-\ning, 2015.\n[14] Zhaoyang Jia, Han Fang, and Weiming Zhang. MBRS: En-\nhancing robustness of DNN-based watermarking by mini-\nbatch of real and simulated jpeg compression. In Proceed-\nings of the 29th ACM International Conference on Multime-\ndia, 2021.\n[15] Junpeng Jing, Xin Deng, Mai Xu, Jianyi Wang, and Zhenyu\nGuan. HiNet: deep image hiding by invertible network. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2021.\n[16] Ki-Ryong Kwon, Seong-Geun Kwon, Suk-Hawn Lee, Tae-\nSu Kim, and Kuhn-Il Lee. Watermarking for 3D polygonal\nmeshes using normal vector distributions of each patch. In\nProceedings 2003 International Conference on Image Pro-\ncessing, 2003.\n[17] Chih-Chin Lai and Cheng-Chih Tsai. Digital image water-\nmarking using discrete wavelet transform and singular value\ndecomposition. IEEE Transactions on Instrumentation and\nMeasurement, 2010.\n[18] David B Lindell, Julien NP Martel, and Gordon Wetzstein.\nAutoint: Automatic integration for fast neural volume ren-\ndering.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2021.\n[19] Jing Liu,\nYajie Yang,\nDouli Ma,\nWenjuan He,\nand\nYinghui Wang. A novel watermarking algorithm for three-\ndimensional point-cloud models based on vertex curvature.\nInternational Journal of Distributed Sensor Networks, 2019.\n[20] Shaohui Liu, Yinda Zhang, Songyou Peng, Boxin Shi, Marc\nPollefeys, and Zhaopeng Cui. DIST: Rendering deep im-\nplicit signed distance function with differentiable sphere\ntracing.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020.\n[21] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se-\nbastian Nowozin, and Andreas Geiger. Occupancy networks:\nLearning 3D reconstruction in function space. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019.\n[22] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar. Local light field fusion: Practical view syn-\nthesis with prescriptive sampling guidelines. ACM Transac-\ntions on Graphics (TOG), 2019.\n[23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Proceedings of the European Conference on Com-\nputer Vision, 2020.\n[24] Thomas Neff, Pascal Stadlbauer, Mathias Parger, Andreas\nKurz, Joerg H. Mueller, Chakravarty R. Alla Chaitanya, An-\nton S. Kaplanyan, and Markus Steinberger. DONeRF: To-\nwards Real-Time Rendering of Compact Neural Radiance\nFields using Depth Oracle Networks. Computer Graphics\nForum, 2021.\n[25] Michael Niemeyer, Lars Mescheder, Michael Oechsle, and\nAndreas Geiger. Differentiable volumetric rendering: Learn-\ning implicit 3D representations without 3D supervision. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 2020.\n[26] Ryutarou Ohbuchi, Akio Mukaiyama, and Shigeo Takahashi.\nA frequency-domain approach to watermarking 3D shapes.\nIn Computer Graphics Forum, 2002.\n[27] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove.\nDeepSDF: Learning\ncontinuous signed distance functions for shape representa-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2019.\n[28] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc\nPollefeys, and Andreas Geiger.\nConvolutional occupancy\nnetworks. In Proceedings of the European Conference on\nComputer Vision, 2020.\n[29] Emil Praun, Hugues Hoppe, and Adam Finkelstein. Robust\nmesh watermarking. In Proceedings of the 26th Annual Con-\nference on Computer Graphics and Interactive Techniques,\n1999.\n[30] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor-\nishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned\nimplicit function for high-resolution clothed human digitiza-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, 2019.\n[31] Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas\nGeiger. GRAF: Generative radiance fields for 3D-aware im-\nage synthesis. Advances in Neural Information Processing\nSystems, 2020.\n[32] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wet-\nzstein.\nScene representation networks: Continuous 3D-\nstructure-aware neural scene representations. Advances in\nNeural Information Processing Systems, 2019.\n[33] Jeongho Son, Dongkyu Kim, Hak-Yeol Choi, Han-Ul Jang,\nand Sunghee Choi. Perceptual 3D watermarking using mesh\nsaliency. In International Conference on Information Sci-\nence and Applications, 2017.\n[34] Matthew Tancik, Ben Mildenhall, and Ren Ng. Stegastamp:\nInvisible hyperlinks in physical photographs. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, 2020.\n[35] R.G. van Schyndel, A.Z. Tirkel, and C.F. Osborne. A digital\nwatermark. In Proceedings of 1st International Conference\non Image Processing, 1994.\n[36] Can Wang, Ruixiang Jiang, Menglei Chai, Mingming\nHe, Dongdong Chen, and Jing Liao.\nNeRF-Art: Text-\ndriven neural radiance fields stylization.\narXiv preprint\narXiv:2212.08070, 2022.\n[37] Xinyu Weng, Yongzhi Li, Lu Chi, and Yadong Mu. High-\ncapacity convolutional video steganography with temporal\nresidual modeling. In Proceedings of the International Con-\nference on Multimedia Retrieval, 2019.\n[38] Eric Wengrowski and Kristin Dana. Light field messaging\nwith deep photographic steganography. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2019.\n[39] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3D\nShapeNets: A deep representation for volumetric shapes. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2015.\n[40] Yinghao Xu, Sida Peng, Ceyuan Yang, Yujun Shen, and\nBolei Zhou. 3D-aware image synthesis via learning struc-\ntural and textural representations.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022.\n[41] Peng Yang, Yingjie Lao, and Ping Li. Robust watermark-\ning for deep neural networks via bi-level optimization. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, 2021.\n[42] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan\nAtzmon, Basri Ronen, and Yaron Lipman. Multiview neu-\nral surface reconstruction by disentangling geometry and ap-\npearance. Advances in Neural Information Processing Sys-\ntems, 2020.\n[43] Innfarn Yoo, Huiwen Chang, Xiyang Luo, Ondrej Stava,\nCe Liu, Peyman Milanfar, and Feng Yang.\nDeep 3D-to-\n2D watermarking: Embedding messages in 3D meshes and\nextracting them from 2D renderings.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022.\n[44] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\nPixelNeRF: Neural radiance fields from one or few images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2021.\n[45] Chaoning Zhang, Philipp Benz, Adil Karjauv, Geng Sun, and\nIn So Kweon. Udh: Universal deep hiding for steganography,\nwatermarking, and light field messaging. Advances in Neural\nInformation Processing Systems, 2020.\n[46] Kevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, and\nKalyan Veeramachaneni. Robust invisible video watermark-\ning with attention. arXiv preprint arXiv:1909.01285, 2019.\n[47] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion, 2018.\n[48] Chengxuan Zhu, Renjie Wan, and Boxin Shi. Neural trans-\nmitted radiance fields. In Advances in Neural Information\nProcessing Systems, 2022.\n[49] Chengxuan Zhu, Renjie Wan, Yunkai Tang, and Boxin Shi.\nOcclusion-free scene recovery via neural radiance fields. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 2023.\n[50] Jiren Zhu, Russell Kaplan, Justin Johnson, and Li Fei-Fei.\nHiDDeN: Hiding data with deep networks. In Proceedings\nof the European Conference on Computer Vision, 2018.\n"
  },
  {
    "title": "FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields",
    "link": "https://arxiv.org/pdf/2307.11418.pdf",
    "upvote": "7",
    "text": "FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable\nNeural Radiance Fields\nSungwon Hwang1\nJunha Hyung1\nDaejin Kim2\nMin-Jung Kim1\nJaegul Choo1\n1KAIST\n2Scatter Lab\n{shwang.14, sharpeeee, emjay73, jchoo}@kaist.ac.kr, daejin@scatterlab.co.kr\n\u201chappy face\u201d\n\u201ccrying face\u201d\n(b) Text-driven manipulations to (left) locally descriptive and (right) emotional expression texts\n(a) Dynamic Scene\nReference\n\u201cclosed eyes and \nopened mouth\u201d\n\u201cfrowning eyes and \npursed lips\u201d\n\u201cscared face\u201d\nFigure 1: FaceCLIPNeRF reconstructs a video of a dynamic scene of a face, and conducts face manipulation using texts\nonly. Manipulated faces and their depths in top and bottom rows in (b), respectively, are rendered from novel views.\nAbstract\nAs recent advances in Neural Radiance Fields (NeRF)\nhave enabled high-fidelity 3D face reconstruction and novel\nview synthesis, its manipulation also became an essential\ntask in 3D vision. However, existing manipulation meth-\nods require extensive human labor, such as a user-provided\nsemantic mask and manual attribute search unsuitable for\nnon-expert users. Instead, our approach is designed to re-\nquire a single text to manipulate a face reconstructed with\nNeRF. To do so, we first train a scene manipulator, a latent\ncode-conditional deformable NeRF, over a dynamic scene\nto control a face deformation using the latent code. How-\never, representing a scene deformation with a single latent\ncode is unfavorable for compositing local deformations ob-\nserved in different instances. As so, our proposed Position-\nconditional Anchor Compositor (PAC) learns to represent\na manipulated scene with spatially varying latent codes.\nTheir renderings with the scene manipulator are then op-\ntimized to yield high cosine similarity to a target text in\nCLIP embedding space for text-driven manipulation. To the\nbest of our knowledge, our approach is the first to address\nthe text-driven manipulation of a face reconstructed with\nNeRF. Extensive results, comparisons, and ablation studies\ndemonstrate the effectiveness of our approach.\n1. Introduction\nEasy manipulation of 3D face representation is an\nessential aspect of advancements in 3D digital human\ncontents[32]. Though Neural Radiance Field[20] (NeRF)\nmade a big step forward in a 3D scene reconstruction, many\nof its manipulative methods targets color[4, 34] or rigid ge-\narXiv:2307.11418v3  [cs.CV]  17 Aug 2023\nometry [45, 15, 41, 14] manipulations, which are inappro-\npriate for detailed facial expression editing tasks. While a\nrecent work proposed a regionally controllable face editing\nmethod [13], it requires an exhaustive process of collect-\ning user-annotated masks of face parts from curated train-\ning frames, followed by manual attribute control to achieve\na desired manipulation. Face-specific implicit representa-\ntion methods [6, 47] utilize parameters of morphable face\nmodels [36] as priors to encode observed facial expressions\nwith high fidelity.\nHowever, their manipulations are not\nonly done manually but also require extensive training sets\nof approximately 6000 frames that cover various facial ex-\npressions, which are laborious in both data collection and\nmanipulation phases. On the contrary, our approach only\nuses a single text to conduct facial manipulations in NeRF,\nand trains over a dynamic portrait video with approximately\n300 training frames that include a few types of facial defor-\nmation examples as in Fig. 1a.\nIn order to control a face deformation, our method first\nlearns and separates observed deformations from a canon-\nical space leveraging HyperNeRF[23].\nSpecifically, per-\nframe deformation latent codes and a shared latent code-\nconditional implicit scene network are trained over the\ntraining frames. Our key insight is to represent the defor-\nmations of a scene with multiple, spatially-varying latent\ncodes for manipulation tasks. The insight originates from\nthe shortcomings of na\u00a8\u0131vely adopting the formulations of\nHyperNeRF to manipulation tasks, which is to search for a\nsingle latent code that represents a desired face deformation.\nFor instance, a facial expression that requires a combination\nof local deformations observed in different instances is not\nexpressible with a single latent code. In this work, we de-\nfine such a problem as \u201clinked local attribute problem\u201d and\naddress this issue by representing a manipulated scene with\nspatially varying latent codes. As a result, our manipulation\ncould express a combination of locally observed deforma-\ntions as seen from the image rendering highlighted with red\nboundary in Fig. 2a.\nTo this end, we first summarize all observed deforma-\ntions as a set of anchor codes and let MLP learn to compose\nthe anchor codes to yield multiple, position-conditional la-\ntent codes. The reflectivity of the latent codes on visual\nattributes of a target text is then achieved by optimizing the\nrendered images of the latent codes to be close to a target\ntext in CLIP[27] embedding space. In summary, our work\nmakes the following contributions:\n\u2022 Proposal of a text-driven manipulation pipeline of a\nface reconstructed with NeRF.\n\u2022 Design of a manipulation network that learns to repre-\nsent a scene with spatially varying latent codes.\n\u2022 First to conduct text-driven manipulation of a face re-\nconstructed with NeRF to the best of our knowledge.\n\ud835\udc31!(canonical space)\n\ud835\udc30 (hyper space)\n\ud835\udc31\ud835\udfcf\n! (mouth)\n\ud835\udc31\ud835\udfd0\n! (eyes)\n\ud835\udc3b(\ud835\udc31!, \ud835\udc64\")\n\ud835\udc39(\ud835\udc31\ud835\udfd0\n! , \ud835\udc30\"$)\n\ud835\udc39(\ud835\udc31\ud835\udfd0\n! , \ud835\udc30\"\")\n\ud835\udc39(\ud835\udc31\ud835\udfcf\n! , \ud835\udc30$$)\n\ud835\udc39(\ud835\udc31\ud835\udfcf\n! , \ud835\udc30$\")\n\ud835\udc3b(\ud835\udc31!, \ud835\udc64$)\n\u2217 \ud835\udc30\"# = \ud835\udc3b(\ud835\udc31\"\n$, \ud835\udc64#)\nInterpolative slicing volume\nSlicing surfaces from learned codes\nInterpolative slicing surfaces\nNon-interpolative slicing surface\n(a)\n(b)\n(c)\nFigure 2: (a) Illustration of linked local attribute problem\nin hyper space. Expressing scene deformation with per-\nscene latent code cannot compose local facial deformation\nobserved in different instances. (b) Types of facial defor-\nmations observed during scene manipulator training. (c)\nRenderings of interpolated latent codes with a scene ma-\nnipulator.\n2. Related Works\nNeRF and Deformable NeRF\nGiven multiple images\ntaken from different views of a target scene, NeRF[20] syn-\nthesizes realistic novel view images with high fidelity by\nusing an implicit volumetric scene function and volumet-\nric rendering scheme[12], which inspired many follow-ups\n[1, 35, 19, 37, 44]. As NeRF assumes a static scene, recent\nworks [22, 23, 26, 16] propose methods to encode dynamic\nscenes of interest. The common scheme of the works is to\ntrain a latent code per training frame and a single latent-\nconditional NeRF model shared by all trained latent codes\nto handle scene deformations. Our work builds on this de-\nsign choice to learn and separate the observed deformations\nfrom a canonical space, yet overcome its limitation during\nthe manipulation stage by representing a manipulated scene\nwith spatially varying latent codes.\nText-driven 3D Generation and Manipulation\nMany\nworks have used text for images or 3D manipulation[38,\n9, 25, 11, 29, 10]. CLIP-NeRF[38] proposed a disentan-\ngled conditional NeRF architecture in a generative formu-\nlation supervised by text embedding in CLIP[27] space, and\nconducted text-and-exemplar driven editing over shape and\nappearance of an object. Dreamfields [9] performed gen-\nerative text-to-3D synthesis by supervising its generations\nin CLIP embedding space to a generation text. We extend\nfrom these lines of research to initiate CLIP-driven manip-\nulation of face reconstructed with NeRF.\nNeRF Manipulations\nAmong many works that studied\nNeRF manipulations[18, 45, 36, 13, 34, 33, 7, 48, 15],\nEditNeRF[18] train conditional NeRF on a shape category\nto learn implicit semantics of the shape parts without ex-\nplicit supervision. Then, its manipulation process propa-\ngates user-provided scribbles to appropriate object regions\nfor editing. NeRF-Editing[45] extracts mesh from trained\nNeRF and lets the user perform the mesh deformation. A\nnovel view of the edited scene can be synthesized with-\nout re-training the network by bending corresponding rays.\nCoNeRF[13] trains controllable neural radiance fields using\nuser-provided mask annotations of facial regions so that the\nuser can control desired attributes within the region. How-\never, such methods require laborious annotations and man-\nual editing processes, whereas our method requires only a\nsingle text for detailed manipulation of faces.\nNeural Face Models\nSeveral works[42, 28, 47] built 3D\nfacial models using neural implicit shape representation. Of\nthe works, i3DMM[42] disentangles face identity, hairstyle,\nand expression, making decoupled components to be man-\nually editable. Face representation works based on NeRF\nhave also been exploited[39, 36, 47]. Wang et al.[39] pro-\nposed compositional 3D representation for photo-realistic\nrendering of a human face, yet requires guidance images\nto extract implicitly controllable codes for facial expression\nmanipulation. NerFACE[36] and IMavatar[47] model the\nappearance and dynamics of a human face using learned 3D\nMorphable Model[2] parameters as priors to achieve con-\ntrollability over pose and expressions. However, the meth-\nods require a large number of training frames that cover\nmany facial expression examples and manual adjustment of\nthe priors for manipulation tasks.\n3. Preliminaries\n3.1. NeRF\nNeRF [20] is an implicit representation of geometry and\ncolor of a space using MLP. Specifically, given a point co-\nordinate x = (x, y, z) and a viewing direction d, an MLP\nfunction F is trained to yield density and color of the point\nas (c, \u03c3) = F(x, d). M number of points are sampled along\na ray r = o + td using distances, {ti}M\ni=0, that are collected\nfrom stratified sampling method. F predicts color and den-\nsity of each point, all of which are then rendered to predict\npixel color of the ray from which it was originated as\n\u02c6C(r) =\nM\nX\ni=1\nTi(1 \u2212 exp(\u2212\u03c3i\u03b4i))ci,\n(1)\nwhere \u03b4i = ti+1 \u2212 ti, and Ti = exp(\u2212 Pi\u22121\nj=1 \u03c3j\u03b4j) is an\naccumulated transmittance. F is then trained to minimize\nthe rendering loss supervised with correspondingly known\npixel colors.\n3.2. HyperNeRF\nUnlike NeRF that is designed for a static scene, HyperN-\neRF [23] is able to encode highly dynamic scenes with large\ntopological variations. Its key idea is to project points to\ncanonical hyperspace for interpretation. Specifically, given\na latent code w, a spatial deformation field T maps a point\nto a canonical space, and a slicing surface field H deter-\nmines the interpretation of the point for a template NeRF\nF. Specifically,\nx\u2032 = T(x, w),\n(2)\nw = H(x, w),\n(3)\n(c, \u03c3) = F(x\u2032, w, d),\n(4)\nwhere w \u2190 wn \u2208 {w1 \u00b7 \u00b7 \u00b7 wN} = W is a trainable per-\nframe latent code that corresponds to each N number of\ntraining frames. Then, the rendering loss is finally defined\nas\nLc =\nX\nn\u2208{1\u00b7\u00b7\u00b7N},\nrn\u2208Rn\n||Cn(rn) \u2212 \u02c6Cn(rn)||2\n2,\n(5)\nwhere Cn(rn) is ground truth color at n-th training frame of\na ray rn and Rn is a set of rays from n-th camera. Note that\n(x\u2032, w) and H(x, w) are often referred to canonical hyper-\nspace and slicing surface, since x\u2032 can be interpreted differ-\nently for different w as illustrated in Fig. 2a.\n4. Proposed Method\nWe aim to manipulate a face reconstructed with NeRF\ngiven a target text that represents a desired facial expres-\nsions for manipulation (e.g., \u201ccrying face\u201d, \u201cwink eyes and\nsmiling mouth\u201d). To this end, our proposed method first\ntrains a scene manipulator, a latent code-conditional neural\nfield that controls facial deformations using its latent code\n(\u00a74.1). Then, we elaborate over the pipeline to utilize a tar-\nget text for manipulation (\u00a74.2), followed by proposing an\nMLP network that learns to appropriately use the learned\ndeformations and the scene manipulator to render scenes\nwith faces that reflect the attributes of target texts (\u00a74.3).\n\ud835\udc98\u2217\n\ud835\udc31\n\ud835\udc6e\n(b) Vanilla Inversion\n(a) Scene Manipulator\nTrain for manipulation\nTrained & fixed during manipulation  \n(c) Position-conditional Anchor Compositor (PAC)\n\u0d25\ud835\udc64\ud835\udc45\n\ud835\udc31\u2032\n\ud835\udc30\n\ud835\udf48\n\ud835\udc84\nDeformation \nField\nAmbient\nSlicing Surface\nTemplate\nNeRF\n\ud835\udc31\n\ud835\udc64\n\ud835\udc1d\n\ud835\udc6e\n\u0d24\ud835\udc47\n\u0d25\ud835\udc3b\n\u0d24\ud835\udc39\n\ud835\udc6e\n\ud835\udc31\n\u2026\n\u2026\n\u03a3\n\u0d25\ud835\udc641\n\ud835\udc34\n\u0d25\ud835\udc64\ud835\udc3e\n\ud835\udc34\n\u2026\n\u0d25\ud835\udc641\n\ud835\udc34\n\u0d25\ud835\udc64\ud835\udc3e\n\ud835\udc34\n\ud835\udc64\ud835\udc31\u2217\nAnchor \nCodes\n\ud835\udefc[\ud835\udc31,1]\n\ud835\udc43\nAnchor Composition\nNetwork\n\ud835\udcdb\ud835\udc84\n\ud835\udcdb\ud835\udc8d\ud835\udc8a\ud835\udc91\n\ud835\udefc[\ud835\udc31,\ud835\udc3e]\n\ud835\udcdb\ud835\udc68\ud835\udc6a\ud835\udc79\n\u2219\n\u2219\n\ud835\udcdb\ud835\udc6a\ud835\udc73\ud835\udc70\ud835\udc77\n\ud835\udcdb\ud835\udc6a\ud835\udc73\ud835\udc70\ud835\udc77\nFigure 3: (a) Network structure of scene manipulator G. (b) Vanilla inversion method for manipulation. (c) Position-\nconditional Anchor Compositor (PAC) for manipulation.\n4.1. Scene Manipulator\nFirst,\nwe\nconstruct\na\nscene\nmanipulator\nusing\nHyperNeRF[23] so that deformations of a scene can\nbe controlled by fixing the parameters of the scene manip-\nulator and manipulating its latent code. Specifically, we\ntrain a dynamic scene of interest with a network formulated\nas Eq.(4) following [23], after which we freeze the trained\nparameters of T, H, F, and W and use w as a manipulation\nhandle. In addition, we empirically found that the deforma-\ntion network T tends to learn rigid deformations, such as\nhead pose, while slicing surface field H learns non-rigid\nand detailed deformations, such as shapes of mouth and\neyes. As so, we select and fix a trained latent code for T\nand only manipulate a latent code fed to H. In summary,\nas illustrated in Fig. 3(a), our latent code-conditional scene\nmanipulator G is defined as\nG(x, d, w) := \u00afF( \u00afT(x, \u00afwR), \u00afH(x, w), d),\n(6)\nwhere \u00af\u00b7 represents that the parameters are trained and fixed\nfor manipulation, and \u00afwR is a fixed latent code of the de-\nsired head pose chosen from a set of learned latent codes\n\u00afW. In the supplementary material, we report further exper-\nimental results and discussions over head pose controllabil-\nity of \u00afwR.\nLipschitz MLP\nSince G is only trained to be conditioned\nover a limited set of trainable latent codes W, a subspace\nof w outside the learned latent codes that yields plausible\ndeformations needs to be formulated to maximize the ex-\npressibility of G for manipulation. Meanwhile, HyperNeRF\nwas shown to moderately render images from latent codes\nlinearly interpolated from two learned latent codes. Thus,\na valid latent subspace W can be formulated to include not\nonly the learned latent codes but codes linearly interpolated\nbetween any two learned latent codes as well. Specifically,\nW \u2283 {\u03b3 \u2217 \u00afwi + (1 \u2212 \u03b3) \u2217 \u00afwj | \u00afwi, \u00afwj \u2208 \u00afW,\n0 \u2264 \u03b3 \u2264 1}.\n(7)\nHowever, we learned that the fidelity of images from\ninterpolated latent codes needs to be higher to be lever-\naged for manipulation.\nAs so, we regularize the MLPs\nof the scene manipulator to be more Lipschitz continu-\nous during its training phase. Note that Lipschitz bound\nof a neural network with L number of layers and piece-\nwise linear functions such as ReLU can be approximated\nas c = QL\ni=1 \u2225Wi\u2225p [17, 43], where Wi is an MLP weight\nat i-th layer. Since a function f that is c-Lipschitz has the\nproperty\n\u2225f(w1) \u2212 f(w2)\u2225p \u2264 c\u2225w1 \u2212 w2\u2225p,\n(8)\nsuccessful regularization of c would make smaller differ-\nences between outputs of adjacent latent codes, which in-\nduce interpolated deformations to be more visually natural.\nAs so, we follow [17] and regularize trainable matrix at l-th\nlayer of F by introducing extra trainable parameters cl as\nyl = \u03c3( \u02c6W\nlx+bl), \u02c6W\nl\nj = Wl\nj \u00b7min(1, softplus(cl)\n\u2225Wl\nj\u2225\u221e\n), (9)\nwhere Wl\nj is the j-th row of a trainable matrix at l-th layer\nWl, and \u2225 \u00b7 \u2225\u221e is matrix \u221e-norm. Trinable Lipschitz con-\nstants from the layers are then minimized via gradient-based\noptimization with loss function defined as\nLlip =\nL\nY\nl=1\nsoftplus(cl).\n(10)\nIn summary, networks in Eq. (4) are trained to retrieve\n\u00afF, \u00afT, \u00afH, and \u00afW using our scene manipulator objective\nfunction\nLSM = \u03bbcLc + \u03bblipLlip,\n(11)\nwhere \u03bbc and \u03bblip are hyper-parameters.\n4.2. Text-driven Manipulation\nGiven a trained scene manipulator G, one manipulation\nmethod is to find a single optimal latent code w whose ren-\ndered image using G yields the highest cosine similarity\nwith a target text in CLIP[27] embedding space, so that the\nmanipulated images can reflect the visual attributes of a tar-\nget text. Specifically, given images rendered with G and w\nat a set of valid camera poses [R|t] as IG,w\n[R|t] and a target text\nfor manipulation p, the goal of the method is to solve the\nfollowing problem:\nw\u2217 = arg max\nw\nDCLIP(IG,w\n[R|t], p),\n(12)\nwhere DCLIP measures the cosine similarity of features be-\ntween rendered images and a target text extracted from pre-\ntrained CLIP model.\nAs illustrated in Fig. 3b, a straightforward vanilla ap-\nproach to find an optimal latent embedding w\u2217 is inversion,\na gradient-based optimization of w that maximizes Eq.(12)\nby defining a loss function as LCLIP = 1\u2212DCLIP(IG,w\n[R|t], p).\nHowever, we show that this method is sub-optimal by show-\ning that it inevitably suffers from what we define as a linked\nlocal attributes problem, which we then solve with our pro-\nposed method.\nLinked local attribute problem\nSolutions from the\nvanilla inversion method are confined to represent deforma-\ntions equivalent to those from W. However, W cannot rep-\nresent all possible combinations of locally observed defor-\nmations, as interpolations between two learned latent codes,\nwhich essentially comprise W, cause facial attributes in dif-\nferent locations to change simultaneously.\nFor example,\nconsider a scene with deformations in Fig. 2b and render-\nings of interpolations between two learned latent codes in\nFig. 2c. Not surprisingly, neither the learned latent codes\nnor the interpolated codes can express opened eyes with\nopened mouth or closed eyes with a closed mouth. Simi-\nlar experiments can be done with any pair of learned latent\ncodes and their interpolations to make the same conclusion.\nWe may approach this problem from the slicing surface\nperspective of canonical hyperspace introduced in Sec. 3.2.\nAs in Fig. 2a, hyperspace allows only one latent code to rep-\nresent an instance of a slicing surface representing a global\ndeformation of all spatial locations. Such representation\ncauses a change in one type of deformation in one loca-\ntion to entail the same degree of change to another type of\ndeformation in different locations during interpolation.\nOur method is motivated by the observation and is there-\nfore designed to allow different position x to be expressed\nwith different latent codes to solve the linked local attribute\nproblem.\n4.3. Position-conditional Anchor Compositor\nFor that matter, Position-conditional Anchor Composi-\ntor (PAC) is proposed to grant our manipulation pipeline\nthe freedom to learn appropriate latent codes for different\nspatial positions.\nSpecifically, we define anchor codes { \u00afwA\n1 , \u00b7 \u00b7 \u00b7 \u00afwA\nK} =\n\u00afW A \u2282 \u00afW , a subset of learned latent codes where each rep-\nInterpolative, valid region\nExtrapolative, unseen region\nFigure 4: Illustration of barycentric interpolation of latent\ncodes for validly expressive regions when K = 3.\nresent different types of observed facial deformations, to set\nup a validly explorable latent space as a prior. We retrieve\nanchor codes by extracting facial expression parameters us-\ning DECA[5] from images rendered from all codes in \u00afW\nover a fixed camera pose. Then, we cluster the extracted ex-\npression parameters using DBSCAN[3] and select the latent\ncode corresponding to the expression parameter closest to\nthe mean for each cluster. For instance, we may get K = 4\nanchor codes in the case of the example scenes in Fig. 1a\nand Fig. 2b.\nThen for every spatial location, a position-conditional\nMLP yields appropriate latent codes by learning to com-\npose these anchor codes. By doing so, a manipulated scene\ncan be implicitly represented with multiple, point-wise la-\ntent codes. Specifically, the anchor composition network\nP : R(3+dw) \u2192 R1 learns to yield w\u2217\nx for every spatial\nposition x via barycentric interpolation[8] of anchors as\n\u02c6\u03b1[x,k] = P(x \u2295 \u00afwA\nk ), w\u2217\nx =\nX\nk\n\u03c3k(\u02c6\u03b1[x,k]) \u00afwA\nk ,\n(13)\nwhere dw is the dimension of a latent code, \u2295 is concatena-\ntion, and \u03c3k is softmax activation along k network outputs.\nAlso, denote \u03b1[x,k] = \u03c3k(\u02c6\u03b1[x,k]) as anchor composition ra-\ntio (ACR) for ease of notation. As in the illustrative exam-\nple in Fig. 4, the key of the design is to prevent the com-\nposited code from diverging to extrapolative region of the\nlatent. Thus, barycentric interpolation defines a safe bound\nof composited latent code for visually natural renderings.\nFinally, a set of points that are sampled from rays pro-\njected at valid camera poses and their corresponding set of\nlatent codes [w\u2217\nx] are queried by G, whose outputs are ren-\ndered as images to be supervised in CLIP embedding space\nfor manipulation as\nLCLIP = 1 \u2212 DCLIP(IG,[w\u2217\n[R|t] x ]\n, p),\n(14)\nTotal variation loss on anchor composition ratio\nAs,\nthe point-wise expressibility of PAC allows adjacent latent\ncodes to vary without mutual constraints, P is regularized\nwith total variation (TV) loss.\nSmoother ACR fields al-\nlows similar latent embeddings to cover certain facial po-\n\u201cwink eyes and \nsmiling mouth\u201d\n\u201cclosed eyes and \nsmiling mouth\u201d\n\u201cclosed, frowning eyes and \nsmiling mouth\u201d\n\u201cfrowning eyes and \npursed lips\u201d\n\u201cclosed eyes and \npursed lips\u201d\n\u201cclosed, frowning eyes and \nclosed mouth\u201d\n\u201cwink eyes and \nwidely opened mouth\u201d\nReference\n\u201copened eyes and \nsmiling mouth\u201d\n\u201copened eyes and \nwidely opened mouth\u201d\n\u201cfrowning eyes and \nclosed mouth\u201d\n\u201cwink, frowning eyes and \nclosed mouth\u201d\nFigure 5: Qualitative results manipulated with descriptive\ntexts using our method. Local facial deformations can eas-\nily be controlled using texts only.\nsitions to yield more naturally rendered images. Specifi-\ncally, \u03b1[x,k] is rendered to valid camera planes using the\nrendering equation in Eq. (1) for regularization. Given a\nray ruv(t) = o + tduv, ACR can be rendered for each an-\nchor k at an image pixel located at (u, v) of a camera plane,\nand regularized with TV loss as\n\u02dc\u03b1kuv =\nM\nX\ni=1\nTi(1 \u2212 exp(\u2212\u03c3i\u03b4i))\u03b1[ruv(ti),k],\n(15)\nLACR =\nX\nk,u,v\n\u2225\u02dc\u03b1k(u+1)v \u2212 \u02dc\u03b1kuv\u22252 + \u2225\u02dc\u03b1ku(v+1) \u2212 \u02dc\u03b1kuv\u22252.\n(16)\nIn summary, text-driven manipulation is conducted by\noptimizing P and minimizing the following loss\nLedit = \u03bbCLIP LCLIP + \u03bbACRLACR\n(17)\nwhere \u03bbCLIP and \u03bbACR are hyper-parameters.\n5. Experiments\nDataset\nWe collected portrait videos from six volun-\nteers using Apple iPhone 13, where each volunteer was\nasked to make four types of facial deformations shown in\nFig. 1a and Fig. 2b.\nA pre-trained human segmentation\nnetwork was used to exclude descriptors from the dynamic\npart of the scenes during camera pose computation using\nCOLMAP[31]. Examples of facial deformations observed\n\u201csurprised\nface\u201d\nOurs\nNerfies + I\nReference\nNeRF + FT\n\u201chappy\nface\u201d\nHyperNeRF + I\n\u201cclosed eyes \nand \nopened mouth\u201d\n\u201cwink, frowning \neyes and \nclosed\nmouth\u201d\nFigure 6: Text-driven manipulation results of our method\nand the baselines. Our result well reflects the implicit at-\ntributes of target emotional texts while preserving visual\nquality and face identity.\nduring training for each scene are reported in the supple-\nmentary material.\nManipulation Texts\nWe selected two types of texts for\nmanipulation experiments. First is a descriptive text that\ncharacterizes deformations of each facial parts.\nSecond\nis an emotional expression text, which is an implicit rep-\nresentation of a set of multiple local deformations on all\nface parts hard to be described with descriptive texts. We\nselected 7 frequently used and distinguishable emotional\nexpression texts for our experiment:\n\u201dcrying\u201d, \u201ddisap-\npointed\u201d, \u201dsurprised\u201d, \u201dhappy\u201d, \u201dangry\u201d, \u201dscared\u201d and\n\u201dsleeping\u201d. To reduce text embedding noise, we followed\n[24] by averaging augmented embeddings of sentences with\nidentical meanings.\nBaselines\nSince there is no prior work that is parallel to\nour problem definition, we formulated 3 baselines with ex-\nisting state-of-the-art methods for comparisons: (1) NeRF\n+FT is a simple extension from NeRF [20] that fine-tunes\nthe whole network using CLIP loss, (2) Nerfies+I uses\nNerfies[22] as a deformation network followed by conduct-\ning vanilla inversion method introduced in Sec. \u00a74.2 for\nmanipulation, and (3) HyperNeRF+I replaces Nerfies in (2)\nwith HyperNeRF [23].\nText-driven Manipulation\nWe report qualitative manip-\nulation results of our methods driven with a set of descrip-\ntive sentences in Fig. 5. Our method not only faithfully re-\nflects the descriptions, but also can easily control local fa-\ncial deformations with simple change of words in sentences.\nWe also report manipulated results driven by emotional ex-\n\u201ccrying\u201d\n\u201cdisappointed\u201d\n\u201csurprised\u201d\n\u201cangry\u201d\n\u201chappy\u201d\n\u201cscared\u201d\n\u201csleeping\u201d\nReference\nFigure 7: Extensive face manipulation results driven by a set of frequently used emotional expression texts using our method.\nManipulating to emotional expression texts are challenging, as they implicitly require compositions of subtle facial deforma-\ntions that are hard to be described. Our method reasonably reflects the attributes of the manipulation texts.\npression texts in Fig. 7. As can be seen, our method con-\nducts successful manipulations even if the emotional texts\nare implicit representations of many local facial deforma-\ntions. For instance, result manipulated with \u201dcrying\u201d in first\nrow of Fig. 7 is not expressed with mere crying-looking eyes\nand mouth, but also includes crying-looking eyebrows and\nskin all over the face without any explicit supervision on lo-\ncal deformations. We also compare our qualitative results\nto those from the baselines in Fig. 6. Ours result in the\nhighest reflections of the target text attributes. NeRF+FT\nshows significant degradation in visual quality, while Ner-\nfies+I moderately suffers from low reconstruction quality\nand reflection of target text attributes. HyperNeRF+ I shows\nthe highest visual quality out of all baselines, yet fails to re-\nflect the visual attributes of target texts.\nHigh reflectivity on various manipulation texts can be at-\ntributed to PAC that resolves the linked local attribute prob-\nlem. In Fig. 8, we visualize \u02dc\u03b1kuv for each anchor code k,\nwhich is the rendering of ACR \u03b1[x,k] in Eq. (15), over an\nimage plane. Whiter regions of the renderings are closer\nto one, which indicates that the corresponding anchor code\nis mostly composited to yield the latent code of the region.\nAlso, we display image renderings from each anchor code\non the left to help understand the local attributes for each\nanchor code. As can be seen, PAC composes appropriate\nanchor codes for different positions. For example, when\nmanipulating for sleeping face, PAC reflects closed eyes\nR-Prec.[40] \u2191\nLPIPS[46] \u2193\nCFS \u2191\nNeRF + FT\n0.763\n0.350\n0.350\nNerfies + I\n0.213\n0.222\n0.684\nHyperNeRF + I\n0.342\n0.198\n0.721\nOurs\n0.780 (+0.017)\n0.082 (-0.116)\n0.749 (+0.028)\nTable 1: Quantitative results. R-Prec. denotes R-precision,\nand CFS denotes cosine face similarity. We notate perfor-\nmance ranks as best and second best.\nTR \u2191\nVR \u2191\nFP \u2191\nNeRF + FT\n2.85\n0.18\n0.79\nNerfies + I\n0.33\n3.61\n4.03\nHyperNeRF + I\n2.52\n4.42\n4.39\nOurs\n4.15 (+1.30)\n4.58 (+0.16)\n4.67 (+0.28)\nTable 2: User study results. TR, VR, and FP denote text\nreflectivity, visual realism, and face identity preservability,\nrespectively. Best and second best are highlighted.\nfrom one anchor code and neutral mouth from other anchor\ncodes. In the cases of crying, angry, scared, and disap-\npointed face, PAC learns to produce complicated composi-\ntions of learned deformations, which are inexpressible with\na single latent code.\nQuantitative Results\nFirst of all,\nwe measured R-\nprecision[40] to measure the text attribute reflectivity of\nImage Renderings by Anchor Codes\n\u201chappy\u201d\n\u201cangry\u201d\n\u201csurprised\u201d\n\u201ccrying\u201d\n\u201cdisapp-\nointed\u201d\n\u201csleeping\u201d\n\u201cscared\u201d\nRendered ACR Maps by Anchor Codes\n0\n1\nManipulated Results\nFigure 8: Renderings of learned ACR maps for each anchor\ncodes over different manipulation texts.\nthe manipulations.\nWe used facial expression recogni-\ntion model[30] pre-trained with AffectNet[21] for top-R\nretrievals of each text. Specifically, 1000 novel view im-\nages are rendered per face, where 200 images are rendered\nfrom a face manipulated with each of the five texts that\nare distinguishable and exist in AffectNet labels: \u201dhappy\u201d,\n\u201dsurprised\u201d, \u201dfearful\u201d, \u201dangry\u201d, and \u201dsad\u201d. Also, to es-\ntimate the visual quality after manipulation, we measured\nLPIPS[46] between faces with no facial expressions (neu-\ntral faces) without any manipulations and faces manipu-\nlated with 7 texts, each of which are rendered from 200\nnovel views. Note that LPIPS was our best estimate of vi-\nsual quality since there can be no pixel-wise ground truth of\ntext-driven manipulations. Lastly, to measure how much of\nthe facial identity is preserved after manipulation, we mea-\nsured the cosine similarity between face identity features1\nextracted from neutral faces and text-manipulated faces, all\nof which are rendered from 200 novel views. Table 1 re-\nports the average results over all texts, which shows that\nour method outperforms in all criteria.\nUser Study\nUsers were asked to score from 0 to 5 on 3\ncriteria; (i) Text Reflectivity: how well the manipulated ren-\nderings reflect the target texts, (ii) Visual Realism: how re-\nalistic do the manipulated images look, and (iii) Face iden-\ntity Preservability: how well do the manipulated images\npreserve the identity of the original face, over our method\nand the baselines. The following results are reported in Ta-\nble. 2. Our method outperforms all baselines, and espe-\n1https://github.com/ronghuaiyang/arcface-pytorch\nw/ \u2112!\"#\nw/o \u2112!\"#\nStart\nEnd\nFigure 9:\nRenderings from linearly interpolated latent\ncodes. Lipschitz-regularized scene manipulator interpolates\nunseen shapes more naturally.\nw/o \u2112!\"#\nw/o \u2112$%&\nOurs\nw/o \u2112$%&\nw/ \u2112$%&\n(a)\n(b)\nFigure 10: (a) Qualitative results of the ablation study. Ma-\nnipulations are done using \u201dcrying face\u201d as target text. (b)\nRendered ACR maps with and without LACR.\ncially in text reflectivity by a large margin. Note that the\nout-performance in user responses align with that from the\nquantitative results, which supports the consistency of eval-\nuations.\nInterpolation\nWe experiment with the effect of Lipschitz\nregularization on the scene manipulator by comparing the\nvisual quality of images rendered from linearly interpolated\nlatent codes, and report the results in Fig. 9. Lipschitz-\nregularized scene manipulator yields more visually nat-\nural images, which implies that learned set of anchor-\ncomposited latent codes [w\u2217\nx] are more likely to render re-\nalistically interpolated local deformations under Lipschitz-\nregularized scene manipulator.\nAblation Study\nWe conducted an ablation study on our\nregularization methods: Llip and LACR.\nAs shown in\nFig. 10a, manipulation without Llip suffers from low visual\nquality. Manipulation without LACR yields unnatural ren-\nderings of face parts with large deformation range such as\nmouth and eyebrows. This can be interpreted with learned\nACR maps of PAC in Fig. 10b. ACR maps learned with\nLACR introduces reasonable continuities of latent codes on\nboundaries of the dynamic face parts, thus yielding natu-\nrally interpolated face parts.\n6. Conclusion\nWe have presented FaceCLIPNeRF, a text-driven ma-\nnipulation pipeline of a 3D face using deformable NeRF.\nWe first proposed a Lipshitz-regularized scene manipula-\ntor, a conditional MLP that uses its latent code as a control\nhandle of facial deformations. We addressed the linked lo-\ncal attribute problem of conventional deformable NeRFs,\nwhich cannot compose deformations observed in different\ninstances. As so, we proposed PAC that learns to produce\nspatially-varying latent codes, whose renderings with the\nscene manipulator were trained to yield high cosine simi-\nlarity with target text in CLIP embedding space. Our ex-\nperiments showed that our method could faithfully reflect\nthe visual attributes of both descriptive and emotional texts\nwhile preserving visual quality and identity of 3D face.\nAcknowledgement\nThis material is based upon work\nsupported by the Air Force Office of Scientific Research\nunder award number FA2386-22-1-4024, KAIST-NAVER\nhypercreative AI center, and the Institute of Information &\ncommunications Technology Planning & Evaluation (IITP)\ngrant funded by the Korea government (MSIT) (No.2019-\n0-00075, Artificial Intelligence Graduate School Program\n(KAIST)).\nReferences\n[1] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 5855\u20135864,\n2021. 2\n[2] Volker Blanz and Thomas Vetter. A morphable model for\nthe synthesis of 3d faces. In Proceedings of the 26th an-\nnual conference on Computer graphics and interactive tech-\nniques, pages 187\u2013194, 1999. 3\n[3] Martin Ester, Hans-Peter Kriegel, J\u00a8org Sander, Xiaowei Xu,\net al. A density-based algorithm for discovering clusters in\nlarge spatial databases with noise. In kdd, volume 96, pages\n226\u2013231, 1996. 5\n[4] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia\nXu, and Zhangyang Wang. Unified implicit neural styliza-\ntion. arXiv preprint arXiv:2204.01943, 2022. 1\n[5] Yao Feng, Haiwen Feng, Michael J. Black, and Timo\nBolkart.\nLearning an animatable detailed 3D face model\nfrom in-the-wild images. volume 40, 2021. 5\n[6] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias\nNie\u00dfner. Dynamic neural radiance fields for monocular 4d\nfacial avatar reconstruction. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 8649\u20138658, 2021. 2\n[7] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juy-\nong Zhang.\nHeadnerf: A real-time nerf-based parametric\nhead model. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 20374\u2013\n20384, 2022. 3\n[8] Kai Hormann. Barycentric interpolation. In Approximation\nTheory XIV: San Antonio 2013, pages 197\u2013218. Springer,\n2014. 5\n[9] Ajay Jain, Ben Mildenhall, Jonathan T Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object genera-\ntion with dream fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n867\u2013876, 2022. 3\n[10] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter\nAbbeel, and Ben Poole. Zero-shot text-guided object gen-\neration with dream fields. 2022. 3\n[11] Nikolay Jetchev. Clipmatrix: Text-controlled creation of 3d\ntextured meshes. arXiv preprint arXiv:2109.12922, 2021. 3\n[12] James T Kajiya and Brian P Von Herzen. Ray tracing volume\ndensities. ACM SIGGRAPH computer graphics, 18(3):165\u2013\n174, 1984. 2\n[13] Kacper Kania, Kwang Moo Yi, Marek Kowalski, Tomasz\nTrzci\u00b4nski, and Andrea Tagliasacchi. CoNeRF: Controllable\nNeural Radiance Fields. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 2022. 2,\n3\n[14] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-\nmann. Decomposing nerf for editing via feature field distilla-\ntion. In Advances in Neural Information Processing Systems,\nvolume 35, 2022. 2\n[15] Verica Lazova, Vladimir Guzov, Kyle Olszewski, Sergey\nTulyakov, and Gerard Pons-Moll. Control-nerf: Editable fea-\nture volumes for scene rendering and manipulation. arXiv\npreprint arXiv:2204.10850, 2022. 2, 3\n[16] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\net al. Neural 3d video synthesis from multi-view video. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 5521\u20135531, 2022. 2\n[17] Hsueh-Ti Derek Liu, Francis Williams, Alec Jacobson, Sanja\nFidler, and Or Litany. Learning smooth neural functions via\nlipschitz regularization. arXiv preprint arXiv:2202.08345,\n2022. 4\n[18] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard\nZhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional\nradiance fields. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 5773\u20135783,\n2021. 3\n[19] Ben Mildenhall, Peter Hedman, Ricardo Martin-Brualla,\nPratul P Srinivasan, and Jonathan T Barron. Nerf in the dark:\nHigh dynamic range view synthesis from noisy raw images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 16190\u201316199, 2022.\n2\n[20] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 1, 2, 3, 6\n[21] Ali Mollahosseini, Behzad Hasani, and Mohammad H Ma-\nhoor. Affectnet: A database for facial expression, valence,\nand arousal computing in the wild. IEEE Transactions on\nAffective Computing, 10(1):18\u201331, 2017. 8\n[22] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5865\u20135874, 2021. 2, 6\n[23] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T.\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M. Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 40(6), dec 2021. 2, 3,\n4, 6\n[24] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In International Conference of Computer\nVision, pages 2085\u20132094, 2021. 6\n[25] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 3\n[26] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields for\ndynamic scenes. arXiv preprint arXiv:2011.13961, 2020. 2\n[27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In International Conference on Machine Learning,\npages 8748\u20138763. PMLR, 2021. 2, 3, 4\n[28] Eduard Ramon, Gil Triginer, Janna Escur, Albert Pumarola,\nJaime Garcia, Xavier Giro-i Nieto, and Francesc Moreno-\nNoguer. H3d-net: Few-shot high-fidelity 3d head reconstruc-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 5620\u20135629, 2021. 3\n[29] Aditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang,\nChin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malek-\nshan. Clip-forge: Towards zero-shot text-to-shape genera-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 18603\u201318613,\n2022. 3\n[30] Andrey V Savchenko. Frame-level prediction of facial ex-\npressions, valence, arousal and action units for mobile de-\nvices. arXiv preprint arXiv:2203.13436, 2022. 8\n[31] Johannes Lutz Sch\u00a8onberger, Enliang Zheng, Marc Pollefeys,\nand Jan-Michael Frahm. Pixelwise View Selection for Un-\nstructured Multi-View Stereo. In European Conference on\nComputer Vision (ECCV), 2016. 6\n[32] Sahil Sharma and Vijay Kumar. 3d face reconstruction in\ndeep learning era: A survey.\nArchives of Computational\nMethods in Engineering, pages 1\u201333, 2022. 1\n[33] Jingxiang Sun, Xuan Wang, Yichun Shi, Lizhen Wang, Jue\nWang, and Yebin Liu. Ide-3d: Interactive disentangled edit-\ning for high-resolution 3d-aware portrait synthesis.\narXiv\npreprint arXiv:2205.15517, 2022. 3\n[34] Jingxiang Sun, Xuan Wang, Yong Zhang, Xiaoyu Li, Qi\nZhang, Yebin Liu, and Jue Wang. Fenerf: Face editing in\nneural radiance fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n7672\u20137682, 2022. 1, 3\n[35] Matthew Tancik, Ben Mildenhall, Terrance Wang, Divi\nSchmidt, Pratul P Srinivasan, Jonathan T Barron, and Ren\nNg. Learned initializations for optimizing coordinate-based\nneural representations.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2846\u20132855, 2021. 2\n[36] Justus Thies, Michael Zollhofer, Marc Stamminger, Chris-\ntian Theobalt, and Matthias Nie\u00dfner. Face2face: Real-time\nface capture and reenactment of rgb videos.\nIn Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 2387\u20132395, 2016. 2, 3\n[37] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,\nJonathan T. Barron, and Pratul P. Srinivasan.\nRef-NeRF:\nStructured view-dependent appearance for neural radiance\nfields. CVPR, 2022. 2\n[38] Can Wang, Menglei Chai, Mingming He, Dongdong Chen,\nand Jing Liao.\nClip-nerf: Text-and-image driven manip-\nulation of neural radiance fields.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3835\u20133844, 2022. 3\n[39] Ziyan Wang, Timur Bagautdinov, Stephen Lombardi, Tomas\nSimon, Jason Saragih, Jessica Hodgins, and Michael Zoll-\nhofer. Learning compositional radiance fields of dynamic\nhuman heads. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5704\u2013\n5713, 2021. 3\n[40] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition, pages 1316\u2013\n1324, 2018. 7\n[41] Bangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han\nZhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui.\nLearning object-compositional neural radiance field for ed-\nitable scene rendering. In International Conference on Com-\nputer Vision (ICCV), October 2021. 2\n[42] Tarun Yenamandra, Ayush Tewari, Florian Bernard, Hans-\nPeter Seidel, Mohamed Elgharib, Daniel Cremers, and\nChristian Theobalt.\ni3dmm: Deep implicit 3d morphable\nmodel of human heads. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12803\u201312813, 2021. 3\n[43] Yuichi Yoshida and Takeru Miyato. Spectral norm regular-\nization for improving the generalizability of deep learning.\narXiv preprint arXiv:1705.10941, 2017. 4\n[44] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng,\nand Angjoo Kanazawa. Plenoctrees for real-time rendering\nof neural radiance fields. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021. 2\n[45] Yu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma,\nRongfei Jia, and Lin Gao. Nerf-editing: geometry editing of\nneural radiance fields. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18353\u201318364, 2022. 2, 3\n[46] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586\u2013595, 2018. 7, 8\n[47] Yufeng Zheng, Victoria Fern\u00b4andez Abrevaya, Marcel C\nB\u00a8uhler, Xu Chen, Michael J Black, and Otmar Hilliges. Im\navatar: Implicit morphable head avatars from videos. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13545\u201313555, 2022. 2, 3\n[48] Yiyu Zhuang, Hao Zhu, Xusen Sun, and Xun Cao. Mofan-\nerf: Morphable facial neural radiance field. arXiv preprint\narXiv:2112.02308, 2021. 3\n"
  },
  {
    "title": "Diffusion Sampling with Momentum for Mitigating Divergence Artifacts",
    "link": "https://arxiv.org/pdf/2307.11118.pdf",
    "upvote": "7",
    "text": "Diffusion Sampling with Momentum for Mitigating\nDivergence Artifacts\nSuttisak Wizadwongsa\nVISTEC, Thailand\nsuttisak.w_s19@vistec.ac.th\nWorameth Chinchuthakun\nTokyo Institute of Technology, Japan\nchinchuthakun.w.aa@m.titech.ac.jp\nPramook Khungurn\npixiv Inc.\npramook@gmail.com\nAmit Raj\nGoogle\namitrajs@google.com\nSupasorn Suwajanakorn\nVISTEC, Thailand\nsupasorn.s@vistec.ac.th\nAbstract\nDespite the remarkable success of diffusion models in image generation, slow\nsampling remains a persistent issue. To accelerate the sampling process, prior\nstudies have reformulated diffusion sampling as an ODE/SDE and introduced\nhigher-order numerical methods. However, these methods often produce diver-\ngence artifacts, especially with a low number of sampling steps, which limits the\nachievable acceleration. In this paper, we investigate the potential causes of these\nartifacts and suggest that the small stability regions of these methods could be\nthe principal cause. To address this issue, we propose two novel techniques. The\nfirst technique involves the incorporation of Heavy Ball (HB) momentum, a well-\nknown technique for improving optimization, into existing diffusion numerical\nmethods to expand their stability regions. We also prove that the resulting methods\nhave first-order convergence. The second technique, called Generalized Heavy\nBall (GHVB), constructs a new high-order method that offers a variable trade-off\nbetween accuracy and artifact suppression. Experimental results show that our\ntechniques are highly effective in reducing artifacts and improving image quality,\nsurpassing state-of-the-art diffusion solvers on both pixel-based and latent-based\ndiffusion models for low-step sampling. Our research provides novel insights into\nthe design of numerical methods for future diffusion work.\n1\nIntroduction\nDiffusion models [1, 2] are a type of generative models that has garnered considerable attention due\nto their remarkable image quality. Unlike Generative Adversarial Networks (GANs) [3], which may\nsuffer from mode collapse and instabilities during training, diffusion models offer reduced sensitivity\nto hyperparameters [1, 4] and improve sampling quality [5]. Additionally, diffusion models have\nbeen successfully applied to various image-related tasks, such as text-to-image generation [6], image-\nto-image translation [7], image composition [8], adversarial purification [9, 10], super-resolution\n[11], and text-to-audio conversion [12].\nOne significant drawback of diffusion models, however, is their slow sampling speed. This is\nbecause the sampling process involves a Markov chain that requires a large number of iterations to\ngenerate high-quality results. Recent attempts to accelerate the process include improvements to the\nnoise schedule [13, 14] and network distillation [15\u201317]. Fortunately, the sampling process can be\nrepresented by ordinary or stochastic differential equations, and numerical methods can be used to\nreduce the number of iterations required. While DDIM [2], a 1st-order method, is the most commonly\nused approach, it still requires a considerable number of iterations. Higher-order numerical methods,\nPreprint. Under review.\narXiv:2307.11118v1  [cs.CV]  20 Jul 2023\nDPM-Solver++ (2M) [19]\nPLMS4 [20]\n(a)\n(b)\n(c)\n(a)\n(b)\n(c)\nWithout\nMomentum\nWith HB 0.5\n(Ours)\nGHVB 1.5\n(Ours)\ns = 15\ns = 25\ns = 20\ns = 8\ns = 6\ns = 6\nFigure 1: We demonstrate the occurrence of divergence artifacts in DPM-Solver++[19] and\nPLMS4[20] with 15 sampling steps, where s denotes the text-guidance scale. By integrating HB\nmomentum into these methods, we effectively mitigate the artifacts. Additionally, we compare the\nresults with our GHVB 1.5 method. Prompt: \"photo of a girl face\" (a) Realistic Vision v2.0[22], (b)\nAnything Diffusion v4.0[23], (c) Deliberate Diffusion[24]\nsuch as DEIS [18], DPM-Solver [19], and PLMS [20], have been proposed to generate high-quality\nimages in fewer steps. However, these methods begin to produce artifacts (see Figure 1) when the\nnumber of steps is decreased beyond a certain value, thereby limiting how much we can reduce the\nsampling time.\nIn this study, we investigate the potential causes of these artifacts and found that the narrow stability\nregion of high-order numerical methods can cause solutions to diverge, resulting in divergence\nartifacts. To address this issue and enable low-step, artifact-free sampling, we propose two techniques.\nThe first technique involves incorporating Polyak\u2019s Heavy Ball (HB) momentum [21], a well-known\ntechnique for improving optimization, into existing diffusion numerical methods. This approach\neffectively reduces divergence artifacts, but its accuracy only has first order of convergence. In this\ncontext, the accuracy measures how close the approximated, low-step solution is to the solution\ncomputed from a very high-step solver (e.g., 1,000-step DDIM). The second technique, called\nGeneralized Heavy Ball (GHVB), is a new high-order numerical method that offers a variable trade-\noff between accuracy and artifact suppression. Both techniques are training-free and incur negligible\nadditional computational costs. Figure 1 demonstrates the superiority of both techniques in reducing\ndivergence artifacts compared to previous diffusion sampling methods. Furthermore, our experiments\nshow that our techniques are effective on both pixel-based and latent-based diffusion models.\nThe paper is structured as follows. Section 2 covers background and related work on the diffusion\nsampling process in differential equation forms and stability region. Section 3 analyzes visual artifacts\nin diffusion sampling and establishes a connection to the stability region of the solver. Section 4\nproposes a technique to apply momentum to existing numerical methods, as well as a technique\nthat generalizes momentum to high-order numerical methods. Section 5 presents experiments and\nablation studies. Finally, Section 6 concludes and discusses the implications and impacts of our work.\n2\nBackground\nThis section first presents the theoretical foundation of diffusion sampling when modeled as an\nordinary differential equation (ODE) and related numerical methods. Second, we discuss ODE forms\nfor guided diffusion sampling and prior splitting numerical methods. Third, we cover the concept of\nstability region, which is our primary analysis tool.\n2\n2.1\nDiffusion in ODE Form\nModeling diffusion sampling as an ODE is commonly based on the non-Markovian sampling of\nDenoising Diffusion Implicit Model (DDIM) [2]. DDIM is well-known for its simplicity, as it enables\ndeterministic sampling after a random initialization, given by:\nxt\u22121 =\nr\u03b1t\u22121\n\u03b1t\n\u0000xt \u2212\n\u221a\n1 \u2212 \u03b1t\u03f5\u03b8(xt, t)\n\u0001\n+\np\n1 \u2212 \u03b1t\u22121\u03f5\u03b8(xt, t).\n(1)\nHere, \u03f5\u03b8(xt, t) is a neural network that predicts noise, with learnable parameters \u03b8 that take the\ncurrent state xt and time t as input. The parameter \u03b1t is a schedule that controls the degree of\ndiffusion at each time step. Previous research has shown that DDIM 1 can be rewritten into an ODE,\nmaking it possible to use numerical methods to accelerate the sampling process. Two ODEs have\nbeen proposed in the literature:\nd\u00afx\nd\u03c3 = \u00af\u03f5(\u00afx, \u03c3),\n(2)\nd\u02dcx\nd\u02dc\u03c3 = s(\u02dcx, \u02dc\u03c3),\n(3)\nEquation 2 can be obtained by re-parameterizing \u03c3 = \u221a1 \u2212 \u03b1t/\u221a\u03b1t, \u00afx = xt/\u221a\u03b1t, and \u00af\u03f5(\u00afx, \u03c3) =\n\u03f5\u03b8(xt, t). These transformations are widely used in various diffusion solvers [18\u201320, 25]. If \u03f5\u03b8(xt, t)\nis a sum of multiple terms, such as in guided diffusion, we can easily split Equation 2 and solve\neach resulting equation separately, as demonstrated in [26]. Another ODE, given by Equation 3,\ncan be derived by defining \u02dc\u03c3 = \u221a\u03b1t/\u221a1 \u2212 \u03b1t and \u02dcx = xt/\u221a1 \u2212 \u03b1t, where s(\u02dcx, \u02dc\u03c3) = (xt \u2212\n\u221a1 \u2212 \u03b1t\u03f5\u03b8(xt, t))/\u221a\u03b1t, which is an approximation of the final result. This ODE has the advantage\nof keeping the differentiation bounded within the pixel value range in pixel-based diffusion. Recent\nresearch on DPM-Solver++ [27] has shown that Equation 3 outperforms Equation 2 in many cases.\n2.2\nGuided Diffusion Sampling\nGuided diffusion sampling is a widely used technique for conditional sampling, such as text-to-image\nand class-to-image generation. There are main two approaches for guided sampling:\nClassifier guidance [5, 2] uses a pre-trained classifier model p\u03d5(c | xt, t) to define the conditional\nnoise prediction model at inference time:\n\u02c6\u03f5(xt, t | c) = \u03f5\u03b8(xt, t) \u2212 s\u2207 log p\u03b8(c | xt, t),\n(4)\nwhere s > 0 is a \u201cguidance\u201d scale. The model can be extended to accept any guidance function, such\nas CLIP function [28] for text-to-image generation [29]. This approach only modifies the sampling\nequation at inference time and thus can be applied to a trained diffusion model without retraining.\nClassifier-free guidance, proposed by Ho et al. [30], trains a conditional noise model \u03f5\u03b8(xt, t | c) to\ngenerate data samples with the label c:\n\u02c6\u03f5(xt, t | c) = \u03f5\u03b8(xt, t | \u03d5) + s(\u03f5\u03b8(xt, t | c) \u2212 \u03f5\u03b8(xt, t | \u03d5)),\n(5)\nwhere \u03d5 is a null label to allow for unconditional sampling. The sampling equations in both approaches\ncan be expressed as a \u201cguided ODE\u201d of the form\nd\u00afx\nd\u03c3 = \u00af\u03f5(\u00afx, \u03c3) + g(\u00afx, \u03c3),\n(6)\nwhere g(\u00afx, \u03c3) represents a guidance function. To accelerate guided diffusion sampling, splitting\nnumerical methods have been proposed, such as Lie-Trotter Splitting (LTSP) [26]. This method\ndivides Equation 6 into two subproblems, i) dy\nd\u03c3 = \u00af\u03f5(y, \u03c3) and ii) dz\nd\u03c3 = g(z, \u03c3), but can only apply\nhigh-order numerical methods to the first equation while resorting to the Euler method for the second\nequation to avoid numerical instability. Higher-order splitting methods, such as Strang Splitting\n(STSP) [26], are also able to mitigate artifacts. However, these methods require solving the second\nequation twice per step, which is comparable to increasing the total sampling step to avoid artifacts.\nBoth approaches require non-negligible computation.\n2.3\nStability Region\nThe stability region is a fundamental concept in numerical methods for solving ODEs. It determines\nthe step sizes that enable numerical approximations to converge. To illustrate this concept, let us\n3\nconsider the Euler method, a simple, first-order method for solving ODEs, given by\nxn+1 = xn + \u03b4f(xn),\n(7)\nwhere xn is the approximate solution and \u03b4 is the step size. To analyze the stability of the Euler\nmethod, we can consider a test equation of the form x\u2032 = \u03bbx, where \u03bb is a complex constant. The\nsolution of this test equation can be expressed as\nxn+1 = xn + \u03b4\u03bbxn = (1 + \u03b4\u03bb)xn = (1 + \u03b4\u03bb)n+1x0,\n(8)\nwhere x0 is the initial value. For the approximate solution to converge to the true solution, it is\nnecessary that |1 + \u03b4\u03bb| \u2264 1. Hence, the stability region of the Euler method is S = {z \u2208 C :\n|1 + z| \u2264 1} because if z = \u03b4\u03bb lies outside of S, the solution xn will tend to \u00b1\u221e as n \u2192 \u221e.\nIn diffusion sampling, another common numerical solver\nis the Adams-Bashforth (AB) methods, also referred to as\nPseudo Linear Multi-Step (PLMS). AB methods encompass\nthe Euler method as its first-order special case (AB1), and\nthe second-order AB2 is given by:\nxn+1 = xn + \u03b4\n\u00123\n2f(xn) \u2212 1\n2f(xn\u22121)\n\u0013\n.\n(9)\nThe stability regions of AB methods of various orders are\nderived in Appendix A. To visualize these regions, we use\nthe boundary locus technique [31], which determines the\nboundaries of the stability regions as depicted in Figure 2.\nAs the order of the method increases, the stability region\ndecreases in size, and its boundary becomes more restrictive.\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\nAB1 (Euler)\nAB2\nAB3\nAB4\nFigure 2: Boundaries of stability re-\ngions of the first 4 Adams-Bashforth\nmethods.\n3\nUnderstanding Artifacts in Diffusion Sampling\nOne unique issue in diffusion sampling is the occurrence of \u201cdivergence\u201d artifacts, which are\ncharacterized by regions with unrealistic, oversaturated pixels in the output. This problem typically\narises due to several factors, including the use of high-order numerical solvers, too few sampling\nsteps, or a high guidance scale. (See discussion in Appendix K). The current solution is to simply\navoid these factors, albeit at the cost of slower sampling speed or less effective guidance. This section\ninvestigates the source of these artifacts, then we propose solutions that do not sacrifice sampling\nspeed in the next section.\n3.1\nAnalyzing Diffusion Artifacts\nWe analyze the areas where divergence artifacts occur during sampling by examining the magnitudes\nof the latent variables in those areas. Specifically, we use Stable Diffusion [4], which operates and\nperforms diffusion sampling on a latent space of dimension 64 \u00d7 64 \u00d7 4, to generate images with\nand without artifacts by varying the number of steps. Then, we visualize each latent variable z \u2208 R4\nin the 64 \u00d7 64 spatial grid by subtracting the channel-wise mean and dividing by the channel-wise\nstandard deviation, computed from the COCO dataset [32]. Figure 3 shows the magnitudes of the\nnormalized latent variables after max pooling for visualization purposes.\nWe found that artifacts mainly appear in areas where the latent magnitudes are higher than usual.\nNote that images without artifacts can also have high latent magnitudes in some regions, although this\nis very rare. Conversely, when artifacts appear, those regions almost always have high magnitudes. In\npixel-based diffusion models, the artifacts manifest directly as pixel values near 1 or 0 due to clipping,\nwhich can be observed in Figure 16 in Appendix H.\n3.2\nConnection Between ODE Solver and Artifacts\nWe hypothesize that numerical instability during sampling is the cause of these visual artifacts. To\nsee this mathematically, we analyze the ODE for diffusion sampling in Equation 2 using the problem\n4\n(a) PLMS4\n250 steps\n(a) PLMS4\n15 steps\nFigure 3: Comparison of generated images and latent variable magnitudes with and without artifacts,\nobtained using low and high sampling steps. Latent magnitude maps are max-pooled to 16x16, with\nbrighter colors indicating higher values. These results suggest a relationship between artifacts and\nlarge latent magnitudes.\nreduction technique for stiffness analysis [33]. Assuming that the effect of \u03c3 on the function \u00af\u03f5 is\nnegligible, we use Taylor expansion to approximate the RHS of Equation 2, which yields\nd\u00afx\nd\u03c3 = \u2207\u00af\u03f5(x\u2217)(\u00afx \u2212 x\u2217) + O(\u2225\u00afx \u2212 x\u2217\u22252).\n(10)\nHere, x\u2217 denotes the converged solution that should not have any noise left (i.e. \u00af\u03f5(x\u2217) = 0), and\n\u2207\u00af\u03f5(x\u2217) denotes the Jacobian matrix at x\u2217. As \u00afx converges to x\u2217, the term O(\u2225\u00afx \u2212 x\u2217\u22252) becomes\nnegligibly small, so we may drop it from the equation.\nLet \u03bb be an eigenvalue of \u2207\u00af\u03f5(x\u2217)T and v be the corresponding normalized eigenvector such that\n\u2207\u00af\u03f5(x\u2217)T v = \u03bbv. We define u = vT (\u00afx \u2212 x\u2217) and obtain u\u2032 = \u03bbu as our test equation. According\nto Section2.3, if \u03b4\u03bb falls outside the stability region of a numerical method, the numerical solution\nto u may diverge, resulting in diffusion sampling results with larger magnitudes that later manifest\nas divergence artifacts. Therefore, when the stability region is too small, divergence artifacts are\nmore likely to occur. Although some numerical methods have infinite stability regions, those used\nin diffusion sampling have only finite stability regions, which implies that the solution will always\ndiverge if the step size \u03b4 is sufficiently high. More details about the derivation can be found in\nAppendix B and a 2D toy example illustrating this effect is provided in Appendix C.\nOne possible solution to mitigate artifacts is to reduce the step size \u03b4, which shifts \u03b4\u03bb closer to the\norigin of the complex plane. However, this approach increases the number of steps, making the\nprocess slower. Instead, we will modify the numerical methods to enlarge their stability regions.\n4\nMethodology\nThis section describes two techniques for improving stability region and reducing divergence artifacts.\nSpecifically, we first show how to apply Polyak\u2019s Heavy Ball Momentum (HB) to diffusion sampling,\nand secondly, how to generalize HB to higher orders. Our techniques are designed to be simple to\nimplement and do not require additional training.\n4.1\nPolyak\u2019s Heavy Ball Momentum for Diffusion Sampling\nRecall that Polyak\u2019s Heavy Ball Momentum [21] is an optimization algorithm that enhances gradient\ndescent (xn+1 = xn \u2212 \u03b2n\u2207f(xn)). The method takes inspiration from the physical analogy of a\nheavy ball moving through a field of potential with damping friction. The update rule for Polyak\u2019s\nHB optimization algorithm is given by:\nxn+1 = xn + \u03b1n(xn \u2212 xn\u22121) \u2212 \u03b2n\u2207f(xn),\n(11)\n5\nwhere \u03b1n and \u03b2n are parameters. We can apply HB to the Euler method 7, in which case we typically\nset \u03b1n = (1 \u2212 \u03b2n), to obtain\nxn+1 = xn + (1 \u2212 \u03b2n)(xn \u2212 xn\u22121) + \u03b4\u03b2nf(xn),\n(12)\nand we may show that the numerical method above has the same order of convergence as the original\nEuler method. For simplicity, we assume that \u03b2n = \u03b2 \u2208 (0, 1], which is a constant known as the\ndamping coefficient. Then, we can reformulate Equation 12 as:\nvn+1 = (1 \u2212 \u03b2)vn + \u03b2f(xn),\nxn+1 = xn + \u03b4vn+1,\n(13)\nHere, we may interpret xn as the heavy ball\u2019s position, and vn+1\u2014the exponential moving average\nof f(xn)\u2014as its velocity. We can see that position is updated with \u201cdisplacement = time \u00d7 velocity,\u201d\nmuch like in physics.\nConsider a high-order method of the form xn+1 = xn + \u03b4 Pk\ni=0 bif(xn\u2212i). We can apply HB to it\nas follows:\nvn+1 = (1 \u2212 \u03b2)vn + \u03b2\nk\nX\ni=0\nbif(xn\u2212i),\nxn+1 = xn + \u03b4vn+1.\n(14)\nThe resulting numerical method has a larger stability region, as can be seen in Figures 4b to 4d, in\nwhich we show stability boundaries of AB methods after HB is applied to them with varying \u03b2s. (We\nuse HB 0.4 to denote \u03b2 = 0.4). However, Theorem 1 in Appendix F shows that as soon as \u03b2 deviates\nfrom 1, the theoretical order of convergence drops to 1, leading to a significant decrease in image\nquality, as illustrated in Figure 5. In the next subsection, we propose an alternative approach that\nincreases the stability region while maintaining high order of convergence.\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(a) 1st order\n(PLMS1 w/ HB 0.4 - 1)\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(b) 2nd order\n(PLMS2 w/ HB 0.4 - 1)\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(c) 3rd order\n(PLMS3 w/ HB 0.4 - 1)\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(d) 4th order\n(PLMS4 w/ HB 0.4 - 1)\nFigure 4: Boundaries of stability regions of 1st- to 4th-order AB methods with HB applied to them\nwith different values of the damping coefficient \u03b2.\n\u03b2 = 0.2\n\u03b2 = 0.4\n\u03b2 = 0.6\n\u03b2 = 0.8\nPLMS4 (\u03b2 = 1.0)\n(a) PLMS4 with HB\n\u03b2 = 0.2\n\u03b2 = 0.4\n\u03b2 = 0.6\n\u03b2 = 0.8\n1,000-steps DDIM [2],\n(b) 4th order GHVB\nFigure 5: Comparison between the two techniques we propose: (a) HB and (b) GHVB, applied to\nPLMS4 [20] with 15 sampling steps. Both are effective at reducing artifacts, but HB\u2019s accuracy drops\nfaster than GHVB\u2019s as \u03b2 moves away from 1. Positions of the lanterns in Row (a) deviate more from\nthe ground truth (1000 steps DDIM) than those in Row (b). Moreover, the image at \u03b2 = 0.2 in Row\n(a) becomes blurry as HB yields a numerical method with a lower order of convergence than what\nGHVB does. Prompt: \"A beautiful illustration of people releasing lanterns near a river\".\n6\n4.2\nGeneralizing Polyak\u2019s Heavy Ball to Higher Orders\nIn this section, we generalize Euler method with HB momentum to achieve high-order convergence\nin a similar way to how the Adams\u2013Bashforth methods generalize the Euler method. We define the\nbackward difference operator \u2206 as \u2206xn = xn \u2212 xn\u22121. According to [34], we can express the AB\nformula as:\n\u2206xn+1 = \u03b4\n\u0012\n1 + 1\n2\u2206 + 5\n12\u22062 + 3\n8\u22063 + 251\n720\u22064 + 95\n288\u22065 + . . .\n\u0013\nf(xn).\n(15)\nThe order convergence is determined by the number of terms on the RHS. For example, the 2nd-order\nAB method can be written as \u2206xn+1 = \u03b4\n\u00001 + 1\n2\u2206\n\u0001\nf(xn). The update rule for vn in Equation\n13 can be rewritten as (\u03b2 + (1 \u2212 \u03b2)\u2206)vn+1 = \u03b2f(xn). Multiplying both sides of Equation 15 by\n(\u03b2 + (1 \u2212 \u03b2)\u2206), we have:\n(\u03b2 + (1 \u2212 \u03b2)\u2206)\u2206xn+1 = \u03b4\n\u0012\n\u03b2 + 2 \u2212 \u03b2\n2\n\u2206 + 6 \u2212 \u03b2\n12 \u22062 + 10 \u2212 \u03b2\n24\n\u22063 + . . .\n\u0013\nf(xn).\n(16)\nNext, we can choose the order of convergence by fixing the number of terms on the RHS. To get, say,\na 2nd-order method, we may choose:\n(\u03b2 + (1 \u2212 \u03b2)\u2206)\u2206xn+1 = \u03b4\n\u0012\n\u03b2 + 2 \u2212 \u03b2\n2\n\u2206\n\u0013\nf(xn) = \u03b4\n\u0012\n1 + 2 \u2212 \u03b2\n2\u03b2 \u2206\n\u0013\n\u03b2f(xn)\n(17)\n= \u03b4\n\u0012\n1 + 2 \u2212 \u03b2\n2\u03b2 \u2206\n\u0013\n(\u03b2 + (1 \u2212 \u03b2)\u2206)vn+1.\n(18)\nEliminating (\u03b2 \u2212 (1 \u2212 \u03b2)\u2206) from both sides, we obtain the 2nd-order generalized HB method:\nvn+1 = (1 \u2212 \u03b2)vn + \u03b2f(xn),\nxn+1 = xn + \u03b4\n\u00122 + \u03b2\n2\u03b2 vn+1 + 2 \u2212 \u03b2\n2\u03b2 vn\n\u0013\n.\n(19)\nAlgorithm 2 details a complete implementation. When \u03b2 = 1, the formulation in Equation 19 is\nequivalent to the AB2 formulation in Equation 9. As \u03b2 approaches 0, Equation 17 converges to the\n1st-order Euler method 7. Thus, this generalization also serves as an interpolating technique between\ntwo adjacent-order AB methods, except for the 1st-order GHVB, which is equivalent to the Euler\nmethod with HB momentum in Equation 13.\nWe call this new method the Generalized Heavy Ball (GHVB) and associate with it a momentum\nnumber, whose ceiling indicates the method\u2019s order. For example, GHVB 1.8 refers to the 2nd-order\nGHVB with \u03b2 = 0.8. The main difference between HB and GHVB is that HB calculates the moving\naverage after summing high-order coefficients, whereas GHVB calculates it before the summation.\nWe analyze the stability region of GHVB using the same approach as before and visualize the region\u2019s\nlocus curve in Figure 6. The theoretical order of accuracy of this method is given by Theorem 2 in\nAppendix F. We discuss alternative momentum methods, such as Nesterov\u2019s momentum, which can\noffer comparable performance but are less simple in Appendix E.\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(a) 1st order\n(GHVB 0.4 - 1.0)\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(b) 2nd order\n(GHVB 1.4 - 2.0)\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(c) 3rd order\n(GHVB 2.4 - 3.0)\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(d) 4th order\n(GHVB 3.4 - 4.0)\nFigure 6: Boundary of stability regions for 1st-to 4th- order Generalized Heavy Ball Methods (GHVB).\n7\n5\nExperiments\nWe present a series of experiments to evaluate the effectiveness of our techniques. In Section 5.1, we\nassess the reduction of divergence artifacts through qualitative results and quantitative measurements\nof the latent magnitudes in a text-to-image diffusion model. Besides reducing artifacts, another\nimportant goal is to ensure that the overall sampling quality improves and does not degenerate (e.g.,\nbecoming color blobs). We test this with experiments on both pixel-based and latent-based diffusion\nmodels trained on ImageNet@256[35] (Section 5.2 and 5.3), which show that our techniques indeed\nsignificantly improve image quality, as measured by the standard Fr\u00e9chet Inception Distance (FID)\nscore. Lastly, in Section 5.4, we present an ablation study of GHVB methods with varying degrees of\norders. A similar study on HB methods can be found in Appendix M.\n5.1\nArtifacts Mitigation\nIn this experiment, we apply our HB and GHVB techniques to the most popular 2nd and 4th-order\nsolvers, DPM-Solver++ [19] and PLMS4 [20], using 15 sampling steps and various guidance scales\non three different text-to-image diffusion models. The qualitative results in Figure 1 show our\ntechniques significantly reduce the divergence artifacts and produce realistic results (columns a, c).\nMore qualitative results are in Figure 15 in Appendix G.\nQuantitatively measuring divergence artifacts can be challeng-\ning, as metrics like MSE or LPIPS may only capture the dis-\ncrepancy between the approximated and the true solutions,\nwhich does not necessarily indicate the presence of diver-\ngence artifacts. In this study, we use the magnitudes of latent\nvariables as introduced in Section 3.1 as a proxy metric to\nmeasure artifacts. In particular, we define a magnitude score\nv = P\ni,j f(z\u2032\nij) that sums over the latent variables in a max-\npooled latent grid, where f(x) = x if x \u2265 \u03c4 and 0 otherwise.\nWe generate 160 samples from the same set of text prompts\nand seeds for each method from a fine-tuned Stable Diffusion\nmodel called Anything V4 [23].\nThe results using \u03c4 = 3 (magnitude considered high when\nabove 3 std.) are shown in Figures 7 and 8. We observe that\nthe magnitude score increases as the number of sampling steps\ndecreases and higher-order methods result in higher magnitude\nscores. Figure 7 shows that adding HB momentum to PLMS4\n[20] or DPM-Solver++[27] can reduce their magnitude scores,\nwhile Figure 8 shows that GHVB can also reduce the mag-\nnitude scores by reducing the momentum number. Next, we\nshow that our results with less artifacts also have good image\nquality.\n10\n20\n30\n40\n50\n60\nNumber of Steps\n1.0\n1.5\n2.0\n2.5\n3.0\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9 \nPLMS4 w/ HB 0.8 \nDPM-Solver++\nDPM-Solver++ w/ HB 0.9 \nDPM-Solver++ w/ HB 0.8 \nFigure 7: Average magnitude scores\n10\n20\n30\n40\n50\n60\nNumber of Steps\n1.0\n1.5\n2.0\n2.5\n3.0\nMagnitude score\nPLMS4\nGHVB3.7\nGHVB3.3\nGHVB2.7\nGHVB2.3\nDDIM\nGHVB0.7\nFigure 8: Average magnitude scores\n5.2\nExperiments on Pixel-based Diffusion Models\nWe evaluate our techniques using classifier-guided diffusion sampling with ADM [36], an uncon-\nditioned pixel-based diffusion model, with their classifier model. Additionally, we compare our\nmethods with two other diffusion sampling methods, namely DPM-Solver++ [27] and LTSP [26],\nwhich have demonstrated strong performance in classifier-guided sampling.\n8\nFor DPM-Solver++, we use a 2nd-order multi-step method and\ncompare the results with and without HB momentum. For\nLTSP, a split numerical method, we use PLMS4 [20] to solve\nthe first subproblem (see [26]) and compare different methods\nfor solving the second subproblem, including regular Euler\nmethod and Euler method with HB momentum (equivalent to\nGHVB 0.8).\nOur techniques effectively improve FID scores for both DPM-\nSolver++ and LTSP, as shown in Figure 9. Notably, applying\nour HB momentum to LTSP consistently produces the lowest\nFID scores. This experiment highlights the benefits of using\nHB momentum, which provides a better choice than Euler\nmethod. Table 1 presents additional results, and Figure 16\nprovides examples of the generated images.\n10\n14\n18\n22\n26\nNumber of Steps\n10\n20\n30\n40\n50\n60\n70\nFID Score\nDPM-Solver++\nDPM-Solver++ w/ HB 0.8 \nLTSP [PLMS4,PLMS1]\nLTSP [PLMS4, GHVB 0.8] \nFigure 9: FID scores on ADM. (\u2020ours)\n5.3\nExperiment on Latent-based Diffusion Models\nWe evaluate our techniques using classifier-free guidance diffusion sampling with DiT-XL [36], a\npre-trained latent-space diffusion model. In this particular setting, 4th-order solvers, such as PLMS4\n[20], demonstrate superior performance compared to other methods (refer to Appendix I), making it\nour selected method for comparison.\nIn Figure 10, a significant gap in FID scores can be observed\nbetween 4th-order PLMS4 and 1st-order DDIM, but this is\nmostly due to the difference in convergence order rather than\ndivergence artifacts. Our GHVB 3.8 and 3.9 techniques suc-\ncessfully mitigate numerical divergence and lead to improved\nFID scores compared to PLMS4, particularly when the number\nof steps is below 10. Additionally, HB 0.9 also improves FID\nscores. However, using HB 0.8 with PLMS4 can worsen FID\nscores compared to using PLMS4 alone, since the method has\n1st-order convergence, which is the same as DDIM. For high\nsampling steps, both HB and GHVB achieve comparable per-\nformance to PLMS4 without significant degradation of quality.\nWe provide additional results in Table 2, and example images\nin Figure 17.\n6\n8\n12\n16\nNumber of Steps\n6\n8\n10\n12\n14\nFID Score\nDDIM\nPLMS4\nPLMS4 w/ HB 0.8 \nPLMS4 w/ HB 0.9 \nGHVB 3.8 \nGHVB 3.9 \nFigure 10: FID score on DiT. (\u2020ours)\n5.4\nAblation Study of GHVB\nIn this section, we conduct an ablation study of the GHVB method. As explained in Section 4.2,\nthe damping coefficient \u03b2 of GHVB interpolates between two existing AB methods, DDIM and\nPLMS2. Our goal here is to analyze the convergence error of GHVB methods. The comparison is\ndone on Stable Diffusion 1.5 with the target results obtained from a 1,000-step PLMS4 method. We\nmeasure the mean L2 distance between the sampled results and the target results in the latent space.\nThe results in Figure 11 suggest that the convergence error of GHVB 1.1 to GHVB 1.9 interpolates\nbetween the convergence errors of DDIM and PLMS2 accordingly.\nFurthermore, we empirically verify that GHVB does achieve high order of convergence as predicted\nby Theorem 2. We compute the numerical order of convergence using the formula q \u2248 log(enew/eold)\nlog(knew/kold),\nwhere e is the error between the sampled and the target latent codes, and k is the number of sampling\nsteps. As shown in Figure 12, the numerical orders of GHVB 0.5 and GHVB 1.5 approach 0.5 and\n1.5, respectively, as the number of steps increases. However, for GHVB 2.5 and GHVB 3.5, the\nestimated error e may be too small when tested with large numbers of steps, and other sources of\nerror may hinder their convergence. Nonetheless, these GHVB methods can achieve high orders of\nconvergence. A detailed analysis of this and other methods is in Appendix L.\n9\n0\n100\n200\n300\n400\n500\n600\nNumber of Steps\n10\n2\n10\n1\nMean L2 Distance\nDDIM\nGHVB1.1\nGHVB1.3\nGHVB1.5\nGHVB1.7\nGHVB1.9\nPLMS2\nFigure 11: L2 distance in latent space be-\ntween different sampling methods and the\n1,000-step PLMS4 method.\n40\n80\n160\n320\n640\nNumber of Steps (knew)\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nOrder of Convergence (q)\nGHVB0.5\nGHVB1.5\nGHVB2.5\nGHVB3.5\nFigure 12: the numerical order of conver-\ngence for GHVB.\n6\nDiscussion\nThe findings of our study highlight an issue when employing high-order methods for sampling diffu-\nsion models with a low number of steps. This can result in solution divergence and the emergence of\nartifacts. To tackle these challenges, we propose two techniques inspired by Polyak\u2019s HB momentum,\nwhich effectively reduce artifacts while maintaining efficient sampling.\nOur work is closely related to several other approaches aimed at improving the sampling speed of\ndiffusion models. One approach involves training separate models that can be sampled faster, which\ninclude model distillation [37, 15], Schr\u00f6dinger bridge [38], consistency models [17], and GENIE\n[39]. Another approach focuses on creating better samplers, such as high-order numerical methods,\nthat can be applied to existing diffusion models. While some samplers were designed for the SDE\nformulation of diffusion models [40\u201342], most of them deal with the ODE formulation. These include\nlinear multistep methods [20, 27, 18], predictor-corrector methods [43, 44, 25], and splitting methods\n[26]. Our paper specifically proposes new numerical methods for the ODE formulation, but these\nmethods can also be extended to other sampling approaches involving multiple steps, including the\nSDE formulation. These techniques are not mutually exclusive.\nReferences\n[1] Ho, J., A. Jain, P. Abbeel. (2020), Denoising diffusion probabilistic models. In Proceedings of\nthe 34th International Conference on Neural Information Processing Systems, pages 6840\u20136851.\n[2] Song, J., C. Meng, S. Ermon. (2020), Denoising diffusion implicit models. In International\nConference on Learning Representations.\n[3] Goodfellow, I., J. Pouget-Abadie, M. Mirza, et al. (2014), Generative adversarial nets. Advances\nin neural information processing systems, 27.\n[4] Rombach, R., A. Blattmann, D. Lorenz, et al. (2022), High-resolution image synthesis with\nlatent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10684\u201310695.\n[5] Dhariwal, P., A. Nichol. (2021), Diffusion models beat GANs on image synthesis. Advances in\nNeural Information Processing Systems, 34:8780\u20138794.\n[6] Nichol, A. Q., P. Dhariwal, A. Ramesh, et al. (2022), GLIDE: Towards photorealistic image\ngeneration and editing with text-guided diffusion models. In International Conference on\nMachine Learning, pages 16784\u201316804. PMLR.\n[7] Su, X., J. Song, C. Meng, et al. (2022), Dual diffusion implicit bridges for image-to-image\ntranslation. In The Eleventh International Conference on Learning Representations.\n10\n[8] Sasaki, H., C. G. Willcocks, T. P. Breckon. (2021), Unit-DDPM: Unpaired image translation\nwith denoising diffusion probabilistic models. arXiv preprint arXiv:2104.05358.\n[9] Wang, J., Z. Lyu, D. Lin, et al. (2022), Guided diffusion model for adversarial purification.\narXiv preprint arXiv:2205.14969.\n[10] Wu, Q., H. Ye, Y. Gu. (2022), Guided diffusion model for adversarial purification from random\nnoise. arXiv preprint arXiv:2206.10875.\n[11] Choi, J., S. Kim, Y. Jeong, et al. (2021), ILVR: Conditioning method for denoising diffusion\nprobabilistic models. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV),\npages 14347\u201314356. IEEE.\n[12] Ghosal, D., N. Majumder, A. Mehrish, et al. (2023), Text-to-audio generation using instruction-\ntuned LLM and latent diffusion model. arXiv preprint arXiv:2304.13731.\n[13] Nichol, A. Q., P. Dhariwal. (2021), Improved denoising diffusion probabilistic models. In\nInternational Conference on Machine Learning, pages 8162\u20138171. PMLR.\n[14] Watson, D., J. Ho, M. Norouzi, et al. (2021), Learning to efficiently sample from diffusion\nprobabilistic models. arXiv preprint arXiv:2106.03802.\n[15] Salimans, T., J. Ho. (2022), Progressive distillation for fast sampling of diffusion models. In\nInternational Conference on Learning Representations.\n[16] Watson, D., W. Chan, J. Ho, et al. (2022), Learning fast samplers for diffusion models by differ-\nentiating through sample quality. In International Conference on Learning Representations.\n[17] Song, Y., P. Dhariwal, M. Chen, et al. (2023), Consistency models. International Conference\non Learning Representations.\n[18] Zhang, Q., Y. Chen. (2022), Fast sampling of diffusion models with exponential integrator. In\nNeurIPS 2022 Workshop on Score-Based Methods.\n[19] Lu, C., Y. Zhou, F. Bao, et al. (2022), DPM-Solver: A fast ODE solver for diffusion probabilistic\nmodel sampling in around 10 steps. In Advances in Neural Information Processing Systems.\n[20] Liu, L., Y. Ren, Z. Lin, et al. (2022), Pseudo numerical methods for diffusion models on\nmanifolds. In International Conference on Learning Representations.\n[21] Polyak, B. T. (1987), Introduction to optimization. optimization software. Inc., Publications\nDivision, New York, 1:32.\n[22] Realistic vision v2.0. https://huggingface.co/SG161222/Realistic_Vision_V2.0,\n2023.\n[23] Anything diffusion v4.0. https://huggingface.co/andite/anything-v4.0, 2023.\n[24] Deliberate diffuson. https://huggingface.co/XpucT/Deliberate, 2023.\n[25] Zhao, W., L. Bai, Y. Rao, et al. (2023), UniPC: A unified predictor-corrector framework for\nfast sampling of diffusion models. arXiv preprint arXiv:2302.04867.\n[26] Wizadwongsa, S., S. Suwajanakorn. (2023), Accelerating guided diffusion sampling with\nsplitting numerical methods. International Conference on Learning Representations.\n[27] Lu, C., Y. Zhou, F. Bao, et al. (2022), DPM-Solver++: Fast solver for guided sampling of\ndiffusion probabilistic models. arXiv preprint arXiv:2211.01095.\n[28] Radford, A., J. W. Kim, C. Hallacy, et al. (2021), Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning, pages 8748\u2013\n8763. PMLR.\n[29] Letts, A., C. Scalf, A. Spirin, et al. Disco diffusion. https://github.com/alembics/\ndisco-diffusion, 2021.\n11\n[30] Ho, J., T. Salimans. (2021), Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on\nDeep Generative Models and Downstream Applications.\n[31] Lambert, J. D., et al. (1991), Numerical methods for ordinary differential systems, vol. 146.\nWiley New York.\n[32] Lin, T.-Y., M. Maire, S. Belongie, et al. (2014), Microsoft COCO: Common objects in context.\nIn Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September\n6-12, 2014, Proceedings, Part V 13, pages 740\u2013755. Springer.\n[33] Higham, D. J., L. N. Trefethen. (1993), Stiffness of ODEs. BIT Numerical Mathematics,\n33:285\u2013303.\n[34] Berry, M. M., L. M. Healy. (2004), Implementation of Gauss-Jackson integration for orbit\npropagation. The Journal of the Astronautical Sciences.\n[35] Russakovsky, O., J. Deng, H. Su, et al. (2015), ImageNet large scale visual recognition challenge.\nInternational journal of computer vision, 115(3):211\u2013252.\n[36] Peebles, W., S. Xie. (2022), Scalable diffusion models with transformers. arXiv preprint\narXiv:2212.09748.\n[37] Luhman, E., T. Luhman. (2021), Knowledge distillation in iterative generative models for\nimproved sampling speed. arXiv preprint arXiv:2101.02388.\n[38] De Bortoli, V., J. Thornton, J. Heng, et al. (2021), Diffusion Schr\u00f6dinger bridge with applica-\ntions to score-based generative modeling. Advances in Neural Information Processing Systems,\n34:17695\u201317709.\n[39] Dockhorn, T., A. Vahdat, K. Kreis. (2022), GENIE: Higher-order denoising diffusion solvers.\narXiv preprint arXiv:2210.05475.\n[40] Tachibana, H., M. Go, M. Inahara, et al. (2021), It\u00f4-Taylor sampling scheme for denoising\ndiffusion probabilistic models using ideal derivatives. arXiv e-prints, pages arXiv\u20132112.\n[41] Dockhorn, T., A. Vahdat, K. Kreis. (2022), Score-based generative modeling with critically-\ndamped langevin diffusion. International Conference on Learning Representations.\n[42] Song, Y., J. Sohl-Dickstein, D. P. Kingma, et al. (2020), Score-based generative modeling\nthrough stochastic differential equations. In International Conference on Learning Representa-\ntions.\n[43] Karras, T., M. Aittala, T. Aila, et al. (2022), Elucidating the design space of diffusion-based\ngenerative models. In NeurIPS 2022 Workshop on Deep Generative Models and Downstream\nApplications.\n[44] Zhang, Q., M. Tao, Y. Chen. (2023), gDDIM: Generalized denoising diffusion implicit models.\nInternational Conference on Learning Representations.\n[45] Lucas, J., S. Sun, R. Zemel, et al. (2018), Aggregated momentum: Stability through passive\ndamping. arXiv preprint arXiv:1804.00325.\n[46] Nesterov, Y. (1983), A method for unconstrained convex minimization problem with the rate of\nconvergence o(1/k\u02c62). In Doklady an ussr, vol. 269, pages 543\u2013547.\n[47] Kynk\u00e4\u00e4nniemi, T., T. Karras, S. Laine, et al. (2019), Improved precision and recall metric for\nassessing generative models. Advances in Neural Information Processing Systems, 32.\n[48] Zhang, R., P. Isola, A. A. Efros, et al. (2018), The unreasonable effectiveness of deep features\nas a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 586\u2013595.\n12\nPart I\nAppendices\nAppendix contents\nA Stability Region of Adam-Bashforth Method\n13\nA.1 The Boundary Locus Technique\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\nB\nDerivation of Test Equation\n15\nC Toy ODE Problem\n15\nD Implementation Details of PLMS with HB and GHVB Methods\n16\nE Variance Momentum Methods\n17\nF\nOrder of Convergence\n19\nG Qualitative Comparisons\n21\nH Experimental Details and Results of ADM\n21\nI\nExperimental Details and Results of DiT\n24\nJ\nExtended Comparison on Text-to-Image Comparison\n24\nK Factors Contributing to Artifact Occurrence in Fine-tuned Diffusion Models\n26\nL\nElaboration on the Order of Convergence Approximation\n27\nM Ablation Study on HB Momentum\n32\nN Ablation Study on Nesterov Momentum\n33\nO Statistical Reports\n34\nP\nAblation on Magnitude Score\n34\nP.1\nResults with Alternative Parameter Settings . . . . . . . . . . . . . . . . . . . . .\n35\nP.2\nResults on Alternative Models . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n35\nQ Frequently Asked Questions\n35\nA\nStability Region of Adam-Bashforth Method\nTo investigate the stability of the AB2 method, we apply AB2 to the test equation x\u2032 = \u03bbx, which\nwas also used with the Euler method (Section 2.3). We have xn+1 = xn + \u03b4\n\u0000 3\n2\u03bbxn \u2212 1\n2\u03bbxn\u22121\n\u0001\n. To\n13\nsolve this linear recurrence relation, we substitute xn = rn into the formula, where r is a complex\nconstant. Simplifying the resulting equation, we obtain the characteristic equation:\nr2 \u2212\n\u0012\n1 + 3\n2\u03b4\u03bb\n\u0013\nr + 1\n2\u03b4\u03bb = 0,\n(20)\nwhich has the solutions\nr1 = 1\n2\n\uf8eb\n\uf8ed1 + 3\n2\u03b4\u03bb +\ns\u0012\n1 + 3\n2\u03b4\u03bb\n\u00132\n\u2212 2\u03b4\u03bb\n\uf8f6\n\uf8f8 ,\n(21)\nr2 = 1\n2\n\uf8eb\n\uf8ed1 + 3\n2\u03b4\u03bb \u2212\ns\u0012\n1 + 3\n2\u03b4\u03bb\n\u00132\n\u2212 2\u03b4\u03bb\n\uf8f6\n\uf8f8 .\n(22)\nThe general formulation of xn can be expressed as\nxn = a1rn\n1 + a2rn\n2 ,\n(23)\nwhere a1 and a2 are constants. The numerical solution xn tends to 0 as n tends to infinity when both\n|r1| < 1 and |r2| < 1, which means the stability region of AB2 is determined by the complex region\nS =\n\uf8f1\n\uf8f2\n\uf8f3z \u2208 C :\n\f\f\f\f\f\f\n1\n2\n\uf8eb\n\uf8ed1 + 3\n2z \u00b1\ns\u0012\n1 + 3\n2z\n\u00132\n\u2212 2z\n\uf8f6\n\uf8f8\n\f\f\f\f\f\f\n\u2264 1\n\uf8fc\n\uf8fd\n\uf8fe .\n(24)\nSolving for the complex area from the roots of the characteristic equation can pose significant\nchallenges in numerical analysis. One commonly employed graphical technique to visualize the\nstability region is the boundary locus technique [31].\nA.1\nThe Boundary Locus Technique\nThe boundary locus technique [31] begins by defining the shift operator E such that Exk = xk\u22121.\nNote that E2xk = Exk\u22121 = xk\u22122. Generally, a numerical method can be represented in the\nfollowing form:\nA(E)xn = \u03b4B(E)f(xn),\n(25)\nwhere A and B are polynomials of E. For example, in the case of the AB2 method, we have\nA(E) = 1 \u2212 E and B(E) = 3\n2E \u2212 1\n2E2.\nTo determine the stability region of a numerical method, we apply the boundary locus technique to\nthe general form given by Equation 25. The characteristic equation of the method can be obtained by\nsubstituting f(xn) = \u03bbxn (i.e., the test equation) and xn = rn, which yields\nA(r\u22121) = \u03b4\u03bbB(r\u22121),\n(26)\nwhere r is the root of the method\u2019s characteristic equation. The stability region of the method is the\narea in the complex plane where the characteristic root r have modulus less than 1. The boundary\nof the stability region can be determined by substituting r with a modulus of 1 (which means that\nr = ei\u03b8 for some real value \u03b8) into the characteristic equation and solving for z = \u03b4\u03bb. This yields\nthe locus of points in the complex plane where the characteristic roots of the method are on the\nboundary of the stability region. Specifically, we can obtain the curve z = s(\u03b8) = A(e\u2212i\u03b8)/B(e\u2212i\u03b8),\nwhere \u03b8 \u2208 [\u2212\u03c0, \u03c0], that represents the boundary of the stability region in the complex plane. By\ncomparing the stability regions of different numerical methods, we can determine which method is\nmore stable and accurate for a given problem. The boundary locus technique provides a powerful\ntool for analyzing the stability of numerical methods and can help guide the selection of appropriate\nmethods for solving ODE problems.\nExample A.1. (Euler Method) The Euler method, a numerical technique for approximating solutions\nof ODE, can be expressed as:\n(1 \u2212 E)xn = \u03b4Ef(xn)\n(27)\n14\nThe associated polynomials for this method are:\nA(z) = 1 \u2212 z,\nB(z) = z\n(28)\nThe stability region of the Euler method corresponds to the locus curve in which the solution remains\nbounded. This region can be determined by evaluating the complex function:\ns(\u03b8) = A(e\u2212i\u03b8)\nB(e\u2212i\u03b8) = 1 \u2212 e\u2212i\u03b8\ne\u2212i\u03b8\n= ei\u03b8 \u2212 1,\n\u03b8 \u2208 [\u2212\u03c0, \u03c0].\n(29)\nThe locus curve forms a perfect circle with a radius of 1 and a center at -1.\nExample A.2. (AB Methods) The 2nd-order Adams-Bashforth (AB2) method is given by:\n(1 \u2212 E)xn = \u03b4\n\u00123\n2E \u2212 1\n2E2\n\u0013\nf(xn).\n(30)\nThe locus curve representing the stability region of this method is given by:\ns(\u03b8) =\n1 \u2212 e\u2212i\u03b8\n3\n2e\u2212i\u03b8 \u2212 1\n2e\u22122i\u03b8 =\n2(1 \u2212 e\u2212i\u03b8)\n3e\u2212i\u03b8 \u2212 e\u22122i\u03b8 ,\n\u03b8 \u2208 [\u2212\u03c0, \u03c0].\n(31)\nSimilarly, the stability regions for the AB3 and AB4 methods can be obtained by evaluating the\ncomplex functions:\ns(\u03b8) =\n12(1 \u2212 e\u2212i\u03b8)\n23e\u2212i\u03b8 \u2212 16e\u22122i\u03b8 + 5e\u22123i\u03b8 ,\n\u03b8 \u2208 [\u2212\u03c0, \u03c0].\n(32)\ns(\u03b8) =\n24(1 \u2212 e\u2212i\u03b8)\n55e\u2212i\u03b8 \u2212 59e\u22122i\u03b8 + 37e\u22123i\u03b8 \u2212 9e\u22124i\u03b8 ,\n\u03b8 \u2208 [\u2212\u03c0, \u03c0].\n(33)\nThe locus curves for the boundary of stability regions of first four AB methods are visualized in Figure\n2.\nB\nDerivation of Test Equation\nThis section presents the derivation of the test equation u\u2032 = \u03bbu, which serves as a fundamental tool\nfor analyzing the stability of numerical methods in diffusion sampling, as discussed in the Section\n3.2.\nStarting from the differential equation for \u00afx, we have\nd\u00afx\nd\u03c3 = \u2207\u00af\u03f5(x\u2217)(\u00afx \u2212 x\u2217).\n(34)\nWe then define u = vT (\u00afx \u2212 x\u2217), where v is a normalized eigenvector of \u2207\u00af\u03f5(x\u2217)T corresponding to\nthe eigenvalue \u03bb. Taking the derivative of u with respect to \u03c3 and using the chain rule, we have:\ndu\nd\u03c3 = vT d\nd\u03c3 (\u00afx \u2212 x\u2217) = vT [\u2207\u00af\u03f5(x\u2217)](\u00afx \u2212 x\u2217)\n(35)\n= [\u2207\u00af\u03f5(x\u2217)T v]T (\u00afx \u2212 x\u2217) = (\u03bbv)T (\u00afx \u2212 x\u2217)\n(36)\n= \u03bbu\n(37)\nThus, we obtain the test equation u\u2032 = \u03bbu.\nC\nToy ODE Problem\nThis section aims to demonstrate how the solutions yielded by numerical methods can diverge when\nthe stability regions of the methods are too small. Additionally, we illustrate how our momentum-\nbased techniques can enlarge the stability region. The demonstration is conducted on a 2D toy ODE\nproblem given by:\n15\ndx\ndt =\n\u0014\n0\n1\n\u22129\n\u221210\n\u0015 \u0014\nx1\nx2\n\u0015\n,\nx(0) =\n\u0014\n\u22121\n0\n\u0015\n.\n(38)\nThe eigenvalues of the 2 \u00d7 2 matrix are \u22129 and \u22121, and the exact solution of Equation 38 is given by:\nx(t) = 1\n8\n\u0014\n1\n\u22129\n\u0015\ne\u22129t + 9\n8\n\u0014\n\u22121\n1\n\u0015\ne\u2212t.\n(39)\nAs t increases, x(t) converges to the origin.\nLet us say we want to numerically compute x(3) by integrating the ODE for 26 steps with a numerical\nmethod. We divide the time interval [0, 3] into 26 equal intervals, resulting in a step size of \u03b4 = 3/26.\nFor this particular setting, it turns out that the the 2nd-order Adams-Bashforth (AB2) method diverges,\nbut the Euler method converges. To see this, observe that the stability region of the AB2 method only\ncover the interval [\u22121, 0] of the real line, as depicted in Figures 13b and 13d. So, for the eigenvalue\n\u03bb = \u22129, the product \u03b4\u03bb = \u221227/26 lies just outside the region. Consequently, the numerical solution\nyielded by AB2 diverges, as indicated by the blue line in Figures 13a and 13c. In contrast, the Euler\nmethod\u2019s stability region contains both values of \u03b4\u03bb, and the numerical solution, represented by the\ngreen line in Figures 13a and 13c, appears to be more accurate.\nThe AB2 method can be made to more accurately compute x(3) by applying to it any of our proposed\ntechniques: Heavy Ball momentum (HB) and GHVB. The stability regions of the modified AB2\nmethods are given by the red (\u03b2 = 0.8) and yellow (\u03b2 = 0.9) lines in Figures 13b and 13d. Observe\nthat they contains the points associated with the \u03bb\u03b4 values. As a result, the numerical solutions of the\nmodified methods converge to the origin, as demonstrated by the red and yellow lines in Figures 13a\nand 13c, respectively.\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nx1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx2\nexact\nAB1 (Euler)\nAB2\nAB2 + HB 0.8\nAB2 + HB 0.9\n(a) solution\n4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nAB1 (Euler)\nAB2\nAB2 + HB 0.8\nAB2 + HB 0.9\n(b) stability region\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\nx1\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nx2\nexact\nAB1 (Euler)\nAB2\nGHVB 1.8\nGHVB 1.9\n(c) solution\n4.0\n3.5\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nAB1 (Euler)\nAB2\nGHVB 1.8\nGHVB 1.9\n(d) stability region\nFigure 13: Comparison of solution trajectories and stability regions of various numerical methods\nwhen applied to the toy ODE problem. Here, we seek to compute x(3) in 26 steps with the Euler\nmethod, the AB2 method, and methods resulting from modifying AB2 with our momentum-based\ntechniques. Subfigure (a) presents the numerical solutions obtained using our modified AB2 method\nwith HB momentum, while subfigure (c) showcases those obtained using our GHVB. The stability\nregions of the methods are depicted in subfigures (b) and (d) respectively.\nD\nImplementation Details of PLMS with HB and GHVB Methods\nIn this section, we present the complete algorithms for the PLMS method with the HB momentum\nand the GHVB method in Algorithm 1 and Algorithm 2, respectively. Additionally, we include the\nlocus curves that represent the boundaries of the stability regions for each method.\nFor the PLMS method with HB momentum, the locus curves representing the stability regions with\nparameter \u03b2 are given by:\nPLMS1 with HB \u03b2:\ns(\u03b8) = (1 \u2212 e\u2212i\u03b8)(1 \u2212 (1 \u2212 \u03b2)e\u2212i\u03b8)\n\u03b2e\u2212i\u03b8\n(40)\nPLMS2 with HB \u03b2:\ns(\u03b8) = 2(1 \u2212 e\u2212i\u03b8)(1 \u2212 (1 \u2212 \u03b2)e\u2212i\u03b8)\n\u03b2(3e\u2212i\u03b8 \u2212 e\u22122i\u03b8)\n(41)\n16\nPLMS3 with HB \u03b2:\ns(\u03b8) = 12(1 \u2212 e\u2212i\u03b8)(1 \u2212 (1 \u2212 \u03b2)e\u2212i\u03b8)\n\u03b2(23e\u2212i\u03b8 \u2212 16e\u22122i\u03b8 + 5e\u22123i\u03b8)\n(42)\nPLMS4 with HB \u03b2:\ns(\u03b8) =\n24(1 \u2212 e\u2212i\u03b8)(1 \u2212 (1 \u2212 \u03b2)e\u2212i\u03b8)\n\u03b2(55e\u2212i\u03b8 \u2212 59e\u22122i\u03b8 + 37e\u22123i\u03b8 \u2212 9e\u22124i\u03b8)\n(43)\nThe locus curves representing the boundaries of the stability regions for different values of \u03b2 are\nillustrated in Figure 4.\nFor GHVB method, the locus curve representing the stability region with parameter \u03b2 is given by:\n1st-order GHVB (equivalent to PLMS1 with HB):\ns(\u03b8) = (1 \u2212 e\u2212i\u03b8)(1 \u2212 (1 \u2212 \u03b2)e\u2212i\u03b8)\n\u03b2e\u2212i\u03b8\n(44)\n2nd-order GHVB:\ns(\u03b8) = 2(1 \u2212 e\u2212i\u03b8)(1 \u2212 (1 \u2212 \u03b2)e\u2212i\u03b8)\n((2 + \u03b2)e\u2212i\u03b8 \u2212 (2 \u2212 \u03b2)e\u22122i\u03b8)\n(45)\n3rd-order GHVB:\ns(\u03b8) =\n12(1 \u2212 e\u2212i\u03b8)(1 \u2212 (1 \u2212 \u03b2)e\u2212i\u03b8)\n(18 + 5\u03b2)e\u2212i\u03b8 \u2212 (24 \u2212 8\u03b2)e\u22122i\u03b8 + (6 \u2212 \u03b2)e\u22123i\u03b8\n(46)\n4th-order GHVB:\ns(\u03b8) =\n24(1 \u2212 e\u2212i\u03b8)(1 \u2212 (1 \u2212 \u03b2)e\u2212i\u03b8)\n(46 + 9\u03b2)e\u2212i\u03b8 \u2212 (78 \u2212 19\u03b2)e\u22122i\u03b8 + (42 \u2212 5\u03b2)e\u22123i\u03b8 \u2212 (10 \u2212 \u03b2)e\u22124i\u03b8\n(47)\nThese locus curves describe the boundaries of the stability regions and are shown in Figure 6.\nAlgorithm 1: PLMS with HB momentum\ninput: \u00afxn (previous result), \u03b4 (step size),\n{ei}i<n (evaluation buffer), r (method order),\nvn (previous velocity) ;\nen = \u00af\u03f5\u03c3(\u00afxn) ;\nc = min(r, n) ;\nif c == 1 then\n\u02c6e = en ;\nelse if c == 2 then\n\u02c6e = (3en \u2212 en\u22121)/2 ;\nelse if c == 3 then\n\u02c6e = (23en \u2212 16en\u22121 + 5en\u22122)/12 ;\nelse\n\u02c6e = (55en \u2212 59en\u22121 + 37en\u22123 \u2212 9en\u22124)/24 ;\nvn+1 = (1 \u2212 \u03b2)vn + \u03b2\u02c6e;\nResult: \u00afxn + \u03b4vn+1\nE\nVariance Momentum Methods\nIn 2019, a variant of Polyak\u2019s HB momentum called aggregated momentum [45] was proposed.\nIts objective is to enhance stability while also offering convergence advantages. This modification\n17\nAlgorithm 2: GHVB\ninput: \u00afxn (previous result), \u03b4 (step size), \u03b2 (damping parameter)\n{vi}i\u2264n (evaluation buffer), r (method order), ;\nvn+1 = (1 \u2212 \u03b2)vn + \u03b2\u00af\u03f5\u03c3(\u00afxn) ;\nc = min(r, n) ;\nif c == 1 then\n\u02c6e = vn+1 ;\nelse if c == 2 then\n\u02c6e = ((2 + \u03b2)vn+1 \u2212 (2 \u2212 \u03b2)vn)/2\u03b2 ;\nelse if c == 3 then\n\u02c6e = ((18 + 5\u03b2)vn+1 \u2212 (24 \u2212 8\u03b2)vn\n+(6 \u2212 \u03b2)vn+1)/12\u03b2 ;\nelse if c == 4 then\n\u02c6e = ((46 + 9\u03b2)vn+1 \u2212 (78 \u2212 19\u03b2)vn\n+(42 \u2212 5\u03b2)vn\u22121 \u2212 (10 \u2212 \u03b2)vn\u22122)/24\u03b2 ;\nelse\n\u02c6e = ((1650 + 251\u03b2)vn+1 \u2212 (3420 \u2212 646\u03b2)vn\n+(2880 \u2212 264\u03b2)vn\u22121 \u2212 (1380 \u2212 106\u03b2)vn\u22122\n+(270 \u2212 19\u03b2)vn\u22123)/720\u03b2 ;\nResult: \u00afxn + \u03b4\u02c6e\nintroduces multiple velocities, denoted by v(i)\nn , each associated with its specific damping coefficient\n\u03b2(i).\nv(i)\nn+1 = (1 \u2212 \u03b2(i))v(i)\nn + \u03b2(i)f(xn),\nxn+1 = xn + \u03b4\nK\nX\ni=1\nw(i)v(i)\nn+1\n(48)\nNesterov\u2019s momentum [46] is one version of the classic momentum that can also be applied to\ndiffusion sampling processes to improve stability. It can be written as follows:\nyn+1 = xn + \u03b4\u03b2f(xn),\nxn+1 = yn+1 + (1 \u2212 \u03b2)(yn+1 \u2212 yn)\n(49)\nIn fact, Nesterov\u2019s momentum can be obtained from aggregated momentum by considering the\nfollowing:\nv(1)\nn+1 = (1 \u2212 \u03b2)v(1)\nn\n+ \u03b2f(xn),\nv(2)\nn+1 = f(xn),\nxn+1 = xn + \u03b4((1 \u2212 \u03b2)v(1)\nn+1 + \u03b2v(2)\nn+1).\n(50)\nThe stability regions of Nesterov\u2019s momentum when applied to the Euler method and high-order\nAdams-Bashforth methods are illustrated in Figures 14a through 14d. Observe that the stability\nregions of methods with Nesterov\u2019s momentum become larger in a similar manner to those with\nPolyak\u2019s HB momentum. However, the enlargement due to Nesterov\u2019s momentum is more pronounced\nin the vertical direction, while the Polyak\u2019s HB momentum\u2019s enlargement is more horizontal in nature.\n(See Figures 13b and 13d) The differences in the shapes of the stability regions suggest one type of\nmomentum is more suitable to certain ODE problems than the other.\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(a) 1st order\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(b) 2nd order\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(c) 3rd order\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0.0\n1.0\n0.5\n0.0\n0.5\n1.0\n = 0.4\n = 0.6\n = 0.8\n = 1.0\n(d) 4th order\nFigure 14: Comparison of stability regions for different methods with different levels of Nesterov\u2019s\nmomentum.\n18\nWhile generalizing the aggregated momentum method to higher-order methods is possible, it is no\nlonger as straightforward as it is with the HB method. As an example, we will consider the 2nd-order\ngeneralization of Nesterov\u2019s momentum method.\nWe begin by noting that (\u03b2 + (1 \u2212 \u03b2)\u2206)vn+1 = \u03b2f(xn). Our goal is to find polynomials B and C\nsuch that\n\u2206xn+1 = \u03b4(B(\u2206)vn+1 + C(\u2206)f(xn)).\n(51)\nMultiplying both sides of the equation by (\u03b2 + (1 \u2212 \u03b2)\u2206), we get\n(\u03b2 + (1 \u2212 \u03b2)\u2206)\u2206xn+1 = \u03b4(\u03b2B(\u2206)f(xn) + (\u03b2 + (1 \u2212 \u03b2)\u2206)C(\u2206)f(xn)).\n(52)\nReplace the left side with the first two terms from Equation 16, we obtain\n\u03b4\n\u0012\n\u03b2 + 2 + \u03b2\n2\n\u2206\n\u0013\nf(xn) = \u03b4(\u03b2B(\u2206) + (\u03b2 + (1 \u2212 \u03b2)\u2206)C(\u2206))f(xn).\n(53)\nLet B(\u2206) = b0 + b1\u2206 and C(\u2206) = c0. Then, by balancing the coefficients of \u2206 on both sides, we\nhave 1 = b0 + c0 and 2+\u03b2\n2\n= \u03b2b1 + (1 \u2212 \u03b2)c0. We can now write the final formulation as follows:\nxn+1 = xn + \u03b4(b0vn+1 + b1(vn+1 \u2212 vn) + c0f(xn)).\n(54)\nThis suggests that there are countless different ways to expand the stability region of a numerical\nmethod, which offer many new research opportunities.\nF\nOrder of Convergence\nWhen solving ODEs numerically, it is important to consider the accuracy of the method used. One\nway to measure accuracy is by considering the order method\u2019s of convergence of the. Suppose we\nhave a numerical method of the form\nA(E)xn = \u03b4B(E)f(xn),\n(55)\nwhere A(E) = a0 + a1E + a2E2 + ... + asEs and B(E) = b0 + b1E + ... + bsEs. The method is\nsaid to be of pth order if and only if, for all sufficiently smooth functions x, we have that\ns\nX\nm=0\namx(\u03c3 \u2212 m\u03b4) \u2212 \u03b4\ns\nX\nm=0\nbmx\u2032(\u03c3 \u2212 m\u03b4) = O(\u03b4p+1),\n(56)\nwhere x\u2032 denotes the derivative of x.\nTo derive the order of convergence, we use Taylor expansion for both x and x\u2032, yielding\nL.H.S. =\ns\nX\nm=0\nam\n\u221e\nX\nk=0\n(\u2212m\u03b4)k\nk!\nx(k)(\u03c3) \u2212 \u03b4\ns\nX\nm=0\nbm\n\u221e\nX\nk=0\n(\u2212m\u03b4)k\nk!\nx(k+1)(\u03c3)\n=\n\u221e\nX\nk=0\n \ns\nX\nm=0\nam\n(\u2212m\u03b4)k\nk!\n!\nx(k)(\u03c3) \u2212 \u03b4\n\u221e\nX\nk=0\n \ns\nX\nm=0\nbm\n(\u2212m\u03b4)k\nk!\n!\nx(k+1)(\u03c3)\n=\n\u221e\nX\nk=0\n \ns\nX\nm=0\nam\n(\u2212m\u03b4)k\nk!\n!\nx(k)(\u03c3) +\n\u221e\nX\nk=1\n \ns\nX\nm=0\nbm\nmk\u22121(\u2212\u03b4)k\n(k \u2212 1)!\n!\nxk(\u03c3)\n=\ns\nX\nm=0\nam +\n\u221e\nX\nk=1\n \ns\nX\nm=0\nam\nmk\nk! +\ns\nX\nm=0\nbm\nmk\u22121\n(k \u2212 1)!\n!\nxk(\u03c3)(\u2212\u03b4)k\nwhere x(k)(\u03c3) denotes the kth derivative of x evaluated at \u03c3.\nTherefore, the method has order p if and only if the coefficients satisfy the conditions given by\ns\nX\nm=0\nam = 0,\ns\nX\nm=0\nam\nmk\nk! +\ns\nX\nm=0\nbm\nmk\u22121\n(k \u2212 1)! = 0,\nk = 0, 1, ..., p.\n(57)\n19\nNow, we discuss the convergence order of any numerical method after HB momentum is applied to it.\nAn example of such an algorithm is the modified PLMS method, presented in Algorithm 1.\nTheorem 1 (Convergence order of numerical methods with HB momentum). Suppose that a pth-order\nnumerical method has the form xn+1 = xn + \u03b4 Ps\nm=0 bmf(xn\u2212m), where p \u2265 1. The modified\nmethod that uses HB momentum can be expressed as follows:\nvn+1 = (1 \u2212 \u03b2)vn + \u03b2\ns\nX\nm=0\nbmf(xn\u2212m),\n(58)\nxn+1 = xn + \u03b4vn+1.\n(59)\nIt has first-order convergence.\nProof. From the condition given in 57, it follows that Ps\nm=0 bm = 1. We can rewrite these equations\nas:\nxn+1 \u2212 xn \u2212 (1 \u2212 \u03b2)(xn \u2212 xn\u22121) = \u03b4\u03b2\ns\nX\nm=0\nbmf(xn\u2212m).\n(60)\nTo estimate the order of the modified method, we evaluate Equation 60 and obtain:\ns\nX\nm=0\nam = 1 \u2212 1 \u2212 (1 \u2212 \u03b2)(1 \u2212 1) = 0,\n(61)\ns\nX\nm=0\nam\nm1\n1! +\ns\nX\nm=0\nbm\nm0\n0! = 0 \u2212 1 \u2212 (1 \u2212 \u03b2)(1 \u2212 2) + \u03b2\ns\nX\nm=0\nbm\n= \u2212\u03b2 + \u03b2 = 0.\n(62)\nTherefore, we have shown that the modification to the method has first-order convergence.\nNext, we turn our attention to the GHVB method.\nTheorem 2 (Convergence order of the GHVB method). The rth-order GHVB (Algorithm 2) has order\nof convergence of r.\nProof. We will use the 2nd-order method as an example. Using Equation 19, we can write an\nequivalent equation as:\nxn+1 \u2212 xn \u2212 (1 \u2212 \u03b2)(xn \u2212 xn\u22121) = \u03b4\n\u00122 + \u03b2\n2\nf(xn) \u2212 2 \u2212 \u03b2\n2\nf(xn\u22121)\n\u0013\n.\n(63)\nTo estimate the order of the modified method, we evaluate Equation 60 and obtain:\ns\nX\nm=0\nam = 1 \u2212 1 \u2212 (1 \u2212 \u03b2)(1 \u2212 1) = 0,\ns\nX\nm=0\nam\nm1\n1! +\ns\nX\nm=0\nbm\nm0\n0! = 0 \u2212 1 \u2212 (1 \u2212 \u03b2)(1 \u2212 2) +\n\u00122 + \u03b2\n2\n\u2212 2 \u2212 \u03b2\n2\n\u0013\n= 0,\ns\nX\nm=0\nam\nm2\n2! +\ns\nX\nm=0\nbm\nm1\n1! = 1\n2(0 \u2212 12 \u2212 (1 \u2212 \u03b2)(12 \u2212 22)) +\n\u00122 + \u03b2\n2\n11 \u2212 2 \u2212 \u03b2\n2\n21\n\u0013\n= 2 \u2212 3\u03b2\n2\n\u2212 2 \u2212 3\u03b2\n2\n= 0.\nThus, the method has a convergence order of two. Methods of other orders can be dealt with in a\nsimilar fashion.\n20\nG\nQualitative Comparisons\nFigure 1 compares our momentum-based methods, HB and GHVB, with two different diffusion\nsolver methods, DPM-Solver++ [27] and PLMS4 [20], without momentum. The number of sampling\nsteps is held constant while varying the guidance scale s to intentionally induce divergence artifacts.\n(Note that the guidance scales that yield such artifacts are different between diffusion models.) The\nfigure demonstrates that, under the difficult settings of low step counts and high guidance scales\nwhere the baseline methods produce artifacts, our proposed techniques can successfully eliminate\nthem.\nWe present additional qualitative results to show the effect of the damping parameter \u03b2 on the\nquality of images generated by methods modified with HB momentum. We use methods of varying\norders, including DPLM-Solver++[27], UniPC[25], and PLMS4[20]. The diffusion models utilized\nin our analysis are Realistic Vision v2.01, Anything Diffusion v4.02, Counterfeit Diffusion V2.53,\nPastel-Mix4, Deliberate Diffusion5, and Dreamlink Diffusion V1.06. The results are shown in Figure\n15. Notice that stronger momentum (lower \u03b2) leads to fewer and less severe artifacts.\nH\nExperimental Details and Results of ADM\nIn this section, we present additional details and results for the ADM experiment in Section 5.2. The\nprimary objective of this experiment was to provide a quantitative evaluation of class-conditional\ndiffusion sampling in the context of pixel-based images. The experiment was conducted using the\npre-trained diffusion and classifier model at the following link: 7. The implementation used in our\nexperiment was obtained directly from the official DPM-Solver GitHub repository 8.\nTo enhance the capabilities of DPM-Solver++, we incorporated HB momentum into DPM-Solver++\n(just change a few lines of code) and implemented the splitting method LTSP both with and without\nHB momentum into the DPM-Solver code for comparative purposes. The experiment was done on\nfour NVIDIA RTX A4000 GPUs and a 24-core AMD Threadripper 3960x CPU.\nThe results of the experiment, as measured by the full FID score, are presented in Table 1 (as well as\nin Figrue 9). Our technique is highlighted in grey within the table, while the best FID score for each\nnumber of steps is indicated in bold. Additionally, Figure 16 showcases sample images from this\nexperiment. As this experiment uses pixel-based diffusion models, the observed divergence artifacts\ndiffer from those in latent-based diffusion models. Specifically, the artifacts may display excessive\nbrightness or darkness caused by pixel values nearing the maximum or minimum thresholds.\nNumber of Steps\nMethod\n10\n12\n14\n16\n18\n20\n25\n30\nDPM-Solver++\n66.77\n46.77 34.56 26.97 21.87 19.48 16.31 15.63\nDPM-Solver++ w/ HB 0.8\n47.10\n33.65 25.61 21.42 18.94 17.76 15.98 15.53\nLTSP [PLMS4, PLMS1]\n45.32\n34.08 26.58 21.54 18.54 17.15 15.79 15.51\nLTSP [PLMS4, GHVB 0.8]\n37.43\n29.74 23.60 20.23 18.46 17.14 16.07 15.79\nTable 1: FID scores on classifier-guidance ADM models\n1https://huggingface.co/SG161222/Realistic_Vision_V2.0\n2https://huggingface.co/andite/anything-v4.0\n3https://huggingface.co/gsdf/Counterfeit-V2.5\n4https://huggingface.co/andite/pastel-mix\n5https://huggingface.co/XpucT/Deliberate\n6https://huggingface.co/dreamlike-art/dreamlike-diffusion-1.0\n7https://github.com/openai/guided-diffusion\n8https://github.com/LuChengTHU/dpm-solver\n21\nWithout\nMomentum\nWith HB 0.8\n(Ours)\nWith HB 0.5\n(Ours)\nRealistic Vision V2.0. (s = 16)\nAnything V4.0 (s = 20)\nDeliberate (s = 16)\n(a) DPM-Solver++(2M) [27], a 2nd-order method, using 15 steps. Prompt:\"a tiny cute bunny\"\nWithout\nMomentum\nWith HB 0.8\n(Ours)\nWith HB 0.5\n(Ours)\nAnything V4.0 (s = 18)\nPastel-mix V4.0 (s = 14)\nDreamlike V1.0 (s = 10)\n(b) UniPC [25] method, a 3rd-order method, using 8 steps. Prompt: \"a cute kitty \"\nWithout\nMomentum\nWith HB 0.8\n(Ours)\nWith HB 0.5\n(Ours)\nCounterfeit V2.5 (s = 7)\nDreamlike V1.0 (s = 7)\nDeliberate (s = 7)\n(c) PLMS4 [20], a 4th-order method, using 15 steps, Prompt: \"cute humanoid red panda\"\nFigure 15: The impact of different damping coefficients \u03b2 on HB momentum for 2nd-order (a),\n3rd-order (b), and 4th-order (c) numerical methods. Notably, it is observed that incorporating higher\nmomentum values (lower \u03b2) helps mitigate the occurrence of divergence artifacts.\n22\nDPM-Solver++[27]\nDPM-Solver++\nLTSP [26]\nLTSP\nwith HB 0.8\n[PLMS4, PLMS1]\n[PLMS4, GHVB 0.8]\n(Ours)\n(Ours))\nFigure 16: Samples from various sampling methods employing classifier guidance diffusion with a\nguidance scale of 10 and 20 sampling steps.\n23\nI\nExperimental Details and Results of DiT\nThis section presents supplementary details of the DiT experiment conducted in Figure 10 of Section\n5.3 and further investigates the performance of the DiT model [36]. The code implementation and\npre-trained DiT model were obtained directly from the official GitHub repository9. The experiment\nwas done on four NVIDIA GeForce RTX 2080 Ti GPUs and a 24-core AMD Threadripper 3960x\nCPU.\nOur baselines include DDIM [2], DPM-Solver++ [27], LTSP4 [26], and PLMS4 [20]. Based on the\nFID score, PLMS4 emerged as the most effective sampling method within the chosen context. As a\nresult, only PLMS4 was included in Section 5.3 of our main paper. Our variations of the PLMS4\nwith HB 0.8 and 0.9, as well as GHVB 3.8 and 3.9. The results of the experiment are presented in\nTable 2. Our technique is highlighted in grey in the table, and the best FID score for each number of\nsteps is in bold. Notably, our method outperforms the others.\nMoreover, we include the improved Precision and Recall metrics [47] in Tables 3 and 4, respectively,\nwhere higher values indicate superior performance. Additionally, Figure 17 displays sample images\ngenerated from different sampling methods.\nNumber of Steps\nMethod\n6\n7\n8\n9\n10\n15\n20\n25\nDDIM\n55.35 36.97 26.06 19.47 15.02 8.04 6.52 5.94\nDPM-Solver++\n18.60 10.80\n7.93\n6.72\n6.13 5.49 5.30 5.24\nLTSP4 [PLMS4, PLMS1]\n13.33\n9.01\n7.49\n6.55\n6.09 5.32 5.20 5.17\nPLMS4\n13.10\n8.94\n7.31\n6.51\n6.03 5.32 5.21 5.17\nPLMS4 w/ HB 0.8\n14.35\n9.25\n7.46\n6.68\n6.19 5.47 5.29 5.24\nPLMS4 w/ HB 0.9\n11.66\n8.16\n6.69\n6.21\n5.78 5.29 5.19 5.17\nGHVB 3.8\n10.99\n7.93\n6.63\n6.19\n5.80 5.31 5.22 5.18\nGHVB 3.9\n11.67\n8.29\n6.83\n6.30\n5.87 5.31 5.22 5.18\nTable 2: FID scores on DiT-XL\nNumber of Steps\nMethod\n6\n8\n10\n20\nDDIM\n0.36 0.56 0.67 0.79\nDPM-Solver++\n0.63 0.75 0.79 0.81\nLTSP4\n0.67 0.74 0.78 0.81\nPLMS4\n0.68 0.75 0.78 0.81\nPLMS4 w/ HB 0.8 0.68 0.77 0.79 0.81\nPLMS4 w/ HB 0.9 0.70 0.77 0.79 0.81\nGHVB3.8\n0.71 0.77 0.79 0.81\nGHVB3.9\n0.70 0.76 0.78 0.81\nTable 3: Precision on DiT-XL\nNumber of Steps\nMethod\n6\n8\n10\n20\nDDIM\n0.60 0.64 0.65 0.67\nDPM-Solver++\n0.67 0.68 0.68 0.68\nLTSP4\n0.70 0.70 0.69 0.68\nPLMS4\n0.70 0.70 0.69 0.68\nPLMS4 w/ HB 0.8\n0.68 0.68 0.67 0.68\nPLMS4 w/ HB 0.9\n0.69 0.69 0.67 0.68\nGHVB3.8\n0.69 0.70 0.68 0.68\nGHVB3.9\n0.70 0.70 0.69 0.68\nTable 4: Recall on DiT-XL\nJ\nExtended Comparison on Text-to-Image Comparison\nTo provide a more comprehensive evaluation of the methods discussed in Section 5.1, we utilize a\nfine-tuned variant of Stable-Diffusion called Anything V4. We consider full-path samples generated\nby PLMS4 [20] at 1,000 steps as reference solutions. The performance of each method is evaluated\nby measuring the image similarity between the generated samples produced using a reduced number\nof steps and the reference samples. Importantly, both sets of samples originate from identical initial\nnoise maps. This comparison allows us to assess how well the solution from each configuration\nmatches the full-path reference solution.\n9https://github.com/facebookresearch/DiT\n24\nNumber of steps\n6\n8\n10\n15\n25\nPLMS4 [20]\nPLMS4 w/ HB 0.9\nGHVB3.8\nPLMS4 [20]\nPLMS4 w/ HB 0.9\nGHVB3.8\nFigure 17: Comparison of samples generated from DiT-XL with a guidance scale of 3, using different\nsampling methods and sampling steps.\nTo quantify image similarity, we use the Learned Perceptual Image Patch Similarity (LPIPS) [48],\nwhere lower values indicate higher similarity. Additionally, we measure similarity using the L2 norm\nin the latent space, as discussed in Section 4.2, again with lower values indicating higher similarity.\nThe outcomes of these analyses are visually presented in Figure 18.\nDiscrepancies between the numerical solutions and the 1,000-step reference solution can arise from\ntwo primary factors: the accuracy of the employed method and the presence of divergence artifacts.\nNotably, in this particular context, divergence artifacts tend to outweigh errors stemming from method\naccuracy. Consequently, higher-order methods exhibit greater discrepancies in both LPIPS and L2\nsimilarity measurements. It is worth highlighting that our techniques demonstrate a remarkable ability\nto minimize deviations from the reference solution in the majority of cases. Furthermore, Figure 18\nconsistently demonstrates the superiority of our techniques compared to other methods, as evidenced\nby the lower LPIPS and L2 similarity scores. These results indicate that our techniques effectively\nboth reduce divergence artifacts and handle errors related to method accuracy.\n25\n10\n20\n30\n40\n60\nNumber of Steps\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nLPIPS\nPLMS4\nGHVB3.7\nGHVB3.3\nGHVB2.7\nGHVB2.3\n10\n20\n30\n40\n60\nNumber of Steps\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nLPIPS\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n60\nNumber of Steps\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nMean L2 Distance\nPLMS4\nGHVB3.7\nGHVB3.3\nGHVB2.7\nGHVB2.3\n10\n20\n30\n40\n60\nNumber of Steps\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nMean L2 Distance\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\nFigure 18: Comparison of LPIPS in the image space and L2 distance in the latent space across\ndifferent sampling methods, with and without the utilization of our momentum techniques. The\nexperimental setting is similar to that in Section 5.1.\nK\nFactors Contributing to Artifact Occurrence in Fine-tuned Diffusion\nModels\nThis section investigates factors that influence the occurrence of divergence artifacts in diffusion\nsampling, namely the number of steps, guidance scale, and the choice of diffusion models. The\nanalysis includes a qualitative assessment that compares the results obtained from Stable Diffusion\n1.5 (original) (Figure 20) with three fine-tuned diffusion models designed for specific purposes:\ngenerating Midjourney-style images (Figure 22), Japanese animation-style images (Figure 24), and\nphotorealistic images (Figure 26).\nOur observations reveal that several factors contribute to the occurrence of artifacts in diffusion\nsampling, including the number of steps, guidance scale, and choice of diffusion models. Insufficient\nnumbers of steps and high guidance scales positively correlate with the presence of divergence\nartifacts in the generated samples. Fine-tuned models exhibit a higher sensitivity to these factors,\nresulting in a greater incidence of artifacts compared to Stable Diffusion 1.5. Consistent with the\nfindings presented in Section 3.2, reducing the number of steps increases the likelihood of artifact\noccurrence. Furthermore, increasing the guidance scale amplifies the magnitude of eigenvalues,\ncontributing to the presence of artifacts.\nThe choice of diffusion model also has an impact on artifact occurrence. Stable Diffusion 1.5 exhibits\nthe fewest artifacts compared to the fine-tuned models, which demonstrate a higher incidence of\ndivergence artifacts. Among the fine-tuned models, Openjourney demonstrates the lowest occurrence\nof artifacts, while also producing results that look similar to those obtained using the original Stable\nDiffusion 1.5 model. This suggests that extensive changes to the model may alter the eigenvalues and\nresult in an increased presence of artifacts.\nAdditionally, we present the results of our techniques for handling divergence artifacts in Figure 21,\n23, 25, and 27. The choice of the parameter \u03b2 plays a crucial role in achieving a balance between\nreducing artifacts and maintaining accuracy, with its optimal value being influenced by the chosen\nguidance scale and diffusion model.\n10https://huggingface.co/runwayml/stable-diffusion-v1-5\n26\nNumber\nof steps\n9\n12\n15\n18\n21\nPLMS1 (Euler)\nPLMS2\nPLMS3\nPLMS4\nFigure 19: Comparison of samples generated from Stable Diffusion 1.5 10with different sampling\nsteps and method orders. The guidance scale is fixed at 10. Prompt: \"A cat sitting on a window sill\"\nL\nElaboration on the Order of Convergence Approximation\nIn Appendix F, we explored the theoretical aspects of the order of convergence for numerical methods.\nIn this section, we will delve into the estimation of the order of convergence specifically for GHVB\nin Section 4.2.\nTo assess the order of convergence, we focus on the error e, referred to as the global truncation error.\nThis error is quantified by measuring the absolute difference in the latent space between the numerical\nsolution and an approximate exact solution obtained through 1,000-step PLMS4 sampling. The order\nof convergence for a numerical method is defined as q, where the error e follows the relationship\ne = O(\u03b4q), with \u03b4 representing the step size.\nTo estimate the order of convergence practically, we adopt a straightforward approach. It involves\nselecting two distinct step sizes, denoted as \u03b4new and \u03b4old, and computing the corresponding errors\nenew and eold. These errors can be approximated using the following formulas:\nenew \u2248 Cnew(\u03b4new)q,\neold \u2248 Cold(\u03b4old)q\n(64)\nHere, we make the assumption that Cnew is approximately equal to Cold. By taking the ratio of enew\nto eold, we obtain:\n11https://huggingface.co/runwayml/stable-diffusion-v1-5\n12https://huggingface.co/prompthero/openjourney\n13https://huggingface.co/hakurei/waifu-diffusion\n14https://huggingface.co/dreamlike-art/dreamlike-photoreal-2.0\n27\n\"A post-apocalyptic world with ruined\nbuildings, overgrown vegetation, and a red sky\"\n\"A girl standing in a park in\nJapanese animation style\"\ns = 7.5\ns = 15\ns = 22.5\ns = 7.5\ns = 15\ns = 22.5\n10 steps\n15 steps\n20 steps\n40 steps\nFigure 20: Comparison of samples generated from Stable Diffusion 1.5 11using PLMS4 [20] with\ndifferent sampling steps and guidance scale.\n\"A post-apocalyptic world with ruined\nbuildings, overgrown vegetation, and a red sky\"\n\"A girl standing in a park in\nJapanese animation style\"\ns = 7.5\ns = 15\ns = 22.5\ns = 7.5\ns = 15\ns = 22.5\n10 steps\n15 steps\n20 steps\n40 steps\nFigure 21: Comparison of samples generated from Stable Diffusion 1.5 using PLMS4 with HB \u03b2\nunder various sampling steps and guidance scale s. Specifically, we employ \u03b2 = 0.9 for s = 7.5,\n\u03b2 = 0.8 for s = 15, and \u03b2 = 0.6 for s = 22.5 to account for the varying degrees of artifact\nmanifestation associated with each guidance scale.\n28\n\"A post-apocalyptic world with ruined\nbuildings, overgrown vegetation, and a red sky\"\n\"A girl standing in a park in\nJapanese animation style\"\ns = 7.5\ns = 15\ns = 22.5\ns = 7.5\ns = 15\ns = 22.5\n10 steps\n15 steps\n20 steps\n40 steps\nFigure 22: Comparison of samples generated from Openjourney 12using PLMS4 [20] with different\nsampling steps and guidance scale.\n\"A post-apocalyptic world with ruined\nbuildings, overgrown vegetation, and a red sky\"\n\"A girl standing in a park in\nJapanese animation style\"\ns = 7.5\ns = 15\ns = 22.5\ns = 7.5\ns = 15\ns = 22.5\n10 steps\n15 steps\n20 steps\n40 steps\nFigure 23: Comparison of samples generated from Openjourney using PLMS4 with HB \u03b2 under\nvarious sampling steps and guidance scale s. Specifically, we employ \u03b2 = 0.8 for s = 7.5, \u03b2 = 0.6\nfor s = 15, and \u03b2 = 0.6 for s = 22.5 to account for the varying degrees of artifact manifestation\nassociated with each guidance scale.\n29\n\"A post-apocalyptic world with ruined\nbuildings, overgrown vegetation, and a red sky\"\n\"A girl standing in a park in\nJapanese animation style\"\ns = 7.5\ns = 15\ns = 22.5\ns = 7.5\ns = 15\ns = 22.5\n10 steps\n15 steps\n20 steps\n40 steps\nFigure 24: Comparison of samples generated from Waifu Diffusion V1.4 13using PLMS4 [20] with\ndifferent sampling steps and guidance scale.\n\"A post-apocalyptic world with ruined\nbuildings, overgrown vegetation, and a red sky\"\n\"A girl standing in a park in\nJapanese animation style\"\ns = 7.5\ns = 15\ns = 22.5\ns = 7.5\ns = 15\ns = 22.5\n10 steps\n15 steps\n20 steps\n40 steps\nFigure 25: Comparison of samples generated from Waifu Diffusion V1.4 using PLMS4 with HB \u03b2\nunder various sampling steps and guidance scale s. Specifically, we employ \u03b2 = 0.8 for s = 7.5,\n\u03b2 = 0.7 for s = 15, and \u03b2 = 0.6 for s = 22.5 to account for the varying degrees of artifact\nmanifestation associated with each guidance scale.\n30\n\"A post-apocalyptic world with ruined\nbuildings, overgrown vegetation, and a red sky\"\n\"A girl standing in a park in\nJapanese animation style\"\ns = 7.5\ns = 15\ns = 22.5\ns = 7.5\ns = 15\ns = 22.5\n10 steps\n15 steps\n20 steps\n40 steps\nFigure 26: Comparison of samples generated from Dreamlike Photoreal 2.0 14using PLMS4 [20]\nwith different sampling steps and guidance scale.\n\"A post-apocalyptic world with ruined\nbuildings, overgrown vegetation, and a red sky\"\n\"A girl standing in a park in\nJapanese animation style\"\ns = 7.5\ns = 15\ns = 22.5\ns = 7.5\ns = 15\ns = 22.5\n10 steps\n15 steps\n20 steps\n40 steps\nFigure 27: Comparison of samples generated from Dreamlike Photoreal V2.0 using PLMS4 with\nHB \u03b2 under various sampling steps and guidance scale s. Specifically, we employ \u03b2 = 0.7 for\ns = 7.5, \u03b2 = 0.6 for s = 15, and \u03b2 = 0.6 for s = 22.5 to account for the varying degrees of artifact\nmanifestation associated with each guidance scale.\n31\n\"A post-apocalyptic world with ruined\nbuildings, overgrown vegetation, and a red sky\"\n\"A girl standing in a park in\nJapanese animation style\"\ns = 7.5\ns = 15\ns = 22.5\ns = 7.5\ns = 15\ns = 22.5\n10 steps\n15 steps\n20 steps\n40 steps\nFigure 28: Comparison of samples generated from Stable Diffusion 1.5 using GHVB(3.0 + \u03b2) under\nvarious sampling steps and guidance scale s. Specifically, we employ \u03b2 = 0.5 for s = 7.5, \u03b2 = 0.2\nfor s = 15, and \u03b2 = 0.1 for s = 22.5 to account for the varying degrees of artifact manifestation\nassociated with each guidance scale.\nenew\neold\n\u2248\n\u0012\u03b4new\n\u03b4old\n\u0013q\n(65)\nConsequently, we can estimate the order of convergence, denoted as q, by evaluating the logarithmic\nratio of errors and step sizes:\nq \u2248 log(enew/eold)\nlog(\u03b4new/\u03b4old)\n(66)\nIn our investigation of GHVB in Section 4.2, we conducted sampling experiments using 20, 40,\n80, 160, 320, and 640 steps. This choice of an exponential sequence for the number of steps\nwas intentional, as it allowed us to approximate \u03b4new/\u03b4old \u2248 1/2. By doing so, we facilitated the\nestimation process. The results, representing the approximated order of convergence for GHVB, are\nvisually depicted in Figure 12.\nM\nAblation Study on HB Momentum\nIncorporating Polyak\u2019s Heavy Ball (HB) momentum directly into existing diffusion sampling methods\nis a more straightforward approach to mitigating divergence artifacts than GHVB. This can be\nachieved by modifying a few lines of code. In this section, we conduct a comprehensive analysis of\nthe convergence speed of this approach.\nTo evaluate its effectiveness, we generate target results using the 1,000-step PLMS4 method. We\ncompare the target results with those obtained from several methods with and without HB momentum,\nusing LPIPS in the image space and L2 in the latent space. We then estimate their orders of\nconvergence, as explained in Appendix L. The results of this analysis are visually presented in Figure\n29.\n32\n0\n50\n100\n150\n200\n250\n300\nNumber of Steps\n10\n1\nLPIPS\nDDIM\nPLMS2 w/ HB 0.1\nPLMS2 w/ HB 0.3\nPLMS2 w/ HB 0.5\nPLMS2 w/ HB 0.7\nPLMS2 w/ HB 0.9\nPLMS2\n0\n50\n100\n150\n200\n250\n300\nNumber of Steps\n10\n1\n100\nMean L2\nDDIM\nPLMS2 w/ HB 0.1\nPLMS2 w/ HB 0.3\nPLMS2 w/ HB 0.5\nPLMS2 w/ HB 0.7\nPLMS2 w/ HB 0.9\nPLMS2\n40\n80\n160\n320\nNumber of Steps (knew)\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\nOrder of Convergence (q)\nDDIM w/ HB 0.5\nPLMS2 w/ HB 0.5\nPLMS3 w/ HB 0.5\nPLMS4 w/ HB 0.5\nFigure 29: Comparison of LPIPS, mean L2 distance, and order of convergence of HB when using\ndifferent damping coefficients. Statistical means are averaged from 160 initial latent codes.\n0\n50\n100\n150\n200\n250\n300\nNumber of Steps\n10\n1\nLPIPS\nDDIM\nPLMS2 w/ NT 0.1\nPLMS2 w/ NT 0.3\nPLMS2 w/ NT 0.5\nPLMS2 w/ NT 0.7\nPLMS2 w/ NT 0.9\nPLMS2\n0\n50\n100\n150\n200\n250\n300\nNumber of Steps\n10\n1\n100\nMean L2\nDDIM\nPLMS2 w/ NT 0.1\nPLMS2 w/ NT 0.3\nPLMS2 w/ NT 0.5\nPLMS2 w/ NT 0.7\nPLMS2 w/ NT 0.9\nPLMS2\n40\n80\n160\n320\nNumber of Steps (knew)\n0.2\n0.3\n0.4\n0.5\n0.6\nOrder of Convergence (q)\nDDIM w/ NT 0.5\nPLMS2 w/ NT 0.5\nPLMS3 w/ NT 0.5\nPLMS4 w/ NT 0.5\nFigure 30: Comparison of LPIPS, mean L2 distance, and order of convergence of Nesterov\u2019s\nmomentum when using different damping coefficients. Statistical means are averaged from 160 initial\nlatent codes.\nIn contrast to the interpolation-like behavior observed in Figure 12 for GHVB, we observe that the\nuse of HB momentum leads to an increase in both the LPIPS score and the L2 distance when selecting\nvalues of \u03b2 that are less than 1. This is even worse than the 1st-order method DDIM when \u03b2 is\nbelow 0.7. These findings indicate a deviation from the desired convergence behavior, highlighting a\npotential decrease in solution accuracy, even though HB momentum has been shown to successfully\nmitigate divergence artifacts.\nAdditionally, we find that the numerical orders of convergence also tend to approach the same value.\nThese observations align with our analysis in Theorem 1 of Appendix F, indicating that when \u03b2\ndeviates from 1, the employed approach exhibits 1st-order convergence and is unable to achieve\nhigh-order convergence. These conclusions emphasize the importance of carefully considering the\nchoice of \u03b2 in order to strike a balance between convergence speed and solution quality. Further\ndetails and insights into the performance of the HB momentum approach can be obtained from Figure\n29, enhancing our understanding of its behavior within the context of the studied problem.\nN\nAblation Study on Nesterov Momentum\nIn Appendix E, we investigated the potential of incorporating different types of momentum, such\nas Nesterov\u2019s momentum, into existing diffusion sampling methods to mitigate divergence artifacts.\nSimilar to the analysis conducted in Section 5.4 and Appendix M, the primary objective of this section\nis to explore the convergence speed of Nesterov\u2019s momentum by comparing two key metrics: LPIPS\nin the image space and L2 in the latent space.\nFigure 30 presents the results, which reveal intriguing parallels with the behavior of HB momentum\nobserved in Figure 29. When Nesterov\u2019s momentum is applied to the PLMS2 method, the accuracy\nof the model progressively diminishes as the value of \u03b2 deviates from 1, as indicated by the corre-\nsponding increase in both LPIPS and L2 metrics. Notably, the model\u2019s accuracy drops below that of\nthe DDIM when \u03b2 falls below 0.5.\nFurthermore, our analysis of the order of convergence demonstrates that Nesterov\u2019s momentum does\nnot achieve a high order of convergence, similar to HB momentum. These findings emphasize the\nimportance of carefully considering the choice of momentum method, along with the specific values\nassigned to \u03b2, in order to strike an optimal balance between convergence speed and solution quality.\n33\nNumber of steps\nMethod\n10\n15\n20\n25\n30\n60\nDPM\n1.113 \u00b1 .090 1.369 \u00b1 .102 1.087 \u00b1 .083 0.972 \u00b1 .075 0.919 \u00b1 .068 0.869 \u00b1 .067\nDPM w/ HB 0.8\n0.974 \u00b1 .078 1.057 \u00b1 .079 0.916 \u00b1 .072 0.857 \u00b1 .070 0.831 \u00b1 .067 0.834 \u00b1 .065\nDPM w/ HB 0.9\n1.043 \u00b1 .082 1.186 \u00b1 .088 0.986 \u00b1 .075 0.921 \u00b1 .073 0.867 \u00b1 .068 0.844 \u00b1 .065\nPLMS4 w/ HB 0.8 1.958 \u00b1 .116 1.469 \u00b1 .105 1.213 \u00b1 .097 1.060 \u00b1 .087 0.963 \u00b1 .076 0.838 \u00b1 .063\nPLMS4 w/ HB 0.9 2.499 \u00b1 .118 1.888 \u00b1 .112 1.534 \u00b1 .112 1.270 \u00b1 .104 1.116 \u00b1 .091 0.887 \u00b1 .066\nPLMS4\n3.149 \u00b1 .116 2.460 \u00b1 .116 1.911 \u00b1 .115 1.597 \u00b1 .116 1.372 \u00b1 .106 0.957 \u00b1 .075\nTable 5: 95% confidence intervals for the magnitude scores of HB (Figure 7)\nNumber of steps\nMethod\n10\n15\n20\n25\n30\n60\nDDIM\n0.844 \u00b1 .076 0.765 \u00b1 .064 0.728 \u00b1 .060 0.744 \u00b1 .063 0.761 \u00b1 .062 0.778 \u00b1 .062\nGHVB2.1 1.238 \u00b1 .097 0.924 \u00b1 .072 0.832 \u00b1 .067 0.820 \u00b1 .066 0.825 \u00b1 .066 0.829 \u00b1 .064\nGHVB2.3 1.291 \u00b1 .102 0.952 \u00b1 .072 0.872 \u00b1 .070 0.836 \u00b1 .064 0.831 \u00b1 .066 0.828 \u00b1 .062\nGHVB2.5 1.392 \u00b1 .103 1.016 \u00b1 .081 0.907 \u00b1 .071 0.851 \u00b1 .069 0.842 \u00b1 .065 0.845 \u00b1 .063\nGHVB2.7 1.514 \u00b1 .105 1.095 \u00b1 .085 0.968 \u00b1 .077 0.877 \u00b1 .068 0.864 \u00b1 .067 0.826 \u00b1 .063\nGHVB2.9 1.673 \u00b1 .107 1.203 \u00b1 .090 1.023 \u00b1 .079 0.934 \u00b1 .075 0.901 \u00b1 .071 0.835 \u00b1 .063\nPLMS4\n3.149 \u00b1 .116 2.460 \u00b1 .116 1.911 \u00b1 .115 1.597 \u00b1 .116 1.372 \u00b1 .106 0.957 \u00b1 .075\nTable 6: 95% confidence intervals for the magnitude scores of GHVB (Figure 8)\nO\nStatistical Reports\nIn this section, we present detailed statistical reports for the experiments conducted in Section\n5.1 and Section 4.2. These reports provide detailed information, including mean values and their\ncorresponding 95% confidence intervals, to offer a thorough understanding of the experimental\nresults.\nFirstly, we focus on the experiment related to mitigating the magnitude score in Section 5.1. The\nresults depicted in Figure 7 are presented in Table 5. Additionally, the outcomes illustrated in Figure\n8 are reported in Table 6. For the ablation study of GHVB in Section 4.2, we provide the results\nshown in Figure 11 in Table 7. Similarly, the findings presented in Figure 12 are reported in Table 8.\nFurthermore, we include a runtime comparison of each sampling method in Table 9, detailing the\nwall clock time required for each method. The results indicate that all the methods exhibit similar\nsampling times, ensuring a fair comparison across the different approaches.\nP\nAblation on Magnitude Score\nIn this section, our objective is to provide further verification and justification of the experiment\nconducted in Section 5.1 by exploring various parameter settings for the magnitude score and\nassessing their effects on the selected model.\nNumber of steps\nMethod\n10\n20\n40\n80\n160\n320\n640\nDDIM\n0.584 \u00b1 .034 0.409 \u00b1 .029 0.304 \u00b1 .029 0.210 \u00b1 .026 0.139 \u00b1 .022 0.085 \u00b1 .014 0.048 \u00b1 .009\nGHVB1.1 0.592 \u00b1 .034 0.406 \u00b1 .029 0.295 \u00b1 .030 0.189 \u00b1 .026 0.113 \u00b1 .020 0.054 \u00b1 .010 0.019 \u00b1 .005\nGHVB1.3 0.609 \u00b1 .035 0.410 \u00b1 .029 0.276 \u00b1 .029 0.158 \u00b1 .023 0.086 \u00b1 .017 0.030 \u00b1 .007 0.009 \u00b1 .003\nGHVB1.5 0.624 \u00b1 .036 0.409 \u00b1 .029 0.261 \u00b1 .029 0.145 \u00b1 .023 0.067 \u00b1 .014 0.021 \u00b1 .005 0.006 \u00b1 .002\nGHVB1.7 0.645 \u00b1 .037 0.411 \u00b1 .030 0.254 \u00b1 .028 0.133 \u00b1 .023 0.053 \u00b1 .011 0.016 \u00b1 .005 0.004 \u00b1 .002\nGHVB1.9 0.663 \u00b1 .037 0.414 \u00b1 .030 0.246 \u00b1 .028 0.123 \u00b1 .021 0.044 \u00b1 .009 0.013 \u00b1 .004 0.003 \u00b1 .001\nPLMS2\n0.676 \u00b1 .038 0.418 \u00b1 .030 0.246 \u00b1 .028 0.119 \u00b1 .021 0.041 \u00b1 .009 0.011 \u00b1 .003 0.003 \u00b1 .001\nTable 7: 95% confidence intervals for L2 norm of GHVB (Figrue 11)\n34\nNumber of steps (knew)\nMethod\n40\n80\n160\n320\n640\nGHVB0.5 0.247 \u00b1 .030 0.235 \u00b1 .029 0.351 \u00b1 .045 0.450 \u00b1 .057 0.474 \u00b1 .057\nGHVB1.5 0.550 \u00b1 .072 0.717 \u00b1 .086 0.922 \u00b1 .089 1.337 \u00b1 .102 1.519 \u00b1 .102\nGHVB2.5 0.624 \u00b1 .077 1.121 \u00b1 .115 1.546 \u00b1 .132 1.906 \u00b1 .153 1.846 \u00b1 .147\nGHVB3.5 0.459 \u00b1 .063 0.920 \u00b1 .107 1.877 \u00b1 .170 1.960 \u00b1 .147 1.779 \u00b1 .163\nTable 8: 95% confidence intervals for the numerical orders of convergence of GHVB (Figure 12)\nNumber of steps\nMethod\n15\n30\n60\nDPM-Solver++\n2.49 4.84 9.54\nDPM-Solver++ w/ HB 0.9 2.49 4.84 9.54\nPLMS4\n2.49 4.84 9.54\nPLMS4 w/ HB 0.9\n2.46 4.79 9.43\nPLMS4 w/ NT 0.9\n2.53 4.93 9.70\nGHVB3.9\n2.50 4.84 9.54\nTable 9: Comparison of the average sampling time per image (in seconds) when using different\nnumbers of steps in Stable Diffusion 1.5 on an NVIDIA GeForce RTX 3080. The time differences\nare marginal.\nP.1\nResults with Alternative Parameter Settings\nTo gain deeper insights into the integration of momentum into sampling methods, we analyze the\nresults of the magnitude scores depicted in Figure 7 (Section 5.1). This analysis involves varying\nthe threshold \u03c4 and the kernel size k for max-pooling in the calculation of the magnitude score. By\ninvestigating different parameter settings, we aim to validate the outcomes of the experiment and\nuncover the scaling impact of the magnitude score. The results, shown in Figure 32, highlight the\ninfluence of threshold \u03c4 and kernel size k on the magnitude score. It is important to note that while\nextreme values of \u03c4 or k may introduce ambiguity in interpreting the outcomes, the overall observed\ntrends remain consistent.\nP.2\nResults on Alternative Models\nIn this section, we present the findings from our analysis conducted on alternative diffusion models,\nnamely Stable Diffusion 1.5, Waifu Diffusion V1.4, and Dreamlike Photoreal 2.0. The primary aim\nof this investigation is to assess the impact of different models on the magnitude score and determine\nwhether the trends identified in Section 5.1 hold across diverse model architectures.\nFor this analysis, we employed the same magnitude score parameters as in Section 5.1. The results\nof our examination are illustrated in Figure 33, which showcases the magnitude scores for each\nmodel. One important observation is that the change in model architecture only affects the scale of\nthe magnitude score, while the overall trend remains consistent across all models.\nQ\nFrequently Asked Questions\nQ: What does the term \u201cdivergence artifacts\u201d refer to?\nIn this paper, the term \u201cdivergence artifacts\u201d is used to describe visual anomalies that occur when the\nnumerical solution diverges, resulting in unusually large magnitudes of the results. In the context of\nlatent-based diffusion, we specifically define divergence artifacts as visual artifacts caused by latent\ncodes with magnitudes that exceed the usual range. These artifacts commonly arise when the stability\nregion of the numerical method fails to handle all eigenvalues of the system, leading to a divergent\nnumerical solution. To visually demonstrate the presence of divergence artifacts, we have included\nFigure 34. This illustration showcases the process of starting with diffusion results and subsequently\nscaling the latent code within a 4 \u00d7 4 square located at the center of the latent image. This scaling\nis achieved by multiplying the latent code with a constant factor. As a result of this manipulation,\ndivergence artifacts become distinctly visible, particularly at the center of the resulting image. This\n35\nillustration provides a clear representation of the impact that scaling the latent code can have on the\noccurrence of divergence artifacts.\nQ: Can we directly interpolate two existing numerical methods instead of using the GHVB\nmethod?\nIndeed, this is possible. However, the order of the resulting method will be the lowest order of the\ntwo methods. To illustrate this point, let us consider a direct interpolation between the 1st-order Euler\nmethod (AB1) and the 2nd-order Adams-Bashford method (AB2), expressed as follows:\nxn+1 = xn + \u03b4\n\u0012\n(1 \u2212 \u03b2)f(xn) + \u03b2 3\n2f(xn) \u2212 \u03b2 1\n2f(xn\u22121)\n\u0013\n(67)\nAs outlined in Appendix F, despite the orders of the interpolated methods, the resulting method is a\n1st-order method.\n36\n\u03b2 = 0.2\n\u03b2 = 0.4\n\u03b2 = 0.6\n\u03b2 = 0.8\n\u03b2 = 1.0\n(a) w/ HB\n\u03b2 = 0.2\n\u03b2 = 0.4\n\u03b2 = 0.6\n\u03b2 = 0.8\n(b) w/ NT\n(a) Prompt: \"A beautiful illustration of a schoolgirl riding her bicycle to school in a small village\"\n\u03b2 = 0.2\n\u03b2 = 0.4\n\u03b2 = 0.6\n\u03b2 = 0.8\n\u03b2 = 1.0\n(a) w/ HB\n\u03b2 = 0.2\n\u03b2 = 0.4\n\u03b2 = 0.6\n\u03b2 = 0.8\n(b) w/ NT\n(b) Prompt: \"A beautiful illustration of a cozy cafe in a futuristic city\"\n\u03b2 = 0.2\n\u03b2 = 0.4\n\u03b2 = 0.6\n\u03b2 = 0.8\n\u03b2 = 1.0\n(a) w/ HB\n\u03b2 = 0.2\n\u03b2 = 0.4\n\u03b2 = 0.6\n\u03b2 = 0.8\n(b) w/ NT\n(c) Prompt: \"A painting of an old European castle in a deep forest with a Blood Moon in the background\"\nFigure 31: Comparison between two variations of momentum: (a) Polyak\u2019s Heavy Ball (HB) and\n(b) Nesterov (NT). These momentum variations are applied to PLMS4 [20] on a fine-tuned Stable\nDiffusion model called Anything V4 [23] with 15 sampling steps and a guidance scale of 15. Both\nvariations effectively reduce artifacts. However, the choice of the effectiveness parameter \u03b2 might\ndiffer due to the distinct shapes of their respective stability regions.\n37\nThreshold\n\u03c4 = 0\n\u03c4 = 1.5\n\u03c4 = 3.0\n\u03c4 = 10.0\nkernel size\nk = 1\n10\n20\n30\n40\n50\n60\nNumber of Steps\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n1.2\n1.4\n1.6\n1.8\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n0.25\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nMagnitude score\n1e\n3\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\nkernel size\nk = 4\n10\n20\n30\n40\n50\n60\nNumber of Steps\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n2.4\n2.6\n2.8\n3.0\n3.2\n3.4\n3.6\n3.8\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n1.0\n1.5\n2.0\n2.5\n3.0\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\nMagnitude score\n1e\n2\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\nkernel size\nk = 64\n10\n20\n30\n40\n50\n60\nNumber of Steps\n6.0\n6.5\n7.0\n7.5\n8.0\n8.5\n9.0\n9.5\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n6.0\n6.5\n7.0\n7.5\n8.0\n8.5\n9.0\n9.5\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n6.0\n6.5\n7.0\n7.5\n8.0\n8.5\n9.0\n9.5\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nMagnitude score\nPLMS4\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\nFigure 32: Comparison of magnitude scores on Anything V4 on different combinations of threshold\n\u03c4 and kernel size k used in max-pooling. #samples = 160\nStable Diffusion 1.5\nWaifu Diffusion V1.4\nDreamlike Photoreal 2.0\n10\n20\n30\n40\n50\n60\nNumber of Steps\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nMagnitude score\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nPLMS4\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nMagnitude score\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nPLMS4\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\nMagnitude score\nPLMS4 w/ HB 0.9\nPLMS4 w/ HB 0.8\nPLMS4\nDPM-Solver++\nDPM-Solver++ w/ HB 0.9\nDPM-Solver++ w/ HB 0.8\n10\n20\n30\n40\n50\n60\nNumber of Steps\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\n2.25\nMagnitude score\nPLMS4\nGHVB3.7\nGHVB3.3\nGHVB2.7\nGHVB2.3\nDDIM\nGHVB0.7\n10\n20\n30\n40\n50\n60\nNumber of Steps\n0.50\n0.75\n1.00\n1.25\n1.50\n1.75\n2.00\nMagnitude score\nPLMS4\nGHVB3.7\nGHVB3.3\nGHVB2.7\nGHVB2.3\nDDIM\nGHVB0.7\n10\n20\n30\n40\n50\n60\nNumber of Steps\n1.0\n1.5\n2.0\n2.5\nMagnitude score\nPLMS4\nGHVB3.7\nGHVB3.3\nGHVB2.7\nGHVB2.3\nDDIM\nGHVB0.7\nFigure 33: Comparison of magnitude scores in different diffusion models. #samples = 160\n38\nScaling factor\nOriginal (no scaling)\n\u00d73.0\n\u00d76.0\nStable Diffusion 1.5\nAnything Diffusion V4.0\nPastel-mix Diffusion V4.0\nFigure 34: Visualization of divergence artifacts through latent code scaling. The figure illustrates the\ndiffusion results obtained by scaling the latent code within a 4 \u00d7 4 square positioned at the center\nof the latent image. This scaling process involves multiplying the latent code with a constant factor.\nAs a consequence, the resulting image showcases the emergence of divergence artifacts, which are\nparticularly prominent at the center. The samples presented in this figure were generated using\nPLMS4 [20] with a guidance scale of 15 and 250 sampling steps. Prompt: \"A beautiful illustration of\na couple looking at fireworks in a summer festival in Japan\"\n39\n"
  }
]