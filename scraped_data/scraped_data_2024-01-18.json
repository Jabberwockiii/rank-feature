[
  {
    "title": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model",
    "link": "https://arxiv.org/pdf/2401.09417.pdf",
    "upvote": "46",
    "text": "Vision Mamba: Efficient Visual Representation Learning with Bidirectional\nState Space Model\nLianghui Zhu1\u2217, Bencheng Liao1\u2217, Qian Zhang2, Xinlong Wang3, Wenyu Liu1, Xinggang Wang1 \u00001 Huazhong University of Science and Technology\n2 Horizon Robotics\n3 Beijing Academy of Artificial Intelligence\nCode & Models: hustvl/Vim\n42\n43\n44\n45\n46\nDetection\nmAP (%)\n36\n37\n38\n39\n40\nIns. Seg.\nmAP (%)\n71\n73\n75\n77\nClassification\nTop-1 Acc. (%)\n38\n39\n40\n41\nSem. Seg.\nmIoU (%)\n(a) Accuracy Comparison\n1\n1.4\n1.8\n2.2\n2.6\n512\n640\n738\n1024\n1248\nFPS w/ log scale\nResolution\nDeiT-Ti\nVim-Ti\n2.54\n2.25\n2.05\n1.57\n1.26\n2.29\n2.07\n1.91\n1.71\n(b) Speed Comparison\n0\n20\n40\n60\n80\n512\n640\n738\n1024\n1248\nGPU Memory (GB)\nResolution\nDeiT-Ti\nVim-Ti\n4.56\n4.22\n12.48\n8.13\n11.14\n8.09\n5.03\n40.09\nOOM\n(c) GPU Memory Comparison\n3.32\nDeiT-Ti\nVim-Ti\nFaster\nSmaller\n2.8\u00d7 faster\n-86.8% memory\nFigure 1. Performance and efficiency comparisons between DeiT [59] and our Vim model. For the accuracy comparison, we first pretrain\nDeiT and Vim on IN1K classification dataset [9], then we finetune the generic backbones on different downstream dense prediction tasks,\ni.e., semantic segmentation, object detection, instance segmentation. Results show that the proposed Vim outperforms DeiT on both\npretraining and finetuning tasks. Vim is also more computation and memory efficient than DeiT in dealing with high-resolution images.\nFor example, Vim is 2.8\u00d7 faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images\nwith a resolution of 1248\u00d71248, i.e., 6084 tokens per image.\nAbstract\nRecently the state space models (SSMs) with efficient\nhardware-aware designs, i.e., the Mamba deep learning\nmodel, have shown great potential for long sequence model-\ning. Meanwhile building efficient and generic vision back-\nbones purely upon SSMs is an appealing direction. How-\never, representing visual data is challenging for SSMs due\nto the position-sensitivity of visual data and the require-\nment of global context for visual understanding.\nIn this\npaper, we show that the reliance on self-attention for vi-\nsual representation learning is not necessary and propose\na new generic vision backbone with bidirectional Mamba\nblocks (Vim), which marks the image sequences with po-\nsition embeddings and compresses the visual representa-\ntion with bidirectional state space models. On ImageNet\nclassification, COCO object detection, and ADE20k seman-\ntic segmentation tasks, Vim achieves higher performance\ncompared to well-established vision transformers like DeiT,\nwhile also demonstrating significantly improved computa-\n\u2217 Lianghui Zhu and Bencheng Liao contributed equally to this work.\n\u0000 Corresponding author: Xinggang Wang (xgwang@hust.edu.cn).\ntion & memory efficiency. For example, Vim is 2.8\u00d7 faster\nthan DeiT and saves 86.8% GPU memory when performing\nbatch inference to extract features on images with a res-\nolution of 1248\u00d71248. The results demonstrate that Vim\nis capable of overcoming the computation & memory con-\nstraints on performing Transformer-style understanding for\nhigh-resolution images and it has great potential to be the\nnext-generation backbone for vision foundation models.\n1. Introduction\nRecent research advancements have led to a surge of inter-\nest in the state space model (SSM). Originating from the\nclassic Kalman filter model [29], modern SSMs excel at\ncapturing long-range dependencies and benefit from paral-\nlel training. Some SSM-based methods, such as the linear\nstate-space layers (LSSL) [21], structured state space se-\nquence model (S4) [20], diagonal state space (DSS) [23],\nand S4D [22], are proposed to process sequence data across\na wide range of tasks and modalities, particularly on mod-\neling long-range dependencies. They are efficient in pro-\ncessing long sequences because of convolutional computa-\n1\narXiv:2401.09417v2  [cs.CV]  10 Feb 2024\ntion and near-linear computation. 2-D SSM [2], SGCon-\nvNeXt [36], and ConvSSM [51] combine SSM with CNN\nor Transformer architecture to process 2-D data. The recent\nwork, Mamba [19], incorporates time-varying parameters\ninto the SSM and proposes a hardware-aware algorithm to\nenable very efficient training and inference. The superior\nscaling performance of Mamba indicates that it is a promis-\ning alternative to Transformer in language modeling. Nev-\nertheless, a generic pure-SSM-based backbone network has\nnot been explored for processing visual data, such as images\nand videos.\nVision Transformers (ViTs) have achieved great suc-\ncess in visual representation learning, excelling in large-\nscale self-supervised pre-training and high performance on\ndownstream tasks.\nCompared with convolutional neural\nnetworks, the core advantage lies in that ViT can provide\neach image patch with data/patch-dependent global context\nthrough self-attention. This differs from convolutional net-\nworks that use the same parameters, i.e., the convolutional\nfilters, for all positions. Another advantage is the modality-\nagnostic modeling by treating an image as a sequence of\npatches without 2D inductive bias, which makes it the pre-\nferred architecture for multimodal applications [3, 35, 39].\nAt the same time, the self-attention mechanism in Trans-\nformers poses challenges in terms of speed and memory us-\nage when dealing with long-range visual dependencies, e.g.,\nprocessing high-resolution images.\nMotivated by the success of Mamba in language mod-\neling, it is appealing that we can also transfer this success\nfrom language to vision, i.e., to design a generic and ef-\nficient visual backbone with the advanced SSM method.\nHowever, there are two challenges for Mamba, i.e., unidi-\nrectional modeling and lack of positional awareness. To\naddress these challenges, we propose the Vision Mamba\n(Vim) model, which incorporates the bidirectional SSMs\nfor data-dependent global visual context modeling and posi-\ntion embeddings for location-aware visual recognition. We\nfirst split the input image into patches and linearly project\nthem as vectors to Vim. Image patches are treated as the\nsequence data in Vim blocks, which efficiently compresses\nthe visual representation with the proposed bidirectional se-\nlective state space. Furthermore, the position embedding\nin Vim block provides the awareness for spatial informa-\ntion, which enables Vim to be more robust in dense predic-\ntion tasks. In the current stage, we train the Vim model on\nthe supervised image classification task using the ImageNet\ndataset and then use the pretrained Vim as the backbone to\nperform sequential visual representation learning for down-\nstream dense prediction tasks, i.e., semantic segmentation,\nobject detection, and instance segmentation. Like Trans-\nformers, Vim can be pretrained on large-scale unsupervised\nvisual data for better visual representation. Thanks to the\nbetter efficiency of Mamba, the large-scale pretraining of\nVim can be achieved with lower computational cost.\nCompared with other SSM-based models for vision\ntasks, Vim is a pure-SSM-based method and models im-\nages in a sequence manner, which is more promising for a\ngeneric and efficient backbone. Thanks to the bidirectional\ncompressing modeling with positional awareness, Vim is\nthe first pure-SSM-based model to handle dense prediction\ntasks. Compared with the most convincing Transformer-\nbased model, i.e., DeiT [59], Vim achieves superior per-\nformance on ImageNet classification.\nFurthermore, Vim\nis more efficient in terms of GPU memory and inference\ntime for high-resolution images. The efficiency in terms of\nmemory and speed empowers Vim to directly perform se-\nquential visual representation learning without relying on\n2D priors (such as the 2D local window in ViTDet [37]) for\nhigh-resolution visual understanding tasks while achieving\nhigher accuracy than DeiT.\nOur main contributions can be summarized as follows:\n\u2022 We propose Vision Mamba (Vim), which incorporates\nbidirectional SSM for data-dependent global visual con-\ntext modeling and position embeddings for location-\naware visual understanding.\n\u2022 Without the need of attention, the proposed Vim has\nthe same modeling power as ViT while it only has\nsubquadratic-time computation and linear memory com-\nplexity. Specifically, Vim is 2.8\u00d7 faster than DeiT and\nsaves 86.8% GPU memory when performing batch in-\nference to extract features on images at the resolution of\n1248\u00d71248.\n\u2022 We conduct extensive experiments on ImageNet classi-\nfication and dense prediction downstream tasks. The re-\nsults demonstrate that Vim achieves superior performance\ncompared to the well-established and highly-optimized\nplain vision Transformer, i.e., DeiT.\n2. Related Work\nArchitectures for generic vision backbone. In the early\neras, ConvNet [33] serves as the de-facto standard network\ndesign for computer vision. Many convolutional neural ar-\nchitectures [24, 25, 32, 49, 50, 55\u201357, 62, 71] have been\nproposed as the vision backbone for various visual applica-\ntions. The pioneering work, Vision Transformer (ViT) [13]\nchanges the landscape. It treats an image as a sequence of\nflattened 2D patches and directly applies a pure Transformer\narchitecture. The surprising results of ViT on image classi-\nfication and its scaling ability encourage a lot of follow-up\nworks [15, 58, 60, 61]. One line of works focuses on hy-\nbrid architecture designs by introducing 2D convolutional\npriors into ViT [8, 12, 14, 68]. PVT [65] proposes a pyra-\nmid structure Transformer. Swin Transformer [41] applies\nself-attention within shift windows. Another line of works\nfocuses on improving traditional 2D ConvNets with more\n2\nadvanced settings [40, 66]. ConvNeXt [42] reviews the de-\nsign space and proposes pure ConvNets, which can be scal-\nable as ViT and its variants. RepLKNet [11] proposes to\nscale up the kernel size of existing ConvNets to bring im-\nprovements.\nThough these dominant follow-up works demonstrate\nsuperior performance and better efficiency on ImageNet [9]\nand various downstream tasks [38, 73] by introducing 2D\npriors, with the surge of large-scale visual pretraining [1, 5,\n16] and multi-modality applications [3, 28, 34, 35, 39, 48],\nvanilla Transformer-style model strikes back to the center\nstage of computer vision. The advantages of larger mod-\neling capacity, unified multi-modality representation, be-\ning friendly to self-supervised learning etc., make it the\npreferred architecture. However, the number of visual to-\nkens is limited due to the quadratic complexity of Trans-\nformer. There are plenty of works [6, 7, 10, 31, 47, 54, 64]\nto address this long-standing and prominent challenge,\nbut few of them focus on visual applications.\nRecently,\nLongViT [67] built an efficient Transformer architecture for\ncomputational pathology applications via dilated attention.\nThe linear computation complexity of LongViT allows it to\nencode the extremely long visual sequence. In this work,\nwe draw inspiration from Mamba [19] and explore build-\ning a pure-SSM-based model as a generic vision backbone\nwithout using attention, while preserving the sequential,\nmodality-agnostic modeling merit of ViT.\nState space models for long sequence modeling.\n[20]\nproposes a Structured State-Space Sequence (S4) model, a\nnovel alternative to CNNs or Transformers, to model the\nlong-range dependency. The promising property of linearly\nscaling in sequence length attracts further explorations. [52]\nproposes a new S5 layer by introducing MIMO SSM and\nefficient parallel scan into S4 layer.\n[17] designs a new\nSSM layer, H3, that nearly fills the performance gap be-\ntween SSMs and Transformer attention in language mod-\neling. [45] builds the Gated State Space layer on S4 by\nintroducing more gating units to improve the expressivity.\nRecently, [19] proposes a data-dependent SSM layer and\nbuilds a generic language model backbone, Mamba, which\noutperforms Transformers at various sizes on large-scale\nreal data and enjoys linear scaling in sequence length. In\nthis work, we explore transferring the success of Mamba to\nvision, i.e., building a generic vision backbone purely upon\nSSM without attention.\nState space models for visual applications.\n[26] uses\n1D S4 to handle the long-range temporal dependencies for\nvideo classification.\n[46] further extends 1D S4 to han-\ndle multi-dimensional data including 2D images and 3D\nvideos. [27] combines the strengths of S4 and self-attention\nto build TranS4mer model, achieving state-of-the-art per-\nformance for movie scene detection. [63] introduces a novel\nselectivity mechanism to S4, largely improving the perfor-\nmance of S4 on long-form video understanding with a much\nlower memory footprint. [72] supplants attention mecha-\nnisms with a more scalable SSM-based backbone to gen-\nerate high-resolution images and process fine-grained rep-\nresentation under affordable computation.\n[44] proposes\nU-Mamba, a hybrid CNN-SSM architecture, to handle the\nlong-range dependencies in biomedical image segmenta-\ntion. The above works either apply SSM to specific visual\napplications or build a hybrid architecture by combining\nSSM with convolution or attention. Different from them,\nwe build a pure-SSM-based model, which can be adopted\nas a generic vision backbone.\n3. Method\nThe goal of Vision Mamba (Vim) is to introduce the ad-\nvanced state space model (SSM), i.e., Mamba [19], to com-\nputer vision. This section begins with a description of the\npreliminaries of SSM. It is followed by an overview of Vim.\nWe then detail how the Vim block processes input token se-\nquences and proceed to illustrate the architecture details of\nVim. The section concludes with an analysis of the effi-\nciency of the proposed Vim.\n3.1. Preliminaries\nThe SSM-based models, i.e., structured state space se-\nquence models (S4) and Mamba are inspired by the con-\ntinuous system, which maps a 1-D function or sequence\nx(t) \u2208 R 7\u2192 y(t) \u2208 R through a hidden state h(t) \u2208 RN.\nThis system uses A \u2208 RN\u00d7N as the evolution parameter and\nB \u2208 RN\u00d71, C \u2208 R1\u00d7N as the projection parameters.\nh\u2032(t) = Ah(t) + Bx(t),\ny(t) = Ch(t).\n(1)\nThe S4 and Mamba are the discrete versions of the con-\ntinuous system, which include a timescale parameter \u2206 to\ntransform the continuous parameters A, B to discrete pa-\nrameters A, B. The commonly used method for transfor-\nmation is zero-order hold (ZOH), which is defined as fol-\nlows:\nA = exp (\u2206A),\nB = (\u2206A)\u22121(exp (\u2206A) \u2212 I) \u00b7 \u2206B.\n(2)\nAfter the discretization of A, B, the discretized version\nof Eq. (1) using a step size \u2206 can be rewritten as:\nht = Aht\u22121 + Bxt,\nyt = Cht.\n(3)\nAt last, the models compute output through a global con-\n3\nvolution.\nK = (CB, CAB, . . . , CA\nM\u22121B),\ny = x \u2217 K,\n(4)\nwhere M is the length of the input sequence x, and K \u2208 RM\nis a structured convolutional kernel.\n3.2. Vision Mamba\nAn overview of the proposed Vim is shown in Fig. 2. The\nstandard Mamba is designed for the 1-D sequence.\nTo\nprocess the vision tasks, we first transform the 2-D image\nt \u2208 RH\u00d7W\u00d7C into the flattened 2-D patches xp \u2208 RJ\u00d7(P2\u00b7C),\nwhere (H, W) is the size of input image, C is the number of\nchannels, P is the size of image patches. Next, we linearly\nproject the xp to the vector with size D and add position\nembeddings Epos \u2208 R(J+1)\u00d7D, as follows:\nT0 = [tcls; t1\npW; t2\npW; \u00b7 \u00b7 \u00b7 ; tJ\npW] + Epos,\n(5)\nwhere tj\np is the j-th patch of t, W \u2208 R(P2\u00b7C)\u00d7D is the\nlearnable projection matrix.\nInspired by ViT [13] and\nBERT [30], we also use class token to represent the whole\npatch sequence, which is denoted as tcls. We then send\nthe token sequence (Tl\u22121) to the l-th layer of the Vim en-\ncoder, and get the output Tl. Finally, we normalize the out-\nput class token T0\nL and feed it to the multi-layer perceptron\n(MLP) head to get the final prediction \u02c6p, as follows:\nTl = Vim(Tl\u22121) + Tl\u22121,\nf = Norm(T0\nL),\n\u02c6p = MLP(f),\n(6)\nwhere Vim is the proposed vision mamba block, L is the\nnumber of layers, and Norm is the normalization layer.\n3.3. Vim Block\nThe original Mamba block is designed for the 1-D se-\nquence, which is not suitable for vision tasks requiring\nspatial-aware understanding. In this section, we introduce\nthe Vim block, which incorporates the bidirectional se-\nquence modeling for the vision tasks. The Vim block is\nshown in Fig. 2.\nSpecifically, we present the operations of Vim block in\nAlgo. 21. The input token sequence Tl\u22121 is first normal-\nized by the normalization layer. Next, we linearly project\nthe normalized sequence to the x and z with dimension size\nE. Then, we process the x from the forward and backward\ndirections. For each direction, we first apply the 1-D convo-\nlution to the x and get the x\u2032\no. We then linearly project the\nx\u2032\no to the Bo, Co, \u2206o, respectively. The \u2206o is then used to\ntransform the Ao, Bo, respectively. Finally, we compute the\nAlgorithm 1 Vim Block Process\nRequire: token sequence Tl\u22121 : (B, M, D)\nEnsure: token sequence Tl : (B, M, D)\n1: /* normalize the input sequence T\u2032\nl\u22121 */\n2: T\u2032\nl\u22121 : (B, M, D) \u2190 Norm(Tl\u22121)\n3: x : (B, M, E) \u2190 Linearx(T\u2032\nl\u22121)\n4: z : (B, M, E) \u2190 Linearz(T\u2032\nl\u22121)\n5: /* process with different direction */\n6: for o in {forward, backward} do\n7:\nx\u2032\no : (B, M, E) \u2190 SiLU(Conv1do(x))\n8:\nBo : (B, M, N) \u2190 LinearB\no (x\u2032\no)\n9:\nCo : (B, M, N) \u2190 LinearC\no (x\u2032\no)\n10:\n/* softplus ensures positive \u2206o */\n11:\n\u2206o\n:\n(B, M, E) \u2190 log(1 + exp(Linear\u2206\no (x\u2032\no) +\nParameter\u2206\no ))\n12:\n/* shape of ParameterA\no is (E, N) */\n13:\nAo : (B, M, E, N) \u2190 \u2206o\nN ParameterA\no\n14:\nBo : (B, M, E, N) \u2190 \u2206o\nN Bo\n15:\nyo : (B, M, E) \u2190 SSM(Ao, Bo, Co)(x\u2032\no)\n16: end for\n17: /* get gated yo */\n18: y\u2032\nforward : (B, M, E) \u2190 yforward\nJ SiLU(z)\n19: y\u2032\nbackward : (B, M, E) \u2190 ybackward\nJ SiLU(z)\n20: /* residual connection */\n21: Tl : (B, M, D) \u2190 LinearT(y\u2032\nforward + y\u2032\nbackward) + Tl\u22121\nReturn: Tl\nyforward and ybackward through the SSM. The yforward\nand ybackward are then gated by the z and added together to\nget the output token sequence Tl.\n3.4. Architecture Details\nIn summary, the hyper-parameters of our architecture are\nlisted as follows:\nL: the number of blocks,\nD: the hidden state dimension,\nE: expanded state dimension,\nN: SSM dimension.\nFollowing ViT [13] and DeiT [60], we first employ 16\u00d716\nkernel size projection layer to get a 1-D sequence of non-\noverlapping patch embeddings. Subsequently, we directly\nstack L Vim blocks. By default, we set the number of blocks\nL to 24, SSM dimension N to 16. To align with the model\nsizes of DeiT series, we set the hidden state dimension D to\n192 and expanded state dimension E to 384 for the tiny-size\nvariant. For the small-size variant, we set D to 384 and E to\n768.\n3.5. Efficiency Analysis\nTraditional SSM-based methods leverage the fast Fourier\ntransform to boost the convolution operation as shown in\nEq. (4). For data-dependent methods, such as Mamba, the\n4\nEmbedded Patches\nNorm\n\ud835\udc65\nForward\nConv1d\nBackward\nConv1d\nForward\nSSM\nBackward\nSSM\nL\u00d7\nVision Mamba Encoder\nInput Image\nVision Mamba Encoder\nFlatten & Linear Projection\nProjection Layer\nPatch Tokens\nPosition Embed.\nClass Token\n0 1\n*\nVision Mamba (Vim)\nActivation\n\ud835\udc67\nMLP & Prediction\n0\n1\n2\n3\n4\n5 * 6\n7\n8\n9\nFigure 2. The overview of the proposed Vim model. We first split the input image into patches, and then project them into patch tokens.\nLast, we send the sequence of tokens to the proposed Vim encoder. To perform ImageNet classification, we concatenate an extra learnable\nclassification token to the patch token sequence. Different from Mamba for text sequence modeling, Vim encoder processes the token\nsequence with both forward and backward directions.\nSSM operation in Line 11 of Algo. 21 is no longer equiv-\nalent to convolution. To address this problem, Mamba and\nthe proposed Vim choose a modern-hardware-friendly way\nto ensure efficiency. The key idea of this optimization is\nto avoid the IO-bound and memory-bound of modern hard-\nware accelerators (GPUs).\nIO-Efficiency. The high bandwidth memory (HBM) and\nSRAM are two important components for GPUs. Among\nthem, SRAM has a larger bandwidth and HBM has a bigger\nmemory size. The standard implementation of Vim\u2019s SSM\noperation with HBM requires the number of memory IO on\nthe order of O(BMEN). Inspired by Mamba, Vim first reads\nin O(BME + EN) bytes of memory (\u2206o, Ao, Bo, Co) from\nslow HBM to fast SRAM. Then, Vim gets the discrete Ao,\nBo of a size of (B, M, E, N) in SRAM. Last, Vim performs\nSSM operations in SRAM and writes the output of a size of\n(B, M, E) back to HBM. This method can help to reduce IOs\nfrom O(BMEN) to O(BME + EN).\nMemory-Efficiency.\nTo avoid out-of-memory problems\nand achieve lower memory usage when dealing with long\nsequences, Vim chooses the same recomputation method as\nMamba. For the intermediate states of size (B, M, E, N) to\ncalculate the gradient, Vim recomputes them at the network\nbackward pass. For intermediate activations such as the out-\nput of activation functions and convolution, Vim also re-\ncomputes them to optimize the GPU memory requirement,\nas the activation values take a lot of memory but are fast for\nrecomputation.\nComputation-Efficiency. SSM in Vim block (Line 11 in\nAlgo.21) and self-attention in Transformer both play a key\nrole in providing global context adaptively. Given a visual\nsequence T \u2208 R1\u00d7M\u00d7D and the default setting E = 2D, the\ncomputation complexity of a global self-attention and SSM\nare:\n\u2126(self-attention) = 4MD2 + 2M2D,\n(7)\n\u2126(SSM) = 3M(2D)N + M(2D)N,\n(8)\nwhere self-attention is quadratic to sequence length M, and\nSSM is linear to sequence length M (N is a fixed parameter,\nset to 16 by default). The computational efficiency makes\nVim scalable for gigapixel applications with large sequence\nlengths.\n4. Experiment\n4.1. Image Classification\nSettings.\nWe benchmark Vim on the ImageNet-1K\ndataset [9], which contains 1.28M training images and 50K\nvalidation images from 1,000 categories. All models are\ntrained on the training set, and top-1 accuracy on the vali-\ndation set is reported. For fair comparisons, our training set-\ntings mainly follow DeiT [60]. Specifically, we apply ran-\ndom cropping, random horizontal flipping, label-smoothing\nregularization, mixup, and random erasing as data augmen-\ntations. When training on 2242 input images, we employ\nAdamW [43] with a momentum of 0.9, a total batch size of\n1024, and a weight decay of 0.05 to optimize models. We\ntrain the Vim models for 300 epochs using a cosine sched-\nule, 1\u00d710\u22123 initial learning rate, and EMA. During testing,\nwe apply a center crop on the validation set to crop out 2242\nimages. Experiments are performed on 8 A800 GPUs.\nLong Sequence Fine-tuning To make full use of the effi-\ncient long sequence modeling power of Vim, we continue\nto fine-tune Vim with a long sequence setting for 30 epochs\nafter ImageNet pretraining. Specifically, we set a patch ex-\ntraction stride of 8 while keeping the patch size unchanged,\na constant learning rate of 10\u22125, and a weight decay of\n10\u22128.\nResults.\nTab. 1 compares Vim with ConvNet-based,\nTransformer-based and SSM-based backbone networks.\n5\nMethod\nimage\nsize\n#param.\nImageNet\ntop-1 acc.\nConvnets\nResNet-18\n2242\n12M\n69.8\nResNet-50\n2242\n25M\n76.2\nResNet-101\n2242\n45M\n77.4\nResNet-152\n2242\n60M\n78.3\nResNeXt50-32\u00d74d\n2242\n25M\n77.6\nRegNetY-4GF\n2242\n21M\n80.0\nTransformers\nViT-B/16\n3842\n86M\n77.9\nViT-L/16\n3842\n307M\n76.5\nDeiT-Ti\n2242\n6M\n72.2\nDeiT-S\n2242\n22M\n79.8\nDeiT-B\n2242\n86M\n81.8\nSSMs\nS4ND-ViT-B\n2242\n89M\n80.4\nVim-Ti\n2242\n7M\n76.1\nVim-Ti\u2020\n2242\n7M\n78.3 +2.2\nVim-S\n2242\n26M\n80.5\nVim-S\u2020\n2242\n26M\n81.6 +1.1\nTable 1. Comparison with different backbones on ImageNet-1K\nvalidation set. \u2020 represents the model is fine-tuned with our long\nsequence setting.\nMethod\nBackbone\nimage\nsize\n#param.\nval\nmIoU\nDeepLab v3+\nResNet-101\n5122\n63M\n44.1\nUperNet\nResNet-50\n5122\n67M\n41.2\nUperNet\nResNet-101\n5122\n86M\n44.9\nUperNet\nDeiT-Ti\n5122\n11M\n39.2\nUperNet\nDeiT-S\n5122\n43M\n44.0\nUperNet\nVim-Ti\n5122\n13M\n41.0\nUperNet\nVim-S\n5122\n46M\n44.9\nTable 2. Results of semantic segmentation on the ADE20K val\nset.\nCompared to ConvNet-based ResNet [24], Vim demon-\nstrates superior performance. For example, when the pa-\nrameters are roughly similar, the top-1 accuracy of Vim-\nSmall reaches 80.5, which is 4.3 points higher than that of\nResNet50. Compared with the conventional self-attention-\nbased ViT [13], Vim outperforms it by considerable margins\nin terms of both parameter numbers and classification accu-\nracy. When compared to the highly-optimized ViT-variant,\ni.e., DeiT [60], Vim surpasses it at different scales with\nBackbone\nAPbox APbox\n50\nAPbox\n75\nAPbox\ns\nAPbox\nm\nAPbox\nl\nDeiT-Ti\n44.4\n63.0\n47.8\n26.1\n47.4\n61.8\nVim-Ti\n45.7\n63.9\n49.6\n26.1\n49.0\n63.2\nBackbone\nAPmask APmask\n50\nAPmask\n75\nAPmask\ns\nAPmask\nm\nAPmask\nl\nDeiT-Ti\n38.1\n59.9\n40.5\n18.1\n40.5\n58.4\nVim-Ti\n39.2\n60.9\n41.7\n18.2\n41.8\n60.2\nTable 3. Results of object detection and instance segmentation on\nthe COCO val set using Cascade Mask R-CNN [4] framework.\ncomparable parameter numbers: 3.9 points higher for Vim-\nTiny over DeiT-Tiny, and 0.7 points higher for Vim-Small\nover DeiT-Small. Compared with SSM-based S4ND-ViT-\nB [46], Vim achieves higher top-1 accuracy with 3\u00d7 fewer\nparameters.\nAfter long sequence fine-tuning, Vim-Tiny\u2020\nand Vim-S\u2020 all achieve higher results. Among them, Vim-\nS\u2020 even achieves similar results with DeiT-B. The results\ndemonstrate that Vim can be adapted to longer sequence\nmodeling easily and extract stronger visual representation.\nFig. 1 (b) and (c) compare the FPS and GPU mem-\nory of tiny-size Vim and DeiT. Vim demonstrates bet-\nter efficiency in speed and memory as image resolution\ngrows. Specifically, when the image size is 512\u00d7512, Vim\nachieves similar FPS and memory as DeiT. As the image\nsize grows to 1248\u00d71248, Vim is 2.8\u00d7 faster than DeiT and\nsaves 86.8% GPU memory. The pronounced superiority of\nVim\u2019s linear scaling in sequence length makes it ready for\nhigh-resolution downstream vision applications and long-\nsequence multi-modality applications.\n4.2. Semantic Segmentation\nSettings. We conduct experiments for semantic segmen-\ntation on the ADE20K [73] and use UperNet [70] as the\nsegmentation framework. We provide detailed settings in\nSec. B.\nResults. As shown in Tab. 2, Vim consistently outperforms\nDeiT across different scales: 1.8 mIoU higher for Vim-Ti\nover DeiT-Ti, and 0.9 mIoU higher for Vim-S over DeiT-S.\nCompared to the ResNet-101 backbone, our Vim-S achieves\nthe same segmentation performance with nearly 2\u00d7 fewer\nparameters.\nTo further evaluate the efficiency for downstream tasks,\ni.e., segmentation, detection, and instance segmentation, we\ncombine the backbones with a commonly used feature pyra-\nmid network (FPN) module and benchmark their FPS and\nGPU memory. As shown in Fig. 4 and Fig. 3, the efficiency\ncurves demonstrate similar comparison results of the pure\nbackbone (Fig. 1), though we append a heavy FPN on the\nbackbones. The exceptional linear scaling performance is\nattributed to our proposed efficient backbone Vim, which\n6\n5\n20\n35\n50\n65\n80\n512\n640\n738\n1024\n1248\nGPU Memory (GB)\nResolution\nDeiT\nVim\nSmaller\n-73.2% memory\nOOM\n5.04\n6.88\n8.54\n15.86\n22.59\n5.52\n8.09\n12.48\n40.03\nFigure 3.\nGPU memory efficiency comparison between DeiT-\nTi [59] and our Vim-Ti on the commonly used downstream frame-\nwork. We perform batch inference and benchmark the GPU mem-\nory on the architecture with the backbone and FPN. Vim requires\ncomparable GPU memory to DeiT with a small resolution, i.e.,\n512\u00d7512. As the input image resolution increases, Vim will use\nsignificantly less GPU memory.\nbuilds the foundation for learning gigapixel-level visual rep-\nresentation in an end-to-end manner without the need for\nmulti-stage encoding (e.g., aerial image, medical image,\nand computational pathology).\n4.3. Object Detection and Instance Segmentation\nSettings. We conduct experiments for object detection and\ninstance segmentation on the COCO 2017 dataset [38] and\nuse ViTDet [70] as the basic framework. We provide de-\ntailed settings in Sec. B.\nResults. Tab. 3 compares Vim-Ti with DeiT-Ti using Cas-\ncade Mask R-CNN framework [4]. Vim-Ti surpasses DeiT-\nTi by 1.3 box AP and 1.1 mask AP. For the middle-size\nand large-size objects, Vim-Ti outperforms DeiT-Ti by 1.6\nAPbox\nm /1.3 APmask\nm\nand 1.4 APbox\nl\n/1.8 APmask\nl\n, demonstrating\nbetter long-range context learning than DeiT (Fig. 5).\nWe highlight that the accuracy superiority is non-trivial\nsince DeiT is equipped with window attention while Vim\nworks in a pure sequence modeling manner. Specifically,\nto perform representation learning on high-resolution im-\nages (i.e., 1024\u00d71024), we follow ViTDet [37] and modify\nthe DeiT backbone with the use of 2D window attention,\nwhich injects 2D prior and breaks the sequential modeling\nnature of Transformer. Thanks to the efficiency illustrated\nin Sec. 3.5, Fig. 1 and Fig. 3, we can directly apply Vim on\n1024\u00d71024 input images and learn sequential visual rep-\nresentation for object detection and instance segmentation\nwithout need for 2D priors in the backbone.\n4.4. Ablation Study\nBidirectional SSM. We ablate the key bidirectional de-\nsign of Vim, using ImageNet-1K classification and the\n1\n1.4\n1.8\n2.2\n2.6\n512\n640\n738\n1024\n1248\nFPS w/ log scale\nResolution\nDeiT\nVim\n2.52\n2.24\n2.00\n1.56\n1.25\n2.27\n2.06\n1.90\n1.70\nFaster\n2.8\u00d7 faster\nFigure 4. FPS comparison between DeiT-Ti [59] and our Vim-\nTi on the commonly used downstream framework. We perform\nbatch inference and benchmark the log-scaled FPS on the archi-\ntecture with the backbone and FPN. Vim achieves comparable per-\nformance to DeiT with a small resolution, i.e., 512\u00d7512. As the\ninput image resolution increases, Vim has a higher FPS.\nBidirectional strategy\nImageNet\ntop-1 acc.\nADE20K\nmIoU\nNone\n73.2\n32.3\nBidirectional Layer\n70.9\n33.6\nBidirectional SSM\n72.8\n33.2\nBidirectional SSM + Conv1d\n73.9\n35.9\nTable 4. Ablation study on the bidirectional design. To ensure a\nfair comparison, we do not use the class token for each experiment.\nThe default setting for Vim is marked in blue .\nSegmenter [53] semantic segmentation framework on\nADE20K. To fully evaluate the power of learned represen-\ntation on ImageNet, we use a simple Segmenter head with\nonly 2 layers to perform transfer learning on semantic seg-\nmentation. We study these bidirectional strategies:\n\u2022 None. We directly adopt the Mamba block to process vi-\nsual sequence with only the forward direction.\n\u2022 Bidirectional Sequence. During training, we randomly\nflip the visual sequence. This works like data augmenta-\ntion.\n\u2022 Bidirectional Block. We pair the stacked blocks. The\nfirst block of each pair processes visual sequence in the\nforward direction and the second block of each pair pro-\ncesses in the backward direction.\n\u2022 Bidirectional SSM. We add an extra SSM for each block\nto process the visual sequence in the backward direction.\n\u2022 Bidirectional SSM + Conv1d.\nBased on Bidirectional\nSSM, we further add a backward Conv1d before the back-\nward SSM (Fig. 2).\nAs shown in Tab. 4, directly adopting the Mamba block\nachieves good performance in classification. However, the\nunnatural unidirectional manner poses challenges in down-\nstream dense prediction. Specifically, the preliminary bidi-\nrectional strategy of using Bidirectional Block achieves 7\n7\nClassification strategy\nImageNet top-1 acc.\nMean pool\n73.9\nMax pool\n73.4\nHead class token\n75.2\nDouble class token\n74.3\nMiddle class token\n76.1\nTable 5. Ablation study on the classification design. The default\nsetting for Vim is marked in blue .\npoints lower top-1 accuracy on classification. Yet, it outper-\nforms the vanilla unidirectional Mamba block by 1.3 mIoU\non semantic segmentation. By adding extra backward SSM\nand Conv1d, we achieve superior classification accuracy\n(73.9 top-1 acc vs. 73.2 top-1 acc) and exceptional segmen-\ntation superiority (35.9 mIoU vs. 32.3 mIoU). We use the\nstrategy of Bidirectional SSM + Conv1d as the default set-\nting in our Vim block.\nClassification Design. We ablate the classification design\nof Vim, benchmarking on ImageNet-1K classification. We\nstudy the following classification strategies:\n\u2022 Mean pool. We adopt mean pooling on the output feature\nfrom the last Vim block and perform classification on this\npooled feature.\n\u2022 Max pool. We first adapt the classification head on each\ntoken of the visual sequence and then perform max pool-\ning on the sequence to get the classification prediction\nresult.\n\u2022 Head class token. Following DeiT [60], we concatenate\nthe class token at the head of the visual sequence and per-\nform classification.\n\u2022 Double class token. Based on the head class token strat-\negy, we additionally add a class token at the tail of the\nvisual sequence.\n\u2022 Middle class token. We add a class token at the middle of\nthe visual sequence and then perform classification on the\nfinal middle class token.\nAs shown in Tab. 5, experiments show that the middle\nclass token strategy can fully exploit the recurrent nature of\nSSM and the central object prior in ImageNet, demonstrat-\ning the best top-1 accuracy of 76.1.\n5. Conclusion and Future Work\nWe have proposed Vision Mamba (Vim) to explore the\nvery recent efficient state space model, i.e., Mamba, as\ngeneric vision backbones. Unlike prior state space mod-\nels for vision tasks which use hybrid architecture or equiva-\nlent global 2D convolutional kernel, Vim learns visual rep-\nresentation in the sequence modeling manner and does not\nintroduce image-specific inductive biases. Thanks to the\nproposed bidirectional state space modeling, Vim achieves\ndata-dependent global visual context and enjoys the same\nmodeling power as Transformer, while having lower com-\nputation complexity. Benefiting from the hardware-aware\ndesigns of Mamba, the inference speed and memory us-\nage of Vim are significantly better than ViTs when pro-\ncessing high-resolution images. Experiment results on stan-\ndard computer vision benchmarks have verified the model-\ning power and high efficiency of Vim, showing that Vim has\ngreat potential to be the next-generation vision backbone.\nIn future works, Vim with the bidirectional SSM model-\ning with position embeddings is suitable for unsupervised\ntasks such as mask image modeling pretraining and the\nsimilar architecture with Mamba enables multimodal tasks\nsuch as CLIP-style pretraining.\nBased on the pretrained\nVim weights, exploring the usefulness of Vim for analyz-\ning high-resolution medical images, remote sensing images,\nand long videos, which can be regarded as downstream\ntasks, is very straightforward.\nAcknowledgement\nWe would like to acknowledge Tianheng Cheng, Yuxin\nFang, Shusheng Yang, Bo Jiang, and Jingfeng Yao for their\nhelpful feedback on the draft.\nReferences\n[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit:\nBERT pre-training of image transformers. In ICLR, 2022. 3\n[2] Ethan Baron, Itamar Zimerman, and Lior Wolf. 2-d ssm: A\ngeneral spatial layer for visual transformers. arXiv preprint\narXiv:2306.06635, 2023. 2\n[3] Rohan Bavishi, Erich Elsen, Curtis Hawthorne, Maxwell\nNye, Augustus Odena, Arushi Somani, and Sa\u02d8gnak Tas\u00b8\u0131rlar.\nIntroducing our multimodal models, 2023. 2, 3\n[4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: High\nquality object detection and instance segmentation. TPAMI,\n2019. 6, 7, 11\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4e J\u00b4egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, 2021. 3\n[6] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 3\n[7] Krzysztof Marcin Choromanski,\nValerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin,\nLukasz Kaiser, David Benjamin Belanger, Lucy J Colwell,\nand Adrian Weller. Rethinking attention with performers. In\nICLR, 2021. 3\n[8] Zihang Dai, Hanxiao Liu, Quoc V Le, and Mingxing Tan.\nCoatnet: Marrying convolution and attention for all data\nsizes. NeurIPS, 34, 2021. 2\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 1, 3, 5\n8\n[10] Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shao-\nhan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei.\nLongnet:\nScaling transformers to 1,000,000,000 tokens.\narXiv preprint arXiv:2307.02486, 2023. 3\n[11] Xiaohan Ding, Xiangyu Zhang, Jungong Han, and Guiguang\nDing. Scaling up your kernels to 31x31: Revisiting large\nkernel design in cnns. In CVPR, 2022. 3\n[12] Xiaoyi Dong, Jianmin Bao, Dongdong Chen, Weiming\nZhang, Nenghai Yu, Lu Yuan, Dong Chen, and Baining Guo.\nCswin transformer: A general vision transformer backbone\nwith cross-shaped windows. In CVPR, 2022. 2\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. In ICLR, 2020. 2, 4,\n6\n[14] St\u00b4ephane d\u2019Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S\nMorcos, Giulio Biroli, and Levent Sagun. Convit: Improving\nvision transformers with soft convolutional inductive biases.\nIn ICML, 2021. 2\n[15] Jiemin Fang, Lingxi Xie, Xinggang Wang, Xiaopeng Zhang,\nWenyu Liu, and Qi Tian. Msg-transformer: Exchanging lo-\ncal spatial information by manipulating messenger tokens. In\nCVPR, 2022. 2\n[16] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,\nXinggang Wang, Tiejun Huang, Xinlong Wang, and Yue\nCao. Eva: Exploring the limits of masked visual representa-\ntion learning at scale. In CVPR, 2023. 3\n[17] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W\nThomas, Atri Rudra, and Christopher Re. Hungry hungry\nhippos: Towards language modeling with state space mod-\nels. In ICLR, 2023. 3\n[18] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-\nYi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple\ncopy-paste is a strong data augmentation method for instance\nsegmentation. In CVPR, 2021. 11\n[19] Albert Gu and Tri Dao.\nMamba: Linear-time sequence\nmodeling with selective state spaces.\narXiv preprint\narXiv:2312.00752, 2023. 2, 3\n[20] Albert Gu, Karan Goel, and Christopher R\u00b4e.\nEfficiently\nmodeling long sequences with structured state spaces. arXiv\npreprint arXiv:2111.00396, 2021. 1, 3\n[21] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri\nDao, Atri Rudra, and Christopher R\u00b4e. Combining recurrent,\nconvolutional, and continuous-time models with linear state\nspace layers. In NeurIPS, 2021. 1\n[22] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00b4e.\nOn the parameterization and initialization of diagonal state\nspace models. In NeurIPS, 2022. 1\n[23] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state\nspaces are as effective as structured state spaces. In NeurIPS,\n2022. 1\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition.\nIn CVPR,\n2016. 2, 6\n[25] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger.\nDensely connected convolutional net-\nworks. In CVPR, 2017. 2\n[26] Md Mohaiminul Islam and Gedas Bertasius. Long movie\nclip classification with state-space video models. In ECCV,\n2022. 3\n[27] Md Mohaiminul Islam, Mahmudul Hasan, Kishan Shamsun-\ndar Athrey, Tony Braskich, and Gedas Bertasius. Efficient\nmovie scene detection using state-space transformers.\nIn\nCVPR, 2023. 3\n[28] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,\nHieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom\nDuerig. Scaling up visual and vision-language representation\nlearning with noisy text supervision. In ICML, 2021. 3\n[29] Rudolph Emil Kalman. A new approach to linear filtering\nand prediction problems. 1960. 1\n[30] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n4\n[31] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Re-\nformer: The efficient transformer. In ICLR, 2020. 3\n[32] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classification with deep convolutional neural net-\nworks. In NeurIPS, 2012. 2\n[33] Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\n2\n[34] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation.\nIn ICML,\n2022. 3\n[35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-image pre-training with\nfrozen image encoders and large language models.\narXiv\npreprint arXiv:2301.12597, 2023. 2, 3\n[36] Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and De-\nbadeepta Dey. What makes convolutional models great on\nlong sequence modeling? In ICLR, 2022. 2\n[37] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He.\nExploring plain vision transformer backbones for object de-\ntection. In ECCV, 2022. 2, 7, 11\n[38] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nECCV, 2014. 3, 7, 11\n[39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. arXiv preprint arXiv:2304.08485,\n2023. 2, 3\n[40] Shiwei Liu, Tianlong Chen, Xiaohan Chen, Xuxi Chen, Qiao\nXiao, Boqian Wu, Tommi K\u00a8arkk\u00a8ainen, Mykola Pechenizkiy,\nDecebal Mocanu, and Zhangyang Wang. More convnets in\nthe 2020s: Scaling up kernels beyond 51x51 using sparsity.\narXiv preprint arXiv:2207.03620, 2022. 3\n[41] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\n9\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 2\n[42] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In CVPR, 2022. 3\n[43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 5\n[44] Jun Ma, Feifei Li, and Bo Wang.\nU-mamba: Enhancing\nlong-range dependency for biomedical image segmentation.\narXiv preprint arXiv:2401.04722, 2024. 3\n[45] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam\nNeyshabur. Long range language modeling via gated state\nspaces. In ICLR, 2023. 3\n[46] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey\nShah, Tri Dao, Stephen Baccus, and Christopher R\u00b4e. S4nd:\nModeling images and videos as multidimensional signals\nwith state spaces. In NeurIPS, 2022. 3, 6\n[47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically\ngated recurrent neural network for sequence modeling. In\nNeurIPS, 2023. 3\n[48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML, 2021. 3\n[49] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Doll\u00b4ar. Designing network design\nspaces. In CVPR, 2020. 2\n[50] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 2\n[51] Jimmy TH Smith, Shalini De Mello, Jan Kautz, Scott Linder-\nman, and Wonmin Byeon. Convolutional state space models\nfor long-range spatiotemporal modeling. In NeurIPS, 2023.\n2\n[52] Jimmy T.H. Smith, Andrew Warrington, and Scott Linder-\nman. Simplified state space layers for sequence modeling.\nIn ICLR, 2023. 3\n[53] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia\nSchmid. Segmenter: Transformer for semantic segmenta-\ntion. In ICCV, 2021. 7\n[54] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing\nXia, Jilong Xue, Jianyong Wang, and Furu Wei. Retentive\nnetwork: A successor to transformer for large language mod-\nelss. arXiv preprint arXiv:2307.08621, 2023. 3\n[55] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich.\nGoing deeper with\nconvolutions. In CVPR, 2015. 2\n[56] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model\nscaling for convolutional neural networks. In ICML, 2019.\n[57] Mingxing Tan and Quoc Le. Efficientnetv2: Smaller models\nand faster training. In ICML, 2021. 2\n[58] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lu-\ncas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nAndreas Steiner, Daniel Keysers, Jakob Uszkoreit, et al.\nMlp-mixer: An all-mlp architecture for vision. In NeurIPS,\n2021. 2\n[59] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In ICML, 2021. 1, 2, 7\n[60] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training\ndata-efficient image transformers & distillation through at-\ntention. In ICML, 2021. 2, 4, 5, 6, 8, 11\n[61] Hugo Touvron, Piotr Bojanowski, Mathilde Caron, Matthieu\nCord, Alaaeldin El-Nouby, Edouard Grave, Gautier Izac-\nard, Armand Joulin, Gabriel Synnaeve, Jakob Verbeek, et al.\nResmlp: Feedforward networks for image classification with\ndata-efficient training. TPAMI, 2022. 2\n[62] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,\nChaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui\nTan, Xinggang Wang, et al. Deep high-resolution represen-\ntation learning for visual recognition. TPAMI, 2020. 2\n[63] Jue Wang, Wentao Zhu, Pichao Wang, Xiang Yu, Linda Liu,\nMohamed Omar, and Raffay Hamid.\nSelective structured\nstate-spaces for long-form video understanding. In CVPR,\n2023. 3\n[64] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and\nHao Ma. Linformer: Self-attention with linear complexity.\narXiv preprint arXiv:2006.04768, 2020. 3\n[65] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense pre-\ndiction without convolutions. In ICCV, 2021. 2\n[66] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang,\nZhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu,\nHongsheng Li, et al. Internimage: Exploring large-scale vi-\nsion foundation models with deformable convolutions. In\nCVPR, 2023. 3\n[67] Wenhui Wang, Shuming Ma, Hanwen Xu, Naoto Usuyama,\nJiayu Ding, Hoifung Poon, and Furu Wei. When an image is\nworth 1,024 x 1,024 words: A case study in computational\npathology. arXiv preprint arXiv:2312.03558, 2023. 3\n[68] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing con-\nvolutions to vision transformers. In ICCV, 2021. 2\n[69] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understand-\ning. In ECCV, 2018. 11\n[70] Tete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and\nJian Sun. Unified perceptual parsing for scene understand-\ning. In ECCV, 2018. 6, 7\n[71] Saining Xie, Ross Girshick, Piotr Doll\u00b4ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In CVPR, 2017. 2\n[72] Jing Nathan Yan, Jiatao Gu, and Alexander M Rush.\nDiffusion models without attention.\narXiv preprint\narXiv:2311.18257, 2023. 3\n[73] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fi-\ndler, Adela Barriuso, and Antonio Torralba. Semantic under-\nstanding of scenes through the ade20k dataset. IJCV, 2019.\n3, 6, 11\n10\nA. Visualization\nGT\nVim-Ti\nDeiT-Ti\nFigure 5. Visualization comparison of DeiT-Ti [60] and our Vim-Ti on the Cascade Mask R-CNN [4] framework. Thanks to the long-range\ncontext learning of SSM, we can capture the very large object in the image, which the DeiT-Ti counterpart fails to perceive.\nB. Additional Setting\nSettings for Semantic Segmentation. We conduct experiments for semantic segmentation on the ADE20K [73] dataset.\nADE20K contains 150 fine-grained semantic categories, with 20K, 2K, and 3K images for training, validation, and testing,\nrespectively. We choose UperNet [69] as our base framework. In training, we employ AdamW with a weight decay of\n0.01, and a total batch size of 16 to optimize models. The employed training schedule uses an initial learning rate of 6\u00d710\u22125,\nlinear learning rate decay, a linear warmup of 1, 500 iterations, and a total training of 160K iterations. The data augmentations\nfollow common settings, including random horizontal flipping, random re-scaling within the ratio range [0.5, 2.0], and random\nphotometric distortion. During evaluation, we rescale the image to have a shorter side of 512.\nSettings for Object Detection and Instance Segmentation. We conduct experiments for object detection and instance\nsegmentation on the COCO 2017 dataset [38]. The COCO 2017 dataset contains 118K images for training, 5K images for\nvalidating, and 20K images for testing. We use the canonical Cascade Mask R-CNN [4] as the base framework. For ViT-based\nbackbones, we apply extra configurations (e.g., interleaved window & global attention) to handle the high-resolution images\nfollowing ViTDet [37]. For SSM-based Vim, we directly use it without any modifications. Other training and evaluation\nsettings are just the same. During training, we employ AdamW with a weight decay of 0.1, and a total batch size of 64 to\noptimize models. The employed training schedule uses an initial learning rate of 1\u00d710\u22124, linear learning rate decay, and a\ntotal training of 380K iterations. The data augmentations use large-scale jitter data augmentation [18] to 1024\u00d71024 input\nimages. During evaluation, we rescale the image to have a shorter side of 1024.\n11\n"
  },
  {
    "title": "ReFT: Reasoning with Reinforced Fine-Tuning",
    "link": "https://arxiv.org/pdf/2401.08967.pdf",
    "upvote": "24",
    "text": "REFT: Reasoning with REinforced Fine-Tuning\nTrung Quoc Luong\u2217, Xinbo Zhang\u2217, Zhanming Jie*, Peng Sun\u2020, Xiaoran Jin, Hang Li\nByteDance Research\n{trung.luong, zhangxinbo.freya, allan}@bytedance.com\n{wanhesong, xiaoran.jin, lihang.lh}@bytedance.com\nAbstract\nOne way to enhance the reasoning capability of\nLarge Language Models (LLMs) is to conduct\nSupervised Fine-Tuning (SFT) using Chain-of-\nThought (CoT) annotations.\nThis approach\ndoes not show sufficiently strong generaliza-\ntion ability, however, because the training only\nrelies on the given CoT data. In math problem-\nsolving, for example, there is usually only one\nannotated reasoning path for each question in\nthe training data. Intuitively, it would be better\nfor the algorithm to learn from multiple an-\nnotated reasoning paths given a question. To\naddress this issue, we propose a simple yet ef-\nfective approach called Reinforced Fine-Tuning\n(ReFT) to enhance the generalizability of learn-\ning LLMs for reasoning, with math problem-\nsolving as an example. ReFT first warmups\nthe model with SFT, and then employs on-line\nreinforcement learning, specifically the PPO\nalgorithm in this paper, to further fine-tune the\nmodel, where an abundance of reasoning paths\nare automatically sampled given the question\nand the rewards are naturally derived from the\nground-truth answers. Extensive experiments\non GSM8K, MathQA, and SVAMP datasets\nshow that ReFT significantly outperforms SFT,\nand the performance can be potentially further\nboosted by combining inference-time strategies\nsuch as majority voting and re-ranking. Note\nthat ReFT obtains the improvement by learn-\ning from the same training questions as SFT,\nwithout relying on extra or augmented training\nquestions. This indicates a superior generaliza-\ntion ability for ReFT. The code of this work is\npublicly available1.\n1\nIntroduction\nThe state-of-the-art approaches to solving math\nproblems (Uesato et al., 2022; Luo et al., 2023;\nWang et al., 2023a) employ Supervised Fine-\n* indicates equal contribution, \u2020 indicates corresponding\nauthor\n1https://github.com/lqtrung1998/mwp_ReFT\nSupervised Fine-Tuning\nModel\nQuestion (x):\u00a0Weng earns $12 an hour for babysitting. Yesterday, she just did\n50 minutes of babysitting. How much did she earn?\nChain-of-Thought (e): We need to calculate her hourly rate and then multiply\nit by the amount of time she worked. First, we need to convert 50 minutes to\nhours. There are 60 minutes in an hour, so 50 minutes is equal to 50/60 = 5/6\nhours. Next, we can calculate Weng's earnings by multiplying her hourly rate\nby the amount of time she worked:  $12/hour x 5/6 hour = $10. Therefore,\nWeng earned $10 for 50 minutes of babysitting. The answer is 10.\nAnswer (y): 10\nReinforced Fine-Tuning\nquestion\n(x, e', y') Golden\nReward\nReinforcement Learning\nFinal\nPolicy\nWarm-up\n{(x, e, y)}\n{(x, e, y)}\n{(x, e, y)}\nSFT Epochs\nSFT Epochs\nSFT Epochs\ny == y' ?\nOn-Policy\nSampling\nFigure 1: An example of question (x), CoT (e), and\nanswer (y) in GSM8K (Cobbe et al., 2021a). The SFT\nprocess iterates several epochs on the training data. The\nproposed ReFT warm-up from SFT and performs RL\ntraining on the same data.\nTuning (SFT) to train the models using Chain-of-\nThought (CoT) annotations (Wei et al., 2022). As\nshown in Figure 1, a CoT annotation outlines the\nintermediate reasoning steps toward solving a math\nproblem.\nUsually there is one CoT annotation for each\nquestion in the training data, i.e., one correct rea-\nsoning path, which is utilized in SFT. We observe\nthat this may result in relatively weak generaliza-\ntion abilities of the SFT models. It is often the case\nthat multiple valid CoT annotations exist for the\nsame question (Cobbe et al., 2021a; Zhang et al.,\n2023), underscoring the need for a more powerful\nfine-tuning approach. To address this problem, we\npropose a simple yet effective approach called Re-\ninforced Fine-Tuning (ReFT), depicted in the lower\npart of Figure 1.\nReFT commences with a warm-up stage involv-\ning Supervised Fine-Tuning (SFT) in one or two\nepochs (Figure 1, shaded box). This initial stage\narXiv:2401.08967v1  [cs.CL]  17 Jan 2024\nQuestion\nunexplored CoT\u00a0\ngold CoT\nQuestion\nSampled CoT\nSFT\nReFT\ngold CoT\nFigure 2: Comparison between SFT and ReFT on the\npresence of CoT alternatives.\nequips the model with the ability to generate cor-\nrect responses to mathematical problems to some\nextent, as demonstrated in prior work (Cobbe et al.,\n2021a). Next, ReFT proceeds to further refine the\nmodel through the utilization of an online Rein-\nforcement Learning (RL) algorithm (Sutton and\nBarto, 2018), specifically Proximal Policy Opti-\nmization (PPO) (Schulman et al., 2017) in this pa-\nper. In this way, ReFT is able to sample multiple\ncorrect reasoning paths or CoT annotations and\nlearn from them (Figure 2, right).\nSince the training data include ground-truth an-\nswers, the golden rewards can be naturally derived\nfrom them when training PPO. Consequently, there\nis no requirement for a separately trained reward\nmodel. In contrast, RLHF (Ouyang et al., 2022)\nhas to utilize a reward model that is learned from\nhuman-labeled data.\nDuring the warm-up stage, ReFT acquires a cer-\ntain level of accuracy by supervised learning. In\nthe RL stage, ReFT further enhances its ability by\nreinforcement learning through sampling various\nCoT reasoning paths. In this way, ReFT gets much\nricher supervision signals than SFT. This approach\nenables ReFT to greatly improve generalization in\nmath problem-solving (Gao et al., 2018; Brown\net al., 2020). Note that ReFT outperforms SFT by\nusing the same training questions as SFT, without\nrelying on extra or augmented training questions.\nIn fact, ReFT does not conflict with such a data\nengineering, and can be seamlessly combined with\nit.\nOur contributions can be summarized as fol-\nlows:\n\u2022 We introduce a novel fine-tuning approach, re-\ninforced fine-tuning (ReFT), which utilizes re-\ninforcement learning to solve math problems.\nReFT exhibits enhanced generalization capa-\nbilities compared to conventional supervised\nfine-tuning (SFT) when trained on the same\ndataset.\n\u2022 We conduct extensive experiments using two\nfoundational models, CodeLLAMA (Touvron\net al., 2023; Roziere et al., 2023) and Galac-\ntica (Taylor et al., 2022), on three standard\nmathematical datasets: GSM8K (Cobbe et al.,\n2021a), MathQA (Amini et al., 2019), and\nSVAMP (Patel et al., 2021). Our experiments\ncover both natural language and program-\nbased CoTs, demonstrating the significantly\nimproved performance and generalization abil-\nity of ReFT.\n\u2022 Additionally, we demonstrate that ReFT ben-\nefits from both majority voting (Wang et al.,\n2023b) and reward model reranking (Uesato\net al., 2022) at inference-time, further improv-\ning its performance.\n2\nRelated Work\nMath Problem Solving\nRecent research efforts\nfocus on CoT prompt design and data engineering.\nMost of them attempted to make CoT comprehen-\nsive and fine-grained to present the step-by-step\nreasoning solutions (Nye et al., 2021; Fu et al.,\n2023; Zhou et al., 2023b; Khot et al., 2023; Imani\net al., 2023; Miao et al., 2023). Gao et al. (2023)\nfurther proposed to use the Python program as CoT\nprompt, demonstrating more accurate reasoning\nsteps and significant improvements over the natu-\nral language CoT (Wei et al., 2022). Zhou et al.\n(2023a) introduced a prompting method that gener-\nates code to verify the intermediate reasoning step\nwith GPT-4 (OpenAI, 2023), thus achieving state-\nof-the-art performance on GSM8K (Cobbe et al.,\n2021a) and MATH (Hendrycks et al., 2021). An-\nother line of work focuses on improving the quality\nof CoT (Wang et al., 2023a; Liu et al., 2023; Yu\net al., 2023) and increasing the amount of CoT\ndata (Luo et al., 2023; Yue et al., 2023) from Ope-\nnAI\u2019s ChatGPT (gpt-3.5-turbo) or GPT-42.\nReinforcement Learning\nOur work is mostly\nrelated to the recent work that applies PPO (Schul-\nman et al., 2017) to natural language process\nfor aligning human preferences (Ouyang et al.,\n2022).\nSince then, several training algorithms\nhave been proposed to efficiently improve the\nalignment, including direct preference optimiza-\ntion (DPO) (Rafailov et al., 2023), identity pref-\nerence optimization (IPO) (Azar et al., 2023),\nand Kahneman-Tversky optimization (KTO) (Etha-\n2https://chat.openai.com/\nyarajh et al., 2023). Other than the purpose of\nalignment, we aim to adopt reinforcement learning\nas a fine-tuning paradigm to improve performance\nover conventional supervised fine-tuning.\nSpecifically for solving math problems, Uesato\net al. (2022) and Lightman et al. (2023) trained an\noutcome-based or process-based reward model to\nperform reranking (Cobbe et al., 2021a) to achieve\nmuch better performance over SFT and majority\nvoting (Wang et al., 2023b). While our approach\naims to improve the performance of the policy it-\nself, these reward model reranking approaches can\nbe easily integrated into the resulting policy model.\n3\nMethod\nIn this work, we focus on natural language CoT\n(N-CoT) (Wei et al., 2022) (Figure 1) and program-\nbased CoT (Gao et al., 2023) (P-CoT) using\nPython. Gao et al. (2023) proposed the program-\nbased CoT for math problem solving. We can\nsimply execute the program to obtain the answer.\nTo ensure clarity and avoid ambiguity, we use the\nterms N-CoT and P-CoT to represent natural lan-\nguage and program-based CoTs in the rest of this\npaper, respectively.\n3.1\nReinforced Fine-Tuning\nThe proposed Reinforced Fine-Tuning (ReFT) pro-\ncess consists of two stages: the warm-up stage and\nthe reinforcement learning stage. The overall algo-\nrithm is shown in Algorithm 1.\nWarm-up\nIn this stage, the policy is fine-tuned\nfor a few epochs on a dataset comprising of the\n\u201c(question, CoT)\u201d tuples: (x, e). It enables the\nmodel to have basic problem-solving skills to gen-\nerate a proper response for a question3. Formally,\nthe CoT generation process can be decomposed\ninto a sequence of next token prediction actions.\nThe last action token, <eos>, signals the genera-\ntion process to terminate. The CoT e is written\nas:\ne = [a1, a2, ..., aL\u22121, aL=<eos>]\nwhere L represents the maximum length.\nAt\ntimestep t, the action at is sampled from a policy\n\u03c0\u03b8(\u00b7|st) where at can be any token in the vocabu-\nlary and the state st comprises of all tokens in the\nquestion and all tokens generated so far. After each\n3The underlying concept is similar to the verifier train-\ning (Cobbe et al., 2021a) to generate multiple solutions.\naction, the resulting state st+1 is the concatenation\nof the current state st and the action at:\nst+1 =\n(\nx,\nt = 0\n[st, at],\n1 \u2264 t \u2264 L .\nAs the produced action corresponds to the <eos>\ntoken, the resulting state sL+1 is the terminal state\nand the generation process is finished. With this no-\ntation, the loss function for a sample can be written\nas in Equation 1:\nLSFT (\u03b8) = \u2212Ee\u223cD\n\" L\nX\ni=1\nlog (\u03c0\u03b8(at|st))\n#\n(1)\nReinforcement Learning\nIn this stage, the pol-\nicy improves its performance via a form of online\nself-learning using a dataset comprising of (ques-\ntion, answer) tuples: (x, y). Specifically, the pol-\nicy model learns by repeatedly sampling responses\n(Figure 2), evaluating the response\u2019s answer cor-\nrectness, and updating its parameters in an online\nfashion (line 7-14 in Algorithm 1). We employ\nPPO (Schulman et al., 2017) with a clipped ob-\njective algorithm for training. Following Ziegler\net al. (2019), the value model V\u03d5 is constructed\nby appending a linear value head on top of the\nlast hidden states of the policy model \u03c0\u03b8, which is\nthe model after the warm-up stage. The reward of\n0 is given for all action resulting in non-terminal\nstate. At the terminal state, we use a reward func-\ntion that directly compares the answer extracted\nfrom the state\u2019s CoT and the ground-truth answer\ny . Here, the reward function returns 1 if the an-\nswer is deemed correct, otherwise 0 is returned.\nOn dataset whose answers are all numeric, partial\nreward (Zhong et al., 2017; Le et al., 2022) of 0.1\ncan be applied when the answer can be extracted\nand of numeric type. For 1 \u2264 t \u2264 L, we write\nr(st, at, st+1)=\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1,\nEXTRACT(st+1) = y\n0.1, EXTRACT(st+1) \u0338= null, \u0338= y\n0,\nEXTRACT(st+1) = null\nSuch a partial reward can help reduce the effect\nof learning from sparse reward (Riedmiller et al.,\n2018; Trott et al., 2019). In addition, following\nZheng et al. (2023), our total reward is the sum\nof reward function score and the Kullback-Leibler\n(KL) divergence (Kullback and Leibler, 1951) be-\ntween the learned RL policy and initial policy\nAlgorithm 1: Reinforced Fine-Tuning\nInput: Dtrain = {(x, e, y)}: Tuples of (question, CoT, answer), W: number of warm-up steps, T:\nnumber of RL steps, U: number of updates per RL step, \u03c0(0)\n\u03b8 : Initial policy.\nOutput: \u03c0\u03b8: Final policy\n1 \u03c0\u03b8 = \u03c0(0)\n\u03b8\n2 // Warm-up stage\n3 for i \u2190 1 to W do\n4\nx, e, _ \u223c Dtrain\n// Sample mini-batch from Dtrain\n5\n\u03b8 = OPTIMIZATION_STEP(LSFT (\u03b8))\n// Update model parameters for this batch (Eq. 1)\n6 // Reinforcement learning stage\n7 for i \u2190 1 to T do\n8\nx, _, y \u223c Dtrain\n// Sample mini-batch without CoT\n9\n\u02c6e \u223c \u03c0\u03b8\n// On-policy CoT sampling\n10\n\u02c6y \u2190 EXTRACT(\u02c6e)\n// Extract the answer from CoT\n11\n\u03c0\u03b8old \u2190 \u03c0\u03b8, V\u03d5old \u2190 V\u03d5\n12\nCompute \u03b4t, \u02c6At, \u02c6Rt using \u03c0\u03b8old, V\u03d5old, x, \u02c6e, \u02c6y and y\n// \u00a73.1 Reinforcement Learning\n13\nfor j \u2190 1 to U do\n14\n\u03b8, \u03d5 = OPTIMIZATION_STEP(LRL(\u03b8, \u03d5))\n// Use the loss in Equation 2\n15 return \u03c0\u03b8\nscaled by a coefficient factor \u03b2.\nrtotal(st,at, st+1) = r(st, at, st+1)\n\u2212 \u03b2KL\n\u0010\n\u03c0\u03b8(\u00b7|st), \u03c0(0)\n\u03b8 (\u00b7|st)\n\u0011\nFor advantage calculation, the generalized advan-\ntage estimate from Schulman et al. (2018) is em-\nployed.\n\u02c6At =\nL\u2212t\nX\nl=0\n(\u03b3\u03bb)l\u03b4t+l,\nwhere the Temporal Difference (TD) is defined as\n\u03b4t\u2032 = \u2212V\u03d5(st\u2032)+rtotal(st\u2032, at\u2032, st\u2032+1)+\u03b3V\u03d5(st\u2032+1)\nwith the terminal state value V\u03d5(sL+1) := 0, \u03bb \u2208\n(0, 1] is the discount factor for rewards, and \u03b3 \u2208\n[0, 1] is the discount factor for TD. For the estimate\nof return, we leverages the \u03bb-return \u02c6Rt, which can\nbe written as the sum of the generalized advantage\nestimate and the value estimate:\n\u02c6Rt = \u02c6At + V\u03d5(st)\nLastly, the policy and value objectives can be writ-\nten as in two equations below\nLpolicy(\u03b8) = \u2212Ee\u223c\u03c0\u03b8old\n\"\nmin\n \n\u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st)\n\u02c6At,\nclip\n\u0012 \u03c0\u03b8(at|st)\n\u03c0\u03b8old(at|st), 1 \u2212 \u03f5, 1 + \u03f5\n\u0013\n\u02c6At\n!#\nLvalue(\u03d5) = 1\n2 Ee\u223c\u03c0\u03b8old\n\"\nmax\n \r\r\rV\u03d5(st) \u2212 \u02c6Rt\n\r\r\r\n2\n,\n\r\r\rclip\n\u0010\nV\u03d5(st) \u2212 \u02c6Rt, \u02c6At \u2212 \u03f5, \u02c6At + \u03f5\n\u0011\r\r\r\n2\n!#\nwhere \u03c0\u03b8old, V\u03d5old are used for sampling CoT and\ncomputing \u02c6At, \u02c6Rt. The unified loss function is the\nweighted sum of the above objectives.\nLRL(\u03b8, \u03d5) = Lpolicy + \u03b1Lvalue\n(2)\nwhere \u03b1 is the coefficient for the value function\nloss.\n4\nExperiments\n4.1\nDatasets\nWe conduct experiments on three math prob-\nlem datasets:\nGSM8K (Cobbe et al., 2021a),\nSVAMP (Patel et al., 2021) and MathQA (Amini\net al., 2019). For both GSM8K and SVAMP, the\nformat of answers is a numeric value. In MathQA,\nthe format is instead a list of multiple choices (i.e.,\nABCD). Table 1 presents the statistics of all datasets.\nWe perform few-shot prompting (Wei et al., 2022;\nGao et al., 2023) using GPT-3.5-turbo to obtain\nboth the N-CoT and P-CoT annotations4. The N-\nCoT and P-CoT annotations are obtained following\n4Examples of N-CoT and P-CoT representations can be\nfound in Appendix A.\nGSM8k\nSVAMP\nMathQAMCQ\nMathQAnumeric\nN-CoT\n7,465\n3,076\n14,862\n8,955\nP-CoT\n7,356\n3,043\n15,250\n7,672\nTest\n1,319\n1,000\n01,605\n1,605\nTable 1: Dataset statistics of two types of CoT in the\ntraining set and the test set.\nJie et al. (2023). We also conducted an additional\nexperiment on a numeric version of MathQA (Jie\nand Lu, 2023) where the format is also a numeric\nvalue. Such experiments are used to demonstrate\nour assumptions of potential reward hacking phe-\nnomenon (Skalse et al., 2022) on MathQA (\u00a74.4).\n4.2\nBaseline\nWe compare ReFT with SFT and self-training (Xie\net al., 2020; Amini et al., 2022) baselines. SFT\nsimply fine-tunes the language model on the train-\ning data.\nExperiments with self-training meth-\nods ensure a relatively fair comparison because\nall these methods share the mechanism that the\ntraining makes use of the samples generated from\nthe model.\nWe implemented Offline Self-Training (Offline-\nST) (He et al., 2020), and Online (Hoi et al., 2021)\nSelf-Training (Online-ST). The Offline-ST method\nis similar to expert iteration (Anthony et al., 2017;\nUesato et al., 2022). We first use the SFT check-\npoint from the early checkpoint to sample the CoTs\nand verify them against the ground truth. We only\nretain those expert samples that have a correct an-\nswer. We perform supervised fine-tuning on the\ncombination of original training data and the expert\nsamples.\nThe Online-ST method is made to be closely\ncomparable to ReFT. Following ReFT, Online-ST\nhas the same warm-up process. After that, we per-\nform continual training with the samples generated\non the fly. At each training step, the model first\nsamples CoTs for a batch and only retains those\nwith correct answers. The resulting batch consists\nof both sampled and ground-truth CoTs. We then\nupdate the model parameters on this batch with\nthe supervised fine-tuning objective LSFT . Com-\npared with ReFT, Online-ST neither makes use of\nnegative responses (with an incorrect answer) nor\nhas a dedicated mechanism to prevent the model\nfrom significantly diverging from the initial model,\nwhich can manifest as task-specific overfitting and\ntraining instability.\n4.3\nExperimental Setup\nWe conduct experiments with two foundation mod-\nels: Galactica-6.7B5 (Taylor et al., 2022) and\nCodellama-7B6 (Roziere et al., 2023). Both models\nare reported to have strong performance in solving\nmath problems and are commonly adopted in recent\nliterature on reasoning tasks (Yue et al., 2023; Luo\net al., 2023). In addition to the comparison with\nbaselines, we also apply common techniques, ma-\njority voting (Wang et al., 2023b) and reward model\nreranking (Lightman et al., 2023) on GSM8K.\nHyper-parameter\nIn all experiments, the train-\ning is done with 8 A100-80GB GPUs using Deep-\nSpeed (Rajbhandari et al., 2020; Rasley et al., 2020)\nZero stage 2 and HuggingFace Accelerate (Gugger\net al., 2022). During the warm-up stage of ReFT,\nwe use AdamW (Loshchilov and Hutter, 2017) op-\ntimizer with 0.1 warm-up ration. The batch size is\nset to 48 and learning rate is 1e-5. The maximum\nlength is set to 1024. The number of epochs in the\nwarm-up stage is either 1 or 2 in all settings except\non MathQAMCQ and MathQAnumeric where we use\nupto 5 and 10 respectively. The model is trained\nfor 300 epochs with a learning rate of 3e-7. Fol-\nlowing Ziegler et al. (2019), the \u03bb, \u03b3, \u03b1, \u03f5 and U in\nPPO are set to 1, 0.95, 5, 0.2, and 2, respectively.\nThe KL coefficient \u03b2 is set to 0.01 for P-CoT and\nis set to 0.05 for N-CoT experiments. Further hy-\nperprameter settings about ReFT can be found in\nAppendix B.\nFor SFT baseline, we train the model for 40\nepochs and choose the checkpoint with best perfor-\nmance. This number of epochs has been chosen\nto be sufficiently large to ensure SFT converges.\nFor Offline-ST baseline, we sample the CoTs by\nusing the checkpoint from the ReFT warm-up stage.\nUsing the generation temperature of 1.0 and max\nlength of 1024, we sample 100 CoTs for each\nquestion and only keep those with a correct an-\nswer. Following Singh et al. (2023), we then sub-\nsample the CoTs to 10 random unique CoTs per\nquestion to balance difficulties of questions. As\nmentioned in \u00a74.2, the Online-ST baseline tries\nto mimic the same setting as in ReFT. We have\nthe same warm-up process and the hyperparameter\nsetting is roughly the same as ReFT.\n5https://huggingface.co/facebook/galactica-6.\n7b\n6https://huggingface.co/codellama/\nCodeLlama-7b-hf\nMethod\nSize\nGSM8K\nSVAMP\nMathQAMCQ\nAverage\nN-CoT\nP-CoT\nN-CoT\nP-CoT\nN-CoT\nP-CoT\nN-CoT\nP-CoT\nGalactica + SFT\n6.7B\n41.0\n57.1\n53.8\n69.3\n58.7\n64.8\n51.2\n63.7\nGalactica + Offline Self-Training\n6.7B\n45.0\n61.0\n56.5\n70.8\n60.7\n67.5\n54.1\n66.5\nGalactica + Online Self-Training\n6.7B\n45.7\n61.9\n58.5\n73.7\n59.7\n62.4\n54.6\n66.0\nGalactica + ReFT\n6.7B\n46.8\n68.4\n62.3\n73.9\n58.3\n70.4\n55.8\n70.9\nCodeLLAMA + SFT\n0.7B\n44.0\n64.4\n59.6\n76.2\n56.5\n64.2\n53.4\n68.3\nCodeLLAMA + Offline Self-Training\n0.7B\n38.8\n65.0\n54.2\n72.5\n57.6\n62.8\n50.2\n66.8\nCodeLLAMA + Online Self-Training\n0.7B\n40.0\n64.3\n59.7\n75.4\n55.5\n68.2\n53.1\n69.3\nCodeLLAMA + ReFT\n0.7B\n53.5\n72.8\n60.0\n78.4\n57.9\n71.5\n57.1\n74.2\nTable 2: Value accuracy comparison among the baselines and proposed ReFT method fine-tuned with two foundation\nmodels on all datasets.\nReward Model Reranking\nFollowing (Cobbe\net al., 2021a; Uesato et al., 2022), we train a reward\nmodel (RM) to determine the correctness of the\nCoT. To construct the RM training data, we use\nthe model from the warm-up stage and perform\nsampling to obtain 100 CoTs for each question in\nthe training set. The CoTs are deduplicated and\nthe binary labels can be obtained by comparing the\nextracted answer against the ground truth.\nAs a common practice, the reward model is a\nlanguage model that is initialized from the best\nSFT checkpoint (Cobbe et al., 2021a; Ouyang et al.,\n2022). Similar to the outcome-based reward model\n(ORM) (Uesato et al., 2022), the reward model is\ntrained to predict a binary label that indicates the\n\u201ccorrect\u201d or \u201cincorrect\u201d solution. Once the input\npasses through the reward model, classification is\nconducted with a linear classifier on the hidden\nstate of the last token. Finally, the solution with\nthe highest \u201ccorrect\u201d score among the candidates\nis selected as the final answer. We train the RM\nmodel for 3 epochs using a batch size of 48 and\nmaximum length of 700.\nEvaluation\nWe report value accuracy for both\nN-CoT and P-CoT on all datasets. Specifically for\nmajority voting and reranking (Table 4), we sam-\nple 100 CoTs for evaluation. In voting, the valid\nanswer with majority counts is chosen as the final\nanswer for computing accuracy. In reranking, we\nchoose the CoT with the highest score and extract\nthe answer.\n4.4\nResults\nReFT Outperforms SFT\nTable 2 compares the\nperformance among the baselines and proposed\nReFT on GSM8K, SVAMP, and MathQA datasets.\nWe can observe that ReFT consistently achieves\nmuch better performance over the SFT and the self-\nQuestion: The diagonals of a rhombus are 18 cm and 22 cm. Find\nits area?\nA) 277, B) 266, C) 198, D) 288, E) 212\nC) 198\nGenerated CoT: To find the area of a rhombus, we need to multiply\nthe lengths of its diagonals and divide the result by 2.\nArea of rhombus = (Product of diagonals) / 2\nArea of rhombus = (18 cm x 22 cm) / 2\nArea of rhombus = 344 cm2 / 2\nArea of rhombus = 172 cm2\nTherefore, the answer is: C\nFigure 3: Example prediction of MathQAMCQ reveals\nreward hacking.\ntraining family approaches except on MathQAMCQ\nN-CoT. Specifically, we have more than 9-point\nand 8-point improvement over SFT with CodeL-\nLAMA on GSM8K N-CoT and P-CoT, respectively.\nOn average, we achieve 3.7-point and 5.9-point\nimprovements with CodeLLAMA on all datasets\nin N-CoT and P-CoT, respectively. More impor-\ntantly, no additional annotations or reward models\nare used in ReFT. Such strong results demonstrate\nrobust generalization of ReFT (see Analysis \u00a75.1)\nand huge potential for further exploring the training\ndata with reinforcement learning (Lu et al., 2023).\nOffline self-training includes the sampling data\nfrom the initial policy for fine-tuning. We can see\nthis simple baseline can sometimes improve the\nperformance compared with SFT (He et al., 2020;\nGulcehre et al., 2023) but the improvements are\nfar behind the one made by ReFT. Such compar-\nisons indicate that \u201cexploring\u201d is essential in ReFT\nto have good performance. Though online self-\ntraining achieves some improvements with Galac-\ntica, it is still far behind ReFT on average. This\nresult indicates that incorrect instances are also\nvery essential to guide the model for better explo-\nration. Comparisons with self-training also suggest\nthe proposed approach with on-policy sampling\nand reinforcement learning is better than standard\ndata augmentation approaches.\nMethod\nN-CoT\nGalactica\nSFT\n41.1\nReFT\n44.9\nCodellama\nSFT\n36.3\nReFT\n41.0\nTable 3: Accuracy of SFT and ReFT with two founda-\ntion models on MathQAnumeric benchmark\nReward Hacking for MathQA\nOur investiga-\ntion of the negative results on MathQAMCQ in-\ndicates that ReFT suffers from the reward hack-\ning (Skalse et al., 2022) on the multi-choice ques-\ntion during training.\nFigure 3 shows how the\nsampled solutions produce \u201cinaccurate rewards\u201d,\nwhich makes the RL training suffer. As we can see,\nthe sampled CoT obtains an incorrect answer \u201c344\u201d\nwhich is not the product of \u201c18\u201d and \u201c22\u201d. How-\never, the final reasoning step still predicts the option\n\u201cC\u201d as the final answer as the model would always\npredict one of the options from {A, B, C, D, E}\nregardless of the correctness of intermediate CoT7.\nThus, such a misleading CoT will receive a pos-\nitive reward \u201c1\u201d and misguide the model to treat\nthis as a correct CoT. The underlying reward hack-\ning phenomenon severely tampers the model train-\ning (Everitt et al., 2021). This is also the reason\nthat we chose the checkpoint with longer warm-up\nsteps for MathQA to reduce the reward hacking\neffect.\nTo further demonstrate the negative effect of\nMCQ questions, we experiment on the MathQA\nvariant by Jie and Lu (2023), MathQAnumeric (Ta-\nble 1), which removed the options in the question,\nand directly predict the numeric answer. Table\n3 presents the comparison against SFT. We can\nobserve that ReFT consistently outperforms SFT\nusing both Galactica and CodeLLAMA.\nMajority Voting and Reranking Benefit ReFT\nFollowing Wang et al. (2023b); Uesato et al. (2022);\nLightman et al. (2023), we also perform majority\nvoting and reward model reranking to show that\nReFT can benefits from these common techniques.\nSpecifically, we perform sampling from both SFT\nand ReFT policies. We sample 100 CoT solutions\nfor each question and apply the reward model de-\nscribed in \u00a74.3. Table 4 shows that ReFT consis-\ntently achieves the best performance on GSM8K\n7We found that program-based CoTs are less likely to\nsuffer as it is more rigorous than natural language.\nMethod\nSize\nGSM8K\nN-CoT\nP-CoT\nGalactica + SFT + Voting\n6.7B\n50.8\n61.1\nGalactica + ReFT + Voting\n6.7B\n58.7\n70.7\nGalactica + SFT + Reranking\n6.7B\n59.5\n72.4\nGalactica + ReFT + Reranking\n6.7B\n62.8\n76.6\nCodeLLAMA + SFT + Voting\n0.7B\n53.8\n67.9\nCodeLLAMA + ReFT + Voting\n0.7B\n65.1\n75.0\nCodeLLAMA + SFT + Reranking\n0.7B\n61.9\n77.6\nCodeLLAMA + ReFT + Reranking\n0.7B\n65.7\n79.3\nExtra Training Data Used \u2020\nWizardMath (Luo et al., 2023)\n07B\n54.9\n-\nWizardMath (Luo et al., 2023)\n13B\n63.9\n-\nMathCoder (Wang et al., 2023a)\n07B\n67.8\n-\nMAmmoTH-Coder (Yue et al., 2023)\n07B\n22.2\n58.8\nMAmmoTH-Coder (Yue et al., 2023)\n70B\n72.4\n76.7\nGPT-3.5-turbo (Jie et al., 2023)\nN.A.\n75.3\n78.0\nGPT-4 (OpenAI, 2023; Zhou et al., 2023a)\nN.A.\n93.0\n97.0\nTable 4: Solving accuracy of majority voting and reward\nmodel reranking for SFT and ReFT on GSM8K. We also\ninclude existing approaches for comparison.\nMethod\nGSM8K\nSVAMP\nMathQAMCQ\nGalactica-125M + SFT\n23.7\n35.6\n58.4\nGalactica-125M + ReFT\n29.8\n39.4\n60.5\nTable 5: Experiments on P-CoT data with Galactica-\n125M.\nby reward model reranking. ReFT + Voting signifi-\ncantly outperforms SFT + Voting by 9.2 points on\naverage across all settings. ReFT with reranking\noutperforms SFT with reranking by 3.3 points on\naverage.\nCompared\nwith\nexisting\nopen-source\nap-\nproaches (Luo et al., 2023; Wang et al., 2023a;\nYue et al., 2023) (Table 4 bottom8), our best\nP-CoT variant achieves the best performance\nwith accuracy 79.3 on GSM8K. In addition, these\napproaches mainly include extra data generated\nfrom ChatGPT and perform distillation during\nfine-tuning. In contrast, we improve the policy\nitself by exploiting the potential of existing\ntraining data and pushing the limit of the policy\nperformance. Our best result reported in Table\n4, i.e., the CodeLLAMA + ReFT + Reranking\nwith P-CoT setting,\neven slightly surpasses\nGPT-3.5-turbo. However, we obtain the result with\na model that is only in the size of 7B.\nExperiments with Small Model\nIntuitively, ex-\nploration could lead to imperfect demonstration\nwith a small language model. We conduct an exper-\n8Numbers are taken from original papers. The N-CoT and\nP-CoT results for MAmmoTH-Coder are reported in their\nappendix.\nModel Setting\nAccuracy\nCodeLLAMA + ReFT\n72.7\n\u2013 remove partial reward\n70.9\n\u2013 KL coefficient \u03b2 = 0\ncollapse\n\u2013 non-shared value model\n72.6\nTable 6: Ablation study on GSM8K P-CoT.\niment on P-CoT data using Galactica-125M9. Table\n5 shows the performance comparison between SFT\nand ReFT. Surprisingly, ReFT still outperforms\nSFT on three datasets even with a small model.\nSuch improvements demonstrate the robustness of\nReFT during the exploration of reasonable pro-\ngrams.\nAblation Study\nWe perform the ablation study\nusing CodeLLAMA on GSM8K P-CoT (Table 6).\nWithout the partial reward, ReFT obtains a lower\naccuracy 70.9 but it is still much better than SFT.\nAs mentioned in \u00a73.1, such a partial reward can\nhelp reduce the effect of sparse reward (Trott et al.,\n2019) during training. In addition, the policy distri-\nbution will easily collapse to produce unexpected\nresults (i.e., 0 accuracy) if we set the KL coefficient\n\u03b2 to 0. It is certainly critical to impose constraints\non the space that the policy explores (Ouyang\net al., 2022). The initial warm-up step essentially\nmakes such constraints and allows the policy to\nfurther explore within the range that is governed\nby \u03b2. Finally, we also experiment with a value\nmodel that has no parameter shared with the policy\nmodel (Andrychowicz et al., 2021; Cobbe et al.,\n2021b). The individual value model initializes the\nparameter the same as the policy model. We found\nthat such a setting allows the model to converge\nfaster and eventually reach equivalent performance\nbut sacrifices two times of original computation\noverhead as we have to perform the forward pass\ntwice for each batch.\n5\nAnalysis\n5.1\nGeneralization\nFigure 4 shows the mean reward, evaluation ac-\ncuracy, and the KL divergence during training of\nReFT10 on GSM8K P-CoT. SFT converges and\nbecomes overfiting when approaching 40th epoch.\n9The smallest model size available in Galactica: https:\n//huggingface.co/facebook/galactica-125m.\n10For illustration purpose, we only shows the mean reward\nand KL for 60 epochs.\nHowever, we can see the mean reward is around\n80% to 90% for the ReFT policy at 40th epoch,\nand the value accuracy is also increasing. In addi-\ntion, we can see that the KL divergence (Figure 4\n(c)) is very large in the beginning and then main-\ntain a reasonable value between 0 and 10. The\nstable KL divergence indicates our policy performs\nexploration within a space that contains appropri-\nate programs. The underlying reinforcement learn-\ning mechanism greatly improves the generalization\nability of ReFT (Brown et al., 2020).\n5.2\nWhen ReFT surpasses SFT?\nTo further investigate the relationship between\nReFT and SFT, we perform ReFT training with\ndifferent number of warm-up steps from SFT. Fig-\nure 5 shows the value accuracy of different ReFT\nvariants against SFT11. Specifically, if the wamrup\nstep is 3, that means the policy initialize from the\n3rd-epoch SFT checkpoint. We can see that all\nReFT policies have worse performance in the be-\nginning where the epoch is less than 8. Because the\nlinear layer in the shared value model is randomly\ninitialized and it could take a few epochs to adjust\nthe distribution. Starting from the 30th epoch, SFT\nconverges and all ReFT variants are still improv-\ning. We can also see that all variants outperform\nSFT by a significant margin and there is no obvious\nadvantage of any specific ReFT variant.\n6\nConclusion\nWe have introduced reinforced fine-tuning (ReFT)\nas a new method for fine-tuning models to solve\nmath problems. In contrast to SFT, ReFT optimizes\na non-differentiable objective by exploring multi-\nple CoT annotations in the search for the correct\nanswer, rather than relying on a single CoT annota-\ntion.\nThrough extensive experimentation on three\ndatasets using two foundation models, we have\ndemonstrated that ReFT outperforms SFT in terms\nof performance and generalization ability. More-\nover, we have showcased the compatibility of mod-\nels trained with ReFT with techniques such as ma-\njority voting (Wang et al., 2023b) and reward model\nreranking (Cobbe et al., 2021a; Uesato et al., 2022).\nFurthermore, ReFT has exhibited superior per-\nformance compared to several publicly available\nopen-source models of comparable sizes in math\n11We only show 60 epochs for illustration purposes. The\nperformance for the later epoch will be shown in Appendix.\n0\n10\n20\n30\n40\n50\n60\n0.4\n0.6\n0.8\n1\nEpoch\nActual Data\n10-point Smooth Moving Average\n(a) Mean Training reward\n0\n20\n40\n60\n80\n100\n60\n70\nEpoch\n(b) Evaluation accuracy\n0\n10\n20\n30\n40\n50\n60\n0\n10\n20\n30\n40\nEpoch\n(c) Mean Sequence KL\nFigure 4: Training reward of ReFT, evaluation accuracy, KL against training epoch on GSM8K P-CoT.\n0\n10\n20\n30\n40\n50\n60\n50\n55\n60\n65\n70\nTraining Epoch\nReFTwarm-up_ep.=1\nReFTwarm-up_ep.=2\nReFTwarm-up_ep.=3\nReFTwarm-up_ep.=4\nSFT\nFigure 5: Accuracy comparison between SFT and ReFT\nwith different number of warm-up epoch.\nproblem-solving. This demonstrates the effective-\nness and practical value of the ReFT approach.\n7\nFuture Work\nWe have made the first attempt of applying re-\ninforcement learning, specifically the PPO algo-\nrithm (Schulman et al., 2017), to fine-tune of LLMs\nfor math problem-solving. Our future work in-\ncludes utilization of offline reinforcement learn-\ning techniques (Levine et al., 2020; Gulcehre\net al., 2023), development of a warm-up free\nmethod to enhance training efficiency and perfor-\nmance, thereby reducing the gap with the rerank-\ning method. Additionally, Lightman et al. (2023)\nsuggests that a well-trained process-based reward\nmodel (PRM) can significantly enhance perfor-\nmance. Hence, it would be worthwhile to explore\nthe implementation of process-based rewards in\nreinforcement learning training. Lastly, as ReFT is\na versatile approach, we intend to apply it to more\ngeneral reasoning tasks where the inference can be\nformalized with CoT.\nLimitations\nTraining Efficiency\nAs depicted in Figure 4 (b),\nit is evident that ReFT necessitates a greater num-\nber of epochs to reach convergence compared to\nSFT. This is primarily due to the fact that ReFT op-\ntimizes a non-differentiable objective and requires\nexploration of the generation space to attain correct\nanswers. While a larger learning rate may expe-\ndite convergence, it also makes the policy more\nsusceptible to instability and potential collapse. Al-\nternatively, using a larger batch size is a viable op-\ntion; however, it comes at the expense of increased\ncomputational costs.\nReward Hacking\nOur reward function relies\nsolely on the final answer to determine the reward.\nHowever, as demonstrated in the experiments con-\nducted on the MathQAMCQ N-CoT dataset, the pol-\nicy can be easily manipulated if the possible space\nof final answers is limited, such as A,B,C,D. To\nmitigate the issue of reward hacking, it may be nec-\nessary to employ a more detailed or process-based\nreward function that takes into account a broader\nrange of factors.\nReferences\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. Mathqa: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of NAACL.\nMassih-Reza\nAmini,\nVasilii\nFeofanov,\nLoic\nPauletto, Emilie Devijver, and Yury Maximov.\n2022.\nSelf-training: A survey.\narXiv preprint\narXiv:2202.12040.\nMarcin Andrychowicz, Anton Raichuk, Piotr Sta\u00b4nczyk,\nManu Orsini, Sertan Girgin, Raphael Marinier,\nL\u00e9onard Hussenot, Matthieu Geist, Olivier Pietquin,\nMarcin Michalski, et al. 2021. What matters in on-\npolicy reinforcement learning? a large-scale empiri-\ncal study. In Proceedings of ICLR.\nThomas Anthony, Zheng Tian, and David Barber. 2017.\nThinking fast and slow with deep learning and tree\nsearch. In Proceedings of NeurIPS.\nMohammad Gheshlaghi Azar, Mark Rowland, Bilal\nPiot, Daniel Guo, Daniele Calandriello, Michal\nValko, and R\u00e9mi Munos. 2023. A general theoret-\nical paradigm to understand learning from human\npreferences. arXiv preprint arXiv:2310.12036.\nDaniel S Brown, Wonjoon Goo, and Scott Niekum.\n2020. Better-than-demonstrator imitation learning\nvia automatically-ranked demonstrations. In Pro-\nceedings of Conference on Robot Learning, pages\n330\u2013359.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021a. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nKarl W Cobbe, Jacob Hilton, Oleg Klimov, and John\nSchulman. 2021b. Phasic policy gradient. In Pro-\nceedings of ICML.\nKawin Ethayarajh, Winnie Xu, Dan Jurafsky, and\nDouwe Kiela. 2023. Human-centered loss functions\n(halos). Technical report, Contextual AI.\nTom Everitt, Marcus Hutter, Ramana Kumar, and Vic-\ntoria Krakovna. 2021. Reward tampering problems\nand solutions in reinforcement learning: A causal\ninfluence diagram perspective. Synthese, 198(Suppl\n27):6435\u20136467.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and\nTushar Khot. 2023. Complexity-based prompting for\nmulti-step reasoning. In Proceedings of ICLR.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2023. PAL: Program-aided language\nmodels. In Proceedings of ICML.\nYang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine,\nand Trevor Darrell. 2018.\nReinforcement learn-\ning from imperfect demonstrations. arXiv preprint\narXiv:1802.05313.\nSylvain Gugger, Lysandre Debut, Thomas Wolf, Philipp\nSchmid, Zachary Mueller, Sourab Mangrulkar, Marc\nSun, and Benjamin Bossan. 2022. Accelerate: Train-\ning and inference at scale made simple, efficient and\nadaptable.\nhttps://github.com/huggingface/\naccelerate.\nCaglar Gulcehre, Tom Le Paine, Srivatsan Srini-\nvasan, Ksenia Konyushkova, Lotte Weerts, Abhishek\nSharma, Aditya Siddhant, Alex Ahern, Miaosen\nWang, Chenjie Gu, et al. 2023.\nReinforced self-\ntraining (rest) for language modeling. arXiv preprint\narXiv:2308.08998.\nJunxian He, Jiatao Gu, Jiajun Shen, and Marc\u2019Aurelio\nRanzato. 2020. Revisiting self-training for neural\nsequence generation. In Proceedings of ICLR.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. In Proceed-\nings of Thirty-fifth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track\n(Round 2).\nSteven CH Hoi, Doyen Sahoo, Jing Lu, and Peilin Zhao.\n2021. Online learning: A comprehensive survey.\nNeurocomputing, 459:249\u2013289.\nShima Imani, Liang Du, and Harsh Shrivastava. 2023.\nMathprompter: Mathematical reasoning using large\nlanguage models. arXiv preprint arXiv:2303.05398.\nZhanming Jie and Wei Lu. 2023. Leveraging training\ndata in few-shot prompting for numerical reasoning.\nZhanming Jie, Trung Quoc Luong, Xinbo Zhang, Xi-\naoran Jin, and Hang Li. 2023. Design of chain-of-\nthought in math problem solving. arXiv preprint\narXiv:2309.11054.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu,\nKyle Richardson, Peter Clark, and Ashish Sabharwal.\n2023. Decomposed prompting: A modular approach\nfor solving complex tasks. In Proceedings of ICLR.\nSolomon Kullback and Richard A Leibler. 1951. On\ninformation and sufficiency. The annals of mathe-\nmatical statistics, 22(1):79\u201386.\nHung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio\nSavarese, and Steven Chu Hong Hoi. 2022. Coderl:\nMastering code generation through pretrained models\nand deep reinforcement learning. In Proceedings of\nNeurIPS.\nSergey Levine, Aviral Kumar, George Tucker, and Justin\nFu. 2020. Offline reinforcement learning: Tutorial,\nreview, and perspectives on open problems. arXiv\npreprint arXiv:2005.01643.\nHunter Lightman, Vineet Kosaraju, Yura Burda, Harri\nEdwards, Bowen Baker, Teddy Lee, Jan Leike,\nJohn Schulman, Ilya Sutskever, and Karl Cobbe.\n2023.\nLet\u2019s verify step by step.\narXiv preprint\narXiv:2305.20050.\nBingbin Liu, Sebastien Bubeck, Ronen Eldan, Janard-\nhan Kulkarni, Yuanzhi Li, Anh Nguyen, Rachel Ward,\nand Yi Zhang. 2023. Tinygsm: achieving> 80% on\ngsm8k with small language models. arXiv preprint\narXiv:2312.09241.\nIlya Loshchilov and Frank Hutter. 2017.\nDecou-\npled weight decay regularization.\narXiv preprint\narXiv:1711.05101.\nXiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla,\nMorteza Ibrahimi, Ian Osband, Zheng Wen, et al.\n2023. Reinforcement learning, bit by bit. Founda-\ntions and Trends\u00ae in Machine Learning, 16(6):733\u2013\n865.\nHaipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jian-\nguang Lou, Chongyang Tao, Xiubo Geng, Qingwei\nLin, Shifeng Chen, and Dongmei Zhang. 2023. Wiz-\nardmath: Empowering mathematical reasoning for\nlarge language models via reinforced evol-instruct.\narXiv preprint arXiv:2308.09583.\nNing Miao, Yee Whye Teh, and Tom Rainforth.\n2023.\nSelfcheck: Using llms to zero-shot check\ntheir own step-by-step reasoning.\narXiv preprint\narXiv:2308.00436.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nOpenAI. 2023. GPT-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback.\nIn Proceedings of\nNeurIPS.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are nlp models really able to solve simple\nmath word problems? In Proceedings of NAACL.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D Manning, and Chelsea Finn.\n2023. Direct preference optimization: Your language\nmodel is secretly a reward model. In Proceedings of\nNeurIPS.\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,\nand Yuxiong He. 2020. Zero: Memory optimizations\ntoward training trillion parameter models. In SC20:\nInternational Conference for High Performance Com-\nputing, Networking, Storage and Analysis.\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and\nYuxiong He. 2020. Deepspeed: System optimiza-\ntions enable training deep learning models with over\n100 billion parameters. In Proceedings of SIGKDD.\nMartin Riedmiller, Roland Hafner, Thomas Lampe,\nMichael Neunert, Jonas Degrave, Tom Wiele, Vlad\nMnih, Nicolas Heess, and Jost Tobias Springenberg.\n2018. Learning by playing solving sparse reward\ntasks from scratch. In Proceedings of ICML.\nBaptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten\nSootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,\nJingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023.\nCode llama: Open foundation models for code. arXiv\npreprint arXiv:2308.12950.\nJohn Schulman, Philipp Moritz, Sergey Levine, Michael\nJordan, and Pieter Abbeel. 2018. High-dimensional\ncontinuous control using generalized advantage esti-\nmation.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal,\nAlec Radford, and Oleg Klimov. 2017.\nProxi-\nmal policy optimization algorithms. arXiv preprint\narXiv:1707.06347.\nAvi Singh, John D. Co-Reyes, Rishabh Agarwal,\nAnkesh Anand, Piyush Patil, Xavier Garcia, Pe-\nter J. Liu, James Harrison, Jaehoon Lee, Kelvin Xu,\nAaron Parisi, Abhishek Kumar, Alex Alemi, Alex\nRizkowsky, Azade Nova, Ben Adlam, Bernd Bohnet,\nGamaleldin Elsayed, Hanie Sedghi, Igor Mordatch,\nIsabelle Simpson, Izzeddin Gur, Jasper Snoek, Jef-\nfrey Pennington, Jiri Hron, Kathleen Kenealy, Kevin\nSwersky, Kshiteej Mahajan, Laura Culp, Lechao\nXiao, Maxwell L. Bileschi, Noah Constant, Roman\nNovak, Rosanne Liu, Tris Warkentin, Yundi Qian,\nYamini Bansal, Ethan Dyer, Behnam Neyshabur,\nJascha Sohl-Dickstein, and Noah Fiedel. 2023. Be-\nyond human data: Scaling self-training for problem-\nsolving with language models.\nJoar Skalse, Nikolaus Howe, Dmitrii Krasheninnikov,\nand David Krueger. 2022. Defining and characteriz-\ning reward gaming. In Proceedings of NeurIPS.\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\nment learning: An introduction. MIT press.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023.\nLlama 2:\nOpen founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nAlexander Trott, Stephan Zheng, Caiming Xiong, and\nRichard Socher. 2019. Keeping your distance: Solv-\ning sparse reward tasks using self-balancing shaped\nrewards. In Proceedings of NeurIPS.\nJonathan Uesato, Nate Kushman, Ramana Kumar, Fran-\ncis Song, Noah Siegel, Lisa Wang, Antonia Creswell,\nGeoffrey Irving, and Irina Higgins. 2022.\nSolv-\ning math word problems with process-and outcome-\nbased feedback. arXiv preprint arXiv:2211.14275.\nKe Wang, Houxing Ren, Aojun Zhou, Zimu Lu, Sichun\nLuo, Weikang Shi, Renrui Zhang, Linqi Song,\nMingjie Zhan, and Hongsheng Li. 2023a.\nMath-\ncoder: Seamless code integration in llms for en-\nhanced mathematical reasoning.\narXiv preprint\narXiv:2310.03731.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V Le,\nEd H Chi, Sharan Narang, Aakanksha Chowdhery,\nand Denny Zhou. 2023b. Self-consistency improves\nchain of thought reasoning in language models. In\nProceedings of ICLR.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. In Proceedings of\nNeurIPS.\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and\nQuoc V Le. 2020. Self-training with noisy student\nimproves imagenet classification. In Proceedings of\nCVPR, pages 10687\u201310698.\nLonghui Yu, Weisen Jiang, Han Shi, Jincheng Yu,\nZhengying Liu, Yu Zhang, James T Kwok, Zhen-\nguo Li, Adrian Weller, and Weiyang Liu. 2023.\nMetamath: Bootstrap your own mathematical ques-\ntions for large language models.\narXiv preprint\narXiv:2309.12284.\nXiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wen-\nhao Huang, Huan Sun, Yu Su, and Wenhu Chen.\n2023. Mammoth: Building math generalist models\nthrough hybrid instruction tuning. arXiv preprint\narXiv:2309.05653.\nMengxue Zhang, Zichao Wang, Zhichao Yang, Weiqi\nFeng, and Andrew Lan. 2023. Interpretable math\nword problem solution generation via step-by-step\nplanning. In Proceedings of ACL.\nRui Zheng, Shihan Dou, Songyang Gao, Yuan Hua, Wei\nShen, Binghai Wang, Yan Liu, Senjie Jin, Qin Liu,\nYuhao Zhou, Limao Xiong, Lu Chen, Zhiheng Xi,\nNuo Xu, Wenbin Lai, Minghao Zhu, Cheng Chang,\nZhangyue Yin, Rongxiang Weng, Wensen Cheng,\nHaoran Huang, Tianxiang Sun, Hang Yan, Tao Gui,\nQi Zhang, Xipeng Qiu, and Xuanjing Huang. 2023.\nSecrets of rlhf in large language models part i: Ppo.\nVictor Zhong, Caiming Xiong, and Richard Socher.\n2017. Seq2sql: Generating structured queries from\nnatural language using reinforcement learning. arXiv\npreprint arXiv:1709.00103.\nAojun Zhou, Ke Wang, Zimu Lu, Weikang Shi, Sichun\nLuo, Zipeng Qin, Shaoqing Lu, Anya Jia, Linqi Song,\nMingjie Zhan, et al. 2023a.\nSolving challenging\nmath word problems using gpt-4 code interpreter\nwith code-based self-verification.\narXiv preprint\narXiv:2308.07921.\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc Le, et al. 2023b.\nLeast-to-most prompting enables complex reasoning\nin large language models. In Proceedings of ICLR.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences.\narXiv\npreprint arXiv:1909.08593.\nQuestion: Weng earns $12 an hour for babysitting. Yesterday, she\njust did 50 minutes of babysitting. How much did she earn?\nN-CoT: We need to calculate her hourly rate and then multiply it by\nthe amount of time she worked.\nFirst, we need to convert 50 minutes to hours.\nThere are 60 minutes in an hour, so 50 minutes is equal to 50/60 =\n5/6 hours.\nNext, we can calculate Weng\u2019s earnings by multiplying her hourly\nrate by the amount of time she worked:\n$12/hour x 5/6 hour = $10.\nTherefore, Weng earned $10 for 50 minutes of babysitting.\nThe answer is 10.\nP-CoT:\ndef solution():\n\"\"\"Weng earns $12 an hour for babysitting. Yesterday, she just did\n50 minutes of babysitting. How much did she earn?\"\"\"\nhourly_rate = 12\nminutes_worked = 50\nhours_worked = minutes_worked / 60\nearnings = hourly_rate * hours_worked\nresult = earnings\nreturn result\nFigure 6: N-CoT and P-CoT examples on GSM8K\nA\nExamples of N-CoT and P-CoT\nRepresentations\nWe present examples of natural language CoT and\nprogram-based CoT from GSM8K dataset in Fig-\nure 6. We follow Jie et al. (2023) to perform few-\nshot prompting and obtain the CoT representations.\nThe natural language CoT is generally the same\nas the one presented in Wei et al. (2022). The for-\nmat program-based CoT is similar to the one in\nPAL (Gao et al., 2023), where we use a function\nto solve the problems. The function starts with a\nPython docstring that repeats the question and then\na list of statements as reasoning steps.\nB\nDetailed Hyperparameter Setting\nSupervised Fine-Tuning\nWe train the model for\n40 epochs with the batch size of 48 and the max-\nimum length of 1024.. For small models, we in-\ncrease the learning rate to 2e-5, and the number of\nepoch for training MathQAMCQ to 100 epochs.\nReFT Warm-up\nFor Galactica, we perform\nwarm-up for 2 epochs on GSM8K, SVAMP for\nboth N-CoT and P-CoT. In terms of MathQAMCQ,\nwe perform warm-up for 5 epochs on MathQAMCQ\nN-CoT and 2 epochs on MathQAMCQ P-CoT. For\nCodeLLAMA, we perform warm-up for 1 epoch\non SVAMP, 2 epochs on GSM8K, 5 epochs on\nMathQAMCQ N-CoT and 2 epochs on MathQAMCQ\nP-CoT. Specifically for MathQAnumeric, we perform\nwarm-up for 10 epochs because this dataset is much\nharder and the number of reasoning chains is longer\nthan other datasets. For small models, we the warm-\nup period is 10 epochs for GSM8K and SVAMP\nand is 40 epochs for MathQAMCQ\nReFT RL\nThe maximum length for question is\nset to 300, and the maximum length during sam-\npling is set to 700. The batch size is 32, which is\nsmaller than SFT due to extra memory consump-\ntion of the value model. The number of updates per\nRL step (i.e., ppo epoch) is set to 2 (Ziegler et al.,\n2019). We do not employ any weight decay and\ndropout following Ziegler et al. (2019). For small\nmodels, we train for 700 epochs with the learning\nrate of 3e-6 and the global batch size of 256.\n"
  },
  {
    "title": "SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding",
    "link": "https://arxiv.org/pdf/2401.09340.pdf",
    "upvote": "17",
    "text": "SCENEVERSE:\nScaling 3D Vision-Language Learning for Grounded Scene Understanding\nBaoxiong Jia\u02da\nYixin Chen\u02da\nHuangyue Yu\nYan Wang\nXuesong Niu\nTengyu Liu\nQing Li\nSiyuan Huang\nBeijing Institute for General Artificial Intelligence (BIGAI)\nhttps://scene-verse.github.io\nSCENE CAPTION\n\u201cIn this scene, there is a fray flat floor. \nA bar is standing on the floor, with \u2026 \nThe room is also designed \u2026\u201d\nOBJECT CAPTION\n\u201cThis is a big cotton sofa against the \nwall. It is made of genuine leather.\u201d\nOBJECT REFERRAL\n\u201cThe ottoman is on the carpet next to \nthe double bed in the bedroom.\u201d\nFigure 1. Overview of SCENEVERSE. A million-scale 3D vision-language dataset that comprises over 68K various 3D indoor scenes and\n2.5M aligned scene-language pairs in the form of scene caption, object caption, and object referral.\nAbstract\n3D vision-language grounding, which focuses on aligning\nlanguage with the 3D physical environment, stands as a\ncornerstone in the development of embodied agents. In com-\nparison to recent advancements in the 2D domain, grounding\nlanguage in 3D scenes faces several significant challenges:\n(i) the inherent complexity of 3D scenes due to the diverse\nobject configurations, their rich attributes, and intricate re-\nlationships; (ii) the scarcity of paired 3D vision-language\ndata to support grounded learning; and (iii) the absence\nof a unified learning framework to distill knowledge from\n\u02da indicates equal contribution.\ngrounded 3D data. In this work, we aim to address these\nthree major challenges in 3D vision-language by examining\nthe potential of systematically upscaling 3D vision-language\nlearning in indoor environments. We introduce the first\nmillion-scale 3D vision-language dataset, SCENEVERSE,\nencompassing about 68K 3D indoor scenes and comprising\n2.5M vision-language pairs derived from both human an-\nnotations and our scalable scene-graph-based generation\napproach. We demonstrate that this scaling allows for a\nunified pre-training framework, Grounded Pre-training for\nScenes (GPS), for 3D vision-language learning. Through ex-\ntensive experiments, we showcase the effectiveness of GPS\nby achieving state-of-the-art performance on all existing 3D\nvisual grounding benchmarks. The vast potential of SCEN-\n1\narXiv:2401.09340v2  [cs.CV]  6 Mar 2024\nEVERSE and GPS is unveiled through zero-shot transfer\nexperiments in the challenging 3D vision-language tasks.\n1. Introduction\nThe foundation of human cognitive development lies in the\ngrounding of language within the physical world [46, 73, 97].\nRecent progress in Large Language Models (LLMs) [11,\n75], often referred to as \u201cfoundation models\u201d [10], has\nmarkedly promoted the alignment between vision and lan-\nguage [3, 51, 66] through utilizing billion-scale vision-\nlanguage datasets [71, 96]. Nonetheless, with these ad-\nvancements predominantly focusing on the 2D domain, the\ngrounded understanding of 3D physical environments re-\nmains in an incipient stage [1, 5, 16]. Recognizing the piv-\notal role of grounded 3D experiences in shaping human\ncognition [7, 8] and the delayed research development in this\narea, there is a compelling need to intensify the exploration\ninto the vision-language learning challenge, specifically in\nthe context of 3D scenes.\nSeeking insights from the 2D vision-language (2D-VL)\nachievements, a major factor to the success was the no-\ntable scale-up of paired vision-language data [15, 45, 71].\nHowever, applying these principles directly from 2D to 3D\nis fraught with challenges. Primarily, 3D data collection\nheavily relies on the scanning device, making it inherently\nmuch more complex and expensive than gathering 2D im-\nages. Despite steady efforts to increase the volume of 3D\nscene data [9, 23, 58, 87], most datasets remain limited to\nthousands of scenes, substantially lagging behind the scale\nof existing 2D datasets. This gap is further widened by the\ninherent complexities of 3D scenes, which feature a mul-\ntitude of object instances with diverse attributes, varying\narrangements, and intricate inter-object relationships. These\nunique aspects of 3D scenes not only make the accurate de-\nscription of objects and their relations more challenging but\nalso considerably increase the number of language descrip-\ntions required for thorough scene depiction. Consequently,\nthis presents a significant challenge in obtaining a sufficient\nsupply of high-quality paired scene-language data crucial\nfor grounded scene understanding.\nTo confront these challenges, we propose consolidat-\ning current efforts to build up SCENEVERSE, the first\nmillion-scale dataset aimed at advancing 3D vision-language\n(3D-VL) learning for grounded scene understanding. At\nthe scene level, we unify 3D scene data from existing\ndatasets [9, 23, 40, 67, 78] and supplement the collection\nwith synthetic scenes [27, 95]. This compilation represents\nthe most extensive 3D scene data gathered to date, amount-\ning to 68, 406 scenes for grounding. Additionally, we pro-\npose an automated generation pipeline utilizing 3D scene\ngraphs [4, 79] and LLMs to create comprehensive, high-\nquality scene-language pairs. This refined collection, in-\ncluding 190, 836 human annotated pairs and totaling 2.5M\nscene-language pairs, provides detailed and comprehensive\nportrayals of both object-level and scene-level descriptions\nwithin the 3D scene.\nWe thoroughly investigate the potential offered by the\ndata scale-up in SCENEVERSE with large-scale pre-training.\nSpecifically, we present a novel and unified pre-training\nframework, Grounded Pre-training for Scenes (GPS), which\nis designed with scene-level and object-level alignment ob-\njectives and devoid of auxiliary losses and designs. Through\nmulti-level contrastive alignment, we observe significant\nperformance improvements across all existing 3D visual\ngrounding benchmarks, achieving new state-of-the-art re-\nsults through a simple and effective pre-training process. Ad-\nditionally, we unveil the vast possibilities offered by SCEN-\nEVERSE and GPS in 3D-VL tasks in a zero-shot transfer\nsetting. At last, we provide a more comprehensive under-\nstanding of the data-scaling effects in SCENEVERSE through\nextensive ablative experiments to point out future directions.\nOur main contributions can be summarized as follows:\n\u2022 We introduce SCENEVERSE, the first million-scale 3D-VL\ndataset for grounded scene understanding. SCENEVERSE\nencompasses 68K 3D scenes coupled with 2.5M scene-\nlanguage pairs, sourced through a combination of human\nannotation and automated generation methods. This repre-\nsents a significant improvement in terms of data diversity\nand scale compared to prior datasets.\n\u2022 We propose GPS, an efficient transformer-based model\ntrained with multi-level scene-text alignment that achieves\nstate-of-the-art results on all existing 3D-VL grounding\nbenchmarks, benefiting from pre-training on multi-level\nscene-language pairs in SCENEVERSE.\n\u2022 We demonstrate that with the data scale-up and model\ndesign, our pre-trained models exhibit emerging zero-shot\ngeneralization capabilities in grounded scene understand-\ning, paralleling the successes seen in 2D-VL models.\n2. Related Work\nDatasets for Grounded 3D Understanding\nObtaining\naligned 3D-language data is an inherently difficult task. In\n3D object modeling, pioneering works like ShapeNet [14]\nsourced 3D assets from online repositories, leading to follow-\nup proliferation of high-quality 3D object datasets [22, 60,\n81]. Notably, recent developments include internet-scale\ndata collection with Objaverse [25, 26], accompanied by\nthe integration of object-level captions [83] for 3D-language\nalignment. Models trained on these datasets demonstrate\nan enhanced understanding of objects, evident in classifica-\ntion [52], generation [53], and captioning tasks [55].\nIn contrast, developing datasets for grounded 3D scene\nunderstanding is even more challenging due to the ex-\ntensive requirements for scene acquisition and annotation.\nExisting works curate RGB-D and scanned indoor scene\ndatasets [9, 13, 23, 58, 67, 78] initially used for benchmark-\n2\ning classical grounding tasks like 3D object detection and\nsegmentation [30, 42, 59, 72, 77]. These semantically la-\nbeled scenes are subsequently applied in fine-grained scene\ngrounding tasks like object referral [1, 16, 93], caption-\ning [17, 19, 20, 88], vision-language-navigation [38, 56, 63,\n80] and reasoning [5, 37, 57]. Recent work exploits the rep-\nresentation of 3D scene graphs (3DSGs) [4, 69, 79], which\nconcisely describes scenes with hierarchical structures. This\nrepresentation is notably advantageous for planning [2, 68]\nand captioning [33], owing to its compatibility with LLMs.\nNevertheless, as shown in Tab. 1, these datasets are signifi-\ncantly constrained in both scene and language scales, under-\nscoring the need for scaling up fine-grained scene-language-\naligned data to enhance grounded scene understanding.\nVision-Language Learning\nRecent years have wit-\nnessed tremendous progress in 2D vision-language learn-\ning [3, 24, 49, 51, 66, 70, 76], empowered by transformer-\nbased pre-training models [11, 28, 62] and large-scale image-\nlanguage datasets [15, 71]. A central theme across language\nand 2D-VL domains is the effectiveness of data scaling [43],\nas demonstrated by improved alignment and expanded ca-\npabilities in open-vocabulary understanding [32, 44, 47, 50]\nthrough a simplified contrastive pre-training pipeline [66].\nHowever, in grounded scene understanding, the primary\nchallenge for models has been the limited availability of\npaired 3D scene-language data, which restricts the applica-\ntion of insights gained from 2D-VL. Current models for 3D\nscene grounding [6, 18, 35, 40, 41, 54, 82, 86, 94] heavily\nrely on task-specific knowledge in both model and loss de-\nsigns or advanced optimization strategies [98]. To bridge\nthis gap, there has been a growing emphasis on employing\npre-trained 2D-VL models for 3D-VL [34, 36, 64, 74, 83, 91,\n92]. Nonetheless, these models predominantly draw on infor-\nmation available from 2D-VL models (e.g., object attribute,\naffordance, etc.), falling short on capturing crucial informa-\ntion like object spatial relationships, which are only attain-\nable through 3D data. This urges the need for a multi-level\nalignment between language and 3D scenes, particularly re-\ngarding 3D-specific information. Considering the nascent\nstage of existing 3D pre-training methods [29, 84, 98], we\nbelieve SCENEVERSE and GPS have the potential to spear-\nhead new avenues in 3D-VL research.\n3. SCENEVERSE\nSCENEVERSE is the first million-scale dataset designed for\ngrounded scene understanding. Our 3D scenes are curated\nfrom diverse existing datasets of both real and synthetic en-\nvironments. Harnessing the power of 3D scene graphs and\nLLMs, we introduce an automated pipeline to generate com-\nprehensive and high-quality language for both object-level\nand scene-level descriptions. We additionally incorporate\nthe most extensive human-annotated object referrals to date,\nproviding new training sources and benchmarks in this field.\nTable 1. Comparison of SCENEVERSE with existing 3DVL\nDatasets. SCENEVERSE expands the data scale of prior work by\nan order of magnitude. Anno.: human annotations. Syn.: template\nor LLM generated descriptions.\nDataset\n3D Data\nLanguage\nTotal\nScene\nObject\nAnno.\nSyn.\nScanRefer[16]\n|\n|\n52K\n-\n52K\nReferIt3D[1]\n42K\n200K\n242K\nScanQA[5]\n1.5K\n33K\n27K\n-\n27K\nSQA3D[57]\n|\n|\n-\n33K\n33K\nMulti3DRefer[93]\n52K\n10K\n62K\nCap3D[55]\n-\n666K\n58K\n666K\n724K\nScanScribe[98]\n3K\n56K\n94K\n184K\n278K\nSCENEVERSE\n68K\n1.5M\n190K\n2.3M\n2.5M\n3.1. Scene Curation\nTo address the scarcity of available 3D scene data, we con-\nstruct SCENEVERSE by unifying 3D scene data from var-\nious existing datasets. We use real-world scene datasets,\nincluding ScanNet [23], ARKitScenes [9], HM3D [67],\n3RScan [78] and MultiScan [58], alongside synthetic en-\nvironments from Structured3D [95] and ProcTHOR [27].\nThe inclusion of these synthetic datasets is mainly motivated\nby their potential as scalable data sources for 3D-VL align-\nment. To ensure cohesion across various sources, we con-\nduct preprocessing steps such as room segmentation, point\nsubsampling, axis alignment, normalization, and semantic\nlabel alignment. Each scan is represented by a point cloud\nP P RN\u02c68, wherein each point is defined by its 3D coordi-\nnates, RGB color, instance id and semantic label. In total,\nwe curate 68, 406 3D scenes in SCENEVERSE.\n3.2. 3D Scene Graph Construction\nOur 3D scene graph is defined as a set of tuples G \u201c pV, Eq,\nwhere the nodes V comprises V1\n\u0164 V2\n\u0164 . . . \u0164 VK, with\nVk representing the set of nodes at a particular hierarchical\nlevel. Each node v represents one distinct 3D object instance,\nparameterized by its centroid pi P R3 and bounding box size\nof bi \u201c pbx, by, bzq P R3. The edges E represent spatial\nrelationships between nodes.\nTo construct the scene graph G, we first instantiate the\nnodes with the instance annotation from the point clouds and\nassign object classes with their corresponding semantic la-\nbels. Following prior work[1, 79], we consider the following\nspatial relations.\nVertical proximity\nThis encompasses both in-contact\nrelationships (e.g., support, inside, embed), and non-\ncontact ones (e.g., above, below).\nHorizontal proximity\nHorizontal relationships de-\nscribe the proximity relations like in front of, next\nto, behind, etc. Relationships like left, right are\ncontextually dependent on a reference view, where another\n3\nObject Referral\nScene Caption\nObject Caption\n(a) 3D Scene\n(b) Language radar chart and sankey diagram of scene-language pairs\n(c) Automated language generation\nSub-graph Context\n{ 'scene_type': 'Bedroom\u2019,\n'object_count': {'nightstand':2, ...},\n'relation': {'nightstand', 'on', 'floor'},\n{'backback', 'in front of', bed}, ...}\nBLIP2 Captions\n1. A bed in a hotel room. (0.85)\n2. A white comforter on a bed. (0.83)\n3. A bed with a striped comforter. (0.83)\n\u2026\nN. A picture of cat. (0.63)\nRelationship Triplets\n1. {'table', 'chair', 'left'},\n2. {'bed', ('lamp', 'mini fridge'), 'between'}\nTemplate-based Referral\n1. The table is to the left of the chair.\n2. It\u2019s a bed in the middle of a lamp and the mini fridge.\nSummary\nPrompt: Provide a summary for a scene from a given \nscene graph delimited by triple backticks, \u2026\nResponse: In this bedroom, there are two nightstands, ... \nThe backpack is in front of the nightstand as well. The room \nappears to be functional, with the nightstands providing \nstorage space and the telephone for communication. \nSummary\nPrompt: Summarize the captions below. The summary    \nshould be a description of the {object}. Focus on the \n{object}\u2019s attributes, like color, shape, material, etc. \nIdentify and correct the potential errors \u2026\nResponse: The bed is in a hotel room with a striped  \ncomforter. It has a white comforter and a blanket on it. \nThe bed is also in a room with a bedside table.\nRephrasing\nPrompt: Rewrite the following sentence using one random \nsentence structure. Focus on the location and relationships \nabout the {target_object}, \u2026\nResponse:\n1. The table is situated to the left of the armchair.\n2. The bed occupies the space between the lamp and the \nmini fridge, creating a cozy atmosphere.\nMultiview Images\nType-token ratio\nUnique\nwords\nTotal\nwords\nAverage\nwords\nAnnotated\nLLM-refined\nTemplate-based\nN-gram\nentropy\nProcTHOR (36K)\nStructured3D (21K)\nARKitScenes (4K)\nHM3D (2K)\nScanNet (1K)\n3RScan (1K)\nMultiScan (0.2K)\nTemplate-based (1.3M)\nLLM-refined (1M)\nAnnotated (19K)\n3D Sub-graph\nFigure 2. SCENEVERSE collection and statistics. Given a 3D scene (a), our automated pipeline (c) generates three types of description\nincluding scene caption, object caption and object referral. (b) The comparison of different language sources and data composition.\nanchor object is utilized to establish the view direction. The\ndistance between the two objects is also calculated to de-\nscribe whether the objects are far or near in space.\nMulti-object Relationships\nThis models the spatial ar-\nrangement of multiple objects, e.g., align and between.\nThe node hierarchy is decided by the support relation-\nship. We traverse all the object nodes to calculate spatial\nrelationships, which undergo an automatic verification proce-\ndure to rectify incorrect ones. For a more detailed description\nof the scene graph construction and relationship determina-\ntion, please refer to Appendix A.2.\n3.3. Language Generation with LLMs\nThe scene-language pairs in SCENEVERSE aim to capture\nvarying aspects of the 3D scene, which include detailed\nobject attribute descriptions in object captioning, spatial rela-\ntionships between objects in object referral, and global scene\ndescriptions in scene captioning. Based on the 3D scene\ngraph, we utilize both templates and LLMs to automatically\ngenerate descriptions on these three granularities.\nObject Captioning\nObject captions aim to provide de-\ntailed descriptions of an object\u2019s visual and physical prop-\nerties, facilitating object-level grounding with its distinctive\nfeatures. Given the multi-view images, we utilize the point\ncloud of the object v P V to identify its occurrence in the\nimages through rendering. The images are then cropped\nwith the rendered bounding boxes and processed through\nBLIP2 [48] to generate initial object captions. To refine the\ncaptions, we select the top 10 sentences with the highest\nCLIP [66] similarity score and minimal occlusion. The se-\nlected sentences are fed into a LLM to obtain a coherent\nsummary of the object captions. In this process, we explic-\nitly instruct the language model to identify and correct the\npotential errors. The detailed object captioning pipeline is\nillustrated in Appendix A.3.\nObject Referral\nObject relationship captions refer to\nobjects by articulating their spatial relationships in the\nscene.\nSpatial relationship triplets pvi, vj, eijq are first\nextracted from the constructed 3D scene graph. We de-\nsign various templates to generate descriptions for each\nrelationship type, assigning the entities in the form of\nptarget-object, spatial-relation, anchor-object(s)q.\nThis re-\nsults in examples like \u201cthe chair is next to the armchair\u201d,\n\u201cfacing the sofa, there is a suitcase far to the right of the\nshoes\u201d, and \u201cthe fridge is between cabinet and sofa\u201d. To\nadd complexity to the template-based descriptions, we de-\nsign \"star-reference\" templates, where the reference to the\ntarget object is generated by describing its relationship to 3\nrandomly chosen adjacent objects in the scene graph. Our\ndesigned templates span passive and active tenses, as well\nas inversion clauses, contributing to the richness of the gen-\nerated text. To enhance the naturalness of the descriptions,\nwe employ LLM for sentence rephrasing. Fig. 2 presents\nstatistics for the descriptions before and after rephrasing.\n4\nScene Captioning\nThe scene-level captions emphasize\nglobal information, portraying the key objects in the scene\nalong with their attributes and functionalities. We lever-\nage the constructed 3D scene graph and prompt LLMs to\ngenerate these captions. To enhance the diversity of scene\ncaptions, we utilize a subgraph sampling strategy, where\na subset of edges and nodes are randomly sampled as the\nscene context. The object counts are also provided as LLM\nprompts, together with the room type and object attributes if\nsuch annotations are available in the dataset.\n3.4. Referral Annotation by Humans\nIn addition to automatically generated scene-text pairs,\nSCENEVERSE includes the most comprehensive set of\nhuman-annotated, context-rich object referrals to date, serv-\ning as a valuable benchmark for assessing grounded scene\nunderstanding capabilities. The human annotations contain\n96, 863 descriptions in ARKitScenes [9], HM3D [67] and\nMultiScan [58]. During the annotation process, one human\nannotator was assigned to write at least 20 words to dis-\ntinctly refer to a single 3D object within a 3D scene. Each\nreferral text then undergoes independent verification by two\nadditional reviewers, both mandated to accurately locate\nthe referenced object based on the 3D scene and the anno-\ntated referral text. Any object referrals that do not pass the\nverification by either reviewer are flagged for re-annotation.\n3.5. Details and Statistics\nIn total, SCENEVERSE comprises a total of 68, 406 room-\nlevel 3D scans, with the source composition shown in\nFig. 2 (b). The dataset contains 1.5M object instances, com-\nprising 21 types of relationships following prior work [1, 79].\nFor the language descriptions, we generate 1M template-\nbased texts and 1M sentences by LLM rephrased by\nLlama [75] and GPT-3.5 [61]. All the rephrasing and sum-\nmary prompts, along with the complete set of relationships,\nare detailed in Appendix A.3. To verify the efficacy of our au-\ntomated language generation pipeline, we conduct a quality\ncheck (QC) where 12K generated object-level descriptions\nare randomly selected for human verification, achieving a\n96.93% pass rate. This shows the capability of our pro-\nposed scene-graph-based generation approach to produce\nhigh-quality language descriptions, laying a robust founda-\ntion for future scalability.\n4. Grounded Pre-training for Scenes\nIn this section, we introduce GPS, an efficient transformer-\nbased model trained with multi-level contrastive losses for\naligning 3D scenes and texts. As shown in Fig. 3, we echo\nthe language descriptions collected at different levels to form\nscene-language pairs at both object-level, referral-object-\nlevel, and scene-level for contrastive objectives in GPS. We\ndescribe the design of each level in the following sections.\nScene Caption\nMax-Pool\nLanguage\nEncoder\n!\nSpatial\nAttention\nTransformer Encoder\n642bdCdAi8WYkzscFb6fj0b5Zvqz3gpIJKg0hGOta54bmkaMlWGE0GqHmkaYtLDHVqzVGJBdSOeXDxAJ1ZpoXagbEmDJurviRgLrfvCt50Cm6e98biGfLFf34tMu3rRsxkGBkqyXRZO+LIBGgcAWoxRYnhfUswUczei0gXK0yMDSplg/Dm314k5fOsd5m9KHiZ3A1MkYQDOIZT8OAKcnAHeSgBAQkvMIRX58kZOm/O+7Q14cxm9uEPnI8fYFiV9g=</latexit><latexit sha1_base64=\"VbyZveP+pqxAzjITRIeUPcA8Yqo=\">AB9XicbVDLSgNBEOyNrxhfUY+KDAbBg4RdEfUY9OIxgbwiWF2MkmGzMwuM7NKWHL0D7zGk3j1mi8R/AZ/wsnjoIkFDUVN91dfsiZNq75SWldW15LrqY3Nre2d9O5eWQeRIrREAh6oqo815UzSkmG02qoKBY+pxW/dzv2K49UaRbIoumHtCFwR7I2I9hY6b7ui7gzeIiLg2Y\ngT\nQPNE29KMrn9YeH7+WCYb6Q/a82ARIJKQzjWuq5oanHWBlGO2napGmISZd3KZVSyUWVNfj8cV9dGSVJmoFypY0aKz+noix0LonfNspsOnoW8kniBf/OdXI9O6rMdMhpGhkyWtSKOTIBGEaAmU5QY3rME8XsvYh0sMLE2KBSNghv9u15UjrNeufZs4KXyV3BEnYg0M4Bg8uIAc3kIciEJDwAgN4dZ6cgfPmvE9aE850Zhf+wPn4AV7PlfU=</latexit><latexit sha1_base64=\"BH4SbmokEGNFUF9edhR/HWwYwU=\">AB9XicbVDLSgNBEOyNrxhfUY+KDAbBg4RdEfUY9OIxQfPAJIbZySQZMjO7zMwqYcnRP/AaT+LVa75E8Bv8CSePgyYWNBRV3XR3+SFn2rjul5NYWFxaXkmuptbWNza30ts7JR1EitAiCXigKj7WlDNJi4YZTiuholj4nJb97vXILz9SpVkg70wvpHWB25K1GMHGSvc1X8Tt/kN82+kM27WH\ngS\n<latexit sha1_base64=\"4HEXhYdFV4N2niqRMOVd5+M8y/Q=\">AB+XicbVDLSsNAFL3xWVsfra7EzWAQXEhJRNRl0Y3LivYBTVom0k7dCYJMxOhP6H27oSt/6L4MpP8BecPhbaeuDC4\nJiTZh5U0Q7uLby6R+XnYvyxf3rl25gRlycATHcAouXEF7qAKNSAg4RnG8GJl1th6td5mrSvWfOYA/sB6/wE0NZbL</latexit>Zx7ufeIOFMacf5tFZW19Y3NnNb+cL2zu5esbRfV3EqCa2RmMeyGWBFOYtoTPNaTORFIuA0YwuJ34jScqFYujRz1MqC9wL2IhI1gbqe1lXiCy/qjD2g/eqFO0nbIzBVom7pzYFfvwqyS/C9VO8cPrxiQVNKEY6VarpNoP8NSM8LpKO+liaYDHCPtgyNsKDKz6ZXj9CJUbojKWpSKOp+nsiw0KpoQhMp8C6rxa9iXiGAvGf30p1eO1nLEpSTSMyWxamHOkYTWJAXSYp0XxoCaSmXsR6WO\n{hS\ni }\n<latexit sha1_base64=\"aewxIQH82iw4GJ2PXDqujvFkBAI=\">ACBXicbVA9SwNBEN3z28SPqI1gsxgFCwl3ImoZtLGwiGA0kISwt5lLlux9sDsXDMfZ+kdstZK0\nFv4HwR+itZuPQhMfDzem2FmnhtJodG2P62Z2bn5hcWl5Ux2ZXVtPbexeavDWHEo81CGquIyDVIEUEaBEiqRAua7Eu7czsXAv+uC0iIMbrAXQd1nrUB4gjM0UiO3XfMZtjmTyVXaSGoI95go8NK0kcvbBXsIOk2cMckX976793sd6mR+6g1Qx7ECXTOuqY0dYT5hCwSWkmVqsIWK8w1pQNTRgPuh6MvwgpftGaVIvVKYCpEP190TCfK17vms\n6B/fqSW8gHlLX/8+vxuid1RMRDFCwEfLvFhSDOkgEtoUCjKniGMK2HupbzNFONogsuYIJzJt6fJ7VHBOSkcXzv54jkZYnskF1yQBxySorkpRImXDyQJ7IM3mxHq1n69Xqj1pnrPHMFvkD6+0HMwedRA=</latexit>Lref\n<latexit sha1_base64=\"dKDhk3aGncI07MjdEjyDFZjdPw4=\">AB+XicbVDLSsNAFL3xWVsfra7EzWAQXEhJRNRl0Y3LivYBTVom0k7dCYJMxOhP6H27oSt/6L4MpP8BecPhba\naD40BPJzL2I9LHERJuw8iYId/HtZVI/L7uX5Yt7167cwAw5OIJjOAUXrqACd1CFGhCQ8AxjeLEya2y9Wm+z1hVrPnMAf2C9/wAxF5bJ</latexit>euDC4Zx7ufeIOFMacf5tFZW19Y3NnNb+cL2zu5esbRfV3EqCa2RmMeyGWBFOYtoTPNaTORFIuA0YwuJ34jScqFYujRz1MqC9wL2IhI1gbqe1lXiCycNRh7Qdv1CnaTtmZAi0Td07sin34VZLfhWqn+OF1Y5IKGmnCsVIt10m0n2GpGeF0lPdSRNMBrhHW4ZGWFDlZ9OrR+jEKF0UxtJUpNFU/T2RYaHUASmU2DdV4veRDxDgfjPb6U6vPYzFiWphGZLQtTjnSMJjGgLpOU\n{f S\ni }\n<latexit sha1_base64=\"uisQyhnM80V6NjRGnzsS8QLxh8=\">ACB3icbVA9SwNBEN3zM8avqGChzWIQLCTciahl0MbCIgHzAUkIe5tJXNzbO3bnxHAc2PpH7EQrsRX8DYKNv8XNR6HRBwOP92aYmedHUh03U9nanpmdm4+s5BdXFpeWc2trVdNGsOFR7KUNd9ZkAKBRUKEeaWCBL6HmX58N/NoNaCNCdYn9CFoB6ynRFZyhldq5rWb\nA8IozmVyk7aSJcIuJ4aAgTdu5vFtwh6B/iTcm+eJm+Us8nr6X2rmPZifkcQAKuWTGNDw3wlbCNAouIc02YwMR49esBw1LFQvAtJLhDyndtUqHdkNtSyEdqj8nEhY0w982zm42Ex6A3Gf+sF/fiPG7krESqKERQfLevGkmJIB6HQjtDAUfYtYVwLey/lV0wzja6rA3Cm3z7L6keFLyjwmHZyxdPyQgZsk12yB7xyDEpknNSIhXCyR15IE/k2bl3npwX53XUOuWMZzbILzhv3+wonYE=</latexit>Lscene\n<latexit sha1_base64=\"re+Md+zJwh+rfnNVmD0VKUd6e6Y=\">ACBXicbVC7SgNBFJ2Nr5j4iNoINoNRsJCwK6KWQRsLiwjmAckSZiezyZjZBzN3g2FZW3/ENlaS\n1sJ/EPwQrZ08Ck08cOFwzr3ce48TCq7AND+N1MLi0vJKejWTXVvf2MxtbVdUEnKyjQgaw5RDHBfVYGDoLVQsmI5whWdbpXI7/aY1LxwL+Dfshsj7R97nJKQEvN3G7DI9ChRMQ3STNuAHuAOHDuk6SZy5sFcw8T6wpyRcPvobvex3qZn7aLQCGnMByqIUnXLDMGOiQROBUsyjUixkNAuabO6pj7xmLj8QcJPtRKC7uB1OUDHqu/J2LiKdX\n3HN05ulfNeiPxGDvef349AvfCjrkfRsB8OlnmRgJDgEeR4BaXjILoa0Ko5PpeTDtEgo6uIwOwp9e5UTgrWeH01soXL9EabSH9tERstA5KqJrVEJlRNEjekYD9GI8GQPj1RhOWlPGdGYH/YHx9gMv6p1C</latexit>Lobj\nObject Captions\nLanguage\nEncoder\n\u2744\nObject PCD\nEncoder\nObject PCDs\nfJfldqHaLH14vJqmgkSYcK9V2nUT7GZaEU7HeS9VNMFkiPu0bWiEBV+Nrt6jM6M0kNhLE1FGs3U3xMZFkqNRGA6BdYDtexNxQsUiP/8dqrDWz9jUZJqGpH5sjDlSMdoGgPqMUmJ5iNDMJHM3IvIAEtMtAkrb4Jwl9eJY3Lsntdvnp07codzJGDEziFc3DhBirwAFWoAwEJzCBFyuzJtar9TZvXbMWM0fwB9b7DzKhlso=</latexit><latexit sha1_base64=\"DGR839j+HBgL6qsfiNSuiJxMpn4=\">AB+XicbVDLSsNAFL3xWVsfra7EzWAQXEhJRNRl0Y3LCn1Bk5bJdNIOnUnCzEQof/htq7Erf8iuPIT/AWnj4W2HrhwOde7r0nSDhT2nE+rbX1jc2t7dxOvrC7t39QLB02VJxKQusk5rFsBVhRziJa10xz2kokxSLgtBkM76d+84lKxeKopkcJ9QXuRyxkBGsjdbzMC0QWjrusU/PG3aLtlJ0Z0CpxF8Su2Md\n{f T\ni }\n<latexit sha1_base64=\"zPrq4nSJMv/v3T+in0iwCNMZuc=\">AB+HicbVBNS8NAEJ3Ur1q/qh69BIvgQUoiRT0WvXizgv2AJi2b7aZdursJuxuhvwPj+pFvPpfPhv3LY5aOuDgcd7M\nvl0nrvOpeVGv3tUr9Ok+kCEdwDKfgwiXU4RYa0AQMEp7hFd6sJ+vFerc+5q0FK585hD+wPn8AyoGTyg=</latexit>8zMC2JGlXacb6uwsrq2vlHcLG1t7+zulfcPWipKJCZNHLFIdgKkCKOCNDXVjHRiSRAPGkH45up34kUtFIPOhJTHyOhoKGFCNtpJ6XegFPw6xPe3de1i9XnKozg71M3JxUIEejX/7yBhFOBEaM6RU13Vi7adIaoZyUpeokiM8BgNSdQgThRfjq7OrNPjDKw0iaEtqeqb8nUsSVmvDgLOCmSM9Uov2VPzP6yY6vPJTKuJE4Hnu8KE2TqypynYAyoJ1mxiCMKSmnNtPEISYW2yKpkc3MW\n{f O\ni }\nObject Referral\nLanguage\nEncoder\n!\n<latexit sha1_base64=\"PTlBuSiz1Tj4BKoJjg9L9meor2U=\">ACHXichVDLSgMxFM3UV62vUVfiJlgEF1JmRKy4KrpxWcE+YDqUTOa2Dc1khiQjlKF/IW79Ebd1J\nS4V/8b0sdBW8FwuHM65l+SeIOFMacf5snJLyura/n1wsbm1vaOvbtXV3EqKdRozGPZDIgCzgTUNMcmokEgUcGkH/Zuw3HkAqFot7PUjAj0hXsA6jRBupbTutVIQgA0koZC38X+Fh2y46JWcCvEjcGSlWrg4ePV4m1b90QpjmkYgNOVEKc91Eu1nRGpGOQwLrVRBQmifdMEzVJAIlJ9NLhviY6OEuBNL0LjifpzIyORUoMoMJMR0T01743FUx\nxEf/leqjuXfsZEkmoQdPpYJ+VYx3gcFQ6ZBKr5wBCJTP/xbRHTFLaBFowQbjzZy+S+lnJvSid37nFyjWaIo8O0RE6QS4qowq6RVUQxQ9oRc0Qq/WszWy3qz36WjOmu3so1+wPr8B1nCgFQ=</latexit>|\n{z\n}\n<latexit sha1_base64=\"DnM+ItXDtu/TXL9DcZWYblTbz0w=\">ACBXicbVDJSgNBEO2JW4xbVBDBS2MQPEiYEVGPIV48JCAWSAJQ0+nJ2nSs9BdI4ZhvPojXuNJ\nvHrxGwQvfoud5aCJDwoe71VRVc8JBVdgml9Gaml5ZXUtvZ7Z2Nza3snu7tVEnKajQgWw6RDHBfVYDoI1Q8mI5wjWcAY3Y79xz6TigX8Hw5B1PNLzucspAS3Z2cO2R6BPiYhLiR23gT1AXC6Vk8TO5sy8OQFeJNaM5AoH1W8+Kn5U7OxnuxvQyGM+UEGUalmCJ2YSOBUsCTjhQLCR2QHmtp6hOPqU48+SDBJ1rpYjeQunzAE/X3REw8pYa\neozvH96p5byeYcf7z29F4F53Yu6HETCfTpe5kcAQ4HEkuMsloyCGmhAqub4X0z6RhIOLqODsObfXiT187x1mb+oWrlCEU2RkfoGJ0iC12hArpFVRDFD2iZzRCL8aTMTJejbdpa8qYzeyjPzDefwDG5xF</latexit>LMLM\n<latexit sha1_base64=\"pd5MD3QTxgSzw6p78ArE+rUOwXw=\">AB9HicbVBNSwMxEJ2tX7V+VT16CRbBg5RdKeqx6MVjhX5hu5ZsmrahSXZJsoWy7L/wqF7Eq/Gg/\nHMkQnRLAHUZ4oSw6eWYKYPReREVaYGJtTwebgLX+9SpqXZe+qXHmolKq3WSJ5OIFTOAcPrqEK91CDBhCQ8Ayv8OZMnBfn3flYtOacbOY/sD5/AHj7pIp</latexit>/GtN2Dtj4YeLw3w8y8IOJMG9f9dnJr6xubW/ntws7u3v5B8fCoqcNYEdogIQ9VO8CaciZpwzDaTtSFIuA01Ywvpv5rQlVmoWybqYR9QUeSjZgBsrPXYDkYzSp6Se9olt+zOgVaJl5ESZKj1il/dfkhiQaUhHGvd8dzI+AlWhFO0I31jTCZIyHtGOpxIJqP5lfnKIzq/TRIFS2pEFz9fdEgoXWUxFcBMI2C2xGetmeif95ndgMbvyEySg2VJLFrk\nhT\n<latexit sha1_base64=\"pd5MD3QTxgSzw6p78ArE+rUOwXw=\">AB9HicbVBNSwMxEJ2tX7V+VT16CRbBg5RdKeqx6MVjhX5hu5ZsmrahSXZJsoWy7L/wqF7Eq/Gg/\nHMkQnRLAHUZ4oSw6eWYKYPReREVaYGJtTwebgLX+9SpqXZe+qXHmolKq3WSJ5OIFTOAcPrqEK91CDBhCQ8Ayv8OZMnBfn3flYtOacbOY/sD5/AHj7pIp</latexit>/GtN2Dtj4YeLw3w8y8IOJMG9f9dnJr6xubW/ntws7u3v5B8fCoqcNYEdogIQ9VO8CaciZpwzDaTtSFIuA01Ywvpv5rQlVmoWybqYR9QUeSjZgBsrPXYDkYzSp6Se9olt+zOgVaJl5ESZKj1il/dfkhiQaUhHGvd8dzI+AlWhFO0I31jTCZIyHtGOpxIJqP5lfnKIzq/TRIFS2pEFz9fdEgoXWUxFcBMI2C2xGetmeif95ndgMbvyEySg2VJLFrk\nhT\nObject-level\nScene-level\nReferral-object-level\nFigure 3. Overview of our proposed GPS model. We leverage\ncontrastive alignment in three levels Lobj, Lscene, and Lref as well as\na masked language modeling objective LMLM for model learning.\n4.1. Object-level Grounding\nGiven a 3D scene point cloud S, we use an off-the-shelf 3D\nobject segmentation model to decompose it into a bag of\nN objects S \u201c to1, o2, \u00a8 \u00a8 \u00a8 , onuN\ni\u201c1. We extract object fea-\ntures tf O\ni u with an object point cloud encoder and text fea-\ntures tf T\ni u by feeding object-captions tT obj\ni\nu into a frozen\nlanguage model. Following [83], we perform cross-modal\nalignment on the object features and text features via:\nLobj \u201c \u00b41\n2\n\u00ff\npp,qq\n\u02dc\nlog\nexp\n`\nDobjpp, qq\n\u02d8\n\u0159\nr exp pDobjpp, rqq`\nlog\nexp\n`\nDobjpp, qq\n\u02d8\n\u0159\nr exp pDobjpr, qqq\n\u00b8\n,\n(1)\nwhere Dobjpp, qq \u201c pf O\np f T\nq {\u03c4q denotes the dot product be-\ntween object and text features and pp, qq denotes a pair of\naligned object-text pair in the training batch and r iterates\nover all object-text pairs in the training batch. Similar to\nCLIP [66], we use a learnable temperature parameter \u03c4 to\nfacilitate model learning.\n4.2. Scene-level Grounding\nWith aligned object features, we encode the scene by incor-\nporating object spatial locations into the extracted object\nfeatures. Specifically, we use a spatial transformer model\nto encode extracted object features tf O\ni u with their spatial\nlocation features tliu following [18, 98]:\nf S \u201c SpatialAttnptf O\ni u, tliuq\nwhere tf S\ni u denotes the feature of object oi after encoding\nwith spatial location features. To perform scene-level align-\n5\nment, we operate on these scene-level object features tf S\ni u\nand align it with the scene caption T scene. Specifically, we\nfeed the object features into a projection layer and use max-\npooling over all object features to obtain the scene feature\ngS. Similar to object-level grounding, we pass the scene cap-\ntion through a tunable language model to obtain text feature\ngT and perform scene-level contrastive alignment through:\nLscene \u201c \u00b41\n2\n\u00ff\npp,qq\n\u02c6\nlog\nexp pDscenepp, qqq\n\u0159\nr exp pDscenepp, rqq`\nlog\nexp pDscenepp, qqq\n\u0159\nr exp pDscenepr, qqq\n\u02d9\n,\n(2)\nwhere Dscenepp, qq \u201c pgS\np gT\nq {\u03c4q denotes the dot product\nbetween scene feature gS\np and scene caption feature gT\nq for\neach pair of aligned scene-text pairs in the training batch and\nr iterates over all scene-text pairs in the training batch.\n4.3. Referral-object-level Grounding\nTo model the relationships revealed in referring expressions,\nwe employ a self-attention-based reasoning transformer for\ngrounding object referrals in scenes. This transformer takes\nin scene-object features tf S\ni u and an object referral T ref\nand performs self-attention to learn relationships between\ntext descriptions and object relationships. We use the same\ntunable language encoder as in scene-level grounding for ex-\ntracting per-object referral features. We pass this text feature\ntogether with scene-object features into the self-attention\ntransformer to obtain the aligned object features hS\ni and\nthe sentence-level referral feature hT . We then perform the\nreferral-object-level contrastive alignment following:\nLref \u201c \u00b4 log\nexp\n`\u00afhShT {\u03c4\n\u02d8\n\u0159\np exp\n`\nhSp hT {\u03c4\n\u02d8,\n(3)\nwhere \u00afhS denotes the feature of the referred object, p iterates\nover all objects within the same scene. Notably, in contrast\nto inter-scene contrast that was done in object- and scene-\nlevel alignment, we force the selection of positive pairs to\nbe within the same scene to provide intra-scene contrast for\nfine-grained object grounding. This mimics the success of\nintra-image and inter-image contrasts commonly used for\nregion-word alignment in 2D-VL models [90].\nTo learn the multi-level alignment between 3D scenes and\nlanguage, we first train the point cloud encoder with object-\nlevel grounding objects to obtain a good feature initialization\nfor grounding objects in scenes. During the scene grounding\nstage, we train our inter- and intra-scene objectives together\nwith a mask language modeling loss LMLM over the inputted\nobject-referral texts to tune the parameters within the lan-\nguage encoder and self-attention transformer. Above all, the\nlearning of GPS could be summarized as optimizing the\nfollowing objective:\nL \u201c Lobj ` Lscene ` Lref ` LMLM.\n5. Experiments\nIn this section, we present the evaluation results addressing\nthe following questions:\n\u2022 How effective is the data scaling in SCENEVERSE for 3D\nvisual grounding? Does the scale-up work for general\npre-training based 3D-VL models?\n\u2022 How well is the GPS pre-training pipeline? Does it exhibit\nsimilar properties of 2D-VL models in 3D-VL tasks?\n\u2022 What potentials are offered by SCENEVERSE and GPS\nfor future research? What is missing?\nIn the following sections, we describe in detail about model\nperformance regarding these key topics. Due to the page\nlimit, we direct readers to the Appendices B and C for im-\nplementation details and more experimental analyses.\n5.1. 3D Visual Grounding\nSettings\nWe evaluate our model on three commonly-\nused datasets for 3D visual grounding: ScanRefer [16],\nNr3D, and Sr3D [1]. For Nr3D and Sr3D, we follow Achliop-\ntas et al. [1] and report the grounding accuracies of models\nusing ground-truth object masks. For ScanRefer, we fol-\nlow Zhu et al. [98] and use Mask3D [72] to generate object\nproposals. Results are reported as Acc@0.5 to evaluate the\ncorrectness of predictions whose object bounding boxes over-\nlap the ground truth with IoU \u0105 0.5. For comparisons, we\ncompare with existing baselines by providing the results of\npre-trained GPS and dataset-specific fine-tuned GPS. Please\nsee more details in the Appendix C.\nResults and Analyses\nAs shown in Tab. 2, GPS trained\non SCENEVERSE achieves state-of-the-art results on all ex-\nisting 3D-VL grounding benchmarks. Initially, when GPS\nis trained directly on the training sets of benchmark datasets,\nlabeled as Ours (scratch), it underperforms compared to ex-\nisting models that employ more complex structures or loss\ndesigns. This result underscores the data-intensive nature\nof the contrastive alignment paradigm. However, when pre-\nsented with extensive training data in SCENEVERSE, the\nresults of our model without additional fine-tuning, i.e.,\nOurs (pre-train), significantly improves and already achieves\nstate-of-the-art results on benchmarks like ScanRefer. More-\nover, the dataset-specific fine-tuned model, i.e., Ours (fine-\ntuned), consistently outperforms existing baselines with only\na simple projection MLP added on top of the pre-trained\nmodel, jointly optimized during fine-tuning without any\nother auxiliary architecture or loss objective. These results\nunderscore the strong potential of both the SCENEVERSE\nand GPS for 3D-VL tasks.\n5.2. Zero-Shot Transfer\nSettings\nTo better evaluate the effectiveness of both the\nSCENEVERSE data and the GPS model, we further perform\nzero-shot transfer experiments to test the models\u2019 capabil-\n6\nTable 2. 3D Visual Grounding results on Nr3D, Sr3D, and ScanRefer. We use \u201cdirect\u201d for our model trained on SCENEVERSE with no\nadditional fine-tune head, and \u201cfine-tune\u201d for the data-specific fine-tuned version of our model. We highlight the best results in bold.\nMethod\nNr3D\nSr3D\nScanRefer Acc@0.5\nOverall\nEasy\nHard\nV-Dep.\nV-Indep.\nOverall\nEasy\nHard\nV-Dep.\nV-Indep.\nOverall\nUnique\nMultiple\n3DVG-Trans [94]\n40.8\n48.5\n34.8\n34.8\n43.7\n51.4\n54.2\n44.9\n44.6\n51.7\n34.7\n60.6\n28.4\nTGNN [39]\n37.3\n44.2\n30.6\n35.8\n38.0\n45.0\n48.5\n36.9\n45.8\n45.0\n29.7\n56.8\n23.2\nTransRefer3D [35]\n48.0\n56.7\n39.6\n42.5\n50.7\n57.4\n60.5\n50.2\n49.9\n57.7\n-\n-\n-\nInstanceRefer [89]\n38.8\n46.0\n31.8\n34.5\n41.9\n48.0\n51.1\n40.5\n45.8\n48.1\n32.9\n66.8\n24.7\nFFL-3DOG [31]\n41.7\n48.2\n35.0\n37.1\n44.7\n-\n-\n-\n-\n-\n34.0\n67.9\n25.7\nLAR [6]\n48.9\n58.4\n42.3\n47.4\n52.1\n59.4\n63.0\n51.2\n50.0\n59.1\n-\n-\n-\nSAT [86]\n56.5\n64.9\n48.4\n54.4\n57.6\n57.9\n61.2\n50.0\n49.2\n58.3\n30.1\n50.8\n25.2\n3D-SPS [54]\n51.5\n58.1\n45.1\n48.0\n53.2\n62.6\n56.2\n65.4\n49.2\n63.2\n37.0\n66.7\n29.8\n3DJCG [12]\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n37.3\n64.3\n30.8\nBUTD-DETR [41]\n54.6\n60.7\n48.4\n46.0\n58.0\n67.0\n68.6\n63.2\n53.0\n67.6\n39.8\n66.3\n35.1\nMVT [40]\n59.5\n67.4\n52.7\n59.1\n60.3\n64.5\n66.9\n58.8\n58.4\n64.7\n33.3\n66.5\n25.3\nViL3DRel [18]\n64.4\n70.2\n57.4\n62.0\n64.5\n72.8\n74.9\n67.9\n63.8\n73.2\n37.7\n68.6\n30.7\nEDA [82]\n52.1\n58.2\n46.1\n50.2\n53.1\n68.1\n70.3\n62.9\n54.1\n68.7\n42.3\n68.6\n37.6\n3D-VisTA (scratch) [98]\n57.5\n65.9\n49.4\n53.7\n59.4\n69.6\n72.1\n63.6\n57.9\n70.1\n41.5\n70.9\n34.8\n3D-VisTA [98]\n64.2\n72.1\n56.7\n61.5\n65.1\n76.4\n78.8\n71.3\n58.9\n77.3\n45.8\n75.1\n39.1\nOurs (scratch)\n58.7\n67.0\n50.9\n55.8\n59.8\n68.4\n70.5\n63.4\n53.1\n69.0\n40.4\n71.3\n34.7\nOurs (pre-train)\n55.2\n62.8\n48.0\n45.5\n58.8\n74.1\n76.4\n68.5\n54.1\n75.0\n47.1\n77.4\n41.6\nOurs (fine-tuned)\n64.9\n72.5\n57.8\n56.9\n67.9\n77.5\n80.1\n71.6\n62.8\n78.2\n48.1\n77.9\n42.7\nTable 3. Zero-shot transfer results on established benchmarks.\nMethod\nNr3D\nSr3D\nScanRefer@0.25\nScanRefer@0.5\n3D-VisTA (scratch)\n57.5\n69.6\n45.9\n41.5\n3D-VisTA (zero-shot)\n35.2\n31.2\n33.2\n29.6\n3D-VisTA (zero-shot text)\n43.1\n36.1\n41.1\n36.4\nOurs (scratch)\n58.7\n68.4\n44.5\n40.4\nOurs (zero-shot)\n32.4\n33.3\n35.2\n31.1\nOurs (zero-shot text)\n41.9\n38.1\n40.7\n35.8\nity in 4 benchmarks, ScanRefer, Sr3D, Nr3D, and SCEN-\nEVERSE-val. We create SCENEVERSE-val using 8.5K an-\nnotated object referrals of 271 scenes in MultiScan, and\nrandomly split the scenes following a 4:1 train / test split\nfor creating the held-out test set. We mainly consider 2\nspecific transfer settings in our experiments: (i) zero-shot:\nmodels trained by removing all the scenes from the target\ndataset, tested on held-out unseen scenes, and (ii) zero-shot\ntext: Models trained on data that include the 3D scenes from\ntraining set of the target dataset, yet tested exclusively with\nunseen scene-text distribution. Specifically, for the zero-shot\ntext setting, we use the generated texts in SCENEVERSE as\nfine-tuning sources for the zero-shot model. We mainly com-\npare our model against a recent pre-training-based model\n3D-VisTA. See more details on experimental setting and\nimplementation in the Appendix C.\nResults and Analyses\nWe present the results of zero-\nshot transfer experiments in Tab. 3 and Tab. 4 with the fol-\nlowing key observations:\n\u2022 Our GPS model demonstrates superior generalization to\nunseen scenes compared to the 3D-VisTA model.\nIn\nzero-shot transfer scenarios, our model consistently out-\nperforms 3D-VisTA across established benchmarks and\nTable 4. Zero-shot transfer on SCENEVERSE-val. We evaluate\nmodels following settings in Nr3D/Sr3D using GT object proposals.\nMethod\nOverall\nEasy\nHard\nV-Dep.\nV-Indep.\n3D-VisTA (scratch)\n40.7\n53.1\n21.6\n37.3\n44.3\n3D-VisTA (zero-shot)\n52.9\n59.6\n35.4\n53.7\n52.2\n3D-VisTA (zero-shot text)\n58.1\n70.0\n39.6\n52.5\n64.1\nOurs (scratch)\n38.5\n50.2\n20.8\n33.7\n43.9\nOurs (zero-shot)\n59.2\n69.4\n44.0\n53.1\n66.3\nOurs (zero-shot text)\n60.6\n70.9\n45.1\n54.8\n67.3\nSCENEVERSE-val. This indicates the effectiveness of con-\ntrastive alignment over traditional classification objectives,\naligning with the advancements seen in 2D-VL models for\nopen-vocabulary grounding and transfer capabilities\n\u2022 SCENEVERSE dataset substantially enhances 3D-VL\ngrounding capabilities through zero-shot transfer, espe-\ncially when provided with relatively limited training data,\ni.e., SCENEVERSE-val. As demonstrated in Tab. 4, there\nis a significantly improved performance when comparing\nmodels trained on SCENEVERSE in a zero-shot manner\nto those trained from scratch. This indicates that SCEN-\nEVERSE can effectively capture knowledge for general\n3D scene grounding. Consequently, this underscores its\npotential as a go-to pre-training dataset for 3D-VL tasks.\n\u2022 The impact of our extensive collection and scalable gener-\nation of scene-text pairs is further evidenced by the results\nin the zero-shot text setting. Notably, as shown in Tab. 3,\nthese automatically generated scene-text pairs supply am-\nple knowledge for comprehending the scene distribution.\nThis contributes significantly to the substantial improve-\nment over the zero-shot performance.\n7\n12.5\n25\n50\n100\nPercentage of data (%)\n20\n30\n40\n50\nAcc@0.25 (%)\nScanRefer\npre-train\nzero-shot\n12.5\n25\n50\n100\nPercentage of data (%)\n30\n40\n50\n60\nAccuracy (%)\nSceneVerse-val\npre-train\nzero-shot\nFigure 4. Model performance v.s. data scale. Models consistently\nimprove in both the pre-train and zero-shot transfer settings on\nScanRefer and SCENEVERSE-val with data scaling-up.\n5.3. Ablative Studies and Discussions\nIn this section, we present ablative studies focused primarily\non the data collected in SCENEVERSE. Our aim is to eluci-\ndate the effects of data scaling more clearly. For discussions\nregarding ablation studies on model architecture, readers\nare referred to the Appendix D. The following points are\nspecifically discussed in this section.\nHow important is data-scaling?\nWe conduct ablation\nstudies over the amount of data used while pre-training GPS.\nWe consider the model trained with 1\n8, 1\n4, 1\n2 of SCENEV-\nERSE to show the effectiveness of data-scaling on model\nperformance in the pre-train and zero-shot transfer settings\nin ScanRefer and SCENEVERSE-val. As shown in Fig. 4,\nwe observe consistent performance improvement over the in-\ncrease of data scale for both settings. We provide additional\nexperiments in the Appendix D to show that such scaling\neffect is not only beneficial for 3D-VL grounding but also\nfor other 3D tasks like semantic segmentation [72, 85].\nHow is the generated data compared with human-\nannotated data?\nWe assess the performance of models\ntrained using various scene-text sources, specifically focus-\ning on their performance in the ScanRefer dataset without\nadditional fine-tuning. As shown in Tab. 5, models trained\nwith our template-based generated texts and Large Language\nModel (LLM)-refined texts show significant improvements\nover models trained solely on ScanRefer. More importantly,\nthese variants of our model already achieve state-of-the-art\nresults compared with previous baselines. This indicates the\neffectiveness of our text-generation pipeline. Finally, we ob-\nserve that adding human-annotated data is still beneficial for\nmodel performance. However, the improvement is relatively\nmarginal over models trained on our generated data.\nWhat is the role of the synthetic scenes in this scale-up\nprocess?\nWith synthetic data providing large-scale and\ndiverse scene data for 3D-VL tasks, we evaluate the mod-\nels\u2019 domain transfer (Sim2Real) capability. Specifically, we\ncompare models trained on all real scenes in SCENEVERSE\nagainst models trained exclusively on two synthetic sub-\nTable 5. Ablation on scene-text pair types used in training. We\nreport model results on ScanRefer with no additional finetuning.\nTemplate\nLLM\nAnno.\nAcc@0.25\nAcc@0.5\n\u2717\n\u2717\n\u2717\n43.5\n38.4\n\u2713\n\u2717\n\u2717\n50.9\n46.1\n\u2713\n\u2713\n\u2717\n51.1\n46.3\n\u2713\n\u2713\n\u2713\n52.0\n47.1\nTable 6. Cross domain transfer results of models learned in real\nand synthetic datasets without additional fine-tuning. \u201cS3D\u201d stands\nfor Structured3D.\nReal\nSynthetic\nSCENEVERSE-val\nS3D\nProcTHOR\nAll\n\u2717\n64.8\n37.1\n43.4\n\u2717\nS3D\n7.0\n85.1\n16.1\n\u2717\nProcTHOR\n4.2\n16.3\n91.0\nsets of SCENEVERSE, i.e., Structured3D and ProcTHOR.\nAs shown in Tab. 6, models trained on synthetic subsets\ndemonstrate remarkable performance on their correspond-\ning test sets while suffering when transferred to real or other\nsynthetic scenes. In contrast, the model trained on real scene-\ntext pairs exhibits less severe performance drops when gen-\neralizing to synthetic scenes. This result affirms the domain\ngap between real and synthetic scenes in 3D-VL grounding\nand shows that a simple scale-up in the amount of scenes\nis insufficient when the scene naturalness can not be guar-\nanteed. Considering the scalability of our quality-ensured\nlanguage generation and also the scaling effect shown in our\nexperiments, the rate-determining step for further scaling-up\n3D-VL comes to the collection of diverse, high-quality, and\nrealistic scenes that capture natural 3D scene distributions.\n6. Conclusion\nIn this work, we scale up 3D-VL in the context of grounded\nscene understanding. We introduce SCENEVERSE, a million-\nscale 3D-VL dataset encompassing various scenes and multi-\nlevel scene descriptions sourced from both human annotation\nand our proposed scene-text generation approach. Utiliz-\ning SCENEVERSE, we propose Grounded Pre-training for\nScenes, a model trained with multi-level scene-language con-\ntrastive alignment over the data collected. Through extensive\nexperiments, we show that GPS achieves state-of-the-art\nresults on all existing 3D-VL grounding tasks. We further\nconduct zero-shot transfer experiments to show the improved\ngeneralization performances of GPS trained on SCENEV-\nERSE compared with previous baselines. We hope our efforts\nand successful scale-up attempts in SCENEVERSE could\npave the way for a new research paradigm in 3D-VL.\n8\n7. Acknowledgement\nThe authors thank Yaowei Zhang from BIGAI for designing\nthe result visualization framework, Jiangyong Huang and\nXiongkun Linghu from BIGAI for suggestions on data gen-\neration and refinement, and colleagues from BIGAI for their\nhelpful discussions and suggestions.\nReferences\n[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed\nElhoseiny, and Leonidas Guibas. Referit3d: Neural listeners\nfor fine-grained 3d object identification in real-world scenes.\nIn Proceedings of European Conference on Computer Vision\n(ECCV), 2020. 2, 3, 5, 6, 1\n[2] Christopher Agia, Krishna Murthy Jatavallabhula, Mohamed\nKhodeir, Ondrej Miksik, Vibhav Vineet, Mustafa Mukadam,\nLiam Paull, and Florian Shkurti. Taskography: Evaluating\nrobot task planning over large 3d scene graphs. In Proceed-\nings of Conference on Robot Learning (CoRL), 2022. 3\n[3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine\nMiech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,\nKatherine Millican, Malcolm Reynolds, et al. Flamingo: a\nvisual language model for few-shot learning. In Proceed-\nings of Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 2, 3\n[4] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir R Zamir,\nMartin Fischer, Jitendra Malik, and Silvio Savarese. 3d scene\ngraph: A structure for unified semantics, 3d space, and cam-\nera. In Proceedings of International Conference on Computer\nVision (ICCV), 2019. 2, 3\n[5] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki\nKawanabe. Scanqa: 3d question answering for spatial scene\nunderstanding. In Proceedings of Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 2, 3\n[6] Eslam Bakr, Yasmeen Alsaedy, and Mohamed Elhoseiny.\nLook around and refer: 2d synthetic semantics knowledge\ndistillation for 3d visual grounding. In Proceedings of Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2022. 3, 7\n[7] Lawrence W Barsalou. Perceptual symbol systems. Behav-\nioral and brain sciences, 22(4):577\u2013660, 1999. 2\n[8] Lawrence W Barsalou. Grounded cognition. Annu. Rev.\nPsychol., 59:617\u2013645, 2008. 2\n[9] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry,\nYuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel\nKurz, Arik Schwartz, et al. Arkitscenes: A diverse real-world\ndataset for 3d indoor scene understanding using mobile rgb-d\ndata. In Proceedings of Advances in Neural Information Pro-\ncessing Systems Datasets and Benchmarks (NeurIPS Datasets\nand Benchmarks Track), 2021. 2, 3, 5, 1\n[10] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Alt-\nman, Simran Arora, Sydney von Arx, Michael S Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.\nOn the opportunities and risks of foundation models. arXiv\npreprint arXiv:2108.07258, 2021. 2\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. Language\nmodels are few-shot learners. In Proceedings of Advances in\nNeural Information Processing Systems (NeurIPS), 2020. 2,\n3\n[12] Daigang Cai, Lichen Zhao, Jing Zhang, Lu Sheng, and Dong\nXu. 3djcg: A unified framework for joint dense captioning\nand visual grounding on 3d point clouds. In Proceedings\nof Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 7\n[13] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal-\nber, Matthias Niessner, Manolis Savva, Shuran Song, Andy\nZeng, and Yinda Zhang. Matterport3d: Learning from rgb-d\ndata in indoor environments. Proceedings of International\nConference on 3D Vision (3DV), 2017. 2\n[14] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat\nHanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis\nSavva, Shuran Song, Hao Su, et al. Shapenet: An information-\nrich 3d model repository. arXiv preprint arXiv:1512.03012,\n2015. 2\n[15] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu\nSoricut. Conceptual 12m: Pushing web-scale image-text pre-\ntraining to recognize long-tail visual concepts. In Proceedings\nof Conference on Computer Vision and Pattern Recognition\n(CVPR), 2021. 2, 3\n[16] Dave Zhenyu Chen, Angel X Chang, and Matthias Nie\u00dfner.\nScanrefer: 3d object localization in rgb-d scans using nat-\nural language. In Proceedings of European Conference on\nComputer Vision (ECCV), 2020. 2, 3, 6, 1\n[17] Dave Zhenyu Chen, Qirui Wu, Matthias Nie\u00dfner, and An-\ngel X Chang. D3net: a speaker-listener architecture for semi-\nsupervised dense captioning and visual grounding in rgb-d\nscans. In Proceedings of European Conference on Computer\nVision (ECCV), 2022. 3\n[18] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi,\nCordelia Schmid, and Ivan Laptev. Language conditioned\nspatial relation reasoning for 3d object grounding. In Proceed-\nings of Advances in Neural Information Processing Systems\n(NeurIPS), 2022. 3, 5, 7, 1\n[19] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu,\nand Tao Chen. End-to-end 3d dense captioning with vote2cap-\ndetr. In Proceedings of Conference on Computer Vision and\nPattern Recognition (CVPR), 2023. 3\n[20] Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, and Angel X\nChang. Scan2cap: Context-aware dense captioning in rgb-d\nscans. In Proceedings of Conference on Computer Vision and\nPattern Recognition (CVPR), 2021. 3\n[21] Zhenyu Chen, Ronghang Hu, Xinlei Chen, Matthias Nie\u00dfner,\nand Angel X Chang. Unit3d: A unified transformer for 3d\ndense captioning and visual grounding. In Proceedings of\nInternational Conference on Computer Vision (ICCV), 2023.\n5\n[22] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar\nLuthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas F Yago\nVicente, Thomas Dideriksen, Himanshu Arora, et al. Abo:\nDataset and benchmarks for real-world 3d object understand-\ning. In Proceedings of Conference on Computer Vision and\nPattern Recognition (CVPR), 2022. 2\n[23] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber,\nThomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-\nannotated 3d reconstructions of indoor scenes. In Proceedings\n9\nof Conference on Computer Vision and Pattern Recognition\n(CVPR), 2017. 2, 3, 1\n[24] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat\nTiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung,\nand Steven Hoi. Instructblip: Towards general-purpose vision-\nlanguage models with instruction tuning.\narXiv preprint\narXiv:2305.06500, 2023. 3\n[25] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo,\nOscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte,\nVikram Voleti, Samir Yitzhak Gadre, et al. Objaverse-xl: A\nuniverse of 10m+ 3d objects. In Proceedings of Advances in\nNeural Information Processing Systems (NeurIPS), 2023. 2\n[26] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs,\nOscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani,\nAniruddha Kembhavi, and Ali Farhadi. Objaverse: A universe\nof annotated 3d objects. In Proceedings of Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 2\n[27] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs,\nKiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve,\nAniruddha Kembhavi, and Roozbeh Mottaghi.\nProcthor:\nLarge-scale embodied ai using procedural generation. In\nProceedings of Advances in Neural Information Processing\nSystems (NeurIPS), 2022. 2, 3, 1\n[28] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional transform-\ners for language understanding. In Proceedings of Conference\nof the North American Chapter of the Association for Compu-\ntational Linguistics (NAACL), 2018. 3\n[29] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song\nBai, and Xiaojuan Qi. Pla: Language-driven open-vocabulary\n3d scene understanding. In Proceedings of Conference on\nComputer Vision and Pattern Recognition (CVPR), 2023. 3\n[30] Zhipeng Ding, Xu Han, and Marc Niethammer. Votenet: A\ndeep learning label fusion method for multi-atlas segmenta-\ntion. In Proceedings of International Conference on Medical\nImage Computing and Computer-Assisted Intervention (MIC-\nCAI), 2019. 3\n[31] Mingtao Feng, Zhen Li, Qi Li, Liang Zhang, XiangDong\nZhang, Guangming Zhu, Hui Zhang, Yaonan Wang, and Aj-\nmal Mian. Free-form description guided 3d visual graph\nnetwork for object grounding in point cloud. In Proceed-\nings of International Conference on Computer Vision (ICCV),\n2021. 7\n[32] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling\nopen-vocabulary image segmentation with image-level labels.\nIn Proceedings of European Conference on Computer Vision\n(ECCV), 2022. 3\n[33] Qiao Gu,\nAlihusein Kuwajerwala,\nSacha Morin,\nKr-\nishna Murthy Jatavallabhula, Bipasha Sen, Aditya Agarwal,\nCorban Rivera, William Paul, Kirsty Ellis, Rama Chellappa,\net al. Conceptgraphs: Open-vocabulary 3d scene graphs for\nperception and planning. arXiv preprint arXiv:2309.16650,\n2023. 3\n[34] Huy Ha and Shuran Song. Semantic abstraction: Open-world\n3d scene understanding from 2d vision-language models. In\nProceedings of Conference on Robot Learning (CoRL), 2022.\n3\n[35] Dailan He, Yusheng Zhao, Junyu Luo, Tianrui Hui, Shaofei\nHuang, Aixi Zhang, and Si Liu. Transrefer3d: Entity-and-\nrelation aware transformer for fine-grained 3d visual ground-\ning. In Proceedings of ACM International Conference on\nMultimedia (MM), 2021. 3, 7\n[36] Deepti Hegde, Jeya Maria Jose Valanarasu, and Vishal Pa-\ntel. Clip goes 3d: Leveraging prompt tuning for language\ngrounded 3d recognition. In Proceedings of International\nConference on Computer Vision (ICCV), 2023. 3, 1\n[37] Yining Hong, Chunru Lin, Yilun Du, Zhenfang Chen,\nJoshua B Tenenbaum, and Chuang Gan. 3d concept learn-\ning and reasoning from multi-view images. In Proceedings\nof Conference on Computer Vision and Pattern Recognition\n(CVPR), 2023. 3\n[38] Yicong Hong, Qi Wu, Yuankai Qi, Cristian Rodriguez-Opazo,\nand Stephen Gould.\nVln bert: A recurrent vision-and-\nlanguage bert for navigation. In Proceedings of Conference\non Computer Vision and Pattern Recognition (CVPR), 2021.\n3\n[39] Pin-Hao Huang, Han-Hung Lee, Hwann-Tzong Chen, and\nTyng-Luh Liu. Text-guided graph neural networks for re-\nferring 3d instance segmentation. In Proceedings of AAAI\nConference on Artificial Intelligence (AAAI), 2021. 7\n[40] Shijia Huang, Yilun Chen, Jiaya Jia, and Liwei Wang. Multi-\nview transformer for 3d visual grounding. In Proceedings\nof Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 2, 3, 7\n[41] Ayush Jain, Nikolaos Gkanatsios, Ishita Mediratta, and Kate-\nrina Fragkiadaki. Bottom up top down detection transform-\ners for language grounding in images and point clouds. In\nProceedings of European Conference on Computer Vision\n(ECCV), 2022. 3, 7\n[42] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-\nWing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping\nfor 3d instance segmentation. In Proceedings of Conference\non Computer Vision and Pattern Recognition (CVPR), 2020.\n3\n[43] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray, Alec\nRadford, Jeffrey Wu, and Dario Amodei. Scaling laws for\nneural language models. arXiv preprint arXiv:2001.08361,\n2020. 3\n[44] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. In Proceedings of International Conference on Com-\nputer Vision (ICCV), 2023. 3\n[45] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson,\nKenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan-\ntidis, Li-Jia Li, David A Shamma, et al. Visual genome:\nConnecting language and vision using crowdsourced dense\nimage annotations. In International Journal of Computer\nVision (IJCV), 2017. 2\n[46] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum,\nand Samuel J Gershman. Building machines that learn and\nthink like people. Behavioral and brain sciences, 40:e253,\n2017. 2\n[47] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen\nKoltun, and Rene Ranftl. Language-driven semantic seg-\nmentation. In Proceedings of International Conference on\nLearning Representations (ICLR), 2022. 3\n10\n[48] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-\n2: bootstrapping language-image pre-training with frozen\nimage encoders and large language models. In ICML, 2023.\n4, 3, 5\n[49] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:\nBootstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In Proceedings of\nInternational Conference on Machine Learning (ICML), 2022.\n3\n[50] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jian-\nwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan,\nLei Zhang, Jenq-Neng Hwang, et al. Grounded language-\nimage pre-training. In Proceedings of Conference on Com-\nputer Vision and Pattern Recognition (CVPR), 2022. 3\n[51] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.\nVisual instruction tuning. In Proceedings of Advances in\nNeural Information Processing Systems (NeurIPS), 2023. 2,\n3\n[52] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Xu-\nanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and Hao\nSu. Openshape: Scaling up 3d shape representation towards\nopen-world understanding. arXiv preprint arXiv:2305.10764,\n2023. 2\n[53] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. In Proceedings of International\nConference on Computer Vision (ICCV), 2023. 2\n[54] Junyu Luo, Jiahui Fu, Xianghao Kong, Chen Gao, Haibing\nRen, Hao Shen, Huaxia Xia, and Si Liu. 3d-sps: Single-stage\n3d visual grounding via referred point progressive selection.\nIn Proceedings of Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 3, 7\n[55] Tiange Luo, Chris Rockwell, Honglak Lee, and Justin John-\nson.\nScalable 3d captioning with pretrained models.\nIn\nProceedings of Advances in Neural Information Processing\nSystems (NeurIPS), 2023. 2, 3\n[56] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt\nKira, Richard Socher, and Caiming Xiong. Self-monitoring\nnavigation agent via auxiliary progress estimation. In Pro-\nceedings of International Conference on Learning Represen-\ntations (ICLR), 2019. 3\n[57] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao\nLiang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated\nquestion answering in 3d scenes. In Proceedings of Interna-\ntional Conference on Learning Representations (ICLR), 2023.\n3\n[58] Yongsen Mao, Yiming Zhang, Hanxiao Jiang, Angel Chang,\nand Manolis Savva. Multiscan: Scalable rgbd scanning for 3d\nenvironments with articulated objects. In Proceedings of Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2022. 2, 3, 5, 1\n[59] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-\nend transformer model for 3d object detection. In Proceed-\nings of International Conference on Computer Vision (ICCV),\n2021. 3\n[60] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna\nTripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-\nscale benchmark for fine-grained and hierarchical part-level\n3d object understanding. In Proceedings of Conference on\nComputer Vision and Pattern Recognition (CVPR), 2019. 2\n[61] OpenAI. Introducing chatgpt. https://openai.com/\nblog/chatgpt, 2022. 5\n[62] OpenAI.\nGpt-4 technical report.\narXiv preprint\narXiv:2303.08774, 2023. 3\n[63] Alexander Pashevich, Cordelia Schmid, and Chen Sun.\nEpisodic transformer for vision-and-language navigation. In\nProceedings of International Conference on Computer Vision\n(ICCV), 2021. 3\n[64] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasac-\nchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene:\n3d scene understanding with open vocabularies. In Proceed-\nings of Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 3\n[65] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. In Proceedings of Advances in\nNeural Information Processing Systems (NeurIPS), 2017. 1,\n5\n[66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 3, 4, 5\n[67] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans,\nOleksandr Maksymets, Alex Clegg, John Turner, Eric Under-\nsander, Wojciech Galuba, Andrew Westbury, Angel X Chang,\net al. Habitat-matterport 3d dataset (hm3d): 1000 large-scale\n3d environments for embodied ai. In Proceedings of Ad-\nvances in Neural Information Processing Systems Datasets\nand Benchmarks (NeurIPS Datasets and Benchmarks Track),\n2021. 2, 3, 5, 1\n[68] Krishan Rana, Jesse Haviland, Sourav Garg, Jad Abou-\nChakra, Ian Reid, and Niko Suenderhauf. Sayplan: Ground-\ning large language models using 3d scene graphs for scalable\nrobot task planning. In Proceedings of Conference on Robot\nLearning (CoRL), 2023. 3\n[69] Antoni Rosinol, Andrew Violette, Marcus Abate, Nathan\nHughes, Yun Chang, Jingnan Shi, Arjun Gupta, and Luca\nCarlone. Kimera: From slam to spatial perception with 3d\ndynamic scene graphs. International Journal of Robotics\nResearch (IJRR), 2021. 3\n[70] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al.\nPhotorealistic text-to-image diffusion models with deep lan-\nguage understanding. Proceedings of Advances in Neural\nInformation Processing Systems (NeurIPS), 2022. 3\n[71] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes,\nAarush Katta, Clayton Mullis, Mitchell Wortsman, et al.\nLaion-5b: An open large-scale dataset for training next gen-\neration image-text models. In Proceedings of Advances in\nNeural Information Processing Systems (NeurIPS), 2022. 2,\n3\n[72] Jonas Schult, Francis Engelmann, Alexander Hermans, Or\nLitany, Siyu Tang, and Bastian Leibe. Mask3d: Mask trans-\nformer for 3d semantic instance segmentation. In Proceed-\n11\nings of International Conference on Robotics and Automation\n(ICRA), 2023. 3, 6, 8\n[73] Linda Smith and Michael Gasser. The development of em-\nbodied cognition: Six lessons from babies. Artificial life,\n11(1-2):13\u201329, 2005. 2\n[74] Ay\u00e7a Takmaz, Elisabetta Fedele, Robert W Sumner, Marc\nPollefeys, Federico Tombari, and Francis Engelmann. Open-\nmask3d: Open-vocabulary 3d instance segmentation. In Pro-\nceedings of Advances in Neural Information Processing Sys-\ntems (NeurIPS), 2023. 3\n[75] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\ntinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Roz-\ni\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama:\nOpen and efficient foundation language models.\narXiv\npreprint arXiv:2302.13971, 2023. 2, 5\n[76] Maria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Es-\nlami, Oriol Vinyals, and Felix Hill. Multimodal few-shot\nlearning with frozen language models. Proceedings of Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2021. 3\n[77] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and\nChang D Yoo. Softgroup for 3d instance segmentation on\npoint clouds. In Proceedings of Conference on Computer\nVision and Pattern Recognition (CVPR), 2022. 3\n[78] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico\nTombari, and Matthias Nie\u00dfner. Rio: 3d object instance\nre-localization in changing indoor environments. In Proceed-\nings of International Conference on Computer Vision (ICCV),\n2019. 2, 3, 1\n[79] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico\nTombari. Learning 3d semantic scene graphs from 3d indoor\nreconstructions. In Proceedings of Conference on Computer\nVision and Pattern Recognition (CVPR), 2020. 2, 3, 5\n[80] Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao,\nDinghan Shen, Yuan-Fang Wang, William Yang Wang, and\nLei Zhang.\nReinforced cross-modal matching and self-\nsupervised imitation learning for vision-language navigation.\nIn Proceedings of Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2019. 3\n[81] Tong Wu, Jiarui Zhang, Xiao Fu, Yuxin Wang, Jiawei Ren,\nLiang Pan, Wayne Wu, Lei Yang, Jiaqi Wang, Chen Qian,\net al. Omniobject3d: Large-vocabulary 3d object dataset\nfor realistic perception, reconstruction and generation. In\nProceedings of Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 2\n[82] Yanmin Wu, Xinhua Cheng, Renrui Zhang, Zesen Cheng, and\nJian Zhang. Eda: Explicit text-decoupling and dense align-\nment for 3d visual grounding. In Proceedings of Conference\non Computer Vision and Pattern Recognition (CVPR), 2023.\n3, 7\n[83] Le Xue, Mingfei Gao, Chen Xing, Roberto Mart\u00edn-Mart\u00edn,\nJiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and\nSilvio Savarese. Ulip: Learning a unified representation of\nlanguage, images, and point clouds for 3d understanding. In\nProceedings of Conference on Computer Vision and Pattern\nRecognition (CVPR), 2023. 2, 3, 5\n[84] Jihan Yang, Runyu Ding, Zhe Wang, and Xiaojuan Qi.\nRegionplc: Regional point-language contrastive learning\nfor open-world 3d scene understanding.\narXiv preprint\narXiv:2304.00962, 2023. 3\n[85] Yu-Qi Yang, Yu-Xiao Guo, Jian-Yu Xiong, Yang Liu, Hao\nPan, Peng-Shuai Wang, Xin Tong, and Baining Guo. Swin3d:\nA pretrained transformer backbone for 3d indoor scene under-\nstanding. arXiv preprint arXiv:2304.06906, 2023. 8, 7\n[86] Zhengyuan Yang, Songyang Zhang, Liwei Wang, and Jiebo\nLuo. Sat: 2d semantics assisted training for 3d visual ground-\ning. In Proceedings of International Conference on Computer\nVision (ICCV), 2021. 3, 7\n[87] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner,\nand Angela Dai. Scannet++: A high-fidelity dataset of 3d\nindoor scenes. In Proceedings of International Conference\non Computer Vision (ICCV), 2023. 2\n[88] Zhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin\nLi, Shuguang Cui, and Zhen Li. X-trans2cap: Cross-modal\nknowledge transfer using transformer for 3d dense captioning.\nIn Proceedings of Conference on Computer Vision and Pattern\nRecognition (CVPR), 2022. 3\n[89] Zhihao Yuan, Xu Yan, Yinghong Liao, Ruimao Zhang, Sheng\nWang, Zhen Li, and Shuguang Cui. Instancerefer: Cooper-\native holistic understanding for visual grounding on point\nclouds through instance multi-level contextual referring. In\nProceedings of International Conference on Computer Vision\n(ICCV), 2021. 7\n[90] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun\nChen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-\nNeng Hwang, and Jianfeng Gao. Glipv2: Unifying localiza-\ntion and vision-language understanding. In Proceedings of Ad-\nvances in Neural Information Processing Systems (NeurIPS),\n2022. 6\n[91] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng\nMiao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Point-\nclip: Point cloud understanding by clip.\nIn Proceedings\nof Conference on Computer Vision and Pattern Recognition\n(CVPR), 2022. 3\n[92] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and Hong-\nsheng Li. Learning 3d representations from 2d pre-trained\nmodels via image-to-point masked autoencoders. In Proceed-\nings of Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), 2023. 3\n[93] Yiming Zhang, ZeMing Gong, and Angel X Chang.\nMulti3drefer: Grounding text description to multiple 3d ob-\njects. In Proceedings of International Conference on Com-\nputer Vision (ICCV), 2023. 3\n[94] Lichen Zhao, Daigang Cai, Lu Sheng, and Dong Xu. 3dvg-\ntransformer: Relation modeling for visual grounding on point\nclouds. In Proceedings of International Conference on Com-\nputer Vision (ICCV), 2021. 3, 7\n[95] Jia Zheng, Junfei Zhang, Jing Li, Rui Tang, Shenghua Gao,\nand Zihan Zhou. Structured3d: A large photo-realistic dataset\nfor structured 3d modeling. In Proceedings of European\nConference on Computer Vision (ECCV), 2020. 2, 3, 1\n[96] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak\nGadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig\nSchmidt, William Yang Wang, and Yejin Choi. Multimodal\nc4: An open, billion-scale corpus of images interleaved with\ntext. arXiv preprint arXiv:2304.06939, 2023. 2\n[97] Yixin Zhu, Tao Gao, Lifeng Fan, Siyuan Huang, Mark Ed-\nmonds, Hangxin Liu, Feng Gao, Chi Zhang, Siyuan Qi,\n12\nYing Nian Wu, et al. Dark, beyond deep: A paradigm shift\nto cognitive ai with humanlike common sense. Engineering,\n6(3):310\u2013345, 2020. 2\n[98] Ziyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan\nHuang, and Qing Li. 3d-vista: Pre-trained transformer for 3d\nvision and text alignment. In Proceedings of International\nConference on Computer Vision (ICCV), 2023. 3, 5, 6, 7, 1\n13\nSCENEVERSE:\nScaling 3D Vision-Language Learning for Grounded Scene Understanding\nSupplementary Material\nIn Appendix A, we introduce more details of SCEN-\nEVERSE, including the 3D scene preprocessing, scene\ngraph construction and automatic language generation. Ap-\npendix B presents more model and implementation details.\nAppendix C include a more in-depth summary of the set-\ntings and implementations for the experiments in the main\npaper, as well as the ablative study and additional experi-\nments on semantic segmentation to demonstrate the benefits\nof SCENEVERSE.\nA. SCENEVERSE\nA.1. 3D Scenes\nTo address the scarcity of available 3D scene data, we con-\nstruct SCENEVERSE by unifying 3D scene data from vari-\nous existing datasets. The curation involves utilizing real-\nworld scene datasets such as ScanNet [23], ARKitScenes [9],\nHM3D [67], 3RScan [78] and MultiScan [58],in conjunc-\ntion with synthetic environments from Structured3D [95]\nand ProcTHOR [27]. The incorporation of these synthetic\ndatasets is primarily driven by their potential as scalable\ndata sources for 3D-VL alignment. To facilitate the training\nprocess, we conduct the following preprocessing steps.\nRoom Segmentation\nThe 3D scenes in HM3D and\nProcTHOR are released at the building level, encompassing\nmultiple rooms and sometimes spanning over 50 meters. To\nalign with existing benchmarks [1, 16], we leverage the as-\nsociated metadata to segment the 3D point cloud at the room\nlevel, facilitating subsequent operations in scene graph con-\nstruction and language description generation. Additionally,\nwe implement a filtering process to exclude extremely large\nrooms and those with fewer than 4 objects in the scene.\nPoint Cloud Normalization\nIn order to mitigate the\ndata disparities arising from diverse capture devices across\nvarious data sources, we subsample each point cloud to a\nmaximum of 240, 000 points. Each point cloud then under-\ngoes a transformation centered on the central point on the\nfloor, followed by rotation to align the room layout with the\naxis following the approach by Chen et al. [18].\nSemantic Label Alignment\nGiven the divergence in\nsemantic label sets across different datasets, we undertake\na comprehensive effort to map all the object class labels to\nthe 607 semantic labels in ScanNet [23] to facilitate close-\nvocabulary object classification [65] in the existing model\nframework [98]. We construct the mapping in each dataset\nthrough LLM and manual verification. Note that the object-\nlevel grounding in GPS can directly deal with open-set ob-\nTable A.1. Relationships in SCENEVERSE. The 3D scene graph\ncaptures 21 types of relationships ranging in 4 categories.\nCategory\nRelation\n|\nsupported by\nIn-contact vertical\nembedded into\nplaced in\n|\ninside\nhanging on\naffixed on\nmounted on\nNon-contact vertical\nabove\nhigher than\nbelow\nlower than\nnear(far) to the left of\nnear(far) to the right of\nis behind\nHorizontal\nis in front of\nclose to\nadjacent to\nbesides\nnext to\nMulti-object\nbetween\naligned\nject labels or captions, similar to CLIP [36].\nAfter the preprocessing, each scan is represented by a\npoint cloud P P RN\u02c68, wherein each point is defined by its\n3D coordinates, RGB color, instance id and semantic label.\nIn total, we curate 68, 406 3D scenes in SCENEVERSE.\nA.2. 3D Scene Graph Construction\nIn Sec. 3.2, we introduce an automated pipeline to construct\n3D scene graphs from point clouds. Here, we provide more\nimplementation details and the relationship definition.\nA.2.1\nRelationships\nOur 3D scene graph captures 21 types of relations as shown\nin Tab. A.1. We provide illustrations of how these relations\nare defined in the 3D space, as can be seen in Fig. A.1.\n1\nA.2.2\nScene Graph Construction\nDue to the inherent noise and incompleteness in the point\ncloud representation, automatically extracting precise and\ncomprehensive relationships from the point clouds is a non-\ntrivial task. Below we detail our 3D scene graph construction\nprocess, as outlined in Alg. 1.\nWe first instantiate the graph nodes with the instance an-\nnotation from the point cloud and parameterize each node\nwith object centroid pi P R3 and size of the axis-aligned\nbounding box bi \u201c pbx, by, bzq P R3 (Line 1-3). Next, we\ntraverse all the nodes to determine their spatial relationships\n(Line 4-22). Notably, in cases where an object node lacks\nany in-contact vertical relationships with other objects in\nthe scene, we designate such objects as \"hangable\" and cal-\nculate their non-contact vertical relationships (Line 9-13).\nExamples of such objects include paintings, curtains, etc.\nFinally, we establish relationships between multiple objects\n(Line 23): i) When a target object is connected with two\nedges labeled left and right, the target object, along\nwith the two neighboring nodes, forms a between relation-\nship triplets. ii) If the offset of the center point coordinates\nof a group of objects in either the X-axis or Y-axis direc-\ntion is smaller than a specified offset threshold \u03b4, then this\ngroup of objects forms an align relationship. The offset\nthreshold \u03b4 will be adjusted based on the size of the scene.\nIn additional, we utilize an automatic verification procedure\nto validate the scene graph, further improving the quality of\nthe scene graph we constructed (line 24). One of the verifi-\ncation operations involves manually maintaining a mapping\nbetween objects and relationship descriptions based on com-\nmon sense. For example, people usually use \u201cmounted on\u201d\nto describe the relation between TV and wall, rather than\n\u201changing on\u201d. Therefore, we would automatically refined\n( TV, hanging on, wall) to ( TV, mounted on, wall).\nIn our constructed 3D scene graph G \u201c pV, Eq, the nodes\nV comprises V1\n\u0164 V2\n\u0164 . . . \u0164 VK, with Vk representing the\nset of nodes at a particular hierarchical level. The hierarchies\nare determined by the support relationship; for instance,\nobjects supported by the floor constitute V0, while objects\nsupported by the table will form V1, etc. Note that edges\noriginating from one node v P Vk may only terminate in\nnearby hierarchies Vk YVk`1 YVk`1. In other words, edges\nin the scene graph exclusively connect nodes within the same\nhierarchical level, or one level higher or lower.\nA.3. Language Generation Details\nIn Sec. 3.3, we adopt both templates and LLM to automati-\ncally generate scene-language pairs in SCENEVERSE. More\ntechnical details and examples are provided in this section.\nAlgorithm 1: Scene Graph Construction Pipeline\nInput\n: M object point clouds tP1, P2, . . . , Pmu\nOutput :3D scene graph GpV, Eq\n1: for i from 1 to M do\n2:\nCreate node vi P V using the centroid pi and\nbounding box size bi of object point cloud Pi\n3: end for\n4: for i from 1 to M do\n5:\nfor j from i ` 1 to M do\n6:\nRelsTypev \u00d0 VerticalInContactpvi, vjq\n7:\nAdd in-contact vertical relationship triplets\npvi, vj, ei,jq with RelsTypev to G\n8:\nend for\n9:\nif No objects horizontally related to vi then\n10:\nfor k from 1 to M and i \u2030 k do\n11:\nRelsTypev \u00d0 VerticalNonContactpvi, vkq\n12:\nAdd non-contact vertical relationship triplets\npvi, vk, ei,kq with RelsTypev to G\n13:\nend for\n14:\nend if\n15: end for\n16: for vi P V do\n17:\nlet tvi1, vi2, ..., viN u be the N different nodes with\nthe same in-contact vertical parent node vi\n18:\nfor j from 1 to N do\n19:\nRelsTypeh \u00d0 Horizontalpvi, vijq\n20:\nAdd horizontal relationship triplets pvi, vij, ei,ijq\nwith RelsTypeh to G\n21:\nend for\n22: end for\n23: Update G \u00d0 MultiObjectspGq\n24: Update G with automatic verification procedure\nSupport \nEmbed\nInside / Placed in\nAbove / Below\nHanging\nNear\nIn front of / Behind\nLeft / Right\nBetween\nAlign\nFigure A.1. Overview of the relationships in SCENEVERSE. The\ntarget object is colored in blue.\nA.3.1\nObject Captioning Pipeline\nObject captions aim to provide detailed descriptions of an ob-\nject\u2019s visual and physical properties, facilitating object-level\ngrounding with its distinctive features. The detailed object\ncaptioning pipeline is outlined in Alg. 2. Given the multi-\nview images tI1, I2, . . . , Inu, we utilize the point cloud Po\nof the object o to get the visible points P vis\no,v in the images v\n2\nThe nightstand in the \napartment is a small white \ntable with a suitcase on it, \nalong with a laptop and a bag.\nIn a real apartment, a \nwooden stool is seen in the \nkitchen, placed on a tile floor \nnext to a table.\nA vibrant green chair with a \npolka dot pattern adds a lively \ntouch to various settings, \nincluding a desk and table.\nA small round table adorned \nwith a glass and \naccompanied by two chairs \nstands in a restaurant\nFigure A.2. Examples of object captioning. We color the target object in bold.\nScene Caption\nIn this apartment, there are 5 cabinets, 1 bed, 3 trash cans, 1 microwave, \nand 1 TV. The cabinets are positioned in front of the trash cans, while the \nbed is in front of the cabinet. The trash cans are also behind the cabinet and \nto the left of the bed. The TV is inside one of the cabinets. The bed is \npositioned behind the cabinet and to the right of the trash cans. This \napartment seems to be well-equipped with storage options and has a \ncomfortable sleeping area.\nScene Caption\nIn this room, there is an architectural floor and wall. The wall are attached to \nthe floor, creating a room with a big door. There are blind hanging on the wall, \nclose to the window. The room has a wide window, a heater connected to a \nwall, and a ceiling overhead. The room is furnished with a sofa, a table, and a \nchair. There are cushion and beanbag on the sofa, and a plant and lamp \nnearby. The room also has a TV, a whiteboard, and some clutter on the floor. \nThe overall style of the room is comfortable and modern.\nScene Caption\nIn this room, there is a bed, two windows, three lamps, three blankets, a TV, \nsix pillows, two cups, a curtain, and four shelves. The TV is positioned higher \nthan the shelf, while the sofa is to the right of the bed. One of the pillows is \ninside the bed, and the bed is located to the left of the sofa. Additionally, the \nlamp is positioned higher than the power outlet, which is lower than the lamp. \nThe room appears to be a comfortable living space with various objects for \nrelaxation and entertainment.\nFigure A.3. Examples of scene captioning.\nthrough rendering. The occlusion score socc\no,v is calculated as\nthe ratio between the number of visible points and the object\npoint cloud. The image is then cropped with the rendered\nbounding box and processed through BLIP2 [48] to generate\nthe initial object caption Co,v. For each initial caption, we\ncalculate its CLIP [66] similarity score between the text and\nthe cropped image, denoted by sclip\no,v . To get a refined object\ncaption, we select the top 10 initial captions with the highest\nCLIP score and minimal occlusion. The selected sentences\nare fed into a LLM to obtain a coherent summary of the\nobject captions. In this process, we explicitly instruct the\nlanguage model to identify and correct the potential errors.\nA.3.2\nAutomatic Language Generation\nTemplate-based\nWe create diverse templates to gener-\nate descriptions for each type of relationship. We categorized\nthe templates into three types based on the number of objects\ninvolved and their spatial relationships.\n\u2022 Pair-wise: The pair-wise templates are used to describe\nthe positional relationship between the target object\nand the anchor object in the scene. We design various\ntemplates to enrich the templated-based descriptions,\nspanning active and passive tense, as well as inversion\nclauses. Typical examples are shown below:\n- The target-object (is) spatial-relation the anchor-object.\n3\nTemplate-based\nThe shelf is hanging on the \nwall\nLLM-rephrased\nThe wall is adorned with a \nsuspended shelf\nTemplate-based\nThe sofa is supported by \nthe floor\nLLM-rephrased\nThe sofa rests upon the floor\nTemplate-based\nClose to the beanbag chair \nis another beanbag chair\nLLM-rephrased\nAnother beanbag chair lies \nnearby, within close proximity \nto the first one\nTemplate-based\nThe tv is higher than shelf\nLLM-rephrased\nThe tv sits atop the shelf\nTemplate-based\nThe microwave is inside \nthe kitchen cabinet\nLLM-rephrased\nThe kitchen cabinet contains \na microwave\nTemplate-based\nIt is a couch in the middle of \nstool and the bicycle\nLLM-rephrased\nThe bicycle and stool are \npositioned on either side of \nthe couch\nTemplate-based\nThe dish rack, bag and \nbottle are in aligned\nLLM-rephrased\nThe dish rack, bag, and \nbottle are arranged in a \nharmonious triad\nTemplate-based\nThe plant is to the right of \nthe shelf\nLLM-rephrased\nThe plant is positioned to the \nright of the shelf\nTemplate-based\nThe lamp is hung on the \nwall  and is higher than \nshelf, also is above light \nswitch\nLLM-rephrased\nThe lamp, situated at a \ncomfortable height above the \nlight switch and positioned on \nthe wall, creating an inviting \natmosphere perfect for \nrelaxation or reading a book \non the adjacent shelf\nTemplate-based\nThe sofa is close to the \ntable  and is to the left of \nwhiteboard and beanbag \nchair\nLLM-rephrased\nThe plush sofa, with its soft \ncushions, is strategically \npositioned near the table, \nwhile also being conveniently \naccessible from the \nwhiteboard and beanbag \nchair\nTemplate-based\nThe kitchen cabinet is above \nthe counter and microwave \nand bag\nLLM-rephrased\nThe kitchen cabinet, a \nconvenient storage space for \nculinary essentials, sits \nproudly above the counter \nand microwave, within easy \nreach for bagging groceries\nTemplate-based\nThe bed is in front of the \ncase  and is lower than \nlamp, also is to the left of \ntrash bin\nLLM-rephrased\nThe bed, situated in front of \nthe case and lower than the \nlamp, is also positioned to \nthe left of the trash bin, \nserving as a comfortable \nspot for rest and relaxation.\nFigure A.4. Examples of object referral. Note that the green bounding box indicates the target object and yellow bounding box indicates\nthe anchor object(s).\n- It is a target-object that (is) spatial-relation the anchor-\nobject.\n- There is a target-object that (is) spatial-relation the anchor-\nobject.\n- Spatial-relation the anchor-object is the target-object.\n- Spatial-relation the anchor-object, a target-object is placed.\n\u2022 Multi-objects: This is utilized when the target object\nforms a between or align relationship with multiple\nanchor objects in the scene. The templates follow the\nsame construction rules as the Pair-wise templates.\n4\nAlgorithm 2: Object Captioning Pipeline\nInput\n: M object point clouds tP1, P2, . . . , Pmu;\nN multiview images tI1, I2, . . . , Inu\nOutput :Captions for each object in the scene\ntC1, C2, . . . , Cmu\n1: for o \u201c 1, 2, . . . , M do\n2:\nfor v \u201c 1, 2, . . . , N do\n3:\nProject Po on Iv to get visible points P vis\no,v\n4:\nCrop Iv with the bounding box of P vis\no,v to get\nIcrop\no,v\n5:\nGet the image caption Co,v for Icrop\no,v\nusing\nBLIP2 [48]\n6:\nCalculate the similarity score sclip\no,v between Co,v\nand Icrop\no,v\nwith CLIP [66]\n7:\nCalculate the occlusion score socc\no,v \u201c\n#P vis\no,v\n#Po\n8:\nend for\n9:\nSelect the top-10 tCo,vu with highest sclip\no,v \u02da socc\no,v\n10:\nSummary selected tCo,vu with GPT-3.5 to get Co\n11: end for\n\u2022 Star-reference: To increase complexity in templated-\nbased descriptions, we design \u201cstar-reference\u201d to de-\nscribe the target object and its relationship with 3\nrandomly selected anchor objects in the scene graph.\nIn particular, we perform cluster analysis on the se-\nlected relationship triplets.\nBased on the diversity\nof the analysis, different templates will be chosen\nto generate descriptions.\nFor example, when the\nrelations between 3 anchor objects and the target\nobject is the same, we prefer to use the template\nlike: \u201cThe target-object (is) spatial-relation the anchor-\nobject-1, anchor-object-2 and anchor-object-3\u201d. If 2\nout of the 3 anchor objects have the same relations\nwith the target object, we would use a template like:\n\u201cThe target-object (is) spatial-relation-1 the anchor-\nobject-1 and anchor-object-2, and (is) spatial-relation-\n2 the anchor-object-3.\u201d\nLLM-rephrasing\nTo increase description diversity we\nuse the GPT-3.5[61] and Llama[75] for description rephras-\ning. This improves the diversity and naturalness of the\ntemplate-based descriptions, as is shown in Fig. 2. The\ndetailed prompts are provided in Tab. A.2.\nMore examples of the scene-language pairs in SCENEV-\nERSE are shown in Fig. A.2, Fig. A.3 and Fig. A.4.\nB. Model Details\nB.1. Spatial-Attention Transformer\nIn Sec. 4.2, we leveraged and spatial-attention based trans-\nformer architecture to aggregate object-level point cloud\nfeatures with spatial location information. In this section,\nwe provide the detailed design of this proposed module.\nFormally, given object features tf O\ni uN\ni\u201c1 and their loca-\ntions tliuN\ni\u201c1, we first construct pair-wise spatial relationship\nfeature via:\nmij \u201c rdij, sinp\u03b8hq, cosp\u03b8hq, sinp\u03b8vq, cosp\u03b8vqs ,\nwhere dij denotes the Euclidean distance between objects\nand \u03b8h, \u03b8v are the horizontal and vertical angles of the line\nconnecting the centers of objects i, j. We then use this\npair-wise relationship feature M \u201c rmijs P RN\u02c6N\u02c65 to\nmodulate the attention weights of the self-attention layer in\nthe transformer when aggregating object features as shown\nbelow:\nAttnpQ, K, V, Mq \u201c softmax\n\u02c6QKT\n?dh\n` log \u03c3pM\u03c9q\n\u02d9\nV,\nwhere \u03c9 P R5 is a projection layer mapping spatial pair-wise\nfeatures to the attention scores and \u03c3 denotes the sigmoid\nfunction. This process could be equivalently interpreted\nas using the spatial location of objects to adjust the self-\nattention feature aggregation between objects, making spa-\ntially related objects have more attention weights.\nB.2. Pre-training Details\nFor training our model GPS, we conduct a two-stage train-\ning approach. As described in Sec. 4.3, we first pre-train\nthe object point cloud encoder with the object-level ground-\ning objective. Next, we freeze the object point cloud en-\ncoder during the second pre-training stage for scene-level\npre-training that includes model training with scene-level\ngrounding and referral object grounding objectives. This\ndesign is inspired by recent works like [21, 98] that demon-\nstrated a well-grounded initialization of object representa-\ntions is beneficial for 3D scene grounding.\nObject-level pre-training\nTo correctly align objects in\nscenes with their captions, we utilize the ground-truth object\nbounding boxes provided with the datasets to segment all\nobjects in the scenes. Next, we utilize a PointNet++ [65]\nencoder to encode and align these object point clouds with\nobject captions provided in SCENEVERSE following Sec. 4.1.\nFor object instances with no object captions synthesized,\nwe follow [66] and construct captions with their semantic\nclass labels like \u201cthe point cloud of <CLASS>\u201d. Notably, as\nour model design sets no constraints on object point cloud\nencoders, the choice of object encoder mainly depends on\nthe computing resources available.\nScene-level pre-training\nWith pre-trained object fea-\nture extractors, we further use both scene captions and object-\nreferring expressions for scene-level pre-training. We use a\n4-layer BERT encoder for encoding both scene captions and\nobject referrals. As discussed in Sec. 4.2, we apply a 4-layer\nspatial transformer to encode object features with their loca-\ntions. For scene-level grounding, we adopt a max-pooling\n5\nTable A.2. Prompts used in SCENEVERSE.\nDescription type\nPrompt\nObject caption\nSummarize caption below.\nThe summary should be a description of the target-object.\nFocus on\nthe target-object\u2019s attribute, like color, shape and material, etc. Identify and correct the potential er-\nrors.\ncaption: A bed in a hotel room. A white comforter on a bed. A bed with a striped comforter...\ntarget-object: Bed\nObject referral\nRewrite the following caption using one random sentence structure. You should give me only one rewritten\nsentence without explanation.\ncaption: The bed is between desk and nightstand.\nRewrite the following caption. You should give me only one rewritten sentence about target-object without\nexplanation. Make sure target-object is the subject of the sentence, not anchor-object(s). If the sentence is\nin full inversion, keep the inversion.\ncaption: The armchair is next to the sofa.\ntarget-object: Armchair\nanchor-object(s): Sofa\nRewrite the following caption using one random sentence structure. You need to focus on the location and\nrelations of the target-object that appears in the sentence. If multiple target-object appear in the sentence,\nyou need to focus on the first target-object that appears. You can also add the target-object\u2019s function and\ncomfort level based on the sentence, e.g., how the objects can be used by humans and human activities in the\nscene. You should give me only one rewritten sentence without explanation.\ncaption: Far from the bowl and peppershaker, the vase is to the left, it is also on the top of countertop.\ntarget-object: Vase\nScene captioning\nYour task is to provide a summary for a scene from a given scene graph. The scene contains some objects,\nwhich compose a scene graph in json format.\nThere are 3 types of descriptions in scene graph: \u201cscene type\u201d denotes the type of the scene. \u201cobjects count\u201d\nthen listed the objects in the scene and their quantity, it should be noted that the actual objects in the room\nmay be more than listed. \u201cobjects relations\u201d describe the spatial relations with objects.\nAlso describe the scene concerning commonsense, e.g., how the objects can be used by human and human\nactivity in the scene. The description should conform to the given scene graph. The spatial relations between\nobjects can only be inferred from the \u201cobjects relations\u201c in scene graph. Don\u2019t describe each object in the\nscene, pick some objects of the scene for summary. Don\u2019t describe each relations in the scene, pick some\nrelations of the scene for summary. You can also summarize the room\u2019s function, style, and comfort level\nbased on the arrangement and count of objects within the room. The summary should be about the object\ntypes, object attributes, relative positions between objects. Your summary must not exceed 80 words. You\nmust write using one random sentence structure.\nscene graph: { \u2019scene_type\u2019:\n\u2019Bedroom\u2019, \u2019object_count\u2019:\n{\u2019nightstand\u2019:2,\n...}, \u2019relation\u2019:\n{\u2019nightstand\u2019, \u2019on\u2019, \u2019floor\u2019}, {\u2019backback\u2019, \u2019in\nfront of\u2019, bed}, ...}\nlayer to aggregate features from the spatial transformer and\nalign with the [CLS] feature of the scene caption. For\nreferral-object-level grounding, we further pass the obtained\nobject features as well as the referral language features into\na 4-layer self-attention transformer and use the grounding\nobjective described in Sec. 4.3 to match the referred object\u2019s\nfeature and the [CLS] feature of the referring expression.\nTraining\nFor object-level pre-training, we utilize an\nAdamW optimizer with a learning rate of 1 \u02c6 10\u00b42 for 1500\nepochs and no warm-up periods. During training, we use a\nbatch size of 512 and leverage a cosine annealing scheme\nfor learning rate scheduling with a minimum learning rate of\n1 \u02c6 10\u00b43. For scene-level pre-training, we use an AdamW\noptimizer with a learning rate of 1\u02c610\u00b45 for the language en-\ncoder, a learning rate of 1 \u02c6 10\u00b44 for the spatial transformer,\na learning rate of 1 \u02c6 10\u00b44 for the self-attention transformer,\nand a learning rate of 5 \u02c6 10\u00b44 for all remaining learnable\nparameters (e.g., projections). For all experiments, we train\nthe model for 150 epochs with a warm-up period of 500\nand also a cosine annealing scheme for learning rate with a\nminimum learning rate ratio of 0.1. All pre-training experi-\nments are run on 8 NVIDIA-A100 GPUs with the longest\npre-training on SCENEVERSE taking about 2 days.\n6\nC. Experimental Details\nIn this section, we provide details on experimental settings,\nmodel implementation, and additional results.\nC.1. 3D Visual Grounding\nSetting\nFor all datasets, we evaluate all models with\nonly the training sets provided.\nFollowing previous\nworks [98], we report model performance on the validation\nset of all datasets in Tab. 2. Notably, we used an off-the-shelf\nMask3D segmentation model for generating object proposals\nwith no optimization.\nImplementation\nAs briefly mentioned in Sec. 5.1, we\nmainly considered three model settings in 3D visual ground-\ning experiments, namely scratch, pre-train, and fine-tuned.\nFor the pre-train setting, we follow the same setting men-\ntioned in Appendix B.2. In the scratch and fine-tuned set-\ntings, to fairly compare with other dataset-specific fine-tuned\nmodels, we add an additional 2-layer MLP over the ob-\nject features from the referral grounding self-attention trans-\nformer. During training, we fine-tune this grounding head\ntogether with all model weights for 100 epochs with a learn-\ning rate of 1 \u02c6 10\u00b44 for the added projection layer and set\nall other settings the same as the implementation described\nin Appendix B.2.\nC.2. Zero-shot Transfer\nSetting\nIn the zero-shot experiments, we first construct\nthe held-out test set by aggregating scene-text pairs in SCEN-\nEVERSE from scenes in ScanNet and MultiScan. Specif-\nically, we use the validation set of ScanRefer, Nr3D, and\nSr3D. For scene-text pairs in the SCENEVERSE-val, we\nconstruct the test set by randomly sampling 1\n5 of human-\nannotated object referrals in the MultiScan dataset. This\nresults in a test set with around 1.7K object referrals ran-\ndomly drawn from 8.5k human-annotated object referrals\nin the MultiScan dataset. In the zero-shot settings, we use\nall scene-text pairs from datasets in SCENEVERSE except\nfor ScanNet and MultiScan. This includes both human-\nannotated and generated texts in ARKitScenes, 3RScan, and\nHM3D. This setting serves to test models\u2019 generalization\ncapability in grounding objects with both unseen scenes and\nunseen texts. In the zero-shot text setting, we add generated\nscene-text pairs in ScanNet and MultiScan into the data used\nin the zero-shot setting, thereby making the held-out test\ncontaining mainly unseen object referrals.\nImplementation\nIn the zero-shot experiments, we\nmainly considered three model settings scratch, zero-shot,\nand zero-shot text. For the zero-shot setting, we pre-train the\nmodel following Appendix B.2 without additional ground-\ning heads considering there is no additional training data\navailable in the zero-shot transfer setting. In the scratch and\nzero-shot text setting, we follow the model implementation\nTable A.3. Semantic segmentation results on ScanNet validation\nset. : denotes model trained with surface normals as an additional\ninput. S3D indicates models initialized with the original SWIN3D\nmodel weights pre-trained on Structured3D provided by Yang et\nal. [85].\nMethods\nInit.\nSCENEVERSE Pre.\nmIoU\nmAcc\nSWIN3Dn-S:\n\u2717\n\u2717\n75.2\n-\nSWIN3Dn-S:\nS3D\n\u2717\n75.6\n-\nSWIN3D-S\n\u2717\n\u2717\n63.2\n72.8\nSWIN3D-S\nS3D\n\u2717\n64.1\n75.1\nSWIN3D-S (pre-train)\n\u2717\n\u2713\n67.7\n78.0\nSWIN3D-S (pre-train)\nS3D\n\u2713\n69.5\n80.1\nSWIN3D-S (fine-tuned)\nS3D\n\u2713\n70.6\n80.2\ndescribed in Appendix C.1 and add an additional 2-layer\nMLP over the object features from the self-attention trans-\nformer. We follow the same fine-tuning setting described\nin Appendix C.1.\nD. Additional Results\nIn this section, we provide additional experimental results.\nSpecifically, we leverage our collected SCENEVERSE as\nthe pre-training data source for a traditional 3D semantic\nsegmentation task. Next, we provide ablative analyses of our\nmodel design.\nD.1. Semantic Segmentation\nSetting\nTo test if the scaling effect of SCENEVERSE\nis universally beneficial for 3D understanding tasks, we use\n3D semantic segmentation as a signature task to illustrate\nthe effectiveness of SCENEVERSE. Notably, a recent work\nthat introduced the Swin3D model [85] has identified the im-\nportance of pre-training for 3D semantic segmentation [85].\nFollowing the same setting, we test if the proposed SWIN3D\nmodel could be further improved by substituting the pre-\ntraining data to SCENEVERSE. Specifically, we test models\u2019\nperformance on the ScanNet semantic segmentation task\nwith 20 semantic categories and report the mean IoU and\nmean Acc on the validation set of ScanNet. As the original\nimplementation of SWIN3D pre-training requires surface\nnormals as additional inputs, we reimplement the model and\npre-train all models with only point coordinates and colors.\nComparison\nAs shown in Tab. A.3, we observe a sig-\nnificant model performance improvement (\u201e6%) by training\nSWIN3D-S model on our SCENEVERSE dataset. Compar-\ning our pre-training set to Structured 3D, we also observe\nconsistent model performance improvement, showcasing\nthe benefit of scaling-effect in SCENEVERSE. Moreover,\nwe fine-tune the model on ScanNet after pre-training on\nSCENEVERSE. This process further brings improvement in\nmodel performance on semantic segmentation. We believe\nthese results serve as strong pieces of evidence validating\nthe effectiveness of data scaling in SCENEVERSE and also\n7\nTable A.4. Model ablation of our model on SCENEVERSE-val.\nObj-lvl\nMLM\nScene-lvl\nOverall\nEasy\nHard\n\u2717\n\u2717\n\u2717\n64.8\n75.4\n48.7\n\u2713\n\u2717\n\u2717\n65.2\n77.1\n47.4\n\u2713\n\u2713\n\u2717\n62.4\n73.4\n45.8\n\u2713\n\u2713\n\u2713\n66.9\n77.8\n50.3\nits potential benefit for all 3D tasks in addition to 3D visual\ngrounding.\nD.1.1\nModel Ablation\nIn this section, we provide ablative analyses of our multi-\nlevel contrastive alignment design. We mainly consider\nremoving objectives in our model as ablations. We choose\nthe referral-object-level alignment objective as the default\nsetting and consider removing: (i) object-level alignment\nobjective, (ii) masked language modeling objective, and (iii)\nscene-level alignment objective. For removing the object-\nlevel alignment objective, we remove the first stage pre-\ntraining of the object point cloud encoder and jointly learn\nthis module within the referral-object-level alignment. As\nshown in Tab. A.4, we test different model settings on the\nSCENEVERSE-val without additional fine-tuning. First, we\nshow that the scene-level alignment objective is crucial for\nreferral object grounding in SCENEVERSE-val with the \u201e5%\nperformance drop. Similar observations could be made for\nthe model trained without object-level alignment (\u201e2% drop)\nand masked language modeling objective (\u201e1.5% drop). We\nbelieve these results affirm the effectiveness of our overall\nmodel design.\n8\n"
  },
  {
    "title": "GARField: Group Anything with Radiance Fields",
    "link": "https://arxiv.org/pdf/2401.09419.pdf",
    "upvote": "16",
    "text": "GARField: Group Anything with Radiance Fields\nChung Min Kim\u22171 Mingxuan Wu\u22171 Justin Kerr\u22171 Ken Goldberg1\nMatthew Tancik2 Angjoo Kanazawa1\n\u2217 Denotes equal contribution\n1UC Berkeley\n2 Luma AI\nNeRF\nMulti-Level Masks\nGroup Anything with Radiance Fields\nMulti-Scale Clustering\nlarger\nsmaller\nScale\nHierarchical Asset Extraction\nFigure 1. Group Anything with Radiance Fields (GARField) We present GARField, which distills multi-level groups represented as\nmasks into NeRF to create a scale-conditioned 3D affinity field (top left). Once trained, this affinity field can be clustered at a variety of\nscales to decompose the scene at different levels of granularity, like breaking apart the excavator into its subparts (bottom). 3D assets can\nbe extracted from this hierarchy by extracting every group in the scene automatically or via user clicks, as visualized here (top right).\nAbstract\nGrouping is inherently ambiguous due to the multiple\nlevels of granularity in which one can decompose a scene\u2014\nshould the wheels of an excavator be considered separate or\npart of the whole? We present Group Anything with Radi-\nance Fields (GARField), an approach for decomposing 3D\nscenes into a hierarchy of semantically meaningful groups\nfrom posed image inputs.\nTo do this we embrace group\nambiguity through physical scale: by optimizing a scale-\nconditioned 3D affinity feature field, a point in the world\ncan belong to different groups of different sizes. We optimize\nthis field from a set of 2D masks provided by Segment Any-\nthing (SAM) in a way that respects coarse-to-fine hierarchy,\nusing scale to consistently fuse conflicting masks from dif-\nferent viewpoints. From this field we can derive a hierar-\nchy of possible groupings via automatic tree construction\nor user interaction. We evaluate GARField on a variety of\nin-the-wild scenes and find it effectively extracts groups at\nmany levels: clusters of objects, objects, and various sub-\nparts. GARField inherently represents multi-view consis-\ntent groupings and produces higher fidelity groups than the\ninput SAM masks. GARField\u2019s hierarchical grouping could\nhave exciting downstream applications such as 3D asset ex-\ntraction or dynamic scene understanding. See the project\nwebsite at https://www.garfield.studio/\n1. Introduction\nConsider the scene in Figure 1. Though recent technologies\nlike NeRFs [20] can recover photorealistic 3D reconstruc-\n1\narXiv:2401.09419v1  [cs.CV]  17 Jan 2024\ntions of this scene, the world is modeled as a single volume\nwith no structural meaning. As humans, not only can we\nreconstruct the scene, but we also have the ability to group\nit at multiple levels of granularity \u2014 at the highest level, we\nsee the parts of the scene i.e. the excavator, bushes, and the\nsidewalk, but we are also able to decompose the excavator\ninto its parts such as its wheels, crane, and the cabin. This\nability to perceive the scene at multiple levels of groupings\nis a key component of our scene understanding, enabling\nus to interact with the 3D world by understanding what be-\nlongs together. However, these different levels of granular-\nity introduce ambiguity in groups, making it a challenge to\nrepresent them in a coherent 3D representation. While there\nare multiple ways to break this ambiguity, we focus on the\nphysical scale of entities as a cue to consolidate groups into\na hierarchy.\nIn this work we introduce Group Anything with Ra-\ndiance Fields (GARField), an approach that, given posed\nimages, reconstructs a 3D scene along with a scale-\nconditioned affinity field that enables decomposing the\nscene into a hierarchy of groups. For example, GARField\ncan extract both the entire excavator (Fig. 1 Top Right) as\nwell as its subparts (Bottom Right). This dense hierarchical\n3D grouping enables applications such as 3D asset extrac-\ntion and interactive segmentation.\nGARField distills a set of 2D segmentation masks into\na 3D volumetric scale-conditioned affinity field. Because\ngrouping is an ambiguous task, these 2D labels can be over-\nlapping or conflicting. These inconsistencies pose a chal-\nlenge for distilling masks into consistent 3D groups. We\novercome this issue by leveraging a scale-conditioned fea-\nture field. Specifically GARField optimizes a dense 3D fea-\nture field which is supervised such that feature distance re-\nflects points\u2019 affinity. The scale conditioning enables two\npoints to have higher affinity at a large scale but low affinity\nat a smaller scale (i.e. wedges of the same watermelon), as\nillustrated in Figure 2.\nThough in principle GARField can distill any source of\n2D masks, we derive mask candidates from Segment Any-\nthing Model (SAM) [15] because they align well with what\nhumans consider as reasonable groups. We process input\nimages with SAM to obtain a set of candidate segmenta-\ntion masks. For each mask, we compute a physical scale\nbased on the scene geometry. To train GARField, we distill\ncandidate 2D masks with a contrastive loss based on mask\nmembership, leveraging 3D scale to resolve inconsistencies\nbetween views or mask candidates.\nA well-behaved affinity field has: 1) transitivity , which\nmeans if two points are mutually grouped with a third, they\nshould themselves be grouped together, and 2) containment,\nwhich means if two points are grouped at a small scale, they\nshould be grouped together at higher scales. GARField\u2019s\nuse of contrastive loss in addition to a containment auxiliary\nA\nPUSH\nPULL\nScale-conditioned Af\ufb01nity\n: PUSH\n: PULL\nA\nB\nAre\ntogether?\nB\nA\nB\nsmall\nlarge\nFigure 2. Importance of Scale When Grouping A single point\nmay belong to multiple groups. GARField uses scale-conditioning\nto reconcile these conflicting signals into one affinity field.\nloss encourages both of these properties.\nWith the optimized scale-conditioned affinity field,\nGARField extracts a 3D scene hierarchy via recursively\nclustering them at descending scales until no more clusters\nemerge. By construction, this recursive clustering ensures\nthat generated groups are subparts of the prior cluster in a\ncoarse-to-fine manner. We evaluate GARField on a variety\nof real scenes with annotated hierarchical groupings, test-\ning its ability to capture object hierarchy, and its consis-\ntency across different views. By leveraging multiple views,\nGARField is able to produce detailed groupings, often im-\nproving upon the quality of input 2D segmentation masks.\nMoreover, these groups are 3D consistent by design, while\n2D baselines do not guarantee view consistency. We show\ndownstream applications of GARField for hierarchical 3D\nasset extraction and click-based interactive segmentation.\nGiven GARField\u2019s scene decomposition capabilities, we\u2019re\nhopeful for its potential in other downstream applications\nlike enabling robots to understand they can interact with or\nas a prior for dynamic reconstruction. Code and data will\nbe released upon publication. Please see the supplemental\nvideo for more visualizations.\n2. Related Work\nHierarchical Grouping\nMulti-level grouping has long\nbeen studied in 2D images since the early days of fore-\nground segmentation [28]. Several methods build on this\nidea of spectral clustering for multi-level segmentation [5]\nand more complex hierarchical scene parsing [1, 25, 31].\nThese approaches rely on extracting contours either via\nclassic texture cues and create a hierarchy either via a top-\ndown [37] or bottom-up consolidation [1].\nMore recent\ndeep learning approaches use edges [36] computed at multi-\nple scales to create the hierarchy, and Ke et al. [11] proposes\na transformer based unsupervised hierarchical segmentation\napproach guided by the outputs of a classic hierarchical seg-\nmentation [1].\nMany works circumvent the question of ambiguity in\ngrouping by defining a set of categories within which in-\nstances are to be segmented, i.e. panoptic segmentation [10,\n14]. Recently, Segment Anything (SAM) [15] off-loads this\nambiguity into prompting, where at each pixel multiple seg-\n2\nr1\nr2\nsB\nsA\nx, y, z, \u03b8, \u03d5\nscale\nNetwork\nFg(x1, sA)\nRGB\nDensity\nAf\ufb01nity Feature\nNetwork\nFg(x2, sB)\nRGB\nDensity\nLpush = ReLU(m \u2212 \u2225Fg(x1, sA) \u2212 Fg(x2, sB)\u2225)\nx1\nx2\nMB\nMA\nPUSH\n3D Mask Scale Generation\nSet of SAM Masks\nMA\nsA\nScale-Conditioned Af\ufb01nity Field Training\nLpull = \u2225Fg(x1, sC) \u2212 Fg(x2, sC)\u2225\nPULL\nGroup Preprocessing\nImages\nMC, sC\nx1\nx2\nFigure 3. GARField Method: (Left) given an input image set, we extract a set of candidate groups by densely querying SAM, and assign\neach a physical scale by deprojecting depth from the NeRF. These scales are used to train a scale-conditioned affinity field (Right). During\ntraining, pairs of sampled rays are pushed apart if they reside in different masks, and pulled together if they land in the same mask. Affinity\nis supervised only at the scale of each mask, which helps resolve conflicts between them.\nmentation masks can be proposed. However SAM does not\nrecover a consistent set of hierarchical groups in the scene,\nwhich we enable by multi-scale 3D distillation.\nHierarchical part decomposition has also been explored\nin 3D objects, either in a supervised [17, 21, 35], or un-\nsupervised manner [24]. Our approach distills information\nfrom a 2D model, and we consider full scenes while these\napproaches focus on 3D objects.\nSegmentation in NeRFs Existing approaches for segmen-\ntation in NeRFs typically distill segmentation masks into\n3D either by using ground-truth semantic labels [29, 38],\nmatching instance masks [18], or training 3D segmenta-\ntion networks on NeRF [34]. However, these techniques\ndo not consider hierarchical grouping, and are only inter-\nested in a flat hierarchy of objects or instances. Ren et al.\n[27] leverages human interaction in the form of image scrib-\nbles to segment objects with interaction. More recently, Cen\net al. [3] try to recover a 3D consistent mask from SAM\nby tracking the 2D masks between neighboring views via\nuser prompting. Chen et al. [4] attempt this by distilling\nSAM encoder features into 3D and querying the decoder.\nIn contrast with these approaches, our approach GARField\ndoes not require user input; it is able to obtain a hierarchical\ngrouping of the scene automatically, and furthermore the\nrecovered groups are view-consistent by definition.\n3D Feature Fields Distilling higher-dimensional features\ninto a neural field, in tandem with a radiance field (view-\ndependent color and density), has been thoroughly ex-\nplored. Methods like Semantic NeRF [38], Distilled Feature\nFields [16], Neural Feature Fusion Fields [33], and Panoptic\nLifting [29] distill per-pixel 2D features into 3D by optimiz-\ning a 3D feature field to reconstruct the 2D features after\nvolumetric rendering. These features can be either from\npretrained vision models such as DINO or from semantic\nsegmentation models.\nLERF [13] extends this idea to a\nscale-conditioned feature field, enabling the training of fea-\nture fields from global image embeddings like CLIP [26].\nGARField similarly optimizes a scale-conditioned feature\nfield in 3D; however, the purpose of the multi-scale\nfeatures is to resolve ambiguity in grouping, instead of\nreconstructing an explicit 2D feature like CLIP. In addition\nLERF has no spatial grouping, a shortcoming GARField\naddresses. The aforementioned methods are based on direct\nsupervision from image features, while other methods such\nas NeRF-SOS [8] and Contrastive Lift [2] optimize an\narbitrary feature field at a single scale using a contrastive\nloss between pairs of rays based on similarity. GARField\nuses this contrastive approach because it allows for defining\npairwise relationships between points based on mask\nlabels. However, we design a scale-conditioned contrastive\nloss, which allows for distilling conflicting masks into\n3D. In addition, GARField does not require the slow-fast\nformulation of Bhalgat et al. [2] for stable training, perhaps\nenabled by scale-conditioned training.\n3. Method\n3.1. 2D Mask Generation\nGARField takes as input a set of posed images and produces\na hierarchical 3D grouping of the scene, along with a stan-\ndard 3D volumetric radiance field and a scale-conditioned\naffinity field. To do this, we first pre-process input images\nwith SAM to obtain mask candidates. Next, we optimize a\nvolumetric radiance field along with the affinity field which\ntakes in a single 3D location and a euclidean scale, and\noutputs a feature vector. Affinity is obtained by compar-\ning pairs of points\u2019 feature vectors. After optimization, the\nresulting affinity field can be used to decompose a scene by\nrecursively clustering the feature embeddings in 3D at de-\nscending scales in a coarse-to-fine manner, or for segment-\ning user specified queries. The overall pipeline is illustrated\nin Figure 3.\nIn order to train a GARField, we first mine 2D mask\ncandidates from an image and then assign a 3D scale for\n3\nDensi\ufb01ed Scale Supervision\nNaive Scale Supervision\n1. Continuous Supervision\n2. Containment\n0\nPUSH\nPULL\nUnde\ufb01ned?\nUnde\ufb01ned?\ns0\ns1\nMB\nMC\nMC\ns0\ns1\ns \u223c U(0,s0), U(s0, s1), \u2026\n\u2200s \u2265 s1, x \u2261 y\nMA\nMA\nMB\nMC\nr1\nr2\nr1\nr2\nr1\nr2\nFigure 4.\nDensified Scale Supervision: Consider two grapes\nwithin a cluster. Naively using scale for contrastive loss supervises\naffinities only at the grape and grape trio levels, leaving entire in-\ntervals unsupervised. In GARField, we densify the supervision by\n1) augmenting scale between mask euclidean scales and 2) impos-\ning an auxiliary loss on containment of larger scales.\neach mask. Specifically, we use SAM\u2019s automatic mask\ngenerator [15], which queries SAM in a grid of points and\nproduces 3 candidate segmentation masks per query point.\nThen, it filters these masks by confidence and deduplicates\nnearly identical masks to produce a list of mask candidates\nof multiple sizes which can overlap or include each other.\nThis process is done independently of viewpoint, producing\nmasks which may not be consistent across views. In this\nwork we aim to generate a hierarchy of groupings based on\nobjects\u2019 physical size. As such, we assign each 2D mask\na physical 3D scale as in Fig. 3. To do this we partially\ntrain a radiance field and render a depth image from each\ntraining camera pose. Next, for each mask we consider the\n3D points within that mask and pick the scale based on the\nextent of the points\u2019 position distribution. This method en-\nsures the 3D scale of masks resides in the same world-space,\nenabling scale-conditioned affinity.\n3.2. Scale-Conditioned Affinity Field\nScale-conditioning is a key component of GARField which\nallows consolidating inconsistent 2D mask candidates: The\nsame point may be grouped in several ways depending on\nthe granularity of the groupings desired. Scale-conditioning\nalleviates this inconsistency because it resolves ambiguity\nover which group a query should belong to. Under scale-\nconditioning, conflicting masks of the same point no longer\nfight each other during training, but rather can coexist in the\nsame scene at different affinity scales.\nWe define the scale-conditioned affinity field Fg(x, s) 7\u2192\nRd over a 3D point x and euclidean scale s, similar to the\nformulation in LERF [13]. Output features are constrained\nto a unit hyper-sphere, and the affinity between two points\nat a scale is defined by A(x1, x2, s) = \u2212||Fg(x1, s) \u2212\nFigure 5. 3D Asset Extraction with Interactive Selection: Users\ncan interactively select view-consistent 3D groups with GARField\nusing a click point and a scale.\nFg(x2, s)||2. These features can be volumetrically rendered\nwith a weighted average using the same rendering weights\nbased on NeRF density to obtain a value on a per-ray basis.\n3.2.1\nContrastive Supervision\nThe field is supervised with a margin-based contrastive ob-\njective, following the definition provided by DrLIM [9].\nThere are two core components of the loss: at a given scale,\none which pulls features within the same group to be close,\nand another which pushes features in different groups apart.\nSpecifically, consider two rays rA, rB sampled from\nmasks MA, MB within the same training image, with cor-\nresponding scales sA and sB. We can volumetrically ren-\nder the scale-conditioned affinity features along each ray\nto obtain ray-level features FA and FB. If MA = MB,\nthe features are pulled together with L2 distance: Lpull =\n||FA \u2212 FB||. If MA \u0338= MB, the features are pushed apart:\nLpush = ReLU(m \u2212 ||FA \u2212 FB||) where m is the lower\nbound distance, or margin. Importantly, this loss is only\napplied among rays sampled from the same image, since\nmasks across different viewpoints have no correspondence.\n3.2.2\nDensifying Scale Supervision\nThe supervision provided by the previous contrastive losses\nalone are not sufficient to preserve hierarchy. For exam-\nple in Fig. 4, although the egg is correctly grouped with the\nsoup at scale 0.22, at a larger scale it fragments apart. We\nhypothesize this grouping instability is because 1) scale su-\npervision is defined sparsely only when a mask exists and 2)\nnothing imposes containment such that small scale groups\nremain at larger scales. We address these shortcomings here\nby introducing the following modifications:\nContinuous scale supervision By using 3D mask\nscales, groups are only defined at discrete values where\nmasks are chosen. This results in large unsupervised re-\ngions of scale, as shown at the top of Fig. 9. We densify\n4\nTop Level Clustering\nHierarchical Decomposition\nNovel View\nFigure 6. 3D Decomposition: GARField can be recursively queried at decreasing scale to cluster a scene into objects and their subparts.\nscale supervision by augmenting the scale s uniformly ran-\ndomly between the current mask\u2019s scale and the next small-\nest mask\u2019s scale. When a ray\u2019s mask is the smallest mask\nfor the given viewpoint, we interpolate between 0 and s0.\nThis ensures continuous scale supervision throughout the\nfield leaving no unsupervised regions.\nContainment Auxiliary Loss: If two rays r1 and r2 are\nin the same mask with scale s, then they should also be\npulled together at any scale larger than s. Intuitively, two\ngrapes within the same cluster (Fig. 4) are also grouped to-\ngether at larger scales (e.g., the entire bunch). At each train-\ning step, for the rays grouped together at scale s, we addi-\ntionally sample a larger scale s\u2032 > s at which the rays are\nalso pulled together. This ensures that affinities at smaller\nscales are not lost at larger scales.\n3.2.3\nRay and Mask Sampling\nJust like standard NeRF training, we sample rays over\nwhich to compute losses. Because GARField uses a con-\ntrastive loss within each train image, naively sampling pix-\nels uniformly during training is inadequate to provide a\ntraining signal in each minibatch of rays. To ensure suf-\nficient pairs in each train batch, we first sample N images,\nand sample M rays within each image. To balance the num-\nber of images as well as the number of point pairs for su-\npervision, we sample 16 images and 256 points per image,\nresulting in 4096 samples per train iteration.\nFor each ray sampled, we must also choose a mask to use\nas the group label for the train step in question. To do this,\nwe retain a mapping from pixels to mask labels through-\nout training, and at each train step randomly select a mask\nfor each ray from its corresponding list of masks. There\nare two important caveats in this sampling process: 1) The\nprobability a mask is chosen is weighted inversely with the\nlog of the mask\u2019s 2D pixel area. This prevents large scales\nfrom dominating the sampling process, since larger masks\ncan be chosen via more pixels. 2) During mask selection\nwe coordinate the random scale chosen across rays in the\nsame image to increase the probability of positive pairs. To\ndo this, we sample a single value between 0 and 1 per im-\nage, and index into each pixel\u2019s mask probability CDF with\nthe same value, ensuring pixels which land within the same\ngroup are assigned the same mask. Otherwise, the loss is\ndominated by pushing forces which destabilize training.\n3.3. Implementation Details\nThe method is built in Nerfstudio [32] on top of the Nerfacto\nmodel by defining a separate output head for the grouping\nfield. The grouping field is represented with a hashgrid [23]\nwith 24 layers and a feature dimension of 2 per layer, and a\n4-layer MLP with 256 neurons and ReLU activation which\ntakes in scale as an extra input concatenated with hash-\ngrid feature. We cap scale at 2\u00d7 the extent of cameras,\nand normalize the scale input to the MLP using sklearn\u2019s\nquantile transform on the distribution of computed 3D mask\nscales (Sec 3.1). Output embeddings are d = 256 dimen-\nsions. Gradients from the affinity features do not affect the\nRGB outputs from NeRF, as these representations share no\nweights or gradients.\nWe begin training the grouping field after 2000 steps of\nNeRF optimization, giving geometry time to converge. In\naddition, to speed training we first volumetrically render the\nhash value, then use it as input to the MLP to obtain a ray\nfeature. With this deferred rendering, the same ray can be\nqueried at different scales with only one extra MLP call.\nWe normalize the result of volume rendering to unit norm\nbefore inputting to the MLP, and for point-wise queries,\nthe individual hashgrid value is normalized. Preprocessing\nSAM masks takes around 3-10 minutes, followed by about\n20 minutes for training on a GTX 4090.\n4. Hierarchical Decomposition\nOnce we have optimized a scale-conditioned affinity,\nGARField generates a hierarchy of 3D groups, organized\nin a tree such that each node is broken into potential sub-\ngroups. To do this we recursively cluster groups by decreas-\ning the scale for affinity, using HDBSCAN [19], a density\nbased clustering algorithm which does not require a prior on\nnumber of clusters. This clustering process can be done in\n2D on volumetrically rendered features in an image which\n5\nFigure 7. Results: From a GARField we extract objects from the global scene by selecting top-level clusters, then visualize their local\nclusters at decreasing scales. GARField can produce complete 3D object masks, and break these objects into meaningful subparts based\non the input masks. We use Gaussian Splats [12] to produce these visualizations in 3D. See the Supplemental video for more results.\n6\nRGB\nSAM (2D)\nGARField\nFigure 8. Segment-Anything [15] vs. GARField: SAM\u2019s auto-\nmatic mask generator may encounter difficulty recalling all masks\nfrom a given viewpoint, especially when there are clusters of small\nmasks and the camera is far away from the object. In contrast,\nGARField\u2019s scale-conditioned affinity field incorporates masks\nfrom multiple viewpoints in 3D.\nyields masks, or in 3D across points to yield pointclouds.\nSee Fig. 6 for a visualization of scene decomposition.\nInitialization: First, to initialize the hierarchy, we first\nglobally cluster features at a large scale smax, which we set\nto 1.0 for all experiments, corresponding to the extent of the\ninput cameras\u2019 positions. These clusters form the top-level\nnodes in the scene decomposition.\nRecursive Clustering: Next, to produce a hierarchical tree\nof scene nodes, we iteratively reduce scale by a fixed epsilon\n(we use 0.05), running HDBSCAN on each leaf node. If\nHDBSCAN returns more than one cluster for a given node,\nwe add those clusters as children and recurse. This con-\ntinues until we reach scale 0, at which point the procedure\nterminates, returning the current tree.\n5. Experiments\nWe assess GARField\u2019s ability to decompose in-the-wild 3D\nscenes into hierarchical groups which vary widely in size\nand semantics. Existing 3D scan datasets tend to focus on\nobject-level scans [7, 22], are simulated [2], or contain pri-\nmarily indoor household scenes [6]. To evaluate GARField,\nwe instead use a wide variety of indoor and outdoor scenes\nfrom the Nerfstudio and LERF datasets, as well as addi-\ntional captures for this paper. We experiment on scenes\nwhich possess significant object hierarchy, testing the de-\ncomposition ability of GARField. We provide qualitative\nresults in Fig. 3 and Fig. 6, and quantitatively evaluate by\nannotating ground truth masks on select scenes, a full list of\nwhich are in the Supplement.\n5.1. Qualitative Scene Decomposition\nWe use Gaussian Splatting [12] to visualize the decomposi-\ntion by querying GARField\u2019s affinity field at gaussian cen-\nters. We do this because gaussian splats are easier to seg-\nment in 3D compared to NeRFs. See the Supplement for a\nAf\ufb01nity w/\nGARField (-dense hierarchy)\nGARField\ns\n0.0\n0.06\n0.22\n0.5\n0.04\nFigure 9. Ablation: Without dense hierarchy supervision, points\nmay have inconsistent affinities across scales. There may be 1)\nspurious large affinities at unsupervised scales or 2) unexpected\ndrops in affinity at larger scales.\nfull description of the pipeline. All renderings are of com-\nplete 3D models, not segmentations of 2D image views.\nWe visualize two types of hierarchical clustering results.\nIn Fig. 7 we globally cluster the scene at a hand selected\ncoarse scale, then from these scene-wide clusters we select\ngroups corresponding to few objects and further decompose\nthem into subgroups. We visualize clusters obtained at suc-\ncessively decreasing scales, which increases the granularity\nof groups. GARField achieves high-fidelity 3D groupings\nacross a wide range of scenes and objects, from man-made\nobjects \u2013 such as keyboards, where each key is considered\na group at a small scale, to the parts of the NERF gun and\nthe Lego bulldozer \u2013 to complex natural objects like plants,\nwhere it can group individual flowers as well as their petal\nand leaves. By varying the scale, one can separate objects at\ndifferent levels, for instance the succulent from its pot ver-\nsus each individual leaf (first row), or identifying the bunny\ntoy in the bulldozer\u2019s scooper, which is further grouped into\nits shirt, ears, and head (fifth row, right). See Fig. 10 for\nselect scene-wide cluster visualizations.\nIn Fig. 6 we visualize a tree decomposition produced by\nthe method described in Sec. 4. We first show the global\nclustering at a top level node, from which we select the\ncentral statue to illustrate the tree decomposition. Arrows\ndenote children in the hierarchy, illustrating how the stat-\nues decomposes gradually all the way down to its hair, legs,\ntorso, etc. See the Supplement for more tree visualizations.\n5.2. Quantitative Hierarchy\nWe quantitatively evaluate our approach against annotated\nimages using two metrics: the first measuring view con-\nsistency against annotations from multiple views and the\nsecond measuring recall of various hierarchical masks via\nmIOU against ground truth human annotations.\n3D Completeness: For downstream tasks it is useful for\ngroups to correspond to complete 3D objects, for example\ngroups that contain an entire object rather than just one of its\nsides. Though GARField always produces view-consistent\ngroups by construction, it may not necessarily contain com-\nplete objects. We evaluate for completeness by checking\n7\nFine\nMedium\nCoarse\nScene\nSAM Ours\nSAM Ours\nSAM Ours\nteatime\n81.6 92.7\n97.3 97.9\n-\n-\nbouquet\n17.4 76.0\n73.5 81.6\n76.1 85.4\nkeyboard\n65.3 88.8\n73.6 98.4\n-\n-\nramen\n53.3 79.2\n74.7 90.7\n92.6 95.5\nliving room\n85.3 90.5\n74.2 80.7\n88.6 94.4\nTable 1. 3D Completeness. We report mIOU of scene anno-\ntations for a single point with up to three levels of hierarchy.\nSAM struggles to produce view-consistent fine groups compared\nto GARField.\nScene\nSAM [15]\nOurs\nOurs\nOurs\n(-scale)\n(-dense)\nramen\n74.9\n64.1\n74.1\n85.6\nteatime\n64.9\n67.7\n66.1\n86.6\nkeyboard\n23.2\n57.6\n73.1\n77.9\nbouquet\n34.4\n49.8\n72.9\n76.4\nliving room\n59.6\n49.7\n62.1\n76.6\nTable 2. Hierarchical Grouping Recall: We report mIOU against\nhuman annotations of multi-scale groups of different objects.\nthat an entire 3D object is grouped together across a range\nof viewpoints. To do this, on 5 scenes we choose a 3D point\nto be projected into 3 different viewpoints, and label 3 cor-\nresponding view-consistent ground truth masks containing\nthat point at coarse, medium, and fine levels. At these points\nwe mine multiple masks from GARField across multiple\nscales at 0.05 increments, where at each scale a mask is ob-\ntained based on feature similarity thresholded at 0.9. We\nalso compare against SAM by clicking the point in the im-\nage and taking all 3 masks. We report the maximum mIOU\ncomputed over all candidate masks for both methods.\nResults are shown in Table 1. GARField produces more\ncomplete 3D masks than SAM across viewpoints, result-\ning in higher mIOU with multi-view human annotations of\nobjects. This effect is especially apparent at the most gran-\nular level, where from certain perspectives SAM struggles\nto produce fine groups, like the keyboard keys from afar in\nFig. 8. See the Supplement for figures of comparisons and\nvisualization of the groundtruth masks.\nHierarchical\nGrouping\nRecall:\nHere\nwe\nmeasure\nGARField\u2019s ability to recall groups at multiple granulari-\nties. Across 5 scenes, we choose one novel viewpoint and\nlabel up to 3 ground truth hierarchical groups for 1-2 ob-\njects. GARField outputs a set of masks as described in Sec-\ntion 4 by clustering image-space features, outputting one\nmask per tree node. We compare against SAM\u2019s automatic\nmask generation by keeping all output masks. We ablate\nGARField in two ways: GARField (-scale) removes scale-\nconditioning; and GARField (-hierarchy) removes the den-\nFigure 10.\nScene-Wide Clustering visualizations for selected\nscenes from Fig. 7.\nsified supervision in Sec. 3.2.2.\nIn Table 2 we report mIOU of the ground truth mask\nwith the highest overlap, either from the set of SAM masks\nor the tree generated by GARField. Because GARField has\nfused groups from multiple perspectives, it results in higher\nfidelity groupings than any single view of SAM, leading to\nhigher mIOU with annotations. Our ablations show that\nscale conditioning and scale densification is necessary for\nhigh quality groupings. Fig. 9 illustrates affinity degrading\nat higher scale with naive supervision.\n6. Limitations\nGARField at its core is distilling outputs from a 2D mask\ngenerator, so if the masks fail to contain a desired group,\nthis will not emerge in 3D. Regions with uneven viewpoints\ncan suffer from artificial group boundaries, for example if\nan object is only viewed from close up, it may never be\ngrouped together because no input view contained it in full.\nWe handle group ambiguity using physical size, but there\ncould be multiple groupings within a single scale. For ex-\nample, conflicts may happen with objects contained in a\ncontainer because the container with and without the ob-\nject can have the same scale. Future work could consider\nother ways to resolve grouping ambiguity such as affor-\ndances. Another consequence of scale-conditioning is that\nobject parts of different sizes branch off the tree separately\nrather than all at once: multiple objects on the same table\nmay appear at different levels of the tree. The tree gener-\nation in this work is a naive greedy algorithm, which can\nresult in spurious small groups at deeper levels, as seen in\nthe trees in the Supplement. Future work may explore more\nsophisticated ways of hierarchical clustering.\n7. Conclusion\nWe present GARField, a method for distilling multi-level\nmasks into a dense scale-conditioned affinity field for hi-\nerarchical 3D scene decomposition. By leveraging scale-\nconditioning, the affinity field can learn meaningful groups\nfrom conflicting 2D group inputs and break apart the scene\nat multiple different levels, which can be used for extracting\n8\nassets at a multitude of granularities. GARField could have\napplications for tasks that require multi-level groupings like\nrobotics, dynamic scene reconstruction, or scene editing.\n8. Acknowledgements\nThis project was funded in part by NSF:CNS-2235013 and\nDARPA Contract No. HR001123C0021. Chung Min and\nJustin are supported in part by the NSF Graduate Research\nFellowship Program under Grant No. DGE 2146752. Any\nopinions, findings, and conclusions or recommendations ex-\npressed in this material are those of the author(s) and do not\nnecessarily reflect the views of the National Science Foun-\ndation.\nReferences\n[1] Pablo Arbel\u00b4aez, Michael Maire, Charless Fowlkes, and Ji-\ntendra Malik. Contour detection and hierarchical image seg-\nmentation. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, 33(5):898\u2013916, 2011. 2\n[2] Yash Bhalgat, Iro Laina, Jo\u02dcao F Henriques, Andrew Zisser-\nman, and Andrea Vedaldi. Contrastive lift: 3d object instance\nsegmentation by slow-fast contrastive fusion.\nNeurIPS,\n2023. 3, 7\n[3] Jiazhong Cen, Zanwei Zhou, Jiemin Fang, Chen Yang, Wei\nShen, Lingxi Xie, Xiaopeng Zhang, and Qi Tian. Segment\nanything in 3d with nerfs. 2023. 3\n[4] Xiaokang Chen, jiaxiang Tang, Diwen Wan, Jingbo Wang,\nand Gang Zeng. Interactive segment anything nerf with fea-\nture imitation. arXiv preprint arXiv:2211.12368, 2023. 3\n[5] T. Cour, F. Benezit, and J. Shi. Spectral segmentation with\nmultiscale graph decomposition. In 2005 IEEE Computer\nSociety Conference on Computer Vision and Pattern Recog-\nnition (CVPR\u201905), pages 1124\u20131131 vol. 2, 2005. 2\n[6] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-\nber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes.\nIn\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 5828\u20135839, 2017. 7\n[7] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kin-\nman, Ryan Hickman, Krista Reymann, Thomas B McHugh,\nand Vincent Vanhoucke. Google scanned objects: A high-\nquality dataset of 3d scanned household items. In 2022 In-\nternational Conference on Robotics and Automation (ICRA),\npages 2553\u20132560. IEEE, 2022. 7\n[8] Zhiwen Fan, Peihao Wang, Yifan Jiang, Xinyu Gong, De-\njia Xu, and Zhangyang Wang.\nNerf-sos: Any-view self-\nsupervised object segmentation on complex scenes. arXiv\npreprint arXiv:2209.08776, 2022. 3\n[9] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensional-\nity reduction by learning an invariant mapping. In 2006 IEEE\ncomputer society conference on computer vision and pattern\nrecognition (CVPR\u201906), pages 1735\u20131742. IEEE, 2006. 4\n[10] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 2\n[11] Tsung-Wei Ke, Jyh-Jing Hwang, Yunhui Guo, Xudong\nWang, and Stella X. Yu. Unsupervised hierarchical semantic\nsegmentation with multiview cosegmentation and clustering\ntransformers. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2022. 2\n[12] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis.\n3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics, 42\n(4), 2023. 6, 7, 1\n[13] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo\nKanazawa, and Matthew Tancik. Lerf: Language embedded\nradiance fields. In International Conference on Computer\nVision (ICCV), 2023. 3, 4\n[14] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten\nRother, and Piotr Doll\u00b4ar. Panoptic segmentation. In Pro-\n9\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 9404\u20139413, 2019. 2\n[15] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. ICCV, 2023. 2, 4, 7, 8\n[16] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitz-\nmann. Decomposing nerf for editing via feature field dis-\ntillation. NeurIPS, 35:23311\u201323330, 2022. 3\n[17] Jun Li, Kai Xu, Siddhartha Chaudhuri, Ersin Yumer, Hao\nZhang, and Leonidas Guibas. Grass: Generative recursive\nautoencoders for shape structures.\nACM Transactions on\nGraphics (TOG), 36(4):1\u201314, 2017. 3\n[18] Yichen Liu, Benran Hu, Junkai Huang, Yu-Wing Tai, and\nChi-Keung Tang. Instance neural radiacne field. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision (ICCV), 2023. 3\n[19] Leland McInnes, John Healy, and Steve Astels.\nhdbscan:\nHierarchical density based clustering. J. Open Source Softw.,\n2(11):205, 2017. 5\n[20] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In ECCV, 2020. 1\n[21] Kaichun Mo, Paul Guerrero, Li Yi, Hao Su, Peter Wonka,\nNiloy Mitra, and Leonidas J Guibas. Structurenet: Hierarchi-\ncal graph networks for 3d shape generation. arXiv preprint\narXiv:1908.00575, 2019. 3\n[22] Kaichun Mo, Shilin Zhu, Angel X Chang, Li Yi, Subarna\nTripathi, Leonidas J Guibas, and Hao Su. Partnet: A large-\nscale benchmark for fine-grained and hierarchical part-level\n3d object understanding. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 909\u2013918, 2019. 7\n[23] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 5\n[24] Despoina Paschalidou, Luc Van Gool, and Andreas Geiger.\nLearning unsupervised hierarchical part decomposition of\n3d objects from a single rgb image.\nIn Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1060\u20131070, 2020. 3\n[25] Jordi Pont-Tuset, Pablo Arbelaez, Jonathan T Barron, Fer-\nran Marques, and Jitendra Malik. Multiscale combinatorial\ngrouping for image segmentation and object proposal gener-\nation. IEEE transactions on pattern analysis and machine\nintelligence, 39(1):128\u2013140, 2016. 2\n[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n[27] Zhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexan-\nder G Schwing, and Oliver Wang. Neural volumetric ob-\nject selection. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 6133\u2013\n6142, 2022. 3\n[28] Jianbo Shi and J. Malik. Normalized cuts and image segmen-\ntation. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 22(8):888\u2013905, 2000. 2\n[29] Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul\u00b4o, Nor-\nman M\u00a8uller, Matthias Nie\u00dfner, Angela Dai, and Peter\nKontschieder. Panoptic lifting for 3d scene understanding\nwith neural fields. arXiv preprint arXiv:2212.09802, 2022.\n3\n[30] Piotr Skalski. Make Sense. https://github.com/\nSkalskiP/make-sense/, 2019. 3\n[31] Richard Socher, Cliff C Lin, Chris Manning, and Andrew Y\nNg.\nParsing natural scenes and natural language with re-\ncursive neural networks. In Proceedings of the 28th inter-\nnational conference on machine learning (ICML-11), pages\n129\u2013136, 2011. 2\n[32] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,\nBrent Yi, Justin Kerr, Terrance Wang, Alexander Kristof-\nfersen, Jake Austin, Kamyar Salahi, et al. Nerfstudio: A\nmodular framework for neural radiance field development.\narXiv preprint arXiv:2302.04264, 2023. 5\n[33] Vadim Tschernezki, Iro Laina, Diane Larlus, and Andrea\nVedaldi.\nNeural Feature Fusion Fields: 3D distillation of\nself-supervised 2D image representations. In 3DV, 2022. 3\n[34] Suhani Vora, Noha Radwan, Klaus Greff, Henning Meyer,\nKyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea\nTagliasacchi, and Daniel Duckworth. Nesf: Neural semantic\nfields for generalizable semantic segmentation of 3d scenes.\narXiv preprint arXiv:2111.13260, 2021. 3\n[35] Yanzhen Wang, Kai Xu, Jun Li, Hao Zhang, Ariel Shamir,\nLigang Liu, Zhiquan Cheng, and Yueshan Xiong. Symme-\ntry hierarchy of man-made objects. In Computer graphics\nforum, pages 287\u2013296. Wiley Online Library, 2011. 3\n[36] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In Proceedings of the IEEE international conference\non computer vision, pages 1395\u20131403, 2015. 2\n[37] Stella X Yu. Segmentation using multiscale cues. In Pro-\nceedings of the 2004 IEEE Computer Society Conference\non Computer Vision and Pattern Recognition, 2004. CVPR\n2004., pages I\u2013I. IEEE, 2004. 2\n[38] Shuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and An-\ndrew Davison. In-place scene labelling and understanding\nwith implicit scene representation. In ICCV, 2021. 3\n10\nGARField: Group Anything with Radiance Fields\nSupplementary Material\nA. Additional Results\nWe show additional figures and videos using GARField for\n1) hierarchical decomposition, 2) global clustering, and 3)\ninteractive selection. All video visualizations use Gaussian\nSplatting [12], as described below.\nA.1. Gaussian Splat Visualizations\nWe use Gaussian Splatting [12] to emphasize the 3D nature\nof GARField and its applications for 3D group extraction.\nHere, for simplicity, we do not optimize GARField directly\nwith gaussians. Instead, we train a NeRF-based GARField\nand a Gaussian Splatting model separately. Then, we assign\nan affinity feature to every gaussian by querying the feature\nfield at the gaussian\u2019s center point. We use these features\nto manipulate the 3D scene, e.g. clustering, selection, and\nfiltering. All implementation described here will be made\npublic. To visualize clusters in 3D, we override each gaus-\nsian\u2019s color parameters to the RGB color of the colormap.\nA.2. 3D Hierarchical Decomposition\nIn the main text, we visualized hand-picked nodes from the\nresulting hierarchy in Main Paper Fig. 6. Here, we exhaus-\ntively visualize entire subtrees of selected scenes by select-\ning the primary region of interest (i.e. desk, dozer, bouquet).\nA.2.1\nFull Tree Visualizations\nIn Fig. 11 and in provided videos we visualize each layer\nof the resulting tree organized by node depth in different\nrows. Each node is shown colorized by the number of inter-\nnal clusters, with the remainder of the tree drawn with low\nopacity to give context. Note that nodes at the same level\ndo not necessarily correspond to the same scale because in-\ntermediate nodes are pruned.\nOne can see how each part is recursively broken into\nsubparts in lower layers of the tree, for example the statue\ngets broken into the base and rest of the statue, followed\nby shield, torso, hair, and etc. Note how some nodes can\ncontain noise or partial clusters, for example the third row,\nlast node of Fig. 11, where the red cluster is a spurious clus-\nter which more suitably belongs to the base of the statue in\nthe prior tree level. Artifacts such as this can happen as a\nresult of our greedy tree building approach, and might be\naddressed with a more sophisticated tree construction algo-\nrithm. Videos of trees showcase the view-consistency of 3D\nscene decomposition, with whole objects being clustered to-\ngether like the bear or dozer, which can then be broken into\ncoherent subparts. The lowest levels of the tree contain very\nfine details such as petals of flowers, or hooves of the sheep.\nThese exhaustive tree visualizations also exhibit limita-\ntions, such as spurious background points being grouped to-\ngether with the object of interest, a behavior which could be\nremedied by more strongly taking geometric proximity into\naccount when constructing the tree. Another failure mode\nis that when view coverage is insufficient, different sides of\nthe same object can be grouped separately. For example, in\nrows 3 and 4 of Fig. 11 the two sides of the statue\u2019s face are\ngrouped differently.\nA.2.2\nCompressed Tree Visualizations\nWe additionally provide videos of compressed trees, where\neach layer of the tree is merged into one visual by distinctly\ncoloring all clusters. Leaf nodes at one layer are further\npropagated to deeper layers of the tree to visualize all clus-\nters at the lowest level, corresponding to the most granular\ndecomposition. Though these visualizations do not show\nhierarchy because they merge all nodes, they illustrate how\nlower layers of the scene decomposition correspond to se-\nmantically meaningful high granularity and higher levels\ncorrespond to coarser granularities.\nA.3. Multi-Scale Clustering\nWe provide video versions of Main Paper Fig. 7 to show-\ncase the view-consistency of the results shown in the im-\nages. These videos first show the global clustering of the\nscene, followed by video renderings of sub-object clusters.\nA.4. Global Clustering\nTo emphasize that GARField can model scene-level group-\nings, we cluster GARField features globally i.e. all gaus-\nsians in a scene. Figures 13 through 19 show all scenes in\nFig. 7 globally clustered at scales 0 to 1, at increments of\n0.05.\nWe also include a video where the excavator scene in\nMain Paper Fig. 1 is globally clustered at three distinct\nscales. We find that GARField successfully groups together\nlarge group in the backgrounds, like the road or bushes on\nthe sidewalk.\nA.5. Interactive Selection\nPeople can use clicks to interact with GARField and extract\ngroups of different sizes, as shown in Fig. 5 of the main pa-\nper. User clicks are transformed into 3D points using pro-\njective geometry (visualized with a red sphere in the video).\nAt a given scale, we select a set of 3D gaussians based on\n1\nTree Depth\nDescending Node Size\nFigure 11. Complete Tree: A complete visualization of all layers and all nodes in the tree from Fig. 6. Colors illustrate different clusters\nwithin each node, and each row visualizes all the nodes at a given depth in the tree, sorted by size.\n2\ntheir affinity with the selected point. To retrieve multiple\ngroups, we query GARField across a range of scales and\nmerge groups with large overlap. In the video, a user can\nextract the excavator, crane, and scooper from Fig. 1 with a\nsingle click.\nB. Experiment details\nFigure 12. Masks for 3D Completeness Experiments: Overlap-\nping masks (egg, noodles, nori masks inside ramen mask) model\nthe desired hierarchical groupings. We labeled these polygonal\nmasks using \u2018Make Sense\u2019 [30], an online tool for mask annota-\ntion.\nB.1. Hierarchical Decomposition\nOnce we select a cluster of interest, we construct a tree by\nrecursively clustering with HDBSCAN. For this process we\nuse an HDBSCAN cluster epsilon of 0.1 and a minimum\ncluster size of 40, fixed for all experiments. The tree is con-\nstructed greedily in a depth-first search, by recursing only\non non-noise clusters. Note that because we add noise clus-\nters back to the tree after constructing it, this can result in\nsmall disappearing regions, like in the lower levels of the\nsucculent scene. These artifacts would better be addressed\nwith a non-greedy tree construction, which we hope to ad-\ndress in future work.\nTo speed tree contruction, we first sub-sample the in-\nput gaussian splat with Open3D\u2019s voxel-downsampling to\nreduce the resolution of points to 0.01\u00d7 the scale being\nqueried, for example an affinity of 0.1 scale downsamples\nto .001 voxel resolution. After tree construction, the result-\ning tree is pruned to remove chains of nodes with one child\nand one parent.\nB.2. Treatment of Clustering Noise\nOne challenge to overcome is the fact that HDBSCAN can\noutput \u2018noise\u2019 clusters, which do not get any cluster la-\nbels. These can arise because of gaussians which do not\nalign well with NeRF geometry, features which are noisy\nbecause they lie on the boundary of two groups, or noise in\nthe trained affinity field. To handle these noise clusters, we\nassign labels to gaussians considered noise with the label\nof the nearest physical clusters computed in the Euclidean\nspace, as opposed to the feature space. We find this pro-\nduces more cohesive results than soft clustering within the\nfeature space itself. During global clustering (Figs. 10, 7)\nthese noise clusters are assigned to custers across the entire\nscene, and during tree decomposition (Fig. 6) these noise\nclusters are locally assigned from the clusters available at\neach node only.\nB.3. 3D Completeness Experiment\nB.3.1\nGround Truth Annotation\nWe annotate ground truth segmentation masks on a ran-\ndomly selected novel view using the online tool \u2018Make\nSense\u2019 [30], employing a polygon shape for the annotation.\nIn Fig. 12, we present the visualization on our state during\nthe data annotation process.\nThe annotation process begins with the assignment of a\nspecific label point to each target object within a given view.\nNote that the selection of the view is randomized, involving\nzooming in, zooming out, or changing the angle to enhance\nthe evaluation of view consistency effectively. These label\npoints serve as the basis for the subsequent mask annota-\ntion, which are made at a varying level of granularity. As\na case in Fig. 21, in bouquet scene, considering the click\npoints from different angles, we annotate the masks at dif-\nferent hierarchical levels: the petal of the flower (fine level),\nthe individual flower (medium level) and the whole bou-\nquet (coarse level). For ground truth masks in other scenes,\nwe follow similar rules, building a mask hierarchy based on\nthe semantic meaning, ranging from fine part of the object\nto coarse whole object. However, note that the number of\nmask levels may vary depending on the complexity and the\nnature semantics in the scene. For example, the bear\u2019s arm\nin the teatime scene, Fig. 20, is only annotated with two\nlevels of hierarchy: the left hand and the whole bear.\nB.3.2\nComplete Visualizations\nA comprehensive presentation of the evaluation results re-\ngarding to the view consistency of GARField is shown in\nFigs. 20, 21, 22, 23, 24. This includes all the scenes not\nshown in the main text.\nFor each scene, we show the\nclicked label points for the annotated randomly selected\nviews, ground truth masks at different hierarchical levels\nand the comparison of the closest masks obtained by SAM\nand GARField. We also provide the zoomed-in images of\nthe results for better visualization.\n3\nB.4. Hierarchical Grouping Recall Experiment\nB.4.1\nGround Truth Annotation\nIn this experiment, we annotate one novel view for each of\nthe five scenes. For each novel view, we mark one or several\nobjects which has a rich hierarchy. The ground truth masks\nare any parts, subparts, or the entire object of the scene that\ncan be considered as groups by a human. Taking the ramen\nscene (Figs. 12, 25) as an example, the parts or subparts of\nthe objects labeled include nori, egg ,egg yolk, noodles, and\nso on. Additionally, the complete soup and the entire ramen\nbowel is also annotated as a group. Unlike the experiments\non 3D completeness, this experiment aims to test whether\nthe model can extract all the reasonable masks of the objects\nwhich contain rich hierarchy. Therefore, we did not stratify\nthe level of the annotated masks.\nB.4.2\nComplete Visualization\nIn Fig. 25, We show the ground truth masks as well as all the\nmethods masks at the finest masks. Note that all the ground\ntruth masks are arranged in descending order of size. In our\nexperiment, we systematically recover all the masks that\ncorresponds to the annotated ground truth through differ-\nent method. For each distinct method employed, which are\nSAM, GARField without scale condition, GARField with-\nout dense supervision, we sequentially showcase the masks\nthat get the highest IOU score of the correspondence to the\nground truth masks. We will release all the ground truth\nannotations for all experiments.\n4\nFigure 13. Global Clustering Results (\u201cBouquet\u201d): Global clusters at smaller scales (s = 0) distinguish between different sections of\nthe bouquet, as well as the two halves of the table. At a larger scale, the bouquet and table are considered whole.\n5\nFigure 14. Global Clustering Results (\u201cDesk\u201d): At larger scales (s = 0.5), the desk is grouped together with the clutter on it e.g.\nkeyboard, card, bird figurine).\n6\nFigure 15. Global Clustering Results (\u201cDonuts\u201d): At a very small scale (s = 0.0), GARField can distinguish between different pieces\nof the breakfast sandwich in the middle of the scene. As scale increases, its grouping shifts quite noticably \u2014 into its two halves, or the\nfull sandwich with the checkerboard packaging.\n7\nFigure 16. Global Clustering Results (\u201cTable\u201d): At the smallest scale (s = 0.0), the global clusters highlight parts of objects e.g. labels\non water bottles, pieces of chocolate.\n8\nFigure 17. Global Clustering Results (\u201cTeatime\u201d): The food, utensils, and the table are included in different clusters at small scales, and\nthe same cluster at larger scales. Parts of the stuffed animals (e.g. sheep hooves, bear nose) can also be seen at s = 0.0.\n9\nFigure 18. Global Clustering Results (\u201cSucculent\u201d): Global clusters at smaller scales (s = 0.0) distinguish between fine features like\nsucculent leaves, while they are considered a single group at larger scales (s = 1.0).\n10\nFigure 19. Global Clustering Results (\u201cLiving Room\u201d: The individual hexagonal tiles on the floor may be grouped separately (s = 0.0)\nor together (s = 0.5).\n11\nFine\nSAM\nGARField\nGT\nInput view+click\nview1\nview2\nview3\nMedium\nSAM\nGARField\nGT\nFigure 20. View Consistency Experiment-Teatime: We constructed two hierarchies, which are fine and medium. These correspond to\nthe semantic meanings of the bear\u2019s left hand and the whole bear, respectively.\n12\nFine\nSAM\nGARField\nGT\nInput view+click\nview1\nview2\nview3\nMedium\nCoarse\nSAM\nGARField\nGT\nSAM\nGARField\nGT\nFigure 21. View Consistency Experiment-Bouquet: We constructed three hierarchies, which are fine medium and coarse. These corre-\nspond to the semantic meanings of the petal of the flower, the individual flower and the whole bouquet, respectively.\n13\nFine\nSAM\nGARField\nGT\nInput view+click\nview1\nview2\nview3\nMedium\nSAM\nGARField\nGT\nFigure 22. View Consistency Experiment-Keyboard: We constructed two hierarchies, which are fine and medium. These correspond to\nthe semantic meanings of single key and the whole keyboard, respectively.\n14\nFine\nSAM\nGARField\nGT\nInput view+click\nview1\nview2\nview3\nMedium\nCoarse\nSAM\nGARField\nGT\nSAM\nGARField\nGT\nFigure 23. View Consistency Experiment-Ramen: We constructed three hierarchies, which are fine, medium and coarse. These corre-\nspond to the semantic meanings of egg yolk , one single egg and the whole soup area, respectively.\n15\nFine\nSAM\nGARField\nGT\nInput view+click\nview1\nview2\nview3\nMedium\nCoarse\nSAM\nGARField\nGT\nSAM\nGARField\nGT\nFigure 24. View Consistency Experiment-Living room: We constructed two hierarchies, which are fine medium and coarse. These\ncorrespond to the semantic meanings of the small orange part of the nerf gun, medium blue part of the nerf gun and the whole nerf gun,\nrespectively.\n16\nFigure 25. Hierarchical Grouping Recall Experiments: We concentrate on methods such as SAM and the ablation study of GARField.\nGARField outperforms SAM in obtaining finer, smaller masks (e.g. capturing all the tiny keys in a keyboard scene). Unlike GARField\nwithout hierarchy grouping, GARField achieves more layered grouping results (e.g. in the ramen scene, it successfully identifies the entire\nramen mask through hierarchical clustering). Furthermore, compared to GARField without dense supervision, GARField provides more\nstable and thorough grouping outcomes (e.g. in the teatime scene, GARField more comprehensively identifies the small labels on the\ncookie bag).\n17\n"
  },
  {
    "title": "UniVG: Towards UNIfied-modal Video Generation",
    "link": "https://arxiv.org/pdf/2401.09084.pdf",
    "upvote": "15",
    "text": "UNIVG: TOWARDS UNIFIED-MODAL VIDEO GENERATION\nLudan Ruan, Lei Tian, Chuanwei Huang, Xu Zhang, Xinyan Xiao\nBaidu Inc.\nBeijing, China\n{ruanludan, tianlei09}@baidu.com\nhuangcw21@gmail.com\n{zhangxu44, xiaoxinyan}@baidu.com\nA cat is eating carrot.\nA girl is floating \nwith fish around.\nA cat is eating carrot.\nA girl is floating \nwith fish around.\nFigure 1: UniVG is a unified video generation framework that supports various video generation tasks, such as Text-to-\nVideo, Image-to-Video, and Text&Image-to-Video. Here displays two sets of examples. Row 1: Input text to generate\nsemantically consistent videos; Row 2: Input image to produce pixel-aligned videos; Row 3: Combine the semantic of\ninput text and image to create semantically aligned videos. All videos are shown on https://univg-baidu.github.\nio.\nABSTRACT\nDiffusion based video generation has received extensive attention and achieved considerable success\nwithin both the academic and industrial communities. However, current efforts are mainly concen-\ntrated on single-objective or single-task video generation, such as generation driven by text, by image,\nor by a combination of text and image. This cannot fully meet the needs of real-world application\nscenarios, as users are likely to input images and text conditions in a flexible manner, either individu-\nally or in combination. To address this, we propose a Unified-modal Video Genearation system that\nis capable of handling multiple video generation tasks across text and image modalities. To this end,\nwe revisit the various video generation tasks within our system from the perspective of generative\narXiv:2401.09084v1  [cs.CV]  17 Jan 2024\nPRIME AI paper\nfreedom, and classify them into high-freedom and low-freedom video generation categories. For\nhigh-freedom video generation, we employ Multi-condition Cross Attention to generate videos that\nalign with the semantics of the input images or text. For low-freedom video generation, we introduce\nBiased Gaussian Noise to replace the pure random Gaussian Noise, which helps to better preserve the\ncontent of the input conditions. Our method achieves the lowest Fr\u00e9chet Video Distance (FVD) on\nthe public academic benchmark MSR-VTT, surpasses the current open-source methods in human\nevaluations, and is on par with the current close-source method Gen2. For more samples, visit\nhttps://univg-baidu.github.io.\n1\nIntroduction\nIn recent years, diffusion-based generative models [1, 2, 3] have significant progress in image generation [4, 5, 6, 7, 8, 9]\nwith applications rapidly expanding to video generation [10, 11, 12, 13, 14]. The majority of video generation\nmodels employ textual descriptions as conditional inputs [15, 16, 17, 18, 10, 11, 19]. However, recent studies\nhave begun to explore the use of image conditions to improve the detail of generated videos [20] or for pixel-level\ncontrolling [21, 22, 13, 12, 23, 24]. Additionally, to enhance the temporal smoothness and spatial resolution of the\ngenerated videos, current approaches often incorporate modules for frame interpolation and super-resolution [10, 11].\nHowever, existing works focus exclusively on single-objective or single-task video generation, where the input is limited\nto text [10, 11, 9, 18], an image [13], or a combination of text and image [12, 24]. This single-objective or single-task\npipeline lacks the necessary flexibility to satisfy all user needs. In practice, users may not have the requisite text or\nimage conditions for input, rendering the model unusable. Alternatively, the introduction of conflicting text-image pairs\nmay lead to the generation of static videos or videos with abrupt transitions (similar conclusion is proposed in [24]).\nIn essence, all models used in video generation are conditional generative models that accept one or more conditions\nto produce a corresponding video. These conditions can be text, images, low-resolution videos, even control signals.\nIn order to construct a versatile video generation system capable of handling multiple video generation tasks, we\nrevisit existing methods and categorize the relevant methods based on generative freedom rather than the task itself.\nThe concept of generative freedom that we propose corresponds to the range of solution space for video generation\nmodels given certain conditions. In this paper, we categorize various video generation tasks as either high-freedom\nor low-freedom video generation. Specifically, high-freedom video generation is characterized by input conditions,\ni.e., text and image, that are weakly constrained at the semantic level, so that the generative model in this scenario has\na larger solution space, providing a higher degree of freedom. Conversely, low-freedom video generation typically\ninvolves strongly constrained conditions at the low-level information (i.e., pixel), such as in image animation and video\nsuper-resolution. These constraints limit the solution space available to the generative model, resulting in a lower degree\nof freedom.\nIn order to better match the characteristics of various video generation tasks, different strategies with varying degrees of\ngenerative freedom should be taken for video generation. For high-freedom video generation, the standard diffusion\nGeneration Paradigm is appropriate and has been extensively utilized in existing research some refs should be provided\n@ludan. Specifically, during training stage, the diffusion model learns the added noise in the forward processing,\nand predicts the target distribution by reversing from a purely random Gaussian distribution during inference stage.\nClassifier guidance [4] and classifier free guidance [25] are employed to align the predicted distribution with the one\nspecified by the input conditions. For low-freedom video generation, the Editing Paradigm is more suitable. Taking\nimage editing [26] as a case in point, a prevalent practice involves adding noise to the original image up to a certain\nlevel and then using text as the editing signal to steer the distribution toward the intended outcome. This approach,\ncompared to generation from scratch, offers better retention of the original input\u2019s content. Video super-resolution has\nutilized a similar technique to that of image editing [23]. However, the Editing Paradigm has a limitation in the form\nof a discrepancy between training stage and inference one. Specifically, the model is trained solely to approximate\nthe target distribution without learning the transition from the conditional distribution to the target distribution. This\ndiscrepancy results in a trade-off-related issue, i.e., the less noise that is introduced, the weaker the model\u2019s ability\nto edit, whereas the more noise that is added, the less capable the model is of preserving the input. In extreme\ncases, when the noise level approaches that of a completely random Gaussian distribution, editing paradigm becomes\nanalogous to generation one, significantly diminishing the model\u2019s capability to preserve the content of the original\ninput. How to reconcile the training and inference stages of editing models to balance their editing capabilities while\npreserving the input is also a problem that needs to be addressed but has been overlooked in previous work.\nIn this paper, we propose a unified system Unified-modal Video Generation (i.e.UniVG), designed to support flexible\nvideo generation conditioned on the arbitrary combination of image and text. To achieve this, we categorize all models\nwithin the system into two groups: high-freedom video generation and low-freedom video generation. For high-freedom\nvideo generation, we present a base model that is capable of the requirements of handling arbitrary combinations\n2\nPRIME AI paper\nof text and image conditions. We accomplish this by enhancing the original cross-attention module of the UNet\narchitecture with a multi-condition cross-attention module. With regard to low-freedom video generation, we propose\ntwo corresponding models that are individually tailored for image animation and video super-resolution task. These\nmodels utilize the editing paradigm, as opposed to the generation paradigm. To reconcile the differences between the\ntraining process based on generation paradigm and the inference process based on editing one, in this paper, we predict\nBiased Gaussian Noise (shorted as BGN) that is directed towards the target distribution, instead of standard Gaussian\nnoise, by refining the objective function during training stage.\nThe proposed UniVG system comprises a Base model, an Image Animation model and a Super Resolution model.\nThe Base model is capable of handling arbitrary combinations of text and image conditions and outputs a video\nsequences of 24 \u00d7 320 \u00d7 576 that are semantically aligned with the input conditions at 8 frames per second (fps). The\nImage Animation model that fine-tuned from the Base model with the additional condition of image concatenation,\ngenerates video frames of 24 \u00d7 320 \u00d7 576 that are pixel-aligned with the input image. The Super Resolution model\nenhances the resolution of each frame to 720 \u00d7 1280 pixels. Compared to previous works, Our UniVG demonstrates\nbetter tasks adaptability for video generation, i.e., handling various video generation tasks within an unified system,\nbut also significantly improvements on the generation details and frame consistency. Experiments have proven the\neffectiveness of our method. On objective metrics, our method significantly surpasses other existing methods, and in\nmanual evaluations, our approach is on par with Gen2 and exceeds the other methods.\nOur contributions can be summarized as follows:\n1. We propose UniVG, the first video generation system that is capable of handling multiple video generation\ntasks, such as semantically aligned text/image-to-video generation, image animation.\n2. We introduce Biased Gaussian Noise and confirm its effectiveness for low-freedom video generation tasks,\nsuch as image animation and super-resolution.\n3. Experiments demonstrate that our method surpasses existing text/image-to-video generation methods in terms\nof objective metrics and is on par with Gen2 in subjective evaluations.\n2\nRelated Work\n2.1\nText-to-Video Generation\nEarly works on Text-to-Video generation utilized GANs [27, 28, 29], VQ-VAEs [30, 31], auto-regressive models [30, 18],\nor transformer structure [32], but were limited by low resolution and suboptimal visual quality. Following the success\nof diffusion models in image generation [4, 5, 6, 7], audio generation [33, 34, 35], and other domains [36, 37, 38],\nVideoDiffusion [39] marked the first application of diffusion models in video generation. Subsequently, Make-A-\nVideo [10] and ImagenVideo [11] expanded video generation into the open domain by extending the 2D U-Net from\ntext-to-image generation to 3D U-Nets. Until then, researchers had been studying video modeling in the pixel space,\nwhich requires massive GPU memory consumption and high training costs. To address this issue, many researchers\nshifted their focus to conducting the diffusion process in the latent space instead of pixel space [8, 15, 40, 16], and\nto improving the sampling efficiency by learning-free sampling [2, 41, 42, 43] or learning-based sampling [44, 45].\nAdditionally, some work has concentrated on reducing the training cost to that of a single video [46] or to no training\ncost at all [47].\n2.2\nImage-to-Video Generation\nGenerating video directly from text is a challenging task with high complexity. A natural thought is to use images\nas an intermediate bridge. Similar to Text-to-Video generation, early works on video prediction used non-diffusion\nmethods [48, 49, 50], which were often limited in low resolutions or specific domains. With the significant advancements\nin diffusion-based methods in Text-to-Video tasks, I2VGen-XL [23] is, to our knowledge, the first to utilize diffusion\nfor open-domain Image-to-Video generation. It replaces the textual CLIP features with image CLIP features within the\ntext-to-video framework, achieving video generation semantically aligned with the input image. Similarly, SVD [13]\nalso fine-tunes from a text-to-video model to an image-to-video model but further concatenates the image\u2019s VAE\nfeatures as a stronger controlling signal. Concurrently, videogen [21], VideoCrafter1 [20], EMU Video [12] and Make\nPixels Dance [24] remain their objective of text-to-video generation, but they introduce Text-to-Image synthesis as\nan intermediate step. The generated images are incorporated into the video generation framework either through\nconcatenation or by CLIP features.\nAs can be inferred from the above, although text-to-video generation and image-to-video generation serve different\napplications, they share many similarities in their technical approaches. Therefore, this paper explores whether a\n3\nPRIME AI paper\nCLIP\n(b) Multi-condition\nCross Attention\n\u2026\nSpatial Layer\nTemporal Layer\nMulti-condition\nCross Attention \n(a) Pipeline of UNIMO-VG\n\u2131!\n\u2131\"\nconcatenate\nA dog is\nswimming\n\u2131#$\n\ud835\udc39!\"\n\ud835\udc39#\nText\nImage\nCLIP\nLinear\nCross Attn\nCross Attn\n\ud835\udc39$\n\ud835\udc39%&'\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\n\u2026\u2026\nFigure 2: Overview of the proposed UniVG system. (a) displays the whole pipeline of UniVG, which includes the Base\nModel FB, the Animation model FA, and the Super Resolution model FSR. (b) illustrates the Multi-condition Cross\nAttention involved in FB and FA.\nsingle framework can unify these two objectives. The primary distinction of our UniVG from earlier works is that we\ndifferentiate various models included in video generation from the perspective of generative freedom rather than task.\n3\nMethod\nThis section presents our proposed Unified-modal Video Generation (i.e. UniVG) for flexibly conditional video\ngeneration. Before diving into specific designs, we first briefly recap the preliminary knowledge of diffusion models in\nSec 3.1. We then illustrate the overview of the whole system UniVG in Sec 3.2, the Multi-condition Cross Attention (i.e.\nMCA) used for high-freedom generation in Sec 3.3, and the Biased Guassian Noise (i.e. BGN) used for low-free\ngeneration in Sec 3.4.\n3.1\nPreliminaries\nDiffusion Models [1] are a class of generative models that are trained to generate the samples by iteratively denoising\nfrom Gaussian noise. During training, timestep t(0 < t \u2264 N) determined noise is added at the original input x to\nget noisy input xt = \u221a\u03b1tx0 + \u221a1 \u2212 \u03b1t\u03f5 (\u03b1 refers to noise schedule and \u03f5 refers to the noise that sampled from\nstandard Gaussian distribution N(0, I)), the model is trained to predict the added noise by either \u03f5-prediction [1] or\nv-prediction [45]. During inference, samples are generated from pure noise xN \u223c N(0, I) by iteratively denoising.\nFurthermore, Conditional Diffusion Models [4, 25] introduce extra conditional signals to bias the predicted distribution\nby xt = p\u03b8(xt+1) + wc(p\u03b8(xt+1, c) \u2212 p\u03b8(xt+1)), where \u03b8 defines the diffusion model, c defines input condition, and\nwc defines guidance scale of control intensity. Another mainstream adopted diffusion models are Latent Diffusion\nModels (LDM) [8], which consists of a Variational AutoEncoder (VAE) [51] and a latent diffusion model that denoising\nin latent hidden space. This approach reduces the complexity of fitting distributions at high resolution. In this paper,\neach single model of UniVG is a Conditional Latent Diffusion Model. That is, the video V consists of F RGB frames\nis first compressed into latent space X \u2208 RF \u00d7C\u00d7H\u00d7W with an image auto encoder, then input into UNet with one or\nmultiple conditions (text condition T, image condition I, and low resolution video V lr).\n3.2\nUniVG\nAs illustrated in Figure 2-(a), our entire UniVG consists of three models: (1) A Base model FB accepts any combination\nof text and image conditions for high-freedom video generation. (2) An Image Animation FA model accepts text-image\npairs to generated video aligned with input image in pixel level, and (3) a Super-resolution model FSR for improving\nspatial resolution. Each model is a latent diffusion model with 3D UNet architecture composed of Spatial Layers,\nTemporal Layers, and Cross Attention Layers. Following previous works [10, 13], the Spatial Layer consists of 2D\nConvolution layer and spatial transformers, while the Temporal Layer consists of 1D temporal Convolution layer and\ntemporal transformers. The cross attention module is used to process semantic control signals, such as text and image\nfeature.\n(1) For the Base Model FB, we employ an image encoder that matches the text encoder of CLIP [52] inspired by\nVideoCrafter1 [20]. To fully utilize the global semantics and local details of input image and text, we utilize all\n4\nPRIME AI paper\n(b) Biased Forward & Backward Process\n(a) Original Forward & Backward Process\nForward process\nIdeal Backward process\nStandard Gaussian Distribution\nCondition Distribution\nTarget Distribution\n\ud835\udc5e(\ud835\udc63!|\ud835\udc63\", \ud835\udc61)\n\ud835\udc5e(\ud835\udc63!|\ud835\udc63\",\ud835\udc63# ,\ud835\udc61)\n\ud835\udc5e(\ud835\udc63!|\ud835\udc63\", \ud835\udc61)\n\ud835\udc5e(\ud835\udc63!|\ud835\udc63\",\ud835\udc61)\n\ud835\udc61$\n\ud835\udc61!\nActual Backward process\nFigure 3: The forward & backward diffusion process with Random Gaussian Noise and Biased Gaussian Noise.\nKI visual tokens FI = {f I\ni }KI\ni=0 and all KT text tokens FT = {f T\ni }KT\ni=0 from the last layer of CLIP ViT. To enable\nthe model with the ability of processing more than one semantic features, we extend the original Cross Attention\nto Multi-condition Cross Attention and introduce its mechanism in Sec 3.3. (2) In order to further generate videos\nthat aligned with the input image at the pixel level, we train the Image Animation model FA by finetuning FB and\nconcatenating the hidden space feature of the first frame as an additional condition. Because of the additional condition,\nthe corresponding channel dimension of the initial convolution layer\u2019s kernel changes from C to 2C. We initialize the\nextra parameters to zero to preserve the performance of the original model. Using either FB or FA, we can obtain video\nframes of 24 \u00d7 320 \u00d7 576. (3) To upscale the clarity of the generated videos, we further finetune a Super-Resolution\nmodel FSR from FB. Since super-resolution tasks have no image condition, the multi-condition cross attention module\nreverts to a regular cross-attention module that only accepts the text condition. During training, FSR accepts videos of\nlow resolution V lr, which are obtained by destroying high-definition videos through RamdomBlur, RandomResize,\nJPEG Compression and so on. As we classify the tasks corresponding to FA, and FSR as low-freedom generation, we\npresent the Biased forward and backward processes from conditional distribution to target distribution by adjusting the\nstandard Gaussian Noise to Biased Gaussian Noise (BGN that is introduced in Sec 3.4).\n3.3\nMulti-condition Cross Attention\nSince our base model FB and Image Animation model FA accept text and image CLIP features, we use Multi-condition\nCross Attention instead of the standard Cross Attention. This module\u2019s architecture mainly follows VideoCrafter [20],\nwhich computes Fout by:\nFout = Softmax\n\u0012QinK\u22ba\nT\n\u221a\nd\n\u0013\n\u00b7 VT + Softmax\n\u0012QinK\u22ba\nI\n\u221a\nd\n\u0013\n\u00b7 VI\nQin = WQ \u00b7 Fin, KT = WKT \u00b7 FT , VT = WVT \u00b7 FT , KI = WKI \u00b7 FI, VI = WVI \u00b7 FI\nwhere dk is the dimensionality of the key/query vectors and Qin is shared between FI and FT . The weights WKI and\nWVI are initialized from WKT and WVT , respectively. Unlike VideoCrafter1 that treats image as an additional input\nenhancement, we regard the image as an equally significant control signal along with the text. This is achieved by\napplying a certain proportion of image dropout throughout the training process. By extension, MCA can accommodate\nmore than two conditions by increasing the number of cross-attention units, without the necessity for retraining (e.g.\nstronger text features). This flexibility greatly reduces the cost of extending the model\u2019s training to handle new\nconditions.\n3.4\nBiased Gaussian Noise\nOur proposed Biased Gaussian Noise is used to transfer condition distribution to target distribution for low-freedom\nvideo generation. As illustrated in Figure 3-(a), the standard forward diffusion process transitions from the target\ndistribution vT to the standard Gaussian distribution \u03f5 via vT\nt\n= \u221a\u03b1tvT + \u221a1 \u2212 \u03b1t\u03f5. However, typically in the\nbackward process, these are the only two distributions involved. This can result in suboptimal editing outcomes when\nthe samples are introduced from a condition distribution vC during inference. To account for the condition distribution\nin both forward and backward processes, we segment the original diffusion into three parts, as illustrated in Figure 3-(b).\nFor timestep between 0 to tm, vt is calculated by the target sample with q(vt|vT , t) = \u221a\u03b1tvT\n0 +\u221a1 \u2212 \u03b1t\u03f5(0 \u2264 t < tm)\nthat followed the original forward process. For timestep between tn to N, vt is calculated by the condition sample with\nq(vt|vC, t) = \u221a\u03b1tvC + \u221a1 \u2212 \u03b1t\u03f5(tn \u2264 t < N). The core problem is how to design q(vt|vC, vT , t) that can smoothly\ntransition from vtm to vtn. To preserve the original diffusion schedule, we introduce a variable for the noise \u03f5, denoted\nas \u03f5\u2032. Assume that for timesteps between tm and tn, we have q(vt|vC, vT , t) = \u221a\u03b1tvT + \u221a1 \u2212 \u03b1t\u03f5\u2032, which meets the\n5\nPRIME AI paper\nconditions vtm = \u221a\u03b1tmvT\n0 + \u221a1 \u2212 \u03b1tm\u03f5 and vtn = \u221a\u03b1tnvC\n0 + \u221a1 \u2212 \u03b1tn\u03f5. Thus, the corresponding \u03f5\u2032 should satisfy\nthe following formulas at timestep tm and tn.\n\u03f5\u2032\ntm = \u03f5, \u03f5\u2032\ntn = \u03f5 +\n\u221a\u03b1tn\n\u221a1 \u2212 \u03b1tn\n\u00d7\n\u0000vC \u2212 vT \u0001\nIn theory, there are an infinite number of solutions to \u03f5\u2032. In this paper, we simply define \u03f5\u2032 as a linear transformation\nfollowing\n\u03f5\u2032\nt = \u03f5 +\n\u221a\u03b1t\n\u221a1 \u2212 \u03b1t\n\u00d7 t \u2212 tm\ntn \u2212 tm\n\u00d7\n\u0000vC \u2212 vT \u0001\n, (tm \u2264 t < tn)\nThe \u03f5\u2032 is sampled from a Biased Gaussian distribution, with its mean value shifted by a weighted combination of vC\nand vT . This bias is crucial to bridging the diffusion process from the condition distribution to the target distribution.\nAlternative solutions for \u03f5\u2032 will be explored in our future work.\n4\nExperiments\n4.1\nImplementation Details\nDataset\nOur training datasets include publicly available academic datasets such as WebVid-10M [53] and LAION-\nCOCO [54], along with self-collected data. WebVid-10M is a large and diverse text-video dataset containing ap-\nproximately 10 million open-domain videos with a resolution of 336 \u00d7 596 pixels. LAION-COCO is a substantial\ntext-image dataset comprising 600 million high-quality images, filtered from LAION-2B and scored using the Aesthetic\nand Semantic Estimate (ASE). To further enhance the quality of the generated videos and to address the issue of\nwatermarks present in WebVid-10M, we continue training on our own curated datasets of videos and images, which\ncontain high-quality visual content. We prepare the self-collected videos by first proportionally compressing them to\n720p resolution along their shorter edge and then segmenting them into 10-second clips. This process yielded 5 million\nhigh-quality text-video pairs. Additionally, our self-curated image dataset includes 1.3 million high-quality text-image\npairs, with a focus on artistic styles.\nTraining\nOur FB is trained with an image:video:video frame ratio of 1:1:1, where the training video frames were\nsampled with equal probability from 8 to 24 frames. We set the text dropout to 0.5 and the image dropout to 0.1. In\naddition, we utilize offset noise [55] with a strength of 0.1 and zero terminal Signal-to-Noise Ratio (SNR) [12]. Offset\nnoise has been proven helpful to be helpful in generating extremely dark or bright images. Zero terminal-SNR has been\nshown to be beneficial for generating high-quality and high-resolution visual content by adding noise to pure Gaussian\nnoise following a rescaled schedule. Both techniques have proven useful in our experiments. Subsequently, we continue\nfinetuning FB to obtain FA and FSR, using Biased Gaussian Noise (BGN) on our self-curated video dataset only.\nFor FA, we set the text dropout to 0.1 and the image dropout to 0.1, the BGN is experimentally set during timesteps\ntm = 600 to tn = 990 since the earlier steps determine the content [26]. For FSR, the text dropout is set to 0.1, and the\nBGN is applied during timesteps tm = 0 to tn = 700 since the later steps deciding the details [26]. We incorporate\n\u03f5\u2212prediction [1] for FB and FA, v\u2212prediction for FSR. The learning rate of all models is fixed at 1 \u00d7 10\u22125. We use\nDPM Solver [56] for accelerating sampling: 50 steps for FB and FA, and 7 steps for FSR since we set initial weight to\n0.7.\nEvaluation\nWe use both objective metrics and human evaluation as the assessment criteria for UniVG. In terms of\nobjective metrics, we follow the previous work [21, 24] to use the test set of MSR-VTT [57] as the standard benchmark.\nThis test set comprises 2, 990 test videos, each corresponding to 20 prompts, totaling 59, 800 prompts. For efficiency\nin our ablation study, we randomly selected one prompt for each test video, ultimately obtaining 2, 990 prompts as\nthe evaluation set. We calculate the CLIPSIM [30] between the generated videos and the corresponding text, and\nFVD [58] between the generated videos and the original videos as comparative metrics. Since some studies [12]\nhave pointed out that objective metrics may not always align with human perception, we primarily employ human\nevaluation. Specifically, we adopt the categorization of video generation metrics from EMU video [12], which\nincludes Visual Quality (including Visual Quality consists of pixel sharpness and recognizable objects/scenes), Motion\nQuality (including frame consistency, motion smoothness and amount of motion), Text Faithfulness (Includes text-\nspatial alignment and text-temporal alignment). Since UniVG supports conditional generation for any combination of\ntext and image, we further introduce Image Faithfulness (Includes text-spatial alignment and text-temporal alignment)\nto measure the alignment performance of generated videos with given images. Evaluators also provide their Overall\nLikeness of the two videos being compared, serving as a complement to the aforementioned sub-indicators. The\nprompts used for human evaluation were collected from the webpages of previous work [10, 11, 21, 24, 16], totaling\n6\nPRIME AI paper\n131 in number. To simplify the annotation process, annotators need only select G (our method is better), S (equally\ngood), or B (other methods are better) for each indicator. To ensure fairness, the videos being compared are randomized\nduring the annotation process. Our six annotators provide a total of 6 \u00d7 131 (prompts) \u00d710 (sub-metrics) = 7, 860\nevaluation results.\n4.2\nComparison with SOTA\nTable 1: Zero-shot performance comparison on MSR-VTT.\nT refers that the input condition contains text and I refers to\nimage. UniVG-HG refers to high-freedom generation within\nour UniVG, UniVG-LG refers to low-freedom generation\nwithin our UniVG. Best in bold\nMethod\nInput\nCLIPSIM\u2191\nFVD\u2193\nCogVideo(En) [18]\nT\n0.2631\n1294\nMagicVideo [59]\nT\n-\n1290\nLVDM [19]\nT\n0.2381\n742\nVideo-LDM [15]\nT\n0.2929\n-\nInternVid [60]\nT\n0.2951\n-\nModelscope [17]\nT\n0.2939\n550\nMake-a-Video [10]\nT\n0.3049\n-\nLatent-Shift [61]\nT\n0.2773\n-\nVideoFactory [9]\nT\n0.3005\n-\nPixelDance [24]\nT+I\n0.3125\n381\nVideogen [21]\nT+I\n0.3127\n-\nUniVG-HG\nT\n0.3014\n336\nUniVG-HG\nT+I\n0.3136\n331\nUniVG-LG\nT+I\n0.3140\n291\n88.99 \n68.40 \n43.34 \n52.98 \n38.36 \n7.24 \n16.79 \n25.18 \n21.37 \n20.55 \n3.77 \n14.81 \n31.48 \n25.65 \n41.09 \nI2VGen\nVideoCrafter1\nSVD\nPika beta\nGen2\nOurs Preferred\nNo Preference\nOther Preferred\nFigure 4: Percentage(%) of Overall Preference of UniVG-\nLG generated videos compared with other SOTA meth-\nods.\nAutomatic Metrics\nDue to the previous work involving both plain text-to-video and image-to-video generations, we\nadopt aligned settings to conduct a fair comparison with them. For text-to-video generation, we use only text as the input\ncondition to generate videos(FB+ FSR). For image-to-video generation, we start by creating images from prompts\nusing SDXL1.0+refiner [62] and then proceed with both high-free generation (FB+ FSR) and low-free generation (FA+\nFSR) using UniVG for the combination of text and images. Since the i3d model [63] used for testing FVD can only\naccept 16 frames, we random sample 16 frames from our generated 24 frame and the test videos in MSR-VTT. The\nresults are shown in Table 1. Whether utilizing only text as the input condition or using both text and image together,\nour method generates videos that outperform those created by other methods under the same settings. Even when using\nonly text as the condition, the videos generated by our method surpass in the FVD metric those generated by others that\nuse both text and image. This demonstrates the capability of UniVG to concurrently process text and image conditions\nand generate high-quality videos with both good visual content and text alignment.\nTable 2: The winning rate (%) of UniVG-LG compared to other methods in human evaluations across 10 sub-\ndimensions(The abbreviations include VQ: Visual Quality, MQ: Motion Quality, TF: Text Faithfulness, IF:Image\nFaithfulness, OL: Overall Likeness, PS: Pixel Sharpness, RO/S: Recognizable Objects/Scenes, FC: Frame Consistency,\nMotion Smoothness, AM: Amount of Motion, TSA: Text-Spatial Alignment, TTA: Text-Temporal Alignment, ISA:\nImage-Spatial Alignment, ITA:Image-Temporal Alignment)\nMethod\nresolution\nVQ\nMQ\nTF\nIF\nOL\nPS\nRO/S\nMS\nFC\nAM\nTSA\nTTA\nISA\nITA\nI2VGen-XL [23]\n32 \u00d7 720 \u00d7 1280\n98.79\n72.85\n87.63\n63.20\n-11.61\n26.24\n27.30\n97.13\n73.76\n85.22\nVideoCrafter1 [20]\n16 \u00d7 576 \u00d7 1024\n73.74\n11.45\n80.61\n20.92\n-12.52\n-3.66\n-3.05\n92.82\n54.35\n53.59\nSVD [13]\n25 \u00d7 576 \u00d7 1024\n28.11\n4.41\n79.06\n12.59\n-41.43\n-0.44\n-4.39\n-14.79\n-1.76\n11.86\nPika beta [64]\n72 \u00d7 576 \u00d7 1024\n55.11\n2.44\n16.34\n9.62\n8.09\n3.76\n6.26\n0.92\n2.14\n27.33\nGen2 [65]\n96 \u00d7 1536 \u00d7 2816\n-34.86\n-2.19\n-3.72\n1.75\n-14.64\n-1.09\n4.04\n-14.54\n3.17\n-2.73\nHuman Evaluation\n1 Due to the fact that automatic metrics are not able to fully reflect an individual\u2019s subjective\nperception of video quality, we further conduct human evaluations. Since many projects are close sourced, in this\npaper, we chose to compare with accessible works, including open-source works I2VGen-XL [23], VideoCrafter1 [20],\n1Done in December 18th. The compared I2VGen is the version released in September.\n7\nPRIME AI paper\nInput text: A cat is eating carrots under the sea. \nA cat is reading\na book at the \nsnow mountain\nA cat is \nyawning\nA cat is \ndrinking beer\n\ud835\udc64! = 9.5\n\ud835\udc64\" = 0.0 \n\ud835\udc64! = 9.5\n\ud835\udc64\" = 5.5 \n\ud835\udc64! = 0.0\n\ud835\udc64\" = 5.5 \nInput Image:\nInput \ud835\udc98 : \ud835\udc64# = 9.5, \ud835\udc64\" = 5.5 \nInput Image:\n(a) Video Generation with different \ud835\udc64\n(b) Video Generation with different text prompts\nFigure 6: The generation cases of FB with different classifier free guidance scale of text wT and wI and different text\nprompts.\nSVD [13], and closed-source works Pika beta [64] and Gen2 [65] that we can obtain the results from website or\ndiscord. All of these are recent works and represent the current best level in text/image-to-video generation. For a fair\ncomparison, except for SVD and Pika beta which only support image input, all other works were kept consistent in\nterms of text and image inputs (The images are generated from text prompt by SDXL1.0 and refiner). The comparison\nresults are shown in Figure 4 and Table 2. Figure 4 shows a comparison of Overall Likeness between videos generated\nby our model (FA + FSR) and those produced by other methods. We find that the videos generated by our method\noutperform open-source Text/Image-to-video models and the closed-source method Pika beta, and are on par with the\nclosed-source method Gen2. Table 2 records the winning rates for other sub-metrics. The formula for calculating the\nwinning rate from GSB is (G \u2212 B)/(G + S + B). The number>0 indicates our method is better, and the number<0\nindicates the other method is better. We found that the prominent advantage of our method lies in its FC, which is\ndue to our adoption of an editing paradigm for low-freedom video generation, benefiting FA in producing more stable\nvideos. Additionally, our generated videos exhibit superior PS compared to videos of similar resolution (except for\ngen2 that generates videos of much larger resolution). This is because we employ BGN, ensuring consistency between\ntraining and inference by directly predicting high-resolution videos from low-resolution ones. One significant drawback\nof our generated videos is the AM, due to our current lack of filtering for static videos in the training data. Addressing\nthis will be part of our future work.\n4.3\nAblation Studies\n160k\n200k\n240k\n270k\n280k\niter\n400\n500\n600\nFVD\n659.22\n521.28\n636.03\n443.81\n477.55\n468.92\n504.04\ntext-to-video\nimage-to-video\ntext+image-to-video\nFigure 5: FVD Scores on MSR-VTT during the Training Process of\nFB.\nTable 3: FVD scores on MSR-VTT\nof FA and FSR that w/ or w/o BGN\nmodel\nBGN\nFVD\u2193\nFA\nw/o BGN\n393.53\nFA\nw/ BGN\n369.27\nFSR\nw/o BGN\n648.68\nFSR\nw/ BGN\n491.32\nTraining Process of Base Model\nAs our base model FB emphasizes the conditional video generation with arbitrary\ncombinations of text and images, a core question is whether the base model can maintain capabilities in text-to-video,\nimage-to-video, and text/image-to-video generation simultaneously. Therefore, we take the checkpoints from the\ntraining process of FB and test their performance in text-to-video, image-to-video, and text&image-to-video generation\nwith FVD. The results are shown in Figure 5, where the overall trends of three curves are downward, indicating that\n8\nPRIME AI paper\nInput\nlow resolution\nw/o BGN, \nInit step = 700\nw/o BGN, \nInit step = 900\nw/ BGN, \nInit step = 700\nFigure 7: The generation cases of FSR w/o or w/ BGN.\nthe training process enhances the base model\u2019s ability to generate videos from text or images. This proves that for\nhigh-freedom video generation, multi-condition video generation can be integrated into one single model.\nBiased Gaussian Noise\nTo demonstrate that Biased Gaussian Noise (BGN) better suits low-freedom video generation\ntasks, we conducted ablation studies on the Animation Model FA and the Video Super Resolution model FSR. The\nresults, shown in Table 3, indicate that BGN enhances video quality in both Image Animation and Super Resolution,\nas evidenced by lower FVDs. It proves more beneficial for Super Resolution, a task with less freedom than Image\nAnimation. Figure 7 visualizes FSR\u2019s performance with and without BGN. The first row shows the original, low-\nresolution input video. Rows 2 and 3 depict the outputs from FSR without BGN, processed from the upscaled\nlow-resolution input and subjected to 700 and 900 denoising steps, respectively. The fourth row presents the output from\nFSR using BGN at timestep tm = 700 to tn = 0, illustrating how a low-resolution video upscaled to high-resolution\ncan be denoised effectively after 700 steps. Each row\u2019s far right offers a magnified view to better showcase the detail in\nthe model-generated content. Our observations indicate that absent BGN, a smaller initial noise step count results in\nless clarity (second row), while a larger count produces a clear yet inconsistent output due to noise overpowering the\noriginal content (third row). With BGN, the model directly predicts high-resolution videos from low-resolution inputs,\nachieving clarity and preserving original features (fourth row). We also acknowledge that BGN\u2019s application can extend\nto other low-freedom video generation tasks, such as frame interpolation and video editing, which we aim to explore in\nfuture work.\nText&Image Conditions\nSince our system is capable of generating videos that align both image and text flexibly, we\nexplore the videos generated under different inference weights for these two conditions. Given text prompt T and image\ncondition I, the inference formula we use is Vout = FB(\u2205) + wT (FB(T) \u2212 FB(\u2205)) + wI(FB(I) \u2212 FB(\u2205)). We\nadjust the classifier free guidance scale of text wT and image wI, the generating videos are shown in Figure 6-(a), we\nfind that adjusting the wT and wI can bias the generated video towards the text or image conditions. Figure 6-a shows\nthat in row 1, wI = 0, FB generates a video that is almost unrelated to the input image, while in row 3, wT = 0, FB\nproduces a video that is almost unrelated to the text. By adjusting both wT and wI to appropriate values, the second\nrow\u2019s generated video retains the characteristics of the input image and is also aligned with the textual semantics. Based\non this feature, our FB can achieve different video generation with the same input image combined with different text\nprompts, as shown in Figure 6-(b). We have also explored whether FA possesses similar properties. However, due to\nthe concatenated image features having much more stronger constraints than text, the generated videos mainly rely on\nimage semantics. Nevertheless, inputting consistent text helps to enhance the dynamic effects of the generated videos.\n9\nPRIME AI paper\n5\nConclusion\nIn this paper, we propose the UniVG system, designed for multi-task conditional video generation that leverages\nboth text and images. We propose a novel categorization of models within our system based on generative freedom,\ndistinguishing between high-freedom and low-freedom video generation. The high-freedom component of UniVG\nfeatures a base model capable of modulating the influence of text and images to produce videos under varying semantic\nconditions. For low-freedom video generation, UniVG includes an Image Animation model and a Super Resolution\nmodel, which generate videos closely pixel-aligned with the input. In low-freedom generation, we propose Biased\nGaussian Noise to replace the standard random Gaussian noise, facilitating a more direct connection between the\nconditional and the target distributions. Our experiments show that our system outperforms existing methods in objective\nassessments and matches Gen2 in subjective evaluations.\nReferences\n[1] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020.\n[2] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole.\nScore-based generative modeling through stochastic differential equations. In ICLR, 2021.\n[3] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In NeurIPS,\n2020.\n[4] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In NeurIPS, 2021.\n[5] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever,\nand Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models.\nIn ICML, 2022.\n[6] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[7] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. In NeurIPS, 2022.\n[8] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022.\n[9] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen Zhu, Jianlong Fu, and Jiaying Liu. Videofactory:\nSwap attention in spatiotemporal diffusions for text-to-video generation. arXiv preprint arXiv:2305.10874, 2023.\n[10] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron\nAshual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. In ICLR, 2023.\n[11] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion\nmodels. arXiv preprint arXiv:2210.02303, 2022.\n[12] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh Rambhatla, Akbar\nShah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing text-to-video generation by explicit image\nconditioning. arXiv preprint arXiv:2311.10709, 2023.\n[13] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam\nLevi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models\nto large datasets. arXiv preprint arXiv:2311.15127, 2023.\n[14] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate\nyour personalized text-to-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023.\n[15] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten\nKreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023.\n[16] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang,\nMing-Yu Liu, and Yogesh Balaji. Preserve your own correlation: A noise prior for video diffusion models. In\nCVPR, 2023.\n[17] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-\nvideo technical report. arXiv preprint arXiv:2308.06571, 2023.\n10\nPRIME AI paper\n[18] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for\ntext-to-video generation via transformers. In ICLR, 2022.\n[19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for\nhigh-fidelity video generation with arbitrary lengths. arXiv preprint arXiv:2211.13221, 2022.\n[20] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv\npreprint arXiv:2310.19512, 2023.\n[21] Xin Li, Wenqing Chu, Ye Wu, Weihang Yuan, Fanglong Liu, Qi Zhang, Fu Li, Haocheng Feng, Errui Ding,\nand Jingdong Wang. Videogen: A reference-guided latent diffusion approach for high definition text-to-video\ngeneration. arXiv preprint arXiv:2309.00398, 2023.\n[22] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv\npreprint arXiv:2309.15818, 2023.\n[23] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and\nJingren Zhou. I2vgen-xl: High-quality image-to-video synthesis via cascaded diffusion models. arXiv preprint\narXiv:2311.04145, 2023.\n[24] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance:\nHigh-dynamic video generation. arXiv preprint arXiv:2311.10982, 2023.\n[25] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna\nRohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. In WACV,\n2023.\n[26] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:\nGuided image synthesis and editing with stochastic differential equations. In ICLR, 2022.\n[27] Yitong Li, Martin Renqiang Min, Dinghan Shen, David E. Carlson, and Lawrence Carin. Video generation from\ntext. In AAAI, 2017.\n[28] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. To create what you tell: Generating videos from\ncaptions. In ACM MM, 2017.\n[29] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin. Generating\nvideos with dynamics-aware implicit generative adversarial networks. In ICLR, 2022.\n[30] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva:\nGenerating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021.\n[31] Gaurav Mittal, Tanya Marwah, and Vineeth N. Balasubramanian. Sync-draw: Automatic video generation using\ndeep recurrent attentive architectures. In ACM MM, 2017.\n[32] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia-Bin Huang, and Devi Parikh.\nLong video generation with time-agnostic VQGAN and time-sensitive transformer. In ECCV, 2022.\n[33] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model\nfor audio synthesis. In ICLR, 2021.\n[34] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wavegrad: Estimating\ngradients for waveform generation. In ICLR, 2021.\n[35] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail A. Kudinov. Grad-tts: A diffusion\nprobabilistic model for text-to-speech. In ICML.\n[36] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M. Susskind, Christian Theobalt, Lingjie Liu, and Ravi Ra-\nmamoorthi. Nerfdiff: Single-image view synthesis with nerf-guided distillation from 3d-aware diffusion. In ICML,\n2023.\n[37] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In CVPR, 2021.\n[38] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu, Jianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining\nGuo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In CVPR, 2023.\n[39] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video\ndiffusion models. In NeurIPS, 2022.\n[40] Chin-Wei Huang, Milad Aghajohari, Joey Bose, Prakash Panangaden, and Aaron C Courville. Riemannian\ndiffusion models. In NeurIPS, 2022.\n11\nPRIME AI paper\n[41] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021.\n[42] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In\nICLR, 2022.\n[43] Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. In ICLR,\n2023.\n[44] Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers for diffusion\nmodels by differentiating through sample quality. In ICLR, 2023.\n[45] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In ICLR, 2022.\n[46] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Weixian Lei, Yuchao Gu, Wynne Hsu, Ying Shan, Xiaohu Qie, and\nMike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. arXiv\npreprint arXiv:2212.11565, 2022.\n[47] Levon Khachatryan, Andranik Movsisyan, Vahram Tadevosyan, Roberto Henschel, Zhangyang Wang, Shant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-image diffusion models are zero-shot video generators.\narXiv preprint arXiv:2303.13439, 2023.\n[48] Ruslan Rakhimov, Denis Volkhonskiy, Alexey Artemov, Denis Zorin, and Evgeny Burnaev. Latent video\ntransformer. In Giovanni Maria Farinella, Petia Radeva, Jos\u00e9 Braz, and Kadi Bouatouch, editors, VISIGRAPP,\n2021.\n[49] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using VQ-VAE and\ntransformers. arXiv preprint arXiv:2104.10157, 2021.\n[50] Yu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N. Metaxas, and Sergey Tulyakov. A good\nimage generator is what you need for high-resolution video synthesis. In ICLR, 2021.\n[51] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014.\n[52] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\nsupervision. In International conference on machine learning. PMLR, 2021.\n[53] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder\nfor end-to-end retrieval. In ICCV, 2021.\n[54] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta,\nTheo Coombes, Jenia Jitsev, and Aran Komatsuzaki. LAION-400M: open dataset of clip-filtered 400 million\nimage-text pairs. arXiv preprint arXiv:2111.02114, 2021.\n[55] Nicholas Guttenberg. Diffusion with offset noise, 1 2023.\n[56] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast ODE solver for\ndiffusion probabilistic model sampling in around 10 steps. In NeurIPS, 2022.\n[57] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and\nlanguage. In CVPR, 2016.\n[58] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross B. Girshick, and Kaiming He. A large-scale study on\nunsupervised spatiotemporal representation learning. In CVPR, 2021.\n[59] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022.\n[60] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinyuan Chen, Yaohui Wang, Ping Luo,\nZiwei Liu, Yali Wang, Limin Wang, and Yu Qiao. Internvid: A large-scale video-text dataset for multimodal\nunderstanding and generation. arXiv preprint arXiv:2307.06942, 2023.\n[61] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin Huang, Jiebo Luo, and Xi Yin. Latent-shift: Latent\ndiffusion with temporal shift for efficient text-to-video generation. arXiv preprint arXiv:2304.08477, 2023.\n[62] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00fcller, Joe Penna, and\nRobin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. arXiv preprint\narXiv:2307.01952, 2023.\n[63] Jo\u00e3o Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In\nCVPR, 2017.\n[64] Pika labs. Accessed December 18, 2023. [Online]. Available: https://www.pika.art/.\n[65] Gen-2. Accessed December 18, 2023. [Online]. Available: https://research.runwayml.com/gen2.\n12\n"
  },
  {
    "title": "VideoCrafter2: Overcoming Data Limitations for High-Quality Video Diffusion Models",
    "link": "https://arxiv.org/pdf/2401.09047.pdf",
    "upvote": "13",
    "text": "VideoCrafter2: Overcoming Data Limitations for High-Quality\nVideo Diffusion Models\nHaoxin Chen\nYong Zhang\u2020\nXiaodong Cun\nMenghan Xia\nXintao Wang\nChao Weng\nYing Shan\nTencent AI Lab\nHomepage: https://ailab-cvc.github.io/videocrafter\nGithub: https://github.com/AILab-CVC/VideoCrafter\nDiscord: https://discord.gg/RQENrunu92\nIn cyberpunk, neonpunk style,\nKung Fu Panda, jump and kick.\nCinematic photo melting pistachio ice\ncream dripping down the cone.\n35mm photograph, film, bokeh.\nLarge motion, surrounded by butterflies,\na girl walks through a lush garden.\nFigure 1. Give a text prompt, our method can generate a video with high visual quality and accurate text-video alignment. Note that it is\ntrained with only low-quality videos and high-quality images. No high-quality videos are required. Best viewed with Acrobat Reader. Click\nthe images to play the video clips.\nAbstract\nText-to-video generation aims to produce a video based\non a given prompt. Recently, several commercial video mod-\nels have been able to generate plausible videos with mini-\nmal noise, excellent details, and high aesthetic scores. How-\never, these models rely on large-scale, well-filtered, high-\nquality videos that are not accessible to the community.\nMany existing research works, which train models using\nthe low-quality WebVid-10M dataset, struggle to generate\nhigh-quality videos because the models are optimized to fit\nWebVid-10M. In this work, we explore the training scheme of\nvideo models extended from Stable Diffusion and investigate\n\u2020 Corresponding author.\nthe feasibility of leveraging low-quality videos and synthe-\nsized high-quality images to obtain a high-quality video\nmodel. We first analyze the connection between the spatial\nand temporal modules of video models and the distribution\nshift to low-quality videos. We observe that full training of\nall modules results in a stronger coupling between spatial\nand temporal modules than only training temporal modules.\nBased on this stronger coupling, we shift the distribution\nto higher quality without motion degradation by finetun-\ning spatial modules with high-quality images, resulting in a\ngeneric high-quality video model. Evaluations are conducted\nto demonstrate the superiority of the proposed method, par-\nticularly in picture quality, motion, and concept composition.\narXiv:2401.09047v1  [cs.CV]  17 Jan 2024\n1. Introduction\nBenefiting from the development of diffusion models [25,\n43], video generation has achieved breakthroughs, particu-\nlarly in basic text-to-video (T2V) generation models. Most\nexisting methods [14, 23, 26, 30, 48, 63] follow a logic to\nobtain video models, i.e., extending a text-to-image (T2I)\nbackbone to a video model by adding temporal modules\nand then training it with videos. Several models train video\nmodels from scratch, while most start from a pre-trained T2I\nmodel, typically Stable Diffusion (SD) [38]. Models can also\nbe categorized into two groups based on the space modeled\nby diffusion models, i.e., pixel-space models [26, 30, 60]\nand latent-space models [14, 23, 48, 63]. The latter is the\ndominant approach. Picture quality, motion consistency, and\nconcept composition are essential dimensions for evaluat-\ning a video model. Picture quality refers to aspects such\nas sharpness, noise, distortion, aesthetic score, and more.\nMotion consistency refers to the appearance consistency\nbetween frames and motion smoothness. Concept composi-\ntion represents the ability to combine different concepts that\nmight not appear simultaneously in real videos.\nRecently, a few commercial startups have released their\nT2V models [5, 6, 8, 9] that can produce plausible videos\nwith minimal noise, excellent details, and high aesthetic\nscores. However, they are trained on a large-scale and well-\nfiltered high-quality video dataset, which is not accessi-\nble to the community and academia. Collecting millions\nof high-quality videos is challenging due to copyright restric-\ntions and post-filtering processing. Though there are a few\nopen-source video datasets collected from the Internet for\nvideo understanding, such as HowTo100M [33], HD-VILA-\n100M [56], and InterVid [51], there exist many issues for\nvideo generation, e.g., poor picture quality and caption, mul-\ntiple clips in one video, and static frames or slides. WebVid-\n10M [12] is the most widely used dataset to train video gen-\neration models in academia. The clips are well-segmented,\nand the diversity is good. However, the picture quality is\nunsatisfactory, and most videos have a resolution of about\n320p. The lack of high-quality datasets poses a significant\nobstacle to training high-quality video models in academia.\nIn this work, we target a quite challenging problem,\ni.e., training high-quality video models without using high-\nquality videos. We dive into the training process of SD-based\nvideo models to analyze the connection between spatial and\ntemporal modules under different training strategies and\ninvestigate the distribution shift to low-quality videos. We\nmake a meaningful observation that the full training of all\nmodules results in a stronger coupling between appearance\nand motion than just training temporal modules. The full\ntraining can achieve more natural motion and tolerate more\nsubsequent modification of spatial modules, which is the\nkey to improving the quality of generated videos. Based on\nthe observation of the connection, we propose a method to\novercome the data limitation by disentangling motion from\nappearance at the data level. Specifically, instead of high-\nquality videos, we exploit low-quality videos to guarantee\nmotion consistency and use high-quality images to ensure\npicture quality and concept composition ability. Benefiting\nfrom the successful T2I models such as SDXL and Mid-\njourney, it is convenient to obtain a large set of images with\nhigh-resolution and complex concept composition. Follow-\ning the guidelines from the analysis, we design a pipeline\nto fully train a video model extended from SD. Then, by ex-\nploring different ways of modifying the spatial and temporal\nmodules of the fully trained model with synthesized images,\nwe identify that finetuning spatial weights only is better than\nother ways, and directly finetuning is better than LORA [29].\nFig. 1 shows visual examples generated by our method.\nOur main contributions are summarized as follows:\n\u2022 We propose a method to overcome the data for training\nhigh-quality video models by disentangling motion from\nappearance at the data level.\n\u2022 We investigate the connection between spatial and tem-\nporal modules, and the distribution shift. We identify the\nkeys to obtain a high-quality video model.\n\u2022 We design an effective pipeline based on the observations,\ni.e., obtaining a fully trained video model first and tuning\nthe spatial modules with synthesized high-quality images.\n2. Related Work\nThe evolution of video generation techniques goes along\nwith the development of generative models. Generative ad-\nversarial networks [17] and variational auto-encoders [18]\nare the commonly used backbones in early research of video\ngeneration, e.g., VGAN [47], TGAN [40], MoCoGAN [44],\nGODIA [52], StyleGAN-V [41], and MCVD [46]. Then,\nsince transformers have been successfully applied in var-\nious fields, they are also introduced for video synthesis,\ne.g., CogVideo [28], VideoGPT [57], NUVA-infinity [53],\nTATS [19], MAGVIT [58], Phenaki [45].\nRecently, diffusion models (DMs) [25, 42, 43] have been\na famous star in generative models, especially in text-to-\nimage (T2I) generation [13, 21, 24, 34, 36\u201339, 62]. For\nvideo generation, Video Diffusion Models (VDMs) are pro-\nposed to model the distribution of videos. VDM [27] is\nthe first to utilize a space-time factorized U-Net to model\nvideos in pixel space for unconditional video generation.\nIt uses an image-video joint training strategy to avoid con-\ncept forgetting. Imagen Video [26] and Make-a-Video [30]\nare two cascade models that target text-to-video generation\nin pixel space. Show-1 [60] is another cascade model that\nuses IF [1] as the base model and LDM extended video\nmodel for super-resolution. Subsequently, LVDM [15, 23]\nand MagicVideo [63] propose to extend LDM [38] to model\nvideos in the latent space of an auto-encoder. Many other\nmethods use the same paradigm, including ModelScope [48],\nAlign Your Latent [14], Hotshot-XL [7], LAVIE [50], PY-\nOCO [20], VideoFactory [49], VPDM [59], VIDM [32], and\nLatent-Shift [11]. Besides text-to-video generation, a few\nmethods, such as [16, 55, 61], attempt to generate videos\nfrom a given image and a prompt as condition.\nSeveral startups release their text-to-video generation ser-\nvices, e.g., Gen-2 [5], Pika Labs [9], Moonvalley [8], and\nGenmo [6]. Their models can generate plausible results with\nminimal noise, excellent details, and high aesthetic scores.\nHowever, those methods are trained with a large-scale well-\nfiltered high-quality video dataset that is not accessible to\nresearchers. The video models are also not available, leading\nto the slow development of downstream tasks to a certain\nextent. The most widely used video dataset is WebVid-10M,\na large-scale dataset of short videos with textual descriptions\nsourced from stock footage sites. The videos are diverse and\nrich in their content, and each video is well-segmented, how-\never, the picture quality is unsatisfactory and most videos\nare 320p. Training a high-quality video model under the data\nlimitation is quite challenging.\nAnimateDiff [22] finds that combining temporal modules\nfrom a video model trained on WebVid-10M and a LORA\nSD model can improve the picture quality of the generated\nvideos. However, this is not a generic model and does not al-\nways work. There are a few severe issues. First, the temporal\nmodules can only be combined with a few selected LORA\nmodels, which makes it not a generic model. Second, since\neach LORA model is a personalized model, the composed\nvideo model might suffer from degraded concept compo-\nsition if the LORA model trained with limited data. Third,\nthe motion quality degenerates when the modules do not\nmatch the LORA model well. Unlike AnimateDiff, we ana-\nlyze the connection between spatial and temporal modules\ninstead of direct combination, and design a pipeline to train a\ngeneric high-quality video model without high-quality video\nby disentangling appearance and motion at the data level.\n3. Method\nWe propose an effective method to overcome the data limi-\ntation for training high-quality video diffusion models. We\nfirst analyze the connection between spatial and temporal\nmodules of SD-based video models under different training\nstrategies. Based on the observations, we then develop a\npipeline to train high-quality video models with just low-\nquality videos and high-quality images, i.e., disentangling\nappearance from motion at the data level.\n3.1. Spatial-temporal Connection Analyses\nBase T2V model.\nTo leverage the prior in SD trained\non a large-scale image dataset, most text-to-video diffu-\nsion models inflate the SD model to a video model by\nadding temporal modules, including Align Your Latent [14],\nAnimateDiff [22], LVDM [23], Magic Video [63], Mod-\nelScopeT2V [48], and LAVIE [50]. They follow VDM [27]\nto use a particular type of 3D-UNet that is factorized over\nspace and time.\nThese models can be categorized into two groups accord-\ning to their training strategies. One is to use videos to learn\nboth the spatial and temporal modules with the SD weights\nas initialization, called full training. The other is to train tem-\nporal modules with spatial ones fixed, called partial training.\nAlign Your Latent and AnimateDiff belong to the first group,\nwhile other T2V models belong to the other group.\nThough these SD-based T2V models have similar archi-\ntectures, they are trained under different training settings.\nWe use one typical model to investigate the connection be-\ntween spatial and temporal modules under the two training\nstrategies. We follow the architecture of the open-source\nVideoCrafter1 [15] with FPS (frames per second) condi-\ntion. We also incorporate the temporal convolution in Mod-\nelScopeT2V [48] to improve temporal consistency.\nParameter Perturbation for Full and Partial Training.\nWe apply the two training strategies to the same architecture\nusing the same data. The model is initialized from pretrained\nSD weights. WebVid-10M [12] is exploited as the training\ndata. To avoid concept forgetting, LAION-COCO 600M [3]\nis also used for video and image joint training. The resolution\nis 512 \u00d7 320. For simplicity, the fully trained video model\nis denoted as MF (\u03b8T , \u03b8S), while the partially trained one is\ndenoted as MP (\u03b8T , \u03b80\nS), where \u03b8T and \u03b8S are the learned\nparameters of the temporal and spatial modules, respectively.\n\u03b80\nS are the original spatial parameters of SD.\nTo evaluate the connection strength between spatial and\ntemporal modules, we perturb the parameters of the specified\nmodules by using another high-quality image dataset DI\nfor finetuning. The image data is JDB [35] that consists of\nsynthesized images from Midjornery [4]. As the JDB has 4\nmillion images, we use LORA [29] for finetuning.\nSpatial Perturbation. We first perturb the spatial param-\neters of the two video models using the image dataset. The\ntemporal parameters are frozen. The perturbation process of\nthe fully trained base model MF can be denoted as:\nM\n\u2032\nF (\u03b8T , \u03b8S + \u2206\u03b8S) \u2190 PERTBLORA\n\u03b8S\n(MF (\u03b8T , \u03b8S), DI),\nwhere PERTBLORA\n\u03b8S\ndenote finetuning MF with respect to \u03b8S\non the image dataset DI using LORA. \u2206\u03b8S represents the\nparameters of the LORA branch. Similarly, we can obtain\nthe perturbed model of the partially trained video model:\nM\n\u2032\nP (\u03b8T , \u03b80\nS + \u2206\u03b8S) \u2190 PERTBLORA\n\u03b8S\n(MP (\u03b8T , \u03b80\nS), DI).\nFor easy understanding, we also use the name \u2018F-Spa-\nLORA\u2019 to denote model M\n\u2032\nF and \u2018P-Spa-LORA\u2019 for M\n\u2032\nP . \u2018F\u2019\ndenotes the fully trained base model while \u2018P\u2019 stands for the\npartially trained model. \u2018Spa\u2019 and \u2018Temp\u2019 mean finetuning\niter = 1K\niter = 10K\nP-Spa-LORA\nF-Spa-LORA\nA professional dancer gracefully performs ballet on stage.\nFigure 2. Perturbing spatial modules using LORA. Best viewed\nwith Acrobat Reader. Click the images to play the video clips.\nRobot dancing in times square\nBeautiful pink rose\nbackground. blooming rose\nflower rotation, close-up.\nP-Temp-LORA\nF-Temp-LORA\nFigure 3. Perturbing temporal modules using LORA. Best viewed\nwith Acrobat Reader. Click the images to play the video clips.\nspatial and temporal modules, respectively. \u2018LORA\u2019 repre-\nsents using LORA for finetuning, while \u2018DIR\u2019 means direct\nfinetuning without LORA. For example, \u2018F-Spatial-LORA\u2019\nrepresents perturbing spatial modules of the fully trained\nT2V model using LORA.\nComparing the synthesized videos of the two resulting\nmodels, we have the following observations. First, the mo-\ntion quality of F-Spa-LORA is more stable than that of\nP-Spa-LORA (see user study in Table 4). The motion of\nP-Spa-LORA becomes worse quickly during the finetuning\nprocess. The more finetuning steps, the video tends to be\nmore still with local flicker (see Fig. 2). While the motion\nof F-Spa-LORA slightly degenerates compared to the fully\ntrained base model. Second, P-Spa-LORA achieves much\nbetter visual quality than F-Spa-LORA (see Fig. 2). The pic-\nture quality and aesthetic score of F-Spa-LORA are greatly\nimproved compared to the partially trained base model (see\nTable 3). Surprisingly, the watermark is also removed. While\nF-Spa-LORA obtains a slight improvement in picture quality\nand aesthetic score, the generated videos are still noisy.\nFrom the two observations, we can conclude that the\ncoupling strength between spatial and temporal modules of\nthe fully trained model is stronger than that of the partially\ntrained model. Because the spatial-temporal coupling of the\npartially trained model can be easily broken, leading to quick\nmotion degeneration and picture quality shift. A stronger\nconnection can tolerate parameter perturbation more than a\nweak one. Our observation can be used to explain the qual-\nity improvement and motion degeneration of AnimateDiff.\nAnimateDiff is not a generic model and only works for se-\nlected personalized SD models. The reason is that its motion\nmodules are obtained with the partially training strategy, and\nthey cannot tolerate large parameter perturbations. When the\npersonalized model does not match the temporal modules,\nboth picture and motion quality will degenerate.\nTemporal Perturbation. The partially trained model has\nonly the temporal modules updated, but the picture quality\nis shifted to the quality of WebVid-10M. Hence, the tempo-\nral modules take responsibility for not only the motion but\nalso the picture quality. We perturb the temporal modules\nwhile fixing the spatial modules with the image dataset. The\nperturbation processes can be denoted as:\nM\n\u2032\u2032\nF (\u03b8T + \u2206\u03b8T , \u03b8S) \u2190 PERTBLORA\n\u03b8T\n(MF (\u03b8T , \u03b8S), DI),\nM\n\u2032\u2032\nP (\u03b8T + \u2206\u03b8T , \u03b80\nS) \u2190 PERTBLORA\n\u03b8T\n(MP (\u03b8T , \u03b80\nS), DI).\nWe observe that the picture quality of P-Temp-LORA (M\n\u2032\u2032\nP )\nis better than F-Temp-LORA (M\n\u2032\u2032\nF ). However, the fore-\nground and background of the videos are more shaky, i.e., the\ntemporal consistency becomes worse (see Fig. 3). The pic-\nture of F-Temp-LORA is improved, but the watermark is still\nthere. Its motion is close to the base model and much better\nthan P-Temp-LORA (see Table 4). Those observations also\nsupport the conclusion obtained from spatial perturbation.\nF-Spa&Temp-LORA\nF-Temp-DIR\nF-Spa-DIR\nF-Spa&Temp-DIR\nAn astronaut is waving his hands on the moon.\nFigure 4. Module selection based on the fully trained T2V model. Best viewed with Acrobat Reader. Click the images to play the video clips.\n3.2. Data-level Disentanglement of Appearance and\nMotion\nSince obtaining a large-scale, high-quality video dataset\nwith high diversity is challenging due to copyright issues,\nwe explore the possibility of training a high-quality video\nmodel without using high-quality videos. We have access to\nlow-quality videos such as WebVid-10M and high-quality\nimages such as JDB. We propose to disentangle motion from\nappearance at the data level, i.e., learning motion from low-\nquality videos while learning picture quality and aesthetics\nfrom high-quality images. We can first train a video model\nwith videos and then fine-tune the video model with images.\nThe keys lie in how to train a video model and how to fine-\ntune it with images.\nAccording to the study of the connection between spatial\nand temporal modules, a fully trained model is more suitable\nfor the subsequent finetuning with high-quality images. This\nis because the strong spatial-temporal coupling can tolerate\nthe parameter perturbation for both spatial and temporal\nmodules without obvious motion degeneration.\nNext, we need to investigate how to fine-tune the base\nmodel with images. In both spatial and temporal perturba-\ntion (Sec. 3.1), the picture quality can be improved but not\nvery significantly. To obtain a greater quality improvement,\nwe evaluate two strategies. One is to involve more parame-\nters, i.e., finetuning both spatial and temporal modules with\nimages. The other is to change the finetuning method, i.e.,\nusing direct finetuning without LORA. We can evaluate the\nfollowing four cases:\nM A\nF (\u03b8T + \u2206\u03b8T , \u03b8S + \u2206\u03b8S) \u2190 PERTBLORA\n\u03b8T ,\u03b8S(MF (\u03b8T , \u03b8S), DI),\nM B\nF (\u03b8T , \u03b8S + \u2206\u03b8S) \u2190 PERTB\u03b8S(MF (\u03b8T , \u03b8S), DI),\nM C\nF (\u03b8T + \u2206\u03b8T , \u03b8S) \u2190 PERTB\u03b8T (MF (\u03b8T , \u03b8S), DI),\nM D\nF (\u03b8T + \u2206\u03b8T , \u03b8S + \u2206\u03b8S) \u2190 PERTB\u03b8T ,\u03b8S(MF (\u03b8T , \u03b8S), DI),\nWhere M A\nF (F-Spa&Temp-LORA) is obtained by following\nthe first strategy, M B\nF , M C\nF , and M D\nF are obtained using\nAnime illustration of a blue pig, riding a scooter near a\nlake, with the sun in the sky\nF-Spa-DIR-LAION\nF-Spa-DIR\nFigure 5. Influence of image data on concept composition. \u2018F-Spa-\nDIR-LAION\u2019 uses the LAION aesthetics V2 as the image data\nwhile \u2018F-Spa-DIR\u2019 uses JDB. Best viewed with Acrobat Reader.\nClick the images to play the video clips.\nthe second strategy. M B\nF (F-Spa-DIR) and M C\nF (F-Temp-\nDIR) represent directly finetuning the spatial and temporal\nmodules, respectively. M D\nF (F-Spa&Temp-DIR) represents\ndirectly finetuning all modules.\nComparing the generated videos of the four models, we\nhave the following observations. First, F-Spa&Temp-LORA\ncan further improve the picture quality of F-Spa-LORA, but\nthe quality is still unsatisfying. The watermark exists in most\ngenerated videos, and the noise is obvious. Second, F-Temp-\nDIR achieves better picture quality than F-Temp-LORA. It\nis also better than F-Spa&Temp-LORA. The watermark is\nremoved or lightened in half of the videos. Third, F-Spa-\nDIR and F-Spa&Temp-DIR achieve the best picture quality\namong the fine-tuned models. However, the motion of F-\nSpa-DIR is better (see Fig. 4 and Table 4). The foreground\nand background of F-Spa&Temp-DIR are flashing in videos\ngenerated by M D\nF , especially local textures.\nBy exploring the finetuning strategies and different mod-\nules, we identify that directly finetuning spatial modules with\nhigh-quality images is the best way to improve the picture\nquality without marginal loss of motion quality. At this point,\nour data-level disentanglement pipeline can be summarized\nas follows: fully training a video model with low-quality\nvideos first and then directly finetuning the spatial modules\nonly with high-quality images.\n3.3. Promotion of Concept Composition\nTo improve the concept composition ability of video models,\nwe propose to use synthesized images with complex con-\ncepts instead of using real images at the partial finetuning\nstage. The success of T2I models such as SDXL and Mid-\njornery is built upon large-scale high-quality images. They\nhave the ability to composite concepts that do not appear\nin the real world. Rather than using their training images,\nwe propose transferring their concept composition ability\nto video models by synthesizing a set of images with com-\nplex concepts. In this way, we can alleviate the burden of\ncapturing both concept and motion well at the same time.\nTo validate the effectiveness of synthesized images, we\nuse JDB and LAION-aesthetics V2 [2] as image data for the\nsecond finetuning stage. LAION-aesthetics V2 consists of\nweb-collected images while JDB contains images synthe-\nsized by Midjourney. We observe that the model trained with\nJDB has much better concept composition ability (see Fig. 5\nand Table 3). More results are in the supplementary material.\n4. Experiments\n4.1. Settings\nData.\nTo overcome data limitations, we utilized WebVid-\n10M [12] as the source of low-quality video data and\nJDB [35] for high-quality image data. WebVid-10M is a\nlarge-scale, diverse video dataset comprising approximately\n10 million text-video pairs. The resolution of most videos is\n336 \u00d7 596, and each video consists of a single shot. During\ntraining, we sample from videos at varying frame rates. JDB\nis a large-scale image dataset featuring around 4 million\nhigh-resolution images from Midjourney, each annotated\nwith a corresponding text prompt. To prevent concept for-\ngetting during the training of the base T2V model, we also\nemploy LAION-COCO[3], a dataset comprising 600 million\ngenerated high-quality captions for publicly available web\nimages, for both image and video training.\nMetrics.\nWe exploit EvalCrafter [31] for quantitative eval-\nuation. EvalCrafter is a benchmark to evaluate text-to-video\ngeneration models, which contains around 18 objective met-\nrics for visual quality, content quality, motion quality, and\ntext-caption alignment. It provides about 512 prompts. The\nobjective metrics are aligned to user opinions from five sub-\njective studies, i.e., motion quality, text-video alignment,\ntemporal consistency, visual quality, and user favor. The\nVisual\nText-Video\nMotion\nTemporal\nQuality\nAlignment\nQuality\nConsistency\nPikaLab\u2217\n63.52\n54.11\n57.74\n69.35\nGen2\u2217\n67.35\n52.30\n62.53\n69.71\nVideoCrafter1\n61.64\n66.76\n56.06\n60.36\nShow-1\n52.19\n62.07\n53.74\n60.83\nAnimeDiff\n58.89\n74.79\n51.38\n56.61\nOurs\n63.28\n64.67\n53.95\n62.02\nTable 1. Comparison on the EvalCrafter benchmark. Higher score\nindicates better performance. * commercial models.\nmotion quality considers three metrics: action recognition,\naverage flow, amplitude classification score, while temporal\nconsistency considers warping error, semantic consistency,\nface consistency. The technical and aesthetic scores in Eval-\nCrafter are adapted from DOVER [54]. Besides, we conduct\nuser studies of human preference since there still lacks a\ncomprehensive objective metric to measure motion quality.\nTraining Details.\nIn Sec 3.1, the two based models\nshare the same architecture, adapted from the open-source\nVideoCrafter1 [15], and incorporate temporal convolution\nfrom ModelScopeT2V [48]. The spatial modules are ini-\ntialized with weights from SD 2.1, and the outputs of the\ntemporal modules are initialized to zeros. The training reso-\nlution is set at 512 \u00d7 320. For joint image and video training,\nwe utilize the low-quality WebVid-10M and LAION-COCO\ndatasets. The models are trained on 32 NVIDIA A100 GPUs\nfor 270K iterations with a batch size of 128. The learning\nrate is set at 5\u00d710\u22125 for all training tasks. When employing\nLORA for the perturbation of temporal or spatial modules,\nwe exclusively use JDB for tuning. The finetuning is con-\nducted on 8 A100 GPUs for 30K iterations with a batch\nsize of 256. Given that the images from JDB have a square\nresolution, we adjust the finetuning resolution to 512 \u00d7 512.\n4.2. Comparison with State-of-the-Art T2V Models\nWe compare our approach with several state-of-the-art T2V\nmodels, including popular commercial models such as Gen-\n2 [5] and Pika Labs [9], as well as open-source mod-\nels like Show-1 [60], VideoCrafter1 [15], and AnimateD-\niff [22]. Gen-2, Pika Labs, and VideoCrafter1 all utilize\nhigh-quality videos for training their T2V models. It is note-\nworthy that AnimateDiff and our models use only the videos\nfrom WebVid-10M. Show-1 employs additional high-quality\nvideos for finetuning to eliminate the watermark in WebVid-\n10M. AnimateDiff is not a generic T2V model; it works only\nwhen the LORA SD model is compatible with its temporal\nmodules. For our comparison, we use its temporal modules\n(second version) based on SD v1.5 and employ Realistic\nVision V2.0 [10] as its corresponding LORA model.\nA bear rummages through a\ndumpster, searching for food\nscraps\nA Time 1980 painting of a boy\ngoing to school\nA man cruises through the city\non a motorcycle, feeling the\nadrenaline rush\nA monkey eating a pizza in\ncentral park, GoPro film style\nGen-2\nPika Labs\nVideoCrafter1\nShow-1\nAnimeDiff\nOurs\nFigure 6. Comparison of different text-to-video generation models. Best viewed with Acrobat Reader. Click the images to play the video clips.\nQuantitative Evaluation.\nThe quantitative results ob-\ntained using EvalCrafter are presented in Table 1. Our\nmethod achieves visual quality comparable to that of\nVideoCrafter1 and Pika Labs, which use high-quality videos\nfor training. This underscores the effectiveness of employing\nhigh-quality images to enhance picture quality and aesthetic\nMethods\nText-Video\nAlignment\nMotion\nQuality\nVisual\nQuality\nOurs vs Gen2\n56%\n46%\n34%\nOurs vs AnimeDiff\n55%\n64%\n69%\nOurs vs Show-1\n59%\n59%\n82%\nOurs vs VideoCrafter1\n61%\n63%\n61%\nTable 2. Human preference. The numbers represent the probability\nof users choosing our method.\nscores. Furthermore, our text-video alignment performance\nis ranked second. In terms of motion quality, our perfor-\nmance surpasses that of Show-1 but falls short of models\nthat utilize a larger volume of videos to learn motion. This\nindicates that our method can enhance visual quality without\nsignificant motion degradation.\nQualitative Evaluation.\nThe visual comparison is de-\npicted in Fig. 6. Additional results are provided in the sup-\nplementary material. The visual quality of our results is on\npar with that of commercial models such as Gen-2 and Pika\nLabs. Since we employ JDB as the image dataset, the picture\nquality of our synthesized videos shifts from WebVid-10M\nto JDB. Regarding motion, our motion quality is superior to\nthat of AnimateDiff and comparable to Show-1. Although\nthe integration of temporal modules with a LORA SD model\ncan enhance visual quality, AnimateDiff experiences motion\ndegradation in generic scenes.\nUser Study.\nFor further evaluation, we conduct a user\nstudy to compare our method with other video models. We\nselect 50 prompts from EvalCrafter, covering diverse scenes,\nstyles, and objects. When comparing a model pair, three\nvideo production experts are asked to select their preferred\nvideo from three options: method 1, method 2, and com-\nparable results, according to the given subject, i.e., visual\nquality, motion quality, and text-video alignment. The re-\nsults are shown in Table 4. Our method has better visual\nquality than AnimateDiff and Show-1 and is comparable to\nVideoCrafter1. Our method is more preferred than Show-1\nand AnimateDiff in motion quality.\n4.3. Strategy Evaluation\nSpatial-temporal Connection.\nIn Sec. 3.1, we show a\nvisual comparison of perturbing the spatial and temporal\nparameters of the fully and partially trained models in Fig. 2\nand Fig. 3. Here we provide the quantitative comparisons\nabout the visual quality in Table 3, including aesthetic and\ntechnical scores from DOVER [54]. We observe that fine-\ntuning the partially trained model can always achieve better\nvisual quality than the fully trained model. It means that the\ndistribution of the partially trained model can be shifted more\neasily. Besides, we conduct a user study asking participants\nMethod\nAesthetic Score (\u2191)\nTechnical Score (\u2191)\nP-base\n34.32\n42.69\nF-base\n46.55\n51.76\nP-Spa-LORA\n78.25\n72.74\nF-Spa-LORA\n77.97\n59.60\nP-Temp-LORA\n77.40\n54.85\nF-Temp-LORA\n66.26\n50.32\nF-Spa-DIR\n82.57\n70.35\nF-Temp-DIR\n82.77\n65.34\nF-Spa&Temp-DIR\n83.59\n67.75\nF-Spa&Temp-LORA\n80.44\n63.61\nF-Spa-DIR-LAION\n67.83\n54.26\nTable 3. Visual quality evaluation of the perturbed T2V models.\nMethods\nMotion Quality\nF-Spa-LORA vs P-Spa-LORA\n87%\nF-Temp-LORA vs P-Temp-LORA\n73%\nF-Spa-DIR vs F-Spa&Temp-DIR\n67%\nTable 4. User study on the motion of the perturbed T2V models.\nto choose a favorable model that has better performance in\nmotion, in terms of foreground/background flash and motion\nflicker. The results are shown in Table 4. It can be observed\nthat the motion quality of perturbed fully trained models is\nbetter. The fully trained model can tolerate larger param-\neter perturbations than the partially trained model. These\nobservations show that the fully trained model has stronger\nspatial-temporal coupling.\nModule Selection.\nAfter selecting the fully trained model\nas the base, we use two strategies to identify the most effec-\ntive module to fine-tune, resulting in four models in Sec. 3.2.\nThe visual quality evaluation of these models is shown in the\nbottom part of Table 3. The visual quality of F-Spa-DIR and\nF-Spa&Temp-DIR is much better than the other two models.\nIt reveals that directly finetuning spatial modules is the key\nto improving picture quality.\nSince F-Spa-DIR and F-Spa&Temp-DIR achieve close\nvisual quality, we conduct a user study on motion quality to\ndetermine the final model. The results are shown in the last\nrow of Table 4. Directly finetuning the spatial modules only\nperforms better in motion. As shown in Fig. 4, F-Spa-DIR\nis more stable and has better temporal consistency than F-\nSpa&Temp-DIR. The latter has obvious flashes in both the\nforeground and background.\nInfluence of Image Data.\nTo verify the effectiveness\nof synthesized images, we use the LAION Aesthetics V2\ndataset and JDB to directly fine-tune the spatial modules\nin the second stage, respectively. The visual examples are\nshown in Fig. 5. It shows that the model trained with\nJDB composite concepts better than the model trained with\nLAION Aesthetics V2. The quantitative evaluation of visual\nquality is shown in Table 3. F-Spa-DIR is much better than\nF-Spa-DIR-LAION in both aesthetic and technical scores.\n5. Conclusion\nTo overcome data limitations, we propose a method for train-\ning high-quality video diffusion models without using high-\nquality videos. We delve into the training schemes of SD-\nbased video models and investigate the coupling strength\nbetween spatial and temporal dimensions. We observe that\nfully trained T2V models exhibit stronger spatial-temporal\ncoupling than partially trained models. Based on this obser-\nvation, we propose disentangling appearance from motion\nat the data level, i.e., by exploiting low-quality videos for\nmotion learning and high-quality images for appearance\nlearning. Additionally, we suggest using synthetic images\nwith complex concepts for finetuning, rather than real im-\nages. Quantitative and qualitative evaluations are conducted\nto demonstrate the effectiveness of the proposed method.\nReferences\n[1] If. Accessed October 22, 2023 [Online] https://github.\ncom/deep-floyd/IF. 2\n[2] Laion-aesthetics.\nAccessed October 22, 2023 [Online]\nhttps://laion.ai/blog/laion-aesthetics/,\n. 6\n[3] Laion-coco. Accessed October 22, 2023 [Online] https:\n//laion.ai/blog/laion-coco/, . 3, 6\n[4] Midjourney. Accessed October 22, 2023 [Online] https:\n//www.midjourney.com/home. 3\n[5] Gen-2. Accessed October 22, 2023 [Online] https://\nresearch.runwayml.com/gen2, . 2, 3, 6\n[6] Genmo. Accessed October 22, 2023 [Online] https://\nwww.genmo.ai/, . 2, 3\n[7] Hotshot-xl. Accessed October 22, 2023 [Online] https:\n//github.com/hotshotco/Hotshot-XL. 3\n[8] Moonvalley. Accessed October 22, 2023 [Online] https:\n//moonvalley.ai/. 2, 3\n[9] Pika labs.\nAccessed October 22, 2023 [Online] https:\n//www.pika.art/. 2, 3, 6\n[10] Realistic vision v2.0. Accessed October 22, 2023 [Online]\nhttps://huggingface.co/ckpt/realistic-\nvision-v20. 6\n[11] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video generation.\narXiv preprint arXiv:2304.08477, 2023. 3\n[12] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisserman.\nFrozen in time: A joint video and image encoder for end-to-\nend retrieval. In ICCV, 2021. 2, 3, 6\n[13] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Ji-\naming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli\nLaine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion\nmodels with an ensemble of expert denoisers. arXiv preprint\narXiv:2211.01324, 2022. 2\n[14] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 2, 3\n[15] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xi-\naodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng\nChen, Xintao Wang, et al. Videocrafter1: Open diffusion\nmodels for high-quality video generation. arXiv preprint\narXiv:2310.19512, 2023. 2, 3, 6\n[16] Xinyuan Chen, Yaohui Wang, Lingjun Zhang, Shaobin\nZhuang, Xin Ma, Jiashuo Yu, Yali Wang, Dahua Lin, Yu\nQiao, and Ziwei Liu. Seine: Short-to-long video diffusion\nmodel for generative transition and prediction. arXiv preprint\narXiv:2310.20700, 2023. 3\n[17] Antonia Creswell, Tom White, Vincent Dumoulin, Kai\nArulkumaran, Biswa Sengupta, and Anil A Bharath. Genera-\ntive adversarial networks: An overview. IEEE signal process-\ning magazine, 35(1):53\u201365, 2018. 2\n[18] Carl Doersch. Tutorial on variational autoencoders. arXiv\npreprint arXiv:1606.05908, 2016. 2\n[19] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang,\nDavid Jacobs, Jia-Bin Huang, and Devi Parikh. Long video\ngeneration with time-agnostic vqgan and time-sensitive trans-\nformer. In ECCV, pages 102\u2013118. Springer, 2022. 2\n[20] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew\nTao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-\nYu Liu, and Yogesh Balaji. Preserve your own correlation: A\nnoise prior for video diffusion models. In ICCV, 2023. 3\n[21] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang,\nDongdong Chen, Lu Yuan, and Baining Guo. Vector quan-\ntized diffusion model for text-to-image synthesis. In CVPR,\n2022. 2\n[22] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao,\nDahua Lin, and Bo Dai. Animatediff: Animate your personal-\nized text-to-image diffusion models without specific tuning.\narXiv preprint arXiv:2307.04725, 2023. 3, 6\n[23] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 2, 3\n[24] Yingqing He, Shaoshu Yang, Haoxin Chen, Xiaodong Cun,\nMenghan Xia, Yong Zhang, Xintao Wang, Ran He, Qifeng\nChen, and Ying Shan.\nScalecrafter: Tuning-free higher-\nresolution visual generation with diffusion models. arXiv\npreprint arXiv:2310.07702, 2023. 2\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 2\n[26] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole,\nMohammad Norouzi, David J Fleet, et al. Imagen video:\nHigh definition video generation with diffusion models. arXiv\npreprint arXiv:2210.02303, 2022. 2\n[27] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion\nmodels. In NeurIPS, 2022. 2, 3\n[28] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie\nTang. Cogvideo: Large-scale pretraining for text-to-video gen-\neration via transformers. arXiv preprint arXiv:2205.15868,\n2022. 2\n[29] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora:\nLow-rank adaptation of large language models. arXiv preprint\narXiv:2106.09685, 2021. 2, 3\n[30] Yaosi Hu, Chong Luo, and Zhenzhong Chen. Make it move:\ncontrollable image-to-video generation with text descriptions.\nIn CVPR, 2022. 2\n[31] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong\nZhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond\nChan, and Ying Shan. Evalcrafter: Benchmarking and evalu-\nating large video generation models, 2023. 6\n[32] Kangfu Mei and Vishal Patel. Vidm: Video implicit diffusion\nmodels. In AAAI, pages 9117\u20139125, 2023. 3\n[33] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,\nMakarand\nTapaswi,\nIvan\nLaptev,\nand\nJosef\nSivic.\nHowTo100M: Learning a Text-Video Embedding by\nWatching Hundred Million Narrated Video Clips. In ICCV,\n2019. 2\n[34] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever,\nand Mark Chen. Glide: Towards photorealistic image genera-\ntion and editing with text-guided diffusion models. In ICML,\n2022. 2\n[35] Junting Pan, Keqiang Sun, Yuying Ge, Hao Li, Haodong\nDuan, Xiaoshi Wu, Renrui Zhang, Aojun Zhou, Zipeng Qin,\nYi Wang, et al. Journeydb: A benchmark for generative image\nunderstanding. arXiv preprint arXiv:2307.00716, 2023. 3, 6\n[36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,\nTim Dockhorn, Jonas M\u00a8uller, Joe Penna, and Robin Rombach.\nSdxl: Improving latent diffusion models for high-resolution\nimage synthesis. arXiv preprint arXiv:2307.01952, 2023. 2\n[37] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022.\n[38] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 2\n[39] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. NeurIPS, 2022. 2\n[40] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal\ngenerative adversarial nets with singular value clipping. In\nICCV, pages 2830\u20132839, 2017. 2\n[41] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. In CVPR, pages\n3626\u20133636, 2022. 2\n[42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 2\n[43] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\nIn ICLR, 2021. 2\n[44] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz. Mocogan: Decomposing motion and content for video\ngeneration. In CVPR, pages 1526\u20131535, 2018. 2\n[45] Ruben Villegas, Mohammad Babaeizadeh, Pieter-Jan Kinder-\nmans, Hernan Moraldo, Han Zhang, Mohammad Taghi Saffar,\nSantiago Castro, Julius Kunze, and Dumitru Erhan. Phenaki:\nVariable length video generation from open domain textual\ndescription. arXiv preprint arXiv:2210.02399, 2022. 2\n[46] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal.\nMcvd-masked conditional video diffusion for prediction, gen-\neration, and interpolation. In NeurIPS, 2022. 2\n[47] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Gen-\nerating videos with scene dynamics. NeurIPS, 29, 2016. 2\n[48] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2,\n3, 6\n[49] Wenjing Wang, Huan Yang, Zixi Tuo, Huiguo He, Junchen\nZhu, Jianlong Fu, and Jiaying Liu. Videofactory: Swap atten-\ntion in spatiotemporal diffusions for text-to-video generation.\narXiv preprint arXiv:2305.10874, 2023. 3\n[50] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo\nYu, Peiqing Yang, et al. Lavie: High-quality video gener-\nation with cascaded latent diffusion models. arXiv preprint\narXiv:2309.15103, 2023. 3\n[51] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin\nMa, Xinyuan Chen, Yaohui Wang, Ping Luo, Ziwei Liu, et al.\nInternvid: A large-scale video-text dataset for multimodal un-\nderstanding and generation. arXiv preprint arXiv:2307.06942,\n2023. 2\n[52] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei\nJi, Fan Yang, Guillermo Sapiro, and Nan Duan. Godiva:\nGenerating open-domain videos from natural descriptions.\narXiv preprint arXiv:2104.14806, 2021. 2\n[53] Chenfei Wu, Jian Liang, Xiaowei Hu, Zhe Gan, Jianfeng\nWang, Lijuan Wang, Zicheng Liu, Yuejian Fang, and Nan\nDuan. Nuwa-infinity: Autoregressive over autoregressive\ngeneration for infinite visual synthesis.\narXiv preprint\narXiv:2207.09814, 2022. 2\n[54] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-\nwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi\nLin. Exploring video quality assessment on user generated\ncontents from aesthetic and technical perspectives. In CVPR,\npages 20144\u201320154, 2023. 6, 8\n[55] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xin-\ntao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter:\nAnimating open-domain images with video diffusion priors.\narXiv preprint arXiv:2310.12190, 2023. 3\n[56] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,\nBei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-\nvancing high-resolution video-language representation with\nlarge-scale video transcriptions. In CVPR, 2022. 2\n[57] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srini-\nvas. Videogpt: Video generation using vq-vae and transform-\ners. arXiv preprint arXiv:2104.10157, 2021. 2\n[58] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han\nZhang, Huiwen Chang, Alexander G Hauptmann, Ming-\nHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked\ngenerative video transformer. In CVPR, 2023. 2\n[59] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video\nprobabilistic diffusion models in projected latent space. In\nCVPR, pages 18456\u201318466, 2023. 3\n[60] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui\nZhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng\nShou. Show-1: Marrying pixel and latent diffusion models\nfor text-to-video generation, 2023. 2, 6\n[61] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao,\nHangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and Jin-\ngren Zhou. I2vgen-xl: High-quality image-to-video synthesis\nvia cascaded diffusion models, 2023. 3\n[62] Yuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-\nworld image variation by aligning diffusion inversion chain.\narXiv preprint arXiv:2305.18729, 2023. 2\n[63] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n"
  },
  {
    "title": "DeepSpeed-FastGen: High-throughput Text Generation for LLMs via MII and DeepSpeed-Inference",
    "link": "https://arxiv.org/pdf/2401.08671.pdf",
    "upvote": "11",
    "text": "DeepSpeed-FastGen: High-throughput Text Generation for\nLLMs via MII and DeepSpeed-Inference\nConnor Holmes, Masahiro Tanaka, Michael Wyatt, Ammar Ahmad Awan, Jeff\nRasley, Samyam Rajbhandari, Reza Yazdani Aminabadi, Heyang Qin, Arash\nBakhtiari, Lev Kurilenko, Yuxiong He\nMicrosoft DeepSpeed (www.deepspeed.ai)\nAbstract\nThe deployment and scaling of large language models (LLMs) have become critical as\nthey permeate various applications, demanding high-throughput and low-latency serving\nsystems. Existing frameworks struggle to balance these requirements, especially for work-\nloads with long prompts. This paper introduces DeepSpeed-FastGen, a system that em-\nploys Dynamic SplitFuse, a novel prompt and generation composition strategy, to deliver\nup to 2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower\n(token-level) tail latency, compared to state-of-the-art systems like vLLM. We leverage a\nsynergistic combination of DeepSpeed-MII and DeepSpeed-Inference to provide an efficient\nand easy-to-use serving system for LLMs. DeepSpeed-FastGen\u2019s advanced implementation\nsupports a range of models and offers both non-persistent and persistent deployment op-\ntions, catering to diverse user scenarios from interactive sessions to long-running applica-\ntions. We present a detailed benchmarking methodology, analyze the performance through\nlatency-throughput curves, and investigate scalability via load balancing. Our evaluations\ndemonstrate substantial improvements in throughput and latency across various models\nand hardware configurations. We discuss our roadmap for future enhancements, includ-\ning broader model support and new hardware backends. The DeepSpeed-FastGen code is\nreadily available for community engagement and contribution.\n1\nIntroduction\nLarge language models (LLMs) like GPT-4 [1] and LLaMA [2] have emerged as a dominant\nworkload in serving a wide range of applications infused with AI at every level. From general\nchat models to document summarization, and from autonomous driving to copilots at every\nlayer of the software stack, the demand to deploy and serve these models at scale has skyrock-\neted. While frameworks like DeepSpeed, PyTorch [3], and several others can regularly achieve\ngood hardware utilization during LLM training, the interactive nature of these applications\nand the poor arithmetic intensity of tasks like open-ended text generation have become the\nbottleneck for inference throughput in existing systems.\nTo this end, frameworks like vLLM [4] powered by PagedAttention and research systems\nlike Orca [5] have significantly improved the performance of inference for LLMs.\nHowever,\nthese systems still struggle to provide consistent quality of service, particularly for workloads\nwith longer prompts. These long prompt workloads are becoming increasingly important as\nmore and more models, like MPT-StoryWriter [6], and systems, such as DeepSpeed Ulysses [7],\n1\narXiv:2401.08671v1  [cs.PF]  9 Jan 2024\nsupport context windows stretching to tens of thousands of tokens. To better understand the\nproblem space, we provide detailed examples of how text generation works for LLMs in two\ndistinct phases called prompt processing and generation. When systems treat them as distinct\nphases, generation will be preempted by prompt processing that risks breaking the service level\nagreements (SLAs).\nToday, we are glad to present DeepSpeed-FastGen, a system that overcomes these limi-\ntations by leveraging the proposed Dynamic SplitFuse technique and offers up to 2.3x higher\neffective throughput, 2x lower latency on average, and up to 3.7x lower (token-level) tail latency,\ncompared to state-of-the-art systems like vLLM. DeepSpeed-FastGen leverages the combination\nof DeepSpeed-MII and DeepSpeed-Inference to provide an easy-to-use serving system.\n2\nExisting LLM Serving Techniques in Literature\nA text generation workload for a single sequence consists of two phases: 1) prompt processing,\nin which the user-provided text is efficiently processed as a batch of tokens to build a key-value\n(KV) cache for attention, and 2) token generation, which will add a single token to that cache\nand generate a new token. Over the course of generating a sequence of text, the model will make\nmany forward calls to the model to generate the full sequence of text. Two major techniques\nhave been proposed in the literature and deployed in systems that address various limitations\nand bottlenecks that may arise during these phases.\n2.1\nBlocked KV Caching\nvLLM identified that memory fragmentation due to large monolithic KV-caches significantly\nreduced the concurrency of LLM serving systems and proposed Paged Attention [8] to enable\nnon-contiguous caches and increase total system throughput. Rather than assign individual\nvariable-sized contiguous chunks of memory, the underlying storage in the KV cache is fixed-\nsized blocks (also known as pages). The blocked KV-cache increases system throughput by\nincreasing the amount of potential sequence concurrency by eliminating KV-cache induced\nmemory fragmentation. Non-contiguous KV cache implementations are also included in Hug-\ngingFace TGI [9] and NVIDIA TensorRT-LLM [10].\n2.2\nContinuous Batching\nIn the past, dynamic batching, in which a server would wait for multiple requests to process\nin phase with each other, was used to improve GPU utilization. However, this approach has\ndrawbacks, as it typically requires padding inputs to identical lengths or stalling the system to\nwait to construct a larger batch.\nRecent advancement in large language model (LLM) inference and serving has been fo-\ncusing on fine granularity scheduling and optimizing memory efficiency. For instance, Orca\nproposes iteration-level scheduling (also known as continuous batching) which makes distinct\nscheduling decisions at each forward pass of the model. This allows requests to join/leave the\nbatch as needed, eliminating the need for padding requests thus improving the overall through-\nput. In addition to Orca, continuous batching has been implemented in NVIDIA TRT-LLM,\nHuggingFace TGI, and vLLM.\nIn current systems, there are two primary approaches to implement continuous batching.\nIn TGI and vLLM, the generation phase is preempted to perform prompt processing (called\ninfill in TGI) before continuing with generation. In Orca, these phases are not distinguished;\ninstead, Orca will add a prompt into the running batch so long as the total number of sequences\n2\ndoesn\u2019t reach a fixed bound. These approaches to varying degrees need to stall generation to\nprocess long prompts (see Section 3.2).\nWe propose a novel prompt and generation composition strategy, Dynamic SplitFuse dis-\ncussed at length in the next section.\n3\nDynamic SplitFuse: A Novel Prompt and Generation\nComposition Strategy\nDeepSpeed-FastGen is built to leverage continuous batching and non-contiguous KV caches to\nenable increased occupancy and higher responsivity for serving LLMs in the data center, similar\nto existing frameworks such as TRT-LLM, TGI, and vLLM. iIn order to achieve a new level of\nperformance, DeepSpeed-FastGen introduces SplitFuse which leverages dynamic prompt and\ngeneration decomposition and unification to further improve continuous batching and system\nthroughput.\n3.1\nThree Performance Insights\nBefore describing Dynamic SplitFuse, we answer three key performance questions that together\nmotivate its design.\n3.1.1\nWhat factors impact the forward pass of a single LLM?\nIn order to effectively schedule, it is necessary to understand what are the relevant independent\nvariables the scheduling loop should control. We observe below that the composition of se-\nquences in a forward pass (the batch size in sequences) has a negligible impact on performance\ncompared to the raw number of tokens in the forward pass. This means an effective scheduler\ncan be built around a single signal, the number of tokens in the forward pass.\nFigure 1: Token latency (ms) is predominantly determined by the number of forward tokens\nrather than the batch sizes.\n3\n3.1.2\nHow does a model\u2019s throughput respond to changing the number of tokens\nin the forward pass?\nAn LLM has two key operating regions with a relatively steep transition. With a small number\nof tokens, the GPU bottleneck is reading the model from memory and so throughput scales with\nthe number of tokens, whereas with many tokens the model is throughput bound by compute\nand sees near-constant throughput. The model should run highly efficiently if all forward passes\nare in the throughput-saturating region.\nFigure 2: The system reaches peak performance (throughput saturation region) as the number\nof tokens in the forward pass increase. Beyond that, near-constant throughput is observed.\n3.1.3\nHow should a pool of tokens be scheduled across multiple forward passes?\nWe observe above that for well-aligned inputs the token-throughput curve is concave, which\nmeans the second derivative is bound to be less than or equal to 0. As an example, let f(x) be\na concave function of latency to throughput for a given model. For a concave function f(x),\nthe following holds:\n0 \u2265 lim\nh\u21920\nf(x + h) \u2212 2f(x) + f(x \u2212 h)\nh2\n0 \u2265 f(x + h) \u2212 2f(x) + f(x \u2212 h)\n2f(x) \u2265 f(x + h) + f(x \u2212 h)\nThis states that for a given pool of 2x tokens to process, the manner that maximizes through-\nput is that which evenly splits them between two batches. More generally, in a system that\nmust consume and process P tokens over F forward passes, the ideal partitioning scheme will\ndivide them equally.\n4\n3.2\nDynamic SplitFuse\nDynamic SplitFuse is a novel token composition strategy for prompt processing and token\ngeneration.\nDeepSpeed-FastGen utilizes Dynamic SplitFuse to run at a consistent forward\nsize by leveraging the capability to take partial tokens from prompts and compose this with\ngeneration.\nA similar approach has been proposed in Sarathi[11] where it splits a prompt\ninto smaller chunks to combine more token generation with prompt processing and to run\nforward passes with consistent batch sizes. In particular, Dynamic SplitFuse performs two key\nbehaviors:\n1. Long prompts are decomposed into much smaller chunks and scheduled across multiple\nforward passes (iterations) with only the final pass performing any generation.\n2. Short prompts will be composed to exactly fill a target token budget. Even short prompts\nmay be decomposed to ensure the budget is precisely met and the forward sizes are well-\naligned.\nTogether, these two techniques provide concrete benefits on all user metrics:\n1. Better Responsiveness: Since long prompts no longer require extremely long forward\npasses to process, the model will provide lower client latency. More forward passes are\nperformed within the same window of time.\n2. Higher Efficiency: Fusion of short prompts to larger token budgets enables the model to\nconsistently operate in the high throughput regime.\n3. Lower variance and better consistency: Since forward passes are of consistent size and\nforward pass size is the primary determinant of performance, the latency of each forward\npass is much more consistent than competing systems as is the perceived generation\nfrequency. There are no preemption or long-running prompts to increase the latency as\nin other prior work. This translates to a reduction of up to 3.7x P95 latency in generation\nas we show in Section 4.\nFigure 3: Illustration of continuous batching strategies. Each block shows the execution of a\nforward pass. An arrow indicates that the forward pass has sequences with one or more tokens\ngenerated. vLLM performs either token generations or prompt processing in a forward pass;\ntoken generation preempts prompt processing. Orca runs prompts at their complete length\nalongside generation. Dynamic SplitFuse performs dynamic composition of fixed-sized batches\ncomposed of both generation and prompt tokens.\n5\nConsequently, DeepSpeed-FastGen will consume tokens from incoming prompts at a rate\nthat permits fast ongoing generation while adding tokens to the system that increase system\nutilization, providing lower latency and higher throughput streaming generation to all clients\nas compared to other state-of-the-art serving systems.\n4\nPerformance Evaluation\nDeepSpeed-FastGen provides state-of-the-art LLM serving performance leveraging its blocked\nKV cache and Dynamic SplitFuse continuous batching.\nWe evaluate DeepSpeed-FastGen\nagainst vLLM [4] on a range of models and hardware configurations following the benchmarking\nmethodology discussed below. The evaluation shows that DeepSpeed-FastGen achieves up to\n2.3x higher effective throughput, 2x lower latency on average, and up to 3.7x lower (token-level)\ntail latency, compared to state-of-the-art systems like vLLM.\n4.1\nBenchmarking Methodology\nWe use two primary quantitative schemes for measuring performance.\n4.1.1\nThroughput-Latency Curves\nTwo key metrics for production readiness are throughput (measured in requests per second)\nand latency (the responsiveness of each request).\nTo measure this, we instantiate multiple\nclients (ranging from 1 to 32) concurrently and send requests (512 in total) to the server. The\nresulting latency of each request is measured at the endpoint and throughput is measured by\nthe end-to-end time to complete the experiment.\n4.1.2\nEffective Throughput\nInteractive applications, such as chat applications, can have more stringent and complex re-\nquirements than can be captured by top-level metrics like end-to-end latency. In particular, we\nfocus on the increasingly popular chat user scenario:\n1. A user initiates a task by sending a prompt.\n2. The system processes the prompt and returns the first token.\n3. Subsequent tokens are streamed to the user as they are produced.\nAt each point in this process there is an opportunity for a system to provide an adverse\nuser experience; for example, if the first token arrives too slowly or the generation appears to\nstop for some time. We propose an SLA framework that considers both of these dimensions.\nAs the lengths of prompts and generated texts vary significantly, affecting computational\ncosts, it is impractical to set rigid SLA values for throughput and latency.\nTherefore, we\ndefine the SLA for prompt latency as \u2014tokens in prompt\u2014\n512\nseconds (=512 tokens/s). Additionally,\nconsidering humans\u2019 reading speed, we set the SLA for generation latency on the Exponential\nMoving Average (EMA) to 2, 4, or 6 tokens/sec.\nRequests that adhere to these SLAs are\ndeemed successful, and the throughput of these successful requests is referred to as effective\nthroughput.\nWe evaluate vLLM and DeepSpeed-FastGen on both Llama-2 7B, Llama-2 13B, and Llama-2\n70B [2] on NVIDIA A100, H100, and A6000.\n6\nFigure 4: Throughput and latency of text generation using Llama 2 70B (Tensor parallelism\nacross 4 A100-80GB GPUs). A normal distribution was applied to prompt and generation\nlengths with averages of 1200/2600 and 128/60, respectively, and a 30% variance.\n4.2\nThroughput-Latency Analysis\nFigure 5: Throughput and latency of text generation using Llama 2 13B (A100-80GB GPU, no\ntensor parallelism). A normal distribution was applied to prompt and generation lengths with\naverages of 1200/2600 and 60/128, respectively, and a 30% variance.\nIn this experiment, DeepSpeed-FastGen outperforms vLLM in both throughput and latency,\nproviding equivalent latency with greater throughput or more responsive latency and the same\nthroughput. On Llama-2 70B with 4 A100x80GB, DeepSpeed-FastGen demonstrates up to 2x\nhigher throughput (1.36 rps vs. 0.67 rps) at identical latency (9 seconds) or up to 50% latency\nreduction (7 seconds vs. 14 seconds) while achieving the same throughput (1.2 rps), as shown\nin Figure 2. These trends hold when evaluating Llama-2 13B as shown in Figure 3.\n4.3\nEffective Throughput Analysis\nUnder the effective throughput analysis that considers both first token latency and the rate\nat which generation occurs, DeepSpeed-FastGen provides up to 2.3x higher throughput than\nvLLM. Figure 4 presents a comparative analysis of the effective throughputs of DeepSpeed-\nFastGen and vLLM. Each plotted point denotes the effective throughput derived from a spe-\n7\nFigure 6: Effective throughput of DeepSpeed-FastGen and vLLM (Llama 2 70B/A100-80GB\nusing tensor parallelism across 4 A100-80GB GPUs.\nA normal distribution was applied to\nprompt and generation lengths with averages of 2600 and 60, respectively, and a 30% variance).\ncific number of clients. As we scaled the number of clients, we initially observed an increase\nin effective throughput.\nHowever, the latency also significantly increases as the number of\nclients approaches the system\u2019s capacity, causing many requests to fail in meeting the SLA.\nConsequently, the effective throughput will either saturate or decrease at some point. From a\nusability perspective, it\u2019s not particularly relevant how many clients are required to achieve the\nmax effective throughput; the maximum point of the line is the optimal serving point.\n4.4\nSignificant Tail Latency Reduction for Token Generation\nFigure 5 displays the P50, P90, and P95 latencies of the generation processes. Both vLLM\nand DeepSpeed-FastGen exhibit similar P50 latencies, but vLLM demonstrates significantly\nhigher latencies for P90 and P95. Regarding the P95 latencies, DeepSpeed-FastGen achieved\na reduction of 3.7 times.\nThis discrepancy is due to a noticeable spike in vLLM\u2019s generation latency when it preempts\nthe ongoing generation to process new prompts.\nIn contrast, DeepSpeed-FastGen typically\nprocesses the prompt and generation for previous requests concurrently, leading to much more\nconsistent generation latency.\n4.5\nScalability using Load Balancing\nDeepSpeed-FastGen offers replica-level load balancing that evenly distributes requests across\nmultiple servers, allowing you to effortlessly scale up your application. Figure 6 illustrates the\nscalability of DeepSpeed-FastGen when employing the load balancer and up to 16 replicas. Note\nthat we utilized 4 A100 GPUs to compute the Llama 2 70B model. In total, we employed 8\nnodes to run the 16 replicas. The results demonstrate nearly perfect scalability with DeepSpeed-\nFastGen. Given that the throughput of a single replica is 1.46 queries/sec, the throughput with\n16 replicas reaches 23.7 queries/sec, marking a linear 16x increase compared to a single replica.\n4.6\nOther Hardware Platforms\nIn addition to the deep analysis on A100, we provide additional benchmarking results for H100\nand A6000. The same performance trends were observed on both A6000 and H100 as A100.\n8\nFigure 7: Per-Token generation Latency of Llama 2 70B/A100-80GB using tensor parallelism\nacross 4 A100-80GB GPUs, 16 clients.\nA normal distribution was applied to prompt and\ngeneration lengths with averages of 2600 and 128, respectively, and a 30% variance.\nFigure 8: Scalability using the load balancing feature. A normal distribution was applied to\nprompt and generation lengths with averages of 2600 and 60, respectively, and a 30% variance.\nFigure 9: Throughput-latency curve and effective throughput of Llama 2 70b using 8 H100\nGPUs. A normal distribution was applied to prompt and generation lengths with averages of\n2600 and 60, respectively, and a 30% variance.\n9\nFigure 10: Throughput-latency curve and effective throughput of Llama 2 7b using A6000. A\nnormal distribution was applied to prompt and generation lengths with averages of 2600 and\n60, respectively, and a 30% variance.\n5\nDeepSpeed-FastGen: Implementation and Usage\nDeepSpeed-FastGen is the synergistic composition of DeepSpeed-MII and DeepSpeed-Inference\nas illustrated in the figure below. Together, both of these software packages provide various\ncomponents of the system including the frontend APIs, the host and device infrastructure to\nschedule batches using Dynamic SplitFuse, optimized kernel implementations, and the tools to\nconstruct new model implementations.\nFigure 11: Architecture of DeepSpeed-FastGen\nThe fastest way to get started with our alpha release of DeepSpeed-FastGen is by running\nthe following command:\npip\ni n s t a l l\ndeepspeed\u2212mii\n10\n5.1\nSupported Models\nWe currently support the following HuggingFace model families1 in this alpha release of DeepSpeed-\nFastGen:\n\u2022 LLaMA and LLaMA-2\n\u2022 Mistral\n\u2022 Facebook OPT\nAll current models leverage HuggingFace APIs in our backend to provide both the model\nweights and the model\u2019s corresponding tokenizer. We plan to add additional models in the\ncoming weeks and months after the initial release. If there are specific model architectures you\nwould like supported, please file an issue and let us know.\n5.2\nDeployment Options\nAll of the examples below are runnable in DeepSpeedExamples. Once installed you have two\noptions for deployment: an interactive non-persistent pipeline or a persistent serving deploy-\nment:\n5.2.1\nNon-persistent Pipeline\nThe non-persistent pipeline deployment is a great and fast way to get started and can be done\nwith only a few lines of code. Non-persistent models are only around for the duration of the\npython script you are running but are useful for temporary interactive sessions.\nfrom mii\nimport\np i p e l i n e\npipe = p i p e l i n e ( \u201d m i s t r a l a i / Mistral \u22127B\u2212v0 .1 \u201d )\noutput = pipe ( [ \u201d Hello ,  my name  i s \u201d , \u201dDeepSpeed  i s \u201d ] ,\nmax new tokens=128)\nprint ( output )\n5.2.2\nPersistent Deployment\nA persistent deployment is ideal for use with long-running and production applications. The\npersistent deployment uses a lightweight GRPC server that can be created using the following\n2 lines:\nimport\nmii\nmii . serve ( \u201d m i s t r a l a i / Mistral \u22127B\u2212v0 .1 \u201d )\nThe above server can be queried by multiple clients at once thanks to the built-in load\nbalancer from DeepSpeed-MII. Creating a client also just takes 2 lines of code:\nc l i e n t = mii . c l i e n t ( \u201d m i s t r a l a i / Mistral \u22127B\u2212v0 .1 \u201d )\noutput = c l i e n t . generate ( \u201dDeepspeed  i s \u201d ,\nmax new tokens=128)\nprint ( output )\nA persistent deployment can be terminated when it is no longer needed:\nc l i e n t . terminate server ()\n1https://huggingface.co/models\n11\n5.3\nAdvanced Installation Information\nFor ease of use and a significant reduction in lengthy compile times that many projects require\nin this space, we distribute a pre-compiled Python wheel covering the majority of our custom\nkernels through a new library called DeepSpeed-Kernels. We have found this library to be very\nportable across environments with NVIDIA GPUs with compute capabilities 8.0+ (Ampere+),\nCUDA 11.6+, and Ubuntu 20+. In most cases, you shouldn\u2019t even need to know this library\nexists as it is a dependency of DeepSpeed-MII and will be installed with it. However, if for\nwhatever reason you need to compile our kernels manually please see our advanced installation\ndocs.\n6\nRelease: Try Out DeepSpeed-FastGen\nWe are very excited to share this DeepSpeed-FastGen alpha release. To get started, please visit\nour GitHub Landing Page page for DeepSpeed-MII.\nDeepSpeed-FastGen is part of the bigger DeepSpeed ecosystem comprising a multitude of\nDeep Learning systems and modeling technologies. To learn more,\n\u2022 Please visit our website for detailed blog posts, tutorials, and helpful documentation.\n\u2022 You can also follow us on our English Twitter, Japanese Twitter, and Chinese Zhihu for\nthe latest news on DeepSpeed.\nDeepSpeed welcomes your contributions! We encourage you to report issues, contribute\nPRs, and join discussions on the DeepSpeed GitHub page. Please see our contributing guide\nfor more details. We are open to collaborations with universities, research labs, and companies,\nsuch as those working together on deep learning research, applying DeepSpeed to empower real-\nworld AI models and applications, and so on. For such requests (and other requests unsuitable\nfor GitHub), please directly email to deepspeed-info@microsoft.com.\n6.1\nRoadmap\nThe following items are on our roadmap and we plan to engage with our community on these\nthrough our GitHub issues and PRs:\n\u2022 Performance improvements\n\u2022 Broader model support\n\u2022 New hardware backends through collaboration with partners\n\u2022 Release performance benchmarks (used to generate plots in this blog)\n\u201cStar\u201d our DeepSpeed GitHub and DeepSpeedMII GitHub repositories if you\nlike our work!\nAcknowledgment\nWe thank the entire DeepSpeed team for their contributions on developing, debugging, testing,\nand releasing the DeepSpeed-FastGen software. We would like to thank various open-source\ncommunity projects including HuggingFace, vLLM, and HuggingFace TGI. We have leveraged\n12\nHF APIs to support models and tokenizers in our alpha release and will continue to add more\nmodels. We especially acknowledge and thank the developers of Flash Attention[12, 13] for\ntheir great work. We have extensively leveraged FlashAttention kernels in our system with\nmodifications that have been acknowledged in our code repositories at appropriate file headers.\nFinally, we want to thank the developers of FasterTransformer [14] kernels that we have used\nin our MoE kernels (released as part of DeepSpeed-Kernels repository).\nReferences\n[1] OpenAI. Gpt-4. https://openai.com/gpt-4, 2023.\n[2] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971,\n2023.\n[3] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:\nAn imperative style, high-performance deep learning library, 2019.\n[4] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\nJoseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large\nlanguage model serving with pagedattention. arXiv preprint arXiv:2309.06180, 2023.\n[5] GI Yu, Jeong JSeong, GW Kim, S Kim, and BG Chun. Orca: A distributed serving system\nfor {Transformer-Based} generative models. 2022.\n[6] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable\nllms. https://www.mosaicml.com/blog/mpt-7b, 2023.\n[7] Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Leon Song, Samyam\nRajbhandari, and Yuxiong He.\nDeepspeed ulysses: System optimizations for enabling\ntraining of extreme long sequence transformer models. arXiv preprint arXiv:2309.14509,\n2023.\n[8] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,\nJoseph E Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large\nlanguage model serving with pagedattention. arXiv preprint arXiv:2309.06180, 2023.\n[9] HuggingFace. Text generation inference. https://huggingface.co/text-generation-\ninference.\n[10] NVIDIA.\nNvidia tensorrt-llm: A tensorrt toolbox for large language model.\nhttps:\n//github.com/NVIDIA/TensorRT-LLM.\n[11] Amey Agrawal, Ashish Panwar, Jayashree Mohan, Nipun Kwatra, Bhargav S Gulavani,\nand Ramachandran Ramjee. Sarathi: Efficient llm inference by piggybacking decodes with\nchunked prefills. arXiv preprint arXiv:2308.16369, 2023.\n[12] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00b4e. FlashAtten-\ntion: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural\nInformation Processing Systems, 2022.\n13\n[13] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning.\n2023.\n[14] NVIDIA. Fastertransformer. https://github.com/NVIDIA/FasterTransformer.\n14\n"
  },
  {
    "title": "SiT: Exploring Flow and Diffusion-based Generative Models with Scalable Interpolant Transformers",
    "link": "https://arxiv.org/pdf/2401.08740.pdf",
    "upvote": "10",
    "text": "SiT: Exploring Flow and Diffusion-based Generative Models\nwith Scalable Interpolant Transformers\nNanye Ma\nMark Goldstein\nMichael S. Albergo\nNicholas M. Boffi\nEric Vanden-Eijnden\u2020\nSaining Xie\u2020\nNew York University\nCode: https://github.com/willisma/SiT\nAbstract\nWe present Scalable Interpolant Transformers (SiT), a\nfamily of generative models built on the backbone of Diffu-\nsion Transformers (DiT). The interpolant framework, which\nallows for connecting two distributions in a more flexible\nway than standard diffusion models, makes possible a mod-\nular study of various design choices impacting generative\nmodels built on dynamical transport: using discrete vs. con-\ntinuous time learning, deciding the objective for the model to\nlearn, choosing the interpolant connecting the distributions,\nand deploying a deterministic or stochastic sampler. By care-\nfully introducing the above ingredients, SiT surpasses DiT\nuniformly across model sizes on the conditional ImageNet\n256x256 benchmark using the exact same backbone, number\nof parameters, and GFLOPs. By exploring various diffusion\ncoefficients, which can be tuned separately from learning,\nSiT achieves an FID-50K score of 2.06.\n1. Introduction\nContemporary success in image generation has come from a\ncombination of algorithmic advances and improvements in\nmodel architecture and progress in scaling neural network\nmodels and data. State-of-the-art diffusion models [25, 51]\nproceed by incrementally transforming data into Gaussian\nnoise as prescribed by an iterative stochastic process, which\ncan be specified either in discrete or continuous time. At an\nabstract level, this corruption process can be viewed as defin-\ning a time-dependent distribution that is iteratively smoothed\nfrom the original data distribution into a standard normal\ndistribution. Diffusion models learn to reverse this corrup-\ntion process and push Gaussian noise backwards along this\nconnection to obtain data samples. The objects learned to\nperform this transformation are conventionally either predict-\ning the noise in the corruption process [25] or predicting the\nscore of the distribution connecting the data and the Gaus-\n\u2020Equal advising.\nModel\nParams(M)\nTraining Steps\nFID \u2193\nDiT-S\n33\n400K\n68.4\nSiT-S\n33\n400K\n57.6\nDiT-B\n130\n400K\n43.5\nSiT-B\n130\n400K\n33.5\nDiT-L\n458\n400K\n23.3\nSiT-L\n458\n400K\n18.8\nDiT-XL\n675\n400K\n19.5\nSiT-XL\n675\n400K\n17.2\nDiT-XL\n675\n7M\n9.6\nSiT-XL\n675\n7M\n8.6\nDiT-XL (cfg=1.5)\n675\n7M\n2.27\nSiT-XL (cfg=1.5)\n675\n7M\n2.06\nTable 1. Scalable Interpolant Transformers. We systematically\nvary the following aspects of a generative model: discrete vs. con-\ntinuous time, model prediction, interpolant, choice of sampler.\nThe resulting Scalable Interpolant Transformer (SiT) model, under\nidentical training compute, consistently outperforms the Diffusion\nTransformer (DiT) in generating 256\u00d7256 ImageNet images. All\nmodels employ a patch size of 2. In this work, we ask the question:\nWhat is the source of the performance gain?\nsian [62], though alternatives of these choices exist [27, 54].\nThe neural network architectures used to represent these\nobjects have been shown to perform well on a variety of tasks.\nWhile diffusion models were originally built upon a U-Net\nbackbone [25, 52], recent work has highlighted that archi-\ntectural advances in vision such as the Vision Transformer\n(ViT) [21] can be incorporated into the standard diffusion\nmodel pipeline to improve performance [48]. The aims of\n[48] were to push improvements on the model side of the\nduality of algorithm and model.\nOrthogonally, significant research effort has gone into ex-\nploring the structure of the noising process, which has been\nshown to lead to performance benefits [32, 35, 36, 58]. Yet,\nmany of these efforts do not move past the notion of passing\n1\narXiv:2401.08740v1  [cs.CV]  16 Jan 2024\nFigure 1. Selected samples from SiT-XL models trained on ImageNet [53] at 512\u00d7512 and 256\u00d7256 resolution with cfg = 4.0, respectively.\ndata through a diffusion process with an equilibrium distri-\nbution, which is a restricted type of connection between the\ndata and the Gaussian. The recently-introduced stochastic\ninterpolants [3] lift such constraints and introduce more flex-\nibility in the noise-data connection. In this paper, we further\nexplore its performance in large scale image generation.\nIntuitively, we expect that the difficulty of the learning\nproblem can be related to both the specific connection cho-\nsen and the object that is learned. Our aim is to clarify these\ndesign choices, in order to simplify the learning problem\nand improve performance. To glean where potential benefits\narise in the learning problem, we start with Denoising Dif-\nfusion Probabilistic Models (DDPMs) and sweep through\nadaptations of: (i) which object to learn, and (ii) which\ninterpolant to choose to reveal best practices.\nIn addition to the learning problem, there is a sampling\nproblem that must be solved at inference time. It has been\nacknowledged for diffusion models that sampling can be\neither deterministic or stochastic [61], and the choice of\nsampling method can be made after the learning process.\nYet, the diffusion coefficients used for stochastic sampling\nare typically presented as intrinsically tied to the forward\nnoising process, which need not be the case in general.\nThroughout this paper, we explore how the design of the\ninterpolant and the use of the resulting model as either a de-\nterministic or a stochastic sampler impact performance. We\ngradually transition from a typical denoising diffusion model\nto an interpolant model by taking a series of orthogonal steps\nin the design space. As we progress, we carefully evaluate\nhow each move away from the diffusion model impacts the\nperformance. In summary, our main contributions are:\n\u2022 By moving from discrete to continuous time, changing\nthe model prediction, interpolant, and the choice of sam-\npler, we observe a consistent performance improvement\nover the Diffusion Transformer (DiT).\n\u2022 We systematically study where these improvements come\nfrom by addressing these factors one by one: learning\nin continuous time; learning a velocity as compared to\na score; changing the interpolant connecting the the two\ndistributions; and using the velocity in an SDE sampler\nwith particular choices of diffusion coefficients.\n\u2022 We show that the SDE for the interpolant can be instanti-\nated using just a velocity model, which we use to push the\nperformance of these methods beyond previous results.\n2\n200K\n300K\n400K\n500K\n600K\n700K\nTraining Steps\n45\n50\n55\n60\n65\n70\n75\n80\nFID-50K\nSiT-S\nDiT-S\n200K\n300K\n400K\n500K\n600K\nTraining Steps\n25\n30\n35\n40\n45\n50\n55\nFID-50K\nSiT-B\nDiT-B\n200K\n300K\n400K\n500K\n600K\n700K\n800K\nTraining Steps\n10\n15\n20\n25\n30\n35\nFID-50K\nSiT-L\nDiT-L\n200K\n300K\n400K\n500K\n600K\n700K\n800K\nTraining Steps\n12.5\n15.0\n17.5\n20.0\n22.5\n25.0\nFID-50K\nSiT-XL\nDiT-XL\nFigure 2. SiT observes improvement in FID across all model sizes. We show FID-50K over training iterations for both DiT and SiT. All\nresults are produced by a Euler-Maruyama sampler using 250 integration steps. Across all model sizes, SiT converges much faster.\n2. SiT: Scalable Interpolant Transformers\nWe begin by recalling the main ingredients for building flow-\nbased and diffusion-based generative models.\n2.1. Flows and diffusions\nIn recent years, a flexible class of generative models based\non turning noise \u03b5 \u223c N(0, I) into data x\u2217 \u223c p(x) have been\nintroduced. These models use the time-dependent process\nxt = \u03b1tx\u2217 + \u03c3t\u03b5,\n(1)\nwhere \u03b1t is a decreasing function of t and \u03c3t is an increas-\ning function of t. Stochastic interpolants and other flow\nmatching methods [1, 3, 39, 42] restrict the process (1) on\nt \u2208 [0, 1], and set \u03b10 = \u03c31 = 1, \u03b11 = \u03c30 = 0, so that xt\ninterpolates exactly between x\u2217 at time t = 0 and \u03b5 and time\nt = 1. By contrast, score-based diffusion models [32, 36, 62]\nset both \u03b1t and \u03c3t indirectly through different formulations\nof a stochastic differential equation (SDE) with N(0, I) as\nits equilibrium distribution. Moreover, they consider the\nprocess xt on an interval [0, T] with T large enough that xT\napproximates a Gaussian distribution.\nProbability flow.\nCommon to both stochastic interpolants\nand score-based diffusion models is the observation that the\nprocess xt can be sampled dynamically using either an SDE\nor a probability flow ordinary differential equation (ODE).\nMore precisely, the marginal probability distribution pt(x)\nof xt in (1) coincides with the distribution of the probability\nflow ODE with a velocity field\n\u02d9Xt = v(Xt, t),\n(2)\nwhere v(x, t) is given by the conditional expectation\nv(x, t) = E[ \u02d9xt|xt = x],\n= \u02d9\u03b1tE[x\u2217|xt = x] + \u02d9\u03c3tE[\u03b5|xt = x].\n(3)\nEquation (3) is derived in Appendix A.1. By solving the\nprobability flow ODE (2) backwards in time from XT =\n\u03b5 \u223c N(0, I), we can generate samples from p0(x), which\napproximates the ground-truth data distribution p(x). We\nrefer to (2) as a flow-based generative model.\nReverse-time SDE.\nThe time-dependent probability distri-\nbution pt(x) of xt also coincides with the distribution of the\nreverse-time SDE [5]\ndXt = v(Xt, t)dt + 1\n2wts(Xt, t)dt + \u221awtd \u00af\nWt,\n(4)\nwhere \u00af\nWt is a reverse-time Wiener process, wt > 0 is an\narbitrary time-dependent diffusion coefficient, v(x, t) is the\nvelocity defined in (3), and where s(x, t) = \u2207 log pt(x) is\nthe score. Similar to v, this score is given by the conditional\nexpectation\ns(x, t) = \u2212\u03c3\u22121\nt\nE[\u03b5|xt = x].\n(5)\nThis equation is derived in Appendix A.3. Similarly, solving\nthe reverse SDE (4) backwards in time from XT = \u03b5 \u223c\nN(0, I) enables generating samples from the approximated\ndata distribution p0(x) \u223c p(x). We refer to (2) as a diffusion-\nbased generative model.\nDesign choices.\nScore-based diffusion models typically\ntie the choice of \u03b1t, \u03c3t, and wt in (4) to the drift and dif-\nfusion coefficients used in the forward SDE that generates\nxt (see (10) below). The stochastic interpolant framework\ndecouples the formulation of xt from the forward SDE and\nshows that there is more flexibility in the choices of \u03b1t, \u03c3t,\nand wt. Below, we will exploit this flexibility to construct\ngenerative models that outperform score-based diffusion\nmodels on standard benchmarks in image generation task.\n2.2. Estimating the score and the velocity\nPractical use of the probability flow ODE (2) and the reverse-\ntime SDE (4) as generative models relies on our ability to\nestimate the velocity v(x, t) and/or score s(x, t) fields that\nenter these equations. The key observation made in score-\nbased diffusion models is that the score can be estimated\nparametrically as s\u03b8(x, t) using the loss\nLs(\u03b8) =\nZ T\n0\nE[\u2225\u03c3ts\u03b8(xt, t) + \u03b5\u22252]dt.\n(6)\nThis loss can be derived by using (5) along with standard\nproperties of the conditional expectation. Similarly, the\n3\nvelocity in (3) can be estimated parametrically as v\u03b8(x, t)\nvia the loss\nLv(\u03b8) =\nZ T\n0\nE[\u2225v\u03b8(xt, t) \u2212 \u02d9\u03b1tx\u2217 \u2212 \u02d9\u03c3t\u03b5\u22252]dt.\n(7)\nWe note that any time-dependent weight can be included un-\nder the integrals in both (6) and (7). These weight factors are\nkey in the context of score-based models when T becomes\nlarge [35]; in contrast, with stochastic interpolants where\nT = 1 without any bias, these weights are less important and\nmight impose numerical stability issue (see Appendix B).\nModel prediction.\nWe observed that only one of the two\nquantities s\u03b8(x, t) and v\u03b8(x, t) needs to be estimated in\npractice. This follows directly from the constraint\nx = E[xt|xt = x],\n= \u03b1tE[x\u2217|xt = x] + \u03c3tE[\u03b5|xt = x],\n(8)\nwhich can be used to re-express the score (5) in terms of the\nvelocity (3) as\ns(x, t) = \u03c3\u22121\nt\n\u03b1tv(x, t) \u2212 \u02d9\u03b1tx\n\u02d9\u03b1t\u03c3t \u2212 \u03b1t \u02d9\u03c3t\n.\n(9)\nWe will use this relation to specify our model prediction.\nConversely, we can also express v(x, t) in terms of s(x, t).\nIn our experiments, we typically learn the velocity field\nv(x, t) and use it to express the score s(x, t) when using\nan SDE for sampling. We include a detailed derivation\nin Appendix A.4.\nNote that by our definitions \u02d9\u03b1t < 0 and \u02d9\u03c3t > 0, so that\nthe denominator of (9) is never zero. Yet, \u03c3t vanishes at\nt = 0, making the \u03c3\u22121\nt\nin (9) appear to cause a singularity\nthere1. This suggests the choice wt = \u03c3t in (4) to cancel this\nsingularity (see Appendix A.3), for which we will explore\nthe performance in the numerical experiments.\n2.3. Specifying the interpolating process\nScore-based diffusion.\nIn Score-Based Diffusion Models\n(SBDM), the choice of \u03b1t and \u03c3t in (1) is typically deter-\nmined by the choice of the forward SDE used to define\nthis process, though recent work has tried to reconsider this\n[32, 35]. For example, if we use the standard variance-\npreserving (VP) SDE [62]\ndXt = \u22121\n2\u03b2tXtdt +\np\n\u03b2tdWt\n(10)\nfor some \u03b2t > 0, it can be shown (see Appendix B) that the\nsolution to (10) has the same probability distribution pt(x)\n1We remark that s(x, t) can be shown to be non-singular at t = 0\nanalytically if the data distribution p(x) has a smooth density [3], though\nthis singularity appears in numerical implementations and losses in general.\nas the process xt defined in (1) for the choice\nVP: \u03b1t = e\u2212 1\n2\nR t\n0 \u03b2sds,\n\u03c3t =\nq\n1 \u2212 e\u2212\nR t\n0 \u03b2sds. (11)\nThe only design flexibility in (11) comes from the choice\nof \u03b2t, because it determines both \u03b1t and \u03c3t2. For example,\nsetting \u03b2t = 1 leads to \u03b1t = e\u2212t and \u03c3t =\n\u221a\n1 \u2212 e\u22122t.\nThis choice necessitates taking T sufficiently large [25] or\nsearching for more appropriate choices of \u03b2t [16, 58, 62] to\nreduce the bias induced by the fact that the solution to the\nSDE (10) only converges to \u03b5 \u223c N(0, I) as t \u2192 \u221e.\nGeneral interpolants.\nIn the stochastic interpolant frame-\nwork, the process (1) is defined explicitly and without any\nreference to a forward SDE, creating more flexibility in the\nchoice of \u03b1t and \u03c3t. Specifically, any choice satisfying:\n(i) \u03b12\nt + \u03c32\nt > 0 for all t \u2208 [0, 1];\n(ii) \u03b1t and \u03c3t are differentiable for all t \u2208 [0, 1];\n(iii) \u03b11 = \u03c30 = 0, \u03b10 = \u03c31 = 1;\ngives a process that interpolates without bias between\nxt=0 = x\u2217 and xt=1 = \u03b5. In our numerical experiments, we\nexploit this design flexibility to test, in particular, the choices\nLinear:\n\u03b1t = 1 \u2212 t,\n\u03c3t = t,\nGVP:\n\u03b1t = cos( 1\n2\u03c0t),\n\u03c3t = sin( 1\n2\u03c0t),\n(12)\nwhere GVP refers to a generalized VP which has constant\nvariance across time for any endpoint distributions with the\nsame variance. We note that the fields v(x, t) and s(x, t)\nentering (2) and (4) depend on the choice of \u03b1t and \u03c3t,\nand typically must be specified before learning3. This is in\ncontrast to the diffusion coefficient w(t), as we now describe.\n2.4. Specifying the diffusion coefficient\nAs stated earlier, the SBDM diffusion coefficient used in the\nreverse SDE (4) is usually taken to match that of the forward\nSDE (10). That is, one sets wt = \u03b2t. In the stochastic\ninterpolant framework, this choice is again subject to greater\nflexibility: any wt > 0 can be used. Interestingly, this choice\ncan be made after learning, as it does not affect the velocity\nv(x, t) or the score s(x, t). In our experiments, we exploit\nthis flexibility by considering the choices listed in Table 2.\n2.5. Time-discretization and link with DDPM\nDuring inference, continuous time models must be dis-\ncretized when solving the probability flow ODE (2) and\nthe reverse-time SDE (4). This allows us to make a link with\nDDPMs [25].\n2VP is the only linear scalar SDE with an equilibrium distribution [58];\ninterpolants extend beyond \u03b12\nt + \u03c32\nt = 1 by foregoing the requirement of\nan equilibrium distribution.\n3The requirement to learn and sample under one choice of path specified\nby \u03b1t, \u03c3t, at training time may be relaxed and is explored in [2].\n4\nIncreasing transformer sizes\n\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nFigure 3. Increasing transformer size increases sample quality. Best viewed zoomed-in. We sample from all 4 of our SiT model (SiT-S,\nSiT-B, SiT-L and SiT-XL) after 400K training steps using the same latent noise and class label.\nExpression for wt\n\u03b2t = \u22122\u03c3t( \u02d9\u03c3t \u2212 \u03c3t \u02d9\u03b1t\n\u03b1t )\n\u03c3t\n1 \u2212 t\nsin2(\u03c0t)\n(cos(\u03c0t) + 1)2\n(cos(\u03c0t) \u2212 1)2\nTable 2. Diffusion coefficients. They can be specified after learning\nto maximize performance. wt = \u03b2t in the first row corresponds\nto SBDM (see Eq. (11)), which is coupled to the forward process.\nA detailed derivation is provided in Appendix B. wt = \u03c3t in the\nsecond row is used to eliminate the singularity at t = 0 following\nthe explanation at the end of Sec. 2.2. In other choices we decouple\nwt from the interpolant and experiment with removing diffusivity\nat different times in sampling.\nAssuming that we discretize time using a grid 0 = t0 <\nt1 < t2 < . . . < tN = T, the process (1) can be evaluated\nat each grid point, xti = \u03b1tix\u2217 +\u03c3ti\u03b5, and both the velocity\nand the score can be estimated on these points via the losses\nLN\ns (\u03b8) =\nN\nX\ni=0\nE[\u2225\u03c3tis\u03b8(xti, ti) + \u03b5\u22252],\n(13)\nLN\nv (\u03b8) =\nN\nX\ni=0\nE[\u2225v\u03b8(xti, ti) \u2212 \u02d9\u03b1tix\u2217 \u2212 \u02d9\u03c3ti\u03b5\u22252].\n(14)\nMoreover, only the learned s\u03b8(x, ti) or v\u03b8(x, ti) is needed\nto integrate the probability flow ODE (2) and the reverse-\ntime SDE (4) on the same grid. The resulting procedure, in\nwhich we define xt iteratively on the grid, is a generalization\nof DDPM. Starting from xt0 = x\u2217, we set for i \u2265 0,\nxti+1 =\np\n1 \u2212 h\u03b2tixti +\np\nh\u03b2ti\u03b5ti,\n(15)\nwhere h = ti+1 \u2212 ti and where we assume that the grid\nis uniform. Because\np\n1 \u2212 h\u03b2ti = 1 \u2212 1\n2h\u03b2ti + o(h), it is\neasy to see that (15) is a consistent time-discretization of the\nforward SDE (10). Our results show that it is not necessary\nto specify the time discretized process xti using (15), but\ninstead we can directly use (1) on the time grid.\n2.6. Interpolant Transformer Architecture\nThe backbone architecture and capacity of generative models\nare also crucial for producing high-quality samples. In order\nto eliminate any confounding factors and focus on our explo-\nration, we strictly follow the standard Diffusion Transformer\n(DiT) [48] and its configurations. This way, we can also test\nthe scalability of our model across various model sizes.\nHere we briefly introduce the model design. Generating\nhigh-resolution images with diffusion models can be compu-\ntationally expensive. Latent diffusion models (LDMs) [51]\naddress this by first downsampling images into a smaller la-\ntent embedding space using an encoder E, and then training\na diffusion model on z = E(x). New images are created by\nsampling z from the model and decoding it back to images\nusing a decoder x = D(z).\nSimilarly, SiT is also a latent generative model and we\nuse the same pre-trained VAE encoder and decoder models\noriginally used in Stable Diffusion [51]. SiT processes a\nspatial input z (shape 32\u00d732\u00d74 for 256\u00d7256\u00d73 images)\nby first \u2018patchifying\u2019 it into T linearly embedded tokens\nof dimension d. We always use a patch size of 2 in these\nmodels as they achieve the best sample quality. We then\n5\nModel\nLayers N\nHidden size d\nHeads\nSiT-S\n12\n384\n6\nSiT-B\n12\n768\n12\nSiT-L\n24\n1024\n16\nSiT-XL\n28\n1152\n16\nTable 3. Details of SiT models. We follow DiT [48] for the Small\n(S), Base (B), Large (L) and XLarge (XL) model configurations.\napply standard ViT [21] sinusoidal positional embeddings to\nthese tokens. We use a series of N SiT transformer blocks,\neach with hidden dimension d.\nOur model configurations\u2014SiT-{S,B,L,XL}\u2014vary in\nmodel size (parameters) and compute (flops), allowing for\na model scaling analysis. For class-conditional generation\non ImageNet, we use the AdaLN-Zero block [48] to process\nadditional conditional information (times and class labels).\nSiT architectural details are listed in Table 3.\n3. Experiments\nTo provide a more detailed answer to the question raised in\nTab. 1 and make a fair comparison between DiT and SiT, we\ngradually transition from a DiT model (discretized, score pre-\ndiction, VP interpolant) to a SiT model (continuous, velocity\nprediction, Linear interpolant) in the following four subsec-\ntions, and present the impacts on performance. Throughout\nour experiments in each subsection, we uses a DiT-B model\nat 400K training steps as our backbone. For solving the\nODE (2), we adopt a fixed Heun integrator; for solving the\nSDE (4), we used an Euler-Maruyama integrator. With both\nsolver choices we limit the number of function evaluations\n(NFE) to be 250 to match the number of sampling steps used\nin DiT. All numbers presented in the following sections are\nFID-50K scores evaluated on the ImageNet256 training set.\n3.1. Discrete to Continuous Time\nTo understand the role of continuous-time versus discrete-\ntime models, we study discrete-time DDPM against\ncontinuous-time SBDM-VP with estimation of the score.\nThe results are presented in Table 4, where we find a\nmarginal improvement in FID scores when going from a\ndiscrete-time denoiser to a continuous-time score field.\nModel\nObjective\nFID\nDDPM\nNoise\nLN\ns\n44.2\nSBDM-VP\nScore\nLs\n43.6\nTable 4. DDPM vs. SBDM.\n100K\n200K\n300K\n400K\n500K\n600K\n700K\nTraining Steps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nR\n|v\u03b8(x, t)|2pt(x)dxdt\nSBDM-VP\nLinear\nGVP\nFigure 4.\nPath length.\nThe path length\nC(v)\n=\nR 1\n0 E[|v(xt, t)|2]dt arising from the velocity field at different train-\ning steps for the various models considered: SBDM (VP), Linear,\nand GVP; each curve is approximated by 10000 datapoints.\n3.2. Model parameterizations\nTo clarify the role of the model parameterization in the con-\ntext of SBDM-VP, we now compare learning (i) a score\nmodel using (6), (ii) a weighted score model (see Ap-\npendix A.3), or (iii) a velocity model using (7). The results\nare shown in Table 5, where we find that we obtain a signifi-\ncant performance improvement by learning a weighted score\nmodel or a velocity model.\nInterpolant\nModel\nObjective\nODE\nSBDM-VP\nScore\nLs\n43.6\nSBDM-VP\nScore\nLs\u03bb\n39.1\nSBDM-VP\nVelocity\nLv\n39.8\nTable 5. Effect of model parameterization.\n3.3. Choice of interpolant\nSection 2 highlights that there are many possible ways to\nbuild a connection between the data distribution and a Gaus-\nsian by varying the choice of \u03b1t and \u03c3t in the definition of\nthe interpolant (1). To understand the role of this choice, we\nnow study the benefits of moving away from the commonly-\nused SBDM-VP setup. We consider learning a velocity\nmodel v(x, t) with the Linear and GVP interpolants pre-\nsented in (12), which make the interpolation between the\nGaussian and the data distribution exact on [0, 1]. We bench-\nmark these models against the SBDM-VP in Table 6, where\nwe find that both the GVP and Linear interpolants obtain\nsignificantly improved performance.\nOne possible explanation for this observation is given\nin Fig. 4, where we see that the path length (transport\ncost) is reduced when changing from SBDM-VP to GVP\nor Linear. Numerically, we also note that for SBDM-VP,\n\u02d9\u03c3t = \u03b2te\u2212\nR t\n0 \u03b2sds/(2\u03c3t) becomes singular at t = 0: this\n6\ncan pose numerical difficulties inside Lv, leading to diffi-\nculty in learning near the data distribution. This issue does\nnot appear with the GVP and Linear interpolants.\nInterpolant\nModel\nObjective\nFID\nSBDM-VP\nVelocity\nLv\n39.8\nLinear\nVelocity\nLv\n34.8\nGVP\nVelocity\nLv\n34.6\nTable 6. Effect of interpolant.\n3.4. Deterministic vs stochastic sampling\nUsing the SBDM diffusion coefficient.\nAs shown\nin Sec. 2, given a learned model, we can sample using either\nthe probability flow equation (2) or an SDE (4). In SBDM\nwe conventionally take as diffusion coefficient wt = \u03b2t.\nFor Linear and GVP interpolant, we follow the derivation\nin Appendix B to express \u03b2t in terms of \u03b1t and \u03c3t.\nOur results are shown in Tab. 7, where we find perfor-\nmance improvements by sampling with an SDE over the\nODE, which is in line with the bounds given in [3]: the\nSDE has better control over the KL divergence between the\ntime-dependent density at t = 0 and the ground truth data\ndistribution. We note that the performance of ODE and SDE\nintegrators may differ under different computation budgets.\nAs shown in Fig. 5, the ODE converges faster with fewer\nNFE, while the SDE is capable of reaching a much lower\nfinal FID score when given a larger computational budget.\nInterpolant\nModel\nObjective\nODE\nSDE\nSBDM-VP\nVelocity\nLv\n39.8\n37.8\nLinear\nVelocity\nLv\n34.8\n33.6\nGVP\nVelocity\nLv\n34.6\n32.9\nTable 7. ODE vs. SDE, SBDM diffusion.\nTunable diffusion coefficient.\nMotivated by the improved\nperformance of SDE sampling, we now consider the effect\nof tuning the diffusion coefficient in a manner that is distinct\nfrom the choices made in SBDM, as detailed in Tab. 2. As\nshown in Tab. 8, we find that the optimal choice for sam-\npling is both model prediction and interpolant dependent.\nWe picked the three functions with the best performance\nand sweep through all different combinations of our model\nprediction and interpolant, and present the result in Tab. 8.\nWe also note that the influences of different diffusion co-\nefficients can vary across different model sizes. Empirically,\nwe observe the best choice for our SiT-XL is a velocity model\nwith Linear interpolant and sampled with \u03c3t coefficient.\n3.5. Classifier-free guidance\nClassifier-free guidance (CFG) [24] often leads to improved\nperformance for score-based models. In this section, we\n8\n16\n32\n64\n128\n256\n512\n1024\nNFE\n34\n36\n38\n40\n42\n44\nFID-10K\nODE\nSDE: \u03c3t\nSDE: \u03b2t\nSDE: sin(\u03c0t)2\nFigure 5. Comparison of ODE and SDE w/ choices of diffusion\ncoefficients. We evaluate each sampler using a 400K steps trained\nSiT-B model with Linear interpolant and learning the v(x, t).\ngive a concise justification for adopting it on the velocity\nmodel, and then empirically show that the drastic gains in\nperformance for DiT case carry across to SiT.\nGuidance for a velocity field means that: (i) that the\nvelocity model v\u03b8(x, t; y) takes class labels y during train-\ning, where y is occasionally masked with a null token \u2205;\nand (ii) during sampling the velocity used is v\u03b6\n\u03b8(x, t; y) =\n\u03b6v\u03b8(x, t; y) + (1 \u2212 \u03b6)v\u03b8(x, t; \u2205) for a fixed \u03b6 > 0. In Ap-\npendix C, we show that this indeed corresponds to sampling\nthe tempered density p(xt)p(y|xt)\u03b6 as proposed in [46].\nGiven this observation, one can leverage the usual argument\nfor classifier-free guidance of score-based models.\nFor a CFG scale of \u03b6 = 1.5, DiT-XL sees an improve-\nment in FID from 9.6 (non-CFG) down to 2.27 (CFG). We\nobserved similar performance improvement with our largest\nSiT-XL model under identical computation budget and CFG\nscale. Sampled with an ODE, the FID-50K score improves\nfrom 9.4 to 2.15 (Tab. 9 and Appendix D); with an SDE, the\nFID improves from 8.6 to 2.06 (Tab. 1 and Tab. 9). This\nshows that SiT benefits from the same training and sam-\npling choices explored previously, and can surpass DiTs\nperformance in each training setting, not only with respect\nto model size, but also with respect to sampling choices.\n4. Related Work\nTransformers.\nThe transformer architecture [66] has\nemerged as a powerful tool for application domains as di-\nverse as vision [21, 47], language [68, 69], quantum chem-\nistry [67], active matter systems [9], and biology [11]. Sev-\neral works have built on DiT and have made improvements\nby modifying the architecture to internally include masked\nprediction layers [22, 70]; these choices are orthogonal to\nthe transition from DiT to SiT studied in this work; they may\nbe fruitfully combined in future work.\n7\nInterpolant\nModel\nObjective\nwt = \u03b2t\nwt = \u03c3t\nwt = sin2(\u03c0t)\nSBDM-VP\nvelocity\nLv\n37.8\n38.7\n39.2\nscore\nLs\u03bb\n35.7\n37.1\n37.7\nGVP\nvelocity\nLv\n32.9\n33.4\n33.6\nscore\nLs\n38.0\n33.5\n33.2\nLinear\nvelocity\nLv\n33.6\n33.5\n33.3\nscore\nLs\n41.0\n35.3\n34.4\nTable 8. Evaluation of our SDE samplers. All results in the table are FID-50K scores produced by an SiT-B model at 400K training steps.\nThe last three columns specify different diffusion coefficients wt detailed in Tab. 2. To make the SBDM-VP competitive when learning a\nscore, we use a weighted score given in Appendix A.4, as per the remarks below (7).\nClass-Conditional ImageNet 256x256\nModel\nFID\u2193 sFID\u2193\nIS\u2191\nPrecision\u2191 Recall\u2191\nBigGAN-deep[10]\n6.95\n7.36\n171.4\n0.87\n0.28\nStyleGAN-XL[55]\n2.30\n4.02 265.12\n0.78\n0.53\nMask-GIT[12]\n6.18\n-\n182.1\n-\n-\nADM[19]\n10.94 6.02 100.98\n0.69\n0.63\nADM-G, ADM-U\n3.94\n6.14 215.84\n0.83\n0.53\nCDM[26]\n4.88\n-\n158.71\n-\n-\nRIN[30]\n3.42\n-\n182.0\n-\n-\nSimple Diffusion(U-Net)[27] 3.76\n-\n171.6\n-\n-\nSimple Diffusion(U-ViT, L)\n2.77\n-\n211.8\n-\n-\nVDM++[35]\n2.12\n-\n267.7\n-\n-\nDiT-XL(cfg = 1.5)[48]\n2.27\n4.60 278.24\n0.83\n0.57\nSiT-XL(cfg = 1.5, ODE)\n2.15\n4.60 258.09\n0.81\n0.60\nSiT-XL(cfg = 1.5, SDE:\u03c3t)\n2.06\n4.50 270.27\n0.82\n0.59\nTable 9. Benchmarking class-conditional image generation on\nImageNet 256x256. SiT-XL surpasses DiT-XL in FID when either\nof the samplers, ODE or SDE-based.\nTraining and Sampling in Diffusions.\nDiffusion models\narose from [25, 59, 62] and have close historical relationship\nwith denoising methods [28, 29, 57]. Various efforts have\ngone into improving the sampling algorithms behind these\nmethods in the context of DDPM [60] and SBDM [32, 61];\nthese are also orthogonal to our studies and may be combined\nto push for better performance in future work. Improved Dif-\nfusion ODE [71] also studies several combinations of model\nparameterizations (velocity versus noise) and paths (VP ver-\nsus Linear) for sampling an ODE; they report best results for\nvelocity model with smoother probability flow; they focus\non lower dimensional experiments, benchmark with likeli-\nhoods, and do not consider SDE sampling. In our work,\nwe explore the effects of changing between VP, Linear, and\nGVP interpolants, as well as score and velocity parameter-\nizations in depth and show how these choices individually\nimprove performance on the larger scale ImageNet256. We\nalso document how FIDs change with respect to a family of\nsampling algorithms including black-box ODEs and SDEs\nindexed by a choice of diffusion coefficients, and show that\nthe best coefficient choice may depend on the model and\ninterpolant. This brings the observations about the flexibility\nand trade-offs of sampling from [3] into practice.\nInterpolants and flow matching.\nVelocity field parame-\nterizations using the Linear interpolant were also studied in\n[39, 42], and were generalized to the manifold setting in [6].\nA trade-off in bounds on the KL divergence between the tar-\nget distribution and the model arises when considering sam-\npling with SDEs versus ODE; [3] shows that minimizing the\nobjectives presented in this work controls KL for SDEs, but\nnot for ODEs. Error bounds for SDE-based sampling with\nscore-based diffusion models are studied in [13, 14, 37, 38].\nError bounds on the ODE are also explored in [7, 15], in\naddition to the Wasserstein bounds provided in [1].\nOther related works make improvements by changing\nhow noise and data are sampled during training. [50, 64]\ncompute mini-batch optimal couplings between the Gaus-\nsian and data distribution to reduce the transport cost and\ngradient variance; [4] instead build the coupling by flowing\ndirectly from the conditioning variable to the data for image-\nconditional tasks such as super-resolution and in-painting.\nFinally, various work considers learning a stochastic bridge\nconnecting two arbitrary distributions [18, 41, 49, 56]. These\ndirections are compatible with our investigations; they spec-\nify the learning problem for which one can then vary the\nchoices of model parameterizations, interpolant schedules,\nand sampling algorithms.\nDiffusion in Latent Space.\nGenerative modeling in latent\nspace [51, 65] is a tractable approach for modeling high-\ndimensional data. The approach has been applied beyond\nimages to video generation [8], which is a yet-to-be explored\nand promising application area for velocity trained models.\n[17] also train velocity models in the latent space of the pre-\ntrained Stable Diffusion VAE. They demonstrate promising\nresults for the DiT-B backbone with a final FID-50K of 4.46;\ntheir study was one motivation for the investigation in this\nwork regarding which aspects of these models contribute to\nthe gains in performance over DiT.\n8\n5. Conclusion\nIn this work, we have presented Scalable Interpolant Trans-\nformers, a simple and powerful framework for image genera-\ntion tasks. Within the framework, we explored the tradeoffs\nbetween a number of key design choices: the choice of a con-\ntinuous or discrete-time model, the choice of interpolant, the\nchoice of model prediction, and the choice of diffusion coef-\nficient. We highlighted the advantages and disadvantages of\neach choice and demonstrated how careful decisions can lead\nto significant performance improvements. Many concurrent\nworks [23, 31, 40, 45] explore similar approaches in a wide\nvariety of downstream tasks, and we leave the application of\nSiT to these tasks for future works.\nAcknowledgements.\nWe would like to thank Adithya\nIyer, Sai Charitha Akula, Fred Lu, Jitao Gu, and Edwin\nP. Gerber for helpful discussions and feedback.\nThe\nresearch is partly supported by the Google TRC program.\nReferences\n[1] Michael S Albergo and Eric Vanden-Eijnden. Building nor-\nmalizing flows with stochastic interpolants. In ICLR, 2023. 3,\n8\n[2] Michael S Albergo, Nicholas M Boffi, Michael Lindsey, and\nEric Vanden-Eijnden. Multimarginal generative modeling\nwith stochastic interpolants. arXiv preprint arXiv:2310.03695,\n2023. 4\n[3] Michael S. Albergo, Nicholas M. Boffi, and Eric Vanden-\nEijnden. Stochastic interpolants: A unifying framework for\nflows and diffusions. arXiv preprint arXiv:2303.08797, 2023.\n2, 3, 4, 7, 8, 1, 5, 6\n[4] Michael S Albergo, Mark Goldstein, Nicholas M Boffi, Ra-\njesh Ranganath, and Eric Vanden-Eijnden. Stochastic in-\nterpolants with data-dependent couplings. arXiv preprint\narXiv:2310.03725, 2023. 8\n[5] Brian D.O. Anderson. Reverse-time diffusion equation mod-\nels. Stochastic Processes and their Applications, 1982. 3\n[6] Heli Ben-Hamu, Samuel Cohen, Joey Bose, Brandon Amos,\nAditya Grover, Maximilian Nickel, Ricky TQ Chen, and\nYaron Lipman. Matching normalizing flows and probabil-\nity paths on manifolds. In ICML, 2022. 8\n[7] Joe Benton, George Deligiannidis, and Arnaud Doucet. Er-\nror bounds for flow matching methods.\narXiv preprint\narXiv:2305.16860, 2023. 8\n[8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 8\n[9] Nicholas M Boffi and Eric Vanden-Eijnden. Deep learning\nprobability flows and entropy production rates in active matter.\narXiv preprint arXiv:2309.12991, 2023. 7\n[10] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis. In\nICLR, 2019. 8\n[11] Abel Chandra, Laura T\u00a8unnermann, Tommy L\u00a8ofstedt, and\nRegina Gratz. Transformer-based deep learning for predicting\nprotein properties in the life sciences. Elife, 12:e82819, 2023.\n7\n[12] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T.\nFreeman. Maskgit: Masked generative image transformer. In\nCVPR, 2022. 8\n[13] Hongrui Chen, Holden Lee, and Jianfeng Lu.\nImproved\nanalysis of score-based generative modeling: User-friendly\nbounds under minimal smoothness assumptions. In ICML,\n2023. 8\n[14] Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim,\nand Anru Zhang. Sampling is as easy as learning the score:\ntheory for diffusion models with minimal data assumptions.\nIn ICLR, 2023. 8\n[15] Sitan Chen, Giannis Daras, and Alex Dimakis. Restoration-\ndegradation beyond linear diffusions: A non-asymptotic anal-\nysis for DDIM-type samplers. In ICML, 2023. 8\n[16] Ting Chen. On the importance of noise scheduling for diffu-\nsion models. arXiv preprint arXiv:2301.10972, 2023. 4\n[17] Quan Dao, Hao Phung, Binh Nguyen, and Anh Tran. Flow\nmatching in latent space. arXiv preprint arXiv:2307.08698,\n2023. 8\n[18] Valentin De Bortoli, James Thornton, Jeremy Heng, and Ar-\nnaud Doucet. Diffusion schr\u00a8odinger bridge with applications\nto score-based generative modeling. In NeurIPS, 2021. 8\n[19] Prafulla Dhariwal and Alex Nichol. Diffusion models beat\ngans on image synthesis. In NeurIPS, 2021. 8, 4, 7\n[20] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-\nbased generative modeling with critically-damped langevin\ndiffusion. In ICLR, 2022. 6\n[21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In ICLR, 2021. 1, 6, 7\n[22] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng\nYan. Masked diffusion transformer is a strong image synthe-\nsizer. arXiv preprint arXiv:2303.14389, 2023. 7\n[23] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera\nHahn, Li Fei-Fei, Irfan Essa, Lu Jiang, and Jos\u00b4e Lezama.\nPhotorealistic video generation with diffusion models. arXiv\npreprint arXiv:2312.06662, 2023. 9\n[24] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 7, 4\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. In NeurIPS, 2020. 1, 4, 8\n[26] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. arXiv preprint\narXiv:2106.15282, 2021. 8\n[27] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple\ndiffusion: End-to-end diffusion for high resolution images. In\nICML, 2023. 1, 8\n[28] Aapo Hyv\u00a8arinen. Estimation of non-normalized statistical\nmodels by score matching. JMLR, 2005. 8\n9\n[29] Aapo Hyv\u00a8arinen. Sparse code shrinkage: Denoising of non-\ngaussian data by maximum likelihood estimation. Neural\nComputation, 1999. 8\n[30] Allan Jabri, David Fleet, and Ting Chen. Scalable adaptive\ncomputation for iterative generation. In ICML, 2023. 8\n[31] Tomas Jakab, Ruining Li, Shangzhe Wu, Christian Rupprecht,\nand Andrea Vedaldi. Farm3D: Learning articulated 3d ani-\nmals by distilling 2d diffusion. In 3DV, 2024. 9\n[32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. In NeurIPS, 2022. 1, 3, 4, 8, 6\n[33] Patrick Kidger. On Neural Differential Equations. PhD thesis,\nUniversity of Oxford, 2021. 6\n[34] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization. In ICLR, 2015. 7\n[35] Diederik P Kingma and Ruiqi Gao. Understanding the diffu-\nsion objective as a weighted integral of elbos. arXiv preprint\narXiv:2303.00848, 2023. 1, 4, 8, 3\n[36] Diederik P. Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. In NeurIPS, 2021. 1, 3, 6\n[37] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence for\nscore-based generative modeling with polynomial complexity.\nIn NeurIPS, 2022. 8\n[38] Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of\nscore-based generative modeling for general data distributions.\nIn ALT, 2023. 8\n[39] Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maxi-\nmilian Nickel, and Matt Le. Flow matching for generative\nmodeling. In ICLR, 2023. 3, 8\n[40] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,\nSergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot\none image to 3d object. In ICCV, 2023. 9\n[41] Xingchao Liu, Lemeng Wu, Mao Ye, and Qiang Liu. Let us\nbuild bridges: Understanding and extending diffusion genera-\ntive models. arXiv preprint arXiv:2208.14699, 2022. 8\n[42] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight\nand fast: Learning to generate and transfer data with rectified\nflow. In ICLR, 2023. 3, 8\n[43] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. In ICLR, 2019. 7\n[44] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Dpm-solver: A fast ode solver for diffusion\nprobabilistic model sampling in around 10 steps. In NeurIPS,\n2022. 6\n[45] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun\nWu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image\nsynthesis and editing with stochastic differential equations.\nIn ICLR, 2022. 9\n[46] Alex Nichol and Prafulla Dhariwal. Improved denoising\ndiffusion probabilistic models. In ICML, 2021. 7\n[47] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, \u0141ukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage Transformer. In ICML, 2018. 7\n[48] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In ICCV, 2023. 1, 5, 6, 8, 7\n[49] Stefano Peluchetti. Non-denoising forward-time diffusions.\nIn ICLR, 2022. 8\n[50] Aram-Alexandre\nPooladian,\nHeli\nBen-Hamu,\nCarles\nDomingo-Enrich, Brandon Amos, Yaron Lipman, and Ricky\nT. Q. Chen. Multisample flow matching: Straightening flows\nwith minibatch couplings. In ICML, 2023. 8\n[51] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, 2022. 1, 5,\n8\n[52] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net:\nConvolutional networks for biomedical image segmentation.\nIn MICCAI, 2015. 1\n[53] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al. Imagenet large scale\nvisual recognition challenge. IJCV, 2015. 2\n[54] Tim Salimans and Jonathan Ho. Progressive distillation for\nfast sampling of diffusion models. In ICLR, 2022. 1, 6\n[55] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl:\nScaling stylegan to large diverse datasets. In SIGGRAPH,\n2022. 8\n[56] Yuyang Shi, Valentin De Bortoli, Andrew Campbell, and\nArnaud Doucet. Diffusion schr\u00a8odinger bridge matching. In\nNeurIPS, 2023. 8\n[57] Eero P. Simoncelli and Edward H. Adelson. Noise removal\nvia bayesian wavelet coring. In ICIP, 1996. 8\n[58] Raghav Singhal, Mark Goldstein, and Rajesh Ranganath.\nWhere to diffuse, how to diffuse, and how to get back: Auto-\nmated learning for multivariate diffusions. In ICLR, 2023. 1,\n4\n[59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In ICML, 2015. 8\n[60] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. In ICLR, 2021. 8\n[61] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.\nMaximum likelihood training of score-based diffusion models.\nIn NeurIPS, 2021. 2, 8, 3\n[62] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\nIn ICLR, 2021. 1, 3, 4, 8, 6\n[63] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.\nConsistency models. In ICML, 2023. 6\n[64] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei\nZhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and\nYoshua Bengio. Improving and generalizing flow-based gen-\nerative models with minibatch optimal transport. In ICML\nWorkshop on New Frontiers in Learning, Control, and Dy-\nnamical Systems, 2023. 8\n[65] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based\ngenerative modeling in latent space. In NeurIPS, 2021. 8, 3\n[66] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, 2017. 7\n[67] Ingrid von Glehn, James S. Spencer, and David Pfau. A Self-\nAttention Ansatz for Ab-initio Quantum Chemistry. In ICLR,\n2023. 7\n10\n[68] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li,\nDerek F Wong, and Lidia S Chao. Learning deep transformer\nmodels for machine translation. In ACL, 2019. 7\n[69] Manzil Zaheer, Guru Guruganesh, Avinava Dubey, Joshua\nAinslie, Chris Alberti, Santiago Ontanon, Philip Pham,\nAnirudh Ravula, Qifan Wang, Li Yang, and Amr Ahmed.\nBig Bird: Transformers for Longer Sequences. In NeurIPS,\n2020. 7\n[70] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anand-\nkumar. Fast training of diffusion models with masked trans-\nformers. arXiv preprint arXiv:2306.09305, 2023. 7\n[71] Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Im-\nproved techniques for maximum likelihood estimation for\ndiffusion odes. In ICML, 2023. 8\n11\nAppendix\nA. Proofs\nIn all proofs below, we use \u00b7 for dot product and assume all bold notations (x, \u03b5, etc.) are real-valued vectors in Rd. Most\nproofs are derived from Albergo et al. [3].\nA.1. Proof of the probability flow ODE (2) with the velocity in Eq. (3).\nConsider the time-dependent probability density function (PDF) pt(x) of xt = \u03b1tx\u2217 + \u03c3t\u03b5 defined in Eq. (1). By definition,\nits characteristic function \u02c6pt(k) =\nR\nRd eik\u00b7xpt(x)dx is given by\n\u02c6pt(k) = E[eik\u00b7xt]\n(16)\nwhere E denotes expectation over x\u2217 and \u03b5. Taking time derivative on both sides, and using the tower property of conditional\nexpectation, we have\n\u2202t\u02c6pt(k) = ik \u00b7 E[ \u02d9xteik\u00b7xt]\n(17)\n= ik \u00b7 Ex\u223cpt[E[ \u02d9xteik\u00b7xt|xt = x]]\n(18)\n= ik \u00b7 Ex\u223cpt[E[( \u02d9\u03b1tx\u2217 + \u02d9\u03c3t\u03b5)eik\u00b7xt|xt = x]]\n(19)\n= ik \u00b7 Ex\u223cpt[E[( \u02d9\u03b1tx\u2217 + \u02d9\u03c3t\u03b5)|xt = x]eik\u00b7x]\n(20)\n= ik \u00b7 Ex\u223cpt[v(x, t)eik\u00b7x]\n(21)\nwhere v(x, t) = E[( \u02d9\u03b1tx\u2217 + \u02d9\u03c3t\u03b5)|xt = x] = \u02d9\u03b1tE[x\u2217|xt = x] + \u02d9\u03c3tE[\u03b5|xt = x] is the velocity defined in Eq. (3). Explicitly,\nEq. (21) reads\n\u2202t\nZ\nRd eik\u00b7xpt(x)dx = ik \u00b7\nZ\nRd v(x, t)eik\u00b7xpt(x)dx\n(22)\nfrom which we deduce\nZ\nRd eik\u00b7x\u2202tpt(x)dx =\nZ\nRd v(x, t) \u00b7 \u2207x[eik\u00b7x]pt(x)dx = \u2212\nZ\nRd \u2207x \u00b7 [v(x, t)pt(x)]eik\u00b7xdx\n(23)\nwhere \u2207x \u00b7 [vpt] = Pd\ni=1\n\u2202\n\u2202xi [vipt] is the divergence operator and we take advantage of the divergence theorem and used\nintegration by parts to get the second equality. By the properties of Fourier transform, Eq. (23) implies that pt(x) satisfies the\ntransport equation\n\u2202tpt(x) + \u2207x \u00b7 (v(x, t)pt(x)) = 0.\n(24)\nSolving this equation by the method of characteristic leads to probability flow ODE (2).\nA.2. Proof of the SDE (4)\nWe show that the SDE (4) has marginal density pt(x) with any choice of wt \u2265 0. To this end, recall that solution to the SDE\ndXt = [v(Xt, t) + 1\n2wts(Xt, t)]dt + \u221awtd \u00af\nWt\nhas a PDF that satisfies the Fokker-Planck equation\n\u2202tpt(x) = \u2212\u2207x \u00b7\n\u0000[v(x, t) + 1\n2wts(x, t)]pt(x)\n\u0001\n+ 1\n2wt\u2206xpt(x)\n(25)\nwhere \u2206x is the Laplace operator defined as \u2206x = \u2207x \u00b7 \u2207x = Pd\ni=0\n\u22022\n\u2202x2\ni . Reorganizing the equation and usng the definition\nof the score s(x, t) = \u2207x log pt(x) = p\u22121\nt (x)\u2207xpt(x), we have\n\u2202tpt(x) = \u2212\u2207x \u00b7 [v(x, t)pt(x)]\n|\n{z\n}\n= \u2202tpt(x) by Eq. (24)\n\u22121\n2wt\u2207x \u00b7 [\u2207x log pt(x)pt(x)\n|\n{z\n}\n= \u2207xpt(x)\n] + 1\n2wt\u2206xpt(x)\n(26)\n=\u21d2\n0 = \u22121\n2wt\u2207x \u00b7 \u2207xpt(x) + 1\n2wt\u2206xpt(x)\n(27)\n1\nBy definition of Laplace operator, the last equation holds for any wt \u2265 0. When wt = 0, the Fokker-Planck equation reduces\nto a continuity equation, and the SDE reduces to an ODE, so the connection trivially holds.\nA.3. Proof of the expression for the score in Eq. (5)\nWe show that s(x, t) = \u2212\u03c3\u22121\nt\nE[\u03b5|xt = x]. Letting \u02c6f(k, t) = E[\u03b5ei\u03c3tk\u00b7\u03b5], we have\n\u02c6f(k, t) = \u2212 i\n\u03c3t\n\u2207kE[ei\u03c3tk\u00b7\u03b5]\n(28)\nSince \u03b5 \u223c N(0, I), we can compute the expectation explicitly to obtain\n\u02c6f(k, t) = \u2212 i\n\u03c3t\n(\u2207ke\u2212 1\n2 \u03c32\nt |k|2)\n(29)\n= i\u03c3tke\u2212 1\n2 \u03c32\nt |k|2\n(30)\nSince x\u2217 and \u03b5 are independent random variable, we have\nE[\u03b5eik\u00b7xt] = \u02c6f(k, t)E[ei\u03b1tk\u00b7x\u2217] = i\u03c3tk e\u2212 1\n2 \u03c32\nt |k|2E[ei\u03b1tk\u00b7x\u2217]\n|\n{z\n}\ncombine this\n= i\u03c3tk\u02c6pt(k)\n(31)\nwhere \u02c6pt(k) is the characteristic function of xt = \u03b1tx\u2217 + \u03c3t\u03b5 defined in Eq. (16). The left hand-side of this equation can also\nbe written as:\nE[\u03b5eik\u00b7xt] =\nZ\nRd E[\u03b5eik\u00b7xt|xt = x]pt(x)dx\n(32)\n=\nZ\nRd E[\u03b5|xt = x]eik\u00b7xpt(x)dx,\n(33)\nwhereas the right hand-side is\ni\u03c3tk\u02c6pt(k) = i\u03c3tk\nZ\nRd eik\u00b7xpt(x)dx\n(34)\n= \u03c3t\nZ\nRd \u2207x[eik\u00b7x]pt(x)dx\n(35)\n= \u2212\u03c3t\nZ\nRd eik\u00b7x\u2207xpt(x)dx\n(36)\n= \u2212\u03c3t\nZ\nRd eik\u00b7xs(x, t)pt(x)dx\n(37)\nwhere we again used divergence theorem and integration by parts to get the third equality, and again the definition of the score\nto get the last. Comparing Eq. (33) and Eq. (37) we deduce that, when \u03c3t \u0338= 0,\ns(x, t) = \u2207x log pt(x) = \u2212\u03c3\u22121\nt\nE[\u03b5|xt = x]\n(38)\nFurther, setting wt to \u03c3t in Eq. (4) gives\n1\n2wts(xt, t) = \u22121\n2E[\u03b5|xt = x]\n(39)\nfor all t \u2208 [0, 1]. This bypass the constraint of \u03c3t \u0338= 0 and effectively eliminate the singularity at t = 0.\n2\nA.4. Proof of Eq. (9)\nWe note that there exists a straightforward connection between v(x, t) and s(x, t). From Eq. (1), we have\nv(x, t) = \u02d9\u03b1tE[x\u2217|xt = x] + \u02d9\u03c3tE[\u03b5|xt = x]\n(40)\n= \u02d9\u03b1tE[xt \u2212 \u03c3t\u03b5\n\u03b1t\n|xt = x] + \u02d9\u03c3tE[\u03b5|xt = x]\n(41)\n= \u02d9\u03b1t\n\u03b1t\nx + ( \u02d9\u03c3t \u2212 \u02d9\u03b1t\u03c3t\n\u03b1t\n)E[\u03b5|xt = x]\n(42)\n= \u02d9\u03b1t\n\u03b1t\nx + ( \u02d9\u03c3t \u2212 \u02d9\u03b1t\u03c3t\n\u03b1t\n)(\u2212\u03c3ts(x, t))\n(43)\n= \u02d9\u03b1t\n\u03b1t\nx \u2212 \u03bbt\u03c3ts(x, t)\n(44)\nwhere we defined\n\u03bbt = \u02d9\u03c3t \u2212 \u02d9\u03b1t\u03c3t\n\u03b1t\n(45)\nGiven Eq. (44) is linear in terms of s, reverting it will lead to Eq. (9).\nNote that we can also plug Eq. (44) into the loss Lv in Eq. (7) to deduce that\nLv(\u03b8) =\nZ T\n0\nE[\u2225 \u02d9\u03b1t\n\u03b1t\nx\n|{z}\nExpand to xt = \u03b1tx\u2217 + \u03c3t\u03b5\n+\u03bbt(\u2212\u03c3ts\u03b8(xt, t)) \u2212 \u02d9\u03b1tx\u2217 \u2212 \u02d9\u03c3t\u03b5\u22252]dt\n(46)\n=\nZ T\n0\nE[\u2225 \u02d9\u03b1tx\u2217 + \u02d9\u03b1t\u03c3t\n\u03b1t\n\u03b5 + \u03bbt(\u2212\u03c3ts\u03b8(xt, t)) \u2212 \u02d9\u03b1tx\u2217 \u2212 \u02d9\u03c3t\u03b5\u22252]dt\n(47)\n=\nZ T\n0\nE[\u2225\u03bbt(\u2212\u03c3ts\u03b8(xt, t)) \u2212 \u03bbt\u03b5\u22252]dt\n(48)\n=\nZ T\n0\n\u03bb2\ntE[\u2225\u03c3ts\u03b8(xt, t) + \u03b5\u22252]dt\n(49)\n\u2261 Ls\u03bb(\u03b8)\n(50)\nwhich defines the weighted score objective Ls\u03bb(\u03b8). This observation is consistent with the claim made in Kingma and\nGao [35] that the score objective with different monotonic weighting functions coincides with losses for different model\nparameterizations. In Appendix B we show that \u03bbt corresponds to the square of the maximum likelihood weighting proposed\nin Song et al. [61] and Vahdat et al. [65].\nB. Connection with Score-based Diffusion\nAs shown in Song et al. [62], the reverse-time SDE from Eq. (10) is\ndXt = [\u22121\n2\u03b2tXt \u2212 \u03b2ts(Xt, t)]dt +\np\n\u03b2td \u00af\nWt\n(51)\nLet us show this SDE is Eq. (4) for the specific choice wt = \u03b2t. To this end, notice that the solution Xt to Eq. (51) for the\ninitial condition Xt=0 = x\u2217 with x\u2217 fixed is Gaussian distributed with mean and variance given respectively by\nE[Xt] = e\u2212 1\n2\nR t\n0 \u03b2sdsx\u2217 \u2261 \u03b1tx\u2217\n(52)\nvar[Xt] = 1 \u2212 e\u2212\nR t\n0 \u03b2sds \u2261 \u03c32\nt\n(53)\nUsing Eq. (44), the velocity of the score-based diffusion model can therefore be expressed as\nv(x, t) = \u22121\n2\u03b2tx + (\u22121\n2\u03b2t(1 \u2212 e\u2212\nR t\n0 \u03b2sds) \u2212 1\n2\u03b2te\u2212\nR t\n0 \u03b2sds)s(x, t)\n(54)\n= \u22121\n2\u03b2tx \u2212 1\n2\u03b2ts(x, t)\n(55)\nwe see that 2\u03bbt\u03c3t is precisely \u03b2t, making \u03bbt correspond to the square of maximum likelihood weighting proposed in Song\net al. [62]. Further, if we plug Eq. (55) into Eq. (4) with wt = \u03b2t, we arrive at Eq. (51).\n3\nA useful observation for choosing velocity versus noise model.\nWe see that in the velocity model, all of the path-dependent\nterms (\u03b1t, \u03c3t) are inside the squared loss, and in the score model, the terms are pulled out (apart from the necessary \u03c3t in score\nmatching loss) and get squared due to coming out of the norm. So which is more stable depends on the interpolant. In the\npaper we see that for SBDM-VP, due to the blowing up behavior of \u02d9\u03c3t near t = 0, both Lv and Ls\u03bb are unstable.\nYet, shown in Tab. 5, we observed better performance with Ls\u03bb for SBDM-VP, as the blowing up \u03bbt near t = 0 will\ncompensate for the diminishing gradient inside the squared norm, where Lv would simply experience gradient explosion\nresulted from \u02d9\u03c3t. The behavior is different for the Linear and GVP interpolant, where the source of instability is \u03b1\u22121\nt\nnear\nt = 1. We note Lv is stable since \u03b1\u22121\nt\ngets cancelled out inside the squared norm, while in Ls\u03bb it remains in \u03bbt outside the\nnorm.\nC. Sampling with Guidance\nLet pt(x|y) be the density of xt = \u03b1tx\u2217 + \u03c3t\u03b5 conditioned on some extra variable y. By argument similar to the one given in\nAppendix A.1, it is easy to see that pt(x|y) satisfies the transport equation (compare Eq. (24))\n\u2202tpt(x|y) + \u2207x \u00b7 (v(x, t|y)pt(x, |y)) = 0,\n(56)\nwhere (compare Eq. (3))\nv(x, t|y) = E[ \u02d9xt|xt = x, y] = \u02d9\u03b1tE[x\u2217|xt = x, y] + \u02d9\u03c3tE[\u03b5|xt = x, y]\n(57)\nProceeding as in Appendix A.3 and Appendix A.4, it is also easy to see that the score s(x, t|y) = \u2207x log pt(x|y) is given by\n(compare Eq. (5))\ns(x, t|y) = \u2212\u03c3\u22121\nt\nE[\u03b5|xt = x, y]\n(58)\nand that v(x, t|y) and s(x, t|y) are related via (compare Eq. (44))\nv(x, t|y) = \u02d9\u03b1t\n\u03b1t\nx \u2212 \u03bbt\u03c3ts(x, t|y)\n(59)\nConsider now\ns\u03b6(x, t|y) \u2261 (1 \u2212 \u03b6)s(x, t) + \u03b6s(x, t|y)\n(60)\n= \u2207 log pt(x) \u2212 \u03b6\u2207 log pt(x) + \u03b6\u2207 log pt(x|y)\n(61)\n= \u2207 log pt(x) \u2212 \u03b6\u2207 log pt(x) +\n\u0010\n\u03b6\u2207 log pt(y|x) + \u03b6\u2207 log pt(x)\n\u0011\n(62)\n= \u2207 log pt(x) + \u03b6\u2207 log pt(y|x)\n(63)\n= \u2207 log[pt(x)p\u03b6\nt (y|x)]\n(64)\nwhere we have used the fact \u2207x log pt(x|y) = \u2207x log pt(y|x)+\u2207x log pt(x) that follows from pt(x|y)p(y) = pt(y|x)pt(x),\nand \u03b6 to be some constant greater than 1. Eq. (64) shows that using the score mixture s\u03b6(x, t|y) = (1 \u2212 \u03b6)s(x, t) + \u03b6s(x, t|y),\nand the velocity mixture associated with it, namely,\nv\u03b6(x, t|y) = (1 \u2212 \u03b6)v(x, t) + \u03b6v(x, t|y)\n(65)\n= \u02d9\u03b1t\n\u03b1t\nx \u2212 \u03bbt\u03c3t[(1 \u2212 \u03b6)s(x, t) + \u03b6s(x, t|y)]\n(66)\n= \u02d9\u03b1t\n\u03b1t\nx \u2212 \u03bbt\u03c3ts\u03b6(x, t|y),\n(67)\nallows one to to construct generative models that sample the tempered distribution pt(xt)p\u03b6\nt (y|xt) following classifier guidance\n[19]. Note that pt(x)p\u03b6\nt (y|x) \u221d p\u03b6\nt (x|y)p1\u2212\u03b6\nt\n(x), so we can also perform classifier free guidance sampling [24]. Empirically,\nwe observe significant performance boost by applying classifier free guidance, as showed in Tab. 1 and Tab. 9.\n4\nModel\nTraining Steps(K)\nFID\u2193\nsFID\u2193\nIS\u2191\nPrecision\u2191\nRecall\u2191\nSiT-S\n400\n58.97 / 57.64\n8.95 / 9.05\n23.34 / 24.78\n0.40 / 0.41\n0.59 / 0.60\nSiT-B\n400\n34.84 / 33.45\n6.59 / 6.46\n41.53 / 43.71\n0.52 / 0.53\n0.64 / 0.63\nSiT-L\n400\n20.01 / 18.79\n5.31 / 5.29\n67.76 / 72.02\n0.62 / 0.64\n0.64 / 0.64\nSiT-XL\n400\n18.04 / 17.19\n5.17 / 5.07\n73.90 / 76.52\n0.63 / 0.65\n0.64 / 0.63\nSiT-XL\n7000\n9.35 / 8.61\n6.38 / 6.32\n126.06 / 131.65\n0.67 / 0.68\n0.68 / 0.67\nTable 1. FID-50K scores produced by ODE and SDE. We demonstrate the comparison between ODE and SDE across all of our model\nsizes. All statistics are produced without classifier free guidance. Each cell in the table is showing [ODE results] / [SDE results]. We note\nthe better performances of SDE observed in all model sizes are in line with the bounds given in [3], and that ODE has its advantage in lower\nNFE region, as shown in Fig. 5\nD. Sampling with ODE and SDE\nIn the main body of the paper, we used a Heun integrator for solving the ODE in Eq. (2) and an Euler-Maruyama integrator for\nsolving the SDE in Eq. (4). We summarize all results in Tab. 1, and present the implementations below.\nAlgorithm 1 Deterministic Heun Sampler\nprocedure HEUNSAMPLER(v\u03b8(x, t, y), ti\u2208{0,\u00b7\u00b7\u00b7 ,N}, \u03b1t, \u03c3t)\nsample x0 \u223c N(0, I)\n\u25b7 Generate initial sample\n\u2206t \u2190 t1 \u2212 t0\n\u25b7 Determine fixed step size\nfor i \u2208 {0, \u00b7 \u00b7 \u00b7 , N \u2212 1} do\ndi \u2190 v\u03b8(xi, ti, y)\n\u02dcxi+1 \u2190 xi + \u2206tdi\n\u25b7 Euler Step at ti\ndi+1 \u2190 v\u03b8(\u02dcxi+1, ti+1, y)\nxi+1 \u2190 xi + \u2206t\n2 [di + di+1]\n\u25b7 Explicit trapezoidal rule at ti+1\nend for\nreturn xN\nend procedure\nAlgorithm 2 Stochastic Euler-Maruyama Sampler\nprocedure EULERSAMPLER(v\u03b8(x, t, y), wt, ti\u2208{0,\u00b7\u00b7\u00b7 ,N}, T, \u03b1t, \u03c3t)\nsample x0 \u223c N(0, I)\n\u25b7 Generate initial sample\ns\u03b8 \u2190 convert from v\u03b8 following Appendix A.4 \u25b7 Obtain \u2207x log pt(x) in Eq. (4)\n\u2206t \u2190 t1 \u2212 t0\n\u25b7 Determine fixed step size\nfor i \u2208 {0, \u00b7 \u00b7 \u00b7 , N \u2212 1} do\nsample \u03b5i \u223c N(0, I)\nd\u03b5i \u2190 \u03b5i \u2217\n\u221a\n\u2206t\ndi \u2190 v\u03b8(xi, ti, y) + 1\n2wtis\u03b8(xi, ti, y)\n\u25b7 Evaluate drift term at ti\n\u00afxi+1 \u2190 xi + \u2206tdi\nxi+1 \u2190 \u00afxi+1 + \u221awtid\u03b5i\n\u25b7 Evaluate diffusion term at ti\nend for\nh \u2190 T \u2212 tN\n\u25b7 Last step size; T denotes the time where xT = x\u2217\nd \u2190 v\u03b8(xN, tN, y) + 1\n2wtN s\u03b8(xN, tN, y)\nx \u2190 xN + h \u2217 d\n\u25b7 Last step; output noiseless sample without diffusion\nreturn x\nend procedure\nIt is feasible to use either a velocity model v\u03b8 or a score model s\u03b8 in applying the above two samplers. If learning the score\n5\n200K\n300K\n400K\n500K\n600K\n700K\nTraining Steps\n45\n50\n55\n60\n65\n70\n75\n80\nFID-50K\nSiT-S (ODE)\nDiT-S (DDIM)\n200K\n300K\n400K\n500K\n600K\n700K\nTraining Steps\n25\n30\n35\n40\n45\n50\n55\nFID-50K\nSiT-B (ODE)\nDiT-B (DDIM)\n200K\n300K\n400K\n500K\n600K\n700K\nTraining Steps\n10\n15\n20\n25\n30\n35\nFID-50K\nSiT-L (ODE)\nDiT-L (DDIM)\n200K\n300K\n400K\n500K\n600K\n700K\nTraining Steps\n15\n20\n25\n30\nFID-50K\nSiT-XL (ODE)\nDiT-XL (DDIM)\nFigure 1. SiT observes improvement in FID across all model sizes. We show FID-50K over training iterations for both DiT and SiT\nmodels. Across all model sizes, SiT converges faster. We acknowledge this is not directly an apples-to-apples comparison. This is because\nDDIM is essentially a discrete form of the first-order Euler\u2019s method, whereas in sampling SiT, we employ the second-order Heun\u2019s method.\nNevertheless, both the SiT and DiT results are produced by a deterministic sampler with a 250 NFE.\nfor the deterministic Heun sampler, we could always convert the learned s\u03b8 to v\u03b8 following Appendix A.4. However, as there\nexists potential numerical instability (depending on interpolants) in \u02d9\u03c3t, \u03b1\u22121\nt\nand \u03bbt, it\u2019s recommended to learn v\u03b8 in sampling\nwith deterministic sampler instead of s\u03b8. For the stochastic sampler, it\u2019s required to have both v\u03b8 and s\u03b8 in integration, so we\nalways need to convert from one (either learning velocity or score) to obtain the other. Under this scenario, the numerical issue\nfrom Appendix A.4 can only be avoided by clipping the time interval near t = 0. Empirically we found clipping the interval\nby h = 0.04 and doing a long last step from t = 0.04 to 0 can greatly benefit the performance. A detailed summary of sampler\nconfiguration is provided in Appendix E.\nAdditionally, we could replace v\u03b8 and s\u03b8 by v\u03b6\n\u03b8 and s\u03b6\n\u03b8 presented in Appendix C as inputs of the two samplers and enjoy the\nperformance improvements coming along with guidance. As guidance requires evaluating both conditional and unconditional\nmodel output in a single step, it will impose twice the computational cost when sampling.\nComparison between DDPM and Euler-Maruyama\nWe primarily investigate and report the performance comparison\nbetween DDPM and Euler-Maruyama samplers. We set our Euler sampler\u2019s number of steps to be 250 to match that of DDPM\nduring evaluation. This comparison is made direct and fair, as the DDPM method is equivalent to a discretized Euler\u2019s method.\nComparison between DDIM and Heun\nWe also investigate the performance difference produced by deterministic samplers\nbetween DiT and our models. In Fig. 1, we show the FID-50K results for both DiT models sampled with DDIM and SiT\nmodels sampled with Heun. We note that this is not directly an apples-to-apples comparison, as DDIM can be viewed as a\ndiscretized version of the first order Euler\u2019s method, while we use the second order Heun\u2019s method in sampling SiT models,\ndue to the large discretization error with Euler\u2019s method in continuous time. Nevertheless, we control the NFEs for both DDIM\n(250 sampling steps) and Heun (250 NFE).\nHigher order solvers\nThe performances of an adaptive deterministics dopri5 solver and a second order stochastic Heun\nSampler [32] are also tested. For dopri5, we set atol and rtol to 1e-6 and 1e-3, respectively; for Heun, we again\nmaintain the NFE to be 250 to match that of DDPM. In both solvers we do not observe performance increment; under the CFG\nscale of \u03b6 = 1.5, dopri5 and stochastic Heun gives FID-50K of 2.15 and 2.07, respectively.\nWe also note that our models are compatible with other samplers [36, 44] specifically tuned to diffusion models as well as\nsampling distillation [54, 63]. We do not include the evaluations of those methods in our work for the sake of apples-to-apples\ncomparison with the DDPM model, and we leave the investigation of potential performance improvements to future work.\nE. Additional Implementation Details\nWe implemented our models in JAX following the DiT PyTorch codebase by Peebles and Xie [48]4, and referred to Albergo\net al. [3]5, Song et al. [62]6, and Dockhorn et al. [20]7 for our implementation of the Euler-Maruyama sampler. For the Heun\nsampler, we directly used the one from diffrax [33]8, a JAX-based numerical differential equation solver library.\n4https://github.com/facebookresearch/DiT\n5https://github.com/malbergo/stochastic-interpolants\n6https://github.com/yang-song/score sde\n7https://github.com/nv-tlabs/CLD-SGM\n8https://github.com/patrick-kidger/diffrax\n6\nTraining configurations\nWe trained all of our models following identical structure and hyperparameters retained from DiT\n[48]. We used AdamW [34, 43] as optimizer for all models. We use a constant learning rate of 1 \u00d7 10\u22124 and a batch size\nof 256. We used random horizontal flip with probability of 0.5 in data augmentation. We did not tune the learning rates,\ndecay/warm up schedules, AdamW parameters, nor use any extra data augmentation or gradient clipping during training. Our\nlargest model, SiT-XL, trains at approximately 6.8 iters/sec on a TPU v4-64 pod following the above configurations. This\nspeed is slightly faster compared to DiT-XL, which trains at 6.4 iters/sec under identical settings. We also gather the training\nspeed of other model sizes and summarize them below.\nS\nB\nL\nXL\nDiT\n20.0\n19.8\n9.3\n6.4\nSiT\n19.7\n20.8\n9.3\n6.8\nTable 2. Training speed (iters/sec) across all model sizes. All training speeds are measured on a TPU v4-64 pod. We note that the training\nspeed is largely influenced by the hardware state.\nSampling configurations\nWe maintain an exponential moving average (EMA) of all models weights over training with a\ndecay of 0.9999. All results are sampled from the EMA checkpoints, which is empirically observed to yield better performance.\nWe summarize the start and end points of our deterministic and stochastic samplers with different interpolants below, where\neach t0 and tN are carefully tuned to optimize performance and avoid numerical instability during integration.\nInterpolant\nModel\nObjective\nHeun\nEuler-Maruyama\nt0\ntN\nt0\ntN\nSBDM-VP\nvelocity\nLv\n1\n1e-5\n1\n4e-2\nscore\nLs\u03bb\n1\n1e-5\n1\n4e-2\nGVP\nvelocity\nLv\n1\n0\n1\n4e-2\nscore\nLs\n1 - 1e-5\n0\n1 - 1e-3\n4e-2\nLinear\nvelocity\nLv\n1\n0\n1\n4e-2\nscore\nLs\n1 - 1e-5\n0\n1 - 1e-3\n4e-2\nTable 3. Sampler configurations\nFID calculation\nWe calculate FID scores between generated images (10K or 50K) and all available real images in ImageNet\ntraining dataset. We observe small performance variations between TPU-based FID evaluation and GPU-based FID evaluation\n(ADM\u2019s TensorFlow evaluation suite [19]9). To ensure consistency with the basline DiT, we sample all of our models on GPU\nand obtain FID scores using the ADM evaluation suite.\n9https://github.com/openai/guided-diffusion/tree/main/evaluations\n7\nF. Additional Visual results\nFigure 2. Uncurated 512 \u00d7 512 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dvolcano\u201d(980)\nFigure 3. Uncurated 512 \u00d7 512 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201darctic fox\u201d(279)\n8\nFigure 4. Uncurated 512 \u00d7 512 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dloggerhead turtle\u201d(33)\nFigure 5. Uncurated 512 \u00d7 512 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dballoon\u201d(417)\n9\nFigure 6. Uncurated 512 \u00d7 512 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dred panda\u201d(387)\nFigure 7. Uncurated 512 \u00d7 512 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dgeyser\u201d(974)\n10\nFigure 8. Uncurated 256 \u00d7 256 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dmacaw\u201d(88)\nFigure 9. Uncurated 256 \u00d7 256 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dgolden retriever\u201d(207)\n11\nFigure 10. Uncurated 256 \u00d7 256 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201d ice cream\u201d(928)\nFigure 11. Uncurated 256 \u00d7 256 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dcliff\u201d(972)\n12\nFigure 12. Uncurated 256 \u00d7 256 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dhusky\u201d(250)\nFigure 13. Uncurated 256 \u00d7 256 SiT-XL samples.\nClassifier-free guidance scale = 4.0\nClass label = \u201dvalley\u201d(979)\n13\n"
  },
  {
    "title": "Asynchronous Local-SGD Training for Language Modeling",
    "link": "https://arxiv.org/pdf/2401.09135.pdf",
    "upvote": "9",
    "text": "Asynchronous Local-SGD Training for\nLanguage Modeling\nBo Liu*1, Rachita Chhaparia2, Arthur Douillard2, Satyen Kale3, Andrei A. Rusu2, Jiajun Shen2, Arthur Szlam2\nand Marc\u2019Aurelio Ranzato2\n*Work done as an intern at Google DeepMind, 1The University of Texas at Austin, 2Google DeepMind, 3Google Research\nLocal stochastic gradient descent (Local-SGD), also referred to as federated averaging, is an approach to\ndistributed optimization where each device performs more than one SGD update per communication.\nThis work presents an empirical study of asynchronous Local-SGD for training language models; that\nis, each worker updates the global parameters as soon as it has finished its SGD steps. We conduct a\ncomprehensive investigation by examining how worker hardware heterogeneity, model size, number of\nworkers, and optimizer could impact the learning performance. We find that with naive implementations,\nasynchronous Local-SGD takes more iterations to converge than its synchronous counterpart despite\nupdating the (global) model parameters more frequently. We identify momentum acceleration on the\nglobal parameters when worker gradients are stale as a key challenge. We propose a novel method\nthat utilizes a delayed Nesterov momentum update and adjusts the workers\u2019 local training steps based\non their computation speed. This approach, evaluated with models up to 150M parameters on the C4\ndataset, matches the performance of synchronous Local-SGD in terms of perplexity per update step, and\nsignificantly surpasses it in terms of wall clock time.\nKeywords: asynchronous training, language modeling, large-scale distributed learning\n1. Introduction\nLarge language models (LLMs) have revolution-\nized many applications, transforming the way\nmachines interact with human language. The\ncornerstone of this revolution is training these\nmodels at massive scale. To manage such large-\nscale training in reasonable amounts of time, it\nhas been necessary to distribute computations\nacross multiple devices. However, the standard\napproaches to this distributed training uses co-\nlocated devices with fast interconnects.\nOne might hope to be able to effectively har-\nness a broader range of computational resources,\nperhaps geographically distant from each other,\nin order to build even more powerful large mod-\nels. However, utilizing numerous distant devices\nfaces a significant hurdle: communication latency.\nWhen devices focus solely on computing gradi-\nents before sending them back to a central server,\nthe communication time can overshadow the com-\nputation time, creating a bottleneck in efficiency.\nTraining starts\nTraining ends\nSync.\nAsync.\nTime\nTime\nModel synchronization\n\u2026\n\u2026\nFigure 1 | Illustration of async. v.s. sync. training\nwith 2 workers (in blue and red). Sync. train-\ning suffers from the straggler effect, while async.\ntraining reduces the idling time of the fast worker.\nLocal Stochastic Gradient Descent (Local-SGD)\nis a collection of optimization methods that can\nreduce communication bottlenecks.1 These meth-\nods involve each device performing multiple local\ngradient steps before syncing their parameter up-\ndates with a parameter server. While Local-SGD\nenhances training efficiency by reducing commu-\nnication frequency, it can suffer from the straggler\n1The term Local-SGD, sometimes also known as Feder-\nated Average (FedAvg), is used here to emphasize its roots\nin distributed optimization, where users have control over\ndata allocation to different workers.\nCorresponding author(s): bliu@cs.utexas.edu\n\u00a9 2024 Google DeepMind. All rights reserved\narXiv:2401.09135v1  [cs.LG]  17 Jan 2024\nAsynchronous Local-SGD Training for Language Modeling\n20k\n22k\n24k\nFigure 2 | Comparative evaluation of language models using sync. and async. Local-SGD methods with\n4 heterogeneous workers on a 20M parameter model. The state-of-the-art sync. Local-SGD method,\nDiLoCo (Douillard et al., 2023), employs AdamW and Nesterov momentum as the worker-side and\nserver-side optimizers, respectively. This optimizer combination remains the strongest for async.\nLocal-SGD training (See Figure 5), yet underperforms DiLoCo significantly. By integrating Delayed\nNesterov (DN) (Algorithm 3) for outer optimization and Dynamic Local Updates (DyLU) (Section 5),\nwe significantly bridge the performance gap in terms of perplexity versus updates between sync. and\nasync. training in language modeling. Moreover, the proposed method significantly surpasses DiLoCo\nin terms of perplexity versus wall clock time.\neffect caused by heterogeneous devices. For in-\nstance, faster devices are idle waiting for slower\nones to catch up, undermining the overall effi-\nciency of the system. Moreover, all devices are\nforced to communicate at the same time requiring\nhigh bandwidth connection with the parameter\nserver. Asynchronous Local-SGD presents a more\nviable solution (illustrated in Figure 1), as it al-\nlows the server to update the model as soon as\nthe updates of a worker are available, thereby\nenhancing computational utilization and mini-\nmizing communication bandwidth requirements.\nIn this study, we explore the viability of asyn-\nchronously training LLMs using Local-SGD. We\nexpand upon previous works that have attempted\nto alternate steps on subsets of workers or ran-\ndomly drop certain subset of workers during\nsynchronous Local-SGD (Douillard et al., 2023;\nRyabinin et al., 2021). The main content is struc-\ntured into three parts:\n1. Framework (Section 3).\nThe first part intro-\nduces our high-level design for the asynchronous\ntraining framework. We discuss how each worker\ndetermines which data shard to train on, for how\nmany steps, with what learning rates, and how\nthe server updates models asynchronously.\n2. Optimization Challenge (Section 4).\nIn the\nsecond part, we conduct an empirical study of\nvarious existing optimization strategies suitable\nfor asynchronous Local-SGD. This includes both\nworker-side optimization (inner optimization)\nand server-side optimization (outer optimization).\nWe uncover a key challenge in utilizing momen-\ntum effectively. Notably, while adaptive momen-\ntum methods generally accelerate convergence\nof both inner and outer optimizations, their effi-\ncacy in asynchronous Local-SGD is comparatively\nreduced when both optimizations employ mo-\nmentum techniques, especially when contrasted\nwith the synchronous implementation.\n3. Proposed Solutions (Section 5).\nWe intro-\nduce two simple and effective techniques: the De-\nlayed Nesterov momentum update (DN) and Dy-\nnamic Local Updates (DyLU). These techniques,\nwhen combined and evaluated on training lan-\n2\nAsynchronous Local-SGD Training for Language Modeling\nAlgorithm 1 DiLoCo Algorithm (synchronous)\nRequire: Initial pretrained model \ud835\udf03(0)\nRequire: \ud835\udc58 workers\nRequire: Data shards {D1, . . . , D\ud835\udc58}\nRequire: Optimizers InnerOpt and OuterOpt\n1: for outer step \ud835\udc61 = 1 . . . \ud835\udc47 do\n2:\nparallel for worker \ud835\udc56 = 1 . . . \ud835\udc58 do\n3:\n\ud835\udf03(\ud835\udc61)\n\ud835\udc56\n\u2190 \ud835\udf03(\ud835\udc61\u22121)\n4:\nfor inner step \u210e = 1 . . . \ud835\udc3b\ndo\n5:\n\ud835\udc65 \u223c D\ud835\udc56\n6:\nL \u2190 \ud835\udc53 (\ud835\udc65, \ud835\udf03(\ud835\udc61)\n\ud835\udc56 )\n7:\n\ud835\udf03(\ud835\udc61)\n\ud835\udc56\n\u2190 InnerOpt(\ud835\udf03(\ud835\udc61)\n\ud835\udc56\n, \u2207L)\n8:\nend for\n9:\n\ud835\udeff(\ud835\udc61)\n\ud835\udc56\n= \ud835\udf03(\ud835\udc61\u22121) \u2212 \ud835\udf03(\ud835\udc61)\n\ud835\udc56\n10:\nend parallel for\n11:\n\u0394(\ud835\udc61) \u2190 1\n\ud835\udc58\n\u00cd\ud835\udc58\n\ud835\udc56=1 \ud835\udeff(\ud835\udc61)\n\ud835\udc56\n\u22b2 outer gradient\n12:\n\ud835\udf03(\ud835\udc61) \u2190 OuterOpt(\ud835\udf03(\ud835\udc61\u22121), \u0394(\ud835\udc61))\n13: end for\nguage model, allow asynchronous Local-SGD to\napproach synchronous Local-SGD in terms of per-\nplexity versus the total number of local updates,\nand further improve asynchronous Local-SGD vs.\nsynchronous Local-SGD in terms of perplexity ver-\nsus wall-clock, as detailed in Figure 2.\n2. Background\nIn this study, we focus on the distributed opti-\nmization of shared model parameters \ud835\udf03 across \ud835\udc58\ndata shards, denoted as D = {D1, . . . , D\ud835\udc58}, with\n\ud835\udc58 workers.2 The primary goal is described by the\nfollowing equation:\nmin\n\ud835\udf03\n\ud835\udc58\n\u2211\ufe01\n\ud835\udc56=1\n|D\ud835\udc56|\n\u00cd\n\ud835\udc57 |D\ud835\udc57|\ud835\udd3c\ud835\udc65\u223cD\ud835\udc56\n\u0002\n\u2113(\ud835\udc65; \ud835\udf03)\n\u0003\n,\n(1)\nwhere \u2113(\u00b7; \ud835\udf03) represents the loss function (for in-\nstance, cross entropy loss for next token predic-\ntion in language modeling), and | \u00b7 | indicates the\nset size.\nWe extend the definition of Local-SGD in this\nwork to include not just the original Local-SGD\n2We assume the number of workers (\ud835\udc58) equals the num-\nber of data shards, though our methods are also applicable\nwhen there are fewer workers than data shards.\nmethod, but also its variants that incorporate\nadvanced optimization techniques. We partic-\nularly focus on DiLoCo (Douillard et al., 2023),\nwhich sets the standard for synchronous Local-\nSGD in language modeling. DiLoCo\u2019s method-\nology is detailed in Algorithm 1. Each worker \ud835\udc56\nperforms \ud835\udc3b local updates using an inner optimizer\non their data shard D\ud835\udc56 before sending the param-\neter change (pseudo-gradient) \ud835\udeff(\ud835\udc61)\n\ud835\udc56\n= \ud835\udf03(\ud835\udc61\u22121) \u2212 \ud835\udf03(\ud835\udc61)\n\ud835\udc56\nback to the server. The server then computes the\naggregated outer gradient \u0394(\ud835\udc61) = 1\n\ud835\udc58\n\u00cd\ud835\udc58\n\ud835\udc56=1 \ud835\udeff(\ud835\udc61)\n\ud835\udc56 , and\napplies an outer optimizer with \u0394(\ud835\udc61) to update\n\ud835\udf03. A key insight from DiLoCo is the optimal use\nof AdamW and Nesterov Momentum as the best\ninner and outer optimizers, respectively.\n3. Async. Local-SGD Framework\nThis section outlines the asynchronous Local-SGD\npipeline design, where we assume a central server\ncontrols all workers and asynchronously aggre-\ngates their updates.\nData Shard Sampling\nUnlike in the federated\nlearning setting where each device is attached\nto its own data, in distributed optimization, the\nuser has the right to choose which data shard is\nassigned to which worker, even dynamically. To\nbalance the learning progress on different data\nshards (as workers are heterogeneous), whenever\na worker is ready to start a new local optimiza-\ntion round, we sample a data shard inversely pro-\nportional to its \u201clearning progress\". Specifically,\ndefine \ud835\udc5b\ud835\udc56 as the number of learned data points in\nD\ud835\udc56, then we sample a shard \ud835\udc56sampled according to:\n\ud835\udc56sampled \u223c \ud835\udc5d,\nwhere \ud835\udc5d\ud835\udc56 \u221d max(\n|D\ud835\udc56|\n\u00cd\n\ud835\udc57 |D\ud835\udc57| \u2212\n\ud835\udc5b\ud835\udc56\n\u00cd\n\ud835\udc57 \ud835\udc5b\ud835\udc57\n, 0).\n(2)\nIn other words, we sample a data shard only when\nit is \u201cunder-sampled\" (i.e.,\n\u00cd\ud835\udc5b\ud835\udc56\n\ud835\udc57 \ud835\udc5b\ud835\udc57 \u2264\n\u00cd| D\ud835\udc56|\n\ud835\udc57 | D\ud835\udc57| ). The\ndegree to which a shard is under-sampled deter-\nmines its sampling rate. By doing so, we ensure\nthat the data shard with slower progress is more\nlikely to be sampled for training, therefore bal-\nancing the learning progress across shards.\n3\nAsynchronous Local-SGD Training for Language Modeling\nLearning Rate Scheduling\nIn contrast to syn-\nchronous training methods like DiLoCo, asyn-\nchronous training can lead to uneven progress\nacross different data shards, especially when\nworkers are allowed varying numbers of train-\ning steps. This raises the question of how to\neffectively schedule learning rates. In our ap-\nproach we assign each data shard its own learn-\ning rate schedule. Specifically, we implement a\nlinear warmup combined with a cosine learning\nrate decay, where \ud835\udc47 represents the target total\ntraining iterations for each data shard:\n\ud835\udf02\ud835\udc61 =\n\uf8f1\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3\n\ud835\udc61\ud835\udf02max/\ud835\udc61warmup\n\ud835\udc61 < \ud835\udc61warmup\n\ud835\udf02min + 0.5(\ud835\udf02max \u2212 \ud835\udf02min)\n\u00001 + cos \u0000 \ud835\udc61\u2212\ud835\udc61warmup\n\ud835\udc47\u2212\ud835\udc61warmup \ud835\udf0b\u0001\u0001\n\ud835\udc61 \u2265 \ud835\udc61warmup.\n(3)\nIn practice, asynchronous training may conclude\nwith different final iteration counts (\ud835\udc61end) for each\ndata shard. Since we cannot predetermine \ud835\udc61end\ndue to the unpredictability of asynchrony, we\nset the minimum learning rate (\ud835\udf02min) to a small\npositive value. This ensures continued progress\neven if \ud835\udc61 exceeds \ud835\udc47.\nAdditionally, we adjust\n\ud835\udc47 \u2212 \ud835\udc61warmup to be non-negative and ensure that\nthe ratio\n\ud835\udc61\u2212\ud835\udc61warmup\n\ud835\udc47\u2212\ud835\udc61warmup remains within the range of\n[0, 1]. This helps maintain effective learning rate\nadjustments throughout the training process.\nGrace Period for Model Synchronization\nIn\nasynchronous training, the completion time of\neach worker\u2019s tasks can vary. For example, if\nworker B completes training shortly after worker\nA, it might be beneficial for A to wait briefly un-\ntil the server processes updates from both work-\ners before receiving the updated model for its\nnext training task. However, this waiting period\nshould be minimal and occur only when neces-\nsary. Specifically, if no other worker completes\nits task within the grace period while worker A is\nsynchronizing with the server\u2019s model, A should\npromptly commence its new training task using\nthe server\u2019s current model. For a visual represen-\ntation of this process, please refer to Figure 3.\nAsynchronous\nTask\nScheduling\nIn\nAlgo-\nrithm 2, we present the asynchronous task\nscheduling pipeline. Throughout the algorithm,\nGrace period\nTime\n1st sync.\nModel synchronization\nA\nB\nC\nTraining ends\nFigure 3 | We consecutively synchronize the up-\ndate from B after we synchronize A because B\nfinishes its training after A but before the end of\nthe grace period. A and B will therefore use the\nsame server model to start the new training jobs,\nwhile C will start its own grace period.\nwe use \ud835\udf0f to denote the actual wall clock time\nand \ud835\udc61 to denote model updates. In line 1-4, we\ninitialize the model, total local updates \ud835\udc61local,\nand the list of workers W and the completed\nworkers Wcompleted. In line 5, we start the first\ntraining job for all workers with the initial model\nparameter \ud835\udf03(0). Note that the train() function\nimplements the data sampling technique and\nperforms the learning rate scheduling mentioned\nbefore. In line 6, we reset the starting time of\nthe grace period \ud835\udf0fsync to \u221e. This is because we\nwant to synchronize with a worker only when\nits completion time is within \ud835\udf0fsync + \ud835\udf0fgrace. The\nmain asynchronous Local-SGD training loop is\nprovided in line 6-19. Within the loop, we first\nattempt to get a completed worker \ud835\udc64 (line 7).\nWe retrieve the earliest completed worker that\nwe have not yet processed yet, as long as its\ncompletion time is still within the grace period\n(e.g., \ud835\udc64.completed_time \u2264 \ud835\udf0fsync + \ud835\udf0fgrace). If no\nsuch workers exist, get_worker() will return\nnull.\nIn line 10-15 where such a worker \ud835\udc64\nis found, we synchronize its update with the\nserver model \ud835\udf03. In line 17-20 when no such\nworkers are found, we assign new training jobs\nfor all completed workers and empty the list of\ncompleted workers. For the detailed pseudocode\nof the train() and get_worker() functions, please\nrefer to Appendix 10.2. In practice, for the sake\nof reproducibility of research, we implement\na determininistic version of Algorithm 2 with\nfaked training time based on real-world device\nstatistics.\nWe validate the correctness of the\ntraining pipeline by simulating synchronous\nupdates using the asynchronous framework.\n4\nAsynchronous Local-SGD Training for Language Modeling\nAlgorithm 2 Async. Local-SGD Task Scheduling.\nRequire: Initial pretrained model \ud835\udf03(0)\nRequire: \ud835\udc58 workers\nRequire: Grace period \ud835\udf0fgrace\nRequire: Total local updates \ud835\udc61max\n1: \ud835\udc61local = 0\n2: \ud835\udf03 \u2190 \ud835\udf03(0)\n3: W = [init_worker() for \ud835\udc56 in [\ud835\udc58]]\n4: Wcompleted = []\n5: train(W, \ud835\udf03)\n6: \ud835\udf0fsync = \u221e\n\u22b2 start of the grace period\n7: while \ud835\udc61local < \ud835\udc61max do\n8:\n\ud835\udc64 = get_worker(W, \ud835\udf0fgrace, \ud835\udf0fsync)\n9:\n\u22b2 get a completed worker\n10:\nif \ud835\udc64 exists then\n11:\n\u22b2 synchronize the update with server\n12:\n\ud835\udf0fsync = min(\ud835\udf0fsync, \ud835\udc64.completed_time)\n13:\n\ud835\udf03 \u2190 sync(\ud835\udf03, \ud835\udc64.update)\n14:\nWcompleted.add(\ud835\udc64)\n15:\n\ud835\udc61local += \ud835\udc64.local_updates\n16:\nelse\n17:\n\u22b2 assign jobs for completed workers\n18:\n\ud835\udf0fsync = \u221e\n19:\ntrain(Wcompleted, \ud835\udf03)\n20:\nWcompleted = []\n21:\nend if\n22: end while\n4. Optimization Challenge\nEffect of InnerOpt + OuterOpt\nTo study\nhow optimization affects the language modeling\nperformance in asynchronous Local-SGD, we\nfirst experiment with different combinations\nof the inner and outer optimizers (we use\nA+B to denote A and B as the inner and outer\noptimizer,\nrespectively):\nSGD+Nesterov,\nSGD+Adam,\nAdamW+SGD,\nAdamW+SGD\nMomentum, AdamW+Adam, AdamW+Nesterov.\nThe hyperparameters for each combination are\ntuned separately, for AdamW as InnerOpt we\nkept the default values. We assume there are\n\ud835\udc58 = 4 workers, whose device speed is shown in\nFigure 4. Then we apply asynchronous Local-SGD\nfinetuning on a 20M-parameter language model\nfor 64,000 steps per worker (256,000 local steps\nin total), where the initial model checkpoint was\npretrained for 24,000 steps with Adam without\ndistributed training. We choose finetuning with\nLocal-SGD as it has been observed that Local-SGD\nstyle methods work well in finetuning but is less\nefficient from scratch (Lin et al., 2018), though\nothers have also observed that Local-SGD works\nwell even for training from scratch (Douillard\net al., 2023). The learning rate scheduling and\ntask scheduling follow the procedures described\nin Section 3. We use inner steps = 50 across all\nworkers in all experiments by default. The result\nis shown in Figure 5.\nFigure 4 | Steps per second for each device.\nFigure 5 | Performance of using different combi-\nnations of inner and outer optimizers for asyn-\nchronous Local-SGD training on a 20M language\nmodel with 4 workers.\nObservation The analysis reveals that combin-\ning AdamW as the inner optimizer with Nesterov\nmomentum as the outer optimizer yields the best\nresults, aligning with findings from synchronous\ntraining, like the DiLoCo method. Notably, us-\ning AdamW as the outer optimizer is less effec-\ntive. This may be because AdamW, derived from\nAdam, introduces a normalization effect, which\ncan be counterproductive in Local-SGD where\n5\nAsynchronous Local-SGD Training for Language Modeling\npseudo-gradients tend to be larger than true gra-\ndients, potentially slowing convergence. When\nAdamW is used in the inner optimization, SGD,\nSGD Momentum, and Nesterov show compara-\nble performance. However, Nesterov not only\nstabilizes the learning curve but also slightly im-\nproves final performance. This can be attributed\nto its update mechanism (here we abuse the no-\ntation and let \ud835\udc61 denote \ud835\udc61server):\n\ud835\udc5a\ud835\udc61+1 = \ud835\udefd\ud835\udc5a\ud835\udc61 + \ud835\udc54\ud835\udc61\n\ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 \u2212 \ud835\udf16\u0000\ud835\udefd2\ud835\udc5a\ud835\udc61 + (1 + \ud835\udefd)\ud835\udc54\ud835\udc61\n\u0001\n,\n(4)\nwhere \ud835\udf16 is the learning rate, \ud835\udc5a\ud835\udc61 is the momentum,\n\ud835\udc54\ud835\udc61 the gradient at time \ud835\udc61, and \ud835\udefd \u2208 (0, 1) the decay\nfactor. The key difference between Nesterov\nand SGD Momentum is in how Nesterov adjusts\nthe weightings, reducing the momentum compo-\nnent (\ud835\udefd2 instead of \ud835\udefd) and increasing the gradient\ncomponent (1 + \ud835\udefd instead of 1). This suggests\nthat momentum plays a crucial yet intricate role\nin Local-SGD.\nMomentum in the OuterOpt\nTo delve deeper\ninto the momentum term\u2019s impact on the outer\noptimizer, we conducted comparative analyses be-\ntween AdamW+SGD and AdamW+Nesterov un-\nder both synchronous and asynchronous training\nsettings. These experiments were carried out un-\nder identical conditions as described earlier. The\nresults are reported in Figure 6.\nFigure 6 | Comparison of AdamW+SGD and\nAdamW+Nesterov in both synchronous and\nasynchronous Local-SGD training.\nObservation The figure clearly shows that in\nasynchronous Local-SGD, AdamW+SGD, which\nlacks a momentum term, leads to better fi-\nnal perplexity and learning efficiency than its\nsynchronous counterpart. However, incorporat-\ning Nesterov momentum into the OuterOpt\nsignificantly boosts the performance of syn-\nchronous Local-SGD, outperforming the asyn-\nchronous version.\nIt\u2019s noteworthy that asyn-\nchronous AdamW+Nesterov remains the best\nperformer across all tested combinations of in-\nner and outer optimizers (as seen in Figure 5).\nThis observation indicates that while momentum\nis beneficial in asynchronous Local-SGD for lan-\nguage modeling, its effect is more pronounced in\nsynchronous settings.\nIs Staleness the Cause?\nWe further apply the\nasynchronous DiLoCo algorithm with homoge-\nneous devices. By doing so, we maximally di-\nminish the staled gradient problem in Local-SGD,\nwhich refers to the fact that we are using an out-\ndated outer gradient to update the server model.\nIn particular, this means if we have \ud835\udc58 workers, all\nof them will return the computed outer gradient\nback to the server at the same time. Therefore,\nthe only staleness comes from the fact that we\nare sequentially applying the individual updates\ninstead of aggregating them together and apply\nit once. Results are summarized in Figure 7.\nFigure 7 | Async. DiLoCo with heterogeneous\ndevices.\nObservation Figure 7 reveals a notable finding:\neven with homogeneity among workers, asyn-\n6\nAsynchronous Local-SGD Training for Language Modeling\nchronous DiLoCo significantly lags behind its syn-\nchronous counterpart.\nThis suggests that the\ninherent staleness from sequentially applying si-\nmultaneous updates leads to considerable per-\nformance drops. To elucidate this effect, let\u2019s\nconsider a scenario with \ud835\udc58 = 4 workers provid-\ning identical outer gradients (denoted as \ud835\udc54). The\nstandard Nesterov momentum update is outlined\nin Equation (4). In a sequential application of\npseudo-gradients:\n\ud835\udc5a\ud835\udc61+1 = \ud835\udefd4\ud835\udc5a\ud835\udc61 + (1 + \ud835\udefd + \ud835\udefd2 + \ud835\udefd3)\ud835\udc54\n\ud835\udf03\ud835\udc61+1 = \ud835\udf03\ud835\udc61 \u2212 \ud835\udf16\u0000(4 + 4\ud835\udefd + 3\ud835\udefd2 + 2\ud835\udefd3 + \ud835\udefd4)\ud835\udc54\n+ \ud835\udefd2(1 + \ud835\udefd + \ud835\udefd2 + \ud835\udefd3)\ud835\udc5a\ud835\udc61\n\u0001\n.\n(5)\nFrom this, we observe that sequential application\nresults in a more rapidly decaying momentum\nterm but amplifies the actual change in parameter\n\ud835\udf03. Consequently, a higher \ud835\udefd maintains more re-\ncent momentum but may lead to greater changes\nin parameters, and vice versa. Importantly, this\nimbalance cannot be simply rectified by reducing\nthe learning rate.\nBaselines\nWe consider several synchronous\nbaselines from the literature, and their naive ap-\nplication to an asynchronous setting: 1) Finetune\n1 worker (4xbatch): This involves finetuning a\nsingle worker with a larger batch size, equating\nto synchronous SGD. 2) DiLoCo (Douillard et al.,\n2023): This synchronous Local-SGD method com-\nbines AdamW with Nesterov. 3) Async. DiLoCo:\nThe asynchronous version of DiLoCo.\nExisting Fixes\nWe investigated potential fixes\nfrom the asynchronous Local-SGD literature to\naddress observed challenges. The following meth-\nods were considered:\n1) Async.\nDiLoCo +\nPoly (Xie et al., 2019): Extends Async. DiLoCo\nby downweighting the pseudo-gradient with \ud835\udc54 \u2190\n(1 + staleness)\u22120.5\ud835\udc54. 2) Async. DiLoCo + Poly-\nThres: Adds a threshold to discard updates with\nstaleness beyond 10. 3) Async. DiLoCo + De-\nlay Comp. (Zheng et al., 2017): Introduces de-\nlay compensation (Delay Comp.) to approximate\ntrue pseudo-gradients. Denote the gradient func-\ntion at \ud835\udf03 as \ud835\udc54(\ud835\udf03), then the main idea of delay\ncompensation is to approximate the true gradi-\nent \ud835\udc54(\ud835\udf03\ud835\udc61) by a stale gradient \ud835\udc54(\ud835\udf03\ud835\udc61\u2032) (\ud835\udc61\u2032 < \ud835\udc61) with\nthe first-order Taylor approximation, e.g., \ud835\udc54(\ud835\udf03\ud835\udc61) \u2248\n\ud835\udc54(\ud835\udf03\ud835\udc61\u2032) + \u2207\ud835\udc54(\ud835\udf03\ud835\udc61\u2032)(\ud835\udf03\ud835\udc61 \u2212 \ud835\udf03\ud835\udc61\u2032). In practice, the Hessian\n\u2207\ud835\udc54(\ud835\udf03\ud835\udc61\u2032) is approximated by diagonalized gradi-\nent outer product, e.g., \u2207\ud835\udc54(\ud835\udf03\ud835\udc61\u2032) \u2248 \ud835\udf06\ud835\udc54(\ud835\udf03\ud835\udc61\u2032) \u2299 \ud835\udc54(\ud835\udf03\ud835\udc61\u2032),\nwhere \ud835\udf06 \u2208 \u211d+ is a hyperparameter. In our setting,\nwe apply the delay compensation technique to\npseudogradients instead of gradients. 4) Async.\nBuffer: Accumulates and averages all gradients\nin a First-In, First-Out fashion before applying\nNesterov updates; a variation of the original\nFedBuff algorithm (Nguyen et al., 2022), using\nAdamW+Nesterov. The results are provided in\nFigure 8.\nFigure 8 | Comparison of different asynchronous\nLocal-SGD approaches. Poly, PolyThres, and De-\nlay Comp. barely improve the async. Local-SGD\nperformance. Async. Buffer significantly closes\nthe gap between sync. and async. training, while\nintroducing instability in early stage of training.\nObservation\nPolynomial discounting of the\npseudo-gradient shows marginal benefits. Thresh-\nolding and delay compensation techniques don\u2019t\noffer much improvements. Again, the fact that\ndelay compensation is not working well points\nout the difference between asynchronous SGD\nand asynchronous Local-SGD. The Async. Buffer\nmethod excels at convergence but exhibits insta-\nbility early in training. Crucially, none of the meth-\nods match the performance of the synchronous\nDiLoCo baseline.\n7\nAsynchronous Local-SGD Training for Language Modeling\n5. Proposed Solutions\nIn addressing the optimization challenges out-\nlined in Section 4, we developed two strategies.\nDelayed Nesterov Update\nNotably, the Async.\nBuffer method demonstrated promising perfor-\nmance (as shown in Figure 8).\nAdditionally,\nour analysis revealed that asynchronous training\nwith AdamW+SGD, sans outer momentum, outper-\nforms synchronous methods (Figure 5). Based on\nthese insights, we propose the Delayed Nesterov\n(DN) strategy, which represents the sync() func-\ntion in Algorithm 2. This approach involves us-\ning the Nesterov update intermittently\u2014every\n\ud835\udc41 server updates. Between Nesterov updates,\nwe aggregate pseudo-gradients in a buffer \u0394 and\nupdate the model parameters using gradient de-\nscent (or gradient descent plus a small fraction\nof the old momentum). To balance gradient and\nmomentum-based descent, we introduce a pa-\nrameter \ud835\udc50 \u2208 [0, 1/\ud835\udc41]. A \ud835\udc50 value of 0 indicates\npure gradient descent between Nesterov up-\ndates, while \ud835\udc50 equal to 1 evenly distributes the\nmomentum term over \ud835\udc41 updates. The specifics of\nthis algorithm are detailed in Algorithm 3. Unlike\nthe Async. Buffer (Nguyen et al., 2022), which\nupdates model parameters only once in \ud835\udc41 periods,\nthe Delayed Nesterov continuously updates using\ngradients, incorporating a fraction of the old mo-\nmentum, and updates the momentum term once\nevery \ud835\udc41 server updates.\nDynamic Local Updates\nThe Delayed Nesterov\napproach addresses the momentum challenge in\nthe OuterOpt by buffering pseudo-gradients and\nstrategically delaying momentum updates. An al-\nternative perspective considers synchronous train-\ning as a solution, where pseudo-gradients from\nall workers are synchronized. However, the di-\nversity in device capabilities often hinders simul-\ntaneous pseudo-gradient returns, if each worker\nexecutes the same number of local training steps.\nA viable workaround involves customizing local\ntraining steps (e.g., \ud835\udc64.steps) based on the process-\ning speed of each device. In particular, denote\n\ud835\udc63(\ud835\udc64) as the training speed (in terms of steps per\nsecond) for worker \ud835\udc64, we can compute a worker\u2019s\nAlgorithm 3 Delayed Nesterov Update.\nRequire: Initial model parameter \ud835\udf030\nRequire: Momentum decay \ud835\udefd \u2208 (0, 1)\nRequire: Momentum activation \ud835\udc50 \u2208 [0, 1/\ud835\udc41]\n\u22b2 default to \ud835\udc50 = 0\nRequire: Buffer size \ud835\udc41\n\ud835\udc61 = 0\n\ud835\udc5a0 = 0\n\u22b2 momentum\n\u0394 = 0\n\u22b2 aggregated gradient\nwhile not finished do\nReceive the pseudo-gradient \ud835\udc54\ud835\udc61\n\u22b2 sync. step in Alg. 2.\n\u0394 \u2190 \u0394 + \ud835\udc54\ud835\udc61\nif (\ud835\udc61 + 1) % \ud835\udc41 == 0 then\n\ud835\udc5a\ud835\udc61+1 \u2190 \ud835\udefd\ud835\udc5a\ud835\udc61 + \u0394/\ud835\udc41\n\ud835\udf03\ud835\udc61+1 \u2190 \ud835\udf03\ud835\udc61 \u2212 \ud835\udf16\u0000(1 \u2212 \ud835\udc50\ud835\udc41 + \ud835\udc50)\ud835\udefd\ud835\udc5a\ud835\udc61+1 + \ud835\udc54\ud835\udc61/\ud835\udc41\u0001\n\u0394 = 0\nelse\n\ud835\udc5a\ud835\udc61+1 \u2190 \ud835\udc5a\ud835\udc61\n\u22b2 delay momentum update\n\ud835\udf03\ud835\udc61+1 \u2190 \ud835\udf03\ud835\udc61 \u2212 \ud835\udf16\u0000\ud835\udc50\ud835\udefd\ud835\udc5a\ud835\udc61+1 + \ud835\udc54\ud835\udc61/\ud835\udc41\u0001\nend if\n\ud835\udc61 \u2190 \ud835\udc61 + 1\nend while\ndesired training steps as:\n\ud835\udc64.step =\n\u0016\n\ud835\udc63(\ud835\udc64)\nmax\ud835\udc64\u2032\u2208W \ud835\udc63(\ud835\udc64\u2032) \ud835\udc3b\n\u0017\n,\n(6)\nwhere \ud835\udc3b denotes the number of local training\nsteps the fastest worker runs and \u230a\ud835\udc65\u230b denotes\nthe largest integer not greater than \ud835\udc65.3 We name\nthis approach the Dynamic Local Updates (DyLU).\nThis adjustment allows slower workers to execute\nfewer steps, aligning the completion times across\ndifferent workers. Incorporating a grace period\nfor model synchronization in this setup further\nreduces the impact of stale gradients, improving\noverall performance.\n6. A Minimal Toy Example\nFor the convenience of future research and quick\nprototyping of new ideas, we present a minimal\ntoy example that replicates the observed optimiza-\ntion challenge in asynchronous Local-SGD (See\n3Here, we implicitly assumes the device speeds are\nknown a priori. If this is not the case, it is straightforward to\nestimate the device speed based on empirical observations.\n8\nAsynchronous Local-SGD Training for Language Modeling\nFigure 9).4 The task is to perform classification\non a mixture of mixtures of Gaussian data.\nThe Dataset\nAdamW + Nesterov\nAdamW + SGD\nAsync.\nSync.\nFigure 9 | Replicating the optimization chal-\nlenge on the toy example.\nLeft: the dataset\nconsists of a mixture of mixtures of Gaus-\nsians.\nRight:\nAsync.\nLocal-SGD performs\nworse/better than sync. Local-SGD when using\nAdamW+Nesterov/AdamW+SGD.\nObservation Comparing Figure 9 to Figure 6,\nwe observe that the toy example demonstrate the\nsame optimization challenge.\n7. Experiments\nThis section details experiments conducted to as-\nsess the efficacy of our two proposed methods,\nDelayed Nesterov (DN) and Dynamic Local Up-\ndates (DyLU). Additionally, ablation studies ex-\nplore the effectiveness of these methods as we\nvary the number of workers and model sizes.\nEvaluating Delayed Nesterov (DN) and Dy-\nnamic Local Updates (DyLU)\nFigure 2 com-\npares asynchronous Local-SGD with DN and DyLU\nagainst baselines such as single worker finetuning\nand DiLoCo, using the same setup as in Figure 8.\nObservation The results demonstrate that DN\ncombined with DyLU significantly reduces per-\nplexity, surpassing the synchronous DiLoCo\u2019s per-\nformance over updates. Additionally, DN+DyLU\noutperforms in terms of time efficiency, avoiding\ndelays from slower workers.\nAssessing Different Levels of Worker Hetero-\ngeneity\nWe examine how both the proposed\nDN+DyLU method and vanilla asynchronous\n4Please check the Colab at https://github.com/\ngoogle-deepmind/asyncdiloco\nDiLoCo fare under varying degrees of worker de-\nvice heterogeneity, as shown in Figure 10 (per-\nplexity curve) and Table 1 (final perplexity).\nLevel of heterogeneity\nno\nslight\nmoderate\nvery\nPretrained (24K)\n61.64\n61.64\n61.64\n61.64\nFinetune (4\u00d7 batch size)\n42.47\n42.47\n42.47\n42.47\nDiLoCo (Douillard et al., 2023)\n41.35\n41.35\n41.35\n41.35\nAsync. DiLoCo\n44.27\n44.38\n44.29\n44.27\nAsync. DN + DyLU (ours)\n41.27\n41.27\n41.09\n41.13\nTable 1 | Varying the level of worker heterogene-\nity (top-left, top-right, bottom-left, and bottom-\nright of Figure 10 correspond to no, slight, mod-\nerate, and very, respectively).\nObservation\nDN+DyLU consistently excels\nacross all heterogeneity levels.5\nInterestingly,\neven with homogeneous devices, vanilla asyn-\nchronous DiLoCo struggles, suggesting that the\nissue partly lies in the sequential application of\npseudogradients. This underscores the impor-\ntance of delayed momentum updates. Addition-\nally, a periodic oscillation in performance is ob-\nserved in certain device groupings, further high-\nlighting the lack of robustness of the original asyn-\nchronous algorithm.\nAblation with Different Numbers of Workers\nWe apply DN+DyLU while varying the number of\nworkers (4, 8, 16) using a 20M model, with re-\nsults summarized in Figure 11 (perplexity curve)\nand Table 2 (final perplexity).\nNumber of workers \ud835\udc58\n4\n8\n16\nPretrained (24K)\n61.64\n61.64\n61.64\nFinetune (\ud835\udc58\u00d7 batch size)\n42.47\n41.28\n40.60\nDiLoCo (Douillard et al., 2023)\n41.35\n41.23\n41.25\nAsync. DiLoCo\n44.27\n44.23\n44.23\nAsync. DN + DyLU (ours)\n41.13\n41.02\n40.98\nTable 2 | Varying the number of workers.\nObservation As the number of workers increases,\nthe benefit of Local-SGD training diminishes. No-\ntably, with 16 workers, single worker finetuning\n(16x batch size) shows the best performance over\n5We notice that Async. DN+DyLU performs slightly bet-\nter than DiLoCo when there is no heterogeneity, this is due\nto the numerical error, as the two methods reduce to the\nsame and the training curves match almost perfectly.\n9\nAsynchronous Local-SGD Training for Language Modeling\nupdates. Yet, DN+DyLU closely aligns with syn-\nchronous DiLoCo in performance, demonstrating\nits potential as a DiLoCo alternative in heteroge-\nneous settings.\nAblation with Different Model Sizes\nLastly,\nwe apply DN+DyLU to models of varying sizes\n(20M, 60M, 150M), with results summarized in\nFigure 12 (perplexity curve) and Table 3 (final\nperplexity).\nModel size\n20M\n60M\n150M\nPretrained (24K)\n61.64\n30.19\n22.80\nFinetune (4x batch size)\n42.47\n24.80\n17.47\nDiLoCo (Douillard et al., 2023)\n41.35\n24.55\n17.23\nAsync. DiLoCo\n44.27\n25.64\n18.08\nAsync. DN + DyLU (ours)\n41.13\n24.53\n17.26\nTable 3 | Varying the model sizes.\nObservation\nBoth synchronous and asyn-\nchronous Local-SGD methods outperform the ap-\nproach of finetuning a single worker with an in-\ncreased batch size. Notably, this advantage be-\ncomes more pronounced during the later stages\nof convergence, aligning with findings from previ-\nous research that highlight Local-SGD\u2019s superior\ngeneralization capabilities (Gu et al., 2023). Addi-\ntionally, our proposed DN+DyLU method demon-\nstrates consistent efficacy across various model\nsizes. It\u2019s important to note that the performance\ndisparity between synchronous and asynchronous\nDiLoCo does not diminish even as the model size\nincreases.\nAblation with Different \ud835\udc50\nWe apply \ud835\udc50 \u2208 {0, 0.1}\nin Async. DN+DyLU with varying \ud835\udc58 (4, 8, 16) and\nmodel sizes (20M, 60M, 150M), with the 4 \u201cvery\"\nheterogeneous workers. This is because when\nthe level of heterogeneity is small, using different\n\ud835\udc50 will have smaller difference (e.g., when there\nis no heterogeneity, any \ud835\udc50 results in the same\nalgorithm). Results are summarized in Table 4.\nObservation Empirically, we observe no signifi-\ncant difference between \ud835\udc50 = 0 and \ud835\udc50 = 0.1, indi-\ncating that adding slight momentum at interme-\ndiate steps does not help too much. As a result,\nwe set \ud835\udc50 = 0 as the default value in Algorithm 3,\nNumber of workers \ud835\udc58\n4\n8\n16\nAsync. DN + DyLU (\ud835\udc50 = 0)\n41.13\n41.02\n40.98\nAsync. DN + DyLU (\ud835\udc50 = 0.1)\n41.16\n40.93\n41.04\nModel size\n20M\n60M\n150M\nAsync. DN + DyLU (\ud835\udc50 = 0)\n41.13\n24.53\n17.26\nAsync. DN + DyLU (\ud835\udc50 = 0.1)\n41.16\n24.69\n17.27\nTable 4 | Varying the \ud835\udc50 \u2208 {0, 0.1} in Algorithm 3.\nwhich corresponds to performing SGD updates\nbetween two consecutive Nesterov updates. Note\nthat setting the value of \ud835\udc50 does not introduce any\noverhead to the overall algorithm.\n8. Related Work\nThis section provides a concise overview of the lit-\nerature on federated learning and local-SGD style\ndistributed optimization, particularly focusing on\ntheir applications in asynchronous settings.\nLocal-SGD\nand\nDistributed\nOptimization\nLocal-SGD is a specific distributed optimization\ntechnique designed to reduce communication\nfrequency (Bijral et al., 2016; Coppola, 2015;\nMcDonald et al., 2010; Stich, 2018; Zhang\net al., 2016; Zinkevich et al., 2010). The core\nprinciple of Local-SGD is to let each worker\nexecute several local training iterations prior\nto engaging in global synchronization.\nThis\ntechnique was later applied to the federated\nlearning setting, leading to the development of\nthe FedAvg method (McMahan et al., 2017),\nwhich aims to reduce communication overhead.\nUnlike Local-SGD, federated learning also ad-\ndresses user privacy issues and typically involves\nheterogeneous devices.\nTo further minimize\ncommunication overhead, FedOpt integrates\nadaptive optimization methods like SGD momen-\ntum and Adam (Reddi et al., 2020). However,\nas client/worker heterogeneity increases, the\nconvergence rate often deteriorates. Methods\nlike SCAFFOLD (Karimireddy et al., 2020) and\nMIME (Karimireddy et al., 2021) have been\nintroduced to adapt these optimization methods\nfor heterogeneous environments.\n10\nAsynchronous Local-SGD Training for Language Modeling\nFigure 10 | Varying the heterogeneity in devices.\n4 workers\n8 workers\n16 workers\nFigure 11 | Varying the number of workers.\nAsynchronous Training\nAsynchronous training\nwas developed to mitigate the \u201cstraggler effect\"\nobserved in synchronous distributed optimization,\nwhere learning efficiency is bottlenecked by the\nslowest worker (Dean et al., 2012; Diskin et al.,\n2021b; Koh et al., 2006; Lian et al., 2015, 2018;\nRecht et al., 2011). A significant challenge in\nasynchronous optimization is the staled gradient\nproblem, which occurs when an outdated gradi-\nent is applied to a recently updated model. Asyn-\nchronous SGD with delay compensation (Zheng\net al., 2017) addresses this issue by approximat-\n11\nAsynchronous Local-SGD Training for Language Modeling\n20M\n60M\n150M\nFigure 12 | Varying the model size.\ning the true gradient using the old gradient. Asyn-\nchronous methods have also been explored in\nfederated learning contexts (Xie et al., 2019). De-\nspite the challenge, asynchronous training has\ndemonstrated success for language modeling as\nwell (Diskin et al., 2021b), by using heteroge-\nneous devices across the world.\nLocal-SGD for Language Modeling\nThe con-\ncept of local-SGD (or FedAvg) has previously been\napplied in the realm of language modeling. Cross-\ndevice federated learning, for instance, has been\nutilized to pretrain and fine-tune language mod-\nels (Borzunov et al., 2022; Diskin et al., 2021a;\nHilmkil et al., 2021; Presser, 2020; Ro et al., 2022;\nRyabinin et al., 2021). More recently, DiLoCo has\nextended the local-SGD methodology to encom-\npass larger language models, specifically propos-\ning the use of AdamW + Nesterov momentum\nas the InnerOpt + OuterOpt pairing. In asyn-\nchronous settings, the FedBuff (Nguyen et al.,\n2022) algorithm buffers pseudogradients from\nclients, updating the server model only after accu-\nmulating a sufficient number of pseudogradients.\nTimelyFL (Zhang et al., 2023) aims to reduce\nasynchrony by allowing slower devices to train\nonly parts of the model.\n9. Limitations\nThis study, while comprehensive, has several lim-\nitations. First, we identify a significant optimiza-\ntion challenge linked to momentum updates in\nthe OuterOpt, but the precise cause of this issue\nremains unclear. Understanding this challenge\nwith robust theoretical backing presents an in-\ntriguing avenue for future research. Second, our\nempirical observations suggest that the advan-\ntages of the Local-SGD method diminish with an\nincreasing number of workers, a phenomenon\nwhose underlying reasons are yet to be under-\nstood. This issue currently hinders the scalabil-\nity of asynchronous Local-SGD. Finally, although\nour proposed method DN+DyLU shows improved\nempirical performance, it lacks formal theoretical\nconvergence guarantees, an aspect that merits\nfurther investigation.\n10. Conclusion\nThis study presents a thorough examination of\nasynchronous Local-SGD in language modeling.\n12\nAsynchronous Local-SGD Training for Language Modeling\nOur central finding is that while momentum in\nthe outer optimization loop is crucial, it may be\nless effective in asynchronous scenarios compared\nto synchronous ones when implemented naively.\nTo bridge this gap, we introduce a novel approach,\nfocusing on sporadic momentum updates using\nbuffered pseudogradients, combined with con-\ntinuous stochastic pseudogradient updates. Fur-\nthermore, our research reveals that tailoring lo-\ncal training steps to each worker\u2019s computational\nspeed is not only a straightforward but also an\nefficient strategy to enhance performance.\nHowever, there is much work to be done. In\nthe standard (as opposed to the \u201clocal\u201d) gradient\ndescent setting, the optimal batch size in terms of\ndecreasing loss as quickly as possible in terms of\nnumber of weight updates is not usually \u201cas large\nas possible\u201d. In our view, similarly, there is hope\nfor asynchronous Local-SGD methods that give\nbetter results per local update than synchronous\nLocal-SGD.\nAcknowledgements\nWe would like to thank Adam Fisch for his valu-\nable feedback.\nReferences\nAvleen S Bijral, Anand D Sarwate, and Nathan\nSrebro. On data dependence in distributed\nstochastic\noptimization.\narXiv\npreprint\narXiv:1603.04379, 2016.\nAlexander Borzunov, Dmitry Baranchuk, Tim\nDettmers, Max Ryabinin, Younes Belkada,\nArtem Chumachenko, Pavel Samygin, and\nColin Raffel. Petals: Collaborative inference\nand fine-tuning of large models. arXiv preprint\nlibrary, 2022.\nGregory Francis Coppola. Iterative parameter\nmixing for distributed large-margin training\nof structured predictors for natural language\nprocessing. 2015.\nJeffrey Dean, Greg Corrado, Rajat Monga, Kai\nChen, Matthieu Devin, Mark Mao, Marc\u2019aurelio\nRanzato, Andrew Senior, Paul Tucker, Ke Yang,\net al. Large scale distributed deep networks.\nAdvances in neural information processing sys-\ntems, 25, 2012.\nMichael\nDiskin,\nAlexey\nBukhtiyarov,\nMax\nRyabinin, Lucile Saulnier, Quentin Lhoest,\nAnton Sinitsin, Dmitry Popov, Dmitry Pyrkin,\nMaxim Kashirin, Alexander Borzunov, Albert\nVillanova del Moral, Denis Mazur, Ilia Kobelev,\nYacine Jernite, Thomas Wolf, and Gennady\nPekhimenko.\nDistributed deep learning in\nopen collaborations.\nAdvances in Neural\nInformation\nProcessing\nSystems\n(NeurIPS),\n2021a.\nMichael\nDiskin,\nAlexey\nBukhtiyarov,\nMax\nRyabinin,\nLucile Saulnier,\nAnton Sinitsin,\nDmitry Popov,\nDmitry V Pyrkin,\nMaxim\nKashirin, Alexander Borzunov,\nAlbert Vil-\nlanova del Moral, et al.\nDistributed deep\nlearning in open collaborations. Advances in\nNeural Information Processing Systems, 34:\n7879\u20137897, 2021b.\nArthur Douillard, Qixuan Feng, Andrei A Rusu,\nRachita Chhaparia, Yani Donchev, Adhiguna\nKuncoro, Marc\u2019Aurelio Ranzato, Arthur Szlam,\nand Jiajun Shen.\nDiloco: Distributed low-\ncommunication training of language models.\narXiv preprint arXiv:2311.08105, 2023.\nXinran Gu, Kaifeng Lyu, Longbo Huang, and San-\njeev Arora. Why (and when) does local sgd\ngeneralize better than sgd?\narXiv preprint\narXiv:2303.01215, 2023.\nAgrin Hilmkil, Sebastian Callh, Matteo Barbi-\neri, Leon Ren\u00e9 S\u00fctfeld, Edvin Listo Zec, and\nOlof Mogren. Scaling federated learning for\nfine-tuning of large language models. In Inter-\nnational Conference on Applications of Natural\nLanguage to Information Systems, pages 15\u201323.\nSpringer, 2021.\nJordan Hoffmann, Sebastian Borgeaud, Arthur\nMensch, Elena Buchatskaya, Trevor Cai, Eliza\nRutherford, Diego de Las Casas, Lisa Anne\nHendricks, Johannes Welbl, Aidan Clark, Tom\nHennigan, Eric Noland, Katie Millican, George\nvan den Driessche, Bogdan Damoc, Aurelia Guy,\nSimon Osindero, Karen Simonyan, Erich Elsen,\nJack W. Rae, Oriol Vinyals, and Laurent Sifre.\n13\nAsynchronous Local-SGD Training for Language Modeling\nTraining compute-optimal large language mod-\nels. Advances in Neural Information Processing\nSystems (NeurIPS), 2022.\nSai Praneeth Karimireddy, Satyen Kale, Mehryar\nMohri, Sashank Reddi, Sebastian Stich, and\nAnanda Theertha Suresh. Scaffold: Stochastic\ncontrolled averaging for federated learning. In\nInternational conference on machine learning,\npages 5132\u20135143. PMLR, 2020.\nSai Praneeth Karimireddy, Martin Jaggi, Satyen\nKale, Mehryar Mohri, Sashank Reddi, Sebas-\ntian U Stich, and Ananda Theertha Suresh.\nBreaking the centralized barrier for cross-\ndevice federated learning. Advances in Neu-\nral Information Processing Systems, 34:28663\u2013\n28676, 2021.\nByung-Il Koh, Alan D George, Raphael T Haftka,\nand Benjamin J Fregly. Parallel asynchronous\nparticle swarm optimization. International jour-\nnal for numerical methods in engineering, 67(4):\n578\u2013595, 2006.\nXiangru Lian, Yijun Huang, Yuncheng Li, and\nJi Liu. Asynchronous parallel stochastic gra-\ndient for nonconvex optimization. Advances\nin neural information processing systems, 28,\n2015.\nXiangru Lian, Wei Zhang, Ce Zhang, and Ji Liu.\nAsynchronous decentralized parallel stochastic\ngradient descent. In International Conference\non Machine Learning, pages 3043\u20133052. PMLR,\n2018.\nTao Lin, Sebastian U Stich, Kumar Kshitij Patel,\nand Martin Jaggi. Don\u2019t use large mini-batches,\nuse local sgd. arXiv preprint arXiv:1808.07217,\n2018.\nTao Lin, Sebastian U. Stich, Kumar Kshitij Patel,\nand Martin Jaggi. Don\u2019t use large mini-batches,\nuse local sgd. Proceedings of the International\nConference on Learning Representations (ICLR),\n2020.\nRyan McDonald, Keith Hall, and Gideon Mann.\nDistributed training strategies for the struc-\ntured perceptron. In Human language technolo-\ngies: The 2010 annual conference of the North\nAmerican chapter of the association for compu-\ntational linguistics, pages 456\u2013464, 2010.\nBrendan McMahan, Eider Moore, Daniel Ram-\nage, Seth Hampson, and Blaise Aguera y Ar-\ncas. Communication-efficient learning of deep\nnetworks from decentralized data. In Artifi-\ncial intelligence and statistics, pages 1273\u20131282.\nPMLR, 2017.\nJohn Nguyen, Kshitiz Malik, Hongyuan Zhan,\nAshkan Yousefpour, Mike Rabbat, Mani Malek,\nand Dzmitry Huba. Federated learning with\nbuffered asynchronous aggregation. In Interna-\ntional Conference on Artificial Intelligence and\nStatistics, pages 3581\u20133607. PMLR, 2022.\nShawn Presser.\nSwarm training, 2020.\nURL\nhttps://battle.shawwn.com/swarm-\ntraining-v01a.pdf.\nColin Raffel, Noam Shazeer, Adam Roberts,\nKatherine\nLee,\nSharan\nNarang,\nMichael\nMatena, Yanqi Zhou, Wei Li, and Peter J. Liu.\nExploring the limits of transfer learning with\na unified text-to-text transformer. Journal of\nMachine Learning Research, 2020.\nBenjamin Recht, Christopher Re, Stephen Wright,\nand Feng Niu. Hogwild!: A lock-free approach\nto parallelizing stochastic gradient descent. Ad-\nvances in neural information processing systems,\n24, 2011.\nSashank Reddi, Zachary Charles, Manzil Zaheer,\nZachary Garrett, Keith Rush, Jakub Kone\u010dn`y,\nSanjiv Kumar, and H Brendan McMahan. Adap-\ntive federated optimization.\narXiv preprint\narXiv:2003.00295, 2020.\nJae\nHun\nRo,\nTheresa\nBreiner,\nLara\nMcConnaughey,\nMingqing\nChen,\nAnanda\nTheertha\nSuresh,\nShankar\nKu-\nmar, and Rajiv Mathews.\nScaling language\nmodel size in cross-device federated learning.\narXiv preprint arXiv:2204.09715, 2022.\nMax Ryabinin,\nEduard Gorbunov,\nVsevolod\nPlokhotnyuk, and Gennady Pekhimenko. Mosh-\npit sgd: Communication-efficient decentralized\ntraining on heterogeneous unreliable devices.\nAdvances in Neural Information Processing Sys-\ntems, 34:18195\u201318211, 2021.\n14\nAsynchronous Local-SGD Training for Language Modeling\nSebastian U Stich.\nLocal sgd converges fast\nand communicates little.\narXiv preprint\narXiv:1805.09767, 2018.\nCong Xie, Sanmi Koyejo, and Indranil Gupta.\nAsynchronous federated optimization. arXiv\npreprint arXiv:1903.03934, 2019.\nJian\nZhang,\nChristopher\nDe\nSa,\nIoannis\nMitliagkas, and Christopher R\u00e9. Parallel sgd:\nWhen does averaging help?\narXiv preprint\narXiv:1606.07365, 2016.\nTuo Zhang, Lei Gao, Sunwoo Lee, Mi Zhang, and\nSalman Avestimehr. Timelyfl: Heterogeneity-\naware asynchronous federated learning with\nadaptive partial training. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5063\u20135072, 2023.\nShuxin Zheng, Qi Meng, Taifeng Wang, Wei Chen,\nNenghai Yu, Zhi-Ming Ma, and Tie-Yan Liu.\nAsynchronous stochastic gradient descent with\ndelay compensation. In International Confer-\nence on Machine Learning, pages 4120\u20134129.\nPMLR, 2017.\nMartin Zinkevich, Markus Weimer, Lihong Li, and\nAlex Smola. Parallelized stochastic gradient de-\nscent. Advances in neural information processing\nsystems, 23, 2010.\nSupplementary Materials\n10.1. Implementation Details\nHyperparameter\nValue\nInner learning rate\n0.1\nFinal inner learning rate\n0.0, 0.000001, 0.0002\nNumber of warmup steps\n0, 1,000\nWeight decay\n0.1\nBatch Size\n128, 512\nSequence length\n256\nOuter Optimizer\nSGD, SGDM, Nesterov, Adam, delayed momentum SGD\nInner Optimizer\nSGD, AdamW\nOuter learning rate\n0.03, 0.3, 0.1, 0.7\nAsync soup weight\n0.125, 0.25, 0.5, 1.0\nAsync soup method\nconstant, polynomial, svrg\nDelay period\n4, 8, 16\nCommunication frequency \ud835\udc3b\n50, 100, 150\nNumber of pretraining steps\n24, 000\nTable 5 | Optimization Hyperparameters evalu-\nated during in this work. Chosen values for main\nexperiments are highlighted in bold.\nTable 6 | Model Configuration for the three eval-\nuated sizes. All are based on the transformer\narchitecture, chinchilla-style (Hoffmann et al.,\n2022).\nHyperparameter\n20M\n60M\n150M\nNumber of layers\n6\n3\n12\nHidden dim\n256\n896\n896\nNumber of heads\n4\n16\n16\nK/V size\n64\n64\n64\nVocab size\n32,000\nNetwork Architecture\nWe displayed in Table 6\nthe architectural difference between the 20M,\n60M, and 150M models.\nThey are all trans-\nformer decoder-only, based on the Chinchilla fam-\nily (Hoffmann et al., 2022).\nTraining Dataset\nWe consider a language mod-\neling task on the C4 dataset, a dataset derived\nfrom Common Crawl (Raffel et al., 2020). The\ntotal number of steps is set to 88,000 for all mod-\nels, with 24,000 steps of pre-training done with-\nout any federated learning methods, akin to post\nLocal-SGD (Lin et al., 2020).\nHyperparameters\nIn Table 5, we outline the\noptimization hyperparameters considered for this\nstudy.\nInner Optimizer States\nFollowing Douillard\net al. (2023), in all experiments, when worker\nB picks up the data shard worker A just finishes\ntraining on, we reset the AdamW\u2019s optimizer state.\nIn other words, each local worker-side training\nis an independent training process with a new\noptimizer, and only the learning rate is adjusted\naccording as described in Section 3.\n10.2. Aync. Training Pseudocode\nIn this section, we provide the pseudocode for\nthe train() and get_worker() functions in Algo-\nrithm 2.\n15\nAsynchronous Local-SGD Training for Language Modeling\nAlgorithm 4 train() in Algorithm 2.\nRequire: Available workers W\nRequire: Current server model \ud835\udf03\n1: for \ud835\udc64 \u2208 W do\n2:\nSample shard D\u2032 for \ud835\udc64 (Eq. 2).\n3:\n\ud835\udc64.local_updates = DyLU(D\u2032) (Eq. 6).\n4:\nDecide lr schedule (\ud835\udc64.lr) (Eq. 3).\n5:\n\ud835\udc64.update = train_worker(\ud835\udc64, D\u2032, \ud835\udf03).\n6: end for\nAlgorithm 5 get_worker() in Algorithm 2.\nRequire: Workers W\nRequire: Grace period \ud835\udf0fgrace\nRequire: Start of the grace period \ud835\udf0fsync.\n1: if all workers in W are not done then\n2:\nreturn null\n3: else\n4:\n\ud835\udc64 = earliest completed worker in W.\n5:\nif \ud835\udc64.completed_time \u2212 \ud835\udf0fsync \u2264 \ud835\udf0fgrace then\n6:\nreturn \ud835\udc64\n7:\nelse\n8:\nreturn null\n9:\nend if\n10: end if\n16\n"
  },
  {
    "title": "TextureDreamer: Image-guided Texture Synthesis through Geometry-aware Diffusion",
    "link": "https://arxiv.org/pdf/2401.09416.pdf",
    "upvote": "8",
    "text": "TextureDreamer: Image-guided Texture Synthesis through Geometry-aware\nDiffusion\nYu-Ying Yeh13\nJia-Bin Huang23\nChangil Kim3\nLei Xiao3\nThu Nguyen-Phuoc3\nNumair Khan3\nCheng Zhang3\nManmohan Chandraker1\nCarl S Marshall3\nZhao Dong3\nZhengqin Li3\n1University of California, San Diego\n2University of Maryland, College Park\n3Meta\nGenerated textures\nInput Images\nTarget 3D shape\nGenerated textures\nInput Images\nInput Images\nGenerated textures\nFigure 1. Texture transfer from sparse images. Given a small number of images and a target mesh, our method synthesizes geometry-\naware texture that looks similar to the input appearances for diverse objects.\nAbstract\nWe present TextureDreamer, a novel image-guided tex-\nture synthesis method to transfer relightable textures from a\nsmall number of input images (3 to 5) to target 3D shapes\nacross arbitrary categories. Texture creation is a pivotal\nchallenge in vision and graphics. Industrial companies hire\nexperienced artists to manually craft textures for 3D assets.\nClassical methods require densely sampled views and ac-\ncurately aligned geometry, while learning-based methods\nare confined to category-specific shapes within the dataset.\nIn contrast, TextureDreamer can transfer highly detailed,\nintricate textures from real-world environments to arbi-\ntrary objects with only a few casually captured images, po-\ntentially significantly democratizing texture creation. Our\ncore idea, personalized geometry-aware score distillation\n(PGSD), draws inspiration from recent advancements in\ndiffuse models, including personalized modeling for texture\ninformation extraction, variational score distillation for de-\ntailed appearance synthesis, and explicit geometry guid-\nance with ControlNet. Our integration and several essen-\ntial modifications substantially improve the texture quality.\nExperiments on real images spanning different categories\nshow that TextureDreamer can successfully transfer highly\nrealistic, semantic meaningful texture to arbitrary objects,\nsurpassing the visual quality of previous state-of-the-art.\nProject page: https://texturedreamer.github.\nio\n1\narXiv:2401.09416v1  [cs.CV]  17 Jan 2024\n1. Introduction\nHigh-quality 3D content is indispensable for a wide range\nof critical applications, including AR/VR, robotics, film,\nand gaming. In recent years, remarkable progress has been\nmade in democratizing 3D content creation pipelines, fa-\ncilitated by advancements in 3D reconstruction [40, 42]\nand generative models [18, 59]. While substantial atten-\ntion has been devoted to exploring the geometry compo-\nnent [8, 12, 64] and neural implicit representations [44],\nsuch as NeRF [40], creation of high-quality textures is rel-\natively under-explored. Textures are pivotal in creating re-\nalistic, highly detailed appearances and are integral to vari-\nous graphics pipelines, where industry has traditionally re-\nlied on professional, experienced artists to craft textures.\nThis process usually involves manually authoring procedu-\nral graphs [1] and UV maps, making it expensive and in-\nefficient. Automatically transferring the diverse visual ap-\npearance of objects around us to the texture of any target\ngeometry would thus be highly beneficial.\nWe present TextureDreamer, a novel framework to cre-\nate high-quality relightable textures from sparse images.\nGiven 3 to 5 randomly sampled views of an object, we\ncan transfer its texture to an target geometry that may come\nfrom a different category. This is an extremely challenging\nproblem, as previous texture creation methods usually ei-\nther require densely sampled views with aligned geometry\n[3, 32, 68], or can only work for category-specific shapes\n[4, 21, 46, 58].\nOur framework draws inspiration from\nrecent advancements in diffusion-based generative models\n[23, 59, 60]. Trained on billions of text-image pairs, these\ndiffusion models enable text-guided image generation with\nextraordinary visual quality and diversity [52]. Pioneering\nworks have applied these pre-trained 2D diffusion models\nto text-guided 3D content creation [34, 47, 63]. However,\na common limitation among those methods is that text-only\ninput may not be sufficiently expressive to describe com-\nplex, detailed patterns, as demonstrated in Figure 2. In con-\ntrast to text-guided methods, we effectively extract texture\ninformation from a small set of input images by fine-tuning\nthe pre-trained diffusion model with a unique text token\n[16, 54]. Our framework, therefore, addresses the challenge\nof accurately describing complex textures.\nThe Score Distillation Sampling (SDS) [47, 62] is one\ncore element that bridges pre-trained 2D diffusion models\nwith 3D content creation. It is widely used to generate and\nedit 3D contents by minimizing the discrepancy between\nthe distribution of rendered images and the distribution de-\nfined by the pre-trained diffusion models [34, 37]. Despite\nits popularity, two well-known limitations impede its abil-\nity to generate high-quality textures. First, it tends to create\nover-smoothed and saturated appearances due to the unusu-\nally high classifier-free guidance necessary for the method\nto converge. Second, it lacks the knowledge to generate a\nthere is a green chair with a leaf print on it\nInput image\nImage Captioning via BLIP\nTarget 3D shape\nText-guided texturing [TEXTure] Image-guided texturing [Ours]\nFigure 2. Limitation of text-guided texturing. Compared to text-\nguided texturing method which requires a captioning method to\ngenerate a text prompt which might not express all the details of\nthe image, image-based guided texturing can be more effective and\nmore expressive. Image captioning is predicted by BLIP [33], text-\nguided texturing is generated via TEXTure [53], and image-guided\nresult is from our method.\n3D-consistent appearance, often resulting in multi-face ar-\ntifacts and mismatches between textures and geometry.\nWe propose two key design choices to tackle these chal-\nlenges. Instead of using SDS, we build upon Variational\nScore Distillation (VSD) in our optimization approach,\nwhich can generate much more photorealistic and diverse\ntextures. Initially introduced in ProlificDreamer [63], VSD\ntreats the whole 3D representation as a random variable and\naligns its distribution with the pre-trained diffusion model.\nIt does not need a large classifier-free guidance weight to\nconverge, which is essential to create a realistic and diverse\nappearance. However, na\u00a8\u0131vely applying VSD update does\nnot suffice for generating high-quality textures in our ap-\nplication. We identify a simple modification that can im-\nprove texture quality while slightly reducing the compu-\ntational cost.\nAdditionally, VSD loss alone cannot fully\nsolve the 3D consistency issue. Fine-tuning on sparse inputs\nmakes converging harder, as observed by previous work\n[51]. We, therefore, explicitly condition our texture gen-\neration process on geometry information extracted from the\ngiven mesh by injecting rendered normal maps into the fine-\ntuned diffusion model through the ControlNet [67] archi-\ntecture. Our framework, designated as personalized geome-\ntry aware score distillation (PGSD), can effectively transfer\nhighly detailed textures to diverse geometry in a semanti-\ncally meaningful and visually appealing manner. Exten-\nsive qualitative and quantitative experiments demonstrate\nthat our framework substantially outperforms state-of-the-\nart texture-transfer methods.\n2. Related Works\nTexture synthesis and reconstruction\nClassical texture\ncreation methods involve sampling from a distribution de-\nrived from the neighborhood [13, 28], tiling repetitive pat-\nterns [29] or fusing multi-view images onto the object sur-\n2\nfaces [3, 32, 68].\nThe former two fall short in creat-\ning semantic meaningful textures while the latter one re-\nquires highly accurate geometry reconstruction. Numerous\nlearning-based methods were proposed to learn texture cre-\nation from large-scale 3D datasets [4, 11, 21, 46, 58] but\nare confined to specific categories within the dataset. Re-\ncent works also use CLIP model [50] for text-guided tex-\nture generation of arbitrary objects [31, 36, 39, 41], but\ntheir texture qualities are usually low. In contrast, Texture-\nDreamer can create semantically meaningful, high-quality\ntextures for arbitrary objects using uncorrelated sparse im-\nages. Traditionally, textures are represented as a 2D im-\nage and projected to object surfaces through UV mapping.\nLeveraging the recent progress in neural implicit represen-\ntation, our method, along with recent developments in in-\nverse rendering [5, 7, 17, 61] and 3D generation [7, 17],\nrepresents texture as a neural implicit texture field.\nDiffusion models\nDiffusion models [59] have emerged\nas the state-of-the-art generative models [23, 60], demon-\nstrating exceptional visual quality [52].\nIts training and\ninference involve iteratively adding noise with different\nvariances and denoise the data. Trained on internet-scale\nimage-text pair datasets [52], these pre-trained models ex-\nhibit unprecedented capability in text-guided image syn-\nthesis and have proven successful in various image editing\ntasks. Recent works also manage to fine-tune pre-trained\ndiffusion models on much smaller datasets or even a few im-\nages to facilitate customized/personalized image synthesis\n[54] and image generation conditioned on multi-modal data\n[67], such as normal and semantic maps. Building upon this\nprogress, TextureDreamer can effectively extract texture in-\nformation from sparse views and transfer it to a novel target\nobject in a geometry-aware manner.\n3D generation with 2D diffusion priors Diffusion-based\n3D content creation has very recently gained substantial in-\nterest. Several methods directly train 3D diffusion mod-\nels to generate 3D content in various representations, in-\ncluding point cloud [35], neural radiance filed [26], hyper-\nnetwork [14] and texture [66]. Others utilize pre-trained\n2D diffusion models by either progressively fusing gener-\nated images from different views [2, 6, 9, 53] or optimiz-\ning the 3D representation through score distillation sam-\npling [34, 37, 47] and its improved variations [27, 63].\nWhile many methods concentrate on text-guided 3D gener-\nation, fewer attempt to leverage diffusion models to gener-\nate 3D content from images. A number of concurrent works\nfine-tune 2D diffusion models on large-scale 3D datasets\nfor sparse view reconstruction [48, 57], primarily focusing\non whole 3D object reconstruction. In contrast, Texture-\nDreamer targets transferring textures from a small number\nof images to a target 3D shape with unmatched geometry.\nDreambooth3D [51] and TEXTure [53] extract information\nfrom sparse views into a new text token and fine-tuned dif-\nfusion model weights, which can be used to generate per-\nsonalized 3D object or texture unseen objects.\nTexture-\nDreamer employs a similar method to extract information\nfrom sparse images. However, it differs from prior works\non utilizing the extracted information for texture generation,\nleading to improvements in consistency and photorealism.\n3. Method\nWe propose TextureDreamer, a framework which synthe-\nsizes geometry-aware texture for a given mesh with appear-\nance similar to 3-5 input images of an object. In Section 3.1,\nwe first introduce preliminaries on Dreambooth [54], Con-\ntrolNet [67] and score distillation sampling [47, 62, 63]. In\nSection 3.2, we propose personalized geometry-aware score\ndistillation (PGSD), which is our core technical contribu-\ntion that enables high-quality image-guided texture transfer\nfrom sparse images to arbitrary geometries.\n3.1. Preliminaries\nDreambooth [54] is a simple yet effective method to fine-\ntune pre-trained text-to-image diffusion models on a small\nnumber of input images for personalized text-guided im-\nage generation. It stores the subject\u2019s appearance into the\ndiffusion model weights with a specific text-token \u201c[V]\u201d.\nDreambooth is fine-tuned with two loss functions. Recon-\nstruction loss is standard diffusion denoising supervision on\nthe input images. Class-specific prior preservation loss is\nproposed to avoid language drift and loss of diversity caused\nby fine-tuning. It further supervises the pre-trained model\nwith a large number of its own generated examples. Tex-\ntureDreamer uses DreamBooth to distill texture information\nfrom input images. Instead of image synthesis, we apply the\ndistilled information to a 3D object with different geometry.\nControlNet [67] proposes a novel architecture that adds\nspatial conditioning control to pre-trained diffusion models.\nThe key insight is to reuse the large number of diffusion\nmodel parameters trained on billions of images and insert\nsmall convolution networks into the model with window\nsize 1 and zero-initialized weights. It enables robust fine-\ntuning performance on small datasets with different types of\n2D conditions, such as depth, normal, and edge maps. We\nutilize ControlNet models to ensure that our created textures\nare aligned with the given geometry.\nScore Distillation Sampling [47, 62] is the core compo-\nnent of numerous methods that use pre-trained 2D diffusion\nmodels for 3D content creation [10, 34, 47]. It optimizes\nthe 3D representation by pushing its rendered images to a\nhigh-dimensional manifold modeled by the pre-trained dif-\nfusion model. Let \u03b8 be the 3D representation and \u03f5\u03c8 be the\npre-trained diffusion model. The gradient back-propagated\n3\nTextureDreamer Optimization Phase\nTexture Extraction Phase\nAlbedo\nMetallic\nRoughness\n3D Mesh\nOptimized BRDF Field\nRendering\nA photo of [V] object\nCamera Extrinsic\n3D Mesh\n\ud835\udf19\nDifferentiable\nRenderer\n\ud835\udf0c\nCamera Encoder\n+ Noise\nBRDF Field\nGeneric\nDiffusion Model\n\ud835\udf13\nPersonalized\nDiffusion Model\nfor [V] Object \n\ud835\udf03\n\ud835\udf03\u2217\nExtract Texture Maps\nDreambooth finetuning\nInput Images\nRendered Surface Normal\nRendered Image\nText Prompt\n\u2133\n\ud835\udc66\n\ud835\udc58\n\ud835\udc53!(\u22c5)\n\ud835\udc54())\n\ud835\udc50\"\nx\n\ud835\udc65#\n\ud835\udc61\nBackpropagation\nBackpropagation\n\ud835\udc4e\n\ud835\udc5a\n\ud835\udc5f\n\ud835\udc50 = (\ud835\udc45, \ud835\udc47)\nFigure 3. Overview of TextureDreamer, a framework which synthesizes texture for a given mesh with appearance similar to 3-5 input\nimages of an object. We first obtain personalized diffusion model \u03c8 with Dreambooth [54] finetuning on input images. The spatially-\nvarying bidirectional reflectance distribution (BRDF) field f\u03b8 for the 3D mesh M is then optimized through personalized geometric-aware\nscore distillation (PGSD) (detailed in Section 3.2). After optimization finished, high-resolution texture maps corresponding to albedo,\nmetallic, and roughness can be extracted from the optimized BRDF field.\nto the parameter \u03b8 is\n\u2207\u03b8LSDS(\u03b8) \u225c Et,\u03f5\n\u0014\nw(t)(\u03f5\u03c8 (xt, y, t) \u2212 \u03f5) \u2202g(\u03b8, c)\n\u2202\u03b8\n\u0015\n,\nwhere \u03c9(t) is the weight coefficient, y is the text input, t is\nthe time step, c is the camera pose, g(\u00b7) is a differentiable\nrenderer, xt is the noisy image computed by adding noise\nto the rendered image x = g(\u03b8, c) with variance dependent\non time t. Despite its wide usage, SDS requires a much\nhigher weight than normal classifier-free guidance [22] to\nconverge, oversmoothed and oversaturated appearance. To\novercome this issue, Wang et al. [63] propose an improved\nversion, called variational score distillation (VSD), which\ncan converge with standard classifier-free guidance. VSD\ntreats the whole 3D representation \u03b8 as a random variable\nand minimizes the KL divergence between \u03b8 and the dis-\ntribution defined by the pre-trained diffusion model. It in-\nvolves fine-tuning a LoRA [24] network \u03f5\u03d5 (and a camera\nencoder \u03c1 which embeds camera pose c as an condition in-\nput to \u03f5\u03d5) to denoise the noisy images generated from 3D\nrepresentation \u03b8\nmin\n\u03d5 Et,\u03f5,c\n\u0002\n||\u03f5\u03d5(xt, y, t, c) \u2212 \u03f5||2\n2\n\u0003\n(1)\nThe gradient to the 3D representation \u03b8 is then computed as\nEt,\u03f5,c\n\u0014\nw(t)(\u03f5\u03c8 (xt, y, t) \u2212 \u03f5\u03d5(xt, y, t, c)) \u2202g(\u03b8, c)\n\u2202\u03b8\n\u0015\n. (2)\nWhile VSD significantly improves both visual quality and\ndiversity of generated 3D contents, it cannot address the 3D\nconsistency issue due to the inherent lack of 3D knowledge,\nleading to multi-face errors and mismatches between geom-\netry and textures. We address this challenge by explicitly in-\njecting geometry information to make our diffusion model\ngeometry aware.\n3.2. Personalized Geometry-aware Score Distilla-\ntion (PGSD)\nProblem setup.\nWe illustrate our method in Figure 3.\nThe inputs to our framework include a small set of im-\nages (3 to 5) casually captured from different views {I}K\nk=1\nand a target 3D mesh M.\nThe outputs of our frame-\nwork are relightable textures transferred from image set\n{I}K\nk=1 to M in a semantically meaningful and visually\npleasing manner. Our relightable textures are parameter-\nized as standard microfacet bidirectional reflectance distri-\nbution (BRDF) model [25], which consists of 3 parameters,\ndiffuse albedo a, roughness r, and metallic m. We delib-\nerately do not optimize normal maps as it encourages the\npipeline to fake details that are inconsistent with mesh M.\nFollowing the recent trend of neural implicit representation\n[20, 42, 43], during optimization, we represent our texture\nas a neural BRDF field f\u03b8(v) : v \u2208 R3 \u2192, a, r, m \u2208 R5,\nwhere v is an arbitrary point sampled on the surface of M\n4\nand f\u03b8 consists of a multi-scale hash encoding and a small\nMLP. We find such an implicit representation can better reg-\nularize the optimization process, leading to smoother tex-\ntures. However, given the UV mapping of M, our rep-\nresentation can also be converted to standard 2D texture\nmaps that are compatible with standard graphics pipelines,\nby querying every 3D point corresponding to each texel, as\nshown on the right-hand side of Figure 3.\nPersonalized texture information extraction. We follow\nDreambooth [54] to extract texture information from sparse\nimages. To be specific, we fine-tune a personalized diffu-\nsion model on input images with a text prompt y, \u201cA photo\nof [V] object\u201d, where \u201c[V]\u201d is a unique identifier to de-\nscribe the input object. Compared to the alternative textual\ninversion method [16], we observe that Dreambooth con-\nverges faster and can better preserve intricate texture pat-\nterns, possibly due to its larger capacity. We first mask out\nthe background of the target object with a white color. For\nthe reconstruction loss, we resize the shorter edge of input\nimages to 512 and randomly crop 512x512 patches for train-\ning. We do not apply class-specific prior preservation loss,\nas we hope our Dreambooth finetuning model can gener-\nalize to other categories. We also experiment with different\nvariations, including jointly fine-tuning the text encoder and\nreplacing the diffusion denoising network with a pre-trained\nControlNet, but do not observe any improvements.\nGeometry-aware score distillation\nOnce we finish ex-\ntracting texture information with Dreambooth, we trans-\nfer the information to mesh M by adopting the fine-tuned\nDreambooth model as the denoising network \u03f5\u03c8 for score\ndistillation sampling. Specifically, we choose VSD instead\nof the original SDS because of its superior ability to gen-\nerate highly realistic and diverse appearances. To render\nimages x for VSD gradient computation, we follow Fanta-\nsia3D [10] to pre-select a fixed HDR environment map E\nas illumination and use Nvdiffrast [30] as our differentiable\nrenderer. We set the object background to be a constant\nwhite color to match the input images for Dreambooth train-\ning. We observe this can help achieve better color fidelity\ncompared to random color or neutral background.\nHowever, simply replacing SDS with VSD cannot ad-\ndress the limitation of lacking 3D knowledge in 2D diffu-\nsion models. We thus propose geometry-aware score dis-\ntillation, where we inject geometry information extracted\nfrom mesh M into our personalized diffusion model \u03f5\u03c8\nthrough a pre-trained ControlNet conditioned on normal\nmaps k rendered from M. This augmentation significantly\nboosts 3D consistency of generated textures (see Figure 10).\nWith the ControlNet conditioning, the pillow texture from\nthe input images can be accurately matched to the target\nshape, despite the shape mismatch. We experiment with\ndifferent ControlNet conditions and show that normal con-\nditions can best prevent texture-geometry mismatch.\nLet x = g(\u03b8, c) be the rendered image under a fixed envi-\nronment map from camera pose c with the extracted BRDF\nmaps a\u03b8, r\u03b8, m\u03b8. The gradient of proposed Personalized\nGeometry-aware Score Distillation (PGSD) to optimize the\nMLP parameter \u03b8 of BRDF field is:\n\u2207\u03b8LPGSD(\u03b8)\n\u225c Et,\u03f5,c[w(t)(\u03f5\u03c8(xt; y, k, t) \u2212 \u03f5\u03d5(xt; y, k, t, c\u03c1))\u2202x\n\u2202\u03b8],\nwhere xt = \u03b1tx + \u03c3t\u03f5 is the rendered image x perturbed\nby noise \u03f5 \u223c N(0, I) at time t, c\u03c1 is the embedding of the\ncamera extrinsic c encoded by a learnable camera encoder\n\u03c1, \u03f5\u03c8 and \u03f5\u03d5 are the fine-tuned personalized diffusion model\nand the generic diffusion model pretrained on a large-scale\ndataset, respectively. Both models are augmented with Con-\ntrolNet conditioned on normal map k, as shown in the yel-\nlow part underneath the diffusion model in Figure 3.\nWe found that our method does not benefit from\nclassifier-free guidance (CFG) [22], probably because the\npersonalized model \u03f5\u03c8 has been fine-tuned on a small num-\nber of images. Since our goal is to faithfully transfer input\nappearance to target shape, it is not necessary to have CFG\nto increase the diversity. Similar observation can be found\nin recent literature [55].\nWe\nadditionally\nidentify\nseveral\nimportant\ndesign\nchoices through extensive experiments. First, it is impor-\ntant to initialize the \u03f5\u03d5 in Eq. 1 with original pre-trained\ndiffusion model weights while the Dreambooth weight will\nremove texture details. This is probably because the Dream-\nbooth fine-tuning process makes the diffusion model over-\nfit to a small training set, as pointed out by previous work\n[51]. Moreover, we find that removing the LoRA weights\ncan substantially improve texture fidelity. Similar difficul-\nties in training LoRA were also reported in [56]. We there-\nfore implement our personalized geometry-aware score dis-\ntillation loss LP GSD by removing the LoRA structure in \u03f5\u03d5\nand only keeping the camera embedding, achieving the best\nquality. We show more comparisons in Figure 10.\n4. Experiment\n4.1. Experimental setup\nDataset.\nWe conduct our experiments on 4 categories of\nobjects: sofa, bed, mug/bowl, and plush toy. For each cat-\negory, we select 8 instances of objects and create a small\nimage set by casually sampling 3 to 5 views surrounding\nthe object, resulting in 32 image sets in total. For every\nimage in the 32 image sets, we apply U2-Net [49] to obtain\nthe foreground mask automatically or use a semi-auto back-\nground removal application1 to obtain more accurate masks.\nWe perform texture transfer for each image set to diverse\n1https://www.remove.bg/upload\n5\nOurs preferred over\nLatent-Paint\nTEXTure\nImage Fidelity\n71.82%\n69.43%\nTexture Photorealism\n77.03%\n85.52%\nShape-Texture Consistency\n78.49%\n85.16%\nTable 1. User study on image-guided texture transfer.\nmeshes including but not limited to same category shapes,\ndifferent category shapes, or even geometry with different\ngenus numbers. To test our texture-transferring framework,\nwe select 3 meshes for each of the 4 categories that are dis-\nsimilar to the captured image sets. We acquire these 3D\nmeshes from 3D-FUTURE [15] and online repositories.23.\nWe run intra-class texture transfer for all 4 categories of ob-\njects and also run inter-class texture transfer between bed\nand chair, to test our method\u2019s generalization ability.\nImplementation details.\nWe implement our framework\nbased on PyTorch [45] and Threestudio [19]. We use latent\ndiffusion and ControlNet v1.1 as our pre-trained diffusion\nmodel and ControlNet respectively. In all our experiments,\nwe set the classifier-free guidance weight of LP GSD as 1.0\n(equivalent to setting \u03c9 = 0 in the original CFG formu-\nlation). Following DreamFusion [47], we also apply view-\ndependent conditioning to the input text prompt. The BRDF\nfield is parameterized with an MLP using hash-grid posi-\ntional encoding [42], following prior works [10, 63]. Our\ncamera encoder consists of two linear layers that project the\ncamera extrinsic to a latent vector of 1, 280 dimensions to\nbe fused with time and text embedding in U-Net. We em-\npirically set the learning rate to 0.01 for encoding, 0.001 for\nMLP, and 0.0001 for camera encoder for all experiments.\n4.2. Baseline methods\nLatent-paint [37] and TEXTure [53] are two recent text-\nguided texturing methods with 2D diffusion prior. They also\ndemonstrate the capability of texturing meshes from im-\nages. Latent-paint [37] leverages the Texture Inversion [16]\nto extract image information into text embedding and dis-\ntills the texture with SDS. TEXTure [53] first finetunes the\npre-trained diffusion model by combining Texture Inversion\nand Dreambooth [54] and use this fine-tuned model to syn-\nthesize texture with an iterative mesh painting algorithm.\nAs preferred by the previous method [53], we augment the\ninput images with a random color background. We closely\nfollow the original implementation of baseline methods to\nrun the experiments.\n4.3. Image-guided texture transfer\nQualitative evaluation Our method can perform texture\ntransfer to diverse object geometry, including geometry in\n2https://www.cgtrader.com/\n3https://sketchfab.com/\nCLIP similarity \u2191\nLatent-Paint [37]\n0.7969\nTEXTure [53]\n0.7988\nOurs\n0.8296\nTable 2. Quantitative evaluation on image-guided texturing.\nthe same category or across different categories. Figure 4\ndemonstrates our texture transferring results on 4 categories\nof objects. Our method can synthesize geometry-aware and\nseamless textures that has similar patterns and styles as the\ninput. We also demonstrate that our method can transfer\ntextures across different categories. In Figure 1, we show\ntexture transfer results from images of sofa to bed shapes,\nand vice versa. Our method is also capable of performing\ntexture transfers across a broader range of different cate-\ngories. As shown in Figure 5, high-quality and realistic tex-\ntures can be synthesized across chair, mug, and plush toy\ncategories. Since our synthesized texture contains albedo,\nmetallic, and roughness maps, the target objects with the\nsynthesized appearance can be relit, as shown in Figure 6.\nBy using different random seeds, our framework can gener-\nate diverse textures, as shown in Figure 8.\nIn Figure 7, we qualitatively compare our method with\nbaseline methods. Two views are shown in each example.\nLatent-Paint tends to generate textures with colors and pat-\nterns that are different from input images. TEXTure can\npreserve the color and texture better than Latent-Paint, but\nthe texture contains visible seams (possibly due to the itera-\ntive painting). Our results method can reason the semantics\nof the geometry (e.g. the positions of eyes) and demonstrate\nhigher quality, seamless, and geometry-aware texturing re-\nsults with higher fidelity from the input images.\nQuantitative evaluation It is non-trivial to perform quanti-\ntative comparisons for texture transfer due to the shape dif-\nference between geometry and photos. We perform a user\nstudy to evaluate transfer fidelity, texture photorealism, and\ntexture-geometry compatibility across baselines by asking\nusers the following questions: 1) Which one has the texture\nthat looks more similar to input images? 2) Which one has\na texture which looks more like a real object? 3) Which one\nhas the texture which is more compatible with the meshes?\n(Which texture painted more fitted to the geometry?) We\nconduct a user study with Amazon Turk with three sepa-\nrate tasks. For each task, we ask each user 24 questions.\nEach question is a forced single-choice selection with two\noptions among our and one baseline result with the rendered\nimages from the same 4 sampled views and is evaluated by\n20 different users. We only show input photos for the first\nsimilarity question, and hide the input photos for the other\ntwo questions to make the user focus on texture quality. We\nsummarize the results in Table 1. Our results show signifi-\n6\nInput Images\nOur Transfer Results on Diverse Objects\nMugs\nBed\nSofa\nPlush Toy\nFigure 4. Image-guided transfer results from four categories (beds, sofas, plush toys, and mugs) of image sets to diverse objects. Our\nmethod can be applied to a wide range of object types and transfer the textures to diverse object shapes.\ncant preference by the users in terms of image fidelity, tex-\nture photorealism, and shape-texture consistency.\nWe also propose to evaluate the similarity via image-\nbased CLIP feature [41] between reference and the rendered\n7\nFigure 5. Example of cross-category texture transfer results. In the first row, we transfer appearances from plush toys to cups and chairs.\nIn the second row, special patterns from mugs are transferred to bears and chairs. In the thrid row, textures from input sofa are transferred\nto cups and bears.\nHDR Lighting\nRelighting Results\nFigure 6. Example of relighting results. The textures are relit\nby the original HDR environment maps (first row) and the novel\nmaps (second and third rows).\nimages of synthesized textures. The CLIP similarity has\nbeen applied to material matching [65] and stylization [38].\nA good transfer should transfer only the texture from im-\nages and should take into account the target shape geome-\ntry and transfer the texture semantically. For example, the\ntransfer should be painted with respect to each part of the\nshape. We use our evaluation set to compute the compari-\nson. For each image set and target 3D mesh pair, we com-\npute the average of the metric among each reference image\nand each of rendered image from 4 sampled views (i.e. left\nfront, right front, left back, and right back). We average the\nCLIP similarity across all (image set, mesh) pairs. Table 2\nshows our method has the highest CLIP similarity.\n4.4. Ablation Studies\nWe first qualitatively perform an ablation study on the im-\nportance of geometry-aware ControlNet. As shown in Fig-\nure 10, the results suffer from geometry-texture misalign-\nment without ControlNet or the depth-based ControlNet.\nOnly normal-based ControlNet can accurately control the\nsynthesized texture to be consistent with the input mesh ge-\nometry. Next, we validate the importance of score distil-\nlation loss. Only using SDS loss in our framework cannot\nachieve enough input fidelity and the result tends to be more\nblurry. Without LoRA removed (which is usually optimized\nwith vanilla VSD loss), the optimization tends to make the\ndistribution diverge from the Dreambooth-finetuned distri-\nbution. This results in the output containing less original\ntexture but more irrelevant patterns from the input. We hy-\npothesize that this is due to optimizing LoRA weights with\na text condition containing a rare identifier tends to drive the\ndistribution of rendered images to have a rare appearance.\nIf we replace generic diffusion model \u03f5\u03d5 with the person-\nalized diffusion model \u03f5\u03c8 or apply classifier free guidance\nweight 7.5, the result tends to introduce random patterns\nwhich does not exist in the input images. If we choose to\nfreeze the camera encoder weights \u03c1, the result becomes\nworse or more noisy than our full method.\nWe also quantitatively evaluate the importance of each\ncomponent in our system. We use image-based CLIP fea-\nture to measure the similarity between reference images\n8\nTEXTure\nOurs\nLatent-Paint\nInput Images\nFigure 7. Comparison between baseline methods. Compared with Latent-Paint [37] and TEXTure [53], our method can synthesize\nseamless and geometry-aware textures which are compatible with the target mesh geometry.\nInput Images\nTrials with Different Seeds\nFigure 8. Diversity of synthesized textures.\nInput Images\nTransferred Result\nFigure 9. Limitations. Our method may bake-in lighting into tex-\nture, have Janus problem when lacking enough input viewpoints,\nand ignore special and non-repeated patterns from the input.\nand the rendered images.\nTo ensure fair evaluation, the\nbackground of both reference and rendered images are\nmasked with white color.\nAs shown in Table 3, our full\nmethod achieves the highest similarity score among the ab-\nTable 3. Ablation study on image-based texturing w.r.t. CLIP\nimage-based feature similarity. Although w/o ControlNet and w/\nControlNet (Depth) achieve higher similarity score, the transfer re-\nsults tend to ignore target shape and directly paint the texture with-\nout reasoning the geometry. Among the remaining ablative meth-\nods, our full method achieves the highest CLIP similarity w.r.t.\nreference images.\nCLIP similarity \u2191\nw/o ControlNet\n0.8394\nw/ ControlNet (Depth)\n0.8320\nSDS, w/o CFG\n0.8101\nSDS, CFG 100\n0.7983\nw/o LoRA removed\n0.8110\nPersonalized model as \u03d5\n0.8218\nCFG weight as 7.5\n0.8218\nw/o camera encoder \u03c1 updated\n0.8267\nOurs\n0.8296\nlative baselines except w/o ControlNet and w/ ControlNet\n(Depth). As shown in Figure 10, these two methods tend to\nignore the target shape and directly paint the texture without\nadapting to geometry. Thus, they could reach higher score\nby painting the original texture regardless of the shape. We\nalso observe that SDS results tend to be saturated or blurry\nand cannot recover the texture from the inputs. Keeping\nLoRA in the generic diffusion model \u03f5\u03d5 will introduce ran-\ndom patterns to the synthesized texture.\n9\nw/ ControlNet \n(Depth)\nw/o ControlNet\nMesh\nGeometry\nw/o LoRA removed\nSDS\nOurs\nInput\nImages\nOurs\nInput\nImages\nPersonalized model \nas \ud835\udf19\nCFG 7.5\nw/o \ncamera encoder \ud835\udf0c\nupdated\nFigure 10. Ablation study. (First row) With ControlNet conditioned on normal maps, the result has the best texture-geometry consistency.\nWithout ControlNet or with depth-based ControlNet, the results suffer from texture-geometry misalignment. Using SDS loss leads to\nblurry textures. Without the LoRA module removed, the results tend to remove the existing texture from the personalized diffusion model.\nOur full method can synthesize accurate texture which is similar to input appearances. (Second row) If replacing generic diffusion model\n\u03d5 with personalized model or applying classifier guidance scale 7.5, some random patterns might appear in the synthesized texture. If we\nfreeze the camera encoder \u03c1, the result might be worse or more noisy than our full method.\n5. Discussions\nWe proposed a framework to transfer texture from input im-\nages to an arbitrary shape. While our method can transfer\nhigh-quality texture in most cases, there are some limita-\ntions. Figure 9 shows that our method may not be able\nto transfer special and non-repeated texture to the target\nshapes. In addition, our method tends to bake in lighting\nto texture when there are strong specular highlights in the\ninput images. Janus problem might appear when the view-\npoints of input images do not cover the entire object. Nev-\nertheless, we believe that our method can be the first step to\ntackling this challenging problem and will make an impact\nin the 3D content creation community.\n10\nReferences\n[1] Adobe substance 3d. https://docs.substance3d.\ncom/sat. 2\n[2] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil\nKim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d\nhuman digitization with shape-guided diffusion.\nIn SIG-\nGRAPH Asia, 2023. 3\n[3] Sai Bi, Nima Khademi Kalantari, and Ravi Ramamoorthi.\nPatch-based optimization for image-based texture mapping.\nACM Trans. Graph., 36(4):106\u20131, 2017. 2, 3\n[4] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai.\nMesh2tex: Generating mesh textures from image queries.\narXiv preprint arXiv:2304.05868, 2023. 2, 3\n[5] G. Cai, K. Yan, Z. Dong, I. Gkioulekas, and S. Zhao.\nPhysics-based inverse rendering using combined implicit\nand explicit geometries. Computer Graphics Forum, 41(4):\n129\u2013138, 2022. 3\n[6] Tianshi Cao, Karsten Kreis, Sanja Fidler, Nicholas Sharp,\nand Kangxue Yin. Texfusion: Synthesizing 3d textures with\ntext-guided image diffusion models. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 4169\u20134181, 2023. 3\n[7] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano,\nBoxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J\nGuibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient\ngeometry-aware 3d generative adversarial networks. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 16123\u201316133, 2022. 3\n[8] Angel X Chang, Thomas Funkhouser, Leonidas Guibas,\nPat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese,\nManolis Savva, Shuran Song, Hao Su, et al.\nShapenet:\nAn information-rich 3d model repository.\narXiv preprint\narXiv:1512.03012, 2015. 2\n[9] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey\nTulyakov, and Matthias Nie\u00dfner.\nText2tex:\nText-driven\ntexture synthesis via diffusion models.\narXiv preprint\narXiv:2303.11396, 2023. 3\n[10] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 3, 5, 6\n[11] Zhiqin Chen, Kangxue Yin, and Sanja Fidler.\nAuv-net:\nLearning aligned uv maps for texture transfer and synthesis.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 1465\u20131474, 2022. 3\n[12] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin\nChen, and Silvio Savarese.\n3d-r2n2: A unified approach\nfor single and multi-view 3d object reconstruction. In Com-\nputer Vision\u2013ECCV 2016: 14th European Conference, Am-\nsterdam, The Netherlands, October 11-14, 2016, Proceed-\nings, Part VIII 14, pages 628\u2013644. Springer, 2016. 2\n[13] Alexei A Efros and Thomas K Leung.\nTexture synthesis\nby non-parametric sampling. In Proceedings of the seventh\nIEEE international conference on computer vision, pages\n1033\u20131038. IEEE, 1999. 2\n[14] Ziya Erkoc\u00b8, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner,\nand Angela Dai.\nHyperdiffusion:\nGenerating implicit\nneural fields with weight-space diffusion.\narXiv preprint\narXiv:2303.17015, 2023. 3\n[15] Huan Fu, Rongfei Jia, Lin Gao, Mingming Gong, Binqiang\nZhao, Steve Maybank, and Dacheng Tao. 3d-future: 3d fur-\nniture shape with texture. International Journal of Computer\nVision, 129:3313\u20133337, 2021. 6\n[16] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or.\nAn image is worth one word: Personalizing text-to-image\ngeneration using textual inversion. In The Eleventh Interna-\ntional Conference on Learning Representations, 2022. 2, 5,\n6\n[17] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen,\nKangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja\nFidler. Get3d: A generative model of high quality 3d tex-\ntured shapes learned from images. Advances In Neural In-\nformation Processing Systems, 35:31841\u201331854, 2022. 3\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 2\n[19] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian\nLaforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zi-\nXin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang.\nthreestudio: A unified framework for 3d content generation.\nhttps://github.com/threestudio-project/\nthreestudio, 2023. 6\n[20] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.\nShape, light, and material decomposition from images using\nmonte carlo rendering and denoising. Advances in Neural\nInformation Processing Systems, 35:22856\u201322869, 2022. 4\n[21] Paul Henderson, Vagia Tsiminaki, and Christoph H Lampert.\nLeveraging 2d data to learn textured 3d mesh generation. In\nProceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 7498\u20137507, 2020. 2, 3\n[22] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 4, 5\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 3\n[24] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 4\n[25] Brian Karis and Epic Games. Real shading in unreal engine\n4. Proc. Physically Based Shading Theory Practice, 4(3):1,\n2013. 4\n[26] Animesh Karnewar, Andrea Vedaldi, David Novotny, and\nNiloy J Mitra. Holodiffusion: Training a 3d diffusion model\nusing 2d images.\nIn Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n18423\u201318433, 2023. 3\n[27] Oren Katzir, Or Patashnik, Daniel Cohen-Or, and Dani\nLischinski.\nNoise-free score distillation.\narXiv preprint\narXiv:2310.17590, 2023. 3\n[28] Johannes Kopf, Chi-Wing Fu, Daniel Cohen-Or, Oliver\nDeussen, Dani Lischinski, and Tien-Tsin Wong. Solid tex-\n11\nture synthesis from 2d exemplars. In ACM SIGGRAPH 2007\npapers, pages 2\u2013es. 2007. 2\n[29] Vivek Kwatra, Arno Sch\u00a8odl, Irfan Essa, Greg Turk, and\nAaron Bobick. Graphcut textures: Image and video synthe-\nsis using graph cuts. Acm transactions on graphics (tog), 22\n(3):277\u2013286, 2003. 2\n[30] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol,\nJaakko Lehtinen, and Timo Aila.\nModular primitives for\nhigh-performance differentiable rendering. ACM Transac-\ntions on Graphics, 39(6), 2020. 5\n[31] Jiabao Lei, Yabin Zhang, Kui Jia, et al. Tango: Text-driven\nphotorealistic and robust 3d stylization via lighting decom-\nposition. Advances in Neural Information Processing Sys-\ntems, 35:30923\u201330936, 2022. 3\n[32] Marc\nLevoy,\nKari\nPulli,\nBrian\nCurless,\nSzymon\nRusinkiewicz, David Koller, Lucas Pereira, Matt Ginz-\nton, Sean Anderson, James Davis, Jeremy Ginsberg, et al.\nThe digital michelangelo project:\n3d scanning of large\nstatues.\nIn Proceedings of the 27th annual conference\non Computer graphics and interactive techniques, pages\n131\u2013144, 2000. 2, 3\n[33] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.\nBlip: Bootstrapping language-image pre-training for unified\nvision-language understanding and generation, 2022. 2\n[34] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 300\u2013309, 2023. 2, 3\n[35] Shitong Luo and Wei Hu. Diffusion probabilistic models for\n3d point cloud generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 2837\u20132845, 2021. 3\n[36] Yiwei Ma, Xiaoqing Zhang, Xiaoshuai Sun, Jiayi Ji, Haowei\nWang, Guannan Jiang, Weilin Zhuang, and Rongrong Ji.\nX-mesh: Towards fast and accurate text-driven 3d styliza-\ntion via dynamic textual guidance.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 2749\u20132760, 2023. 3\n[37] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and\nDaniel Cohen-Or. Latent-nerf for shape-guided generation\nof 3d shapes and textures. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 12663\u201312673, 2023. 2, 3, 6, 9\n[38] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. arXiv preprint arXiv:2112.03221, 2021. 8\n[39] Oscar Michel, Roi Bar-On, Richard Liu, Sagie Benaim, and\nRana Hanocka. Text2mesh: Text-driven neural stylization\nfor meshes. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 13492\u2013\n13502, 2022. 3\n[40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n2\n[41] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,\nand Tiberiu Popa. Clip-mesh: Generating textured meshes\nfrom text using pretrained image-text models. In SIGGRAPH\nAsia 2022 conference papers, pages 1\u20138, 2022. 3, 7\n[42] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Transactions on Graphics\n(ToG), 41(4):1\u201315, 2022. 2, 4, 6\n[43] Jacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao,\nWenzheng Chen, Alex Evans, Thomas M\u00a8uller, and Sanja Fi-\ndler. Extracting triangular 3d models, materials, and lighting\nfrom images. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 8280\u2013\n8290, 2022. 4\n[44] Jeong Joon Park, Peter Florence, Julian Straub, Richard\nNewcombe, and Steven Lovegrove. Deepsdf: Learning con-\ntinuous signed distance functions for shape representation.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 165\u2013174, 2019. 2\n[45] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. Ad-\nvances in neural information processing systems, 32, 2019.\n6\n[46] Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aurelien\nLucchi. Learning generative models of textured 3d meshes\nfrom real-world images. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 13879\u2013\n13889, 2021. 2, 3\n[47] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2, 3, 6\n[48] Guocheng Qian, Jinjie Mai, Abdullah Hamdi, Jian Ren,\nAliaksandr Siarohin, Bing Li, Hsin-Ying Lee, Ivan Sko-\nrokhodov, Peter Wonka, Sergey Tulyakov, et al. Magic123:\nOne image to high-quality 3d object generation using both\n2d and 3d diffusion priors. arXiv preprint arXiv:2306.17843,\n2023. 3\n[49] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood De-\nhghan, Osmar Zaiane, and Martin Jagersand. U2-net: Going\ndeeper with nested u-structure for salient object detection.\npage 107404, 2020. 5\n[50] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3\n[51] Amit Raj, Srinivas Kaza, Ben Poole, Michael Niemeyer,\nNataniel Ruiz, Ben Mildenhall, Shiran Zada, Kfir Aber-\nman, Michael Rubinstein, Jonathan Barron, et al. Dream-\nbooth3d:\nSubject-driven text-to-3d generation.\narXiv\npreprint arXiv:2303.13508, 2023. 2, 3, 5\n[52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2, 3\n12\n[53] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes,\nand Daniel Cohen-Or. Texture: Text-guided texturing of 3d\nshapes. In ACM SIGGRAPH 2023 Conference Proceedings,\nNew York, NY, USA, 2023. Association for Computing Ma-\nchinery. 2, 3, 6, 9\n[54] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 2, 3, 4, 5, 6\n[55] Prafull Sharma, Varun Jampani, Yuanzhen Li, Xuhui Jia,\nDmitry Lagun, Fredo Durand, William T Freeman, and Mark\nMatthews. Alchemist: Parametric control of material proper-\nties with diffusion models. arXiv preprint arXiv:2312.02970,\n2023. 5\n[56] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu,\nChao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao\nSu. Zero123++: a single image to consistent multi-view dif-\nfusion base model, 2023. 5\n[57] Yichun Shi, Peng Wang, Jianglong Ye, Mai Long, Kejie Li,\nand Xiao Yang. Mvdream: Multi-view diffusion for 3d gen-\neration. arXiv preprint arXiv:2308.16512, 2023. 3\n[58] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan,\nMatthias Nie\u00dfner, and Angela Dai. Texturify: Generating\ntextures on 3d shape surfaces. In European Conference on\nComputer Vision, pages 72\u201388. Springer, 2022. 2, 3\n[59] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International confer-\nence on machine learning, pages 2256\u20132265. PMLR, 2015.\n2, 3\n[60] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2, 3\n[61] Cheng Sun, Guangyan Cai, Zhengqin Li, Kai Yan, Cheng\nZhang, Carl Marshall, Jia-Bin Huang, Shuang Zhao, and\nZhao Dong. Neural-pbir reconstruction of shape, material,\nand illumination. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), 2023. 3\n[62] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,\nand Greg Shakhnarovich. Score jacobian chaining: Lifting\npretrained 2d diffusion models for 3d generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 12619\u201312629, 2023. 2, 3\n[63] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongx-\nuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity\nand diverse text-to-3d generation with variational score dis-\ntillation. arXiv preprint arXiv:2305.16213, 2023. 2, 3, 4,\n6\n[64] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and\nJosh Tenenbaum. Learning a probabilistic latent space of\nobject shapes via 3d generative-adversarial modeling. Ad-\nvances in neural information processing systems, 29, 2016.\n2\n[65] K. Yan, F. Luan, M. Ha\u02c7san, T. Groueix, V. Deschaintre, and\nS. Zhao. Psdr-room: Single photo to scene using differen-\ntiable rendering. In ACM SIGGRAPH Asia 2023 Conference\nProceedings, 2023. 8\n[66] Xin Yu, Peng Dai, Wenbo Li, Lan Ma, Zhengzhe Liu, and\nXiaojuan Qi. Texture generation on 3d meshes with point-\nuv diffusion. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 4206\u20134216, 2023. 3\n[67] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 2, 3\n[68] Qian-Yi Zhou and Vladlen Koltun. Color map optimization\nfor 3d reconstruction with consumer depth cameras. ACM\nTransactions on Graphics (ToG), 33(4):1\u201310, 2014. 2, 3\n13\n"
  },
  {
    "title": "Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis",
    "link": "https://arxiv.org/pdf/2401.09048.pdf",
    "upvote": "7",
    "text": "Published as a conference paper at ICLR 2024\nCOMPOSE\nAND CONQUER:\nDIFFUSION-BASED 3D\nDEPTH AWARE COMPOSABLE IMAGE SYNTHESIS\nJonghyun Lee1,2\u2217, Hansam Cho1,2, Youngjoon Yoo2, Seoung Bum Kim1, Yonghyun Jeong2\u2020\n1Korea University, 2NAVER Cloud\n{tomtom1103, chosam95, sbkim1}@korea.ac.kr\n{youngjoon.yoo,yonghyun.jeong}@navercorp.com\n(a) Foreground/Background object ordering. Samples are generated via two depth maps (top left, bottom left)\nof foreground and background objects, with each object faithfully occluding each other.\n(b) Localized global semantic injection. Given two exemplar images (top left, bottom left),\nsemantics of each image are injected onto a localized area in a disentangled manner.\n(c) Composition with Foreground/Background depth maps and global semantics. By mix & matching\ndepth maps and exemplar images, CnC generates samples with localized local & global conditions.\nFigure 1: Compose and Conquer is able to localize both local and global conditions in a 3D depth\naware manner. For details on the figure, see Section 1.\nABSTRACT\nAddressing the limitations of text as a source of accurate layout representation\nin text-conditional diffusion models, many works incorporate additional signals\nto condition certain attributes within a generated image. Although successful,\nprevious works do not account for the specific localization of said attributes ex-\ntended into the three dimensional plane. In this context, we present a conditional\ndiffusion model that integrates control over three-dimensional object placement\nwith disentangled representations of global stylistic semantics from multiple ex-\nemplar images. Specifically, we first introduce depth disentanglement training to\nleverage the relative depth of objects as an estimator, allowing the model to iden-\ntify the absolute positions of unseen objects through the use of synthetic image\ntriplets. We also introduce soft guidance, a method for imposing global semantics\nonto targeted regions without the use of any additional localization cues. Our in-\ntegrated framework, COMPOSE AND CONQUER (CNC), unifies these techniques\nto localize multiple conditions in a disentangled manner. We demonstrate that our\napproach allows perception of objects at varying depths while offering a versatile\nframework for composing localized objects with different global semantics.\n*First Author. Work done during an internship at NAVER Cloud.\n\u2020Corresponding Author.\n1\narXiv:2401.09048v1  [cs.CV]  17 Jan 2024\nPublished as a conference paper at ICLR 2024\n1\nINTRODUCTION\nFollowing the recent progress in text-conditional diffusion models (Rombach et al., 2022; Ramesh\net al., 2022; Saharia et al., 2022; Nichol et al., 2021), many subsequent studies have emerged to\naddress their inherent limitation in accurately representing the global layout of generated images.\nThese follow-up works enrich the text-based conditioning capabilities of diffusion models by in-\ncorporating additional conditions such as segmentation maps (Zeng et al., 2023; Goel et al., 2023),\ndepth maps (Zhang & Agrawala, 2023; Mou et al., 2023), bounding boxes (Li et al., 2023), and in-\npainting masks (Yang et al., 2023). These modifications effectively retain the extensive knowledge\nencapsulated in the pretrained priors.\nDespite these advancements, two primary challenges persist in the current literature. Firstly, while\nexisting models are efficient in generating an object under locally constrained conditions like depth\nmaps and bounding boxes, which inherently capture structural attributes, they confine the genera-\ntive space to a two-dimensional plane. This limitation makes them less adept at handling object\nplacement within a three-dimensional (3D) or z-axis (depth) perspective, and hence vulnerable to\ngenerating images without properly reflecting the depth-aware placement of multiple objects. Sec-\nondly, the issue of applying global conditions, such as style and semantics, from multiple image\nsources to specific regions of the target image in a controlled manner has yet to be resolved.\nTo address the existing limitations on local and global conditions and also enhance the capabilities\nof image generation models, we introduce COMPOSE AND CONQUER (CnC). Our proposed CnC\nconsists of two building blocks: a local fuser and global fuser designed to tackle each problem.\nFirst, the local fuser operates with a new training paradigm called depth disentanglement training\n(DDT) to let our model understand how multiple objects should be placed in relation to each other\nin a 3D space. DDT distills information about the relative placement of salient objects to the local\nfuser by extracting depth maps from synthetic image triplets, originally introduced in the field of\nimage composition. Second, the global fuser employs a method termed soft guidance, which aids\nour model in localizing global conditions without any explicit structural signals. Soft guidance\nselectively masks out regions of the similarity matrix of cross-attention layers that attend to the\nspecific regions of each salient object.\nFigure 1 demonstrates the main capabilities of our model trained on DDT and soft guidance. In\nFigure 1(a), DDT lets our model infer relative depth associations of multiple objects within one\nimage, and generates objects that are placed in different depths of the z-axis with foreground objects\neffectively occluding other objects. In Figure 1(b), we show that by applying soft guidance, our\nmodel can localize global semantics in a disentangled manner. By utilizing the local and global\nfuser simultaneously as demonstrated in Figure 1(c), our model gives users the ability to compose\nmultiple localized objects with different global semantics injected into each localized area, providing\na vast degree of creative freedom.\nWe quantitatively evaluate our model against other baseline models and gauge the fidelity of sam-\nples and robustness to multiple input conditions, and demonstrate that our model substantially out-\nperforms other models on various metrics. We also evaluate our model in terms of reconstruction\nability, and the ability to order objects into different relative depths. We shed light onto the use\nof DDT, where we demonstrate that DDT dissipates the need to provide additional viewpoints of a\nscene to infer the relative depth placement of objects. Furthermore, we show that soft guidance not\nonly enables our model to inject global semantics onto localized areas, but also prevents different\nsemantics from bleeding into other regions.\nOur contributions are summarized as follows:\n\u2022 We propose depth disentanglement training (DDT), a new training paradigm that facilitates\na model\u2019s understanding of the 3D relative positioning of multiple objects.\n\u2022 We introduce soft guidance, a technique that allows for the localization of global condi-\ntions without requiring explicit structural cues, thereby providing a unique mechanism for\nimposing global semantics onto specific image regions.\n\u2022 By combining these two propositions, we present COMPOSE AND CONQUER (CnC), a\nframework that augments text-conditional diffusion models with enhanced control over\nthree-dimensional object placement and injection of global semantics onto localized re-\ngions.\n2\nPublished as a conference paper at ICLR 2024\n2\nRELATED WORK\nConditional Diffusion Models.\nDiffusion models (DMs) (Sohl-Dickstein et al., 2015; Ho et al.,\n2020) are generative latent variable models that are trained to reverse a forward process that grad-\nually transforms a target data distribution into a known prior. Proving highly effective in its ability\nto generate samples in an unconditional manner, many following works (Dhariwal & Nichol, 2021;\nHo et al., 2022; Nichol et al., 2021; Rombach et al., 2022; Ramesh et al., 2022) formulate the diffu-\nsion process to take in a specific condition to generate corresponding images. Out of said models,\nRombach et al. (2022) proposes LDM, a latent text-conditional DM that utilizes an autoencoder, ef-\nfectively reducing the computational complexity of generation while achieving high-fidelity results.\nLDMs, more commonly known as Stable Diffusion, is one of the most potent diffusion models open\nto the research community. LDMs utilize a twofold approach, where an encoder maps x to its latent\nrepresentation z, and proceeds denoising z in a much lower, memory-friendly dimension. Once\nfully denoised, a decoder maps z to the original image dimension, effectively generating a sample.\nBeyond Text Conditions.\nWhile text-conditional DMs enable creatives to use free-form prompts,\ntext as the sole condition has limitations. Namely, text-conditional DMs struggles with localiz-\ning objects and certain semantic concepts with text alone, because text prompts of large web-scale\ndatasets used to train said models (Schuhmann et al., 2021) do not provide explicit localized de-\nscriptions and/or semantic information. Addressing this limitation, many works have introduced\nmethods to incorporate additional conditional signals to the models while preserving its powerful\nprior, e.g. freezing the model while training an additional module. Among these models, ControlNet\n(Zhang & Agrawala, 2023) and T2I-Adapter (Mou et al., 2023) train additional modules that incor-\nporate modalities such as depth maps and canny edge images to aid generation of localized objects.\nHowever, these models only support a single condition, lacking the ability to condition multiple\nsignals or objects. Taking inspiration from ControlNet, Uni-ControlNet (Zhao et al., 2023) extends\nits framework to accept multiple local conditions and a single global condition at once. Whereas the\nworks detailed above all leverage Stable Diffusion as a source of their priors, Composer (Huang\net al., 2023) operates in the pixel-space. Although being able to process multiple conditions at once,\nboth Composer and Uni-ControlNet struggle in processing incompatible conditions, or conditions\nthat overlap with each other. They also do not provide methods to localize global semantics onto\na localized region. In contrast, our approach directly addresses these challenges by proposing two\nnovel methods, depth disentanglement training and soft guidance, which enables the composition of\nmultiple local/global conditions onto localized regions.\n3\nMETHODOLOGY: COMPOSE AND CONQUER\nThe architecture illustrated in Figure 2 shows the overall framework of our proposed method. CnC\nconsists of a local fuser, a global fuser, and components of a pretrained text-conditional DM. Our\nlocal fuser captures the relative z-axis placements of images through depth maps, and our global\nfuser imposes global semantics from CLIP image embeddings (Radford et al., 2021) on specified\nregions. Explanations of the local and global fuser are detailed below.\n3.1\nGENERATIVE PRIOR UTILIZATION\nIn line with earlier studies that incorporate additional condition signals (Li et al., 2023; Mou et al.,\n2023; Zhang & Agrawala, 2023; Zhao et al., 2023), we use the LDM variant known as Stable\nDiffusion (SD) as our source of prior. Specifically, SD utilizes a UNet (Ronneberger et al., 2015) like\nstructure, where noisy latent features consecutively pass through 12 spatial downsampling blocks,\none center block C, and 12 spatial upsampling blocks. Each block consists of either a ResNet (He\net al., 2016) block or Transformer (Vaswani et al., 2017) block, and for brevity, we respectively refer\nto each group of the 12 blocks as the encoder E and decoder D. Inspired by ControlNet Zhang &\nAgrawala (2023) and Uni-ControlNet Zhao et al. (2023), the Stable Diffusion architecture is utilized\ntwofold in our model, where we first freeze the full model and clone a trainable copy of the encoder\nand center block, denoted E\u2032 and C\u2032. We initialize the weights of the full model from SD, and the\nweights of E\u2032 and C\u2032 from Uni-ControlNet. The cloned encoder E\u2032 and center block C\u2032 receives\nlocalized signals from our local fuser, which acts as the starting point of our model. We detail our\nmodel architecture, methodologies of the two building blocks and corresponding training paradigms\nin the section below.\n3\nPublished as a conference paper at ICLR 2024\n[\n[\nFg. \nStream\nLocal Fuser\nGlobal Fuser\nE\u2032\nC\u2032\nDepth\nMaps\nCLIP\nImage\nEmbeddings\nM\nzt\nExtracted Features\nExtraction\nLayer\nBg. \nStream\nFg. \nStream\nBg. \nStream\nConvolution\nSelf-Attention\nCross-Attention\n\u2026\nyfg\nybg\nytext\n= yfull\nconcat(\n)\nS\u2032 = (QKT /\n\u221a\nd) \u2297 M \u2032\n1\n0\n0\n0\n1\n1\n1\n\u2026\nQ\nK\n\u03d5(M)1 \u2212 \u03d5(M)\nJ\n\u23a1\n\u23a2\u23a3\n1\n. . .\n1\n...\n...\n...\n1\n. . .\n1\n\u23a4\n\u23a5\u23a6\nFigure 2: Model Architecture. Our model consists of a local fuser, a global fuser, and the cloned\nencoder/center block {E\u2032, C\u2032}. The input depth maps are fed into the local fuser, producing four\nlatent representations of different spatial resolutions, which are incorporated into E\u2032. The CLIP\nimage embeddings are fed into the global fuser, producing 2 extra tokens to be concatenated with the\ntext token embeddings. Masks M are flattened and repeated to produce M \u2032 = concat(J, \u03c6(M), 1\u2212\n\u03c6(M)), which serves as a source of soft guidance of the cross-attention layers.\n3.2\nLOCAL FUSER\nWe first provide details on our local fuser, which incorporates depth maps extracted form a pretrained\nmonocular depth estimation network (Ranftl et al., 2020) as our local condition. Specifically, our\nlocal fuser serves as a source of localized signals that are incorporated into the frozen SD blocks.\nWe also provide details on depth disentanglement training, and how synthetic image triplets are\nleveraged as sources of relative depth placements of objects.\nSynthetic Image Triplets.\nFor a model to be able to represent overlapping objects with varied\nscopes of depth during inference, the model needs to learn to recognize different elements obscured\nby objects during training. Although straightforward in a 3D world, informing a model about objects\nthat are occluded by another in a 2D image is non-trivial, due to the fact that once the image is\ncaptured, any spatial information about objects behind it is forever lost. To overcome this limitation,\nwe first adopt a process utilized in image composition (Fang et al., 2019) to generate synthetic image\ntriplets, which serves as training samples for our depth disentanglement training (DDT), detailed in\nthe next section. The synthetic image triplets {If, Ib, M} are derived from a single source image\nIs \u2208 RH\u00d7W \u00d73, and is composed of the foreground image If \u2208 RH\u00d7W \u00d73, the background image\nIb \u2208 RH\u00d7W \u00d73, and a binary foreground object mask M \u2208 {0, 1}H\u00d7W . The foreground image\nIf is derived using the Hadamard product of If = Is \u2297 M, leaving just the salient object of Is.\nTo generate Ib, we utilize Stable Diffusion\u2019s inpainting module (Rombach et al., 2022). This is\nachieved by inpainting the result of Is \u2297 (1 \u2212 \u02dc\nM), where \u02dc\nM is the binary dilated M. Conceptually,\nthis process can be thought of as inpainting the depiction of Is without its salient object, effectively\nletting our model see behind it. For details on this choice, see appendix A.2.\nDepth Disentanglement Training.\nOnce the synthetic image triplets {If, Ib, M} are prepared,\nwe proceed to extract the depth maps of If and Ib to train our local fuser, which we refer to as\ndepth disentanglement training (DDT). Our local fuser incorporates these two depth maps, which\npasses through its own individual stream consisting of ResNet blocks, and are concatenated along\nits channel dimension. Apart from previous works that directly fuse different local conditions before\nentering a network, DDT first process each depth map of If and Ib in their own independent layers.\nReminiscent of early and late fusion methodologies of salient object detection (Zhou et al., 2021),\nwe consider DDT a variant of late fusion, where the network first distinguishes each representation\nin a disentangled manner. Once concatenated, features containing spatial information about objects\n4\nPublished as a conference paper at ICLR 2024\n(a) A beautiful dessert waiting to be shared by two people\n(b) Diamond Minecraft block facing frontwards\non a green background\n(c) Cat knight, portrait, finely detailed armor, intricate\ndesign, silver, silk, cinematic lighting\nFigure 3: Depth disentanglement training. Our model trained on DDT (Left) successfully rec-\nognizes that objects portrayed by the foreground depth map (Top left) should be placed closer that\nthe background depth map (Bottom left), and fully occludes objects that are larger. On the other\nhand, when trained on just the depth maps of Is (Right), our model struggles to disentangle the\ndepth maps, resulting in either objects becoming fused (samples (a), (c)) or completely ignoring the\nforeground object (sample (b)).\nin varying depths are extracted along different resolutions by an extraction layer. These features are\nthen incorporated into the cloned and frozen SD blocks, which we provide additional details in our\nappendix A.2. We formulate DDT to train our model because to represent overlapping objects with\nvaried depths during inference, the model needs to recognize the elements obscured by the salient\nobject. By providing our model with an explicit depth representation of what lies behind the salient\nobject\u2014albeit synthetic\u2014, our model is able to effectively distinguish the relative depths of multiple\nobjects. Figure 3 demonstrates the effect of training our model with DDT compared to training our\nmodel on depth maps of Is, as done in previous works. Even though that our local fuser was trained\non depth maps that convey only the relative depth associations between the foreground salient object\nand background depth maps, we find that our model extends to localizing different salient objects.\n3.3\nGLOBAL FUSER\nWhile our local fuser incorporates depth maps as a source of relative object placements, our global\nfuser leverages soft guidance to localize global semantics onto specific regions. We use image em-\nbeddings derived from the CLIP image encoder (Radford et al., 2021) as our global semantic condi-\ntion. This choice is informed by the training methodology of SD, which is designed to incorporate\ntext embeddings from its contrastively trained counterpart, the CLIP text encoder. The text em-\nbeddings ytext are integrated into the intermediate SD blocks through a cross-attention mechanism.\nIn this setup, the text embeddings serve as the context for keys and values, while the intermediate\nnoised latents act as the queries. Although prior works (Nichol et al., 2021; Ramesh et al., 2022;\nHuang et al., 2023) have merged CLIP image embeddings with text embeddings within the cross-\nattention layers of the DM, this approach lacks the provision of spatial grounding information. As a\nresult, the global semantics are conditioned on the entire generated image, lacking precise localiza-\ntion capabilities. To overcome this limitation, our method leverages the binary foreground mask M\nused to extract If in our synthetic image triplet.\nSoft Guidance.\nIn detail, we first project the image embeddings of Is and Ib using our global\nfuser, which consists of stacked feedforward layers. The global fuser consists of separate fore-\nground/background streams, where each image embedding is projected and reshaped to N global\ntokens each, resulting in yfg and ybg. Unlike our local fuser module, our global fuser doesn\u2019t fuse\neach stream within the feedforward layers. We instead choose to concatenate yfg and ybg directly\nto ytext, where the extended context yfull = concat(ytext, \u03bbfgyfg, \u03bbbgybg) is utilized in the cross-\nattention layers of our cloned and frozen modules. \u03bbfg and \u03bbbg denotes scalar hyperparameters that\ncontrol the weight of each token, which are set to 1 during training. In the cross-attention layers, the\nsimilarity matrix S is given by S = (QKT /\n\u221a\nd) with Q = WQ \u00b7 zt and K = WK \u00b7 yfull, where zt\nis a noised variant of z with the diffusion forward process applied t steps.\nOnce S is calculated, we apply soft guidance by first creating a Boolean matrix M \u2032, which has the\nsame dimensionality of S. Given that S \u2208 Ri\u00d7j, M \u2032 is defined as M \u2032 = concat(J, \u03c6(M), 1 \u2212\n\u03c6(M)), where J \u2208 1i\u00d7j\u22122N denotes an all ones matrix, \u03c6(M) \u2208 Bi\u00d7N denotes the reshaped,\nflattened and repeated boolean mask M, and 1 \u2212 \u03c6(M) denotes the complement of \u03c6(M) of the\nsame shape. By overriding S with the Hadamard product S\u2032 = S \u2297 M \u2032, the attention operation\nsoftmax(S\u2032) \u00b7 V is completed. Intuitively, soft guidance can be thought of as masking out parts of\n5\nPublished as a conference paper at ICLR 2024\nS where zt should not be attending to, e.g. forcing the cross-attention computations of yfg and ybg\nto be performed only on its corresponding flattened values of zt. We find that by letting the tokens\nof ytext attend to the whole latent and restricting the extra tokens, generated samples are able to stay\ntrue to their text conditions while also reflecting the conditioned global semantics in their localized\nareas, even though that spatial information of M is forfeited.\n3.4\nTRAINING\nLDMs are optimized in a noise-prediction manner, utilizing a variation of the reweighted variational\nlower bound on q(x0), first proposed by Ho et al. (2020). We extend this formulation and optimize\nour model with Eq. 1 to learn the conditional distribution of p(z|y), where y denotes the set of\nour conditions of depthmaps and image embeddings accompanied with CLIP text embeddings. As\nmentioned above, we freeze the initial SD model while jointly optimizing the weights of the cloned\nencoder E\u2032, center block C\u2032, and the local/global fuser modules, denoted as \u03b8\u2032.\nmin\n\u03b8\u2032 L = Ez,\u03f5\u223cN (0,I),t\nh\r\r\u03f5 \u2212 \u03f5{\u03b8,\u03b8\u2032} (zt, t, y)\n\r\r2\n2\ni\n(1)\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUP\nDatasets.\nOur synthetic image triplets If, Ib, M are generated from two distinct datasets: COCO-\nStuff (Caesar et al., 2018) and Pick-a-Pic (Kirstain et al., 2023). We refer the readers to Section 4.3\nfor our reasoning behind this choice. The COCO-Stuff dataset, with 164K images, has pixel-wise\nannotations classifying objects into \u201cthings\u201d (well-defined shapes) and \u201cstuff\u201d (background regions).\nWe leverage the fact that objects in the things category can be seen as the salient object, and create\nM by setting each pixel of the indexed mask to 1 if it belongs to either one of the 80 things classes,\nand 0 otherwise. Text prompts are randomly chosen from five available for each image. The Pick-\na-Pic dataset contains 584K synthetic image-text pairs generated by SD and its variants, and was\ncollected as an effort to train a preference scoring model. Each text prompt is paired with two\ngenerated images, and holds a label denoting the preferred image in terms of fidelity and semantic\nalignment to the given prompt. By only keep the preferred image and filtering out inappropriate\ncontent, we end up with 138K image-text pairs. Because Pick-a-Pic doesn\u2019t hold ground truth labels\nfor the salient object unlike COCO-Stuff, we utilize a salient object detection module (Qin et al.,\n2020) to generate M. Combining these two datasets, we generate 302K synthetic image triplets\n{If, Ib, M} through the process detailed in Section 3.2.\nImplementation Details.\nFor depth map representations, we utilize a monocular depth estimation\nnetwork (Ranftl et al., 2020), and the CLIP image encoder (Radford et al., 2021) for global semantic\nconditions. Although formulated as a single model, we empirically find that training the local and\nglobal fuser independently, and finetuning the combined weights lead to faster convergence. During\ntraining, images are resized and center cropped to a resolution of 512 \u00d7 512. We train our local\nfuser with the cloned E\u2032 and C\u2032 for 28 epochs, our global fuser for 24 epochs, and finetune the full\nmodel for 9 epochs, all with a batch size of 32 across 8 NVIDIA V100s. During training, we set\nan independent dropout probability for each condition to ensure that our model learns to generalize\nvarious combinations. For our evaluation, we employ DDIM (Song et al., 2020) sampling with 50\nsteps, and a CFG (Ho & Salimans, 2021) scale of 7 to generate images of 768 \u00d7 768.\n4.2\nEVALUATION\nQualitative Evaluation.\nWe demonstrate the results of our method compared to other baseline\nmodels that incorporate either depth maps as a local condition, CLIP image embeddings as a global\ncondition, or both. The baseline models are listed in Table 1. GLIGEN (Li et al., 2023) and Con-\ntrolNet (Zhang & Agrawala, 2023) are trained to accept a single depth map as a local condition.\nUni-ControlNet (Zhao et al., 2023) and T2I-Adapter (Mou et al., 2023) are trained to accept a depth\nmap and an exemplar image as a source of CLIP image embeddings. In Figure 4, we show quali-\ntative results of our model compared to models that accept both depth maps and exemplar images\nas a source of global semantics. Since our model accepts two depth maps and two exemplar im-\nages, note that we condition the same images for each modality in Figure 4. While other models\n6\nPublished as a conference paper at ICLR 2024\nCnC (Ours)\nUni-\nControlNet\nT2I-\nAdapter\nInput\nDepth\n/Semantics\nPrompt\nGerman home by\nthe lake, trees, boats,\nglass windows\nBioluminescent\nmushrooms\nA green glass bottle\nBeautiful dancing\nunicorns\nAnimal made of\nscrap metal, auto parts,\nand flowers\nCoastal village\nFigure 4: Samples compared to other baseline models. Compared to others, CnC strikes a balance\nbetween the given depth maps, exemplar images, and text prompts.\nModel/Condition\nDepth\nSemantics\nDepth+Semantics\nFID (\u2193)\nIS (\u2191)\nCLIP\nScore (\u2191) FID (\u2193)\nIS (\u2191)\nCLIP\nScore (\u2191) FID (\u2193)\nIS(\u2191)\nCLIP\nScore (\u2191)\nGLIGEN\n18.887\n29.602\n25.815\n-\n-\n-\n-\n-\n-\nControlNet\n17.303\n31.652\n25.741\n-\n-\n-\n-\n-\n-\nUni-ControlNet\n19.277\n31.287\n25.620\n23.632\n28.364\n24.096\n18.945\n28.218\n24.839\nT2I-Adapter\n20.949\n31.485\n26.736\n35.812\n23.254\n23.666\n30.611\n23.938\n24.579\nCnC\n19.804\n27.555\n25.211\n35.178\n21.932\n22.161\n21.318\n25.421\n24.659\nCnC Finetuned\n22.257\n27.981\n26.870\n17.254\n32.131\n25.940\n18.191\n29.304\n25.880\nTable 1: Evaluation metrics on the COCO-Stuff val-set. We omit the results of semantics and\ndepth+semantics on GLIGEN and ControlNet due to the models not supporting these conditions.\nBest results are in bold.\ndo grasp the ideas of each condition, it can be seen that our model exceeds in finding a balance\nbetween the structural information provided by the depth maps and semantic information provided\nby exemplar images and text prompts. Taking the first column as an example, Uni-ControlNet of-\nten fails to incorporate the global semantics, while T2I-Adapter often overconditions the semantics\nfrom the exemplar image, ignoring textual cues such as \u201clake\u201d or \u201cboats\u201d. Our approach adeptly\ninterweaves these aspects, accurately reflecting global semantics while also emphasizing text-driven\ndetails and structural information provided by depth maps. For additional qualitative results, we\nrefer the readers to Figure 5.\nQuantitative Evaluation.\nAs our baseline metrics, we utilize FID (Heusel et al., 2017) and Incep-\ntion Score (Salimans et al., 2016) to evaluate the quality of generated images, and CLIPScore (Hessel\net al., 2021) to evaluate the semantic alignment of the generated images to its text prompts. Table 1\nreports the results evaluated on 5K images of the COCO-Stuff validation set. It can be seen that\nwhile our model with the local and global fuser trained independently and joined during inference\n(CnC, for brevity) shows adequate performance, our finetuned model excels in most metrics, except\nfor FID and IS of our depth-only experiment. This may be attributed to the fact that while baseline\nmodels only take in a single depth map extracted from the validation image, our model takes in an\nadditional depth map with inpainted regions that may not reflect the prior distribution of the dataset.\nThe fact that our model has to process depth maps of conflicting localization information is another\npossible explanation. Though all models report similar CLIPScores, given their shared generative\nprior of SD, our finetuned model excels when generating from just an exemplar image, thanks to the\nintegration of soft guidance. We conclude that our finetuned model achieves substantial performance\n7\nPublished as a conference paper at ICLR 2024\nA statue of a borzoi dog\nA mythical mannequin dancing in the forest\nSailboat, oil painting\nA plate of pasta\nA portrait of a man\nDramatic turtles flying\nA statue of an adorable cat\nA Photo of a raven\nA statue of a dog in a Greco roman hall\nA giant dog in front of a pavilion\nA magician bunny\nFigure 5: Qualitative Results. Foreground/background conditions are on the left of each sample.\nGround Truth\nCnC (Ours)\nUni-ControlNet\nT2I-Adapter\nGround Truth\nCnC (Ours)\nUni-ControlNet\nT2I-Adapter\nCOCO-Stuff\nPick-a-Pic\nFigure 6: Qualitative reconstruction comparison. Samples are generated using conditions ex-\ntracted from validation samples of COCO-Stuff (Left) and Pick-a-Pic (Right).\ndue to its additional finetuning phase, which enables the fusers to better adapt and understand each\nother\u2019s conditioning processes. See Section A.3 for results on the Pick-a-Pic validation set.\nReconstruction.\nWe additionally report quantitative reconstruction metrics evaluated on COCO-\nStuff and Pick-a-Pic validation sets, listed in Table 2. For reconstruction, our model utilizes the\ndepth maps and CLIP image embeddings extracted from the image triplets of ground truth validation\nimages, and baseline models utilize the depth maps and CLIP image embeddings extracted from\nthe ground truth images, since they do not support more than one condition per modality. We\nadopt LPIPS (Zhang et al., 2018) as our metric of perceptual similarity and SSIM (Wang et al.,\n2004) as our metric of structural similarity. We also report the MAE of ground truth depth maps\nand the depth maps extracted from its generated counterpart as an additional measure of structural\nsimilarity, extended to the z-axis. Apart from the SSIM value of COCO-Stuff, we find that our model\noutperforms other models by a large margin. As seen in Figure 6, we find that our model is able to\nfaithfully recreate objects in different localized areas while preserving its depth of field. While other\nbaseline models succeed in localizing objects, they struggle in synthesizing the depth perspective,\nresulting in images looking relatively flat.\nAblation Study.\nThe phenomenon known as concept bleeding (Podell et al., 2023) leads to dif-\nferent semantics to overlap with each other, resulting in unintentional results. Soft guidance allows\n8\nPublished as a conference paper at ICLR 2024\nModel/Condition\nCOCO-Stuff\nPick-a-Pic\nSSIM(\u2191) LPIPS(\u2193) MAE(\u2193) SSIM(\u2191) LPIPS(\u2193) MAE(\u2193)\nUni-ControlNet\n0.2362\n0.6539\n0.1061\n0.2506\n0.6504\n0.1111\nT2I-Adapter\n0.1907\n0.6806\n0.1201\n0.2238\n0.6724\n0.1270\nCnC\n0.2345\n0.6621\n0.1061\n0.2436\n0.6431\n0.1080\nCnC Finetuned\n0.2248\n0.6509\n0.0990\n0.2690\n0.6216\n0.1027\nTable 2: Quantitative reconstruction metrics evaluated on COCO-Stuff and Pick-a-Pic val-sets.\n0.5\n1.0\n2.0\n1.5\n\u03bbbg\n0.0\n\u03bbfg\n1.0\n1.0\n1.0\n1.0\n1.0\nFigure 7: The effect of soft guidance with conflicting semantics. We condition the same depth map\nfor each stream in the local fuser, and generate each sample with the prompt \u201cAn Igloo\u201d. By fixing\n\u03bbfg and increasing \u03bbbg, the effect of the background global semantics substantially increases. Soft\nguidance prevents the two global semantics from bleeding into each other, e.g. concept bleeding,\neffectively maintaining the semantics of the igloo.\nglobal semantics to be conditioned onto localized regions while preventing this undesirable effect.\nFigure 7 demonstrates this capability, where two contradicting semantics are localized. By fixing\n\u03bbfg to 1 and steadily increasing \u03bbbg, the effects of the background semantics are amplified. However,\ndue to soft guidance, even as the background semantics intensify, the contradicting semantic of the\nforeground object stays intact. Even though spatial information of M is lost during soft guidance, we\nfind that it faithfully creates a barrier for any semantics from bleeding in. For additional ablations,\nsee Section A.4.\n4.3\nDISCUSSIONS\nDataset Choices.\nAs mentioned in Section 4.1, the two datasets we employ to train our model are\nvery different from another. Namely, images in COCO-Stuff include everyday scenes while images\nin Pick-a-Pic are fundamentally synthetic, being generated by variants of SD from prompts that\ntranscend any description of real life scenarios. This design choice is intentional: we first point to\nthe fact that most of our baseline models are trained on variants of MS-COCO (Lin et al., 2014).\nThese models show that training only on real images as a method to introduce new conditions are\nadequate, but Kirstain et al. (2023) and Podell et al. (2023) report that COCO zero-shot FID is\nnegatively correlated with human preferences and visual aesthetics of generated images. Although\nimages from COCO and its variants do serve its purpose on introducing new conditions, we argue\nthat leveraging another dataset that aligns with the learnt prior of pretrained DMs provides a safety\nnet from prior drifting. By harnessing the detailed ground truth pixel-wise annotations of COCO-\nStuff and letting our model learn additional representations from its original prior provided by Pick-\na-Pic, we take advantage of the best of both worlds; providing a robust posterior of conditions while\nstaying true to the user-preferred prior of DMs.\n5\nCONCLUSION & LIMITATIONS\nWe presented Compose and Conquer (CnC), a novel text-conditional diffusion model addressing two\nmain challenges in the field: three-dimensional placement of multiple objects and region-specific lo-\ncalization of global semantics from multiple sources. CnC employs two main components: the local\nand global fuser, which respectively leverages the new Depth Disentanglement Training (DDT) and\nsoft guidance techniques. We show that DDT infers the absolute depth placement of objects, and soft\nguidance is able to incorporate semantics on to localized regions. Evaluations on the COCO-stuff\nand Pick-a-Pic datasets illustrates CnC\u2019s proficiency in addressing these challenges, as demonstrated\nthrough extensive experimental results. Since the current framework limits the number of available\nconditions and the disentangled spatial grounds to the foreground and background, we leave the\nfurther decomposition of images into depth portraying primitives and the middle ground to leverage\nfor future work.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nWe thank the ImageVision team of NAVER Cloud for their thoughtful advice and discussions. Train-\ning and experiments were done on the Naver Smart Machine Learning (NSML) platform (Kim et al.,\n2018). This study was supported by BK21 FOUR.\nETHICS STATEMENT\nDiffusion models, as a type of generative model, have the potential to generate synthetic content that\ncould be used in both beneficial and potentially harmful ways. While our work aims to advance the\nunderstanding and capabilities of these models, we acknowledge the importance of their responsi-\nble use. We encourage practitioners to consider the broader societal implications when deploying\nsuch models and to implement safeguards against malicious applications. Specifically, the diffusion\nmodel we utilize as a source of prior in our work is trained on the LAION (Schuhmann et al., 2021)\ndataset, a web-scraped collection. Despite the best intentions of the dataset\u2019s creators to filter out\ninappropriate data, LAION includes content that may be inappropriate for models to internalize,\nsuch as racial stereotypes, violence, and pornography. Recognizing these challenges, we underscore\nthe necessity of rigorous scrutiny in using such models to prevent the perpetuation of harmful biases\nand misinformation.\nREPRODUCIBILITY STATEMENT\nThe source code and pretrained models can be found at https://github.com/tomtom1103/compose-\nand-conquer.\nREFERENCES\nHolger Caesar, Jasper Uijlings, and Vittorio Ferrari. Coco-stuff: Thing and stuff classes in context.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1209\u2013\n1218, 2018.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780\u20138794, 2021.\nHao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost:\nBoosting instance segmentation via probability map guided copy-pasting. In Proceedings of the\nIEEE/CVF international conference on computer vision, pp. 682\u2013691, 2019.\nVidit Goel, Elia Peruzzo, Yifan Jiang, Dejia Xu, Nicu Sebe, Trevor Darrell, Zhangyang Wang, and\nHumphrey Shi. Pair-diffusion: Object-level image editing with structure-and-appearance paired\ndiffusion models. arXiv preprint arXiv:2303.17546, 2023.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770\u2013778, 2016.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\nClipscore: A\nreference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on\nDeep Generative Models and Downstream Applications, 2021.\n10\nPublished as a conference paper at ICLR 2024\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sali-\nmans. Cascaded diffusion models for high fidelity image generation. The Journal of Machine\nLearning Research, 23(1):2249\u20132281, 2022.\nLianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jingren Zhou. Composer: Creative\nand controllable image synthesis with composable conditions. arXiv preprint arXiv:2302.09778,\n2023.\nHanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong Kim, Heungseok Park, Soeun Park, Hyunwoo\nJo, KyungHyun Kim, Youngil Yang, Youngkwan Kim, et al. Nsml: Meet the mlaas platform with\na real-world case study. arXiv preprint arXiv:1810.09957, 2018.\nYuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy.\nPick-a-pic: An open dataset of user preferences for text-to-image generation. arXiv preprint\narXiv:2305.01569, 2023.\nYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li,\nand Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22511\u201322521, 2023.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014,\nProceedings, Part V 13, pp. 740\u2013755. Springer, 2014.\nChong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie.\nT2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion\nmodels. arXiv preprint arXiv:2302.08453, 2023.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\nLi Niu, Wenyan Cong, Liu Liu, Yan Hong, Bo Zhang, Jing Liang, and Liqing Zhang.\nMak-\ning images real again: A comprehensive survey on deep image composition.\narXiv preprint\narXiv:2106.14490, 2021.\nTaesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with\nspatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pp. 2337\u20132346, 2019.\nDustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe\nPenna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\nXuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin\nJagersand. U2-net: Going deeper with nested u-structure for salient object detection. Pattern\nrecognition, 106:107404, 2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.\nRen\u00b4e Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust\nmonocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transac-\ntions on pattern analysis and machine intelligence, 44(3):1623\u20131637, 2020.\n11\nPublished as a conference paper at ICLR 2024\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\nical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013\nMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceed-\nings, Part III 18, pp. 234\u2013241. Springer, 2015.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. Advances in neural information processing systems, 29,\n2016.\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\nAarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\nclip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learn-\ning, pp. 2256\u20132265. PMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Interna-\ntional Conference on Learning Representations, 2020.\nYizhi Song, Zhifei Zhang, Zhe Lin, Scott Cohen, Brian Price, Jianming Zhang, Soo Ye Kim, and\nDaniel Aliaga. Objectstitch: Generative object compositing. arXiv preprint arXiv:2212.00932,\n2022.\nRoman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha,\nAleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky.\nResolution-robust large mask inpainting with fourier convolutions.\nIn Proceedings of the\nIEEE/CVF winter conference on applications of computer vision, pp. 2149\u20132159, 2022.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethink-\ning the inception architecture for computer vision. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 2818\u20132826, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:\nfrom error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013\n612, 2004.\nBinxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and\nFang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18381\u2013\n18391, 2023.\nYu Zeng, Zhe Lin, Jianming Zhang, Qing Liu, John Collomosse, Jason Kuen, and Vishal M Patel.\nScenecomposer: Any-level semantic image synthesis. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pp. 22468\u201322478, 2023.\nLvmin Zhang and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models.\narXiv preprint arXiv:2302.05543, 2023.\n12\nPublished as a conference paper at ICLR 2024\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pp. 586\u2013595, 2018.\nShihao Zhao, Dongdong Chen, Yen-Chun Chen, Jianmin Bao, Shaozhe Hao, Lu Yuan, and Kwan-\nYee K Wong. Uni-controlnet: All-in-one control to text-to-image diffusion models. Advances in\nNeural Information Processing Systems, 2023.\nTao Zhou, Deng-Ping Fan, Ming-Ming Cheng, Jianbing Shen, and Ling Shao. Rgb-d salient object\ndetection: A survey. Computational Visual Media, 7:37\u201369, 2021.\n13\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nA.1\nEXTENDED RELATED WORK\nImage Composition.\nImage composition (Niu et al., 2021) involves the task of blending a given\nforeground with a background to produce a unified composite image. Traditional methods typically\nfollow a sequential pipeline comprising of object placement, image blending/harmonization, and\nshadow generation. These steps aim to minimize the visual discrepancies between the two image\ncomponents. With the recent advances in generative models, notably GANs Goodfellow et al. (2014)\nand DMs Ho et al. (2020), the image composition challenge has been reframed as a generative task.\nWhile GAN-based models have led in terms of the number of research contributions, diffusion-based\nmodels, as exemplified by works such as ObjectStitch Song et al. (2022) and Paint By Example Yang\net al. (2023), showcase the potential of DMs as a one-shot solution for image composition, offering a\ndeparture from the multi-step traditional methods. However, it is essential to note that our approach\ndiverges from typical image composition. Rather than aiming to preserve the distinct identity of the\nforeground and background, our model utilizes them as localized representations for text and global\nsemantics to fill. Although our work aims to solve an inherently different task, we draw parallels to\nimage compositioning in the way we leverage synthetic image triplets and handle the target image\nto be generated.\nA.2\nDETAILS ON CNC\nDetails on the Local Fuser.\nDepth disentanglement training (DDT) leverages synthetic image\ntriplets {If, Ib, M} in order to train the local fuser. DDT first incorporates the depth maps of If and\nIb in their own foreground/background streams, as shown in Figure 2. The features from the streams\nare concatenated along their channel dimension, and features that incorporate both spatial features\nabout If and Ib are extracted in four spatial resolutions. Each extracted feature subsequently passes\nthrough a zero convolution layer, and are finally incorporated into E\u2032 through feature denormaliza-\ntion layers (Park et al., 2019), as done in Zhao et al. (2023). The frozen SD receives the localized\nsignals from E\u2032 and C\u2032 at its decoder D, integrated by residual skip connections. Denoting outputs\nof the i-th blocks of E, D, and C as ei, di, and c respectively, and the corresponding outputs of E\u2032\nand C\u2032 as e\u2032i, and c\u2032, the integration is captured as:\n\u001a concat\n\u0000c + c\u2032, ej + e\u2032\nj\n\u0001\nwhere i = 1,\ni + j = 13.\nconcat\n\u0000di\u22121, ej + e\u2032\nj\n\u0001\nwhere 2 \u2264 i \u2264 12,\ni + j = 13.\n(2)\nDetails on the Global Fuser.\nWe provide an algorithm outlining the process of incorporating soft\nguidance to the cross-attention layers to train the global fuser below.\nAlgorithm 1 Soft guidance for a single training timestep t\nRequire: Is, Ib, M, ytext, \u03bbfg = 1, \u03bbbg = 1, zt, CLIP IMAGE ENCODER, GLOBAL FUSER,\n(E, C, D)(Frozen layers of SD)\n1: (Es, Eb) \u2190 CLIP IMAGE ENCODER(Is, Ib)\n2: (yfg, ybg) \u2190 GLOBAL FUSER(Es, Eb)\n3: yfull \u2190 concat(ytext, \u03bbfgyfg, \u03bbbgybg)\n4: for all cross-attention layers in E, C, D do\n5:\n(Q, K, V ) \u2190 (WQ \u00b7 zt, WK \u00b7 yfull, WV \u00b7 yfull)\n6:\nS \u2190 (QKT /\n\u221a\nd)\n\u25b7 S \u2208 Ri\u00d7j\n7:\nJ \u2190 1i\u00d7(j\u22122N)\n\u25b7 Initialize J as an all ones matrix\n8:\n\u03c6(M) \u2190 Reshape, Flatten, Repeat(M)\n\u25b7 \u03c6(M) \u2208 Bi\u00d7N\n9:\nM \u2032 \u2190 concat(J, \u03c6(M), 1 \u2212 \u03c6(M))\n10:\nS\u2032 \u2190 S \u2297 M \u2032\n11:\nzt \u2190 softmax(S\u2032) \u00b7 V\n12: end for\n14\nPublished as a conference paper at ICLR 2024\nMask\nExtraction\nBinary\nDilation\nMasking\nInpainting\nIs\nIb\nM\nInverse Masking\nIf\nFigure 8: The process of generating our synthetic image triplets.\nIs\nLaMa Ib\nM\nLaMa Ib Depth\nSD Ib\nSD Ib Depth\nFigure 9: Comparison of LaMa and SD for inpainting, and its corresponding depth maps. Although\nimages inpainted by LaMa seem to have their salient objects removed, their corresponding depth\nmaps contain artifacts of the salient object.\nInpainting of Is \u2297 (1 \u2212 \u02dc\nM).\nTo generate Ib in {If, Ib, M}, we utilize a variant of SD specifically\ntrained for inpainting, setting the target prompt as \u201cempty scenery, highly detailed, no people\u201d. We\nalso test LaMa (Suvorov et al., 2022), a widely adopted inpainting model, and gauge its suitability as\nour inpainting module, focusing on the quality of the depth map of Ib. In Figure 9, we observe that\nthe Ib generated from LaMa exhibits certain artifacts that may not align well with the requirements\nof our pipeline. A notable characteristic of LaMa is that the depth maps of Ib often retain the shape of\nthe salient object, which could impact the information relayed to the local fuser. On the other hand,\nthe SD inpainting module proves adept for the generation of Ib. Focusing on Ib of the first row of\nFigure 9, it can be seen that certain objects that weren\u2019t once present in Is has been generated. This\nattribute of SD\u2019s inpainting module deems attractive to leverage in depth disentanglement training:\nto distill information about the relative placement of salient objects, it is critical for our model to\neffectively see objects occluded by the salient object during training. For a visualization on the\nsynthetic image triplets generation pipeline, see Figure 8.\nA.3\nADDITIONAL RESULTS\nMore Quantitative Results.\nWe provide additional quantitative results on the Pick-a-Pic valida-\ntion set in Table 3. Following the trend on the COCO-Stuff validation set, our finetuned model\nexcels over other models in all metrics with the exception of the FID and IS values from our depth-\nonly experiment. Interestingly, when comparing results from the COCO-Stuff validation shown in\nTable 1, we observe that the performance rankings of each model remain largely consistent. How-\never, the specific values for FID and IS metrics deteriorate significantly, while the CLIP Scores see\nnotable improvement. One potential reason for this trend relates to the underlying nature of the pre-\ntrained models, such as Inception-V3 (Szegedy et al., 2016), used in these metrics. While both sets\nof images being compared in this experiment are synthetic, these models are trained on real-world\nimages, inherently capturing real-world image features and patterns. The synthetic nature of the\nPick-a-Pic images might diverge considerably from these real-world expectations, thereby influenc-\n15\nPublished as a conference paper at ICLR 2024\nModel/Condition\nDepth\nSemantics\nDepth+Semantics\nFID (\u2193)\nIS (\u2191)\nCLIP\nScore (\u2191) FID (\u2193)\nIS (\u2191)\nCLIP\nScore (\u2191) FID (\u2193)\nIS(\u2191)\nCLIP\nScore (\u2191)\nGLIGEN\n22.540\n12.733\n28.227\n-\n-\n-\n-\n-\n-\nControlNet\n21.183\n13.685\n28.112\n-\n-\n-\n-\n-\n-\nUni-ControlNet\n24.561\n13.260\n28.053\n28.964\n12.809\n25.245\n22.808\n12.006\n26.722\nT2I-Adapter\n26.262\n13.309\n28.017\n47.996\n11.408\n25.033\n34.698\n10.745\n26.583\nCnC\n28.192\n11.460\n27.347\n36.272\n10.353\n24.301\n25.524\n11.131\n27.109\nCnC Finetuned\n32.155\n11.512\n28.274\n26.042\n12.838\n27.681\n22.484\n12.602\n28.094\nTable 3: Evaluation metrics on the Pick-a-Pic val-set.\nWe omit the results of semantics and\ndepth+semantics on GLIGEN and ControlNet due to the models not supporting these conditions.\nBest results are in bold.\nCnC (Ours)\nUni-ControlNet\nSketch+Depth\nUni-ControlNet\nCanny+Depth\nUni-ControlNet\nCanny+Sketch\nForeground\nCondition\nBackground\nCondition\nForeground\nCondition\nBackground\nCondition\nPrompt: Golden retriever puppies\nFigure 10: Comparing the ordering ability of localized conditions against Uni-ControlNet. Uni-\nControlNet reports that the combinations listed are the most effective, against other combinations of\n7 local conditions.\ning the FID scores. Moreover, even if both datasets under comparison are synthetic, the variance\nand distribution of features in the synthetic Pick-a-Pic dataset could be distinct enough from typical\nreal-world datasets to lead to the observed differences in FID and IS scores. This highlights the nu-\nances associated with evaluating models on synthetic versus real datasets and emphasizes the need\nfor careful consideration when drawing conclusions from such evaluations.\nOrdering of localized objects.\nIn Figure 10, we compare our model\u2019s ability to place objects in\nfront of another through the use of local conditions against Uni-ControlNet. Uni-ControlNet is able\nto take in 7 local conditions, and report that the local conditions pair listed in Figure 10 are the most\nrobust in handling conflicting conditions, i.e. two overlapping objects. Although some samples do\nshow the foreground local condition being placed in front of its counterpart, Uni-ControlNet often\nfails in conveying a sense of depth in its samples, resulting in the two objects to be generated on the\nsame z-axis. On the other hand, even if the two depth maps conditioned have relatively same depth\nvalues, our model is able to consistently occlude overlapping parts of the background depth map.\nAdditional details on Reconstruction.\nIn Figure 11, we provide additional samples of the depth\nmaps extracted from the reconstruction experiment detailed in Section 4.2. Although depth maps\nproduced by MiDaS does not reflect the true metric depth of an object, comparing the depth maps\nof the ground truth images and the reconstructed images shed light onto how well models reflects\nthe images and depth maps exposed to them during training. While the reconstructed depth maps\nof baseline models hold the overall shape of an object, it can be seen that our model succeeds in\ncapturing the relative depth of regions relative to the salient object. Additionally, we elucidate our\nchoice in selecting MAE as a metric for gauging the quality of reconstruction. Although depth\nmaps produced by MiDaS do not predict metric depth as mentioned above, our model and baseline\nmodels were trained to generate images based on image-depth map pairs. Comparing depth maps\n16\nPublished as a conference paper at ICLR 2024\nGround Truth\nCnC (Ours)\nUni-ControlNet\nT2I-Adapter\nGround Truth\nCnC (Ours)\nUni-ControlNet\nT2I-Adapter\nCOCO-Stuff\nPick-a-Pic\nFigure 11: Qualitative comparison of depth maps extracted from reconstructed images.\nCOCO-Stuff\nPick-a-Pic\nModel/Condition\nFID (\u2193)\nIS (\u2191)\nCLIPScore (\u2191) FID (\u2193)\nIS (\u2191)\nCLIPScore (\u2191)\nUni-ControlNet (Canny Edge)\n17.119\n30.440\n25.726\n21.955\n12.469\n28.517\nT2I-Adapter (Canny Edge)\n20.051\n28.449\n25.850\n30.547\n12.230\n28.412\nCnC (Canny Edge, Ours)\n17.745\n29.809\n26.283\n20.501\n12.215\n28.786\nTable 4: Canny edge evaluation metrics on the COCO-Stuff and Pick-a-Pic val-set. Best results are\nin bold.\ncan be thought of how well a model \u201dpredicts\u201d depth maps given ground truth depth maps, which\nin turn gauges how well a model has learned the relationship between images and depth maps. In\nFigure 12, we visualize the relationship between how similar the ground truth depth maps are to\nthe reconstructed depth maps in terms of MAE. Each set was randomly chosen from the top 50\npairs with the lowest/highest MAE values. It can be seen that the pairs with the lowest depth map\nMAE scores directly result in the quality of reconstruction, with the reconstructed images faithfully\nportraying the relative depth present in the ground truth images. On the other hand, the pairs with the\nhighest MAE scores result in sub-par reconstructed images. Taking the second row of Figure 12(b)\nas an example, it can be seen that the reconstructed image fails in capturing the relative depth of the\ntree and person present in the ground truth image.\nA.4\nABLATION STUDY\nAblation on different spatial conditions.\nThe ability of our model to effectively order objects\ninto spatial regions stems from our depth disentanglement training, where the spatial information\nof, and what lies behind the salient object is distilled into their respective streams of the local fuser.\nTo this end, it can be seen that our model can be trained on different types of local conditions, given\nthat the condition holds spatial information of an image. We explore the capabilities of our local\nfuser, and show the effects of training on canny edges compared to depth maps in Figure 13. Canny\nedges hold properties and biases different to that of depth maps, in the way that canny edges values\nare binary, and trade off the ability to represent depth with more fine grained details. Because of\nthese properties, it can be seen that while DDT does learn the relative placement of objects even\nwith canny edges, using canny edges has its own pros and cons. Figure 13 (a) and (c) report cases\nwhere using depth maps are preferred, while (b) and (d) report the opposite. We find that canny\nedges often fail in generating a sense of depth, as seen in case (c) where the apples look relatively\nflat. However, this property can be preferred when leveraging base images that are flat to begin\nwith. Figure 13(b) and (d) demonstrates such cases, where depth maps of flat base images (such\nas posters and vector graphics) fail to capture spatial information, resulting in sub-par images. We\nfind that DDT is able to effectively leverage the inductive biases of a given representation, whether\nit be canny edges or depth maps, and special cases might call for variants of our model trained on\n17\nPublished as a conference paper at ICLR 2024\nGT.\nRecon.\nCOCO-Stuff Lowest Depth MAE\n(a)\nGT. Depth\nRecon. Depth\nGT.\nRecon.\nGT. Depth\nRecon. Depth\nCOCO-Stuff Highest Depth MAE\n(b)\nPick-a-Pic Lowest Depth MAE\n(c)\nPick-a-Pic Highest Depth MAE\n(d)\nFigure 12: Qualitative comparison of depth map pairs with the lowest/highest MAE values from\neach dataset. Each set was randomly chosen from the top 50 pairs of the lowest/highest MAE values\nfrom COCO-Stuff and Pick-a-Pic, respectively.\n18\nPublished as a conference paper at ICLR 2024\n(a) A hello kitty mecha in a dark underground facility\n(b) A tower of cheese in front of a cat propaganda poster\n(c) An apple {in front of/behind} another apple\n(d) A neon sign of a dragon {in front of/behind} a bunny in a cup\nFigure 13: Ablation study on canny edge as a representation for our Local Fuser. (a) and (c) report\ncases where depth maps yield better images, while (b) and (d) report cases where canny edges might\nbe preferred.\n19\nPublished as a conference paper at ICLR 2024\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nInput Depth\nInput Semantics\nPrompt: A castle in the desert\n\u03bbfg, \u03bbbg\nFigure 14: Conflicting text and semantics. We condition the same depth map and exemplar image\nto each foreground/background stream, and increase \u03bbfg, \u03bbbg with the same increments.\ndifferent representations. We additionally report the quantitative results of our local fuser trained on\ncanny edges, compared to other baseline models on the COCO-Stuff and Pick-a-Pic validation sets\nin Table 4. We follow the same experimental setup of our main quantitative experiments detailed\nin Section 4.2. We find the variant of our model trained on canny edges are comparable to other\nbaseline models, resulting in the best CLIPScore for COCO-Stuff, and the best Inception Score and\nCLIPScore for Pick-a-Pic.\nConflicting Text & Semantics.\nWe explore the use of conflicting text prompts and semantics, and\nthe effect of hyperparameters \u03bbfg and \u03bbbg used to determine the influence of each global semantic. In\nFigure 14, same depth maps and exemplar images are conditioned to each foreground/background\nstream to generate each sample. We fix the text prompt to \u201cA castle in the desert\u201d, which serves\nas a semantic contradiction to the exemplar image of an empty icy tundra. The effects of steadily\nincreasing the hyperparameter values can be seen clearly, where the value of 1 strikes an adequate\nbalance between the effect of the text prompt and the global semantic. Although the effect of the\ntext prompt is nullified when the hyperparameters are set to larger values, it can be seen that the\nshape of the original object stays intact due to the combination of the depth map and soft guidance.\nAdditional results on conflicting semantics.\nFigure 15 shows the full effect of the hyperparame-\nters \u03bbfg and \u03bbbg when conditioning two contradicting global semantics. The exemplar image for the\nforeground global semantic is an image of flowing lava, and the background global semantic is an\nimage of a snowy field. By fixing the prompt with \u201cA volcano\u201d and increasing each hyperparameter\n\u03bb, the effect of each semantic is steadily amplified. Empirically, we observe that setting both \u03bbfg\nand \u03bbbg to 1 yields results that are visually harmonious, which aligns with our training configura-\ntion where each hyperparameter is set to 1. We also provide a metric to gauge our model\u2019s ability\nto localize global semantics. Given a foreground and background exemplar image Ifg.sem, Ibg.sem,\nthe corresponding generated image Igen, and the mask utilized for soft guidance M, we leverage\nthe CLIP image encoder to measure how well each semantic is applied to Igen. The mask is uti-\nlized to create two images, Igen \u2297 M and Igen \u2297 (1 \u2212 M), and fed into the CLIP image encoder to\ncreate two image embeddings. These embeddings are then compared with the embeddings of the\noriginal exemplar images Ifg.sem and Ibg.sem via cosine similarity, and the average cosine similarity\ncan be utilized as a metric of how well each semantic is applied into each region of Igen. A high\naverage cosine similarity implies that each semantic has been faithfully applied to each region via\nsoft guidance. This characteristic can be seen in Figure 16, where two contradicting semantics of\na hot air balloon and a coral reef, are localized. The average cosine similarity steadily increases as\n\u03bbbg increases, which reflects how much the effect of the coral reef increases as can be seen in the\ngenerated images. The average cosine similarity drops sharply at \u03bbbg = 2.0, due to the model over\nsaturating the effect of the coral reef, resulting in a low fidelity sample.\n20\nPublished as a conference paper at ICLR 2024\n\u03bbfg\n\u03bbbg\n0.5\n1.0\n2.0\n1.5\n0.5\n1.0\n2.0\n1.5\nPrompt: A volcano\n0.0\n0.0\nFigure 15: Conflicting semantics. The foreground global semantic image is an image of flowing\nlava, and the background global semantic image is an image of a snowy field. We fix the text prompt\nto \u201cA volcano\u201d and demonstrate the effects of hyperparameters \u03bbfg and \u03bbbg.\n21\nPublished as a conference paper at ICLR 2024\n0.5\n1.0\n2.0\n1.5\n\u03bbbg\n0.0\n\u03bbfg\n1.0\n1.0\n1.0\n1.0\n1.0\n0.7089\n0.7207\n0.7372\n0.7731\n0.6924\nAverage\nCosine Similarity\nAverage\nCosine Similarity\nCLIP(\n(\nCLIP(\n(\nCosineSim.(\n(\nCLIP(\n(\nCLIP(\n(\nCosineSim.(\n(\n+\n/2\n=\nFigure 16: Additional results on the effect of soft guidance with conflicting semantics, and its re-\nspective average cosine similarity scores. We condition the same depth image for each stream in the\nlocal fuser, and generate samples without conditioning a prompt.\nM\nVersion 1\nBackground\nImage\nForeground\nImage\nVersion 2 (Ours)\nVersion 3\nFigure 17: Depth map Ablations. We find that our model generalizes well to different versions\nof depth maps. Version 1 refers to depth maps extracted from Is. Version 2 refers to depth maps\nextracted from If. Version 3 refers to depth maps extracted from M \u2297 depthmap(If).\n22\nPublished as a conference paper at ICLR 2024\nDepth map Ablations.\nThe local fuser incorporates depth maps extracted from the synthetic im-\nage triplets {If, Ib, M} during DDT. However, we find that our model is also able to generate\nsamples conditioned on different versions of depth maps, in which we demonstrate in Figure 17.\nVersion 1 refers to conditioning the foreground stream of the local fuser on the depth map extracted\nfrom Is, instead of If. Version 2 refers to conditioning via how the local fuser was originally trained\nwith DDT, or conditioning the depth map of If. Version 3 refers to conditioning the local fuser on\nM \u2297 depthmap(If), or a masked version of the depth map of If. Pedagogically, the difference\nbetween version 2 and 3 can be thought of as whether masking with M is done before or after the\nextraction of the depth map of If. Although our model is trained on depth maps of version 2, the\nsample generated by version 1 shows that our model has learned the relative positioning of objects\nthrough its interaction with the background depth map. Because the foreground depth map of ver-\nsion 1 retains additional depth cues of the ground, the generated sample retains this information and\nplaces parts of the drum set over the ground. Interestingly, we also find that version 3 also generates\nhigh fidelity samples, even though this form of depth maps are never found in the wild. This also can\nbe attributed to the relative depth estimation capabilities of our model paired with the background\ndepth map.\n23\n"
  },
  {
    "title": "ICON: Incremental CONfidence for Joint Pose and Radiance Field Optimization",
    "link": "https://arxiv.org/pdf/2401.08937.pdf",
    "upvote": "5",
    "text": "ICON: Incremental CONfidence for Joint Pose and\nRadiance Field Optimization\nWeiyao Wang1, Pierre Gleize1,\u2020, Hao Tang1,\u2020, Xingyu Chen1, Kevin J Liang1, Matt Feiszli1\n1FAIR at Meta\n\u2020Equal contribution.\nNeural Radiance Fields (NeRF) exhibit remarkable performance for Novel View Synthesis (NVS)\ngiven a set of 2D images. However, NeRF training requires accurate camera pose for each input view,\ntypically obtained by Structure-from-Motion (SfM) pipelines. Recent works have attempted to relax\nthis constraint, but they still often rely on decent initial poses which they can refine. Here we aim\nat removing the requirement for pose initialization. We present Incremental CONfidence (ICON),\nan optimization procedure for training NeRFs from 2D video frames. ICON only assumes smooth\ncamera motion to estimate initial guess for poses. Further, ICON introduces \u201cconfidence\": an adaptive\nmeasure of model quality used to dynamically reweight gradients. ICON relies on high-confidence\nposes to learn NeRF, and high-confidence 3D structure (as encoded by NeRF) to learn poses. We\nshow that ICON, without prior pose initialization, achieves superior performance in both CO3D and\nHO3D versus methods which use SfM pose.\nDate: January 18, 2024\nCorrespondence: Weiyao Wang at weiyaowang@meta.com; Matt Feiszli at mdf@meta.com\n1\nIntroduction\nRobustly lifting objects into 3D from 2D videos is a challenging problem with wide-ranging applications.\nFor example, advances in virtual, mixed, and augmented reality Marchand et al. (2016) are unlocking\nnew interactions with virtual 3D objects; 3D object understanding is important for robotics as well (e.g.,\nmanipulation Kappler et al. (2018); Wen et al. (2022a); Qi et al. (2023) and learning-by-doing Wen et al.\n(2022b); Cheng et al. (2023)).\nBringing objects to 3D requires both extracting 3D structure and tracking 6DoF pose, but existing approaches\nhave limitations. Many Wen and Bekris (2021); Azinovi\u0107 et al. (2022); Wen et al. (2023) rely on depth,\nwhich is a powerful signal for 3D reasoning. However, accurate depth typically requires additional sensors\n(e.g., stereo, LiDAR), which add cost, weight, and power consumption to a device, and is thus often not\nwidely available. Without this depth signal, these methods often fail. Solving only half the problem is also\ncommon: 3D object reconstruction methods often assume pose Mildenhall et al. (2020); Reizenstein et al.\n(a) BARF pose predictions\n(b) ICON pose predictions\n(c) BARF Lin et al. (2021)\nnovel-view synthesis\n(d) ICON novel-view synthe-\nsis\nFigure 1 Novel view and pose visualizations of ICON and BARF when no initial pose is available. We\ntrain on a flyaround video of book from CO3D Reizenstein et al. (2021). BARF trajectories exhibit fragmentation:\ncamera poses split into two forward-facing clusters and create two books. ICON provides high-quality view synthesis\nand recovers poses very precisely. The colored triangle meshes represent ICON predicted poses and grey ones represent\ngroundtruth.\n1\narXiv:2401.08937v1  [cs.CV]  17 Jan 2024\nNeural Confidence Field:\n(\ud835\udc65, \ud835\udc66, \ud835\udc67) \u2192 \ud835\udf01\nAggregate along ray \ud835\udc5f\nRay Confidence: \ud835\udf01!\nPose Confidence: \ud835\udf01\"\nAggregate rays shot from Pose \ud835\udc43\nLearn Pose from higher \nconfidence rays\nLearn NeRF from higher \nconfidence poses\nFigure 2 ICON overview. ICON constructs a Neural Confidence field on top of NeRF to encode confidence \u03b6 for\neach 3D location. The confidence is then used to guide the optimization process.\n(2021); Munkberg et al. (2022); Oechsle et al. (2021); Sun et al. (2021); Wang et al. (2021a); Yariv et al.\n(2021), and object pose estimation methods often assume a 3D model (e.g., CAD) Pauwels and Kragic (2015);\nXiang et al. (2018); Labb\u00e9 et al. (2020). This chicken-and-egg problem often limits the applicability of these\napproaches.\nHere we aim to tackle both problems jointly, learning both an implicit 3D representation and per-frame\ncamera poses from a single monocular RGB video. We supervise both 6DoF poses and reconstruction with a\ndense photometric loss, projecting the 3D representation onto the 2D input frames. Specifically, we represent\nobjects/scenes as a Neural Radiance Field (NeRF) Mildenhall et al. (2020) to obtain 2D rendering.\nWhile recent works Yen-Chen et al. (2021); Lin et al. (2021); Wang et al. (2021b); Jeong et al. (2021); Lin\net al. (2023); Truong et al. (2023) have shown that poses can to some extent be (jointly) learned in this\nsetting, they are most effective when used to refine initial poses with moderate noise. For example, Wang\net al. (2021b) shows they begin to fail when pose noise exceeds approximately 20 degrees of rotation error;\nmore complex trajectories are unrecoverable. Indeed, these methods also fail on even moderately-complex\ntrajectories, for example a full 360-degree flyaround of an object (Sec. 4). This means SfM preprocessing\nremains a prerequisite for constructing a radiance field.\nOne way forward would be to focus on the large-noise case, working to resolve larger pose changes. This\nis promising Meng et al. (2021), but here we go the other way, and focus on the incremental case. This\narises naturally in real-world settings where video is input, e.g., embodied AI. We take inspiration from\nincremental SfM Schonberger and Frahm (2016) and SLAM Davison (2003), training pose and NeRF jointly\nin an incremental setting. In this setup, the model takes a stream of video frames, one at a time. Leveraging\na motion-smoothness prior, we initialize an incoming frame with the previous frame\u2019s pose. Information\nbetween frames is exchanged through view synthesis from NeRF.\nA major challenge comes from the interdependence between 3D structure and pose: high photometric error\nmay be attributable to a poor 3D model despite good pose, or a large error in pose despite a good model.\nWe observe and analyze several interesting failure modes, including fragmentation, a generalization of the\nclassical Bas-Relief ambiguity Belhumeur et al. (1999), and overlapping registration (see Fig. 3).\nTo address the difficulties, we propose ICON (Incremental CONfidence). The intuition is simple (Fig. 2):\n\u201cWhen pose is good, learn the NeRF; when the NeRF is good, learn pose.\" ICON interpolates between these two\nregimes, using a measure of confidence obtained from photometric error, and maintaining a NeRF-style \u201cNeural\nConfidence Field\" to store confidence in 3-space. Confidence is also used as a signal to guide optimization; in\nparticular it can help identify (and escape from) local minima.\nWe perform quantitative evaluation of ICON on CO3D Reizenstein et al. (2021), HO3D Hampali et al. (2020),\nand LLFF Mildenhall et al. (2019). While joint pose-and-3D baselines often fail catastrophically, ICON\nachieves strong performance on CO3D, comparable to NeRFs trained on COLMAP Schonberger and Frahm\n(2016) pose and surpassing a wide selection of baselines, such as DROID-SLAM Teed and Deng (2021) and\nPoseDiffusion Wang et al. (2023). In addition, we evaluate on CO3D videos with background removed; this\n2\nsignificantly increases the difficulty since background texture makes camera pose extraction easier. We note\nthat this case (a single masked object in isolation) is quite valuable: success here means a method will\nwork whether the camera is moving, the object is moving, or both. ICON achieves superior performance to\nNeRF+COLMAP pose and a wide selection of baselines Finally, ICON outperforms RGB baselines and is\ncomparable to SOTA RGB-D method BundleSDF Wen et al. (2023) on dynamic hand-held objects in HO3D.\nTo summarize, we make the following contributions:\n1. We propose an incremental registration for joint pose and NeRF optimization. This setup removes the\nrequirement for pose initialization in common video settings.\n2. We systematically study this incremental setup and discover several challenges. Based on the observations,\nwe propose ICON, an optimization protocol based on confidence in spatial locations and poses.\n3. We evaluate ICON with a focus on object-centric datasets. ICON is SOTA among RGB-only methods,\nand is even competitive with SOTA RGB-D methods.\n2\nRelated Work\nNeural Radiance Field (NeRF) Mildenhall et al. (2020) is a powerful technique to represent 3D from posed\n2D images for novel view synthesis. One major limitation of NeRF resides in its requirement for accurate\ncamera poses. Recent works, including Nerf\u2013 Wang et al. (2021b), BARF Lin et al. (2021), SCNeRF Jeong\net al. (2021), SiNeRF Xia et al. (2022), NeuROIC Kuang et al. (2022), IDR Yariv et al. (2020), GARF Chng\net al. (2022) and SPARF Truong et al. (2023) have attempted to relax this requirement by jointly optimizing\nposes and NeRF. Despite the promising direction, they work the best when refining noisy initial poses and are\nlimited by the robustness of initial pose estimation methods. One direction the community takes to further\nreduce the dependency on pose is by adding additional components or signals for initial pose estimations,\nsuch as GANs Meng et al. (2021), SLAM Rosinol et al. (2022), shape priors Zhang et al. (2021), depth Bian\net al. (2023) and coarse annotations Boss et al. (2022). We tackle this problem from a different angle, where\nwe propose an incremental setup of joint NeRF and pose optimization. Our proposed method ICON does\nnot use additional signals and achieve strong performance on challenging scenarios when camera poses are\ndifficult to obtain.\nPose estimation (Object) aims to infer the 6 Degrees-of-Freedom (DoF) pose of an object from image frames.\nThe line of work can be classified into two main categories: image pose estimation Xiang et al. (2018); Labb\u00e9\net al. (2020) and video pose tracking Muller et al. (2021); Stoiber et al. (2022); Teed and Deng (2020), where\nthe former mostly focuses on inferring pose from sparse frames and the latter takes the temporal information\ninto consideration. However, many methods in video or image pose estimation assume known instance- or\ncategory-level object representations, including object CAD models Xiang et al. (2018); Labb\u00e9 et al. (2020,\n2022); Sundermeyer et al. (2018); Wang et al. (2019); Stoiber et al. (2022); Muller et al. (2021) or pre-captured\nreference views with known poses Liu et al. (2022); Park et al. (2020). Recently, BundleTracks Wen and\nBekris (2021) removes the need for such object priors, thus generalizing to pose tracking for unseen novel\nobjects, and BundleSDF Wen et al. (2023) improves pose tracking by constructing a neural representation for\nthe object. However, both require depth information, limiting their applications.\nSLAM(SimultaneousLocalizationandMapping) builds a map of its environment while simultaneously determining\nits own location within that map Mur-Artal et al. (2015); Mur-Artal and Tard\u00f3s (2017); Davison et al. (2007);\nEngel et al. (2014, 2017); Klein and Murray (2007); Zubizarreta et al. (2020). While most SLAM methods\nfocus on understanding camera pose movement in a static environment, object-centric SLAM McCormac et al.\n(2018); Merrill et al. (2022); Runz et al. (2018); Salas-Moreno et al. (2013); Sharma et al. (2021) focus on\nlearning object pose in a dynamic environment. However, most of those methods require depth signal Runz\net al. (2018); McCormac et al. (2018); Merrill et al. (2022) and struggle with large occlusion or abrupt motion\nWen et al. (2023).\n3\n3\nMethod\nICON takes streaming RGB video frames as input and produces 3D reconstructions and camera pose estimates.\nICON incrementally registers each input frame to optimize 3D reconstruction guided by confidence: the 3D\nreconstruction is learned more from frames with high confidence pose, and pose relies on 3D-2D reprojection\nfrom higher confidence areas of the 3D reconstruction.\n3.1\nPreliminaries: Neural Radiance Fields\nICON relies on Neural Radiance Fields (NeRF) to represent a 3D reconstruction: NeRF encodes a 3D scene\nas a continuous 3D function through a multilayer perceptron (MLP) f parameterized by \u0398: 3D point x and\nviewing direction d form the input (x, d) \u2208 R5 \u2192 (c, \u03c3) \u2208 R4, where c \u2208 R3 is the color and \u03c3 is the opacity.\nTo generate a 2D rendering of a scene at each pixel p = (u, v) in image \u02c6Ii from camera pose Pi, NeRF uses\na rendering function R to aggregate the radiance along a ray shooting from the camera center oi position\nthrough the pixel p into the volume:\n\u02c6Ii(p) = R(p, Pi|\u0398) =\nZ zfar\nznear\nT(z)\u03c3(r(z))c(r(z), d)dz\n(1)\nwhere T(z) = exp(\u2212\nR z\nznear \u03c3(r(z))dz) is the accumulated transmittance along the ray, and r(z) = oi + zd\nis the camera ray from origin oi through p, as determined by camera pose Pi. NeRF implements R by\napproximating the integral via sampled points along the ray, and is trained through a photometric loss between\nthe groundtruth views Ii and the rendered view \u02c6Ii for all images i = 1, ..., N:\n\u0398\u2217 = arg min\u0398Lp(\u02c6I|I, P), where Lp(I, \u02c6I) =\nX\n\u2225Ii \u2212 \u02c6Ii\u22252\n(2)\n3.2\nIncremental frame registrations\nA major limitation for these joint pose and NeRF optimization methods is a requirement for good initial\nposes. If {Pi} contain a diverse set of viewpoints and are initialized all from identity, these methods often\ncollapse. For example, a simple but common collapsing solution is fragmentation: each frame creates its\nown fragmented 3D representation, all mutually invisible to the other views (Fragmentation fig. 3). Indeed,\nBARF Lin et al. (2021) collapses on all sequences of the CO3D dataset when the poses {Pi} consist of a\nclosed-loop flyaround (see Tab. 1). As discussed in Wang et al. (2021b), when no pose prior is provided, a\nbreaking point of 20 degree rotation difference for the whole trajectory is observed.\nTo tackle this problem, we rely on a simple yet effective intuition: camera motions in videos are smooth.\nTherefore, given a frame Ii in a video, its camera pose Pi is likely to be close to Pi\u22121. We leverage this\nobservation and propose to register frames incrementally following the temporal order.\nImplementation. At the start of training, we jointly optimize NeRF parameters \u0398 and poses {P1, P2} from the\nfirst two frames {I1, I2}. After every k iterations, we add a new frame Ii and initialize its pose Pi by Pi\u22121.\nWe freeze the learning rate on poses {Pi}N\ni=1 and NeRF \u0398 until all frames are registered. A learning rate\ndecay schedule may be applied after all N images are added.\n3.3\nConfidence-Based Optimization\nThe incremental registration process aims at providing good initialization for the camera poses. However,\noptimizing poses and NeRF using photometric losses is highly non-convex and contains many local minima Yen-\nChen et al. (2021); Lin et al. (2023). In addition, an incorrectly optimized pose may provide misleading learning\nsignals towards NeRF, increasing the possibility for poses to re-register incorrectly on already registered\nviewpoints (Overlapping Registration fig. 3).\nTo tackle these, we propose a confidence-guided optimization schema. The intuition is simple: when a pose\nPi is confident, it should be trusted more to improve the learned NeRF f(\u0398); when a ray sampled from Pi\ncontains locations that are confident, it should be weighted more to adjust the poses. When pose confidence\n4\nBas Relief\nOverlapping \nRegistration\nFragmentation\nA tube of stacked toytrucks\nthat the camera flies through \nlike a flipbook\nPose Est. vs. GT\nOriginal View\nReconstruction\nA concave apple inside the table\nVacant voxels from \nmissing parts of true \ncamera trajectory\nBlurry rendering \non overlapped \ntrajectory\nFigure 3 Three major failure modes of joint pose and NeRF optimization: fragmentation, Bas Relief,\nand overlapping registration. The colored poses are predictions; grey poses are groundtruth. Fragmentation: Pose\nand NeRF break apart, producing separate, mutually invisible radiance fields. Here a tube of toytrucks is created, each\noccluding the next. Poses fly through this tube flipbook-style, each seeing a single toytruck. See also Fig. 1, where\ncompletely independent reconstructions occur in different regions of 3-space. Bas Relief: Due to an inherent ambiguity\nin RGB reconstruction, the model constructs a \u201crelief\" by creating a concave apple inside the table, which results in\ncamera trajectories inverted by 180 degrees. Overlapping Registration: Two subsets of the pose trajectory are trapped\nin a local minimum, incorrectly observing the same part of the radiance field, leading to blurry rendering and empty\nvoxels. Here, one side of the toaster is blurry due to overlapping views, while the other has no views and is vacant.\n5\ndrops dramatically for a new frame, it is likely that the pose got stuck in a local minima, so we perform\na restart to re-register this pose. This is similar to the trial and error strategy of COLMAP Schonberger\nand Frahm (2016). We next describe how we measure confidence for each pose Pi and each point/viewing\ndirection (x, d) in 3D.\nEncoding confidence in 3D. We construct a Neural Confidence Field on top of NeRF: given an input 3D location\nand direction (x, d), NeRF f also predicts confidence \u03b6(x,d). We add one fully-connected layer on top of the\nfeatures, followed by a sigmoid, similar to the color prediction head.\nThe confidence for a ray r, is then aggregated through volumetric aggregation similar to opacity rendering:\n\u03b6r = (\nZ zfar\nznear\nP(z)dz)(\nZ zfar\nznear\nP(z)\u03b6(r(z), d)dz)\n+ (1 \u2212\nZ zfar\nznear\nP(z)dz)(\nZ zfar\nznear\n\u03b6(r(z), d)dz)\n(3)\nwhere P(z) = T(z)\u03c3(r(z)). We note that the first term is more prominent when the pixel is opaque whereas\nthe latter is more prominent for transparent pixels.\nMeasuring confidence. We measure confidence by how well a pixel reprojects in 2D through photometric error.\nGiven a ray and its confidence \u03b6r, we minimize Lconf = \u2225e\u2212E/\u03c4 \u2212 \u03b6r\u22252, where E is the photometric error used\nto train NeRF and \u03c4 is a temperature parameter. Lconf is only used to train the confidence head; gradient is\nstopped before NeRF parameters \u0398 or poses.\nPose confidence. We compute pose confidence \u03b6Pi for pose Pi by aggregating confidence over rays sampled\nfrom Pi. At the start, P1 has confidence 1 and others have confidence 0. During training, we use a momentum\nschedule to update pose confidence: at training iteration t, we sample B rays {ri\nj}B\nj=1 from pose Pi, and\nupdate confidence \u03b6t\nPi as\n\u03b6t\nPi = \u03b2\u03b6t\u22121\nPi\n+ (1 \u2212 \u03b2) 1\nB\nB\nX\nj=1\n\u03b6ri\nj\n(4)\nThe momentum \u03b2 is 0.9 in our experiments.\nCalibrating loss by confidence. We use confidence to calibrate L. Intuitively:\n\u2022 When we compute gradients for NeRF parameters \u0398, the loss is weighted by {\u03b6Pi}, the pose confidence.\n\u2022 When we compute gradients for pose {Pi}, the per-ray loss is weighted by {\u03b6r}, the ray confidence.\nAt each step, we sample ray {ri\nj}B\nj=1 from Pi. The loss is:\nLNeRF(\u0398| \u02c6P, I) =\nX\ni\n(\nX\nj\nL(ri\nj))\u03b6Pi)/(\nX\ni,j\n\u03b6Pi)\n(5)\nLPose( \u02c6P|\u0398, I) =\nX\ni,j\nL(ri\nj)\u03b6ri\nj/(\nX\ni,j\n\u03b6ri\nj)\n(6)\nLall(\u0398, \u02c6P|I) = LNeRF + LPose + Lconf\n(7)\nPose re-init. Inspired by trial-and-error registration mechanisms in incremental SfM Schonberger and Frahm\n(2016), we do a re-initialization from the previous pose if a new image fails to register. We declare failure\nif we see an abrupt drop in confidence for a newly registered image: after we register (Ii, Pi), we restart if\nnew pose confidence \u03b6Pi is less than \u03bb standard deviations of the mean of the K previous pose confidences:\n\u03b6Pi \u2264 mean({\u03b6Pj}i\u22121\nj=i\u2212K) \u2212 \u03bb \u00b7 std({\u03b6Pj}i\u22121\nj=i\u2212K). We use \u03bb = 2 and K = 10 throughout our experiments.\n3.4\nBas-Relief Ambiguity and Confidence-based Restart\nBas-relief ambiguity Belhumeur et al. (1999), and the related \"hollow-face\" optical illusion, are examples\nof fundamental ambiguity in recovering an object\u2019s 3D structure when objects that differ in shape produce\nidentical images, perhaps under differing photometric conditions like lighting or shadow. For example, a\n6\nsurface with a round convex bump lit from the left may appear identical to the same surface with an concavity\nlit from the right. We refer generically to such situations as \"Bas-Relief\" solutions. Human visual systems\nare known to employ strong priors (e.g. favoring convexity) to select a particular solution among multiple\npossibilities.\nWe observe this phenomenon when jointly optimizing camera poses and NeRF, especially early in optimization\nwhen total camera motion is small. The model becomes stuck in a local minimum and cannot escape. For\nexample, a concave version of the scene may be reconstructed when the groundtruth is a convex scene (see Bas\nRelief in Fig. 3). In this example, the camera movement is off by 180 degrees and moves in opposite directions\ncompared to the groundtruth trajectory. We believe that simple priors, using cues like coarse depth, could\nhelp produce more human-like interpretations of natural scenes. However, for this study we avoid crafting\npriors, and remark that our confidence-based calibration of losses helps reduce this issue (16% to 9%).\nWe also observe that incorrect Bas Relief solutions generally have higher error and lower confidence; Relief\nsolutions tend to be valid for a limited set of viewpoints and wider viewpoints become inconsistent. Hence we\nto propose a generic solution by adopting the restart strategy from incremental SfM. For example, COLMAP\nrestarts to identify different initial pairs if the final reconstruction does not meet certain criteria (e.g. ratio of\nregistered images). For us, we launch K runs independently and measure the confidence after a fixed number\nof iterations. We pick the one with the highest confidence. In practice, we launch 3 runs and measure the\nconfidence at 10% of the training.\n3.5\nConfidence-based geometric constraint\nFollowing recent works Jeong et al. (2021); Truong et al. (2023), we add a geometric constraint to the\noptimization. Different from the ray-distance loss Jeong et al. (2021) and depth consistency loss Truong et al.\n(2023), we adopt sampson distance Hartley and Zisserman (2003), similar to Wang et al. (2023). We extract\ncorrespondence between a frame and its neighbors. We use SIFT Lowe (1999) features, primarily for fair\ncomparison with COLMAP. At training time, for each pose Pi, we sample a pose Pj in its neighbor, then\ncompute Sampson distance:\nLSampson =\n|xiFxj|\n|(xiF)1 + (xiF)2 + (Fxj)1 + (Fxj)2|\n(8)\nwhere F is the fundamental matrix between Pi and Pj and (xiF)k indicates the kth element.\nLoss calibration by confidence. Although geometric cues help constrain the early optimization landscape, the\ncorrespondence pairs can be incorrect and/or not pixel-accurate, especially for objects with little texture. This\ncauses the geometric constraint to be detrimental to ICON for obtaining precise poses and reconstructions.\nWe rely on pose confidence \u03b6Pi to weight the Sampson distance: for a pair of pose Pi and Pz, weight by\n1 \u2212 min(\u03b6Pi, \u03b6Pj).\n4\nExperiments\nDatasets. We focus our study on Common Objects in 3D v2 (CO3D) dataset Reizenstein et al. (2021), a\nlarge-scale dataset consisting of turn-table style videos of objects. Ground truth poses are obtained through\nCOLMAP. We train on two versions of the dataset: full-scene, which uses the unmodified image frames\n(both object and background visible), and object-only, which removes the background leaving only foreground\nobject pixels. We believe the object-only version is a more challenging yet meaningful evaluation set; in\nfull-scene, objects are often placed on textured backgrounds where COLMAP can successfully extract poses.\nThis implicitly equates object pose and camera pose, and this assumption breaks in dynamic scenes where\nboth object and camera are moving. We use 18 categories specified by the dev set, with \u201cvase\u201d and \u201cdonut\u201d\nremoved due to symmetry (indistinguishable in the object-only setting). We select scenes with high COLMAP\npose confidence for camera pose evaluation. We clean the masks using TrackAnything Yang et al. (2023);\nresults on original masks are present in the supplementary. To demonstrate performance on dynamic objects,\nwe additionally re-purpose HO3D Hampali et al. (2020) v2 to evaluate the camera pose tracking and view\nsynthesis quality. HO3D consists of static camera RGBD videos capturing dynamic objects manipulated by\n7\nGT\nNeRF\nICON\nFigure 4 Novel view synthesis visualization of ICON without poses and NeRF trained with GT poses.\nDespite having no pose priors, ICON renders novel views at comparable or higher quality. Results are taken from\nLLFF and CO3D.\nhuman hands. We only use the RGB frames for ICON and select 8 clips (each around 200 frames) from 8\nvideos, each covering a different object. Finally, we show results on LLFF Mildenhall et al. (2019), a dataset\nwith 8 forward-facing scenes commonly used for scene-level novel view synthesis, especially for NeRFs.\nArchitectures and Losses Our architecture follows NeRF Mildenhall et al. (2020) (no hierarchical sampling) and\nset the image\u2019s longer edge to 640. We use the standard MSE loss of NeRF. When using Sampson distance, it\nis weighted by 10\u22124. For the object-only settings in CO3D and HO3D, where object masks are available, we\nuse MSE loss to supervise the opacity. For HO3D, we use hand masks when provided (7 out of 8 clips) to\navoid sampling rays from occluded regions.\nTraining. We use BARF Lin et al. (2021) settings and train for 200k iterations. For CO3D and HO3D, we skip\nevery other frame to reduce training time, producing sequences around 100 frames. For ICON and its variants,\nwe add a new frame every 1k iterations (CO3D/HO3D) / 500 iterations (LLFF) and freeze the learning rate\n(100k iterations for HO3D and CO3D, 30k for LLFF). Following BARF, we do not use positional encodings\nduring registration and apply coarse-to-fine positional encoding after registration.\nEvaluation. Following Lin et al. (2021), we evaluate on the last part (typically 10%) of each sequence. We\nmeasure camera pose quality with Absolute Trajectory Error (ATE) Zhang and Scaramuzza (2018), performing\nUmeyama alignment Umeyama (1991) of predicted camera centers with ground truth. ATE consists of a\ntranslation (ATE) and rotation (ATErot) component, evaluating l2-distance between camera centers and\nangular distance between aligned cameras, respectively. For novel view synthesis, we run an additional\ntest-time pose refinement, following standard practices in previous works Lin et al. (2021); Wang et al. (2021b);\nYen-Chen et al. (2021); Truong et al. (2023). We use PSNR, LPIPS Zhang et al. (2018), and SSIM as metrics.\nBaselines. We build ICON on top of BARF Lin et al. (2021), and compare against BARF for joint pose and\nNeRF optimization. For novel-view synthesis, we train NeRF with ground truth poses. For pose, we compare\nagainst a wide selection of baselines: PoseDiff Wang et al. (2023) models SfM within a probabilistic pose\ndiffusion framework; concurrent work FlowCam FlowCAM Smith et al. (2023) solves pose from estimated 3D\nscene flow; DROID-SLAM Teed and Deng (2021) is a SOTA end-to-end learning-based SLAM system. We also\nuse their predicted poses to initialize and train NeRF. In addition, on object-only CO3D evaluation, we evaluate\nposes from state-of-the-art SfM pipeline COLMAP Schonberger and Frahm (2016) and an augment version of\nCOLMAP Sarlin et al. (2019) using learning-based features SuperPoint DeTone et al. (2017)+SuperGlue Sarlin\net al. (2020) (COLMAP+SPSG). Though ICON only uses RGB, we include popular RGB-D methods on HO3D,\nincluding DROID with ground truth depth input, BundleTrack Wen and Bekris (2021) and state-of-the-art\nBundleSDF Wen et al. (2023).\n4.1\nFull scene from CO3D\nICON is strong on full-scene CO3D. We compare ICON and baselines on full CO3D scenes in Table 1. Without\nprior knowledge, BARF must initialize all camera poses as identity. CO3D\u2019s flyaround captures of objects result\n8\nATE\nATErot\nPSNR\nSSIM\nLPIPS\nPose Source + NeRF\nDROID\n0.431\n8.92\n17.19\n0.526\n0.541\nFLOW-CAM\n2.681\n91.28\n14.40\n0.441\n0.689\nPoseDiff\n1.973\n27.25\n18.82\n0.563\n0.520\nGroundtruth\n-\n-\n21.03\n0.575\n0.629\nJoint Pose + NeRF optimization\nBARF\n6.215\n114.63\n12.77\n0.401\n0.871\nGT-Pose+BARF\n0.417\n3.77\n19.33\n0.558\n0.647\nICON (Ours)\n0.138\n1.16\n22.24\n0.654\n0.428\nTable 1 Comparison on CO3D Reizenstein et al. (2021) full image scenes. While baseline BARF may fail on CO3D due to\nlarger camera motion overall, ICON can estimate poses very precisely and render novel views at quality similar or\nbetter than NeRF trained with GT poses.\nin camera pose variation that significantly exceeds the threshold after which BARF\u2019s performance collapses,\nwith an ATErot exceeding 100 degrees. In contrast, ICON\u2019s incremental approach recovers significantly\nmore precise camera poses (ATE of 0.137 and ATErot of 1.20), while also achieving better visual fidelity,\nboth qualitatively and quantitatively, as measured by PSNR, SSIM, and LPIPS. Interestingly, ICON still\noutperforms BARF even if BARF is provided with the ground truth poses at initialization. We originally\nproposed this setting as an upper bound, but we believe this result reflects instability in early iterations of\nBARF training: CO3D sequences are challenging compared to BARF benchmark scenes (e.g. synthetic dataset\nfrom Mildenhall et al. (2020)/forward facing LLFF). Camera coverage is sparser, with more drastic lighting\nchanges, and motion blur. Among the 18 scenes, BARF suffers from \u2265 10 degree ATErot in 4, dragging down\nthe overall performance.\nWe also make several comparisons with NeRF Mildenhall et al. (2020) and pose prediction methods. We\nprovide NeRF with poses predicted by DROID-SLAM, FLOW-CAM, and PoseDiff, which rely on annotated\nposes to train or additional signals such as optical flow Teed and Deng (2020). However, our joint NeRF and\npose training produces better pose estimates (as measured by ATE and ATErot), and as a result, NeRF\u2019s\nnovel view synthesis suffers in comparison. Even given CO3D\u2019s ground truth poses, ICON can outperform\nNeRF. While this may at first seem surprising, we point out that even the \u201cground truth\u201d poses in CO3D are\nnot true ground truth; they are generated with COLMAP, which is not perfect. Additionally, in contrast to\nCOLMAP, ICON\u2019s joint learning of NeRF and poses means that the estimated poses are specifically optimized\nto also maximize NeRF quality. We hypothesize that this leads to poses more compatible for learning a NeRF,\nas reflected by the better performance we observe. Similar observations were presented in prior works Jeong\net al. (2021); Meng et al. (2021).\n4.2\nObject-only on CO3D\n6DoF pose is inherently tricky to annotate, so past datasets often restrict motion to either the object or the\ncamera; in the latter case, visually distinct backgrounds (e.g., specially designed patterns, such as QR codes\naround the object) are often used to make pose trajectory reconstruction easier. These strategies however do\nnot generalize to more in-the-wild video, especially when both an object and the background (or camera) are\nmoving. For this reason, we also perform evaluations on CO3D with the background masked out; in such a\nsetting, algorithms are forced to only rely on object-based visual signal for estimating pose (Table 4.2).\nIn this challenging setting, we again observe that BARF fails to estimate accurate poses, as the camera\ntrajectory changes beyond what BARF can correct. Additionally, the difficulty of this setting produces further\ndeterioration of BARF\u2019s novel view synthesis. However, we observe that ICON can still handle such videos,\neven without signal from the background. This implies ICON is viable for joint pose estimation and 3D object\nreconstruction on more general videos, when the background cannot be relied on.\nAs with our full-scene CO3D experiments, we compare with methods for estimating pose, and how well those\nposes work when fed to a NeRF. We observe that without being able to leverage the background, these\nmethods struggle mightily. Pose prediction ATE and ATErot from DROID-SLAM in particular shoot up from\n0.431 to 5.903 and 8.92 to 90.25, respectively. With poorer pose, the quality of the learned NeRFs are also\ncorrespondingly worse.\n9\nATE\nATErot\nPSNR\nSSIM\nLPIPS\nPose Source + NeRF\nDROID\n5.903\n90.25\n14.54\n0.181\n0.818\nFLOW-CAM\n6.700\n120.52\n13.08\n0.127\n0.886\nPoseDiff\n4.601\n64.24\n15.42\n0.508\n0.492\nGroundtruth\n-\n-\n20.77\n0.718\n0.301\nCOLMAP variants\nCOLMAP(11)\n1.177\n13.62\n-\nCOLMAP-SPSG(11)\n2.815\n38.37\nCOLMAP-SPSG\n3.616\n43.74\nJoint Pose + NeRF optimization\nGT-Pose+BARF\n2.055\n17.00\n15.65\n0.802\n0.277\nBARF\n6.522\n114.97\n8.22\n0.772\n0.370\nICON (Ours)\n0.215\n1.80\n22.45\n0.893\n0.132\nTable 2 Comparison on CO3D Reizenstein et al. (2021) object-only scenes without background. Despite the challenges with\nbackground removal and failure from other methods, ICON can obtain poses at high precision and render novel views\nat high-quality. Since COLMAP only successfully registered more than 50% of frames on 11 objects, we marked it\nwith \u201c(11)\" for comparison. The SPSG version of COLMAP registers for all scenes, and we include a datapoint on the\n11 scenes subset that vanilla COLMAP succeeds.\nInput\nATE\nATErot\nTrans\nPSNR\nBARF\nRGB\n0.135\n122.38\n0.580\n5.72\nICON\n0.033\n8.07\n0.049\n16.24\nBaselines\nDROID\nRGB\n0.187\n114.71\n0.548\n-\nDROID\nRGB-D\n0.105\n51.93\n0.262\nBundleTrack\n0.046\n29.45\n0.158\nBundleSDF\n0.021\n6.82\n0.030\nTable 3 Comparison on HO3D Hampali et al. (2020). ICON works robustly against faster motion (vs CO3D), hand\nocclusion and lack of background information. In fact, despite only using RGB inputs, ICON can track poses at similar\nprecision as SOTA RGB-D BundleSDF.\nFor pose in particular, we additionally evaluate COLMAP and its variant COLMAP-SPSG, which replaces\nSIFT Lowe (1999) with SuperPoint-SuperGlue DeTone et al. (2017); Sarlin et al. (2020), on how they predict\npose from just the foreground objects of CO3D. We observe that COLMAP performs significantly worse when\nit cannot rely on background cues, far worse than ICON. We believe this finding to be especially significant,\nas COLMAP is often considered the gold standard for camera pose alignment, and is often treated as \u201cground\ntruth\" (as in CO3D). This suggests our incrementally learned joint pose and NeRF optimization represents a\npromising new alternative for posing moving foreground objects, even if the background or camera is also\nmoving.\n4.3\nHand-held dynamic objects on HO3D\nUnderstanding handheld objects is of particular importance to many applications, as the very nature of\ninteraction often implies importance, and hands are often the source of object motion.\nPose and 3D\nreconstructions are key components of understanding objects, so the ability to generate them from videos of\nhandheld interactions is of high utility. We show results on HO3D Hampali et al. (2020) in Table 3.\nCO3D-FullImg\nCO3D-No Background\nHO3D\nIncre Geo. Calib. Restart ATE ATErot PSNR SSIM LPIPS ATE ATErot PSNR SSIM LPIPS ATE ATErot PSNR SSIM LPIPS\n\u2713\n\u2713\n\u2713\n\u2713\n0.138\n1.16\n22.24 0.654 0.428 0.215\n1.80\n22.45 0.893 0.132 0.033\n8.07\n16.24 0.863 0.164\n\u2713\n\u2713\n\u2713\n0.714\n25.40\n20.48 0.632 0.486 0.224\n1.86\n22.47 0.892 0.132 0.035\n27.32\n15.02 0.873 0.670\n\u2713\n\u2713\n\u2713\n1.691\n28.95\n18.66 0.565 0.556 0.340\n3.91\n21.92 0.887 0.140 0.032\n19.19\n14.51 0.866 0.184\n\u2713\n\u2713\n1.283\n36.82\n19.05 0.567 0.562 0.972\n15.94\n21.03 0.875 0.163 0.046\n30.50\n12.86 0.863 0.290\n\u2713\n3.075\n78.49\n14.38 0.454 0.816 0.890\n8.05\n20.67 0.850 0.187 0.076\n32.26\n12.51 0.870 0.189\n6.215 114.63 12.77 0.401 0.871 6.522 114.97\n8.22\n0.772 0.370 0.307 131.16\n7.45\n0.82\n0.29\nTable 4 Ablation study by removing components when possible. We remark that all designed component are critical for\nICON. In addition, we didn\u2019t observe Bas Relief on the CO3D Object-Only (No Background) scenes, so the effect of\nRestart is minimal.\n10\nGT \nICON Novel View \nFigure 5 Visualization of ICON novel view synthesis on HO3D. ICON can recover shapes and textures\naccurately.\nAgain, we primarily compare against BARF for joint object pose estimation and NeRF learning. Similar\nto CO3D object-only version, background is masked out since it moves differently than object. In addition,\nHO3D presents challenges with hand-occlusion and faster pose changes than CO3D. As with CO3D, we\nobserve that BARF struggles to properly learn pose, especially with more drastic camera motion across nearby\nframes. On the other hand, ICON can perform well with these challenges: poses are predicted accurately\n(Tab 3) and textures are rendered properly in novel views (Fig. 5)\nSeveral existing works Wen and Bekris (2021); Wen et al. (2023) addressing this problem additionally use\ndepth, which provides a powerful signal for 3D object reconstruction and pose. On the other hand, depth\nrequires additional sensors and is not always available, and most visual media on the internet is RGB-only.\nInterestingly, we find that our results with ICON are competitive with state-of-the-art methods like BundleSDF\nwhich do require depth. In addition, although we don\u2019t design or optimize ICON for mesh generation, we\ninclude a comparison on mesh by running an off-the-shelf MarchingCube Lorensen and Cline (1987) algorithm.\nWe follow the evaluation protocol in Wen et al. (2023), use ICP for alignment Besl and McKay (1992) and\nreport Chamfer distnace. Despite not using depth signals, we found ICON provides competitive mesh quality\n(0.7cm) compared to BundleSDF (0.77cm). We remark that BundleSDF\u2019s reconstruction performed poorly\non one scene (2.39 cm); removing one worst scene for both method, BundleSDF and ICON achieved 0.54cm\nand 0.56cm. We believe that this represents the potential of monocular RGB-only methods for object pose\nestimation and 3D reconstruction.\n4.4\nAblation studies\nWhat are the key components in ICON? We perform ablation studies to gain deeper insight why our proposed\nmethodology leads to such significant improvements in Table 4, examining the impact of incremental frame\nregistration (\u201cIncre.\u201d), as well as confidence-based geometric constraint (\u201cGeo.\u201d), loss calibration through\nconfidence (\u201cCalib.\u201d), and restarts (\u201cRestart\u201d). Note that the top row, with all options enabled, corresponds\nto our proposed ICON, while the bottom row (with none) is equivalent to BARF. We find all the proposed\ntechniques to be essential\nICON works on forward-facing scenes with minor camera motion. While much of our motivation and experiments\ncenter on the challenging setting of object-centric pose estimation and NeRF representations, we do not\nenforce any object-specific priors in our method. Our approach thus also generalizes to the scene images of\nLLFF Mildenhall et al. (2019), a common benchmark used by the wider NeRF community. Compared to\nthe type of videos in CO3D or HO3D, the images in LLFF tend to be forward-facing: the camera poses for\neach image have only mild differences. Though easier, being able to recover camera poses in such settings is\nstill important for wider applicability. We find that because the camera poses of LLFF only have limited\nvariation, BARF initialized at identity is able to recover good poses and achieve good PSNR, SSIM, and\nLPIPS (Table 5). ICON, however, outperforms both BARF and a standard NeRF provided with ground truth\nposes.\n11\nATE\nATErot\nPSNR\nSSIM\nLPIPS\nGT-Pose+NeRF\n-\n-\n22.06\n0.648\n0.294\nBARF\n0.498\n0.896\n23.89\n0.721\n0.240\nICON\n0.459\n0.806\n24.23\n0.731\n0.221\nTable 5 Comparison on LLFF Mildenhall et al. (2019) dataset. When camera poses have minor or mild motion, BARF\nworks well with identity pose initialization and ICON performs slightly better. ATE is scaled by 100.\n5\nConclusion\nWe proposed to study joint pose and NeRF optimization in an incremental setup and highlighted interesting\nand important challenges in this setting. To tackle them, we have designed ICON, a novel confidence-based\noptimization procedure. The strong empirical performance across multiple datasets suggests that ICON\nessentially removes the requirement for pose initialization in common videos. Although our focus is on\nobject-centric scenarios, there are no priors or heuristics that rule out other settings. ICON\u2019s LLFF and\nfull-scene CO3D results are strong and show promise for more general types of video input, such as scene\nreconstruction from moving cameras (e.g., egocentric Grauman et al. (2022)).\n12\nReferences\nDejan Azinovi\u0107, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, and Justus Thies. Neural rgb-d surface\nreconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR),\npages 6290\u20136301, June 2022.\nPeter N Belhumeur, David J Kriegman, and Alan L Yuille. The bas-relief ambiguity. International journal of computer\nvision, 1999.\nPaul J. Besl and Neil D. McKay. A method for registration of 3-d shapes. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 14(2):239\u2013256, 1992. doi: 10.1109/34.121791. https://doi.org/10.1109/34.121791.\nWenjing Bian, Zirui Wang, Kejie Li, Jiawang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural\nradiance field with no pose prior. 2023.\nMark Boss, Andreas Engelhardt, Abhishek Kar, Yuanzhen Li, Deqing Sun, Jonathan T. Barron, Hendrik P.A. Lensch,\nand Varun Jampani. SAMURAI: Shape And Material from Unconstrained Real-world Arbitrary Image collections.\nIn Advances in Neural Information Processing Systems (NeurIPS), 2022.\nMathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\nEmerging properties in self-supervised vision transformers. In Proceedings of the International Conference on\nComputer Vision (ICCV), 2021.\nShuo Cheng, Caelan Garrett, Ajay Mandlekar, and Danfei Xu. NOD-TAMP: Multi-step manipulation planning with\nneural object descriptors. In Towards Generalist Robots: Learning Paradigms for Scalable Skill Acquisition @\nCoRL2023, 2023. https://openreview.net/forum?id=43MSbj5mSS.\nShin-Fang Chng, Sameera Ramasinghe, Jamie Sherrah, and Simon Lucey. Gaussian activated neural radiance fields for\nhigh fidelity reconstruction and pose estimation. In The European Conference on Computer Vision: ECCV, 2022.\nAngela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet:\nRichly-annotated 3d reconstructions of indoor scenes. Computer Vision and Pattern Recognition (CVPR), pages\n5828\u20135839, 2017. doi: 10.1109/CVPR.2017.618. http://www.scan-net.org/.\nDavison.\nReal-time simultaneous localisation and mapping with a single camera.\nIn Proceedings Ninth IEEE\nInternational Conference on Computer Vision, pages 1403\u20131410. IEEE, 2003.\nAndrew J Davison, Ian D Reid, Nicholas D Molton, and Olivier Stasse. Monoslam: Real-time single camera slam.\nIEEE transactions on pattern analysis and machine intelligence, 29(6):1052\u20131067, 2007.\nDaniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection\nand description. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW),\npages 337\u201333712, 2017. https://api.semanticscholar.org/CorpusID:4918026.\nJakob Engel, Thomas Sch\u00f6ps, and Daniel Cremers. Lsd-slam: Large-scale direct monocular slam. In Computer\nVision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part II 13,\npages 834\u2013849. Springer, 2014.\nJakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis\nand machine intelligence, 40(3):611\u2013625, 2017.\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson\nHamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar\nRamakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao,\nSiddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph\nFeichtenhofer, Adriano Fragomeni, Qichen Fu, Abrham Gebreselasie, Cristina Gonz\u00e1lez, James Hillis, Xuhua Huang,\nYifei Huang, Wenqi Jia, Weslie Khoo, J\u00e1chym Kol\u00e1\u0159, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li,\nYanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi\nNishiyasu, Will Price, Paola Ruiz, Merey Ramazanova, Leda Sari, Kiran Somasundaram, Audrey Southerland,\nYusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang, Xindi Wu, Takuma Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbel\u00e1ez,\nDavid Crandall, Dima Damen, Giovanni Maria Farinella, Christian Fuegen, Bernard Ghanem, Vamsi Krishna Ithapu,\nC. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe, Aude Oliva, Hyun Soo Park, James M.\nRehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra\nMalik. Ego4d: Around the world in 3,000 hours of egocentric video. In Computer Vision and Pattern Recognition,\n2022.\n13\nShreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit. Honnotate: A method for 3d annotation of\nhand and object poses. In Computer Vision and Pattern Recognition, 2020.\nRichard Hartley and Andrew Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press,\nUSA, 2 edition, 2003. ISBN 0521540518.\nYoonwoo Jeong, Seokjun Ahn, Christopher Choy, Anima Anandkumar, Minsu Cho, and Jaesik Park. Self-calibrating\nneural radiance fields. In International Conference on Computer Vision, 2021.\nDaniel Kappler, Franziska Meier, Jan Issac, Jim Mainprice, Cristina Garcia Cifuentes, Manuel W\u00fcthrich, Vincent\nBerenz, Stefan Schaal, Nathan Ratliff, and Jeannette Bohg. Real-time perception meets reactive motion generation.\nIEEE Robotics and Automation Letters, 3(3):1864\u20131871, 2018. doi: 10.1109/LRA.2018.2795645.\nGeorg Klein and David Murray. Parallel tracking and mapping for small ar workspaces. In 2007 6th IEEE and ACM\ninternational symposium on mixed and augmented reality, pages 225\u2013234. IEEE, 2007.\nZhengfei Kuang, Kyle Olszewski, Menglei Chai, Zeng Huang, Panos Achlioptas, and Sergey Tulyakov. Neroic: Neural\nrendering of objects from online image collections. ACM Trans. Graph., 41(4), jul 2022. ISSN 0730-0301. doi:\n10.1145/3528223.3530177. https://doi.org/10.1145/3528223.3530177.\nYann Labb\u00e9, Justin Carpentier, Mathieu Aubry, and Josef Sivic. Cosypose: Consistent multi-view multi-object 6d\npose estimation. In European Conference on Computer Vision, 2020.\nYann Labb\u00e9, Lucas Manuelli, Arsalan Mousavian, Stephen Tyree, Stan Birchfield, Jonathan Tremblay, Justin Carpentier,\nMathieu Aubry, Dieter Fox, and Josef Sivic. Megapose: 6d pose estimation of novel objects via render & compare.\narXiv preprint arXiv:2212.06870, 2022.\nChen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. Barf: Bundle-adjusting neural radiance fields. In\nIEEE International Conference on Computer Vision (ICCV), 2021.\nYunzhi Lin, Thomas M\u00fcller, Jonathan Tremblay, Bowen Wen, Stephen Tyree, Alex Evans, Patricio A. Vela, and Stan\nBirchfield. Parallel inversion of neural radiance fields for robust pose estimation. In ICRA, 2023.\nYuan Liu, Yilin Wen, Sida Peng, Cheng Lin, Xiaoxiao Long, Taku Komura, and Wenping Wang. Gen6d: Generalizable\nmodel-free 6-dof object pose estimation from rgb images. In European Conference on Computer Vision, pages\n298\u2013315. Springer, 2022.\nWilliam E. Lorensen and Harvey E. Cline. Marching cubes: A high-resolution 3d surface construction algorithm.\nComputer Graphics, 21(4):163\u2013169, 1987. doi: 10.1145/37402.37422. https://doi.org/10.1145/37402.37422.\nDavid G. Lowe. Object recognition from local scale-invariant features. International Conference on Computer Vision\n(ICCV), pages 1150\u20131157, 1999. doi: 10.1109/ICCV.1999.790410. https://www.cs.ubc.ca/~lowe/papers/iccv99.pdf.\nEric Marchand, Hideaki Uchiyama, and Fabien Spindler. Pose estimation for augmented reality: A hands-on survey.\nIEEE Transactions on Visualization and Computer Graphics, 22(12):2633\u20132651, 2016. doi: 10.1109/TVCG.2015.\n2513408.\nJohn McCormac, Ronald Clark, Michael Bloesch, Andrew Davison, and Stefan Leutenegger. Fusion++: Volumetric\nobject-level slam. In 2018 international conference on 3D vision (3DV), pages 32\u201341. IEEE, 2018.\nQuan Meng, Anpei Chen, Haimin Luo, Minye Wu, Hao Su, Lan Xu, Xuming He, and Jingyi Yu. GNeRF: GAN-based\nNeural Radiance Field without Posed Camera. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision (ICCV), 2021.\nNathaniel Merrill, Yuliang Guo, Xingxing Zuo, Xinyu Huang, Stefan Leutenegger, Xi Peng, Liu Ren, and Guoquan\nHuang. Symmetry and uncertainty-aware object slam for 6dof object pose estimation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14901\u201314910, 2022.\nBen Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng,\nand Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM\nTransactions on Graphics (TOG), 38(4):1\u201314, 2019.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view synthesis. In European Conference on Computer Vision, 2020.\nNorman Muller, Yu-Shiang Wong, Niloy J Mitra, Angela Dai, and Matthias Nie\u00dfner. Seeing behind objects for 3d\nmulti-object tracking in rgb-d sequences. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 6071\u20136080, 2021.\n14\nJacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas M\u00fcller, and Sanja\nFidler. Extracting Triangular 3D Models, Materials, and Lighting From Images. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR), pages 8280\u20138290, June 2022.\nRaul Mur-Artal and Juan D Tard\u00f3s. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras.\nIEEE transactions on robotics, 33(5):1255\u20131262, 2017.\nRaul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. Orb-slam: a versatile and accurate monocular\nslam system. IEEE transactions on robotics, 31(5):1147\u20131163, 2015.\nMichael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for\nmulti-view reconstruction. In International Conference on Computer Vision (ICCV), 2021.\nKeunhong Park, Arsalan Mousavian, Yu Xiang, and Dieter Fox. Latentfusion: End-to-end differentiable reconstruction\nand rendering for unseen object pose estimation. In Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 10710\u201310719, 2020.\nKarl Pauwels and Danica Kragic. Simtrack: A simulation-based framework for scalable real-time object pose detection\nand tracking. In International Conference on Intelligent Robots and Systems, 2015.\nHaozhi Qi, Brent Yi, Sudharshan Suresh, Mike Lambeta, Yi Ma, Roberto Calandra, and Jitendra Malik. General\nin-hand object rotation with vision and touch.\nIn 7th Annual Conference on Robot Learning, 2023.\nhttps:\n//openreview.net/forum?id=RN00jfIV-X.\nJeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David Novotny.\nCommon objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction. In International\nConference on Computer Vision, 2021.\nAntoni Rosinol, John J Leonard, and Luca Carlone. Nerf-slam: Real-time dense monocular slam with neural radiance\nfields. arXiv preprint arXiv:2210.13641, 2022.\nMartin Runz, Maud Buffier, and Lourdes Agapito. Maskfusion: Real-time recognition, tracking and reconstruction of\nmultiple moving objects. In 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pages\n10\u201320. IEEE, 2018.\nRenato F Salas-Moreno, Richard A Newcombe, Hauke Strasdat, Paul HJ Kelly, and Andrew J Davison. Slam++:\nSimultaneous localisation and mapping at the level of objects. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 1352\u20131359, 2013.\nPaul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical\nlocalization at large scale. In CVPR, 2019.\nPaul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature\nmatching with graph neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 4938\u20134947, 2020.\nJohannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages 4104\u20134113, 2016.\nAkash Sharma, Wei Dong, and Michael Kaess. Compositional and scalable object slam. In 2021 IEEE International\nConference on Robotics and Automation (ICRA), pages 11626\u201311632. IEEE, 2021.\nCameron Smith, Yilun Du, Ayush Tewari, and Vincent Sitzmann. Flowcam: Training generalizable 3d radiance fields\nwithout camera poses via pixel-aligned scene flow, 2023.\nManuel Stoiber, Martin Sundermeyer, and Rudolph Triebel. Iterative corresponding geometry: Fusing region and depth\nfor highly efficient 3d tracking of textureless objects. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6855\u20136865, 2022.\nJiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. NeuralRecon: Real-time coherent 3D\nreconstruction from monocular video. CVPR, 2021.\nMartin Sundermeyer, Zoltan-Csaba Marton, Maximilian Durner, Manuel Brucker, and Rudolph Triebel. Implicit 3d\norientation learning for 6d object detection from rgb images. In Proceedings of the european conference on computer\nvision (ECCV), pages 699\u2013715, 2018.\nZachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In Computer Vision \u2013\nECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part II, page 402\u2013419,\n15\nBerlin, Heidelberg, 2020. Springer-Verlag. ISBN 978-3-030-58535-8. doi: 10.1007/978-3-030-58536-5_24. https:\n//doi.org/10.1007/978-3-030-58536-5_24.\nZachary Teed and Jia Deng. DROID-SLAM: Deep Visual SLAM for Monocular, Stereo, and RGB-D Cameras. Advances\nin neural information processing systems, 2021.\nPrune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from\nsparse and noisy poses. In Computer Vision and Pattern Recognition, 2023.\nVadim Tschernezki, Diane Larlus, and Andrea Vedaldi. NeuralDiff: Segmenting 3D objects that move in egocentric\nvideos. In Proceedings of the International Conference on 3D Vision (3DV), 2021.\nShinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions\non Pattern Analysis & Machine Intelligence, 13(04):376\u2013380, 1991.\nHe Wang, Srinath Sridhar, Jingwei Huang, Julien Valentin, Shuran Song, and Leonidas J Guibas. Normalized object\ncoordinate space for category-level 6d object pose and size estimation. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 2642\u20132651, 2019.\nJianyuan Wang, Christian Rupprecht, and David Novotny. Posediffusion: Solving pose estimation via diffusion-aided\nbundle adjustment. In International Conference on Computer Vision, 2023.\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural\nimplicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint arXiv:2106.10689, 2021a.\nWenshan Wang, Yaoyu Hu, and Sebastian Scherer. Tartanvo: A generalizable learning-based vo. 2020.\nZirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. NeRF\u2212\u2212: Neural radiance fields\nwithout known camera parameters. arXiv preprint arXiv:2102.07064, 2021b.\nBowen Wen and Kostas Bekris. Bundletrack: 6d pose tracking for novel objects without instance or category-level 3d\nmodels. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), page 8067\u20138074.\nIEEE Press, 2021. doi: 10.1109/IROS51168.2021.9635991. https://doi.org/10.1109/IROS51168.2021.9635991.\nBowen Wen, Wenzhao Lian, Kostas Bekris, and Stefan Schaal. Catgrasp: Learning category-level task-relevant grasping\nin clutter from simulation. ICRA 2022, 2022a.\nBowen Wen, Wenzhao Lian, Kostas E. Bekris, and Stefan Schaal.\nYou only demonstrate once: Category-level\nmanipulation from single visual demonstration. ArXiv, abs/2201.12716, 2022b. https://api.semanticscholar.org/\nCorpusID:246430152.\nBowen Wen, Jonathan Tremblay, Valts Blukis, Stephen Tyree, Thomas Muller, Alex Evans, Dieter Fox, Jan Kautz,\nand Stan Birchfield. Bundlesdf: Neural 6-dof tracking and 3d reconstruction of unknown objects. Computer Vision\nand Pattern Recognition, 2023.\nYitong Xia, Hao Tang, Radu Timofte, and Luc Van Gool. Sinerf: Sinusoidal neural radiance fields for joint pose\nestimation and scene reconstruction. In 33rd British Machine Vision Conference 2022, BMVC 2022, London, UK,\nNovember 21-24, 2022. BMVA Press, 2022. https://bmvc2022.mpi-inf.mpg.de/0131.pdf.\nYu Xiang, Tanner Schmidt, Venkatraman Narayanan, and Dieter Fox. Posecnn: A convolutional neural network for 6d\nobject pose estimation in cluttered scenes. In Robotics: Science and Systems (RSS), 2018.\nJinyu Yang, Mingqi Gao, Zhe Li, Shang Gao, Fangjing Wang, and Feng Zheng. Track anything: Segment anything\nmeets videos, 2023.\nLior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview\nneural surface reconstruction by disentangling geometry and appearance. Advances in Neural Information Processing\nSystems, 33, 2020.\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In Thirty-Fifth\nConference on Neural Information Processing Systems, 2021.\nLin Yen-Chen, Pete Florence, Jonathan T. Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. iNeRF:\nInverting neural radiance fields for pose estimation. In IEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), 2021.\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few\nimages. In CVPR, 2021.\n16\nJason Y. Zhang, Gengshan Yang, Shubham Tulsiani, and Deva Ramanan. NeRS: Neural reflectance surfaces for\nsparse-view 3d reconstruction in the wild. In Conference on Neural Information Processing Systems, 2021.\nRichard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 586\u2013595, 2018. doi: 10.1109/CVPR.2018.00068.\nZichao Zhang and Davide Scaramuzza. A tutorial on quantitative trajectory evaluation for visual (-inertial) odometry.\nIn 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 7244\u20137251. IEEE,\n2018.\nWang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense point\ntrajectories for localizing moving cameras in the wild. In European conference on computer vision (ECCV), 2022.\nJon Zubizarreta, Iker Aguinaga, and Jose Maria Martinez Montiel. Direct sparse mapping. IEEE Transactions on\nRobotics, 36(4):1363\u20131370, 2020.\n17\nAppendix\nA\nPer-scene performance breakdown\nWe expand ICON results presented in main paper in section3 on CO3D full scene, CO3D object-only and\nHO3D Hampali et al. (2020) to document per-scene performance. Results are summarized in Tab. 6, Tab. 7\nand Tab. 8.\nCategory\nScene\nATE ATErot PSNR SSIM LPIPS\napple\n189_20393_38136\n0.027\n0.09\n24.83\n0.74\n0.32\nball\n123_14363_28981\n0.454\n2.31\n16.43\n0.43\n0.74\nbench\n415_57121_110109 0.002\n0.12\n26.03\n0.69\n0.33\nbook\n247_26469_51778\n0.219\n1.41\n26.79\n0.76\n0.30\nbowl\n69_5376_12833\n0.338\n2.02\n15.33\n0.35\n0.68\nbroccoli\n372_41112_81867\n0.022\n0.14\n26.40\n0.79\n0.35\ncake\n374_42274_84517\n0.040\n0.31\n23.85\n0.76\n0.26\nhydrant\n167_18184_34441\n0.092\n0.69\n19.05\n0.54\n0.49\nmouse\n377_43416_86289\n0.240\n1.33\n22.33\n0.71\n0.36\norange\n374_42196_84367\n0.200\n3.86\n24.71\n0.80\n0.35\nplant\n247_26441_50907\n0.190\n1.95\n16.30\n0.43\n0.59\nremote\n350_36761_68623\n0.043\n0.28\n27.08\n0.66\n0.42\nskateboard 245_26182_52130\n0.061\n0.34\n21.37\n0.67\n0.58\nsuitcase\n109_12965_23647\n0.110\n1.37\n17.77\n0.61\n0.48\nteddybear 34_1479_4753\n0.050\n0.55\n24.08\n0.76\n0.32\ntoaster\n372_41229_82130\n0.240\n2.57\n20.11\n0.53\n0.50\ntoytrain\n240_25394_51994\n0.170\n1.92\n19.08\n0.66\n0.49\ntoytruck\n190_20494_39385\n0.010\n0.17\n27.39\n0.87\n0.15\nAvg\n0.138\n1.16\n22.24\n0.65\n0.43\nTable 6 Per-scene performance of ICON on CO3D full scene evaluation.\nCategory\nScene\nATE ATErot PSNR SSIM LPIPS\napple\n189_20393_38136\n0.255\n1.70\n26.59\n0.95\n0.06\nball\n123_14363_28981\n0.450\n2.54\n20.27\n0.93\n0.09\nbench\n415_57121_110109 0.183\n1.22\n24.26\n0.80\n0.19\nbook\n247_26469_51778\n0.174\n1.36\n24.24\n0.89\n0.13\nbowl\n69_5376_12833\n0.637\n4.66\n16.91\n0.94\n0.09\nbroccoli\n372_41112_81867\n0.201\n1.65\n24.63\n0.93\n0.09\ncake\n374_42274_84517\n0.058\n0.46\n21.53\n0.91\n0.12\nhydrant\n167_18184_34441\n0.150\n1.05\n23.86\n0.92\n0.12\nmouse\n377_43416_86289\n0.420\n7.09\n15.93\n0.80\n0.31\norange\n374_42196_84367\n0.387\n3.84\n29.34\n0.98\n0.02\nplant\n247_26441_50907\n0.075\n0.62\n18.28\n0.75\n0.27\nremote\n350_36761_68623\n0.109\n0.71\n25.38\n0.94\n0.09\nskateboard 245_26182_52130\n0.194\n1.50\n19.51\n0.81\n0.18\nsuitcase\n109_12965_23647\n0.082\n0.78\n21.17\n0.89\n0.18\nteddybear 34_1479_4753\n0.053\n0.42\n24.56\n0.91\n0.10\ntoaster\n372_41229_82130\n0.225\n1.01\n20.79\n0.94\n0.10\ntoytrain\n240_25394_51994\n0.159\n1.19\n20.35\n0.83\n0.18\ntoytruck\n190_20494_39385\n0.066\n0.68\n26.46\n0.95\n0.05\nAvg\n0.215\n1.80\n22.45\n0.89\n0.13\nTable 7 Per-scene performance of ICON on CO3D object-only evaluation.\n18\nATE\nATErot\nTrans\nPSNR\nCD(cm)\nSiS1\n0.028\n3.80\n0.017\n19.13\n0.23\nMC1\n0.019\n5.90\n0.049\n14.24\n0.41\nABF13\n0.064\n10.67\n0.094\n11.79\n1.72\nGPMF12\n0.029\n11.23\n0.056\n16.27\n0.38\nND2\n0.027\n7.18\n0.015\n20.06\n0.50\nSM2\n0.026\n5.56\n0.032\n13.51\n0.85\nSMu1\n0.017\n13.19\n0.081\n14.46\n1.02\nAP13\n0.058\n7.06\n0.046\n20.42\n0.50\nAvg\n0.033\n8.07\n0.049\n16.24\n0.70\nTable 8 Per-scene performance of ICON on HO3D evaluation. CD stands for Chamfer Distance, measuring mesh\nquality.\nB\nEvaluating ICON on other CO3D categories\nIn this section, we supplement the results reported in the main paper on CO3D Reizenstein et al. (2021). We\nadd a study using all the remaining 33 categories from CO3D and evaluate on the full scene. This makes it\npossible for us to include symmetric objects such as vase whose poses are indistinguishable in the object-only\nevaluation. Since no official subset is specified for these categories, we take top-4 instances from each category\nwith highest camera pose confidence and randomly sample one instance for each category. It is worth noting\nthat the \u201cground-truth\" camera poses are estimated by COLMAP, and may not be 100% accurate, especially\nthese categories are not part of the official benchmarking sets. We use the same (hyper-)parameters as the\nmain paper benchmarking on the 18 categories.\nWe report the results in Tab 9. We observe that most objects achieve similar results as Tab 6. However, there\nare a few objects where ICON yields imprecise poses, dragging down the average metrics. We believe there\nare two causes. First, ICON relies on photometric loss and may suffer from changes in the scenes. Many of\nthe scenes where ICON has \u2265 3 degree rotation error have moving shadows (either object or human), strong\nlighting change (from the builtin flash of the camera) or reflective surfaces. We show a few examples here in\nFig. 6. Second, the groundtruth poses used to evaluate the trajectory are generated by COLMAP, which may\nnot be accurate, especially the categories not included in the official benchmarking sets.\nC\nEvaluation on ScanNet\nICON focuses our study on object-centric videos such as CO3D and HO3D. However, ICON does not apply\nspecific design tailored towards object that prevents it to work on other types of videos. Here, we include a\npreliminary study by benchmarking ICON on ScanNet Dai et al. (2017). We randomly sample 10 out of 20\nscenes in ScanNet test set and use a clip of 200 frames with a stride of 2. Scenes with NaN value in camera\nposes are removed when we sample scenes.\nWe report camera pose quality following prior works Zhao et al. (2022) using Relative Pose Error (RPE) on\nrotation and Absolute Trajectory Error (ATE (m)) for translation. We follow Zhao et al. (2022) to not use\nATErot because some trajectories in ScanNet has very small translation and aligning the trajectory then\nevaluate rotation may not be reliable.\nWe do not change any (hyper-)parameters used in CO3D full scene training for ICON to stress test the system\non the significantly different scenarios in ScanNet. We include four methods designed to work well on ScanNet\nfor comparison: TartanVO Wang et al. (2020), COLMAP Schonberger and Frahm (2016), DROID-SLAM Teed\nand Deng (2021) and current state-of-the-art method ParticleSfM Zhao et al. (2022). We note that COLMAP\nand ParticleSfM may fail to perform well when running only on the short clip, so we run them on the entire\nvideo and report the results on the clip. In addition, as noted in Zhao et al. (2022), since COLMAP often fail\non many ScanNet scenes, we use a tuned version following Tschernezki et al. (2021).\nWe report results in Tab 10. Despite having no tuning or change when transferring from CO3D, ICON\nachieves strong performance on ScanNet compared to the state-of-the-art methods designed to work well on\n19\nFigure 6 Scenes where ICON produces larger errors. ICON mainly suffer from scenes where photometric loss produces\ninconsistent supervisions. The car example consists of moving human shadow and reflective surface on the car. The\nwineglass example contains transparent surface and light reflections. The donut example contains inconsistent lighting,\nwhere the flash from the camera generates brighter color in the front and darken the back part. These inconsistencies\nin different viewpoints cause ICON to produce imprecise camera poses.\n20\nCategory\nScene\nATE ATErot PSNR SSIM LPIPS\nbackpack\n506_72977_141839\n0.060\n0.42\n20.74\n0.59\n0.42\nbanana\n612_97867_196978\n1.691\n11.23\n13.04\n0.15\n0.81\nbaseballbat\n375_42661_85494\n0.791\n7.83\n13.92\n0.61\n0.68\nbaseballglove 350_36909_69272\n0.054\n0.72\n20.52\n0.43\n0.62\nbicycle\n62_4324_10701\n0.700\n5.94\n15.22\n0.19\n0.69\nbottle\n589_88280_175252\n0.098\n1.18\n29.59\n0.76\n0.38\ncar\n439_62880_124254\n0.765\n4.43\n11.40\n0.32\n0.87\ncarrot\n372_40937_81628\n0.873\n2.17\n20.86\n0.63\n0.44\ncellphone\n76_7569_15872\n4.725\n19.55\n13.26\n0.30\n0.85\nchair\n455_64283_126636\n0.009\n0.28\n22.77\n0.73\n0.27\ncouch\n427_59830_115190\n0.140\n1.64\n25.67\n0.84\n0.29\ncup\n44_2241_6750\n0.453\n2.47\n23.50\n0.60\n0.49\ndonut\n403_52964_103416\n2.248\n11.89\n17.60\n0.74\n0.57\nfrisbee\n339_35238_64092\n0.738\n3.75\n22.34\n0.43\n0.66\nhairdryer\n378_44249_88180\n0.022\n0.16\n25.84\n0.82\n0.33\nhandbag\n406_54390_105616\n0.273\n2.32\n26.51\n0.89\n0.26\nhotdog\n618_100797_202003 2.600\n7.23\n19.78\n0.45\n0.78\nkeyboard\n375_42606_85350\n1.596\n7.04\n18.54\n0.46\n0.60\nkite\n428_60143_116852\n0.029\n0.36\n18.01\n0.30\n0.74\nlaptop\n378_44295_88252\n1.128\n7.92\n15.04\n0.36\n0.59\nmicrowave\n504_72519_140728\n0.023\n0.45\n21.17\n0.61\n0.42\nmotorcycle\n367_39692_77422\n0.006\n0.14\n26.52\n0.78\n0.30\nparkingmeter 483_69196_135585\n0.136\n2.48\n17.24\n0.56\n0.56\npizza\n372_41288_82251\n0.036\n0.26\n27.70\n0.69\n0.42\nsandwich\n366_39376_76719\n0.411\n1.67\n19.74\n0.53\n0.51\nstopsign\n617_99969_199015\n3.229\n13.81\n13.99\n0.40\n0.72\ntoilet\n605_94579_188112\n0.252\n5.48\n18.53\n0.69\n0.41\ntoybus\n273_29204_56363\n0.057\n0.40\n23.34\n0.65\n0.60\ntoyplane\n405_53880_105088\n0.020\n0.12\n22.20\n0.53\n0.53\ntv\n48_2742_8095\n0.097\n0.81\n26.32\n0.81\n0.39\numbrella\n191_20630_39388\n1.115\n5.73\n17.35\n0.44\n0.60\nvase\n374_41862_83720\n0.100\n1.27\n29.25\n0.85\n0.28\nwineglass\n401_51903_101703\n1.191\n7.80\n21.43\n0.58\n0.53\nAvg\n0.778\n4.21\n20.57\n0.57\n0.53\nTable 9 Per-scene performance of ICON on other 33 categories in CO3D full-scene evaluation.\nScanNet style videos. We believe this is a proof-of-concept that ICON can be generalized and adapted to\nother types of videos.\nD\nLimitations and future directions\nWhile ICON achieves strong performance to jointly optimize poses and NeRF, it has a few limitations.\nFirst, ICON strongly relies on photometric loss as supervision for both NeRF and poses. This relies on the\nassumption that the color is moderately consistent across different viewpoints. However, this assumption may\nbreak in real-world. Although ICON uses confidence to down-weight volumes with inconsistent photometric\nloss, it will produce imprecise poses (5 to 10 degree rotation error) due to the ambiguity. As shown in Tab 9\nand Fig 6, ICON suffers from motion, reflective surfaces, transparency and strong lighting change. We believe\nleveraging features robust to these changes, such as DINO Caron et al. (2021), may help alleviate this issue.\nIn addition, ICON depends on gradient-based optimization through NeRF Mildenhall et al. (2020), which\ntakes hours to train. We believe that combining ICON with more efficient modeling of 3-space will be a\npromising direction, such as PixelNeRF Yu et al. (2021) and FLOW-CAM Smith et al. (2023).\n21\nTartanVO DROID COLMAP ParticleSfM ICON\nRPE(degree)\n1.41\n0.56\n0.67\n0.34\n0.47\nATE(m)\n0.198\n0.066\n0.091\n0.053\n0.092\nTable 10 Camera pose evaluation on ScanNet. Despite not optimized for ScanNet scenarios, ICON achieves competitive\nperformance, ranking the second on RPE and third on ATE. The difference between ICON and state-of-the-art method\nis very small (0.13 degree on rotation and 0.039m on translation)\n22\n"
  }
]