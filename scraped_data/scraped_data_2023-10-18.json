[
  {
    "title": "BitNet: Scaling 1-bit Transformers for Large Language Models",
    "link": "https://arxiv.org/pdf/2310.11453.pdf",
    "upvote": "87",
    "text": "BitNet: Scaling 1-bit Transformers for\nLarge Language Models\nHongyu Wang\u2217\u2020\u2021\nShuming Ma\u2217\u2020\nLi Dong\u2020\nShaohan Huang\u2020\nHuaijie Wang\u00a7\nLingxiao Ma\u2020\nFan Yang\u2020\nRuiping Wang\u2021\nYi Wu\u00a7\nFuru Wei\u2020\u22c4\n\u2020 Microsoft Research\n\u2021 University of Chinese Academy of Sciences\n\u00a7 Tsinghua University\nhttps://aka.ms/GeneralAI\nAbstract\nThe increasing size of large language models has posed challenges for deployment\nand raised concerns about environmental impact due to high energy consumption.\nIn this work, we introduce BitNet, a scalable and stable 1-bit Transformer architec-\nture designed for large language models. Specifically, we introduce BitLinear as\na drop-in replacement of the nn.Linear layer in order to train 1-bit weights from\nscratch. Experimental results on language modeling show that BitNet achieves\ncompetitive performance while substantially reducing memory footprint and en-\nergy consumption, compared to state-of-the-art 8-bit quantization methods and\nFP16 Transformer baselines. Furthermore, BitNet exhibits a scaling law akin to\nfull-precision Transformers, suggesting its potential for effective scaling to even\nlarger language models while maintaining efficiency and performance benefits.\nTraining from Scratch\nPost-Training Quantization\n16\n8\n4\n2 1\nInference WBits\n16\n8\n4\n1\nTraining WBits\nFP16/BF16\nFP8\nGPTQ QuIP\nSmoothQuant\nBitNet\n8\n4\n2\n1\nWeight Bits\n42\n46\n50\n54\nAccuracy\nBitNet\nRandom\nSoTA Post-Training Quant\nBitNet (Training from Scratch)\n100M\n1B\n10B\n30B\nModel Size\n10x\n20x\n30x\n40x\nEnergy Cost Reduction Ratio\n(Compared with FP16 TRM)\n8.8x\n11.0x\n15.0x\n18.6x\n21.7x\n29.1x\n32.9x\n38.8x\nBitNet\n100M\n1B\n10B\n30B\n100B\nModel Size\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\nLoss\nL = 0.09\nBitNet\nFP16 Transformer\nFigure 1: BitNet trains 1-bit Transformers from scratch, obtaining competitive results in an energy-\nefficient way. BitNet significantly outperforms state-of-the-art quantization methods. As the model\nsize scales up, the cost savings become more significant while achieving competitive performance\nwith the models trained with FP16.\n\u2217 Equal contribution. \u22c4 Corresponding author.\narXiv:2310.11453v1  [cs.CL]  17 Oct 2023\n\u201c\nI don\u2019t think there\u2019s anything unique about human intelligence. All the neurons in the brain\nthat make up perceptions and emotions operate in a binary fashion.\nWilliam Henry Gates III\u201d\n1\nIntroduction\nThe rapid growth of large language models [BMR+20, Ope23, CND+22, ADF+23, TLI+23,\nTMS+23] has led to significant improvements in various tasks. However, it is expensive to host\nlarge language models due to the high inference costs and energy consumption. As the size of\nthese models grows, the memory bandwidth required for accessing and processing the model pa-\nrameters becomes a major bottleneck, limiting the overall inference performance. Moreover, when\ndeploying these models on distributed systems or multi-device platforms, the inter-device commu-\nnication overhead can significantly impact the inference latency and energy consumption. Model\nquantization [FAHA23, CCKS23, XLS+23] has emerged as a promising solution, as it can signifi-\ncantly reduce the memory footprint and computational cost of large-scale models while maintaining\ncompetitive performance.\nMost existing quantization approaches for large language models are post-training. They are simple\nand easy to apply since it does not require any changes to the training pipeline or retraining the model.\nHowever, it will result in a more significant loss of accuracy especially when the precision goes lower,\nbecause the model is not optimized for the quantized representation during training.\nAnother strand of quantizing deep neural networks is quantization-aware training. Compared to\npost-training, it typically results in better accuracy, as the model is trained to account for the reduced\nprecision from the beginning. Moreover, it allows the model to continue-train or do fine-tuning, which\nis essential for large language models. The challenge of quantization-aware training mainly lies in\noptimization, i.e., the model becomes more difficult to converge as the precision goes lower. Besides,\nit is unknown whether quantization-aware training follows the scaling law of neural language models.\nIn this work, we focus on binarization (i.e., 1-bit), which is the extreme case of quantization, applied\nto large language models. Previous studies on binarized neural networks [RORF16, BT19] have\nmostly revolved around convolutional neural networks. Recently, there has been some research\non binarized Transformers. However, these studies have focused on machine translation or BERT\npretraining, which is quite different from large language models. For example, machine translation\nemploys an encoder-decoder architecture, BERT pretraining utilizes a bidirectional encoder, and\nlarge language models use a unidirectional decoder. Furthermore, large language models are typically\nscaled up to a much larger model size, while BERT and machine translation models do not undergo\nsuch extensive scaling.\nTo the best of our knowledge, this work is the first to investigate quantization-aware training for\n1-bit large language models. We propose BitNet, a 1-bit Transformer architecture for large language\nmodels, which aims to scale efficiently in terms of both memory and computation. BitNet employs\nlow-precision binary weights and quantized activations, while maintaining high precision for the\noptimizer states and gradients during training. Our approach is designed to be scalable and stable, with\nthe ability to handle large language models efficiently. The implementation of the BitNet architecture\nis quite simple, requiring only the replacement of linear projections (i.e., nn.Linear in PyTorch)\nin the Transformer. Furthermore, it complements other acceleration methods for large language\nmodels, such as PagedAttention [KLZ+23], FlashAttention [DFE+22, Dao23], and speculative\ndecoding [LKM23].\nWe evaluate BitNet on a range of language modeling benchmarks, comparing with state-of-the-art\nquantization methods and FP16 Transformers. Experimental results demonstrate that BitNet achieves\ncompetitive performance in terms of both perplexity and downstream task accuracy. More importantly,\nBitNet significantly reduces memory footprint and energy consumption compared to the baselines.\nFurthermore, we show that BitNet follows a scaling law similar to that of full-precision Transformers,\nindicating that it can be effectively scaled to even larger language models with potential benefits in\nterms of performance and efficiency.\n2\nBitNet\nBitLinear\nInput\nLayerNorm\nAbsmax \nQuantization\n1-bit\nWeights\nOutput\n\ud835\udefd\n\ud835\udefe\nDequantization\nMulti-Head \nAttention\nFeed-Forward \nNetwork\n\ud835\udc3f-Layer\nInput\nOutput\nFigure 2: (a) The computation flow of BitLinear. (b) The architecture of BitNet, consisting of the\nstacks of attentions and FFNs, where matrix multiplication is implemented as BitLinear.\n2\nBitNet\nAs shown in Figure 2, BitNet uses the same layout as Transformers, stacking blocks of self-attention\nand feed-forward networks. Compared with vanilla Transformer, BitNet uses BitLinear (Eq. 11)\ninstead of conventional matrix multiplication, which employs binarized (i.e., 1-bit) model weights.\nWe leave the other components high-precision, e.g., 8-bit in our experiments. We summarized the\nreasons as follows. First, the residual connections and the layer normalization contribute negligible\ncomputation costs to large language models. Second, the computation cost of QKV transformation\nis much smaller than the parametric projection as the model grows larger. Third, we preserve the\nprecision for the input/output embedding because the language models have to use high-precision\nprobabilities to perform sampling.\n2.1\nBitLinear\nWe first binarize the weights to either +1 or \u22121 with the signum function. Following [LOP+22], we\ncentralize the weights to be zero-mean before binarization to increase the capacity within a limited\nnumerical range. A scaling factor \u03b2 is used after binarization to reduce the l2 error between the\nreal-valued and the binarized weights. The binarization of a weight W \u2208 Rn\u00d7m can be formulated\nas:\nf\nW = Sign(W \u2212 \u03b1),\n(1)\nSign(Wij) =\n\u001a + 1,\nif Wij > 0,\n\u2212 1,\nif Wij \u2264 0,\n(2)\n\u03b1 =\n1\nnm\nX\nij\nWij\n(3)\nWe further quantize the activations to b-bit precision. Following [DLBZ22], we use absmax quanti-\nzation, which scales activations into the range [\u2212Qb, Qb] (Qb = 2b\u22121) by multiplying with Qb and\ndividing by the absolute maximum of the input matrix:\nex = Quant(x) = Clip(x \u00d7 Qb\n\u03b3 , \u2212Qb + \u03f5, Qb \u2212 \u03f5),\n(4)\nClip(x, a, b) = max(a, min(b, x)),\n\u03b3 = ||x||\u221e,\n(5)\nwhere \u03f5 is a small floating-point number that prevents overflow when performing the clipping.\nFor the activations before the non-linear functions (e.g., ReLU), we scale them into the range [0, Qb]\nby subtracting the minimum of the inputs so that all values are non-negative:\n3\nex = Quant(x) = Clip((x \u2212 \u03b7) \u00d7 Qb\n\u03b3 , \u03f5, Qb \u2212 \u03f5),\n\u03b7 = min\nij xij.\n(6)\nIn this work, we quantize the activation to 8-bit and leave lower precision in future work. Moreover,\nthe quantization is performed per tensor during training while per token during inference for both\nstability and efficiency.\nWith the above quantization equations, the matrix multiplication can be written as:\ny = f\nW ex\n(7)\nWe assume that the elements in W and x are mutually independent and share the same distribution,\nand W and x are independent of each other. Then the variance of the output y is estimated as:\nVar(y) = nVar( ewex)\n(8)\n= nE[ ew2]E[ex2]\n(9)\n= n\u03b22E[ex2] \u2248 E[ex2]\n(10)\nFor the full-precision computation, the variance of the output Var(y) is at the scale of 1 with the\nstandard initialization methods (e.g., Kaiming initialization or Xavier initialization), which has a\ngreat benefit to the training stability. To preserve the variance after quantization, we introduce a\nLayerNorm [BKH16] function before the activation quantization. In this way, the variance of the\noutput y is then estimated as Var(y) \u2248 E[LN(ex)2] = 1, which has the same magnitude as the\nfull-precision counterpart Var(y). In the context of Transformers, it has the exact implementation as\nSubLN [WMH+22]. With SubLN and the quantization methods above, we have BitLinear, which\nis formulated as:\ny = f\nW ex = f\nW Quant(LN(x)) \u00d7 \u03b2\u03b3\nQb\n(11)\nLN(x) =\nx \u2212 E(x)\np\nVar(x) + \u03f5\n,\n\u03b2 =\n1\nnm||W||1\n(12)\nFigure 2 provides an illustration of the computation flow of BitLinear. After the SubLN operation,\nthe activations are quantized with the absmax function. The matrix multiplication is performed\nbetween the 1-bit weights and the quantized activations. The output activations are rescaled with\n{\u03b2, \u03b3} to dequantize them to the original precision.\nModel parallelism with Group Quantization and Normalization\nOne essential technique to scale\nup large language models is model parallelism [SPP+19], which partitions the matrix multiplication\non multiple devices. A prerequisite for the existing model parallelism approaches is that the tensors\nare independent along the partition dimension. However, all of the parameters \u03b1, \u03b2, \u03b3, and \u03b7 are\ncalculated from the whole tensors, breaking the independent prerequisite. One solution is to introduce\none all-reduce operation for each parameter. However, even though the communication for each\nparameter is small, the amount of synchronization is growing as the model becomes deeper, which\nsignificantly slows the forward pass. The problem also exists in SubLN, where the mean and the\nvariance should be estimated across the partition dimension.\nTo this end, we propose a simple approach that makes the model parallelism more efficient. We divide\nthe weights and activations into groups and then independently estimate each group\u2019s parameters.\nThis way, the parameters can be calculated locally without requiring additional communication. This\napproach, called Group Quantization, is formulated as follows:\nFor a weight matrix W \u2208 Rn\u00d7m, we divide it into G groups along the partition dimension, and each\ngroup has a size of n\nG \u00d7 m. We then estimate the parameters for each group independently:\n\u03b1g = G\nnm\nX\nij\nW (g)\nij ,\n\u03b2g = G\nnm||W (g)||1,\n(13)\n4\nModels\nSize\nWBits\n7nm Energy (J)\n45nm Energy (J)\nMUL\nADD\nMUL\nADD\nTransformer\n6.7B\n32\n4.41\n1.28\n12.46\n3.03\n16\n1.14\n0.54\n3.70\n1.35\nBitNet\n1\n0.02\n0.04\n0.08\n0.13\nTransformer\n13B\n32\n8.58\n2.49\n24.23\n5.89\n16\n2.23\n1.05\n7.20\n2.62\nBitNet\n1\n0.04\n0.06\n0.12\n0.24\nTransformer\n30B\n32\n20.09\n5.83\n56.73\n13.80\n16\n5.21\n2.45\n16.87\n6.13\nBitNet\n1\n0.06\n0.14\n0.20\n0.53\nTable 1: Energy consumption of BitNet and Transformer varying different model size. Results are\nreported with 512 as input length.\nwhere W (g) denotes the g-th group of the weight matrix. Similarly, for the activations, we can divide\nthe input matrix x \u2208 Rn\u00d7m into G groups and calculate the parameters for each group:\n\u03b3g = ||x(g)||\u221e,\n\u03b7g = min\nij x(g)\nij\n(14)\nFor LN, we can apply the group normalization technique [WH20] to compute the mean and variance\nfor each group independently:\nLN(x(g)) = x(g) \u2212 E(x(g))\np\nVar(x(g)) + \u03f5\n(15)\nIn this way, we can efficiently implement model parallelism with Group Quantization and Normaliza-\ntion, which requires no additional communication and can scale to large language models.\n2.2\nModel Training\nStraight-through estimator.\nTo train our 1-bit model, we employ the straight-through estimator\n(STE)[BLC13] to approximate the gradient during backpropagation. This method bypasses the non-\ndifferentiable functions, such as the Sign (Eq. 2) and Clip (Eq. 5) functions, during the backward pass.\nSTE allows gradients to flow through the network without being affected by these non-differentiable\nfunctions, making it possible to train our quantized model.\nMixed precision training.\nWhile the weights and the activations are quantized to low precision,\nthe gradients and the optimizer states are stored in high precision to ensure training stability and\naccuracy. Following the previous work [LSL+21], we maintain a latent weight in a high-precision\nformat for the learnable parameters to accumulate the parameter updates. The latent weights are\nbinarized on the fly during the forward pass and never used for the inference process.\nLarge learning rate.\nOne challenge for the optimization is that a small update on the latent weights\noften makes no difference in the 1-bit weights. This results in a biased gradient and update which are\nestimated based on the 1-bit weights. This problem is even worse at the beginning of the training,\nwhere the models are supposed to converge as fast as possible. To address this challenge, we explore\nvarious methods, concluding that increasing the learning rate is the simplest and best way to accelerate\nthe optimization. Our experiments show that BitNet benefits from a large learning rate in terms of\nconvergence, while the FP16 Transformer diverges at the beginning of training with the same learning\nrate. More details can be found in Section 3.\n5\n2.3\nComputational Efficiency\nWe estimate the computational efficiency of BitNet in terms of both arithmetic operations energy\nand memory footprint. We mainly focus on the calculation for the matrix multiplication, since it\ncontributes the most to the cost of large language models.\nArithmetic operations energy.\nAccording to the energy model in [Hor14, ZZL22], the energy\nconsumption for different arithmetic operations can be estimated as follows:\nBits\nADD Energy \u02c6Eadd (pJ)\nMUL Energy \u02c6Emul (pJ)\n45nm\n7nm\n45nm\n7nm\nFP32\n0.9\n0.38\n3.7\n1.31\nFP16\n0.4\n0.16\n1.1\n0.34\nINT8\n0.03\n0.007\n0.2\n0.07\nTable 2: ADD and MUL energy consumption [Hor14, ZZL22] for different bit representations at\n45nm and 7nm process nodes.\nIn vanilla Transformers, for matrix multiplication with dimensions m \u00d7 n and n \u00d7 p, the energy\nconsumption can be calculated as follows:\nEadd = m \u00d7 (n \u2212 1) \u00d7 p \u00d7 \u02c6Eadd\n(16)\nEmul = m \u00d7 n \u00d7 p \u00d7 \u02c6Emul\n(17)\nFor BitNet, the energy consumption of the matrix multiplication is dominated by the addition\noperations, as the weights are 1-bit. The multiplication operations are only applied to scale the output\nwith the scalars \u03b2 and\n\u03b3\nQb , so the energy consumption for multiplication can be computed as:\nEmul = (m \u00d7 p + m \u00d7 n) \u00d7 \u02c6Emul\n(18)\nwhich is significantly smaller than that in Transformers. The energy savings of W1A8 BitNet\ncompared to a full-precision (32-32) and half-precision (16-16) Transformer are shown in Table 1. As\ncan be seen, BitNet provides significant energy savings, especially for the multiplication operations,\nwhich are the major component of the matrix multiplication energy consumption.\n3\nComparison with FP16 Transformers\n3.1\nSetup\nWe train a series of autoregressive language models with BitNet of various scales, ranging from 125M\nto 30B. The models are trained on an English-language corpus, which consists of the Pile dataset,\nCommon Crawl snapshots, RealNews, and CC-Stories datasets. We use the Sentencpiece tokenizer\nto preprocess data and the vocabulary size is 16K. Besides BitNet, we also train the Transformer\nbaselines with the same datasets and settings for a fair comparison. More details can be found in the\nappendix.\n3.2\nInference-Optimal Scaling Law\nNeural language models have proven to scale predictably [KMH+20] with vanilla Transformer\narchitecture. The loss scales as the power law with the amount of computation used for training.\nThis allows us to determine the optimal allocation of a computation budget as well as predict the\nperformance of large language models from smaller models.\nTo study the scaling law of binarized Transformer, we start by plotting the scaling curve of both\nBitNet and the FP16 Transformer baseline against the parameter count. We fix the number of training\ntokens and vary the model sizes. Figure 3 shows that the loss scaling of BitNet is similar to the FP16\nTransformer, which follows a power-law. We then fit the scaling law with an irreducible loss term:\nL(N) = aN b + c\n(19)\n6\n1010\n1011\n1012\nInference Energy Consumption (pJ)\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\nLoss\nFP16 Transformer\nBitNet\n100M\n1B\n10B\n30B\n100B\nModel Size\n3.75\n4.00\n4.25\n4.50\n4.75\n5.00\n5.25\nLoss\nL = 0.09\nBitNet\nFP16 Transformer\nFigure 3: Scaling curves of BitNet and FP16 Transformers.\nTo evaluate whether the scaling law can accurately predict the loss, we choose the models from 125M\nto 6.7B to fit the parameters in the power-law and use the law to predict the loss of 13B and 30B. It\nshows that the fitted scaling law predicted BitNet\u2019s loss with high accuracy. Besides, the gap between\nBitNet and FP16 Transformer becomes smaller as the model size grows.\nWhile the power-law above measures the trend of the scaling of BitNet, it does not properly model\nthe relationship between the loss and the actual compute. Previous work [KMH+20, HKK+20,\nHBM+22] estimates the compute by calculating the FLOPs. However, it does not apply to 1-bit\nmodels whose cost is dominated by integer computation. Moreover, it mainly measures the training\ncomputation rather than the inference. To have a better understanding of the scaling efficiency of\nneural language models, we introduce Inference-Optimal Scaling Law. It predicts the loss against the\nenergy consumption. We focus on the inference energy cost as it scales with the usage of the model,\nwhile the training cost is only once. We estimate the energy consumption as in Section 2.3. Figure 3\nshows the scaling curve against the inference energy cost at 7nm process nodes. It proves that BitNet\nhas much higher scaling efficiency. Given a fixed computation budget, BitNet achieves a significantly\nbetter loss. Meanwhile, the inference cost is much smaller to get the same performance as the FP16\nmodels.\n3.3\nResults on Downstream Tasks\nIn addition to the loss, we are also concerned about the capabilities with the scaling of BitNet.\nCompared with the loss, the capacity is more difficult to predict due to the emergent nature of neural\nlanguage models. To evaluate the capabilities with the interpretable metrics, we test both the 0-shot\nand 4-shot results on four downstream tasks, including Hellaswag [ZHB+19], Winogrande [SBBC20],\nWinograd [LDM12], and Storycloze [MCH+16]. Figure 4 reports the average results of BitNet and\nFP16 Transformer with various scales. Similar to the loss scaling curve, the performance on the\ndownstream tasks can scale as the computation budget grows. Besides, the scaling efficiency of\ncapabilities is much higher than the FP16 Transformer baseline, in terms of both zero-shot and\nfew-shot performance.\n3.4\nStability Test\nThe major challenge for training low-bit Transformers is the stability in optimization. Therefore,\nwe perform stability tests for both BitNet and the FP16 baseline by training a series of models\nwith varying peak learning rates. Figure 5a illustrates the results of the stability test. It shows that\nBitNet can converge with a large learning rate while FP16 Transformer can not, demonstrating better\ntraining stability of BitNet. This advantage in optimization enables the training with larger learning\nrates. Figure 5b shows that BitNet can benefit from the increase in learning rate, achieving better\nconvergence in terms of PPL.\n7\n1010\n1011\n1012\nInference Energy Consumption (pJ)\n47.5\n50.0\n52.5\n55.0\n57.5\nAccuracy\nFP16 Transformer\nBitNet\n(a)\n1010\n1011\n1012\nInference Energy Consumption (pJ)\n47.5\n50.0\n52.5\n55.0\n57.5\nAccuracy\nFP16 Transformer\nBitNet\n(b)\nFigure 4: Zero-shot (Left) and few-shot (Right) performance of BitNet and FP16 Transformer against\nthe inference cost.\n1k\n2k\n3k\n4k\n5k\nSteps\n0\n200\n400\n600\n800\nPPL\nFP16 Transformer\nBitNet\n(a)\n10k\n20k\n30k\n40k\nSteps\n15\n25\n35\n45\n55\nPPL\nlr = 2e-4\nlr = 4e-4\nlr = 8e-4\n(b)\nFigure 5: BitNet is more stable than FP16 Transformer with a same learning rate (Left). The training\nstability enables BitNet a larger learning rate, resulting in better convergence (Right).\n4\nComparison with Post-training Quantization\n4.1\nSetup\nWe train BitNet with the same setup as described in Section 3.1. We compare BitNet with state-of-the-\nart quantization methods, including Absmax [DLBZ22], SmoothQuant [XLS+23], GPTQ [FAHA23],\nand QuIP [CCKS23]. These methods are post-training quantization over an FP16 Transformer model,\nwhich follows the same training setting and data as BitNet. Among them, Absmax and SmoothQuant\nquantize both the weights and the activations, while GPTQ and QuIP only reduce the precision of\nweights. We apply the methods to various quantization levels. For the weight-only quantization (i.e.,\nGPTQ and QuIP), we experiment with W4A16 and W2A16. For weight-and-activation quantization\n(i.e., Absmax and SmoothQuant), we use them to quantize the FP16 Transformers to W8A8, W4A4,\nand W1A8. Our implementation of BitNet is binary weight 8-bit activation (W1A8), which has lower\nor equal bits than the baselines.\n4.2\nResults\nTable 3 presents a detailed comparative analysis of the zero-shot performance of our proposed\nmethod, BitNet, against various baseline approaches on four benchmark datasets, namely Winogrande,\nWinograd, Storycloze, and Hellaswag. All models have the model sizes of 6.7B for a fair comparison.\n8\n\u0000\u0001\u0002\u0003\n\u0004\u0001\u0005\u0003\n\u0006\u0001\u0005\u0003\n\u0007\b\t\b\n\u000b\f\u000b\t\r\n\u000e\u000e\n\u000e\u0006\n\u000e\u000f\n\u0010\u0011\n\u0010\u0004\n\u0010\u000e\n\u0010\u0006\n\u0010\u000f\n\u0000\u0001\u0001\u0002\u0003\u0004\u0001\u0005\n\u0000\u0001\u0002\u0003\u0004\u0005\u0006\u0001\u0007\b\u0001\t\n\u000b\f\r\u000e\f\r\u000f\n\u0010\u0011\u0012\u0013\b\u0012\t\n\u000b\f\u000e\u0014\u000f\n\u0015\u0007\u0006\u0006\u0012\u0016\u0017\u0018\u0002\u0003\u0012\t\n\u000b\u0019\u000e\u0019\u000f\n\u0017\u0018\u001a\u001b\t\n\u000b\u001c\u000e\f\r\u000f\n\u001d\u001b\u0000\u0017\t\n\u000b\u001c\u000e\f\r\u000f\n\u001e\u0002\u0003\u001f\u0006\u0007\n1.3B\n2.7B\n6.7B\nParameters\n42\n46\n50\n54\n58\nAccuracy\n(a)\n1.3B\n2.7B\n6.7B\nParameters\n42\n46\n50\n54\n58\nAccuracy\n(b)\nFigure 6: Zero-shot (Left) and few-shot (Right) results for BitNet and the post-training quantization\nbaselines on downstream tasks.\nWBits\nMethods\nPTQ\nPPL\u2193\nWG\u2191\nWGe\u2191\nHS\u2191\nSC\u2191\nAvg\u2191\n-\nRandom\n\u2717\n-\n50.0\n50.0\n25.0\n50.0\n43.8\n16\nTransformer\n\u2717\n15.19\n66.7\n54.3\n42.9\n67.4\n57.8\n8\nAbsmax\n\u2713\n21.43\n60.4\n52.0\n38.3\n62.7\n53.4\nSmoothQuant\n\u2713\n15.67\n65.3\n53.1\n40.9\n67.6\n56.7\n4\nGPTQ\n\u2713\n16.05\n57.2\n51.2\n39.9\n63.4\n52.9\nAbsmax\n\u2713\n4.8e4\n55.8\n50.9\n25.0\n53.1\n46.2\nSmoothQuant\n\u2713\n1.6e6\n53.7\n48.3\n24.8\n53.6\n45.1\n2\nGPTQ\n\u2713\n1032\n51.6\n50.1\n25.8\n53.4\n45.2\nQuIP\n\u2713\n70.43\n56.1\n51.2\n30.3\n58.4\n49.0\n1\nAbsmax\n\u2713\n3.5e23\n49.8\n50.0\n24.8\n53.6\n44.6\nSmoothQuant\n\u2713\n3.3e21\n50.5\n49.5\n24.6\n53.1\n44.4\n1\nBitNet\n\u2717\n17.07\n66.3\n51.4\n38.9\n66.9\n55.9\nTable 3: Zero-shot results for BitNet and the baselines (PTQ: Post-training quantization, WGe: Wino-\ngrande, WG: Winograd, SC: Storycloze, and HS: Hellaswag dataset).\nThe methods are evaluated across several weight bit levels, spanning from 16 down to 1. Besides\nthe zero-shot accuracy on the downstream tasks, the evaluation metrics include language model\nperplexity on the validation set, which provides a comprehensive understanding of each method\u2019s\nperformance.\nThe results demonstrate the effectiveness of BitNet in achieving competitive performance levels\ncompared to the baseline approaches, particularly for lower bit levels. The zero-shot scores of BitNet\nare comparable with the 8-bit models, while the inference cost is much lower. For the 4-bit models, the\nweight-only quantization methods outperform the weight-and-activation quantizers, mainly because\nthe activation is more difficult to quantify. BitNet, as a 1-bit model, significantly achieves better\nresults than both the weight-and-activation quantization methods and the weight-only methods. As\nfor the lower-bit models, BitNet has consistently superior scores over all baselines. This proves\nthe advantages of the quantization-aware training approaches over the post-training quantization\nmethods. Figure 6 summarizes both the zero-shot accuracy and few-shot accuracy of our method\nand the baselines while scaling up the model size from 1.3B to 6.7B. It proves that the advantage is\nconsistent across different scales.\n9\nMethods\nPPL\u2193\nHS\u2191\nWGe\u2191\nWG\u2191\nSC\u2191\nAvg\u2191\nZero-Shot Learning\nBitNet\n20.34\n33.2\n52.1\n60.7\n63.2\n52.3\nElastic + Pre-LN\n24.05\n29.6\n52.9\n56.8\n61.3\n50.2\nAbsmax + Pre-LN\n22.11\n31.6\n50.0\n61.8\n61.6\n51.3\nAbsmax + BMT\n22.98\n31.2\n52.1\n60.4\n62.7\n51.6\nFew-Shot Learning\nBitNet\n20.34\n33.5\n50.4\n62.1\n63.8\n52.5\nElastic + Pre-LN\n24.05\n29.9\n51.7\n57.5\n61.1\n50.1\nAbsmax + Pre-LN\n22.11\n31.4\n51.9\n63.9\n61.6\n52.2\nAbsmax + BMT\n22.98\n31.3\n51.5\n57.5\n62.6\n50.7\nTable 4: Ablation of BitNet (WGe: Winogrande, WG: Winograd, SC: Storycloze, and HS: Hellaswag\ndataset). Elastic is an activation quantization method from [LOP+22], while BMT is the architecture\nfrom [ZGC+23] to stabilize the training of low-bit models.\n5\nAblation Studies\nIn Table 4, we present an ablation study of our compared with several alternative approaches. We\nablate the effect of our choices in activation quantization approaches as well as the techniques to\nstabilize the model training. BitNet implement absmax to quantize the activation and use SubLN for\ntraining stability. One quantization alternative is the elastic function [LOP+22], which dynamically\nadjusts the scales with learnable parameters. In our experiments, we find that absmax has better\nperformance than the elastic function. Besides, the absmax function leads to more stable training,\nwhich enables a larger learning rate for BitNet. We further compare SubLN with the Pre-LN and the\nBMT architecture [ZGC+23]. Pre-LN is the default architecture for GPT pertaining, while BMT has\nproven to improve the stability of binarized models. Our experiments show that SubLN outperforms\nboth Pre-LN and BMT. Therefore, we choose absmax and SubLN as the implementation in BitNet.\n6\nConclusion and Future Work\nWe present BitNet, a novel 1-bit Transformer architecture for large language models. Our approach\nis designed to be scalable and stable, with the ability to handle large language models efficiently.\nThe experimental results demonstrate that BitNet achieves competitive performance in terms of both\nperplexity and downstream task performance, while significantly reducing memory footprint and\nenergy consumption compared to the baselines. Moreover, BitNet follows a scaling law similar to\nthat of full-precision Transformers, indicating that it can be effectively scaled to even larger language\nmodels with potential benefits in terms of performance and efficiency. In the future, we would like to\nscale up BitNet in terms of model size and training steps. We are also interested in applying BitNet\nin other architectures (e.g., RetNet [SDH+23]) for training large language models.\nReferences\n[ADF+23] Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, and\net al. PaLM 2 technical report. CoRR, abs/2305.10403, 2023.\n[BKH16] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR,\n2016.\n[BLC13] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating\ngradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432,\n2013.\n[BMR+20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini\nAgarwal, Ariel Herbert-Voss, Gretchen Krueger, and et al. Language models are\nfew-shot learners. In Advances in Neural Information Processing Systems 33, 2020.\n10\n[BT19] Adrian Bulat and Georgios Tzimiropoulos. XNOR-Net++: improved binary neural\nnetworks. In BMVC 2019, 2019.\n[CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit\nquantization of large language models with guarantees. CoRR, abs/2307.13304, 2023.\n[CND+22] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann,\nParker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao,\nParker Barnes, Yi Tay, Noam Shazeer, and et al. PaLM: scaling language modeling\nwith pathways. CoRR, abs/2204.02311, 2022.\n[Dao23] Tri Dao. FlashAttention-2: faster attention with better parallelism and work partitioning.\nCoRR, abs/2307.08691, 2023.\n[DFE+22] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention:\nfast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022.\n[DLBZ22] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. LLM.int8(): 8-bit\nmatrix multiplication for transformers at scale. CoRR, 2022.\n[FAHA23] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. OPTQ: accurate\nquantization for generative pre-trained transformers. In The Eleventh International\nConference on Learning Representations, 2023.\n[HBM+22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai,\nEliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan\nClark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan\nDamoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae,\nOriol Vinyals, and Laurent Sifre. Training compute-optimal large language models.\nCoRR, abs/2203.15556, 2022.\n[HKK+20] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson,\nHeewoo Jun, Tom B. Brown, Prafulla Dhariwal, Scott Gray, Chris Hallacy, Benjamin\nMann, Alec Radford, Aditya Ramesh, Nick Ryder, Daniel M. Ziegler, John Schulman,\nDario Amodei, and Sam McCandlish. Scaling laws for autoregressive generative\nmodeling. CoRR, abs/2010.14701, 2020.\n[Hor14] Mark Horowitz. 1.1 computing\u2019s energy problem (and what we can do about it). In 2014\nIEEE International Conference on Solid-State Circuits Conference, ISSCC 2014, Digest\nof Technical Papers, San Francisco, CA, USA, February 9-13, 2014, pages 10\u201314, 2014.\n[KLZ+23] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao\nYu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for\nlarge language model serving with pagedattention. CoRR, 2023.\n[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws\nfor neural language models. CoRR, abs/2001.08361, 2020.\n[LDM12] Hector Levesque, Ernest Davis, and Leora Morgenstern. The winograd schema chal-\nlenge. In Thirteenth International Conference on the Principles of Knowledge Repre-\nsentation and Reasoning. Citeseer, 2012.\n[LKM23] Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers\nvia speculative decoding. In International Conference on Machine Learning, ICML\n2023, 23-29 July 2023, Honolulu, Hawaii, USA, 2023.\n[LOP+22] Zechun Liu, Barlas Oguz, Aasish Pappu, Lin Xiao, Scott Yih, Meng Li, Raghuraman\nKrishnamoorthi, and Yashar Mehdad. BiT: robustly binarized multi-distilled transformer.\nIn NeurIPS, 2022.\n11\n[LSL+21] Zechun Liu, Zhiqiang Shen, Shichao Li, Koen Helwegen, Dong Huang, and Kwang-\nTing Cheng. How do adam and training strategies help bnns optimization. In Proceed-\nings of the 38th International Conference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research,\npages 6936\u20136946. PMLR, 2021.\n[MCH+16] Nasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra,\nLucy Vanderwende, Pushmeet Kohli, and James F. Allen. A corpus and evaluation\nframework for deeper understanding of commonsense stories. CoRR, abs/1604.01696,\n2016.\n[Ope23] OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023.\n[RORF16] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. XNOR-Net:\nimagenet classification using binary convolutional neural networks. In Computer Vision\n- ECCV 2016 - 14th European Conference, Amsterdam, The Netherlands, October 11-14,\n2016, Proceedings, Part IV, Lecture Notes in Computer Science, 2016.\n[SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Wino-\nGrande: an adversarial winograd schema challenge at scale. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, pages 8732\u20138740, 2020.\n[SDH+23] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong\nWang, and Furu Wei. Retentive network: A successor to Transformer for large language\nmodels, 2023.\n[SPP+19] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper,\nand Bryan Catanzaro. Megatron-LM: training multi-billion parameter language models\nusing model parallelism. CoRR, abs/1909.08053, 2019.\n[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. LLaMA: open and\nefficient foundation language models. CoRR, abs/2302.13971, 2023.\n[TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan\nBikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David\nEsiobu, Jude Fernandes, Jeremy Fu, and et al. Llama 2: open foundation and fine-tuned\nchat models. CoRR, abs/2307.09288, 2023.\n[WH20] Yuxin Wu and Kaiming He. Group normalization. Int. J. Comput. Vis., 128(3):742\u2013755,\n2020.\n[WMH+22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng,\nYu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav\nChaudhary, Xia Song, and Furu Wei. Foundation transformers. CoRR, 2022.\n[XLS+23] Guangxuan Xiao, Ji Lin, Micka\u00ebl Seznec, Hao Wu, Julien Demouth, and Song Han.\nSmoothQuant: accurate and efficient post-training quantization for large language\nmodels. In International Conference on Machine Learning, ICML 2023, 23-29 July\n2023, Honolulu, Hawaii, USA, 2023.\n[ZGC+23] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang,\nand Orhan Firat. Binarized neural machine translation. CoRR, 2023.\n[ZHB+19] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag:\ncan a machine really finish your sentence? In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, pages 4791\u20134800, 2019.\n[ZZL22] Yichi Zhang, Zhiru Zhang, and Lukasz Lew. PokeBNN: A binary pursuit of lightweight\naccuracy. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n12465\u201312475. IEEE, 2022.\n12\nA\nHyperparameters\nParams\n# Hidden\n# Layers\n# Heads\nLearning Rate\n125M\n768\n12\n12\n2.4e-3\n350M\n1024\n24\n16\n1.2e-3\n760M\n1536\n24\n16\n1e-3\n1.3B\n2048\n24\n32\n8e-4\n2.7B\n2560\n32\n32\n6.4e-4\n6.7B\n4096\n32\n32\n4.8e-4\n13B\n5120\n40\n40\n4e-4\n30B\n7168\n48\n56\n4e-4\nTable 5: Model configuration for BitNet in the scaling experiments.\nHyperparameters\nValue\nTraining updates\n40K\nTokens per sample\n256K\nAdam \u03b2\n(0.9, 0.98)\nLearning rate schedule\nPolynomial decay\nWarmup updates\n750\nGradient clipping\n\u2717\nDropout\n\u2717\nAttention dropout\n\u2717\nWeight decay\n0.01\nTable 6: Hyperparameters for BitNet and the FP16 Transformers in the scaling experiments. For 13B\nand 30B model, we set weight decay to 0.05 for training stability.\nHyperparameters\nValue\nPeak learning rate\n1e-3\nTokens per sample\n128K\nAdam \u03b2\n(0.9, 0.98)\nLearning rate schedule\nPolynomial decay\nWarmup updates\n750\nGradient clipping\n\u2717\nDropout\n\u2717\nAttention dropout\n\u2717\nWeight decay\n0.01\nTable 7: Hyperparameters for the stability test of BitNet and FP16 Transformer.\n13\nHyperparameters\nElastic\nAbsmax\nPeak learning rate\n1e-4\n8e-4\nTraining updates\n40K\nTokens per sample\n256K\nAdam \u03b2\n(0.9, 0.98)\nLearning rate schedule\nPolynomial decay\nWarmup updates\n750\nGradient clipping\n\u2717\nDropout\n\u2717\nAttention dropout\n\u2717\nWeight decay\n0.01\nTable 8: Hyperparameters for the ablations of BitNet.\n14\n"
  },
  {
    "title": "4K4D: Real-Time 4D View Synthesis at 4K Resolution",
    "link": "https://arxiv.org/pdf/2310.11448.pdf",
    "upvote": "36",
    "text": "4K4D: Real-Time 4D View Synthesis at 4K Resolution\nZhen Xu1\nSida Peng1\nHaotong Lin1\nGuangzhao He1\nJiaming Sun2\nYujun Shen3\nHujun Bao1\nXiaowei Zhou1\n1Zhejiang University\n2Image Derivative Inc.\n3Ant Group\nOurs\nLPIPS:\nFPS:\n>200\n0.055\nGround Truth\nENeRF LPIPS:\nFPS:\n~5\n0.056\nKPlanes LPIPS:\nFPS:\n~0.5\n0.118\nFigure 1. Photorealistic and real-time rendering of dynamic 3D scenes. Our proposed method reconstructs a 4D neural representation\nfrom multi-view videos, which can be rendered at 1125\u00d71536 resolution with a speed of over 200 FPS using an RTX 3090 GPU while\nmaintaining state-of-the-art quality on the DNA-Rendering [11] dataset. It is also noteworthy that our method reaches over 80 FPS when\nrendering 4K images with an RTX 4090. Detailed performance under different resolutions using different GPUs can be found in Tab. 7.\nAbstract\nThis paper targets high-fidelity and real-time view synthe-\nsis of dynamic 3D scenes at 4K resolution. Recently, some\nmethods on dynamic view synthesis have shown impressive\nrendering quality. However, their speed is still limited when\nrendering high-resolution images. To overcome this problem,\nwe propose 4K4D, a 4D point cloud representation that\nsupports hardware rasterization and enables unprecedented\nrendering speed. Our representation is built on a 4D feature\ngrid so that the points are naturally regularized and can be\nrobustly optimized. In addition, we design a novel hybrid\nappearance model that significantly boosts the rendering\nquality while preserving efficiency. Moreover, we develop a\ndifferentiable depth peeling algorithm to effectively learn the\nproposed model from RGB videos. Experiments show that\nour representation can be rendered at over 400 FPS on the\nDNA-Rendering dataset at 1080p resolution and 80 FPS on\nthe ENeRF-Outdoor dataset at 4K resolution using an RTX\n4090 GPU, which is 30\u00d7 faster than previous methods and\nachieves the state-of-the-art rendering quality. Our project\npage is available at https://zju3dv.github.io/4k4d.\n1. Introduction\nDynamic view synthesis aims to reconstruct dynamic 3D\nscenes from captured videos and create immersive virtual\nplayback, which is a long-standing research problem in\ncomputer vision and computer graphics. Essential to the\npracticality of this technique is its ability to be rendered\nin real time with high fidelity, enabling its application\nin VR/AR, sports broadcasting, and artistic performance\ncapturing. Traditional methods [6, 12, 14, 23, 54, 55, 94]\nrepresent dynamic 3D scenes as textured mesh sequences\nand reconstruct them using complicated hardware. Therefore,\nthey are typically limited to controlled environments.\nRecently, implicit neural representations [17,38,51] have\nshown great success in reconstructing dynamic 3D scenes\nfrom RGB videos via differentiable rendering. For example,\nLi et al. [38] model the target scene as a dynamic radiance\nfield and leverage volume rendering [15] to synthesize im-\nages, which are compared with input images for optimization.\nDespite impressive dynamic view synthesis results, existing\napproaches typically require seconds or even minutes to\nrender an image at 1080p resolution due to the costly network\nevaluation, as discussed by Peng et al. [61].\n1\narXiv:2310.11448v3  [cs.CV]  28 Oct 2023\nInspired by static view synthesis approaches [18,29,92],\nsome dynamic view synthesis methods [2,61,84] increase\nthe rendering speed by decreasing either the cost or the\nnumber of network evaluations. With these strategies, MLP\nMaps [61] is able to render foreground dynamic humans\nwith a speed of 41.7 fps. However, the challenge of\nrendering speed still exists, since the real-time performance\nof MLP Maps is achieved only when synthesizing moderate-\nresolution images (384\u00d7512). When rendering 4K resolution\nimages, its speed reduces to only 1.3 FPS.\nIn this paper, we propose a novel neural representation,\nnamed 4K4D, for modeling and rendering dynamic 3D\nscenes. As illustrated in Fig. 1, 4K4D significantly outper-\nforms previous dynamic view synthesis approaches [17,43]\nin terms of the rendering speed, while being competitive\nin the rendering quality. Our core innovation lies in a 4D\npoint cloud representation and a hybrid appearance model.\nSpecifically, for the dynamic scene, we obtain the coarse\npoint cloud sequence using a space carving algorithm [33]\nand model the position of each point as a learnable vector.\nA 4D feature grid is introduced for assigning a feature\nvector to each point, which is fed into MLP networks to\npredict the point\u2019s radius, density, and spherical harmonics\n(SH) coefficients [52]. The 4D feature grid naturally applies\nspatial regularization on the point clouds and makes the\noptimization more robust, as supported by the results in\nSec. 5.2. Based on 4K4D, we develop a differentiable depth\npeeling algorithm that exploits the hardware rasterizer to\nachieve unprecedented rendering speed.\nWe find that the MLP-based SH model struggles to\nrepresent the appearance of dynamic scenes. To alleviate this\nissue, we additionally introduce an image blending model\nto incorporate with the SH model to represent the scene\u2019s\nappearance. An important design is that we make the image\nblending network independent from the viewing direction, so\nit can be pre-computed after training to boost the rendering\nspeed. As a two-edged sword, this strategy makes the image-\nblending model discrete along the viewing direction. This\nproblem is compensated for using the continuous SH model.\nIn contrast to 3D Gaussian Splatting [29] that uses the SH\nmodel only, our hybrid appearance model fully exploits\nthe information captured by input images, thus effectively\nimproving the rendering quality.\nTo validate the effectiveness of the proposed pipeline, we\nevaluate 4K4D on multiple widely used datasets for multi-\nview dynamic novel view synthesis, including NHR [88],\nENeRF-Outdoor [43], DNA-Rendering [11], and Neu-\nral3DV [37]. Extensive experiments show that 4K4D could\nnot only be rendered orders of magnitude faster but also\nnotably outperform the state-of-the-art in terms of rendering\nquality. With an RTX 4090 GPU, our method reaches 400\nFPS on the DNA-Rendering dataset at 1080p resolution and\n80 FPS on the ENeRF-Outdoor dataset at 4K resolution.\n2. Related Work\nNeural scene representations. In the domain of novel\nview synthesis, various approaches have been proposed\nto address this challenging problem, including multi-view\nimage-based methods [5,7,16,27,63,97], multi-plane image\nrepresentations [41,50,58,78,80,80], light-field techniques\n[13, 19, 35] as well as explicit surface or voxel-based\nmethods [12,14,54,55,94]. [12] utilizes depth sensors and\nmulti-view stereo techniques to consolidate per-view depth\ninformation into a coherent scene geometry, producing high-\nquality volumetric video. These methods require intricate\nhardware setups and studio arrangements, thus constraining\ntheir accessibility and applicability. Recently, implicit neural\nscene representations [3,21,26,28,45,46,51,71,74\u201376,79,86]\nhave attracted significant interest among researchers. NeRF\n[51] encodes the radiance fields of static scenes using\ncoordinate-based Multi-Layer Perceptrons (MLP), achieving\nexceptional novel view synthesis quality.\nBuilding upon NeRF, a collection of studies [24,38,39,\n56,57,64,88] has extended implicit neural representations\nto accommodate dynamic scenes. DyNeRF [38] introduces\nan additional temporal dimension to NeRF\u2019s 5D input,\nthereby enabling it to model temporal variations in dynamic\nscenes. However, NeRF-based approaches often suffer from\nsubstantial computational costs, leading to rendering times\nof seconds or even minutes for moderate-resolution images,\nwhich significantly limits their practicality. Another line of\nstudies [9,40,85,93] has concentrated on integrating image\nfeatures into the NeRF rendering pipeline. This approach is\neasily applicable to dynamic scenes, as multi-view videos\ncan be effortlessly decomposed into multi-view images.\nNevertheless, the convolution operations employed in these\nmethods, also face challenges in terms of rendering speed as\nthe input image resolution increases, hindering the rendering\nefficiency of these approaches in real-world applications.\nAccelerating neural scene representations. Multiple stud-\nies have focused on accelerating the rendering speed of\nimplicit neural scene representation by distilling implicit\nMLP networks into explicit structures that offer fast query\ncapabilities, including voxel grids [18,22,36,53,67,91,92],\nexplicit surfaces [10,20,25,32,48,60] and point-based repre-\nsentations [1,29,31,34,65,68,95]. These methods effectively\nreduce the cost or the number of NeRF\u2019s MLP evaluations\nrequired. One notable advancement is the development of\n3D Gaussian Splatting (3DGS) [29] which introduces a\ndifferentiable splatting algorithm for differentiable volume\nrendering [4,15]. This technique leverages semi-transparent\nGaussian ellipsoids with spherical harmonics [52] to attain\nboth high-fidelity and high-speed rendering, effectively\neliminating the slow ray marching operation. However, the\naforementioned acceleration techniques are only applicable\nto static scenes.\n2\n(a) Point Cloud Sequence\n(c) Appearance\n(b) Geometry\nSH MLP\nSH\nFeature Vector\nDiscrete IBR\nPoint Radius\nPoint Density\nRadius MLP\nDensity MLP\nView-Dependent RGB\nDistance\nDensity\nView Direction\nRGB Value\nRGB Value\nRGB Value\nView Direction\nView Direction\n(d) Di\ufb00erentiable Depth Peeling\n4D Feature Grid\nFigure 2. Overview of our proposed pipeline. (a) By applying the space-carving algorithm [33], we extract the initial cloud sequence\nx, t of the target scene. A 4D feature grid [17] is predefined to assign a feature vector to each point, which is then fed into MLPs for the\nscene geometry and appearance. (b) The geometry model is based on the point location, radius, and density, which forms a semi-transparent\npoint cloud. (c) The appearance model consists of a piece-wise constant IBR term cibr and a continuous SH model csh. (d) The proposed\nrepresentation is learned from multi-view RGB videos through the differentiable depth peeling algorithm.\nInspired by the success of techniques for accelerating\nstatic neural representations, several approaches [2, 8, 42,\n43,47,61,70,77,77,82,83] have explored the possibility of\nreal-time dynamic view synthesis. HyperReel [2] employs a\nprimitive prediction module to reduce the number of network\nevaluations, thereby achieving real-time rendering speeds\nat moderate resolutions. However, it should be noted that\ntheir rendering speed decreases significantly when rendering\nhigher-resolution images, as evidenced by experiments\ndetailed in Sec. 5 (e.g., 1.5FPS for a 1352\u00d71014 image\nfrom the Neural3DV [38] dataset). In recent developments,\na line of concurrent work [49,87,89,90] has also reported\nreal-time rendering speeds by incorporating correspondence\nor time-dependency into the 3DGS approach [29]. However,\nthese methods do not show results on datasets with large\nand fast motions (DNA-Rendering [11] and NHR [88]) and\ncould only achieve real-time speed at moderate resolution\n(800\u00d7800 [87] and 640\u00d7480 [49]). In contrast, 4K4D\nis capable of achieving real-time rendering even at 4K\nresolution while concurrently maintaining state-of-the-art\nview-synthesis quality on large-motion data (as elaborated\nin Sec. 5).\n3. Proposed Approach\nGiven a multi-view video capturing a dynamic 3D scene,\nour goal is to reconstruct the target scene and perform novel\nview synthesis in real time. To this end, we extract coarse\npoint clouds of the scene using the space-carving algorithm\n[33] (Sec. 4) and build a point cloud-based neural scene\nrepresentation, which can be robustly learned from input\nvideos and enable the hardware-accelerated rendering.\nThe overview of the proposed model is presented in\nFig. 2. In this section, we first describe how to represent the\ngeometry and appearance of dynamic scenes based on point\nclouds and neural networks (Sec. 3.1). Then, we develop\na differentiable depth peeling algorithm for rendering our\nrepresentation (Sec. 3.2), which is supported by the hardware\nrasterizer, thereby significantly improving the rendering\nspeed. Finally, we discuss how to optimize the proposed\nmodel on input RGB videos (Sec. 3.3).\n3.1. Modeling Dynamic Scenes with Point Clouds\n4D embedding. Given the coarse point clouds of the target\nscene, we represent its dynamic geometry and appearance\nusing neural networks and feature grids. Specifically, our\nmethod first defines six feature planes \u03b8xy, \u03b8xz, \u03b8yz, \u03b8tx, \u03b8ty,\nand \u03b8tz. To assign a feature vector f to any point x at frame t,\nwe adopt the strategy of K-Planes [17] to model a 4D feature\nfield \u0398(x, t) using these six planes:\nf = \u0398(x, t) = \u03b8xy(x, y) \u2295 \u03b8xz(x, z) \u2295 \u03b8yz(y, z)\u2295\n\u03b8tx(t, x) \u2295 \u03b8ty(t, y) \u2295 \u03b8tz(t, z),\n(1)\nwhere x = (x, y, z) is the input point, and \u2295 indicates the\nconcatenation operator. Please refer to K-Planes [17] for\nmore implementation details.\nGeometry model. Based on coarse point clouds, the\ndynamic scene geometry is represented by learning three\nentries on each point: position p \u2208 R3, radius r \u2208 R, and\ndensity \u03c3 \u2208 R. Using these point entries, we calculate the\nvolume density of space point x with respect to an image\npixel u for the volume rendering, which will be described in\nSec. 3.2. The point position p is modeled as an optimizable\nvector. The radius r and density \u03c3 are predicted by feeding\nthe feature vector f in Eq. (1) to an MLP network.\nAppearance model. As illustrated in Fig. 2c, we use the\nimage blending technique and the spherical harmonics (SH)\n3\nmodel [52,92] to build a hybrid appearance model, where\nthe image blending technique represents the discrete view-\ndependent appearance cibr and the SH model represents the\ncontinuous view-dependent appearance csh. For point x at\nframe t, its color with viewing direction d is:\nc(x, t, d) = cibr(x, t, d) + csh(s, d),\n(2)\nwhere s means SH coefficients at point x.\nThe discrete view-dependent appearance cibr is inferred\nbased on input images. Specifically, for a point x, we first\nproject it into the input image to retrieve the corresponding\nRGB color ci\nimg. Then, to blend input RGB colors, we\ncalculate the corresponding blending weight wi based on the\npoint coordinate and the input image. Note that the blending\nweight is independent from the viewing direction. Next, to\nachieve the view-dependent effect, we select the N \u2032 nearest\ninput views according to the viewing direction. Finally,\nthe color cibr is computed as PN \u2032\ni=1 wici\nimg. Because the\nN \u2032 input views are obtained through the nearest neighbor\nretrieval, the cibr is inevitably discrete along the viewing\ndirection. To achieve the continuous view-dependent effect,\nwe append the fine-level color csh represented by the SH\nmodel, as shown in Fig. 2c.\nIn practice, our method regresses the SH coefficients s by\npassing the point feature f in Eq. (1) into an MLP network.\nTo predict the blending weight wi in the image blending\nmodel cibr, we first project point x onto the input image to\nretrieve the image feature f i\nimg, and then concatenate it with\nthe point feature f, which is fed into another MLP network\nto predict the blending weight. The image feature f i\nimg is\nextracted using a 2D CNN network.\nDiscussion. Our appearance model is the key to achieving\nthe low-storage, high-fidelity, and real-time view synthesis of\ndynamic scenes. There are three alternative ways to represent\nthe dynamic appearance, but they cannot perform on par\nwith our model. 1) Defining explicit SH coefficients on\neach point, as in 3D Gaussian splatting [29]. When the\ndimensional of SH coefficients is high and the amount of\npoints of dynamic scenes is large, this model\u2019s size could\nbe too big to train on a consumer GPU. 2) MLP-based SH\nmodel. Using an MLP to predict SH coefficients of each\npoint can effectively decrease the model size. However, our\nexperiments found that MLP-based SH model struggles\nto render high-quality images (Sec. 5.2). 3) Continuous\nview-dependent image blending model, as in ENeRF [43].\nWe found that representing the appearance with the image\nblending model has better rendering quality than with only\nMLP-based SH model. However, the network in ENeRF\ntakes the viewing direction as input and thus cannot be easily\npre-computed, limiting the rendering speed during inference.\nIn contrast to these three methods, our appearance model\ncombines a discrete image blending model cibr with a\ncontinuous SH model csh. The image blending model cibr\nboosts the rendering performance. In addition, it supports\nthe pre-computation, as its network does not take the\nviewing direction as input. The SH model csh enables the\nview-dependent effect for any viewing direction. During\ntraining, our model represents the scene appearance using\nnetworks, so its model size is reasonable. During inference,\nwe pre-compute the network outputs to achieve the real-time\nrendering, which will be described in Sec. 3.4.\n3.2. Differentiable Depth Peeling\nOur proposed dynamic scene representation can be\nrendered into images using the depth peeling algorithm [4].\nThanks to the point cloud representation, we are able to\nleverage the hardware rasterizer to significantly speed up\nthe depth peeling process. Moreover, it is easy to make this\nrendering process differentiable, enabling us to learn our\nmodel from input RGB videos.\nWe develop a custom shader to implement the depth\npeeling algorithm that consists of K rendering passes.\nConsider a particular image pixel u. In the first pass, our\nmethod first uses the hardware rasterizer to render point\nclouds onto the image, which assigns the closest-to-camera\npoint x0 to the pixel u. Denote the depth of point x0\nas t0. Subsequently, in the k-th rendering pass, all points\nwith depth value tk smaller than the recorded depth of the\nprevious pass tk\u22121 are discarded, thereby resulting in the\nk-th closest-to-camera point xk for the pixel u. Discarding\ncloser points is implemented in our custom shader, so it\nstill supports the hardware rasterization. After K rendering\npasses, pixel u has a set of sorted points {xk|k = 1, ..., K}.\nBased on the points {xk|k = 1, ..., K}, we use the\nvolume rendering to synthesize the color of pixel u. The\ndensities of points {xk|k = 1, ..., K} for pixel u is defined\nbased on the distance between the projected point and pixel\nu on the 2D image:\n\u03b1(u, x) = \u03c3 \u00b7 max(1 \u2212 ||\u03c0(x) \u2212 u||2\n2\nr2\n, 0),\n(3)\nwhere \u03c0 is the camera projection function. \u03c3 and r are the\ndensity and radius of point x, which are described in Sec. 3.1.\nDuring training, we implement the projection function \u03c0\nusing the PyTorch [59], so Eq. (3) is naturally differentiable.\nDuring inference, we leverage the hardware rasterization\nprocess to efficiently obtain the distance ||\u03c0(x)\u2212u||2\n2, which\nis implemented using OpenGL [72].\nDenote the density of point xk as \u03b1k. The color of pixel\nu from the volume rendering is formulated as:\nC(u) =\nK\nX\nk=1\nTk\u03b1kck, where Tk =\nk\u22121\nY\nj=1\n(1 \u2212 \u03b1j),\n(4)\nwhere ck is the color of point xk, as described in Eq. (2).\n4\n3.3. Training\nGiven the rendered pixel color C(u), we compare it with\nthe ground-truth pixel color Cgt(u) to optimize our model\nin an end-to-end fashion using the following loss function:\nLimg =\nX\nu\u2208U\n||C(u) \u2212 Cgt(u)||2\n2,\n(5)\nwhere U is the set of image pixels. In addition to the MSE\nloss Limg, we also apply the perceptual loss Llpips [96].\nLlpips = ||\u03a6(I) \u2212 \u03a6(Igt)||1,\n(6)\nwhere \u03a6 is the perceptual function (a VGG16 network) and\nI, Igt are the rendered and ground-truth images, respectively.\nThe perceptual loss [96] computes the difference in image\nfeatures extracted from the VGG model [73]. Our exper-\niments in Sec. 5.2 show that it effectively improves the\nperceived quality of the rendered image.\nTo regularize the optimization process of our proposed\nrepresentation, we additionally apply the mask supervision\nto dynamic regions of the target scene. We solely render\npoint clouds of dynamic regions to obtain their masks, where\nthe pixel value is obtained by:\nM(u) =\nK\nX\nk=1\nTk\u03b1k, where Tk =\nk\u22121\nY\nj=1\n(1 \u2212 \u03b1j).\n(7)\nThe mask loss is defined as:\nLmsk =\nX\nu\u2208U\u2032\nM(u)Mgt(u),\n(8)\nwhere U\u2032 means the set of pixels of the rendered mask, and\nMgt is the ground-truth mask of 2D dynamic regions. This\neffectively regularizes the optimization of the geometry of\ndynamic regions by confining it to the visual hulls.\nThe final loss function is defined as\nL = Limg + \u03bblpipsLlpips + \u03bbmskLmsk,\n(9)\nwhere \u03bblpips and \u03bbmsk are hyperparameters controlling\nweights of correspondings losses.\n3.4. Inference\nAfter training, we apply a few acceleration techniques to\nboost the rendering speed of our model. First, we precompute\nthe point location p, radius r, density \u03c3, SH coefficients\ns and color blending weights wi before inference, which\nare stored at the main memory. During rendering, these\nproperties are asynchronously streamed onto the graphics\ncard, overlapping rasterization with memory copy to achieve\nan optimal rendering speed [69, 72]. After applying this\ntechnique, the runtime computation is reduced to only a\ndepth peeling evaluation (Sec. 3.2) and a spherical harmonics\nevaluation (Eq. (2)). Second, we convert the model from\n32-bit floats to 16-bits for efficient memory access, which\nincreases FPS by 20 and leads to no visible performance loss\nas validated in Tab. 6. Third, the number of rendering passes\nK for the differentiable depth peeling algorithm is reduced\nfrom 15 to 12, also leading to a 20 FPS speedup with no\nvisual quality change. Detailed analyses of rendering speed\ncan be found in Sec. 5.2.\n4. Implementation Details\nOptimization. 4K4D is trained using the PyTorch frame-\nwork [59]. Using the Adam optimizer [30] with a learning\nrate 5e\u22123, our models typically converge after 800k iter-\nations for a sequence length of 200 frames, which takes\naround 24 hours on a single RTX 4090 GPU. Specifically,\nthe learning rate of point positions is set to 1e\u22125, and\nthe regularization loss weights \u03bblpips and \u03bbmsk are set to\n1e\u22123. During training, the number of passes K for the\ndifferentiable depth peeling is set to 15, and the number\nof nearest input views N \u2032 is set to 4. The rendering speed\nof our method is reported on an RTX 3090 GPU for the\nexperiments in Sec. 5 unless otherwise stated.\nInitialization of point clouds. We leverage exisiting multi-\nview reconstruction methods to initialize the point clouds.\nFor dynamic regions, we use segmentation methods [44]\nto obtain their masks in input images and utilize the space\ncarving algorithm [33] to extract their coarse geometry. For\nstatic background regions, we leverage foreground masks to\ncompute the mask-weighted average of background pixels\nalong all frames, producing background images without the\nforeground content. Then, an Instant-NGP [53] model is\ntrained on these images, from which we obtain the initial\npoint clouds. After the initialization, the number of points\nfor the dynamic regions is typically 250k per frame, and the\nstatic background regions typically consist of 300k points.\n5. Experiments\nDatasets and metrics. We train and evaluate our method\n4K4D on multiple widely used multi-view datasets, includ-\ning DNA-Rendering [11], ENeRF-Outdoor [43], NHR [88]\nand Neural3DV [38]. DNA-Rendering [11] records 10-\nsecond clips of dynamic humans and objects at 15 FPS\nusing 4K and 2K cameras with 60 views. This dataset is very\nchallenging due to the complex clothing and fast-moving\nhumans recorded. We conduct experiments on 4 sequences\nof DNA-Rendering, with 90% of the views as training\nset and the rest as evaluation set. ENeRF-Outdoor [43]\nrecords multiple dynamic humans and objects in an outdoor\nenvironment at 30FPS using 1080p cameras. We select three\n100-frame sequences with 6 different actors (2 for each\nsequence) holding objects to evaluate our method 4K4D.\n5\nOurs (141.7 FPS)\nENeRF (11.3 FPS)\nKPlanes (1.4 FPS)\nGround Truth\nFigure 3. Qualitative comparison on the ENeRF-Outdoor [43] dataset that contains 960 \u00d7 540 images. Our method achieves much\nhigher rendering quality and can be rendered 13\u00d7 faster than ENeRF [43]. More dynamic results can be found in the supplementary video.\nThis dataset is difficult for dynamic view synthesis in that not\nonly are there multiple moving humans and objects in one\nclip, but the background is also dynamic due to the shadow\nof the humans. Following Im4D [42] and NeuralBody [62],\nwe evaluate metrics on the dynamic regions for the DNA-\nRendering [11] and NHR [88] dataset, which can be obtained\nby predefining the 3D bounding box of the person and\nprojecting it onto the images. For ENeRF-Outdoor [43], we\njointly train the dynamic geometry and appearance of the\nforeground and the dynamic appearance of the background\nto obtain rendering results on the whole image. All images\nare resized with a ratio of 0.5 for evaluation and 0.375 if the\noriginal resolution is more than 2K. For DNA-Rendering, the\nrendered image size is 1024\u00d71224 (and 1125\u00d71536) and\nfor ENeRF-Outdoor, the resolution is 960\u00d7540 during the\nexperiments. The resolutions for Neural3DV video and NHR\nare 1352\u00d71224 and 512\u00d7612 (and 384\u00d7512) respectively.\nDetailed dataset settings can be found in Appendix A.\nTable 1. Quantitative comparison on the DNA-Rendering\n[11] dataset. Image resolutions are 1024\u00d71224 and 1125\u00d71536.\nMetrics are averaged over all scenes. Green and yellow cell colors\nindicate the best and the second best results, respectively.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFPS\nENeRF [43]\n28.108\n0.972\n0.056\n6.011\nIBRNet [85]\n27.844\n0.967\n0.081\n0.100\nKPlanes [17]\n27.452\n0.952\n0.118\n0.640\nIm4D [42]\n28.991\n0.973\n0.062\n15.360\nOurs\n31.173\n0.976\n0.055\n203.610\n5.1. Comparison Experiments\nComparison results. Qualitative and quantitative compar-\nisons on the DNA-Rendering [11] are shown in Fig. 5\nand Tab. 1 respectively. As evident in Tab. 1, our method\n4K4D renders 30x faster than the SOTA real-time dynamic\nview synthesis method ENeRF [43] with superior render-\ning quality. Even when compared with concurrent work\n[42], our method 4K4D still achieves 13x speedup and\n6\nGround Truth\nGround Truth\nOurs\n(110.1 FPS)\nENeRF\n(5.2 FPS)\nIBRNet\n(0.05 FPS)\nHyperReel\n(1.5 FPS)\nKPlanes\n(0.52 FPS)\nENeRF\nHyperReel\nOurs\nIBRNet\nKPlanes\nFigure 4. Qualitative comparison on the Neural3DV [38] dataset that contains 1352\u00d71224 images. Our method can not only recover\nhigh-frequency details of dynamic objects but also maintain sharp edges around occlusion.\nproduces consistently higher quality images. As shown in\nFig. 5, KPlanes [17] could not recover the highly detailed\nappearance and geometry of the 4D dynamic scene. Other\nimage-based methods [42, 43, 85] produce high-quality\nappearance. However, they tend to produce blurry results\naround occlusions and edges, leading to degradation of\nthe visual quality while maintaining interactive framerate\nat best. On the contrary, our method 4K4D can produce\nhigher fidelity renderings at over 200 FPS. Fig. 3 and Tab. 2\nprovides qualitative and quantitaive results on the ENeRF-\nOutdoor [43] dataset. Even on the challenging ENeRF-\nOutdoor dataset with multiple actors and the background,\nour method 4K4D still achieves notably better results while\nrendering at over 140 FPS. ENeRF [43] produces blurry\nresults on this challenging dataset, and the rendering results\nof IBRNet [85] contain black artifacts around the edges\nof the images as shown in Fig. 3. K-Planse [17] fails to\nreconstruct the dynamic humans and varying background\nregions. More comparison results on the NHR [88] dataset\ncan be found in Appendix B.\nTable 2. Quantitative comparison on the ENeRF-Ourdoor [43]\ndataset. This dataset includes 960 \u00d7 540 images. Green and yellow\ncell colors indicate the best and the second best results, respectively.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFPS\nENeRF [43]\n25.452\n0.809\n0.273\n11.309\nIBRNet [85]\n24.966\n0.929\n0.172\n0.140\nKPlanes [17]\n21.310\n0.735\n0.454\n1.370\nOurs\n25.815\n0.898\n0.147\n141.665\nTable 3. Quantitative comparison on the Neural3DV [38] dataset.\nThis dataset includes 1352\u00d71224 images. Green and yellow cell\ncolors indicate the best and the second best results, respectively.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFPS\nENeRF [43]\n30.306\n0.962\n0.185\n5.187\nIBRNet [85]\n31.520\n0.963\n0.169\n0.053\nKPlanes [17]\n31.610\n0.961\n0.182\n0.518\nHyperReel [2]\n32.198\n0.970\n0.161\n1.540\nOurs\n32.855\n0.973\n0.167\n110.063\n5.2. Ablation Studies\nWe perform ablation studies on the 150-frame 0013 01\nsequence of the DNA-Rendering [11] dataset. Qualitative\n7\nGround Truth\nOurs (203.6 FPS)\nENeRF (6.0 FPS)\nIBRNet (0.1 FPS)\nIm4D (15.4 FPS)\nKPlanes (0.6 FPS)\nFigure 5. Qualitative comparison on the DNA-Rendering [11] dataset that contains 1024\u00d71224 (and 1125\u00d71536) images. Our method\ncan produce high-fidelity images at over 200 FPS while other competitors fail to produce high-quality results for highly dynamic scenes.\nand quantitative results are shown in Fig. 6 and Tabs. 4 to 7.\nAblation study on the 4D embedding. The \u201cw/o f\u201d variant\nremoves the proposed 4D embedding (Sec. 3.1) module\nand replaces it with a per-frame and per-point optimizable\nposition, radius, density, and scale. As shown in Fig. 6\nand Tab. 4, the \u201cw/o f\u201d variant produces blurry and noisy\ngeometry without the 4D embedding \u0398, which leads to the\ninferior rendering quality.\nAblation study on the hybrid appearance model. The \u201cw/o\ncibr\u201d variant removes cibr in the appearance formulation\nEq. (2), which not only leads to less details on the recovered\nappearance but also significantly impedes the quality of\nthe geometry. Adding an additional degree for the SH\ncoefficients does not lead to a significant performance\nchange (PSNR 30.202 vs. 30.328). Comparatively, our\nproposed method produces high-fidelity rendering with\nmuch better details. A visualization of the view-dependent\neffect produced by csh can be found in Appendix B.\nAblation study on loss functions. As shown in Tab. 4,\nremoving the Llpips term not only reduces the perceptual\nquality (LPIPS score) but also leads to the degradation of\nother performance metrics. For the highly dynamic DNA-\nRendering [11] dataset, the mask loss Lmsk helps with\nregularizing the optimization of the dynamic geometry.\nTable 4. Ablation studies on the 150-frame 0013 01 sequence\nof the DNA-Rendering dataset [11]. \u201cw/o f\u201d indicates replacing\nthe 4D embedding with a per-frame and per-point optimizable\nposition, radius, density, and scale. See Sec. 5.2 for more detailed\ndescriptions.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nModel Size\nw/o f\n29.779\n0.967\n0.057\n1304.0 MiB\nw/o cibr\n30.259\n0.973\n0.054\n225.0 MiB\nw/o csh\n31.946\n0.981\n0.040\n225.0 MiB\nw/o Llpips\n31.661\n0.979\n0.063\n225.0 MiB\nw/o Lmsk\n29.115\n0.965\n0.073\n225.0 MiB\nOurs\n31.990\n0.982\n0.040\n225.0 MiB\nStorage analysis. For the 150-frame 0013 01 scene, the\nstorage analysis of our method 4K4D is listed in Tab. 5.\nThe point positions p take up the majority of the model\nsize due to its explicit representation. The final storage cost\nfor our method is less than 2 MB per frame with source\nvideos included. The input images of DNA-Rendering [11]\nare provided in JPEG formats. We encode frames of all input\nimages as videos using the HEVC encoder of FFmpeg with\na CRF of 25 [81]. After the encoding, we obverse no change\nin LPIPS (0.040), no loss in SSIM (0.982), and only a 0.42%\ndecrease in PSNR (31.990 vs. 31.855), which indicates that\n8\nGround Truth\nOurs\nw/o\nw/o\nw/o\nFigure 6. Ablation study of proposed components on the 0013 01 sequence of the DNA-Rendering dataset [11]. Removing our proposed\ncomponents leads to noisy geometry and blurry appearance. Our method produces high-fidelity results with perceptually accurate shapes and\ncolors. See Sec. 5.2 for more detailed descriptions.\nTable 5. Storage analysis of our method on the 150-frame 0013 01 sequence of the DNA-Rendering dataset [11]. \u201cStorage\u201d indicates\nthe size of model stored on disk, and \u201cStorage / Frame\u201d indicates the per-frame size.\nPoint Positions p\n4D Embedding \u0398\nMLPs and CNNs\nTotal Model Size\nEncoded Video\nTotal Storage (w/ Videos)\nStorage\n208.09 MB\n16.77 MB\n0.10 MB\n224.96 MB\n62.89 MB\n287.86 MB\nStorage / Frame\n1.387 MB\n0.112 MB\n0.001 MB\n1.500 MB\n0.419 MB\n1.919 MB\nour method 4K4D is robust to the video encoding of input\nimages. After encoding the input images as videos, the\nstorage overhead for Image-Based Rendering (Sec. 3.1) is\nonly 0.419 MB per frame with minimal rendering quality\nchange.\nAs mentioned in Sec. 3.4, we precompute the physical\nproperties on the point clouds for real-time rendering, which\ntakes around 2 seconds for one frame. Although large in\nsize (200 MB for one frame of 0013 01), these precomputed\ncaches only reside in the main memory and are not explicitly\nstored on disk, which is feasible for a modern PC. This\nmakes our representation a form of compression, where the\ndisk file size is small (2 MB per frame) but the information\ncontained is very rich (200 MB per frame).\n5.3. Rendering Speed Analysis\nAs mentioned in Sec. 3.4, we introduce a number of\noptimization techniques to accelerate the rendering speed\nof our method 4K4D, which are only made possible by our\nTable 6. Runtime analysis of the proposed method on the\n0013 01 sequence of DNA-Rendering [11]. The acceleration\ntechniques all lead to minimal quality changes as shown by the cell\ncoloring (green for the best and yellow for the second best). See\nSec. 5.2 for detailed descriptions.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFPS\nw/o fp16\n32.020\n0.982\n0.039\n202.021\nw/o K = 12\n31.951\n0.982\n0.040\n200.397\nw/o Cache\n31.969\n0.982\n0.040\n22.193\nw/o DDP\n31.900\n0.981\n0.041\n29.656\nOurs\n31.990\n0.982\n0.040\n219.430\nproposed hybrid geometry and appearance representation. In\nTab. 6, we ablate the effectiveness and quality impact of those\nproposed techniques on the 150-frame 0013 01 sequence of\nthe DNA-Rendering [11] dataset.\nThe effectiveness of precomputation. For real-time render-\ning, we precompute and cache p, r, \u03c3 and s for all points and\nstore them in the main memory. Thanks to our design choice\n9\nTable 7. Rendering speed on different GPUs and resolutions.\nThe results are recorded on the first frame of the 0013 01 sequence\nof DNA-Rendering [11] and the actor1 4 sequence of ENeRF-\nOutdoor [43] with the interactive GUI. Resolutions are set to 720p\n(720 \u00d7 1280), 1080p (1080 \u00d7 1920), and 4K (2160 \u00d7 3840).\nEven with the overhead of the interactive GUI (\u201cw/ GUI\u201d), our\nmethod still achieves unprecedented rendering speed. More real-\ntime rendering results can be found in the supplementary video.\nDataset\nRes.\nRTX 3060 RTX 3090 RTX 4090\nDNA-Rendering [11]\nw/ GUI\n720p\n173.8 FPS 246.9 FPS 431.0 FPS\n1080p 138.7 FPS 233.1 FPS 409.8 FPS\n4K\n90.0 FPS 147.4 FPS 288.8 FPS\nENeRF-Outdoor [43]\nw/ GUI\n720p\n90.5 FPS 130.5 FPS 351.5 FPS\n1080p 66.1 FPS 103.6 FPS 249.7 FPS\n4K\n25.1 FPS\n47.2 FPS\n85.1 FPS\nof splitting the appearance representation into constant cibr\nand view-dependent csh, we can also precompute and cache\nthe per-image weights w and color cimg for all source images\n(Sec. 3.1). These caches take around 200MB per frame of\nmain memory for the 150-frame 60-view scene of 0013 01\nof the DNA-Rendering [11] dataset. The pre-computation\nenabled by our representation (Sec. 3.1) achieves a 10x\nspeedup (Ours vs. \u201cw/o Cache\u201d).\nDifferentiable depth peeling. We also make comparisons\nwith more traditional CUDA-based differentiable point cloud\nrendering technique provided by PyTorch3D [66] (\u201cw/o\nDDP\u201d) to validate the effectiveness of our proposed differen-\ntiable depth peeling algorithm (Sec. 3.2). Both our proposed\nDDP (Sec. 3.2) and PyTorch3D\u2019s [66] implementation use\nthe same volume rendering equation as in Eq. (4). As shown\nin Tab. 6, our proposed method is more than 7 times faster\nthan the CUDA-based one.\nOther acceleration techniques. The \u201cw/o fp16\u201d variant uses\nthe original 32-bit floating point number for computation.\nThe \u201cw/o K = 12\u201d variant uses 15 passes in the depth\npeeling algorithm as when training. Using 16-bit floats and\n12 rendering passes both lead to a 20FPS speedup.\nRendering speed on different GPUs and resolutions.\nWe additionally report the rendering speed of our method\non different hardware (RTX 3060, 3090, and 4090) with\ndifferent resolutions (720p, 1080p, and 4K (2160p)) in Tab. 7.\nThe rendering speed reported here contains the overhead of\nthe interactive GUI (\u201cw/ GUI\u201d), making them slightly slower\nthan those reported in Sec. 5.1. 4K4D achieves real-time\nrendering speed even when rendering 4K (2160p) images on\ncommodity hardware as shown in the table.\n6. Conclusion and Discussion\nIn this paper, we provide a neural point cloud-based\nrepresentation, 4K4D, for real-time rendering of dynamic\n3D scenes at 4K resolution. We build 4K4D upon a 4D\nfeature grid to naturally regularize the points and develop a\nnovel hybrid appearance model for high-quality rendering.\nFurthermore, we develop a differentiable depth peeling\nalgorithm that utilizes the hardware rasterization pipeline\nto effectively optimize and efficiently render the proposed\nmodel. In our experiments, we show that 4K4D not only\nachieves state-of-the-art rendering quality but also exhibits a\nmore than 30\u00d7 increase in rendering speed (over 200FPS at\n1080p on an RTX 3090).\nHowever, our method still has some limitations. 4K4D\ncannot produce correspondences of points across frames,\nwhich are important for some downstream tasks. Moreover,\nthe storage cost for 4K4D increases linearly with the number\nof video frames, so our method has difficulty in modeling\nlong volumetric videos. How to model correspondences\nand reduce the storage cost for long videos could be two\ninteresting problems for future works.\nReferences\n[1] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry\nUlyanov, and Victor Lempitsky. Neural point-based graphics.\nIn Computer Vision\u2013ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23\u201328, 2020, Proceedings, Part XXII\n16, pages 696\u2013712. Springer, 2020.\n[2] Benjamin Attal, Jia-Bin Huang, Christian Richardt, Michael\nZollhoefer, Johannes Kopf, Matthew O\u2019Toole, and Changil\nKim.\nHyperreel: High-fidelity 6-dof video with ray-\nconditioned sampling.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 16610\u201316620, 2023.\n[3] Benjamin Attal, Jia-Bin Huang, Michael Zollh\u00a8ofer, Johannes\nKopf, and Changil Kim. Learning neural light fields with\nray-space embedding.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 19819\u201319829, 2022.\n[4] Louis Bavoil and Kevin Myers. Order independent trans-\nparency with dual depth peeling. NVIDIA OpenGL SDK,\n1:12, 2008.\n[5] Chris Buehler, Michael Bosse, Leonard McMillan, Steven\nGortler, and Michael Cohen.\nUnstructured lumigraph\nrendering. In Proceedings of the 28th Annual Conference on\nComputer Graphics and Interactive Techniques, SIGGRAPH\n\u201901, page 425\u2013432, New York, NY, USA, 2001. Association\nfor Computing Machinery.\n[6] Dan Casas, Marco Volino, John Collomosse, and Adrian\nHilton. 4d video textures for interactive character appearance.\nIn Computer Graphics Forum, pages 371\u2013380. Wiley Online\nLibrary, 2014.\n[7] Gaurav Chaurasia, Sylvain Duchene, Olga Sorkine-Hornung,\nand George Drettakis. Depth synthesis and local warps for\nplausible image-based navigation. ACM TOG, 2013.\n[8] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and\nHao Su. Tensorf: Tensorial radiance fields. arXiv, 2022.\n[9] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang,\nFanbo Xiang, Jingyi Yu, and Hao Su.\nMvsnerf: Fast\ngeneralizable radiance field reconstruction from multi-view\nstereo. In ICCV, 2021.\n10\n[10] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and\nAndrea Tagliasacchi. Mobilenerf: Exploiting the polygon\nrasterization pipeline for efficient neural field rendering on\nmobile architectures. arXiv preprint arXiv:2208.00277, 2022.\n[11] Wei Cheng, Ruixiang Chen, Wanqi Yin, Siming Fan, Keyu\nChen, Honglin He, Huiwen Luo, Zhongang Cai, Jingbo Wang,\nYang Gao, et al.\nDna-rendering: A diverse neural actor\nrepository for high-fidelity human-centric rendering. arXiv\npreprint arXiv:2307.10173, 2023.\n[12] Alvaro Collet, Ming Chuang, Pat Sweeney, Don Gillett,\nDennis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,\nand Steve Sullivan. High-quality streamable free-viewpoint\nvideo. ACM Transactions on Graphics (ToG), 34(4):1\u201313,\n2015.\n[13] Abe Davis, Marc Levoy, and Fredo Durand. Unstructured\nlight fields. In Computer Graphics Forum, pages 305\u2013314.\nWiley Online Library, 2012.\n[14] Mingsong Dou, Sameh Khamis, Yury Degtyarev, Philip\nDavidson, Sean Ryan Fanello, Adarsh Kowdle, Sergio Orts\nEscolano, Christoph Rhemann, David Kim, Jonathan Taylor,\net al. Fusion4d: Real-time performance capture of challenging\nscenes. ACM TOG, 2016.\n[15] Robert A Drebin, Loren Carpenter, and Pat Hanrahan. Volume\nrendering. ACM Siggraph Computer Graphics, 22(4):65\u201374,\n1988.\n[16] John Flynn, Ivan Neulander, James Philbin, and Noah Snavely.\nDeepstereo: Learning to predict new views from the world\u2019s\nimagery. In CVPR, June 2016.\n[17] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k\nWarburg, Benjamin Recht, and Angjoo Kanazawa. K-planes:\nExplicit radiance fields in space, time, and appearance. In\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 12479\u201312488, 2023.\n[18] Stephan J Garbin, Marek Kowalski, Matthew Johnson, Jamie\nShotton, and Julien Valentin. Fastnerf: High-fidelity neural\nrendering at 200fps.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 14346\u2013\n14355, 2021.\n[19] Steven J Gortler, Radek Grzeszczuk, Richard Szeliski, and\nMichael F Cohen. The lumigraph. In SIGGRAPH, 1996.\n[20] Jon Hasselgren, Nikolai Hofmann, and Jacob Munkberg.\nShape, light, and material decomposition from images using\nmonte carlo rendering and denoising. Advances in Neural\nInformation Processing Systems, 35:22856\u201322869, 2022.\n[21] Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm,\nGeorge Drettakis, and Gabriel Brostow. Deep blending for\nfree-viewpoint image-based rendering. ACM TOG, 2018.\n[22] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall,\nJonathan T. Barron, and Paul Debevec.\nBaking neural\nradiance fields for real-time view synthesis. In ICCV, 2021.\n[23] Anna Hilsmann, Philipp Fechteler, Wieland Morgenstern,\nWolfgang Paier, Ingo Feldmann, Oliver Schreer, and Peter\nEisert. Going beyond free viewpoint: creating animatable\nvolumetric video of human performances. IET Computer\nVision, pages 350\u2013358, 2020.\n[24] Mustafa Is\u00b8\u0131k, Martin R\u00a8unz, Markos Georgopoulos, Taras\nKhakhulin, Jonathan Starck, Lourdes Agapito, and Matthias\nNie\u00dfner. Humanrf: High-fidelity neural radiance fields for\nhumans in motion. arXiv preprint arXiv:2305.06356, 2023.\n[25] Shubhendu Jena, Franck Multon, and Adnane Boukhayma.\nNeural mesh-based graphics. In European Conference on\nComputer Vision, pages 739\u2013757. Springer, 2022.\n[26] Yue Jiang, Dantong Ji, Zhizhong Han, and Matthias Zwicker.\nSdfdiff: Differentiable rendering of signed distance fields for\n3d shape optimization. In CVPR, 2020.\n[27] Nima Khademi Kalantari, Ting-Chun Wang, and Ravi Ra-\nmamoorthi. Learning-based view synthesis for light field\ncameras. ACM TOG, 2016.\n[28] Petr Kellnhofer, Lars C Jebe, Andrew Jones, Ryan Spicer,\nKari Pulli, and Gordon Wetzstein.\nNeural lumigraph\nrendering. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 4287\u20134297,\n2021.\n[29] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler,\nand George Drettakis. 3d gaussian splatting for real-time\nradiance field rendering. ACM Transactions on Graphics\n(TOG), 42(4):1\u201314, 2023.\n[30] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014.\n[31] Georgios Kopanas, Julien Philip, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. Point-based neural rendering with per-\nview optimization. In Computer Graphics Forum, volume 40,\npages 29\u201343. Wiley Online Library, 2021.\n[32] Jonas Kulhanek and Torsten Sattler. Tetra-nerf: Representing\nneural radiance fields using tetrahedra.\narXiv preprint\narXiv:2304.09987, 2023.\n[33] Kiriakos N Kutulakos and Steven M Seitz. A theory of shape\nby space carving. International journal of computer vision,\n38:199\u2013218, 2000.\n[34] Christoph Lassner and Michael Zollhofer. Pulsar: Efficient\nsphere-based neural rendering.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1440\u20131449, 2021.\n[35] Marc Levoy and Pat Hanrahan. Light field rendering. In\nSIGGRAPH, 1996.\n[36] Ruilong Li, Hang Gao, Matthew Tancik, and Angjoo\nKanazawa. Nerfacc: Efficient sampling accelerates nerfs.\narXiv preprint arXiv:2305.04966, 2023.\n[37] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon Green,\nChristoph Lassner, Changil Kim, Tanner Schmidt, Steven\nLovegrove, Michael Goesele, and Zhaoyang Lv. Neural 3d\nvideo synthesis. arXiv preprint arXiv:2103.02597, 2021.\n[38] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon\nGreen, Christoph Lassner, Changil Kim, Tanner Schmidt,\nSteven Lovegrove, Michael Goesele, Richard Newcombe,\net al.\nNeural 3d video synthesis from multi-view video.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 5521\u20135531, 2022.\n[39] Zhengqi Li, Simon Niklaus, Noah Snavely, and Oliver Wang.\nNeural scene flow fields for space-time view synthesis of\ndynamic scenes. In CVPR, 2021.\n[40] Zhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker,\nand Noah Snavely. Dynibar: Neural dynamic image-based\n11\nrendering. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 4273\u20134284,\n2023.\n[41] Zhengqi Li, Wenqi Xian, Abe Davis, and Noah Snavely.\nCrowdsampling the plenoptic function. In ECCV, 2020.\n[42] Haotong Lin, Sida Peng, Zhen Xu, Tao Xie, Xingyi He, Hujun\nBao, and Xiaowei Zhou. High-fidelity and real-time novel\nview synthesis for dynamic scenes.\nIn SIGGRAPH Asia\nConference Proceedings, 2023.\n[43] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai,\nHujun Bao, and Xiaowei Zhou. Efficient neural radiance\nfields for interactive free-viewpoint video. In SIGGRAPH\nAsia Conference Proceedings, 2022.\n[44] Shanchuan Lin, Linjie Yang, Imran Saleemi, and Soumyadip\nSengupta.\nRobust high-resolution video matting with\ntemporal guidance. In Proceedings of the IEEE/CVF Winter\nConference on Applications of Computer Vision, pages 238\u2013\n247, 2022.\n[45] Shichen Liu, Shunsuke Saito, Weikai Chen, and Hao Li.\nLearning to infer implicit surfaces without 3d supervision.\nNeurIPS, 2019.\n[46] Stephen Lombardi, Tomas Simon, Jason Saragih, Gabriel\nSchwartz, Andreas Lehrmann, and Yaser Sheikh. Neural\nvolumes: Learning dynamic renderable volumes from images.\nIn SIGGRAPH, 2019.\n[47] Stephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael\nZollhoefer, Yaser Sheikh, and Jason Saragih. Mixture of\nvolumetric primitives for efficient neural rendering. ACM\nTransactions on Graphics (TOG), 40(4):1\u201313, 2021.\n[48] Fan Lu, Yan Xu, Guang Chen, Hongsheng Li, Kwan-Yee Lin,\nand Changjun Jiang. Urban radiance field representation\nwith deformable neural mesh primitives.\narXiv preprint\narXiv:2307.10776, 2023.\n[49] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva\nRamanan. Dynamic 3d gaussians: Tracking by persistent\ndynamic view synthesis. arXiv preprint arXiv:2308.09713,\n2023.\n[50] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon,\nNima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and\nAbhishek Kar.\nLocal light field fusion: Practical view\nsynthesis with prescriptive sampling guidelines. ACM TOG,\n2019.\n[51] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view\nsynthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\n[52] Claus M\u00a8uller. Spherical harmonics, volume 17. Springer,\n2006.\n[53] Thomas M\u00a8uller, Alex Evans, Christoph Schied, and Alexander\nKeller. Instant neural graphics primitives with a multiresolu-\ntion hash encoding. ACM Transactions on Graphics (ToG),\n41(4):1\u201315, 2022.\n[54] Richard A Newcombe, Dieter Fox, and Steven M Seitz.\nDynamicfusion: Reconstruction and tracking of non-rigid\nscenes in real-time. In CVPR, 2015.\n[55] Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello,\nWayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim,\nPhilip L Davidson, Sameh Khamis, Mingsong Dou, et al.\nHoloportation: Virtual 3d teleportation in real-time. In UIST,\n2016.\n[56] Keunhong Park, Utkarsh Sinha, Jonathan T. Barron, Sofien\nBouaziz, Dan B Goldman, Steven M. Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn ICCV, 2021.\n[57] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz.\nHypernerf: A higher-\ndimensional representation for topologically varying neural\nradiance fields. arXiv preprint arXiv:2106.13228, 2021.\n[58] Steven Parker, Peter Shirley, and Brian Smits. Single sample\nsoft shadows. Technical report, Technical Report UUCS-\n98-019, Computer Science Department, University of Utah,\n1998.\n[59] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\nAndreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\nFang, Junjie Bai, and Soumith Chintala.\nPytorch: An\nimperative style, high-performance deep learning library. In\nNeurIPS, 2019.\n[60] Nikolay Patakin, Dmitry Senushkin, Anna Vorontsova, and\nAnton Konushin.\nNeural global illumination for inverse\nrendering. In 2023 IEEE International Conference on Image\nProcessing (ICIP), pages 1580\u20131584. IEEE, 2023.\n[61] Sida Peng, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei\nZhou. Representing volumetric videos as dynamic mlp maps.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 4252\u20134262, 2023.\n[62] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,\nQing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body:\nImplicit neural representations with structured latent codes\nfor novel view synthesis of dynamic humans. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9054\u20139063, 2021.\n[63] Eric Penner and Li Zhang. Soft 3d reconstruction for view\nsynthesis. ACM TOG, 2017.\n[64] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-nerf: Neural radiance fields\nfor dynamic scenes. In CVPR, 2021.\n[65] Ruslan Rakhimov, Andrei-Timotei Ardelean, Victor Lempit-\nsky, and Evgeny Burnaev. Npbg++: Accelerating neural point-\nbased graphics. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 15969\u2013\n15979, 2022.\n[66] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Tay-\nlor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia\nGkioxari. Accelerating 3d deep learning with pytorch3d.\narXiv:2007.08501, 2020.\n[67] Christian Reiser, Songyou Peng, Yiyi Liao, and Andreas\nGeiger. Kilonerf: Speeding up neural radiance fields with\nthousands of tiny mlps. In ICCV, pages 14335\u201314345, 2021.\n[68] Darius R\u00a8uckert, Linus Franke, and Marc Stamminger. Adop:\nApproximate differentiable one-pixel point rendering. ACM\nTransactions on Graphics (ToG), 41(4):1\u201314, 2022.\n12\n[69] Jason Sanders and Edward Kandrot. CUDA by example: an\nintroduction to general-purpose GPU programming. Addison-\nWesley Professional, 2010.\n[70] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\n4d decomposition for high-fidelity dynamic reconstruction\nand rendering. arXiv, 2022.\n[71] Meng-Li Shih, Shih-Yang Su, Johannes Kopf, and Jia-Bin\nHuang. 3d photography using context-aware layered depth\ninpainting. In CVPR, 2020.\n[72] Dave Shreiner et al. OpenGL programming guide: the official\nguide to learning OpenGL, versions 3.0 and 3.1. Pearson\nEducation, 2009.\n[73] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\n[74] Vincent Sitzmann, Semon Rezchikov, Bill Freeman, Josh\nTenenbaum, and Fredo Durand. Light field networks: Neural\nscene representations with single-evaluation rendering. Ad-\nvances in Neural Information Processing Systems, 34:19313\u2013\n19325, 2021.\n[75] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias\nNie\u00dfner, Gordon Wetzstein, and Michael Zollh\u00a8ofer. Deep-\nvoxels: Learning persistent 3d feature embeddings. In CVPR,\n2019.\n[76] Vincent Sitzmann, Michael Zollh\u00a8ofer, and Gordon Wetzstein.\nScene representation networks: Continuous 3d-structure-\naware neural scene representations. In NeurIPS, 2019.\n[77] Liangchen Song, Anpei Chen, Zhong Li, Zhang Chen, Lele\nChen, Junsong Yuan, Yi Xu, and Andreas Geiger. Nerfplayer:\nA streamable dynamic scene representation with decomposed\nneural radiance fields. IEEE Transactions on Visualization\nand Computer Graphics, 29(5):2732\u20132742, 2023.\n[78] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron, Ravi\nRamamoorthi, Ren Ng, and Noah Snavely.\nPushing the\nboundaries of view extrapolation with multiplane images.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 175\u2013184, 2019.\n[79] Mohammed Suhail, Carlos Esteves, Leonid Sigal, and\nAmeesh Makadia. Light field neural rendering. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pages 8269\u20138279, June 2022.\n[80] Richard Szeliski and Polina Golland. Stereo matching with\ntransparency and matting. In Sixth International Conference\non Computer Vision (IEEE Cat. No. 98CH36271), pages 517\u2013\n524. IEEE, 1998.\n[81] Suramya Tomar.\nConverting video formats with ffmpeg.\nLinux journal, 2006(146):10, 2006.\n[82] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, and\nHuaping Liu. Mixed neural voxels for fast multi-view video\nsynthesis. arXiv preprint arXiv:2212.00190, 2022.\n[83] Liao Wang, Qiang Hu, Qihan He, Ziyu Wang, Jingyi Yu,\nTinne Tuytelaars, Lan Xu, and Minye Wu. Neural residual\nradiance fields for streamably free-viewpoint videos.\nIn\nProceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 76\u201387, 2023.\n[84] Liao Wang, Jiakai Zhang, Xinhang Liu, Fuqiang Zhao,\nYanshun Zhang, Yingliang Zhang, Minye Wu, Jingyi Yu,\nand Lan Xu. Fourier plenoctrees for dynamic radiance field\nrendering in real-time. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13524\u201313534, 2022.\n[85] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-\nBrualla, Noah Snavely, and Thomas Funkhouser.\nIbrnet:\nLearning multi-view image-based rendering. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 4690\u20134699, 2021.\n[86] Suttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon\nYenphraphai, and Supasorn Suwajanakorn. Nex: Real-time\nview synthesis with neural basis expansion. In CVPR, 2021.\n[87] Guanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng\nZhang, Wei Wei, Wenyu Liu, Qi Tian, and Wang Xinggang.\n4d gaussian splatting for real-time dynamic scene rendering.\narXiv preprint arXiv:2310.08528, 2023.\n[88] Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu. Multi-\nview neural human rendering.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 1682\u20131691, 2020.\n[89] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing\nZhang, and Xiaogang Jin.\nDeformable 3d gaussians for\nhigh-fidelity monocular dynamic scene reconstruction. arXiv\npreprint arXiv:2309.13101, 2023.\n[90] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li\nZhang. Real-time photorealistic dynamic scene representation\nand rendering with 4d gaussian splatting. arXiv preprint arXiv\n2310.10642, 2023.\n[91] Alex Yu, Sara Fridovich-Keil, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks. CVPR, 2022.\n[92] Alex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and\nAngjoo Kanazawa. Plenoctrees for real-time rendering of\nneural radiance fields.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 5752\u2013\n5761, 2021.\n[93] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelNeRF: Neural radiance fields from one or few images.\nIn CVPR, 2021.\n[94] Tao Yu, Zerong Zheng, Kaiwen Guo, Jianhui Zhao, Qionghai\nDai, Hao Li, Gerard Pons-Moll, and Yebin Liu. Doublefusion:\nReal-time capture of human performances with inner body\nshapes from a single depth sensor. In CVPR, 2018.\n[95] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz, and\nFelix Heide. Differentiable point-based radiance fields for\nefficient view synthesis. In SIGGRAPH Asia 2022 Conference\nPapers, pages 1\u201312, 2022.\n[96] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pages\n586\u2013595, 2018.\n[97] C Lawrence Zitnick, Sing Bing Kang, Matthew Uyttendaele,\nSimon Winder, and Richard Szeliski. High-quality video\n13\nview interpolation using a layered representation. ACM TOG,\n2004.\n14\nGround Truth\nOurs (237.9 FPS)\nENeRF (10.4 FPS)\nIBRNet (0.4 FPS)\nMlpMaps (30.3 FPS) DyNeRF (0.2 FPS)\nKPlanes (2.0 FPS)\nFigure A1. Qualitative comparison on the NHR [88] dataset that contains 512\u00d7612 (and 384\u00d7512) images.\nTable A1. Number of views and frames for each dataset\u2019s used\nsequences. The basketball sequence of NHR [88] provides 72\nviews compared to the 56 views for the rest of the dataset.\nDataset\nSequence\nCount\nTraininig\nView\nTesting\nView\nFrame\nCount\nDNA-Rendering [11]\n4\n56\n4\n150\nNHR (sport) [88]\n3\n52\n4\n100\nNHR (basketball) [88]\n1\n68\n4\n100\nENeRF-Outdoor [43]\n3\n17\n1\n100\nNeural3DV [38]\n1\n19\n1\n300\nAppendix\nA. Dataset Settings\nIn Tab. A1, we list the detailed sequence count, view\ncount, and frame count for each of the datasets used in our\nexperiments. For image-based baselines [43, 85] and our\nmethod, the training view is also the source view for the IBR\nprocess. No test views are provided as source views.\nB. Additional Experiments\nB.1. Additional Comparisons on NHR [88]\nIn Tab. A2 and Fig. A1, we provide quantitative and\nqualitative comparisons on the NHR dataset [88] with\nmultiple baselines. KPlanes [17] and DyNeRF [38] can\nonly produce blurry images on this dataset that include fast\nmotions, and their rendering speed is also limited (2.0 and\n0.2 FPS). ENeRF [43] and MlpMaps [61] can be rendered\nTable A2. Quantitative comparison on the NHR [88] dataset.\nThis dataset includes 512\u00d7612 and 384\u00d7512 images. Metrics are\naveraged over all scenes.\nPSNR \u2191\nSSIM \u2191\nLPIPS \u2193\nFPS\nENeRF [43]\n30.765\n0.954\n0.052\n10.432\nIBRNet [85]\n33.537\n0.965\n0.078\n0.369\nKPlanes [17]\n32.933\n0.958\n0.101\n1.979\nMlpMaps [61]\n32.203\n0.953\n0.080\n30.303\nDyNeRF [38]\n30.872\n0.943\n0.117\n0.192\nOurs\n33.743\n0.973\n0.045\n237.919\nat interactive frame rates with moderate resolution, but our\nmethod can produce higher quality results at much higher\nframe rates (30FPS vs. 238FPS).\nB.2. Visualization of SH color\nIn Fig. A2, we visualize the SH color csh(s, d) on 5\nrotating views of the 0013 01 sequence of DNA-Rendering\n[11]. The SH model provides the fine-level appearance that\nenables the continuous view-dependent effects.\nB.3. Comparisons with 3DGS [29]\nWe perform additional comparisons with 3DGS [29] on\nthe first frame of the actor1 4 sequence of the ENeRF-\nOutdoor [43] dataset in Tab. A3 and Fig. A3. The storage\ncost of our method contains both the file sizes (in MB) of the\ntrained model and source images. After the precomputation\nin Sec. 3.4, the main memory usage of our method is 100\nMB per frame, which is still smaller than the disk file size\n15\nView 0\nView 1\nView 2\nView 3\nView 4\nFigure A2. Visualization of c(x, t, d) and csh(s, d) on 5 rotating views of the 0013 01 sequence of DNA-Rendering [11]. The\nview-dependent SH color csh compensates the high-quality but discrete IBR color cibr. We increase the brightness of csh for a clearer\nvisualization. Details of the implementation can be found in Sec. 3.1.\nTable A3. Quantitative comparison on the first frame of the all\nthree sequences of the ENeRF-Outdoor [43] dataset. The dataset\ncontains 18 images of 960 \u00d7 540 resolution. \u201cStorage\u201d indicates\nthe disk file size of the trained models (and source images for our\nmethod).\nPSNR \u2191 SSIM \u2191 LPIPS \u2193\nFPS\nStorage Training\nGaussian [29] 21.633\n0.608\n0.349\n88.355\n715 MB 0.5 hour\nOurs\n26.544\n0.907\n0.145\n148.581 16.0 MB 1.5 hour\nof 3DGS. Our method is trained for 5,000 iterations, and\n3DGS is trained for 30,000 iterations (their default settings)\nusing their official source code. 3DGS renders slower than\nour method because too many points were generated during\ntheir training process. Qualitatively, 3DGS overfits the first\nframe and fails to generalize to novel views, as indicated by\nthe last two rows of Fig. A3.\nCompared to training a 3DGS for every frame, our\nmethod is superior in the following ways. First, the storage\ncost for 3DGS is too large for even a 100-frame video (715\nMB per frame), while our method maintains a reasonable\n2 MB per frame storage overhead (Sec. 5.2). Thanks to\nthe implicit compression of our 4D feature grid and IBR\nmodel Sec. 5.2, our method better utilizes the temporal\nredundancy of the dynamic 3D scene. The optimization\nand precomputation of our method can be viewed as a\nform of compression and decompression, where we encode\nand decode the dynamic 3D scene using 4D feature grids,\nnetwork weights, and source videos. Second, the hybrid\nimage-based appearance model (Sec. 3.1) of our method is\nmore expressive than the spherical harmonics utilized by\n3DGS, thus achieving higher rendering quality as shown by\nFig. A3 and Tab. A3.\n16\nGround\nTruth\nOurs\n(148.6 FPS)\nGaussian\n(88.4 FPS)\nGaussian\n(training view)\nFigure A3. Qualitative comparison on the first frame of the actor1 4 sequence of ENeRF-Outdoor [43] dataset. The first frame contains\n18 images of 960 \u00d7 540 resolution. 3D Gaussian Splatting [29] overfits the training view as indicated by the last two rows.\n17\n"
  },
  {
    "title": "VeRA: Vector-based Random Matrix Adaptation",
    "link": "https://arxiv.org/pdf/2310.11454.pdf",
    "upvote": "26",
    "text": "Published as a conference paper at ICLR 2024\nVERA: VECTOR-BASED RANDOM MATRIX ADAPTATION\nDawid J. Kopiczko\u2217\u2020\nQUVA Lab\nUniversity of Amsterdam\nTijmen Blankevoort\nQualcomm AI Research1\nYuki M. Asano\nQUVA Lab\nUniversity of Amsterdam\nABSTRACT\nLow-rank adapation (LoRA) is a popular method that reduces the number of train-\nable parameters when finetuning large language models, but still faces acute stor-\nage challenges when scaling to even larger models or deploying numerous per-\nuser or per-task adapted models. In this work, we present Vector-based Random\nMatrix Adaptation (VeRA), which significantly reduces the number of trainable\nparameters compared to LoRA, yet maintains the same performance. It achieves\nthis by using a single pair of low-rank matrices shared across all layers and\nlearning small scaling vectors instead. We demonstrate its effectiveness on the\nGLUE and E2E benchmarks, image classification tasks, and show its application\nin instruction-tuning of 7B and 13B language models.\n1\nINTRODUCTION\nIn the era of increasingly large and complex language models, the challenge of efficient adaptation\nfor specific tasks has become more important than ever. While these models provide powerful\ncapabilities, their extensive memory requirements pose a significant bottleneck, particularly when\nadapting them for personalized use. Consider, for example, a cloud-based operating system assistant\nthat continuously learns from and adapts to individual user behaviors and feedback. The need to\nstore multiple checkpoints of finetuned models for each user rapidly escalates the required storage,\neven more so when multiple tasks come into play.\nThe situation is further exacerbated when we look at the state-of-the-art models like GPT-4 (OpenAI,\n2023). Finetuning techniques like LoRA (Hu et al., 2022), while effective, still introduce consider-\nable memory overhead. As an illustrative example, applying LoRA with a rank of 16 to the query\nand value layers of GPT-3 (Brown et al., 2020) would demand at least 288MB of memory, if stored\nin singe-precision \u2013 at a million finetuned weights, e.g., one per user, that would amount to 275TB.\nGiven the recent proliferation of language models and their deployment in personalized assistants,\nedge devices, and similar applications, efficient adaptation methods are paramount. We believe\nthere is untapped potential for even more efficient approaches. Previous work (Aghajanyan et al.,\n2021) pointed out the low intrinsic dimensionality of pretrained models\u2019 features. These studies\nreported numbers much lower than the trainable parameters used in LoRA, suggesting there is room\nfor improvement.\nIn parallel to this, recent research has shown the surprising effectiveness of models utilizing random\nweights and projections (Peng et al., 2021; Ramanujan et al., 2020; Lu et al., 2022; Schrimpf et al.,\n2021; Frankle et al., 2021). Such models serve as the basis of our proposed solution, Vector-based\nRandom Matrix Adaptation (VeRA), which minimizes the number of trainable parameters intro-\nduced during finetuning by reparametrizing the weights matrices. Specifically, we employ \u201cscaling\nvectors\u201d to adapt a pair of frozen random matrices shared between layers. With this approach, many\nmore versions of the model can reside in the limited memory of a single GPU.\n\u2217dj.kopiczko@gmail.com; 1Qualcomm AI Research is an initiative of Qualcomm Technologies, Inc.\n\u2020Datasets were solely downloaded and evaluated by the University of Amsterdam.\n1\narXiv:2310.11454v2  [cs.CL]  16 Jan 2024\nPublished as a conference paper at ICLR 2024\nIn summary, our main contributions are as follows:\n\u2022 We introduce a novel finetuning method with no additional inference time cost. Our method\nfurther reduces the number of trainable parameters compared to the state-of-the-art LoRA\nmethod, while yielding comparable results.\n\u2022 We compare our approach with LoRA and other parameter-efficient adaptation methods on\nthe natural language understanding (GLUE) and natural language generation (E2E) bench-\nmarks, and compare against LoRA on instruction-following and image classification tasks.\n\u2022 We perform an ablation study to better understand the individual components of our method\nand their effects on performance.\n2\nRELATED WORK\nLow-Rank Adaptation (LoRA).\nLoRA offers an innovative solution to the computational chal-\nlenges posed by the finetuning of large pretrained language models. Introduced by Hu et al. (2022),\nthe method employs low-rank matrices to approximate the weight changes during finetuning, ef-\nfectively reducing the number of parameters that need to be trained. Among its advantages, LoRA\nsignificantly lowers the hardware barrier for finetuning by reducing the need for gradient calcula-\ntion and optimizer state maintenance for most parameters. It can also work with quantized model\nweights (Dettmers et al., 2023), reducing the requirements even further. Furthermore, LoRA mod-\nules are easily swappable, making task-switching efficient and less resource-intensive. Importantly,\nand different to adapter-based finetuning approaches (Houlsby et al., 2019; Lin et al., 2020; Pfeiffer\net al., 2021; R\u00a8uckl\u00b4e et al., 2021), LoRA incurs no additional inference time cost when deployed, as\nthe trainable matrices can be merged with the frozen weights.\nBased on this, AdaLoRA (Zhang et al., 2023b) extends the LoRA method, introducing dynamic\nrank adjustment for the low-rank matrices during finetuning. The core idea is to optimally distribute\nthe parameter budget by selectively pruning less important components of the matrices based on an\nimportance metric.\nParameter Efficiency in Existing Methods\nWhile methods such as LoRA have shown significant\nimprovements in finetuning performance, they still require a considerable amount of trainable pa-\nrameters. According to Aghajanyan et al. (2021), the upper bound for intrinsic dimensions is much\nsmaller than what is typically utilized in such methods. For instance, the d901 for RoBERTabase is\nreported to be 896, whereas authors of the LoRA paper reported using 0.3M trainable parameters\nfor this model, suggesting that the parameter count could be reduced further.\nAlthough AdaLoRA takes steps in this direction by dynamically allocating parameters to more crit-\nical layers, we posit that a different approach could achieve substantial parameter reduction, while\ntolerating a marginal performance degradation. This sets the stage for the method we introduce in\nthe following section.\nRandom Models and Projections.\nThe concept of using random matrices and projections for\nmodel efficiency is supported by multiple strands of research. Frankle & Carbin (2019) identified\nthat randomly-initialized neural networks contain subnetworks that are capable of reaching high per-\nformance when trained. Meanwhile, Ramanujan et al. (2020) revealed that there exist subnetworks\nthat can achieve impressive results even in the absence of training. Aghajanyan et al. (2021) showed\nthat training only a small number of parameters, randomly projected back into the full space, could\nachieve 90% of the full-parameter model performance. Ruiz et al. (2023) introduced a parameter-\nefficient finetuning method for personalization of text-to-image models, utilising random frozen\nmatrices inside LoRA. Other works (Lu et al., 2022; Schrimpf et al., 2021; Frankle et al., 2021)\nhave shown that frozen, randomly initialized models, with small sections finetuned, can perform\nsurprisingly well.\n1The smallest dimension d that provides a satisfactory solution, which is 90% of the full training metric, as\ndefined by Li et al. (2018).\n2\nPublished as a conference paper at ICLR 2024\nPretrained Weights\n\ud835\udc4a \u2208 \u211d!\u00d7!\n\ud835\udc34 = \ud835\udca9 0, \ud835\udf0e#\nh\nx\ndim$%\ndim&'(\n\ud835\udc4f = 0\n\ud835\udc51 = \ud835\udfd9\nshared \nacross layers\nshared \nacross layers\nPretrained Weights\n\ud835\udc4a \u2208 \u211d!\u00d7!\n\ud835\udc34 = \ud835\udca9 0, \ud835\udf0e#\nh\nx\ndim$%\ndim&'(\n\ud835\udc5f\n- frozen\n- trainable\nFigure 1: Schematic comparison of LoRA (left) and VeRA (right). LoRA updates the weights matrix\nW by training the low-rank matrices A and B, with intermediate rank r. In VeRA these matrices are\nfrozen, shared across all layers, and adapted with trainable vectors d and b, substantially reducing\nthe number of trainable parameters. In both cases, low-rank matrices and vectors can be merged into\noriginal weights matrix W, introducing no additional latency.\nCollectively, these works create a compelling case for the utilization of frozen random matrices in\nfinetuning methods, providing both a theoretical and an empirical foundation for the approach taken\nin this paper.\n3\nMETHOD\nIn this section, we introduce Vector-based Random Matrix Adaptation, a novel parameter-efficient\nfinetuning method that builds upon and extends the state-of-the-art method, LoRA. The central in-\nnovation in VeRA lies in the reparameterization of the low-rank matrices. Specifically, we freeze\na single pair of randomly initialized matrices, shared across all adapted layers, and introduce train-\nable scaling vectors that allow for layer-wise adaptation, as shown in Figure 1. Similarly to LoRA,\ntrained scaling vectors along with low-rank matrices can be merged into original weights, eliminat-\ning additional inference latency.\n3.1\nMETHOD FORMULATION\nLoRA (Hu et al., 2022) finetunes a matrix product of two low-rank matrices to adapt large-language\nmodels for a new task. Formally, for a pretrained weight matrix W0 \u2208 Rm\u00d7n, the weight update\n\u2206W is constrained to a low-rank decomposition, as expressed in Equation 1\nh = W0x + \u2206Wx = W0x + BAx,\n(1)\nwhere we undeline the parameters updated via gradient descent. This approximation enables the\nmodel to keep the original weight W0 frozen while optimizing only the new low-rank matrices A\nand B. These matrices are much smaller in size than the original matrix due to their rank-reduced\nnature. A has shape m \u00d7 r and B has shape r \u00d7 n, where r \u226a min(m, n) serves as the bottleneck\ndimension. In contrast, our VeRA method is expressed as:\nh = W0x + \u2206Wx = W0x + \u039bbB\u039bdAx\n(2)\nIn this approach, B and A are frozen, random, and shared across layers, while the scaling vectors\nb and d are trainable, and formally denoted by diagonal matrices \u039bb and \u039bd. This approach can\neffectively scale and disable rows and columns of both A and B, allowing for layer-wise adaptation\nwith a minimal number of trainable parameters. Note that in this setup, B \u2208 Rm\u00d7r and A \u2208 Rr\u00d7n\n3\nPublished as a conference paper at ICLR 2024\nare not required to be low-rank. This is because they remain static and we do not need to store\ntheir values. Instead, varying r leads to a linear increase in the number of trainable parameters via\nd \u2208 R1\u00d7r.\n3.2\nPARAMETER COUNT\nTable 1: Theoretical memory required to store trained VeRA and LoRA weights for RoBERTabase,\nRoBERTalarge and GPT-3 models. We assume that LoRA and VeRA methods are applied on query\nand key layers of each transformer block.\nLoRA\nVeRA\nRank # Trainable Parameters Required Bytes # Trainable Parameters Required Bytes\nBASE\n1\n36.8K\n144KB\n18.4K\n72KB\n16\n589.8K\n2MB\n18.8K\n74KB\n256\n9437.1K\n36MB\n24.5K\n96KB\nLARGE\n1\n98.3K\n384KB\n49.2K\n192KB\n16\n1572.8K\n6MB\n49.5K\n195KB\n256\n25165.8K\n96MB\n61.4K\n240KB\nGPT-3\n1\n4.7M\n18MB\n2.4M\n9.1MB\n16\n75.5M\n288MB\n2.8M\n10.5MB\n256\n1207.9M\n4.6GB\n8.7M\n33MB\nWe use Ltuned to denote the number of finetuned layers and dmodel to represent the dimension of these\nlayers. The number of trainable parameters in VeRA is then governed by |\u0398| = Ltuned \u00d7(dmodel +r),\ncontrasting with LoRA\u2019s |\u0398| = 2 \u00d7 Ltuned \u00d7 dmodel \u00d7 r. Specifically, for the lowest rank (i.e.,\nr = 1), VeRA requires approximately half the trainable parameters of LoRA. Moreover, as the\nrank increases, VeRA\u2019s parameter count increases by Ltuned for each increment, a substantial saving\ncompared to LoRA\u2019s 2Ltuneddmodel. This parameter efficiency becomes notably significant in the\ncontext of extremely deep and wide models, such as GPT-3 (Brown et al., 2020), which has 96\nattention layers and a hidden size of 12288.\nBuilding on this efficiency, the main advantage of VeRA is its minimal memory footprint for storing\nthe trained weight adjustments. Because the random frozen matrices can be regenerated from a\nrandom number generator (RNG) seed, these do not need to be stored in memory. This substantially\nreduces the memory requirement, which is now limited to the bytes needed for the trained b and d\nvectors and a single RNG seed. The memory efficiency in comparison to LoRA is shown in Table 1.\n3.3\nINITIALIZATION STRATEGIES\n\u2022 Shared Matrices: In our method, we employ Kaiming initialization (He et al., 2015) for\nthe frozen low-rank matrices A and B. By scaling the values based on matrix dimensions,\nit ensures that a matrix product of A and B maintains a consistent variance for all ranks,\neliminating the need to finetune the learning rate for each rank.\n\u2022 Scaling Vectors: The scaling vector b is initialized to zeros, which aligns with the initial-\nization of matrix B in LoRA and ensures that the weight matrix is unaffected during the\nfirst forward pass. The scaling vector d is initialized with a single non-zero value across\nall its elements, thereby introducing a new hyperparameter that may be tuned for better\nperformance.\nFigure 1 illustrates example initializations for the low-rank matrices and scaling vectors in VeRA.\nSpecifically, the low-rank matrices are initialized using a normal distribution, and the d vector is\ninitialized with ones. Note that alternative initializations, such as uniform distribution for A and B,\nand other non-zero constants for d, are also explored in our experiments.\n4\nEXPERIMENTS\nIn this section, we conduct a series of experiments to evaluate our finetuning method. We start by\ncomparing our approach to LoRA and other baselines on the GLUE and E2E benchmarks. Following\n4\nPublished as a conference paper at ICLR 2024\nthis, we turn our attention to instruction-tuning of Llama models, and image classification with\nVision Transformers. Next, we select one task and vary the rank for both methods, LoRA and VeRA,\nto examine how performance scales with the number of trainable parameters. Lastly, an ablation\nstudy sheds light on the importance of each component in our method, including the influence of\ndifferent initializations.\nBaselines.\nWe compare VeRA to the following baselines:\n\u2022 Full finetuning - the model is initialized with pretrained weights and all parameters are\nbeing trained.\n\u2022 Bitfit - this baseline involves the sole finetuning of bias vectors, keeping all other parameters\nfixed. This technique has been investigated in depth by Zaken et al. (2022).\n\u2022 Adapter tuning - initially introduced by Houlsby et al. (2019), involves the integration of\nadapter layers between the self-attention and MLP modules, followed by a residual con-\nnection. This setup includes two fully connected layers and a nonlinearity and is denoted\nas AdapterH. A variation by Lin et al. (2020), AdapterL, employs the adapter layer solely\nafter the MLP module and subsequent to a LayerNorm. This closely resembles an alter-\nnative design suggested by Pfeiffer et al. (2021), referred to as AdapterP. Another base-\nline, termed AdapterDrop by R\u00a8uckl\u00b4e et al. (2021), enhances efficiency by omitting certain\nadapter layers and is represented as AdapterD.\n\u2022 LoRA (Hu et al., 2022) - as introduced in the earlier section.\n4.1\nGLUE BENCHMARK\nWe evaluate our approach on the General Language Understanding Evaluation (GLUE) benchmark\n(Wang et al., 2019), employing the RoBERTabase and RoBERTalarge models (Liu et al., 2019). For\nRoBERTabase we use a rank of 1024, and for RoBERTalarge a rank of 256. The shared matrices are\ninitialized using the uniform version of Kaiming initialization as implemented in PyTorch (Paszke\net al., 2019), with an initial value of 0.1 for the d vector.\nOur experimental setup generally aligns with that of Hu et al. (2022), applying our method to the\nquery and value projection matrices in each self-attention module and fully training the classification\nhead. Unlike Hu et al. (2022), who used an additional hyperparameter \u03b1 to adjust gradients for the\nadapted layers, we introduce separate learning rates for the classification head and the adapted layers.\nWe determine the learning rates and the number of training epochs through hyperparameter tuning;\nfor detailed settings, refer to the Table 8 in Appendix A. The batch size is set to 64 for RoBERTabase\nand 32 for RoBERTalarge, with maximum sequence lengths of 512 and 128 respectively.\nDue to time constraints and budget limitations, we omit the time-intensive MNLI and QQP tasks,\nthus forgoing the use of the MNLI trick2 for tasks MRPC, RTE, and STS-B. In line with Hu et al.\n(2022), we report the number of trainable parameters attributable to the finetuned layers, explicitly\nexcluding the classification head, which is trained in a standard way. We perform 5 runs with\ndifferent random seeds, recording the best epoch\u2019s outcome for each run, and report the median of\nthese results.\nResults.\nTable 2 reveals that VeRA performs competitively with LoRA across both models, yet\nachieves these results with an order of magnitude fewer parameters.\n4.2\nE2E BENCHMARK\nFor the E2E benchmark (Novikova et al., 2017), we follow the experimental setup from Hu et al.\n(2022) and finetune the GPT-2 (Radford et al., 2019) Medium and Large models. For LoRA we use\nthe implementation and set of hyperparameters provided in Hu et al. (2022), while for VeRA we\nchange the rank and learning rate, both of which are tuned. Table with all hyperparameters used can\nbe found in Appendix A.\n2For the RoBERTabase model and MRPC, RTE and STS-B tasks, Hu et al. (2022) initialized the model with\nthe best weights finetuned on the MNLI task.\n5\nPublished as a conference paper at ICLR 2024\nTable 2: Results for different adaptation methods on the GLUE benchmark. We report Matthew\u2019s\ncorrelation for CoLA, Pearson correlation for STS-B, and accuracy for the remaining tasks. In all\ncases, higher values indicate better performance. Results of all methods except VeRA are sourced\nfrom prior work (Hu et al., 2022; Zhang et al., 2023a). VeRA performs on par with LoRA with an\norder of magnitude fewer parameters.\nMethod\n# Trainable\nParameters\nSST-2\nMRPC\nCoLA\nQNLI\nRTE\nSTS-B\nAvg.\nBASE\nFT\n125M\n94.8\n90.2\n63.6\n92.8\n78.7\n91.2\n85.2\nBitFit\n0.1M\n93.7\n92.7\n62.0\n91.8\n81.5\n90.8\n85.4\nAdptD\n0.3M\n94.2\u00b10.1\n88.5\u00b11.1\n60.8\u00b10.4\n93.1\u00b10.1\n71.5\u00b12.7\n89.7\u00b10.3\n83.0\nAdptD\n0.9M\n94.7\u00b10.3\n88.4\u00b10.1\n62.6\u00b10.9\n93.0\u00b10.2\n75.9\u00b12.2\n90.3\u00b10.1\n84.2\nLoRA\n0.3M\n95.1\u00b10.2\n89.7\u00b10.7\n63.4\u00b11.2\n93.3\u00b10.3\n86.6\u00b10.7\n91.5\u00b10.2\n86.6\nVeRA\n0.043M\n94.6\u00b10.1\n89.5\u00b10.5\n65.6\u00b10.8\n91.8\u00b10.2\n78.7\u00b10.7\n90.7\u00b10.2\n85.2\nLARGE\nAdptP\n3M\n96.1\u00b10.3\n90.2\u00b10.7\n68.3\u00b11.0\n94.8\u00b10.2\n83.8\u00b12.9\n92.1\u00b10.7\n87.6\nAdptP\n0.8M\n96.6\u00b10.2\n89.7\u00b11.2\n67.8\u00b12.5\n94.8\u00b10.3\n80.1\u00b12.9\n91.9\u00b10.4\n86.8\nAdptH\n6M\n96.2\u00b10.3\n88.7\u00b12.9\n66.5\u00b14.4\n94.7\u00b10.2\n83.4\u00b11.1\n91.0\u00b11.7\n86.8\nAdptH\n0.8M\n96.3\u00b10.5\n87.7\u00b11.7\n66.3\u00b12.0\n94.7\u00b10.2\n72.9\u00b12.9\n91.5\u00b10.5\n84.9\nLoRA-FA\n3.7M\n96.0\n90.0\n68.0\n94.4\n86.1\n92.0\n87.7\nLoRA\n0.8M\n96.2\u00b10.5\n90.2\u00b11.0\n68.2\u00b11.9\n94.8\u00b10.3\n85.2\u00b11.1\n92.3\u00b10.5\n87.8\nVeRA\n0.061M\n96.1\u00b10.1\n90.9\u00b10.7\n68.0\u00b10.8\n94.4\u00b10.2\n85.9\u00b10.7\n91.7\u00b10.8\n87.8\nTable 3: Results for different adaptation methods on the E2E benchmark and GPT2 Medium and\nLarge models. Results with (1,2,3) are taken from prior work: 1(Hu et al., 2022), 2(Valipour et al.,\n2022), 3(Zi et al., 2023). VeRA outperforms LoRA with 3 and 4 times less trainable parameters, for\nGPT2 Medium and Large respectively.\nMethod\n# Trainable\nParameters\nBLEU\nNIST\nMETEOR\nROUGE-L\nCIDEr\nMEDIUM\nFT1\n354.92M\n68.2\n8.62\n46.2\n71.0\n2.47\nAdptL1\n0.37M\n66.3\n8.41\n45.0\n69.8\n2.40\nAdptL1\n11.09M\n68.9\n8.71\n46.1\n71.3\n2.47\nAdptH1\n11.09M\n67.3\n8.50\n46.0\n70.7\n2.44\nDyLoRA2\n0.39M\n69.2\n8.75\n46.3\n70.8\n2.46\nAdaLoRA3\n0.38M\n68.2\n8.58\n44.1\n70.7\n2.35\nLoRA\n0.35M\n68.9\n8.69\n46.4\n71.3\n2.51\nVeRA\n0.098M\n70.1\n8.81\n46.6\n71.5\n2.50\nLARGE\nFT1\n774.03M\n68.5\n8.78\n46.0\n69.9\n2.45\nAdptL1\n0.88M\n69.1\n8.68\n46.3\n71.4\n2.49\nAdptL1\n23.00M\n68.9\n8.70\n46.1\n71.3\n2.45\nLoRA\n0.77M\n70.1\n8.80\n46.7\n71.9\n2.52\nVeRA\n0.17M\n70.3\n8.85\n46.9\n71.6\n2.54\nResults.\nWe report results from the last epoch. Table 3 shows that VeRA outperforms LoRA with\n3 and 4 times less trainable parameters, for GPT2 Medium and Large respectively.\n4.3\nINSTRUCTION TUNING\nInstruction tuning is a process by which language models are finetuned to follow specific instructions\nmore effectively (Ouyang et al., 2022). We demonstrate the efficacy of VeRA in enabling Llama\n(Touvron et al., 2023a) and Llama2 (Touvron et al., 2023b) models to follow instructions using only\n1.6M and 2.4M trainable parameters, for 7B and 13B variants respectively, in contrast to 159.9M\nand 250.3M trainable parameters when employing LoRA with a rank of 64 as proposed by Dettmers\net al. (2023).\nWe perform finetuning using both LoRA and VeRA, by applying both methods on all linear layers\nexcept the top one, similarly to Dettmers et al. (2023). Additionally, we leverage the quantization\ntechniques from Dettmers et al. (2023) to train the model on a single GPU.\n6\nPublished as a conference paper at ICLR 2024\nFor our experiment, we employ the Alpaca dataset (Taori et al., 2023), specifically its cleaned ver-\nsion3. This dataset comprises 51K instructions and demonstrations and is suitable for instruction-\ntuning. The cleaned version corrects multiple issues such as hallucinations, merged instructions, and\nempty outputs. We train for one epoch, preceded by a learning rate sweep.\nWe evaluate finetuned models on MT-Bench (Zheng et al., 2023), by generating model responses to a\npre-defined set of 80 multi-turn questions and subsequently evaluating these using GPT-4 (OpenAI,\n2023). GPT-4 reviews the answers and assigns a quantitative score on a scale of 10 to each response.\nWe present the average scores alongside the number of trainable parameters in Table 4.\nTable 4: Average scores on MT-Bench assigned by GPT-4 to the answers generated by models\nfine-tuned with VeRA and LoRA methods, and the base Llama 13B model. VeRA closely matches\nperformance of LoRA on the instruction-following task, with 100x reduction in trainable parameters.\nModel\nMethod\n# Parameters\nScore\nLlama 13B\n-\n-\n2.61\nLLAMA 7B\nLoRA\n159.9M\n5.03\nVeRA\n1.6M\n4.77\nLLAMA 13B\nLoRA\n250.3M\n5.31\nVeRA\n2.4M\n5.22\nLLAMA2 7B\nLoRA\n159.9M\n5.19\nVeRA\n1.6M\n5.08\nLLAMA2 13B\nLoRA\n250.3M\n5.77\nVeRA\n2.4M\n5.93\nWe find that despite the 100x reduction in the number of trainable parameters, our method closely\nmatches the performance of LoRA-based finetuning.\n4.4\nIMAGE CLASSIFICATION\nTo evaluate the method on the image classification task, we adapt Vision Transformer (ViT) (Doso-\nvitskiy et al., 2021), Base and Large variants, on datasets - CIFAR100 (Krizhevsky, 2009), Food101\n(Bossard et al., 2014), Flowers102 (Nilsback & Zisserman, 2008), and RESISC45 (Cheng et al.,\n2017). For each dataset we train on a subset of 10 samples per class, and evaluate on the full test set\n(CIFAR100, Food101, Flowers102) or on all the remaining samples (RESISC45). We use weights\nof ViT models pretrained on the ImageNet-21k (Deng et al., 2009) dataset.\nWe evaluated LoRA and VeRA methods applied on the query and value layers of ViT, along with\ntwo baselines - fully-finetuned model (referred to as Full), and training the classification head only\n(referred to as Head). Similarly to the GLUE benchmark, we use rank 8 for LoRA, and rank 256 for\nVeRA. We tuned learning rates for all methods and reported results after 10 epochs in Table 5. The\nreported parameter count excludes the classification head, which has to be trained in all methods.\nWe find that VeRA approaches performance of LoRA on the Base model for three datasets and\noutperforms it for Flowers102, despite using over 10x fewer trainable parameters. For ViT-Large, it\noutperforms LoRA for three datasets: CIFAR100, Flowers102 and RESISC45.\n4.5\nSCALING THE NUMBER OF TRAINABLE PARAMETERS\nFinally, we investigate the trade-offs involved in parameter scalability for both LoRA and our method\nusing the RoBERTalarge model on the RTE task from the GLUE benchmark. We use a set of ranks\nr = {1, 4, 16, 64, 256, 1024} for VeRA and r = {1, 2, 4, 8, 16, 32, 64} for LoRA, and observe\nthe trade-off between trainable parameters and the accuracy. We replicate each configuration five\ntimes for different random seeds, and report the median of results. For LoRA, we employ the\nHuggingFace PEFT (Mangrulkar et al., 2022) implementation, adhering to the hyperparameters\nspecified in Hu et al. (2022). Our own method uses the same hyperparameters as employed in the\n3https://huggingface.co/datasets/yahma/alpaca-cleaned\n7\nPublished as a conference paper at ICLR 2024\nTable 5: Vision models finetuned with VeRA and LoRA on different image classification datasets.\nVeRA approaches performance of LoRA for the smaller model, and outperforms it in the case of the\nlarge model, with over 10x fewer trainable parameters.\nMethod\n# Trainable\nParameters\nCIFAR100\nFood101\nFlowers102\nRESISC45\nVIT-B\nHead\n-\n77.7\n86.1\n98.4\n67.2\nFull\n85.8M\n86.5\n90.8\n98.9\n78.9\nLoRA\n294.9K\n85.9\n89.9\n98.8\n77.7\nVeRA\n24.6K\n84.8\n89.0\n99.0\n77.0\nVIT-L\nHead\n-\n79.4\n76.5\n98.9\n67.8\nFull\n303.3M\n86.8\n78.7\n98.8\n79.0\nLoRA\n786.4K\n87.0\n79.5\n99.1\n78.3\nVeRA\n61.4K\n87.5\n79.2\n99.2\n78.6\n10\n5\n10\n6\n# Trainable Parameters\n83\n84\n85\n86\nAccuracy [%]\nVeRA\nLoRA\nFigure 2: Performance of LoRA and VeRA\nmethods for varying ranks on RTE task.\n0\n5\n10\n15\n20\nLayer\n4\n6\n8\n10\n12\n||d||2\nquery\nvalue\nFigure 3: Magnitude of the adapted d vec-\ntor for query and value matrices across lay-\ners for RoBERTa-L on the RTE task.\nRTE experiments from the previous subsection. The results, depicted in Figure 2, reveal that our\nmethod is significantly more parameter-efficient. Notably, when the higher-rank VeRA has the same\nnumber of parameters as standard LoRA, it outperforms LoRA by 4 accuracy percentage points.\n4.6\nABLATION STUDY\nIn this section, we conduct an ablation study to examine the impact of individual components of our\nmethod. All subsequent experiments focus on the MRPC and RTE tasks and utilize the RoBERTalarge\nmodel. We adhere to the hyperparameters used in previous experiments, modifying only the com-\nponent under investigation for each test. Each experiment is run with 5 random seeds, and we report\nthe mean and standard deviation of the results.\nSingle Scaling Vector\nWe first investigate the necessity of both the d and b scaling vectors in our\nmethod. We create two ablation setups: one that excludes d (termed as only b) and another that omits\nb (termed as only d). In the only d setup, d is initialized with zeros. As shown in Table 6, omitting\neither scaling vector compromises performance. The only d configuration performs slightly better\nthan its only b counterpart. This disparity in performance underscores the higher expressiveness of\nTable 6: Ablation study results for the impact of the d and b scaling vectors and different initializa-\ntion strategies. Our default settings are highlighted with blue color.\n(a) Scaling Vector Ablations\nMethod MRPC\nRTE\nVeRA 90.5\u00b10.7\n85.8\u00b10.7\nonly d 89.7\u00b10.0\n67.0\u00b113.9\nonly b 81.6\u00b110.1 64.3\u00b111.5\n(b) Matrix Initialization\nMatrix Init.\nMRPC\nRTE\nKaiming Unif. 90.5\u00b10.7 85.8\u00b10.7\nKaiming Norm. 90.0\u00b11.1 82.6\u00b15.2\nUniform[0.0,0.1] 68.9\u00b11.3 53.1\u00b10.8\n(c) Vector Initialization\nd Init. MRPC\nRTE\n10\u22121 90.5\u00b10.7 85.8\u00b10.7\n10\u22127 90.8\u00b10.9 84.7\u00b10.9\n1.0\n70.3\u00b11.2 60.3\u00b112.4\n8\nPublished as a conference paper at ICLR 2024\nTable 7: Results for selected GLUE tasks using shared and unique random matrices.\nRandom Matrices\nMRPC\nRTE\nCoLA\nSTS-B\nShared\n90.0\u00b10.9\n84.6\u00b11.5\n67.7\u00b10.8\n91.5\u00b10.6\nUnique\n90.7\u00b10.3\n84.6\u00b10.8\n68.3\u00b11.8\n91.5\u00b10.2\nthe d scaling vector over the b vector. Specifically, d modulates the rows of both low-rank matrices,\nthereby influencing a broader aspect of the final constructed matrix. In contrast, b only scales the\nrows of the final matrix resulting from the product of the low-rank matrices.\nInitialization of Shared Matrices\nWe examine three different initialization schemes for the\nshared matrices: Kaiming normal, Kaiming uniform, and uniform initialization within the range\n[0, 0.1]. As per the results in Table 6, both Kaiming initializations outperform the uniform range\ninitialization, with uniform variant having slightly better results than the normal one.\nInitialization of Scaling Vector\nWe further explore the impact of the initialization values for the\nd vector. Experiments are conducted with dinit set at 1.0, 10\u22121, and 10\u22127. The results in Table\n6 show that the choice of dinit significantly influences the method\u2019s performance; in the settings\nwe examined, values 10\u22121 and 10\u22127 outperformed 1.0, potentially offering more flexibility in the\noptimization process through early sign changes in selected rows of the frozen matrices.\nMagnitude of Adaptation\nIn Figure 3 we provide a visualisation of the magnitude of the changes\nof the d vectors after finetuning on RTE task. Because the low-rank frozen matrices remain the same\nfor each layer, we can directly compare the length of the d vector across layers to account for its\nrelative adaptation. Overall, we find that the largest adaptation happens for query matrices compared\nto the value ones, indicating a larger need or ease for finetuning a model there. Furthermore, similar\nto previous efficient adaptation methods\u2019 findings (Zhang et al., 2023b; Liu et al., 2021) we also\nobserve a higher adaptation for the later layers compared to earlier ones.\nSharing Random Matrices\nWe conduct experiments on RTE, MRPC, CoLA, and STS-B tasks\nto assess the impact of sharing random matrices on the performance. We evaluate two setups - one\nwith random matrices shared across all adapted layers, and another with uniquely generated ones.\nResults in Table 7 show that the mean performance is identical in case of tasks RTE and STS-B, and\nthere is a slight improvement for MRPC and CoLA when using unique matrices.\n5\nCONCLUSION\nIn this work, we introduce a finetuning method that significantly reduces the number of trainable\nparameters compared to LoRA, yielding similar or better results on downstream tasks. Specifically,\nit achieved ten-fold reduction in parameters yielding the same performance on the GLUE benchmark\nfor RoBERTalarge, ten-fold reduction on image classification tasks, and three-fold reduction on the\nE2E benchmark. This method is particularly well-suited for scenarios that require frequent swapping\nof numerous finetuned models, such as cloud-based AI services personalized for individual users.\nDue to the minimal size of the scaling vectors, many versions can reside in the limited memory of a\nsingle GPU, thus substantially improving serving efficiency and removing the bottleneck of loading\nspecific models into memory.\nWhile the current study focuses on language and vision models with Transformer architecture, the\napplicability of the method across different architectures and domains remains an area for future\nresearch. Moreover, the performance of the method may benefit from additional refinements, such\nas dynamic parameter budget allocation, or different initialization and regularization techniques.\nACKNOWLEDGEMENTS\nThis work is financially supported by Qualcomm Technologies Inc., the University of Amsterdam\nand the allowance Top consortia for Knowledge and Innovation (TKIs) from the Netherlands Min-\nistry of Economic Affairs and Climate Policy. We also acknowledge the use of the National Super-\ncomputer Snellius and Distributed ASCI Supercomputer 6 (Bal et al., 2016) for essential computa-\ntional tasks.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nArmen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality explains the ef-\nfectiveness of language model fine-tuning. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the 11th International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers), pp. 7319\u20137328, Online, August 2021. As-\nsociation for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.568. URL https:\n//aclanthology.org/2021.acl-long.568.\nH. Bal, D. Epema, C. de Laat, R. van Nieuwpoort, J. Romein, F. Seinstra, C. Snoek, and H. Wijshoff.\nA medium-scale distributed system for computer science research: Infrastructure for the long\nterm. Computer, 49(05):54\u201363, may 2016. ISSN 1558-0814. doi: 10.1109/MC.2016.127.\nLukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative com-\nponents with random forests. In European Conference on Computer Vision, 2014.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agar-\nwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh,\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei.\nLanguage models are few-shot learners.\nIn\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-\nral Information Processing Systems, volume 33, pp. 1877\u20131901. Curran Associates, Inc.,\n2020.\nURL https://proceedings.neurips.cc/paper_files/paper/2020/\nfile/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.\nGong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Bench-\nmark and state of the art. Proceedings of the IEEE, 105(10):1865\u20131883, 2017.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https:\n//lmsys.org/blog/2023-03-30-vicuna/.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby.\nAn image is worth 16x16 words: Transformers for image recogni-\ntion at scale. In International Conference on Learning Representations, 2021. URL https:\n//openreview.net/forum?id=YicbFdNTTy.\nJonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural\nnetworks. In ICLR, 2019.\nJonathan Frankle, David J. Schwab, and Ari S. Morcos. Training batchnorm and only batchnorm:\nOn the expressive power of random features in {cnn}s. In International Conference on Learning\nRepresentations, 2021. URL https://openreview.net/forum?id=vYeQQ29Tbvx.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification. In 2015 IEEE International Conference on\nComputer Vision (ICCV), pp. 1026\u20131034, 2015. doi: 10.1109/ICCV.2015.123.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp,\n2019.\n10\nPublished as a conference paper at ICLR 2024\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Con-\nference on Learning Representations, 2022.\nURL https://openreview.net/forum?\nid=nZeVKeeFYf9.\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Jason Yosinski. Measuring the intrinsic dimension\nof objective landscapes. In International Conference on Learning Representations, 2018. URL\nhttps://openreview.net/forum?id=ryup8-WCW.\nZhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model\nvia parameter-efficient transfer learning. In Findings of the Association for Computational Lin-\nguistics: EMNLP 2020, pp. 441\u2013459, Online, November 2020. Association for Computational\nLinguistics.\ndoi: 10.18653/v1/2020.findings-emnlp.41.\nURL https://aclanthology.\norg/2020.findings-emnlp.41.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Lam Tam, Zhengxiao Du, Zhilin Yang, and Jie Tang. P-\ntuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks.\narXiv preprint arXiv:2110.07602, 2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach, 2019.\nKevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Frozen pretrained transformers as\nuniversal computation engines. Proceedings of the AAAI Conference on Artificial Intelligence, 36\n(7):7628\u20137636, Jun. 2022. doi: 10.1609/aaai.v36i7.20729. URL https://ojs.aaai.org/\nindex.php/AAAI/article/view/20729.\nSourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, and Sayak Paul.\nPeft:\nState-of-the-art parameter-efficient fine-tuning methods.\nhttps://github.com/\nhuggingface/peft, 2022.\nM-E. Nilsback and A. Zisserman. Automated flower classification over a large number of classes.\nIn Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing,\nDec 2008.\nJekaterina Novikova, Ond\u02c7rej Du\u02c7sek, and Verena Rieser.\nThe E2E dataset: New challenges for\nend-to-end generation. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Di-\nalogue, pp. 201\u2013206, Saarbr\u00a8ucken, Germany, August 2017. Association for Computational Lin-\nguistics. doi: 10.18653/v1/W17-5525. URL https://aclanthology.org/W17-5525.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kel-\nton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike,\nand Ryan Lowe.\nTraining language models to follow instructions with human feedback.\nIn\nAlice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neu-\nral Information Processing Systems, 2022. URL https://openreview.net/forum?id=\nTG8KACxEON.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner,\nLu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance\ndeep learning library. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox,\nand R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Cur-\nran Associates, Inc., 2019. URL https://proceedings.neurips.cc/paper_files/\npaper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf.\n11\nPublished as a conference paper at ICLR 2024\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong.\nRandom feature attention. In International Conference on Learning Representations, 2021. URL\nhttps://openreview.net/forum?id=QtTKTdVrFBB.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R\u00a8uckl\u00b4e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\nFusion: Non-destructive task composition for transfer learning. In Proceedings of the 16th Con-\nference of the European Chapter of the Association for Computational Linguistics: Main Volume,\npp. 487\u2013503, Online, April 2021. Association for Computational Linguistics. doi: 10.18653/v1/\n2021.eacl-main.39. URL https://aclanthology.org/2021.eacl-main.39.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. 2019.\nV. Ramanujan, M. Wortsman, A. Kembhavi, A. Farhadi, and M. Rastegari.\nWhat\u2019s hidden\nin a randomly weighted neural network?\nIn 2020 IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pp. 11890\u201311899, Los Alamitos, CA, USA, jun 2020.\nIEEE Computer Society.\ndoi:\n10.1109/CVPR42600.2020.01191.\nURL https://doi.\nieeecomputersociety.org/10.1109/CVPR42600.2020.01191.\nAndreas R\u00a8uckl\u00b4e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\nIryna Gurevych. AdapterDrop: On the efficiency of adapters in transformers. In Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7930\u20137946,\nOnline and Punta Cana, Dominican Republic, November 2021. Association for Computational\nLinguistics. doi: 10.18653/v1/2021.emnlp-main.626. URL https://aclanthology.org/\n2021.emnlp-main.626.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Wei Wei, Tingbo Hou, Yael Pritch, Neal Wadhwa,\nMichael Rubinstein, and Kfir Aberman. Hyperdreambooth: Hypernetworks for fast personaliza-\ntion of text-to-image models, 2023.\nMartin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A. Hosseini, Nancy\nKanwisher, Joshua B. Tenenbaum, and Evelina Fedorenko.\nThe neural architecture of lan-\nguage: Integrative modeling converges on predictive processing. Proceedings of the National\nAcademy of Sciences, 118(45):e2105646118, 2021.\ndoi: 10.1073/pnas.2105646118.\nURL\nhttps://www.pnas.org/doi/abs/10.1073/pnas.2105646118.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model.\nhttps://github.com/tatsu-lab/stanford_alpaca, 2023.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Ar-\nmand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation\nlanguage models, 2023a.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,\nCristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,\nSaghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel\nKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,\nDiana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,\nIgor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,\nAlan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen\nZhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,\nSergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,\n2023b.\nMojtaba Valipour, Mehdi Rezagholizadeh, Ivan Kobyzev, and Ali Ghodsi. Dylora: Parameter effi-\ncient tuning of pre-trained models using dynamic search-free low-rank adaptation. arXiv preprint\narXiv:2210.07558, 2022.\n12\nPublished as a conference paper at ICLR 2024\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nInternational Conference on Learning Representations, 2019. URL https://openreview.\nnet/forum?id=rJ4km2R5t7.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning\nfor transformer-based masked language-models, 2022.\nLongteng Zhang, Lin Zhang, Shaohuai Shi, Xiaowen Chu, and Bo Li. Lora-fa: Memory-efficient\nlow-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303,\n2023a.\nQingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, and\nTuo Zhao. Adaptive budget allocation for parameter-efficient fine-tuning. In The Eleventh In-\nternational Conference on Learning Representations, 2023b. URL https://openreview.\nnet/forum?id=lq62uWRJjiY.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\nZi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica.\nJudging llm-as-a-judge with mt-bench and chatbot arena, 2023.\nBojia Zi, Xianbiao Qi, Lingzhi Wang, Jianan Wang, Kam-Fai Wong, and Lei Zhang.\nDelta-\nlora: Fine-tuning high-rank parameters with the delta of low-rank matrices.\narXiv preprint\narXiv:2309.02411, 2023.\n13\nPublished as a conference paper at ICLR 2024\nA\nHYPERPARAMETERS\nTable 8: Hyperparameter configurations for different model sizes on GLUE benchmark. Optimizer,\nWarmup Ratio, and LR Schedule are taken from Hu et al. (2022)\nModel\nHyperparameter\nSST-2\nMRPC\nCoLA\nQNLI\nRTE\nSTS-B\nOptimizer\nAdamW\nWarmup Ratio\n0.06\nLR Schedule\nLinear\nInit. of Shared Matrices\nKaiming Uniform\nInitial Value of d\n0.1\nBASE\n# GPUs\n1\nVeRA Rank\n1024\nEpochs\n60\n30\n80\n25\n160\n80\nLearning Rate (Head)\n4E-3\n4E-3\n1E-2\n4E-3\n1E-2\n1E-2\nLearning Rate (VeRA)\n4E-3\n1E-2\n1E-2\n1E-2\n4E-3\n1E-2\nMax Seq. Len.\n512\nBatch Size Per GPU\n64\nLARGE\n# GPUs\n4\nVeRA Rank\n256\nEpochs\n10\n40\n40\n20\n40\n20\nLearning Rate (Head)\n6E-3\n3E-3\n6E-3\n2E-4\n2E-3\n2E-3\nLearning Rate (VeRA)\n1E-2\n3E-2\n1E-2\n1E-2\n2E-2\n2E-2\nMax Seq. Len.\n128\nBatch Size Per GPU\n32\nIn Table 8, we provide the hyperparameters used for the GLUE benchmark in the main paper. Note\nthat due to our academic compute we were not able to run full grid searches on any hyperparame-\nters. We only evaluated different learning rates and number of epochs and even relied on existing\nconfigurations of LoRA (Optimizer, Warmup ratio, LR schedule).\nTable 9: Hyperparameter configurations for instruction-tuning.\nHyperparameter\nLoRA\nVeRA\n# GPUs\n1\nOptimizer\nAdamW\nWarmup Ratio\n0.1\nBatch Size\n4\nAccumulation Steps\n4\nEpochs\n1\nLR Schedule\nCosine\nRank\n64\n1024\nLearning Rate\n4E-4\n4E-3\n14\nPublished as a conference paper at ICLR 2024\nTable 10: Hyperparameter configurations for VeRA on the E2E benchmark, for GPT2 Medium and\nLarge models.\nHyperparameter\nMedium\nLarge\n# GPUs\n1\nOptimizer\nAdamW\nLearning Rate Schedule\nLinear\nWeight Decay\n0.01\nBatch Size\n8\nEpochs\n5\nWarmup Steps\n500\nLabel Smooth\n0.1\nRank\n1024\nLearning Rate\n1E-1\n2E-2\nTable 11: Hyperparameter configurations for VeRA and LoRA for finetuning ViT on the image\nclassification datasets. Full, LoRA and VeRA methods have two learning rates - one for the classi-\nfication head, and the other for the rest.\nModel\nHyperparameter\nCIFAR100\nFood101\nFlowers102\nRESISC45\n# GPUs\n1\nOptimizer\nAdamW\nLR Schedule\nLinear\nWeight Decay\n0.01\nVeRA Rank\n256\nLoRA Rank\n8\nBASE\nLR-Head (Head)\n4E-3\n4E-3\n4E-3\n4E-2\nLR (Full)\n4E-5\n4E-5\n4E-5\n8E-5\nLR-Head (Full)\n4E-3\n4E-2\n4E-3\n4E-3\nLR (VeRA)\n2E-2\n4E-2\n4E-2\n7E-2\nLR-Head (VeRA)\n4E-3\n4E-2\n4E-3\n5E-3\nLR (LoRA)\n4E-3\n4E-3\n4E-3\n4E-3\nLR-Head (LoRA)\n4E-3\n4E-3\n4E-3\n4E-3\nLARGE\nLR-Head (Head)\n4E-4\n4E-3\n4E-3\n4E-3\nLR (Full)\n4E-5\n4E-5\n4E-5\n8E-5\nLR-Head (Full)\n4E-3\n4E-3\n8E-3\n4E-3\nLR (VeRA)\n4E-2\n4E-2\n4E-2\n7E-2\nLR-Head (VeRA)\n2E-3\n2E-3\n2E-3\n3E-3\nLR (LoRA)\n4E-3\n4E-3\n4E-3\n4E-3\nLR-Head (LoRA)\n4E-3\n4E-3\n4E-3\n4E-4\n15\nPublished as a conference paper at ICLR 2024\nB\nRELATIVE PERFORMANCE GAIN.\nAdptP(3M) AdptP(0.8M) AdptH(6M) AdptH(0.8M)\nLoRA\nVeRA\nMethod\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\nPerformance Gain (%)\n0.05\n0.17\n0.02\n0.15\n0.18\n2.64\nFigure 4: Performance gains per 1K trainable parameters on the RTE task for RoBERTalarge model\nrelative to the baseline. Formula: (accuracymethod/accuracybaseline)/parametersmethod \u2217 100\nFigure 4 quantifies the efficiency of each method in terms of performance gains per 1K trainable\nparameters. For a focused comparison, we select the RTE task and RoBERTalarge model.\nTo establish a baseline, we conduct auxiliary experiments where only the classification head is\ntrained while the remainder of the model is frozen. This baseline is constructed using the same\nhyperparameters as in our VeRA method. We then evaluate the performance gain attributable to\neach method, normalized by the additional trainable parameters introduced, relative to the baseline.\nThe results clearly show that VeRA yields the highest performance gain per 1K trainable parameters.\nC\nIMPACT ON TRAINING TIME AND MEMORY USAGE\nTo evaluate the training time and GPU memory benefits of our method, we conducted a comparison\nbetween LoRA and VeRA while fine-tuning LLaMA 7B with the same rank (64) on instruction\ntuning dataset, introduced earlier in this work. The results are summarized in Table 12:\nTable 12: Impact on GPU memory usage and training time.\nMethod\nTraining Time\nGPU Memory\nLoRA\n568 min\n23.42GB\nVeRA\n578 min\n21.69GB\nWhile VeRA includes more operations than LoRA because of the additional vector multiplies in the\nforward pass, we find that it only results in a modest 1.8% increase in training time. For the GPU\nmemory, we observe a 7.4% reduction in memory usage with VeRA, as it does not require storing\noptimizer states and gradients for shared random matrices.\nD\nSIMILARITIES OF TRAINED WEIGHTS\nWe compared the weights trained with LoRA and VeRA at a single rank of 64 across all query\nlayers. For each method and adapted layer, we constructed a weight difference. In LoRA\u2019s case, this\ninvolved the multiplication of two low-rank matrices, while for VeRA, it also included multiplication\nby scaling vectors. We then calculated the cosine similarity of these flattened weights. Additionally,\nwe compared the similarity between trained LoRA weights and randomly initialized matrices as a\nbaseline: We find that similarities of VeRA to LoRA are on average 2e-3 while LoRA to random\nmatrices is -8e-5.\n16\nPublished as a conference paper at ICLR 2024\n0\n5\n10\n15\n20\nLayer\n0.000\n0.002\n0.004\nCosine Similarity\nLoRA-VeRA\nLoRA-Random\nFigure 5: Cosine similarity of LoRA, VeRA, and random weights across layers.\nIn Figure 5 we can see a notable increase in similarity between the trained weights, particularly in the\nlatter layers. This observation aligns with our earlier findings (Figure 3) that the highest adaptation\noccurs in these layers. These results support the notion that VeRA can approximate the weights\ntrained with LoRA.\nE\nEXPRESSIVITY OF VERA\nWe conducted an experiment on the expressivity of LoRA and VeRA on the task of fitting random\nsquare 10x10 matrices, with results seen in Figure 6. For given number of trainable parameters,\nboth methods perform equally well, with VeRA providing more flexibility, e.g. by allowing for\nmuch lower parametrization - below LoRA\u2019s rank 1.\nFigure 6: MSE loss on the task of fitting 10x10 random matrix. LoRA (PQ) and VeRA (XdYb)\nperform equally well for given number of trainable parameters.\n17\nPublished as a conference paper at ICLR 2024\nF\nINSTRUCTION-TUNING WITH VICUNA EVAL\nResults and samples from evaluation of instruction tuned Llama 7B model with Vicuna Eval (Chiang\net al., 2023), predecessor of MT-Bench. The model has been finetuned on a 10K subset of cleaned\nAlpaca dataset.\nTable 13: Average scores assigned by GPT-4 to the answers generated by models fine-tuned with\nVeRA and LoRA methods. Number of trainable parameters and theoretical memory required to\nstore them in single-precision are also shown.\nMethod # Parameters Required Bytes Avg. Score\nLoRA\n159.9M\n610MB\n7.36\nVeRA\n1.4M\n5.4MB\n7.48\nLoRA\nVeRA\nWrite a symphony concert review, discussing the orchestra\u2019s performance and overall audience\nexperience.\nANSWER\nThe symphony concert was a truly mesmerizing\nexperience, with the orchestra\u2019s performance\nleaving the audience in awe.\nThe conductor,\nwith his expertise and passion, led the musi-\ncians through a series of beautifully composed\npieces, each one more captivating than the last.\nThe orchestra\u2019s sound was rich and full, with\neach instrument perfectly balanced and in har-\nmony. The audience was enraptured by the mu-\nsic, and the applause at the end of each piece\nwas thunderous. The concert was a true testa-\nment to the power of music and the talent of the\nmusicians.\nThe symphony concert held at the Lincoln Cen-\nter for the Performing Arts on Saturday, March\n12th, 2022, was a truly mesmerizing experi-\nence.\nThe New York Philharmonic Orches-\ntra, under the direction of Music Director Jaap\nvan Zweden, performed a program of works by\nBeethoven, Strauss, and Mahler.\nThe concert began with Beethoven\u2019s Symphony\nNo. 1, which was a perfect introduction to the\nevening\u2019s program. The orchestra played with\nprecision and energy, capturing the spirit of the\npiece and transporting the audience to a differ-\nent time and place.\nThe audience was immediately drawn in by the\norchestra\u2019s performance, and the energy in the\nroom was palpable. The second piece on the\nprogram was Richard Strauss\u2019s Don Quixote,\nwhich was a real treat for the audience. The\norchestra played with great passion and enthu-\nsiasm, bringing the music to life and capturing\nthe spirit of the piece. The audience was en-\nthralled by the performance, and the applause\nat the end of the piece was thunderous.\nREVIEW\nAssistant 1 provided a general review of a sym-\nphony concert, which was relevant and accu-\nrate. However, it lacked specific details such\nas the location of the concert, the date, the or-\nchestra\u2019s name, the conductor\u2019s name, and the\npieces performed. This lack of detail made the\nreview less helpful and engaging.\nAssistant 2 provided a detailed and engaging re-\nview of a specific symphony concert. It men-\ntioned the location, date, orchestra\u2019s name, con-\nductor\u2019s name, and the pieces performed. It also\ndescribed the audience\u2019s reaction to each piece,\nwhich added depth to the review. This level of\ndetail made the review more helpful, relevant,\naccurate, and engaging.\nSCORE\n7\n9\n18\nPublished as a conference paper at ICLR 2024\nLoRA\nVeRA\nWhat if Isaac Newton had focused on biology instead of physics?\nANSWER\nIsaac Newton was a brilliant scientist who made\nsignificant contributions to both physics and\nbiology. While he is best known for his work in\nphysics, he also made important discoveries in\nbiology, including the discovery of the laws of\nmotion and the theory of gravity.\nIf Newton had focused on biology instead\nof physics, he may have made even greater\ncontributions to the field. He was a skilled ob-\nserver and experimenter, and his keen intellect\nwould have allowed him to make important\ndiscoveries in biology. He may have studied the\nanatomy of plants and animals, or the behavior\nof living organisms, or the interactions between\ndifferent species.\nNewton\u2019s work in physics laid the founda-\ntion for many of the discoveries made in\nbiology, including the understanding of the\nrole of gravity in the movement of objects.\nHis work in biology may have built upon this\nfoundation, leading to even greater advances in\nthe field.\nIn summary, if Newton had focused on bi-\nology instead of physics, he may have made\neven greater contributions to the field, and\nhis work in biology would have been just as\nsignificant as his work in physics.\nIf Isaac Newton had focused on biology instead\nof physics, he may have made significant\ncontributions to the field of biology. Newton\nwas a brilliant scientist and his work in physics\nrevolutionized the field. However, he also had\na keen interest in biology and made several\nobservations and insights about the natural\nworld.\nFor example, Newton was the first to de-\nscribe the process of photosynthesis,\nthe\nprocess by which plants convert sunlight into\nenergy. He also made observations about the\nbehavior of animals, including the way they\nmove and the way they interact with their\nenvironment.\nNewton\u2019s work in biology was not as ex-\ntensive as his work in physics, but it shows that\nhe had a deep interest in the natural world and\na desire to understand it.\nIf he had focused\non biology instead of physics, he may have\nmade even greater contributions to the field of\nbiology.\nREVIEW\nAssistant 1\u2019s response is somewhat helpful and\nrelevant, but it contains a major inaccuracy. The\nassistant claims that Isaac Newton made signifi-\ncant contributions to biology, including the dis-\ncovery of the laws of motion and the theory of\ngravity. However, these are actually contribu-\ntions to physics, not biology. The assistant also\nspeculates on what Newton might have stud-\nied if he had focused on biology, but these are\npurely hypothetical and not based on any his-\ntorical evidence. The assistant\u2019s response is de-\ntailed and well-structured, but the inaccuracy\nundermines its overall quality.\nAssistant 2\u2019s response is less helpful and rele-\nvant, and it contains a major inaccuracy. The\nassistant claims that Isaac Newton was the first\nto describe the process of photosynthesis. This\nis incorrect; the process of photosynthesis was\nnot understood until the 19th century, long after\nNewton\u2019s time. The assistant also makes some\ngeneral observations about Newton\u2019s interest in\nthe natural world, but these are not specific to\nbiology and do not provide a clear answer to the\nuser\u2019s question. The assistant\u2019s response is less\ndetailed and less well-structured than Assistant\n1\u2019s response, and the inaccuracy further reduces\nits quality.\nSCORE\n4\n2\n19\nPublished as a conference paper at ICLR 2024\nLoRA\nVeRA\nHow many times has the Earth orbited the Sun since the beginning of life? Try to explain your\nanswer. Your explanation should take the reader through your reasoning step-by-step.\nANSWER\nThe Earth has orbited the Sun approximately\n5.5 billion times since the beginning of life.\nThis is based on the assumption that life on\nEarth began approximately 4.5 billion years\nago, and that the Earth has been orbiting the\nSun for approximately 4.5 billion years.\nTo calculate the number of times the Earth\nhas orbited the Sun, we can use the formula\nfor the circumference of a circle, which is 2\n* pi * r, where pi is approximately 3.14 and\nr is the radius of the circle. The radius of the\nEarth is approximately 6,371 kilometers, so the\ncircumference of the Earth is approximately 2\n* pi * 6,371 = 12,742,200 kilometers.\nThe Earth orbits the Sun at an average distance\nof approximately 149,600,000 kilometers, so\nthe Earth travels approximately 149,600,000\nkilometers per orbit. Therefore, the Earth has\norbited the Sun approximately 149,600,000 /\n12,742,200 = 1.17 times.\nThis means that the Earth has orbited the\nSun approximately 5.5 billion times since the\nbeginning of life.\nIt is difficult to determine exactly how many\ntimes the Earth has orbited the Sun since the be-\nginning of life, as the exact age of the Earth is\nstill a matter of debate. However, we can make\nan estimate based on the current scientific con-\nsensus.\nThe age of the Earth is estimated to be between\n4.54 and 4.58 billion years old, with the most\nrecent estimates placing it at 4.54 billion years.\nThis means that the Earth has been orbiting the\nSun for approximately 4.54 billion years.\nTo calculate the number of times the Earth has\norbited the Sun since the beginning of life, we\ncan use the formula:\n[N = 1\nT \u00d7\nT\n365.25]\nwhere N is the number of orbits, T is the age of\nthe Earth, and 365.25 is the number of days in a\nyear.\nSubstituting the values into the formula, we get:\n[N =\n1\n4.54\u00d7109 \u00d7 4.54\u00d7109\n365.25 ]\nwhich simplifies to:\n[N = 1.2 \u00d7 1010]\nThis means that the Earth has orbited the Sun\napproximately 1.2 trillion times since the begin-\nning of life.\nIt is important to note that this estimate is based\non the current scientific consensus, and that the\nactual number of orbits may vary slightly due\nto uncertainties in the age of the Earth. Addi-\ntionally, the age of the Earth is constantly being\nrevised as new evidence is discovered, so this\nnumber may change in the future.\nREVIEW\nAssistant 1\u2019s response is confusing and incor-\nrect. The assistant tries to calculate the num-\nber of Earth\u2019s orbits around the Sun by using\nthe formula for the circumference of a circle\nand the Earth\u2019s radius, which is irrelevant to the\nquestion. The assistant then incorrectly states\nthat the Earth has orbited the Sun approximately\n5.5 billion times, which contradicts the initial\nassumption that life on Earth began approxi-\nmately 4.5 billion years ago.\nAssistant 2\u2019s response, on the other hand, is\nmore accurate and detailed. The assistant cor-\nrectly states that the exact number of Earth\u2019s or-\nbits around the Sun since the beginning of life is\ndifficult to determine due to uncertainties in the\nage of the Earth. However, the assistant pro-\nvides a reasonable estimate based on the cur-\nrent scientific consensus on the age of the Earth.\nThe assistant also correctly uses the formula for\ncalculating the number of orbits and provides\na clear, step-by-step explanation of the calcu-\nlation. However, the assistant made a mistake\nin the final calculation, stating that the Earth\nhas orbited the Sun approximately 1.2 trillion\ntimes, which is significantly higher than the ac-\ntual estimate. Despite this error, Assistant 2\u2019s\nresponse is more helpful, relevant, and accurate\nthan Assistant 1\u2019s response.\nSCORE\n4\n7\n20\nPublished as a conference paper at ICLR 2024\nLoRA\nVeRA\nImplement a program to find the common elements in two arrays without using any extra data\nstructures.\nANSWER\nHere is a Python program that can find the\ncommon elements between two arrays without\nusing any extra data structures:\npython\ndef common elements(arr1, arr2):\n# Sort both arrays\narr1.sort()\narr2.sort()\n# Find the length of the shorter array\nlength = min(len(arr1), len(arr2))\n# Create a set to store the common ele-\nments\ncommon elements = set()\n# Loop through the shorter array\nfor i in range(length):\n# Check if the current element is present in the\nshorter array\nif arr1[i] in arr2:\n# Add the element to the common elements set\ncommon elements.add(arr1[i])\n# Return the common elements\nreturn common elements\nThis program takes two arrays as input and\nreturns a set containing the common elements\nbetween the two arrays. The program first sorts\nboth arrays to ensure that the elements are in\nthe same order.\nThen, it finds the length of\nthe shorter array and creates a set to store the\ncommon elements. It then loops through the\nshorter array and checks if the current element\nis present in the shorter array.\nIf it is, the\nelement is added to the common elements set.\nFinally, the common elements are returned as a\nset.\nHere is a Python program that can find the\ncommon elements in two arrays without using\nany extra data structures:\npython\ndef common elements(arr1, arr2):\n# Initialize a set to store the common elements\ncommon set = set()\n# Iterate over the first array\nfor element in arr1:\n# If the element is already in the set, skip it\nif element in common set:\ncontinue\n# Otherwise, add it to the set\ncommon set.add(element)\n# Iterate over the second array\nfor element in arr2:\n# If the element is already in the set, skip it\nif element in common set:\ncontinue\n# Otherwise, add it to the set\ncommon set.add(element)\n# Return the set of common elements\nreturn common set\nThis\nprogram\ntakes\ntwo\narrays\nas\ninput\nand returns a set containing the common\nelements between the two arrays.\nREVIEW\nAssistant 1: The assistant has provided a solu-\ntion, but it does not adhere to the problem state-\nment. The problem specifically states not to use\nany extra data structures, but the assistant\u2019s so-\nlution uses a set to store the common elements.\nThe assistant\u2019s comments are clear and explain\nthe logic of the code well. However, the code\nitself is not efficient as it uses the \u2019in\u2019 opera-\ntor to check if an element is in the array, which\nhas a time complexity of O(n). This makes the\noverall time complexity of the solution O(n \u02c6 2),\nwhich is not efficient.\nAssistant 2: The assistant\u2019s solution also does\nnot adhere to the problem statement as it uses\na set to store the common elements. The assis-\ntant\u2019s comments are clear and explain the logic\nof the code well. However, the code itself is\nnot correct. The assistant\u2019s solution adds all el-\nements from both arrays to the set, which means\nit will not find the common elements but rather\nall unique elements from both arrays. This is a\nfundamental misunderstanding of the problem.\nSCORE\n4\n2\n21\n"
  },
  {
    "title": "Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V",
    "link": "https://arxiv.org/pdf/2310.11441.pdf",
    "upvote": "23",
    "text": "Set-of-Mark Prompting Unleashes\nExtraordinary Visual Grounding in GPT-4V\nJianwei Yang1\u2217\u2021, Hao Zhang2\u2217, Feng Li2\u2217, Xueyan Zou3\u2217, Chunyuan Li1, Jianfeng Gao1\n1 Microsoft Research, Redmond\n2 HKUST\n3 University of Wisconsin-Madison\n\u2217Core Contributor\n\u2021 Project Lead\nPlayground: https://som-gpt4v.github.io/\nFigure 1: Comparisons of GPT-4V prompting techniques: (left) standard prompting and (right) the\nproposed Set-of-Mark Prompting. Simply overlaying ids on image regions unleashes visual grounding\nand corrects answers for GPT-4V. Note that no marks are leaked to user text inputs..\nAbstract\nWe present Set-of-Mark (SoM), a new visual prompting method, to unleash the\nvisual grounding abilities of large multimodal models (LMMs), such as GPT-4V.\nAs illustrated in Fig. 1 (right), we employ off-the-shelf interactive segmentation\nmodels, such as SEEM/SAM, to partition an image into regions at different levels\nof granularity, and overlay these regions with a set of marks e.g., alphanumerics,\nmasks, boxes. Using the marked image as input, GPT-4V can answer the questions\nthat require visual grounding. We perform a comprehensive empirical study to\nvalidate the effectiveness of SoM on a wide range of fine-grained vision and\nmultimodal tasks. For example, our experiments show that GPT-4V with SoM\noutperforms the state-of-the-art fully-finetuned referring expression comprehension\nand segmentation model on RefCOCOg in a zero-shot setting. Code for SoM\nprompting is made public here: https://github.com/microsoft/SoM.\narXiv:2310.11441v2  [cs.CV]  6 Nov 2023\n1\nIntroduction\nIn the past few years, we have witnessed a significant advancement in large language models\n(LLMs) [2, 3, 10, 43, 60, 35]. In particular, Generative Pre-trained Transformers or GPTs [2, 35]\nhave led a number of breakthroughs in the industry and academia. Since the release of GPT-4, large\nmultimodal models (LMMs) have gained a growing interest in the research community. A number of\nworks strive to build multimodal GPT-4 based on open-sourced models [28, 63, 57, 12, 47, 50, 20].\nRecently, GPT-4V(ision) is released and attracts immediate attention from the community for its\noutstanding multimodal perception and reasoning capability. Its superiority and generality are\nshowcased in [49]. Despite the unprecedented, strong, vision-language understanding capabilities,\nGPT-4V\u2019s fine-grained visual grounding ability is relatively weak, or yet to be unleashed. For\nexample, GPT-4V can hardly produce a sequence of accurate coordinates for a dog in the image1, or\na few traffic lights [54].\nOur study is motivated by addressing the failed cases of GPT-4V on fine-grained vision tasks, such\nas object referring. In principle, these tasks require the model to have rich semantic understanding\nand accurate spatial understanding of visual contents. However, prompting GPT-4V to produce\na sequence of tokens that contains textual description and numeric coordinates not only hurts the\nnatural fluency in LLMs but also dismisses the spatial understanding ability in vision models used\nby LMMs, as pointed out by [7]. Therefore, in this study we focus on improving visual inputs\nby proposing a new visual prompting method to unleash the visual grounding capability of LMMs.\nVisual prompting has been explored for various vision and multimodal tasks [9, 19, 65, 45, 41]. These\nworks can be categorized into two categories. One is to encode visual prompts such as point, box,\nand stroke into latent features, which are then used to prompt the vision models [65, 19]. The other is\nto overlay visual marks directly onto the input images. The overlaid marks could be a red circle [41],\na highlighted region [48], or a few circles with arrows [49]. Although these studies demonstrate\nthe promise of pixel-level visual prompting, they are limited to visually referring to one or a few\nobjects. Moreover, all these marks are not easily \u201cspeakable\u201d by LLMs, thus impeding the grounding\ncapability for GPT-4V.\nIn this paper we present a new prompting mechanism called Set-of-Mark (SoM) prompting , i.e.\nsimply adding a set of visual marks on top of image regions. We first partition an input image into a\nset of semantically meaningful regions. Then, we overlay each region with a mark of various formats\nsuch as numbers, alphabets, masks, or boxes. We perform an empirical study to validate whether\nGPT-4V could better ground the regions given these interpretable and \u201cspreadable\u201d visual marks. Our\nexperiments show that SoM can drastically unleash the grounding capability of GPT-4V. As shown\nin Fig. 1, compared to the standard prompting, SoM helps GPT-4V accurately ground its answers\nin corresponding regions, e.g., the right laptop is \u201c9\u201d and the lamp is \u201c12\u201d. Likewise, it can also\nassociate the chair on the left with \u201c7\u201d. Note that no information about the marks is leaked to the\nconversation with GPT-4V, which indicates that the model can not only understand the semantics of\nthe visual marks but also know how and when to associate visual contents with marks. To the best of\nour knowledge, this is the first study to demonstrate that the emergent visual grounding ability of\nGPT-4V can be unleashed by visual prompting, SoM. To summarize, our main contributions are:\n\u2022 We propose a simple yet effective visual prompting technique, called Set-of-Mark (SoM) prompting\nfor GPT-4V, and show empirically that SoM effectively unleashes the extraordinary visual grounding\nability of GPT-4V.\n\u2022 We have developed a new suite of evaluation benchmarks to examine the grounding ability of\nGPT-4V and other LMMs. For each image in the dataset, we employ off-the-shelf segmentation\nmodels to segment an image into regions, and overlay each region with visual marks, such as\nnumeric numbers.\n\u2022 We have conducted quantitative and qualitative analysis to validate the effectiveness of SoM on\na wide range of vision tasks. Our experiments show that SoM significantly improves GPT-4V\u2019s\nperformance on complex visual tasks that require grounding. For example, GPT-4V with SoM\noutperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a\nzero-shot setting.\n1https://blog.roboflow.com/gpt-4-vision/\n2\nFigure 2: Comparing standard GPT-4V and its combination with Set-of-Mark (SoM) Prompting. it\nclearly shows that our proposed prompting method helps GPT-4V to see more precisely and finally\ninduce the correct answer. We highlight the differences between our method and the standard one.\n(The image is generated by Dalle-3 and is better viewed in color.)\n2\nSet-of-Mark Prompting\nThis section introduces Set-of-Mark Prompting and explains how it can be applied to prompt the\nLMMs, GPT-4V in particular.\n2.1\nProblem Definition\nTypically, LMMs F take an image I \u2208 RH\u00d7W \u00d73 and a text query of length of li, T i = [ti\n1, ..., ti\nli]\nas input, and generate a sequence of textual output with length lo, T o = [to\n1, ..., to\nlo], formulated as:\nT o = F(I, T i)\n(1)\nGiven the versatility of current LLMs used in LMMs, the input and output texts can be comprised\nof different types of characters (e.g., both alphabetic and numeric), and also multilingual. A large\nnumber of works have attempted to improve prompt engineering on the input text T i to endow more\nreasoning capability in LLMs [46, 52].\nIn contrast to prompt engineering for LLMs, the goal of this study is to develop a new prompting\nmethod for input images to unleash visual grounding ability of LMMs. In other words, we strive to\nequip a LMM with the ability to see location-by-location. This necessitates two essential properties\nfor the prompting strategy:\n\u2022 The method should be able to partition an image into a set of semantically meaningful regions to\nalign with the textual outputs, an ability known as grounding.\n\u2022 The auxiliary information cast to the input image should be both interpretable and speakable by\nthe LMM, so that it can be described in its textual outputs.\nWith this in mind, we develop Set-of-Mark Prompting, a simple prompting method of overlaying a\nnumber of marks to the meaningful regions in an image. This operation augments the input image\nI to a marked image Im while keeping the other inputs to LMMs unchanged, as shown in Fig. 2.\nMathematically, Eq. (1) becomes\nT o = F(SoM(I)\n| {z }\nIm\n, T i).\n(2)\nAlthough it is straightforward to apply SoM to all LMMs, we find that not all LMMs have the ability\nto \u201cspeak out\u201d about the marks. Actually, we find that only GPT-4V, when equiped with SoM, shows\nthe emergent grounding ability and significantly outperforms the other LMMs. In what follows, we\nwill explain how we partition an image into regions and mark the image regions in SoM.\n2.2\nImage Partition\nGiven an input image, we need to extract meaningful and semantically aligned regions. Ideally, the\nextraction should be automatic or semi-automatic to avoid extra burden on users. To achieve this, we\nemploy a suite of image segmentation tools. To support different use cases, the segmentation tools\nneed to possess the following properties:\n3\nFigure 3: We compile different segmentation models including SEEM [65], Semantic-SAM [21] and\nSAM [19] as the image partition tools used in our work. Users can select which granularity of masks\nto generate, and which mode to use between automatic (top) and interactive (bottom). A higher alpha\nblending value (0.4) is used for better visualization.\n\u2022 Strong performance: the region partition should be precise enough to convey fine-grained spa-\ntial layout information to GPT-4V. In this sense, we choose one of the state-of-the-art image\nsegmentation models MaskDINO [24].\n\u2022 Open vocabulary: the segmentation models should be open-vocabulary so that they can recognize\nobjects out of predefined vocabulary. As such, we opt for the advanced models like SEEM [65].\n\u2022 Rich granularity: Finally, the region of interest might be not only a full object but also a part of it.\nTherefore, we also employ SAM [19] and Semantic-SAM [21].\nWe have collected a suite of image segmentation models that offers a comprehensive toolbox for a\nuser to partition arbitrary images. Most of models are interactive and promotable such that users can\ncustomize the SoM prompts interactively.\nBased on our image partition toolkit, we divide an input image I of size H \u00d7 W into K regions\nR = [r1, ..., rK] \u2208 {0, 1}K\u00d7H\u00d7W , which are represented by K binary masks. We show some SoM\nexamples in Fig. 3 generated using our toolbox.\n2.3\nSet-of-Mark Generation\nOnce we obtain image partition M, we need to generate for each region a mark that is useful for\ngrounding by GPT-4V. We consider two factors, the types and locations of marks.\nMark Type.\nAs we discussed earlier, the mark type depends on whether they can be interpreted\nby GPT-4V. In this work, we consider alphanumeric in that it is not only compact enough to not\noccupy much image space but recognizable by GPT-4V (using its OCR capability). Besides, we also\nconsider boxes and mask boundaries as auxiliary marks. In addition, we note that the mark types\nshould be image-dependent to avoid any conflicts with the original image contents. For example,\ngiven an arithmetic image full of numbers, the numeric marks should be avoided, while the alphabetic\ncharacters should be not used for a screenshot of a document. We leave the automatically determining\nwhich mark types to use and how to combine them to future work. Fig. 4 shows different mark types\nand the interpreted outputs from GPT-4V.\nMark Location.\nGiven an image of size H \u00d7 W, we need to find good mark locations for all K\nregions. A straightforward way is to derive the center coordinate ck = (xc\nk, yc\nk) of k-th mask, and\noverlay the marks there. However, this inevitably introduces some overlaps or conflicts that may\nconfuse GPT-4V, especially for images with dense objects, e.g., two objects centered around similar\nlocations or concave regions, as shown in Fig. 5.\nTo mitigate the problem, we propose a mark allocation algorithm as illustrated in the algorithm\non the right side of Fig. 5. Given the set of masks M, we first calculate the areas for all and sort\n4\nFigure 4: Different types of marks can be used in our Set-of-Mark.\nFigure 4: Different types of marks can be used in our Set-of-Mark.\n// Find center for a region\ndef Find_Center(r)\nD = DT(r) // Run distance transform\nc = arg max(D) // Find maxium location\nreturn c\n// The main function\ndef Mark_Allocation(R):\n\u02c6R = Sorted(R) // Sort regions in ascending\norder of areas\nfor k in range(K): do\nrk = \u02c6R[k] & \u00ac \u02c6R[: k \u2212 1].sum(0) // Exclude\nk \u2212 1 regions\nC[k] = Find_Center(rk)\nend\nreturn C\nFigure 5: Left: some conflicts caused by putting all marks in the center. Right: our proposed mark\nallocation algorithm to address the conflicts.\nTo mitigate the problem, we propose a mark allocation algorithm as illustrated in the algorithm on\nthe right side of Fig. 5. Given the set of masks M, we first calculate the areas for all and sort them\nin ascending order (line 6). This strategy ensures that smaller regions are considered earlier than\nlarge regions. To further avoid the potential overlaps, for k-th mask, we exclude the region that is\ncovered by any k \u2212 1 masks (line 8). The resulting mask is then fed to a distance transform algorithm,\nwhich helps to find the location inside the mask where the minimal distance to all boundary points\nis maximal. In practice, however, a region may be so small that the mark could cover the (almost)\nwhole region. In this case, we move the marks off the region slightly. We find GPT-4V can still build\na decent association between the marks and regions.\nOnce we determine the mark type and locations for all regions, we overlay a mark mk to region\nrk at location ck. We make sure the mark is unique so that the full set M = {m1, ...mk} are\ndistinguishable and speakable by LLMs.\n2.4\nInterleaved Prompt\nThus far we have obtained a new image Im with overlaid marks. Additionally, we have a set of K\nregion-mark pairs {\u27e8r1, m1\u27e9, ..., \u27e8rK, mK\u27e9}. Given the additional cues in Im, we can either use a\nplain text prompt or interleaved ones to prompt LMMs:\n\u2022 Plain Text Prompt. We can use plain text prompts as usual without any explicit reference to the\nmarks/regions in the image. As we already show in Fig. 1, even without any special textual prompt,\nGPT-4V automatically ground itself on the regions and corresponding marks. It can be used in a\nwide range of scenarios when users do not have specific regions of interest.\n\u2022 Interleaved Text Prompt. In addition to plain text prompts, we can also use interleaved ones\nby injecting the marks into the text directly. Since the marks are interpretable to LMMs, we can\nseamlessly blend them into the original text prompt to make a symbolic reference.\nExamples of applying these two types of text prompts based on our Set-of-Mark Prompting are\ndemonstrated in Fig. 6. Note that for each question, we use a new chat window to avoid any context\nleakage during the conversation. In realistic scenarios, the above two text prompting strategies can\nbe combined and used in multi-round conversations with GPT-4V, which significantly enrich the\n5\nFigure 5: Left: some conflicts caused by putting all marks in the center. Right: our proposed mark\nallocation algorithm to address the conflicts.\nthem in an ascending order (line 6). This strategy ensures that smaller regions are considered before\nlarge regions. To further avoid the potential overlaps, for k-th mask, we exclude the region that is\ncovered by any k \u2212 1 masks (line 8). The resulting mask is then fed to a distance transform algorithm,\nwhich helps to find the location inside the mask where the minimal distance to all boundary points\nis maximal. In practice, however, a region may be so small that the mark could cover the (almost)\nwhole region. In this case, we move the marks off the region slightly. We find that GPT-4V can still\nbuild a decent association between the marks and regions.\nOnce we determine the mark type and locations for all regions, we overlay mark mk to region rk\nat location ck. We make sure that each mark is unique so that the full set M = {m1, ...mk} are\ndistinguishable and speakable by LLMs.\n2.4\nInterleaved Prompt\nThus far we have obtained a new image Im with overlaid marks. Additionally, we have a set of K\nregion-mark pairs {\u27e8r1, m1\u27e9, ..., \u27e8rK, mK\u27e9}. Given the additional cues in Im, we can use either a\nplain text prompt or interleaved ones to prompt LMMs:\n\u2022 Plain Text Prompt. We can use plain text prompts as usual without any explicit reference to the\nmarks/regions in the image. As shown in Fig. 1, even without any special textual prompt, GPT-4V\ncan automatically ground itself in the regions and corresponding marks. It can be used in a wide\nrange of scenarios where users do not have specific regions of interest.\n\u2022 Interleaved Text Prompt. We can use interleaved ones by injecting the marks into the text directly.\nSince the marks are interpretable to LMMs, we can seamlessly blend them into the original text\nprompt to make a symbolic reference.\nExamples of applying these two types of text prompts based on SoM are demonstrated in Fig. 6. Note\nthat for each question, we use a new chat window to avoid context leakage during the conversation.\nIn real-world scenarios, the above two text prompting strategies can be combined and used in multi-\nround conversations with GPT-4V to significantly enrich human-AI interaction. Moreover, users can\nalso opt to draw the marks by themselves or revise the marks generated using the toolbox.\n5\nSoM-Enabled Plain and Interleaved Prompts\nUser: What is in the image?\nGPT-4V: In the image, I observe the following\nitems:\nLoaf or Cake (1): A rectangular-shaped baked\nitem, possibly a loaf of bread or a cake.\nCreamy Fruit Mixture (2): A bowl containing\na creamy mixture with various berries and possi-\nbly other fruits. It might be a fruit salad with a\ncreamy dressing, yogurt with fruits, or some kind\nof dessert.\nSliced Fruits (3): A plate with slices of various\nfruits, including what looks like pineapples and\npossibly slices of watermelon or another type.\nTable Surface (4): A portion of the table or coun-\ntertop with a bluish hue.\n(con\u2019t) Additionally, there are various decorative\nelements, plates, and bowls.\nUser: What is in 3?\nGPT-4V: In the image, the item labeled \"3\"\nappears to be a plate of sliced fruits. It looks\nlike there\u2019s pineapple and possibly another type\nof melon or fruit, such as watermelon or pink\ngrapefruit.\nUser: Can I put the fruits in 3 into 2?\nGPT-4V: Yes, based on the image, the fruits in 3\nappear to be sliced fresh fruits, and they can be\nmixed or poured into the creamy fruit mixture in\n2 to enhance flavor and texture. It\u2019s a common\npractice in desserts to combine fresh fruits with\ncreamy mixtures.\nUser: Which can be toasted, 1 or 2?\nGPT-4V: Based on the image, item 1 (which\nappears to be a loaf of bread or cake) is the one\nthat can be toasted.\nItem 2 is a creamy fruit\nmixture and is not suitable for toasting.\nUser:\nHow long it usually takes to toast a\nslice of loaf in 1?\nGPT-4V: Typically, to toast a slice of a loaf like\nthe one labeled as \"1\" in the image, it would take\nbetween 2 to 5 minutes in a toaster on a medium\nsetting.\nFigure 6: We can use two types of text prompts to LLMs: Left shows a plain text prompt, which does\nnot include any clues about the marks; Right is an interleaved text prompt, which uses the marks to\nground image regions.\n3\nSoM Prompting for Vision\nWe highlight that the unique merit of using SoM to prompt GPT-4V is that it can produce outputs\nbeyond texts. Since each mark is exclusively associated with an image region represented by a mask,\nwe can trace back the masks for any mentioned marks in the text outputs. Consider the example in\nFig. 7, the GPT-4V response on the left side contains the names and details of four regions. It can\ninduce the one-to-one mappings between marks and text description, i.e., mk \u2194 textk. Given that\nrk \u2194 mk, we can further associate textual description with the masks for all regions. Finally, we\ncan bridge the triplets \u27e8rk, mk, textk\u27e9 for all regions, i.e., rk \u2194 mk \u2194 textk. The ability to produce\npaired texts and masks allows the SoM prompted GPT-4V to produce plausible visually grounded\ntexts and more importantly support a variety of fine-grained vision tasks, which are challenging for\nthe vanilla GPT-4V model.\nVision Tasks.\nWe quantitatively examine the performance of SoM prompted GPT-4V. With simple\nprompt engineering, it can be readily used in, but not limited to, a wide range of vision tasks such as\n\u2022 Open-vocabulary Image Segmentation: We ask GPT-4V to exhaustively tell the categories for all\nmarked regions and the categories that are selected from a predetermined pool.\n\u2022 Referring Segmentation: Given a referring expression, the task for GPT-4V is selecting the\ntop-matched region from the candidates produced by our image partition toolbox.\n6\nOpen-Vocabulary Segmentation\nUser: I have labeled a bright numeric ID at\nthe center for each visual object in the image.\nPlease enumerate their names. You must an-\nswer by selecting from the following names:\n[Person, ..., curtain] (ignore the 80 categories\nin COCO)\nGPT-4V 1. Person 2. Person 3. Person 4.\nSurfboard 5. Handbag\nReferring Segmentation\nUser: I have labeled a bright numeric ID at\nthe center for each visual object in the image.\nPlease tell me the IDs for: The laptop behind\nthe beer bottle; Laptop turned on.\nGPT-4V The IDs for the items you\u2019ve\nmentioned: The laptop behind the beer bottle:\n6 Laptop turned on: 2\nVideo Object Segmentation\nUser: The 2 images are from the same video,\nwhere the first image is the first frame and\nthe second image is a later frame. In the first\nimage, there are 2 objects labeled with 1,2.\nCan you track these 2 objects in the second\nimage?\nGPT-4V 1. The object labeled with 1 (a\nboxer in red headgear) is most similar to the\nobject labeled with 2 (another boxer in red\nheadgear). 2. The object labeled with 2 (a\nboxer in blue) is most similar to the object\nlabeled with 1 (another boxer in blue).\nGrounded Segmentation\nUser: I have labeled a bright numeric ID at\nthe center for each visual object in the image.\nGiven the image showing a man in glasses\nholding a piece of paper, find the correspond-\ning regions for a man in glasses, a piece of\npaper.\nGPT-4V The \"man in glasses\" corresponds\nto the region labeled with \"2\". The \"piece of\npaper\" corresponds to the region labeled \"5\".\nFigure 7: Examples of some vision tasks we benchmark on GPT-4V.\n\u2022 Phrase Grounding: Slightly different from referring segmentation, phrase grounding uses a com-\nplete sentence consisting of multiple noun phrases. We ask GPT-4V to allocate the corresponding\nregions for all labeled phrases.\n\u2022 Video Object Segmentation: It takes two images as input. The first image is the query image\nwhich contains a few objects of interest to identify in the second image. Given that GPT-4V\nsupports multiple images as input, our prompting method can also be applied to ground visual\nobjects across frames in a video.\nWe show how we prompt GPT-4V using SoM for the above vision tasks in Fig. 7. It is worth noting\nthat SoM can be applied to broader tasks, such as region captioning, keypoint localization, and part\nsegmentation, etc. Due to the limited access (quota) to GPT-4V, we focus on the aforementioned\n7\nVision Task\nDataset\n#Images\n#Instances\nMarks\nMetric\nGeneric Segmentation\nCOCO [26]\n100\n567\nNumber & Mask\nPrecision\nOV Segmentation\nADE20K [62]\n100\n488\nNumber & Mask\nPrecision\nPhrase Grounding\nFlickr30K [38]\n100\n274\nNumber & Box\nRecall@1\nReferring Expression Segmentation\nRefCOCO [18]\n100\n177\nNumber & Mask\nmIoU\nReferring Expression Comprehension\nRefCOCO [18]\n100\n177\nNumber & Mask\nACC@0.5\nVideo Object Segmentation\nDAVIS [37]\n71\n157\nNumber & Mask\nJ&F\nTable 1: Benchmarks used to evaluate the visual grounding capability of GPT-4V. Note that we select\na small subset of images or videos from the corresponding datasets.\nvision tasks and leave the explorations to other tasks for future work. We present the detailed\nempirical study in the next section.\n4\nExperiments\n4.1\nExperimental Setup\nImplementation.\nWe do not need to train any models for our method. However, due to the limited\nquota and absence of GPT-4V API, we have to exhaustively send the SoM augmented images to\nthe ChatGPT interface. The authors in this work used a divide-and-conquer strategy to run the\nexperiments and evaluations. For each instance, we use a new chat window so that there is no context\nleakage during the evaluation. In this sense, all the reported results for our method are zero-shot.\nUnless otherwise mentioned, we always use the numeric numbers as the marks for reporting the main\nresults. The usage of other mark types is shown in our ablation and qualitative studies.\nBenchmarks.\nGiven the limited quota of GPT-4V, it is not possible for now to fully evaluate the\nvalidation set in each individual vision task as we listed above. Consequently, we select a small\nsubset of validation data from each dataset for our study. For each image in the datasets, we overlay a\nset of marks on the regions extracted using our image partition toolbox. Depending on the specific\ntask, we exploit different partition tools to propose regions. In Table 1, we list the setup for each task.\nComparisons.\nWe compare our method with:\n\u2022 GPT-4V baseline predicting coordinates. We use this as our baseline model. By default, GPT-4V\ncan neither predict boxes nor generate masks. Following prior practice, we prompt the model to\npredict box coordinates. Comparing with the default GPT-4V baseline helps to inspect the benefit\nof our proposed Set-of-Mark Prompting.\n\u2022 State-of-the-art specialist models. For each of the vision tasks, a number of methods have been\nproposed. We choose state-of-the-art and representative methods for comparison. More specifically,\nMaskDINO [24] for generic segmentation on COCO, OpenSeeD [55] for zeroshot segmentation on\nADE20K, Grounding DINO [31] and GLIPv2 [56] for phrase grounding on Flick30K, Grounding\nDINO and PolyFormer [29] for referring expression comprehension, PolyFormer and SEEM [65]\nfor referring expression segmentation and SegGPT [45] for video object segmentation. We attempt\nto shed light on the gap between the strongest generalist vision model GPT-4V and specialist\nmodels that are sophisticatedly designed and trained with the take-specific data.\n\u2022 Open-sourced LMMs. We quantitatively evaluate one of the state-of-the-art open-sourced LMMs,\nLLaVa-1.5 [27], and qualitatively compare with MiniGPT-v2 [5] in our study. Note that both\nmodels are trained with a good amount of data from the targeted vision tasks. We are the first to\ncompare closed- and open-sourced LMMs on vision benchmarks.\n4.2\nQuantitative Results\nWe present the experimental results in Table 2.\nImage Segmentation\nOn image segmentation tasks, we evaluate the classification precision of\nGPT-4V + SoM and compare it with the strong segmentation model MaskDINO on COCO Panoptic\nsegmentation dataset and OpenSeeD on ADE20K Panoptic segmentation dataset. For MaskDINO\n8\nModel\nLMM\nZero-shot\nOV Segmentation\nRefCOCOg\nPhrase Grounding\nVOS\nCOCO\nADE20K\nREC\nRES\nFlickr30K\nDAVIS2017\nMaskDINO [23]\n\u2717\n\u2717\n80.7\n-\nn/a\nn/a\nn/a\nn/a\nOpenSeeD [55]\n\u2717\n\u2713\n-\n23.4\nn/a\nn/a\nn/a\nn/a\nGLIPv2 [56]\n\u2717\n\u2717\n-\n-\n-\n-\n87.7\u2217\nn/a\nGDINO [31]\n\u2717\n\u2717\nn/a\nn/a\n86.1\u2217\nn/a\n90.5\nn/a\nX-Decoder [64]\n\u2717\n\u2717\n-\n-\n-\n64.6\u2217\nn/a\n62.8\nPolyFormer [29]\n\u2717\n\u2717\nn/a\nn/a\n85.8\u2217\n67.2\nn/a\nn/a\nSegGPT [45]\n\u2717\n\u2713\nn/a\nn/a\nn/a\nn/a\nn/a\n75.6\nSEEM [65]\n\u2717\n\u2717\n-\n-\n-\n65.7\u2217\n-\n62.8\nRedCircle [41]\n\u2713\n\u2713\nn/a\nn/a\n59.4\u2217\nn/a\nn/a\nn/a\nFGVP [48]\n\u2713\n\u2713\nn/a\nn/a\n63.3\u2217\nn/a\nn/a\nn/a\nShikra [6]\n\u2713\n\u2717\nn/a\nn/a\n82.6\u2217\nn/a\n77.4\nn/a\nLLaVA-1.5 [27]\n\u2713\n\u2717\nn/a\nn/a\n63.3\nn/a\nn/a\nn/a\nMiniGPT-v2 [5]\n\u2713\n\u2717\nn/a\nn/a\n84.4\u2217\nn/a\nn/a\nn/a\nFerret [54]\n\u2713\n\u2717\nn/a\nn/a\n85.8*\nn/a\n81.1\nn/a\nGPT-4V [36]\n\u2713\n\u2713\nn/a\nn/a\n25.7\nn/a\nn/a\nn/a\nGPT-4V [36] + SoM (Ours)\n\u2713\n\u2713\n75.7\n63.4\n86.4\n75.6\n89.2\n78.8\nTable 2: Main quantitative results.\n\u2217 denotes that the number is reported in the original papers\nwhich are evaluated in the full validation datasets. Other numbers are evaluated in our sub-sampled\nvalidation datasets constraint by GPT-4V interface.\nMark Type\nFlickr30K (R@1)\nNumber & Mask\n84.4\nNumber & Mask & Box\n89.2\nTable 3: The performance on Flickr30K with\ndifferent mark types on a subset of our dataset.\nMask type\nRefcocog (mIoU)\nMaskDINO\n75.6\nGT mask\n90.1\nTable 4: The performance on Flickr30K with\ndifferent mask types.\nand OpenSeeD, we give them GT boxes and evaluate the output class. For GPT-4V + SoM, we\noverlay GT masks with alpha=0.4 and 0.2 for COCO and OpenSeeD, respectively, and add the ID\nnumber on each mask. We provide GPT-4V with the vocabulary of the datasets and ask it select a\nclass label for each region. The results show that zero-shot performance of GPT-4V + SoM is close\nto the performance of fine-tuned MaskDINO and is much higher than the zero-shot performance of\nOpenSeeD on ADE20K. The similar performance on COCO and ADE20K for GPT-4V indicates its\nstrong generalization ability to a wide of visual and semantic domains.\nReferring\nFor referring tasks, we evaluate RES and REC on RefCOCOg. We use MaskDINO to\npropose masks and overlay the masks and numbers on the images. We use mIoU as the evaluation\nmetric and compare it with state-of-the-art specialist PolyFormer [29] and SEEM [65]. Accordingly,\nGPT-4V + SoM outperforms PolyFormer by a large margin. Note that the performance of PolyFormer\nis evaluated on our dataset for apple-to-apple comparison. For REC, we convert the masks into boxes\nfor our method. When prompting GPT-4V to directly output the coordinates, we attain significantly\npoor performance (25.7), which verifies our earlier hypothesis. Once augmented with SoM, GPT-\n4V beats both specialists such as Grounding DINO and Polyformer and recent open-source LMM\nincluding Shikra, LLaVA-1.5, MiniGPT-v2, and Ferret.\nPhrase Grounding\nFor phrase grounding on Flickr30K, we use Grounding DINO to generate\nbox proposals for each image. We use a threshold of 0.27 to filter redundant boxes, then we use\nSAM to predict a mask for each box. We draw boxes, masks, and numbers on the images. We give\nGPT-4V a caption and the noun phrases of the caption for each image and let GPT-4V to ground each\nnoun phrase to a region. Our zero-shot performance is comparable with SOTA models GLIPv2 and\nGrounding DINO.\nVideo Object Segmentation\nWe evaluate DAVIS2017 [39] for the video segmentation task. To\nget the mask proposals of each video frame, we use MaskDINO [24]. The predicted masks are\nthen overlayed on the corresponding frames with numeric labels following our SoM. We follow the\nsemi-supervised setting to use the first frame masks as the reference and segment all the other frames\nof the same video. As GPT-4V can take in multiple images, we prompt GPT-4V with the first frame\nand the current frame to do segmentation by comparing similar objects across images. As shown in\n9\nFigure 8: (a, b, c) Examples of some wrong annotations for referring segmentation (RefCOCOg) and\ngrounding (Flikr30k). (e, f) Examples of confusing annotated labels.\nthe last column of Table 2, GPT-4V achieves the best tracking performance (78.8 J&F) compared\nwith other generalist vision models.\n4.3\nAblation Study\nMark type.\nWe study how the choice of mark types affects the final performance for the phrase\ngrounding task on Flickr30k. We compare two types of marks. The first is numbers and masks and\nthe second is numbers, masks, and boxes. As shown in Table 3, the scheme of adding extra boxes can\nimprove the performance significantly.\nGolden mark location.\nThe extracted regions from our toolbox may often come with some errors.\nIn this study, we investigate how GPT-4V behaves when we generate the marks with ground-truth\nannotations. Specifically, we replace the predicted segmentation masks with ground-truth ones in our\nRefCOCOg validation set. This means GPT-4V only needs to select one from regions for annotated\nphrases. As expected, the performance for referring segmentation can be further improved, especially\nwhen the segmentation model has some missed regions. The results are shown in Table 4, which\nshows using ground-truth masks in our SoM improves the RefCOCOg performance by 14.5 mIoU.\nWe also observe that most failure cases when using GT masks are not the GPT-4V\u2019s problem, but the\nannotation itself is ambiguous or incorrect.\n4.4\nQualitative Observations\nWhen running through this work, we observed several intriguing qualitative findings.\nThe \u201cgolden\u201d annotations are not always golden.\nWhen evaluating SoM, we find that a few\nhuman annotations in datasets are incorrect. Some examples are shown in Fig. 8 (a,b,c). For example,\nhuman users annotate several people while the referring expression is \u201cthe legs of a person sitting\non the couch\u201d or only one truck is annotated for referring to \u201ctruck race\u201d. In contrast, GPT-4V with\nSoM yields correct answers. This implies that the \u201cgolden\u201d labels in vision datasets may be subject to\nsome noises while GPT-4V could be a good judge to help find the ambiguous annotations for further\ncleaning.\nThe centered marks are not always the best.\nBy default, our mark allocation algorithm finds\nthe center for each region. However, we observe that putting marks at the central locations does not\nnecessarily bring the best performance. For example in Fig. 8 (e), there are two marks, one is for\nthe person and the other one for the backpack. When being asked which one is the person, GPT-4V\ngives the mark for the backpack which is located at the upper part of the human body. A similar\nsituation also happens in the second image where there is a background spanning the whole image.\nAn example is shown in 8 (f), where the center of the grassland (labeled 2) is on the person (labeled\n1). These results indicate that the focus of GPT-4V to understand visual contents is not necessarily at\nthe center. Developing a better mark allocation algorithm should be considered.\nDynamitc selection of mark types.\nIn realistic scenarios, we find dynamically determining which\ntype of mark to use is important. For example, an image of arithmetic problems usually contains\nmany numbers. In this case, overlaying numeric marks on to will confuse the GPT-4V. Likewise, for\n10\na screenshot of a document, it might be not rational to overlay more alphabets on the image. As a\nresult, to fully leverage the visual capability of GPT-4V, users may need to carefully design the SoM\nprompts before sending the image.\n5\nRelated Work\nWe discuss related work from the perspective of prompting.\nLLMs and Prompting.\nWe have witnessed significant progress in large language models (LLMs) [2,\n3, 10, 43, 60, 35]. Of particular, Generative Pre-trained Transformers, commonly known as GPTs [2,\n35], have led to a breakthrough in the realm of natural language processing. Despite the size\nof LLMs growing dramatically, eliciting reasoning capabilities still requires more sophisticatedly\ndesigned queries, i.e., prompting. In the past, a number of works attempted to do prompt engineering\nto endow more capability for LLMs. In-context learning is a main-stream way to teach LLMs\nto follow specific instructions as instantiated by a few examples [2, 15] As a follow-up, some\nother techniques such as chain-of-thought and tree-of-thought [46, 52] are proposed to enhance the\narithmetic, commonsense, and symbolic reasoning tasks. Analogous to these works, our Set-of-\nMark Prompting can be considered as a way of prompting the model to look at the image regions\nlocation-by-location. However, it also differs in that no in-context examples are needed in our method.\nVisual and Multimodal Prompting.\nIn earlier works on interactive image segmentation [34, 8, 9],\nthe spatial prompting is used so that the model can take multiple user inputs (e.g., clicks) to gradually\nrefine the mask. Recently, the pioneering work SAM [19] and its variants [42, 11] have proposed a\nunified model to support different types of prompting modes including points, boxes, and texts. In\nSEEM [65], the authors proposed a novel visual sampler to expand to visual prompting across images.\nLikewise, PerSAM [58] and SAM-PT [40] finetune SAM to support personalized segmentation and\npoint tracking. Another line of work directly generates the prompts on the input images. In [1], image\ninpainting is used as the prompting to teach the model to predict dense outputs given example images\nand corresponding outputs, both in pixel space. Painter [44] and SegGPT [45] share similar spirits\nwhile using masked image modeling for decoding pixels in continuous space. Visual prompting can\nbe also applied to multimodal models. Colorful prompting tuning (CPT) is one of the first works that\noverlay image regions with different colors and uses masked language models to fill the blanks [53].\nRedCircle [41] draws a red circle on an image to focus the vision model on the enclosed region.\nIn [48], the authors enhance the prompts by specifically segmenting and highlighting a target object\nin the image. Both methods then ask CLIP model to measure the similarity between the prompted\nimage and a number of textual concepts.\nLMMs and Prompting.\nIn a short time, large multimodal models (LMMs) have emerged as a new\nline of research in the community. The goal is to build a generalist multimodal system that behaves as\nLLMs that can perceive and reason with multimodal inputs [28, 63, 57, 12, 59, 47, 50, 6, 20, 5, 54].\nEarlier works like MiniGPT-4 [5] and LLaVa [28] proposed a simple yet effective way to connect\nvision and LLMs followed by an instruction tuning. Later, a similar training strategy is applied to\nvideo tasks [33, 4] and fine-grained vision tasks [6, 59, 20, 5, 54]. Please refer to [20] for a more\ncomprehensive review of LMMs. Thus far, prompting LMMs is rarely explored in academia, partially\nbecause most of the recent open-sourced models are of limited capacity and thus incapable of such\nemerging capability [46]. Most recently, GPT-4V [36] was released followed by a comprehensive\nqualitative study on it [49]. The authors in [49] used a similar prompting strategy as in RedCircle [41]\nto prompt GPT-4V. However, it only shows some referring capability of GPT-4V with hand-drawn\ncircles or arrows, let alone a comprehensive quantitative study.\n6\nDiscussion\nThe mystery in GPT-4V.\nIt is still mysterious why the proposed Set-of-Mark Prompting could\nwork so well with GPT-4V. Out of curiosity, we also ran some examples on other open-sourced\nLMMs such as LLaVa-1.5 and MiniGPT-v2. However, both models can hardly interpret the marks\nand ground themselves on those marks. We hypothesize a few reasons for the extraordinary visual\ngrounding capability exhibited in GPT-4V. First, scale matters. We believe the scale of model and\n11\ntraining data used in GPT-4V is several orders of magnitude than the aforementioned open-sourced\nLMMs. Second, the data curation strategy is probably another secret sauce for GPT-4V. GPT-4V\ncould automatically associate the image regions and marks without any explicit prompt in texts. Such\nkind of data could be probably from literature figures, charts, etc, which are usually clearly labeled or\nmarked [1]. We doubt that GPT-4V specifically employs fine-grained vision data as covered in this\nwork. Note that the amount is extremely limited, and annotating more such kinds of data is costly,\nnot to mention the quality is hard to control. In the end, we note that all the above suspicions are not\ngrounded on the facts but on some empirical studies as we conducted above.\nConnecting visual and LLMs prompting.\nDespite the unknowns behind GPT-4V. Our work does\ntake one of the first steps to connect visual prompting and LLMs prompting. In the past, many works\nhave studied how to make vision models more promptable, which is isolated from the text promptings\nto LLMs. The barrier is mainly due to the language bottleneck in that we can hardly express the\nvisual promptings precisely in language. For example, the shape/location/color of a randomly drawn\nstroke on the image can be hardly described verbally, unless we can encode the visual prompts and\nfinetune the whole model [54]. However, given the limited fine-grained training data and inferior\nopen-sourced large vision and language models. We still see a clear gap. At the current stage, our\nproposed Set-of-Mark Prompting demonstrates a simple yet only feasible way to inherit all existing\ncapabilities of the strongest LMM while unleashing its strongly aspired grounding capability. We\nhope this work as the first to seamlessly connect visual and language prompting, could help to pave\nthe road towards more capable LMMs.\nScaling data via Set-of-Mark Prompting with GPT-4V.\nIn the past, the whole community has\nstrived to build fine-grained, open-vocabulary vision systems, spanning from detection [17, 61, 25,\n56, 51, 31] to segmentation [16, 64, 14], and further expand to 3D [32, 13, 30]. Though tremendous\nprogress, we have been struggling with how to map the rich semantic knowledge from CLIP to the\nfine-grained domains. This is mainly due to the extremely limited number of fine-grained semantic\nannotations, which is still a considerable challenge to the current LMMs. As we already see, the\nfeasibility of scaling data with fine-grained spatial has been demonstrated in SAM [19] and Semantic-\nSAM [22], but how to further annotate these regions with semantic labels is still an open problem.\nIn the light of the study in this work, we are envisioning the potential of using GPT-4V plus our\nSet-of-Mark Prompting to intensively scale up the multimodal data which has both fine-grained spatial\nand detailed language description.\n7\nConclusion\nWe have presented Set-of-Mark Prompting, a simple yet effective visual prompting mechanism for\nLMMs, particularly GPT-4V. We show that simply overlaying a number of symbolic marks on a\nset of regions of an input image can unleash the visual grounding ability of GPT-4V. We present\na comprehensive empirical study on a wide range of fine-grained vision tasks to demonstrate that\nSoM-prompted GPT-4V is superior to fully-finetuned specialist models and other open-sourced\nLMMs. Moreover, our qualitative results show that GPT-4V with SoM possesses extraordinary\nfine-grained multimodal perception, cognition, and reasoning capabilities across the board. We hope\nthat SoM would inspire future works on multimodal prompting for LMMs and pave the road towards\nmultimodal AGI.\nAcknowledgement\nWe thank Fangrui Zhu for the thoughtful discussions. We thank Xuan Li and Biyi Fang from\nMicrosoft Office Team for early brainstorming.\nReferences\n[1] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual\nprompting via image inpainting. Advances in Neural Information Processing Systems, 35:25005\u2013\n25017, 2022.\n12\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M.\nZiegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz\nLitwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020.\n[3] S\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece\nKamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general\nintelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023.\n[4] Guo Chen, Yin-Dong Zheng, Jiahao Wang, Jilan Xu, Yifei Huang, Junting Pan, Yi Wang, Yali\nWang, Yu Qiao, Tong Lu, and Limin Wang. Videollm: Modeling video sequence with large\nlanguage models, 2023.\n[5] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu Liu, Pengchuan Zhang, Raghuraman\nKrishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. Minigpt-v2: Large\nlanguage model as a unified interface for vision-language multi-task learning. github, 2023.\n[6] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra:\nUnleashing multimodal llm\u2019s referential dialogue magic, 2023.\n[7] Ting Chen, Saurabh Saxena, Lala Li, David J. Fleet, and Geoffrey Hinton. Pix2seq: A language\nmodeling framework for object detection, 2022.\n[8] Xi Chen, Zhiyan Zhao, Feiwu Yu, Yilei Zhang, and Manni Duan. Conditional diffusion\nfor interactive segmentation. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7345\u20137354, 2021.\n[9] Xi Chen, Zhiyan Zhao, Yilei Zhang, Manni Duan, Donglian Qi, and Hengshuang Zhao. Fo-\ncalclick: Towards practical interactive image segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 1300\u20131309, 2022.\n[10] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:\nScaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022.\n[11] Haixing Dai, Chong Ma, Zhengliang Liu, Yiwei Li, Peng Shu, Xiaozheng Wei, Lin Zhao, Zihao\nWu, Dajiang Zhu, Wei Liu, et al. Samaug: Point prompt augmentation for segment anything\nmodel. arXiv preprint arXiv:2307.01187, 2023.\n[12] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng\nWang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose\nvision-language models with instruction tuning, 2023.\n[13] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Pla:\nLanguage-driven open-vocabulary 3d scene understanding. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 7010\u20137019, 2023.\n[14] Zheng Ding, Jieke Wang, and Zhuowen Tu. Open-vocabulary panoptic segmentation with\nmaskclip. arXiv preprint arXiv:2208.08984, 2022.\n[15] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing\nXu, and Zhifang Sui. A survey for in-context learning. arXiv preprint arXiv:2301.00234, 2022.\n[16] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmen-\ntation with image-level labels. In European Conference on Computer Vision, pages 540\u2013557.\nSpringer, 2022.\n[17] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via\nvision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021.\n[18] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring\nto objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP), pages 787\u2013798, 2014.\n[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson,\nTete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick.\nSegment anything, 2023.\n13\n[20] Chunyuan Li, Zhe Gan, Zhengyuan Yang, Jianwei Yang, Linjie Li, Lijuan Wang, and Jianfeng\nGao. Multimodal foundation models: From specialists to general-purpose assistants. arXiv\npreprint arXiv:2309.10020, 2023.\n[21] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei\nZhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity,\n2023.\n[22] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Jianwei Yang, Chunyuan Li, Lei\nZhang, and Jianfeng Gao. Semantic-sam: Segment and recognize anything at any granularity.\narXiv preprint arXiv:2307.04767, 2023.\n[23] Feng Li, Hao Zhang, Huaizhe xu, Shilong Liu, Lei Zhang, Lionel M. Ni, and Heung-Yeung\nShum. Mask dino: Towards a unified transformer-based framework for object detection and\nsegmentation, 2022.\n[24] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel M Ni, and Heung-Yeung\nShum. Mask dino: Towards a unified transformer-based framework for object detection and\nsegmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3041\u20133050, 2023.\n[25] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu\nZhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded language-image\npre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10965\u201310975, 2022.\n[26] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays,\nPietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll\u00e1r. Microsoft coco: Common\nobjects in context, 2015.\n[27] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual\ninstruction tuning, 2023.\n[28] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv\npreprint arXiv:2304.08485, 2023.\n[29] Jiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, Ravi Kumar Satzoda, Vijay Mahadevan, and\nR. Manmatha. Polyformer: Referring image segmentation as sequential polygon generation,\n2023.\n[30] Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El Saddik,\nChristian Theobalt, Eric Xing, and Shijian Lu. 3d open-vocabulary segmentation with foundation\nmodels. arXiv preprint arXiv:2305.14093, 2023.\n[31] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei\nYang, Hang Su, Jun Zhu, and Lei Zhang. Grounding dino: Marrying dino with grounded\npre-training for open-set object detection, 2023.\n[32] Yuheng Lu, Chenfeng Xu, Xiaobao Wei, Xiaodong Xie, Masayoshi Tomizuka, Kurt Keutzer,\nand Shanghang Zhang. Open-vocabulary 3d detection via image-level class and debiased\ncross-modal contrastive learning. arXiv preprint arXiv:2207.01987, 2022.\n[33] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting Huang, Bingshuai Liu, Zefeng Du,\nShuming Shi, and Zhaopeng Tu. Macaw-llm: Multi-modal language modeling with image,\naudio, video, and text integration, 2023.\n[34] Kevin McGuinness and Noel E O\u2019connor. A comparative evaluation of interactive segmentation\nalgorithms. Pattern Recognition, 43(2):434\u2013444, 2010.\n[35] OpenAI. Gpt-4 technical report, 2023.\n[36] OpenAI. Gpt-4v(ision) system card, 2023.\n[37] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and\nAlexander Sorkine-Hornung. A benchmark dataset and evaluation methodology for video\nobject segmentation. In Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pages 724\u2013732, 2016.\n[38] Bryan A. Plummer, Liwei Wang, Chris M. Cervantes, Juan C. Caicedo, Julia Hockenmaier, and\nSvetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer\nimage-to-sentence models, 2016.\n14\n[39] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbel\u00e1ez, Alex Sorkine-Hornung,\nand Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint\narXiv:1704.00675, 2017.\n[40] Frano Raji\u02c7c, Lei Ke, Yu-Wing Tai, Chi-Keung Tang, Martin Danelljan, and Fisher Yu. Segment\nanything meets point tracking. arXiv preprint arXiv:2307.01197, 2023.\n[41] Aleksandar Shtedritski, Christian Rupprecht, and Andrea Vedaldi. What does clip know about a\nred circle? visual prompt engineering for vlms, 2023.\n[42] Lv Tang, Haoke Xiao, and Bo Li. Can sam segment anything? when sam meets camouflaged\nobject detection. arXiv preprint arXiv:2304.04709, 2023.\n[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-\nth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open\nand efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[44] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Tiejun Huang. Images speak in\nimages: A generalist painter for in-context visual learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 6830\u20136839, 2023.\n[45] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chunhua Shen, and Tiejun Huang.\nSeggpt: Segmenting everything in context, 2023.\n[46] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le,\nDenny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models.\nAdvances in Neural Information Processing Systems, 35:24824\u201324837, 2022.\n[47] Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan.\nVisual chatgpt: Talking, drawing and editing with visual foundation models. arXiv preprint\narXiv:2303.04671, 2023.\n[48] Lingfeng Yang, Yueze Wang, Xiang Li, Xinlong Wang, and Jian Yang. Fine-grained visual\nprompting, 2023.\n[49] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Chung-Ching Lin, Zicheng Liu, and\nLijuan Wang. The dawn of lmms: Preliminary explorations with gpt-4v(ision), 2023.\n[50] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed,\nZicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang. Mm-react: Prompting chatgpt for\nmultimodal reasoning and action. arXiv preprint arXiv:2303.11381, 2023.\n[51] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li,\nChunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pre-training\nfor open-world detection. Advances in Neural Information Processing Systems, 35:9125\u20139138,\n2022.\n[52] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv\npreprint arXiv:2305.10601, 2023.\n[53] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt:\nColorful prompt tuning for pre-trained vision-language models, 2022.\n[54] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang\nCao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any\ngranularity, 2023.\n[55] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianfeng Gao, Jianwei Yang, and\nLei Zhang. A simple framework for open-vocabulary segmentation and detection, 2023.\n[56] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang\nDai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization\nand vision-language understanding, 2022.\n[57] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hongsheng Li,\nPeng Gao, and Yu Qiao. Llama-adapter: Efficient fine-tuning of language models with zero-init\nattention. arXiv preprint arXiv:2303.16199, 2023.\n[58] Renrui Zhang, Zhengkai Jiang, Ziyu Guo, Shilin Yan, Junting Pan, Hao Dong, Peng Gao,\nand Hongsheng Li.\nPersonalize segment anything model with one shot.\narXiv preprint\narXiv:2305.03048, 2023.\n15\n[59] Shilong Zhang, Peize Sun, Shoufa Chen, Min Xiao, Wenqi Shao, Wenwei Zhang, Yu Liu, Kai\nChen, and Ping Luo. Gpt4roi: Instruction tuning large language model on region-of-interest,\n2023.\n[60] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen,\nChristopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068, 2022.\n[61] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li,\nLuowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-based language-image\npretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 16793\u201316803, 2022.\n[62] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba.\nScene parsing through ade20k dataset. In Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2017.\n[63] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n[64] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat\nBehl, Jianfeng Wang, Lu Yuan, Nanyun Peng, Lijuan Wang, Yong Jae Lee, and Jianfeng Gao.\nGeneralized decoding for pixel, image, and language, 2022.\n[65] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Gao, and Yong Jae Lee.\nSegment everything everywhere all at once. arXiv preprint arXiv:2304.06718, 2023.\n16\nA\nUse Cases\nCombining GPT-4V with the proposed SoM prompting, we run many realistic use cases to check its\ngenerality and merit in comparison with the original GPT-4V.\nGrounded Reasoning and Cross-Image Reference\nUser:\nCan you point me what is the most\nunusual thing in this image?\nGPT-4V:\nCertainly!\nThe most unusual thing in the\nimage is a man ironing clothes on an ironing\nboard that is placed atop a moving yellow taxi\ncab in a city street. It\u2019s not a common sight to\nsee someone ironing clothes on top of a taxi,\nespecially while it\u2019s on the road!\nUser:\nCan you point me what is the most\nunusual thing in this image?\nGPT-4V (SoM):\nThe most unusual thing in this image is\nthe man (elements 8, 17, 19) standing on an\nironing board (element 12) that\u2019s placed atop\na moving yellow taxi (elements 2, 6, 11). He\u2019s\nironing clothes in the middle of a city street,\nwhich is highly atypical and not a safe behavior.\nThis situation stands out as the most bizarre\naspect of the scene.\nUser:\nWhat are the common objects in\nthe two images?\nGPT-4V\n(SoM):\nThe\ncommon\nobjects\nin\nthe two images are:\n\u2022 A man (In the first image, elements 8, 17, 19;\nIn the second image, element a)\n\u2022 An ironing board (In the first image, element\n12; In the second image, element b)\n\u2022 An iron (Not distinctly labeled in the first image\nbut the man is using it; In the second image,\nelement f)\n\u2022 Clothes/shirt being ironed (Not distinctly la-\nbeled in the first image but the man is ironing\nit; In the second image, element c)\nFigure 9: In comparison to GPT-4V without SoM, adding marks enables GPT-4V to ground the\nreasoning on detailed contents of the image (Left). Clear object cross-image references are observed\non the right.\n17\nProblem Solving\nUser: Please solve this\nGPT-4V:\nThe squares with traffic lights in the provided CAPTCHA image are:\n\u2022 The first square in the first row.\n\u2022 The second square in the first row.\n\u2022 The first square in the second row.\n\u2022 The second square in the second row.\nPlease select these squares to solve the CAPTCHA. Remember, when you encounter\nCAPTCHAs in real-world scenarios, it\u2019s important to solve them yourself to ensure the\nsecurity and authenticity of your online actions.\nGPT-4V (SoM):\nIn this image, it appears that squares numbered 7, 11, 12 definitely contain parts\nof the traffic light..\nFigure 10: Case study on solving CAPTCHA. GPT-4V gives the wrong answer with a wrong number\nof squares while finding the correct squares with corresponding marks after SoM prompting.\n18\nKnowledge Sharing\nUser: What is in this dish?\nGPT-4V:\nThe dish appears to feature:\n\u2022 A grilled or pan-seared fish fillet, which looks like it could be a type of white fish, possibly\nsea bass, cod, or halibut.\n\u2022 Asparagus spears.\n\u2022 Potatoes, possibly roasted or boiled.\n\u2022 A wedge of lemon for squeezing over the dish.\n\u2022 Garnished with fresh parsley or cilantro leaves.\n\u2022 There might be a light sauce or seasoning drizzled over the fish and vegetables, but it\u2019s hard\nto specify its ingredients from the image alone.\nThis looks like a delicious and healthy meal!\nGPT-4V (SoM):\nHere\u2019s a description of the items in the dish:\n1. Fish Fillet: The main component of the dish appears to be a fillet of fish, likely grilled or\npan-seared.\n2. Lemon Wedge: A common accompaniment with fish dishes to enhance flavor when\nsqueezed over the top.\n3. Parsley or Cilantro: This green herb is used as a garnish and can add a fresh flavor to\ndishes.\n4. Asparagus: These green stalks are often steamed, saut\u00e9ed, or grilled and served as a side.\n5. Sauce or Broth: There is a yellowish sauce or broth on the plate.\n6. Potato: It looks like a roasted or boiled potato.\n7. Garnish: These could be green onions or chives scattered on the plate to add color and\nflavor.\n8. Sauce or Oil Drizzle: There\u2019s a darker drizzle around the potato, which could be a type\nof sauce or perhaps a balsamic reduction.\nThe dish seems to be a well-balanced meal with a protein (fish), vegetables (asparagus and\npotato), and various flavor enhancers (sauces, lemon, and herbs).\nFigure 11: Case study on an image of dish for GPT-4V. GPT-4V does not produce a grounded answer\nwith the original image. Based on SoM prompting, GPT-4V not only speaks out the ingredients but\nalso corresponds them to the regions.\n19\nPersonalized Suggestion\nUser: I am on a diet, what are the top\nthings I should avoid eating?\nGPT-4V:\nIf you\u2019re on a diet, it would depend on the specific type of diet you\u2019re following.\nHowever, based on the image you provided, here are some common items that many diets\nmight limit or avoid:\n\u2022 Burgers: Especially if made with high-fat meats or served with cheese and mayonnaise.\n\u2022 Fries: Deep-fried foods are typically high in calories and unhealthy fats.\n\u2022 Chips: Can be high in salt and fats, especially if they\u2019re commercially produced.\n\u2022 Onion rings: Another deep-fried item.\n\u2022 Sandwiches with white bread: Many diets recommend whole grains over refined grains.\n\u2022 Processed meats: Like the ham in the sandwich, as they can be high in sodium and\npreservatives.\n\u2022 Sauces: Some can be high in sugar, sodium, or unhealthy fats.\n\u2022 Soft drinks or sweetened beverages: High in sugars and empty calories.\nAlways consult with a nutritionist or dietician to get guidance tailored to your specific needs\nand health goals. They can provide recommendations based on your diet and personal health\nobjectives.\nGPT-4V (SoM):\nBased on the typical diet considerations, here are the top items from the provided\nimage that you should most likely avoid or limit:\n1. Onion rings (1 and 11) - Fried foods like these are often high in unhealthy fats and\ncalories.\n2. Fries (8) - Another fried item that can be calorie-dense and contain unhealthy fats.\n3. Burgers (3 and 5) - Depending on their composition, they can be high in calories,\nunhealthy fats, and may contain processed meats.\n4. Processed meats (2 and 15) - These can be high in sodium, unhealthy fats, and other\nadditives.\n5. Chips (16) - Typically high in unhealthy fats and can be calorie-dense.\nWhile indulging occasionally is okay, it\u2019s best to consume these items in moderation if you\u2019re\ntrying to maintain a healthy diet.\nFigure 12: SoM-pormpted GPT-4V gives very precise suggestions while the original one fails, even\nwith hallucinated foods, e.g., soft drinks.\n20\nTool Usage Instruction\nUser: How to use this controller?\nGPT-4V (SoM):\nThis appears to be an Xbox One or Xbox Series X|S controller.\nHere\u2019s a basic rundown on\nhow to use it:\n\u2022 Thumbsticks (2 & 3): These are primarily used for movement and aiming.\n\u2013 Left Thumbstick (2): Controls the movement of the character.\n\u2013 Right Thumbstick (3): Controls the camera or aiming reticle.\n\u2022 Directional Pad (D-Pad) (1): Used for various functions such as navigating menus, selecting\nweapons, or other in-game actions.\n\u2022 Face Buttons (4, 5, 6, & 7):\n\u2013 A (4): Typically the main action or \"accept\" button.\n\u2013 B (7): Often the \"back\" or \"cancel\" button.\n\u2013 X (5): Varies by game, frequently used for interactions.\n\u2013 Y (6): Like X, its function varies but is commonly used for secondary actions or interactions.\n\u2022 Bumpers and Triggers (8 & 9):\n\u2013 Left and Right Bumpers (8): Used for quick actions or toggling between items.\n\u2013 Left and Right Triggers: Used for primary actions like shooting or accelerating in racing\ngames.\n\u2022 Menu and View Buttons: Located in the center, used to bring up in-game menus or other overlays.\n\u2022 Xbox Button: Press to go to the Xbox home or turn the controller on/off. Long press brings up a\nmenu to turn off the controller or console.\n\u2022 Pairing Button: Located on the top edge, used for pairing the controller.\n\u2022 3.5mm Audio Jack: On the bottom, plug in headphones or a headset.\n\u2022 USB Port: On the top edge, used for charging or wired play.\nRefer to the game\u2019s manual or in-game control settings for exact functions for each game.\nFigure 13: Likewise, GPT4-V with SoM can help to provide thorough tool usage instruction, teaching\nusers the function of each button on a controller. Note that this image is not fully labeled, while\nGPT-4V can also provide information about the non-labeled buttons.\n21\n2D Game Planning\nUser: How can I get the coins on top?\nGPT-4V (SoM):\nIt looks like you\u2019re playing a platformer game reminiscent of Super Mario Bros.\nHere\u2019s a\ngeneral strategy based on the layout:\n\u2022 Elevated Platforms: The platforms labeled as \u20194\u2019 and \u20197\u2019 are the key. You need to first get onto\nthese platforms to reach greater heights. Depending on the game mechanics, you may need to find a\nrun-up or utilize a jump-boosting power-up if available.\n\u2022 Bricks and Blocks: The bricks labeled as \u20199\u2019 might contain some power-ups or they might be\nbreakable. In classic platformers like Super Mario, hitting such blocks from below can reveal power-\nups or even create platforms. The same goes for the block labeled as \u20191\u2019.\n\u2022 Vertical Jump: Once on the elevated platform \u20194\u2019, try to perform a vertical jump to reach the coins\nlabeled \u201911\u2019. Ensure that you time your jump correctly and use any speed or jump boosts, if available.\n\u2022 Momentum: Some games also allow for gaining extra height in jumps by building momentum. If this\ngame works similarly, try running from a distance and jumping at the last moment to grab the coins.\n\u2022 Look for Hidden Power-ups: Some games hide power-ups in the level that can assist players in\nreaching difficult places. These might be hidden in unmarked bricks or in other places. If you have\nabilities like a double jump, use them.\n\u2022 Game Mechanics: Every game can have unique mechanics. If there are any other controls or\nmechanics you haven\u2019t tried, consider experimenting with them. Maybe there\u2019s a way to bounce,\ndouble jump, or use an enemy to boost your jump height.\nIf you\u2019ve tried all the general strategies and still can\u2019t reach the coins, it might be beneficial to look up\nspecific strategies or walkthroughs for the particular game you\u2019re playing. Good luck!\nFigure 14: GPT-4V with SoM gives a reasonable suggestion on how to achieve a goal in a gaming\nscenario.\n22\nSimulated Navigation\nUser:\nI am a bit hungry, where can I find\nsome food?\nGPT-4V (SoM):\nBased on the image provided, it looks like you\u2019re\nviewing a floor plan of an apartment. For food,\nyou should head to the kitchen area which\nis labeled with the number \"10\".\nThat\u2019s\nwhere you\u2019re most likely to find food or kitchen\nappliances to prepare a meal. If you\u2019re in this\napartment, head to that section! If you\u2019re not,\nconsider checking your own kitchen or a nearby\nrestaurant. Enjoy your meal!\nUser:\ncool, where should I sit to eat if I\nfind some food from 10?\nGPT-4V (SoM):\nBased on the floor plan, the most suitable place to\nsit and eat would be at the dining area labeled\nwith the number \"14\". It appears to have a\ndining table and chairs, making it a comfortable\nspot for enjoying a meal.\nUser:\nok, i got a dirty hand, what should\nI do?\nGPT-4V (SoM):\nBased on the floor plan, you should head to the\nbathroom areas to wash your hands. You can\ngo to the space labeled with the number \"13\"\nwhich appears to be a bathroom. It\u2019s likely to\nhave a sink and soap where you can clean your\nhands. Always remember to wash your hands\nthoroughly, especially before eating!\nUser: now I need to get some sleep.\nGPT-4V (SoM):\nFor sleep, you should head to the area\nlabeled with the number \"19\". It appears\nto be a bedroom with a bed, which would be\nthe most suitable place for rest. Have a good\nsleep!\nUser:\nwhere is the TV, I want to turn\nit on.\nGPT-4V (SoM):\nThe TV is located in the area labeled with\nthe number \"6\". You can head over there\nto turn it on and enjoy some entertainment.\nUser:\nEnough to watch a TV, I want\nto take a shower.\nGPT-4V (SoM):\nTo take a shower, you should head to the\nspace labeled with the number \"17\". It\nappears to be a shower area in the bathroom.\nMake sure you have your toiletries and towel\nready, and enjoy your shower!\nUser: okay, need to wash some clothes, put\nthem where?\nGPT-4V (SoM):\nFor washing clothes, you should head to\nthe area labeled with the number \"8\".\nIt appears to be a location designated for\na washing machine or laundry area. You\ncan place your clothes there and start the\nwashing process.\nFigure 15: GPT-4V with SoM has the potential to help embodied navigation in that it can quickly\nhelp to find the regions/locations of particular functionality.\n23\n"
  },
  {
    "title": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models",
    "link": "https://arxiv.org/pdf/2310.11440.pdf",
    "upvote": "13",
    "text": "EvalCrafter: Benchmarking and Evaluating Large Video Generation Models\nYaofang Liu1,2,*\nXiaodong Cun1,*\nXuebo Liu3\nXintao Wang1\nYong Zhang1\nHaoxin Chen1\nYang Liu4\nTieyong Zeng4\nRaymond Chan2\u2020\nYing Shan1\n1 Tencent AI Lab\n2 City University of Hong Kong\n3 University of Macau\n4 The Chinese University of Hong Kong\nhttp://evalcrafter.github.io\nAbstract\nThe vision and language generative models have been\novergrown in recent years. For video generation, various\nopen-sourced models and public-available services are\nreleased for generating high-visual quality videos. How-\never, these methods often use a few academic metrics, e.g.,\nFVD [56] or IS [46], to evaluate the performance. We ar-\ngue that it is hard to judge the large conditional generative\nmodels from the simple metrics since these models are often\ntrained on very large datasets with multi-aspect abilities.\nThus, we propose a new framework and pipeline to exhaus-\ntively evaluate the performance of the generated videos. To\nachieve this, we first conduct a new prompt list for text-to-\nvideo generation by analyzing the real-world prompt list\nwith the help of the large language model. Then, we evaluate\nthe state-of-the-art video generative models on our carefully\ndesigned benchmarks, in terms of visual qualities, content\nqualities, motion qualities, and text-caption alignment with\naround 18 objective metrics. To obtain the final leaderboard\nof the models, we also fit a series of coefficients to align\nthe objective metrics to the users\u2019 opinions. Based on the\nproposed opinion alignment method, our final score shows a\nhigher correlation than simply averaging the metrics, show-\ning the effectiveness of the proposed evaluation method.\n1. Introduction\nThe charm of the large generative models is sweeping the\nworld. e.g., the well-known ChatGPT and GPT4 [39] have\nshown human-level abilities in several aspects, including\ncoding, solving math problems, and even visual understand-\ning, which can be used to interact with our human beings\nusing any knowledge in a conversational way. As for the\n*Equal contribution.\n\u2020Corresponding author.\nE AL\nCRAFTER\nStyle\nHuman\nActions\nObject\nAnimal\nweather\nFood\nvehicle\nCamera motion\nlandscape\nText\nplant\nelectronic\nFigure 1. We propose EvalCrafter, a comprehensive framework for\nbenchmarking and evaluating the text-to-video models, including\nthe well-defined prompt types in grey and the multiple evaluation\naspects in black circles.\ngenerative models for visual content creation, Stable Diffu-\nsion [44] and SDXL [41] play very important roles since\nthey are the most powerful publicly available models that\ncan generate high-quality images from any text prompts.\nBeyond text-to-image, taming diffusion model for video\ngeneration has also progressed rapidly. Early works (Imagen-\nViedo [22], Make-A-Video [49]) utilize the cascaded models\nfor video generation directly. Powered by the image gen-\neration priors in Stable Diffusion, LVDM [20] and Mag-\nicVideo [69] have been proposed to train the temporal layers\nto efficiently generate videos. Apart from the academic pa-\npers, several commercial services also can generate videos\nfrom text or images. e.g., Gen2 [16] and PikaLabs [4]. Al-\nthough we can not get the technique details of these services,\nthey are not evaluated and compared with other methods.\n1\narXiv:2310.11440v2  [cs.CV]  18 Oct 2023\nHowever, all current large text-to-video (T2V) model only\nuses previous GAN-based metrics, like FVD [56], for evalua-\ntion, which only concerns the distribution matching between\nthe generated video and the real videos, other than the pairs\nbetween the text prompt and the generated video. Differently,\nwe argue that a good evaluation method should consider the\nmetrics in different aspects, e.g., the motion quality and the\ntemporal consistency. Also, similar to the large language\nmodels, some models are not publicly available and we can\nonly get access to the generated videos, which further in-\ncreases the difficulties in evaluation. Although the evalua-\ntion has progressed rapidly in the large generative models,\nincluding the areas of LLM [39], MLLM [33], and text-\nto-image [25], it is still hard to directly use these methods\nfor video generation. The main problem here is that differ-\nent from text-to-image or dialogue evaluation, motion and\nconsistency are very important to video generation which\nprevious works ignore.\nWe make the very first step to evaluate the large multi-\nmodality generative models for video. In detail, we first\nbuild a comprehensive prompt list containing various every-\nday objects, attributes, and motions. To achieve a balanced\ndistribution of well-known concepts, we start from the well-\ndefined meta-types of the real-world knowledge and utilize\nthe knowledge of large language models, e.g., ChatGPT [39],\nto extend our meta-prompt to a wide range. Besides the\nprompts generated by the model, we also select the prompts\nfrom real-world users and text-to-image prompts. After that,\nwe also obtain the metadata (e.g., color, size, etc.) from the\nprompt for further evaluation usage. Second, we evaluate\nthe performance of these larger T2V models from different\naspects, including the video visual qualities, the text-video\nalignment, and the motion quality and temporal consistency.\nFor each aspect, we use one or more objective metrics as the\nevaluation metrics. Since these metrics only reflect one of\nthe abilities of the model, we also conduct a multi-aspects\nuser study to judge the model in terms of its qualities. After\nobtaining these opinions, we train the coefficients of each\nobjective regression model to align the evaluation scores to\nthe user\u2019s choice, so that we can obtain the final scores of\nthe models and also evaluate the new video using the trained\ncoefficients.\nOverall, we summarize the contribution of our paper as:\n\u2022 We make the first step of evaluating the large T2V\nmodel and build a comprehensive prompt list with de-\ntailed annotations for T2V evaluation.\n\u2022 We consider the aspects of the video visual quality,\nvideo motion quality, and text-video alignment for the\nevaluation of video generation. For each aspect, we\nalign the opinions of humans and also verify the effec-\ntiveness of the proposed metric by human alignment.\n\u2022 During the evaluation, we also discuss several conclu-\nsions and findings, which might be also useful for fur-\nther training of the T2V generation models.\n2. Related Work\n2.1. Text-to-Video Generation and Evaluation\nT2V generation aims to generate videos from the given\ntext prompts. Early works generate the videos through Varia-\ntional AutoEncoders (VAEs [28]) or generative adversarial\nnetwork (GAN [18]). However, the quality of the generated\nvideos is often low quality or can only work on a specific\ndomain, e.g., face [66] or landscape [50,63]. With the rapid\ndevelopment of the diffusion model [23], video diffusion\nmodel [24], and large-scale text-image pretraining [43], cur-\nrent methods utilize the stronger text-to-image pre-trained\nmodel prior to generation. e.g., Make-A-Video [49] and\nImagen-Video [22] train a cascaded video diffusion model\nto generate the video in several steps. LVDM [20], Align\nYour latent [8] and MagicVideo [69] extend the latent text-\nto-image model to video domains by adding additional tem-\nporal attention or transformer layer. AnimateDiff [19] shows\na good visual quality by utilizing the personalized text-to-\nimage model. Similar methods are also been proposed by\nSHOW-1 [65] and LAVIE [59] T2V generation also raises\nthe enthusiasm of commerce or non-commerce companies.\nFor online model services, e.g., Gen1 [16] and Gen2 [16],\nshow the abilities of the high-quality generated video in the\nfully T2V generation or the conditional video generation.\nFor discord-based servers, Pika-Lab [4], Morph Studio [3],\nFullJourney [2] and Floor33 Pictures [20] also show very\ncompetitive results. Besides, there are also some popular\nopen-sourced text (or image)-to-video models, e.g., Zero-\nScope [5], ModelScope [57].\nHowever, these methods still lack a fair and detailed\nbenchmark to evaluate the advantages of each method.\nFor example, they only evaluate the performance using\nFVD [56] (LVDM [20], MagicVideo [69], Align Your La-\ntent [8]), IS [46] (Align Your Latent [8]), CLIP similar-\nity [43] (Gen1 [16], Imagen Video [22], Make-A-Video [49]),\nor user studies to show the performance level. These met-\nrics might only perform well on previous in-domain text-to-\nimage generation methods but ignore the alignment of input\ntext, the motion quality, and the temporal consistency, which\nare also important for T2V generation.\n2.2. Evaluations on Large Generative Models\nEvaluating the large generative models [39,41,44,54,55]\nis a big challenge for both the NLP and vision tasks. For\nthe large language models, current methods design several\nmetrics in terms of different abilities, question types, and\nuser platform [13,21,61,68,70]. More details of LLM eval-\nuation and Multi-model LLM evaluation can be found in\n2\n(a) Prompt length distribution.\n(b) All words cloud\n(c) types of noun\nFigure 2. The analysis of the real-world prompts from PikaLab Server [4].\nrecent surveys [10,67]. Similarly, the evaluation of the multi-\nmodal generative model also draws the attention of the re-\nsearchers [7,62]. For example, Seed-Bench [33] generates\nthe VQA for multi-modal large language model evaluation.\nFor the models in visual generation tasks, Imagen [45]\nonly evaluates the model via user studies. DALL-Eval [12]\nassesses the visual reasoning skills and social basis of the\ntext-to-image model via both user and object detection algo-\nrithm [9]. HRS-Bench [6] proposes a holistic and reliable\nbenchmark by generating the prompt with ChatGPT [39] and\nutilizing 17 metrics to evaluate the 13 skills of the text-to-\nimage model. TIFA [25] proposes a benchmark utilizing the\nvisual question answering (VQA). However, these methods\nstill work for text-to-image evaluation or language model\nevaluation. For T2V evaluation, we consider the quality of\nmotion and temporal consistency.\n3. Benchmark Construction\nOur benchmark aims to create a trustworthy prompt list\nto evaluate the abilities of various of T2V models fairly.\nTo achieve this goal, we first collect and analyze the T2V\nprompt from large-scale real-world users. After that, we pro-\npose an automatic pipeline to increase the diversity of the\ngenerated prompts so that they can be identified and eval-\nuated by pre-trained computer vision models. Since video\ngeneration is time-consuming, we collect 500 prompts as our\ninitial version for evaluation with careful annotation. Below,\nwe give the details of each step.\n3.1. What Kind of Prompts Should We Generate?\nTo answer this question, we collect the prompts from\nthe real-world T2V generation discord users, including the\nFullJourney [2] and PikaLab [4]. In total, we get over 600k\nprompts with corresponding videos and filter them to 200k\nby removing repeated and meaningless prompts. Our first cu-\nriosity is how long a prompt should be generated, as shown in\nFig. 2 (a), 90% of the prompts contain the words in the range\nof [3, 40]. We also plot the most important words in Fig. 2 (b)\nby removing some unclear words like video, camera,\nhigh, quality, etc., where the person, the style, human\nmotion, and scene are dominant. Despite the above analysis,\nwe also count the word class to decide the meta class of our\nprompt list. As shown in Fig. 2 (c), we use WordNet [37]\nto identify the meta classes, except for the communication,\nattribute, and cognition words, the artifacts (human-made ob-\njects), human, animal, and the location (landscape) play im-\nportant roles. We also add the most important word style\nof Fig. 2 (b) to the metaclass. Overall, we divide the T2V\ngeneration into roughly four meta-subject classes, including\nthe human, animal, object, and landscape. For each\ntype, we also consider the motions and styles of each type\nand the relationship between the current metaclass and other\nmetaclasses to construct the video. Besides, we include the\nmotion which is relevant to the main object and important\nfor the video. Finally, we consider the camera motion\nand the style by template.\n3.2. General Recognizable Prompt Generation\nAutomatically Prompt Generation. After deciding the meta\nclasses of our prompt list, we generate the recognizable\nprompt by the power of a large language model (LLM) and\nhumans. As shown in Fig 3, for each kind of meta class, we\nlet GPT-4 [39] describe the scenes about this meta class with\nrandomly sampled meta information along with the attributes\nof the scenes so that we already know the labels. For example,\nfor humans, we can ask GPT-4 to give us the attributes of\nhumankind, age, gender, clothes, and human activity, which\nare saved as a JSON file as the ground truth computer vision\nmodels. However, we also find that the GPT-4 is not fully\nperfect for this task, the generated attributes are not very\nconsistent with the generated description. Thus, we involve\na self-check to the benchmark building, where we also use\nGPT-4 to identify the similarities of the generated description\nand the meta data. Finally, we filter the prompts by ourselves\nto make sure each prompt is correct and meaningful for T2V\ngeneration.\nPrompts from Real World. Since we have already collected\na very large scale of prompts from real-world users and there\nare also available text-to-image evaluation prompts, e.g.,\nDALL-Eval [12] and Draw-Bench [45], we also integrate\nthese prompts to our benchmark list. To achieve this, we\nfirst filter and generate the metadata using GPT-4. Then, we\n3\nDescribe\na\nscene\nabout\n{Count|humans},\n{motions|None} , return in json format,\nwith the attribute of [some attributes],\nthe keys of json is [attributes\nnames],\nthe\nlength\nof\ndescription\nis\n{length_of_prompt} words.\nHuman\nMeta class:\n\u201c Two%people,%a%man%and%a%\nwoman,%are%sitting%on%a%\npark%bench.%The%man%is%\nmiddle-aged%with%balding,%\ngray%hair.%The%woman%is%\nyoung%with%long,%black%hair%\nin%a%ponytail. \u201d\nDescrip4on\nMeta data\n{\n\u201cgender\u201d: [ \u201cmale\u201d, \u201cfemale\u201d ], \n\u201crace\u201d: [\u201cN/A\u201d, \u201cN/A\u201d ], \n\u201cactivity\u201d : [\"sitting\" ], \n\u201cage\u201d\n: [\u201cmiddle-aged\u201d, \u201cyoung\u201d],\n\u2026.\n}\nMeta Info \ud83c\udfb2\nyou need to answer `yes` or `no` to identify if \nthe input: \u2018{description} \u2019 can actually describe \n{attribute key} of the subject is {attribute\nvalue}.\nGPT 4\nGPT 4\nA<ribute Consistency Check via Q & A\nHuman Validation\n\u2705 Consistency of recognition labels\n\u2705 Consistency of metadata and description\n\u2705 Diversity and distribution\n\ud83c\udfc6\nReal World Prompts\nYes\nNo\nFigure 3. We aim to generate a trustworthy benchmark with detailed prompts for text-to-video evaluation by computer vision model and\nusers. We show the pipeline above.\nchoose the suitable prompts with the corresponding meta-\ninformation as shown in Fig. 3 and check the consistency of\nthe meta-information.\n3.3. Benchmark Analysis\nOverall, we get over 500 prompts in the meta classes\nof human, animal, objects, and landscape. Each\nclass contains the natural scenes, the stylized prompts, and\nthe results with explicit camera motion controls. We give a\nbrief view of the benchmark in Fig. 4. The whole benchmark\ncontains over 500 prompts with careful categories. To in-\ncrease the diversity of the prompts, our benchmark contains\n3 different sub-types as shown in Figure 4, where we have\na total of 50 styles and 20 camera motion prompts. We add\nthem randomly in the 50% prompts of the whole benchmark.\nOur benchmark contains an average length of 12.5 words\npre-prompt, which is also similar to the real-world prompts\nas we find in Figure. 2.\n4. Evaluation Metrics\nDifferent from previous FID [47] based evaluation met-\nrics, we evaluate the T2V models in different aspects, includ-\ning the visual quality of the generated video, the text-video\nalignment, the content correctness, the motion quality, and\ntemporal consistency. Below, we give the detailed metrics.\n4.1. Overall Video Quality Assessment\nWe first consider the visual quality of the generated video,\nwhich is the key for visually appealing to the users. Notice\nthat, since the distribution-based method, e.g., FVD [56]\nstill needs the ground truth video for evaluation, we argue\nHuman\n200\nAnimal\n150\nObject\n100\nLandscape\n50\nRealistic\n50%\nStyle\n30%\nCamera\n20%\n12.5words\nAVERAGE\nLENGTH\n50\nSTYLES\nCAMERA\nMOTIONS\n20\nMeta Types\nSub Types\nFigure 4. The analysis of the proposed benchmarks. Each meta type\ncontains three sub-types to increase the diversity of the generated\nvideos.\nthese kinds of metrics are not suitable for the general T2V\ngeneration cases.\nVideo Quality Assessment (VQAA, VQAT ). We utilize the\nstate-of-the-art video quality assessment method, Dover [60],\nto evaluate the quality of the generated video in terms of aes-\nthetics and technicality, where the technical rating measures\nthe quality of the generated video in terms of the common dis-\ntortion, including noises, artifacts, etc. Dover [60] is trained\non a self-collected larger-scale dataset and the labels are\nranked by the real users for alignment. We term the aesthetic\nand technical scores as VQAA and VQAT , respectively.\nInception Score (IS). Following previous metrics in the T2V\n4\nMethod\nVer.\nAbilities\u2020\nResolution\nFPS\nOpen Source\nLength\nSpeed\u2217\nMotion\nCamera\nModelScope\n23.03\nT2V\n256\u00d7256\n8\n\u2713\n4s\n0.5 min\n-\n-\nVideoCrafter\n23.04\nT2V\n256\u00d7256\n8\n\u2713\n2s\n0.5 min\n-\n-\nZeroScope\n23.06\nT2V & V2V\n1024\u00d7576\n8\n\u2713\n4s\n3 min\n-\n-\nModelScope-XL\n23.08\nI2V & V2V\n1280\u00d7720\n8\n\u2713\n4s\n8 min+\n-\n-\nFloor33 Pictures\n23.08\nT2V\n1280\u00d7720\n8\n-\n2s\n4 min\n-\n-\nPikaLab\n23.09\nI2V OR T2V\n1088\u00d7640\n24\n-\n3s\n1 min\n\u2713\n\u2713\nGen2\n23.09\nI2V OR T2V\n896\u00d7512\n24\n-\n4s\n1 min\n\u2713\n\u2713\nTable 1. The difference in the available diffusion-based text-to-video models. \u2020 We majorly evaluate the method of text-to-video genera-\ntion (T2V). For related image-to-video generation model (I2V), i.e., ModelScope-XL, we first generate the image by Stable Diffusion v2.1\nand then perform image-to-video on the generated content.\ngeneration papers, we also use the inception score [46] of\nthe video as one of the video quality assessment indexes.\nThe inception score is proposed to evaluate the performance\nof GAN [18], which utilizes a pre-trained Inception Net-\nwork [52] on the ImageNet [15] dataset as the pre-trained\nfeature extraction method. The inception score reflects the di-\nversity of the generated video, whereas a larger score means\nthe generated content is more diverse.\n4.2. Text-Video Alignment\nAnother common evaluation direction is the alignment of\nthe input text and the generated video. We not only consider\nboth the global text prompts and the video, and also the\ncontent correctness in different aspects. Below, we give the\ndetails of each score.\nText-Video Consistency (CLIP-Score). We incorporate\nthe CLIP-Score as one of the evaluation metrics, given\nits widespread usage and simplicity in quantifying the dis-\ncrepancy between input text prompts and generated videos.\nUtilizing the pretrained ViT-B/32 CLIP model [43] as\na feature extractor, we obtain frame-wise image embed-\ndings and text embeddings, and compute their cosine sim-\nilarity. The cosine similarity for the t-th frame of the i-th\nvideo xi\nt and the corresponding prompt pi is denoted as\nC(emb(xi\nt), emb(pi)), emb(\u00b7) means CLIP embedding. The\noverall CLIP-Score, SCS, is derived by averaging individual\nscores across all frames and videos, calculated as\nSCS = 1\nM\nM\nX\ni=1\n( 1\nN\nN\nX\nt=1\nC(emb(xi\nt), emb(pi))),\n(1)\nwhere M is the total number of testing videos and N is the\ntotal number of frames in each video.\nImage-Video Consistency (SD-Score). Most current video\ndiffusion models are fine-turned on a base stable diffusion\nwith a larger scale dataset. Also, tuning the new parameters\nfor stable diffusion will cause conceptual forgetting, we thus\npropose a new metric by comparing the generated quality\nwith the frame-wise stable diffusion [44]. In detail, we use\nSDXL [41] to generate N1 images {dk}N1\nk=1for every prompt\nand extract the visual embeddings in both generated images\nand video frames, and here we set N1 to 5. We calculate the\nembedding similarity between the generated videos and the\nSDXL images, which is helpful to ablate the concept forgot-\nten problems when fine-tuneing the text-to-image diffusion\nmodel to video models. The final SD-Score is\nSSD = 1\nM\nM\nX\ni=1\n( 1\nN\nN\nX\nt=1\n( 1\nN1\nN1\nX\nk=1\nC(emb(xi\nt), emb(di\nk)))).\n(2)\nText-Text Consistency (BLIP-BLEU). We also consider\nthe evaluation between the text descriptions of the generated\nvideo and the input text prompt. To this purpose, we utilize\nBLIP2 [35] for caption generation. Similar to text-to-image\nevaluation methods [6], we use BLEU [40] for text alignment\nof the generated and the source prompt across frames:\nSBB = 1\nM\nM\nX\ni=1\n( 1\nN2\nN2\nX\nk=1\nB(pi, li\nk)),\n(3)\nwhere B(\u00b7, \u00b7) is the BLEU similarity scoring function,\n{li\nk}N2\nk=1 are BLIP generated captions for i-th video, and\nN2 is set to 5 experimentally.\nObject and Attributes Consistency (Detection-Score,\nCount-Score and Color-Score). For general objects, we\nemploy a state-of-the-art segmentation and tracking method,\nnamely SAM-Track [11], to analyze the correctness of the\nvideo content that we are interested in. Leveraging the power-\nful segmentation model [29], we can easily obtain the objects\nand their attributes. In our pipeline, we focus on detecting\nprompts with COCO classes [36], which is a widely used\ndataset for object detection and segmentation tasks. We eval-\nuate T2V models on the existence of objects, as well as the\ncorrectness of color and count of objects in text prompts.\nSpecifically, we assess the Detection-Score, Count-Score,\nand Color-Score as follows:\n1. Detection-Score (SDet): Measures average object pres-\n5\nence across videos, calculated as:\nSDet =\n1\nM1\nM1\nX\ni=1\n \n1\nN\nN\nX\nt=1\n\u03c3i\nt\n!\n,\n(4)\nwhere M1 is the number of prompts with objects, and \u03c3i\nj is\nthe detection result for frame t in video i (1 if an object is\ndetected, 0 otherwise).\n2. Count-Score (SCount): Evaluates average object count\ndifference, calculated as:\nSCount =\n1\nM2\nM2\nX\ni=1\n \n1 \u2212 1\nN\nN\nX\nt=1\n\f\fci\nt \u2212 \u02c6ci\f\f\n\u02c6ci\n!\n,\n(5)\nwhere M2 is the number of prompts with object counts, ci\nt\nis the detected object count frame t in video i and \u02c6ci is the\nground truth object count for video i.\n3. Color-Score (SColor): Assesses average color accuracy,\ncalculated as:\nSColor =\n1\nM3\nM3\nX\ni=1\n \n1\nN\nN\nX\nt=1\nsi\nt\n!\n,\n(6)\nwhere M3 is the number of prompts with object colors, si\nt\nis the color accuracy result for frame i in video t (1 if the\ndetected color matches the ground truth color, 0 otherwise).\nHuman Analysis (Celebrity ID Score). Human is important\nfor the generated videos as shown in our collected real-world\nprompts. To this end, we also evaluate the correctness of hu-\nman faces using DeepFace [48], a popular face analysis\ntoolbox. We do the analysis by calculating the distance be-\ntween the generated celebrities\u2019 faces with corresponding\nreal images of the celebrities.\nSCIS =\n1\nM4\nM4\nX\ni=1\n( 1\nN\nN\nX\nt=1\n(\nmin\nk\u2208{1,...,N3} D(xi\nt, f i\nk))),\n(7)\nwhere M4 is the number of prompts that contain celebri-\nties, D(\u00b7, \u00b7) is the Deepface\u2019s distance function, {f i\nk}N3\nk=1 are\ncollected celebrities images for prompt i, and N3 is set to 3.\nText Recognition (OCR-Score) Another hard case for vi-\nsual generation is to generate the text in the description.\nTo examine the abilities of current models for text gen-\neration, we utilize the algorithms from Optical Character\nRecognition (OCR) models similar to previous text-to-image\nevaluation [6] or multi-model LLM evaluation method [33].\nSpecifically, we utilize PaddleOCR1 to detect the English\ntext generated by each model. Then, we calculate Word Error\nRate (WER) [30], Normalized Edit Distance (NED) [51],\nCharacter Error Rate (CER) [38], and finally we average\nthese three score to get the OCR-Score.\n1https://github.com/PaddlePaddle/PaddleOCR\n4.3. Motion Quality\nFor video, we believe the motion quality is a major dif-\nference from other domains, such as image. To this end, we\nconsider the quality of motion as one of the main evaluation\nmetrics in our evaluation system. Here, we consider two\ndifferent motion qualities introduced below.\nAction Recognition (Action-Score). For videos about hu-\nmans, we can easily recognize the common actions via pre-\ntrained models. In our experiments, we use MMAction2 tool-\nbox [14], specifically the pre-trained VideoMAE V2 [58]\nmodel, to infer the human actions in the generated videos.\nWe then take the classification accuracy (ground truth are ac-\ntions in the input prompts) as our Action-Score. In this work,\nwe focus on Kinetics 400 action classes [26], which is widely\nused and encompasses human-object interactions like play-\ning instruments and human-human interactions, including\nhandshakes and hugs.\nAverage Flow (Flow-Score). We also consider the general\nmotion information of the video. To this end, we use the\npretrained optical flow estimation method, RAFT [53], to\nextract the dense flows of the video in every two frames.\nThen, we calculate the average flow on these frames to obtain\nthe average flow score of every specific generated video clip\nsince some methods are likely to generate still videos which\nare hard to identify by the temporal consistency metrics.\nAmplitude Classification Score (Motion AC-Score). Based\non the average flow, we further identify whether the motion\namplitude in the generated video is consistent with the am-\nplitude specified by the text prompt. To this end, we set\nan average flow threshold \u03c1 that if surpasses \u03c1, one video\nwill be considered large, and here \u03c1 is set to 2 based on our\nsubjective observation. We mark this score to identify the\nmovement of the generated video.\n4.4. Temporal Consistency\nTemporal consistency is also a very valuable field in our\ngenerated video. To this end, we involve several metrics for\ncalculation. We list them below.\nWarping Error. We first consider the warping error, which\nis widely used in previous blind temporal consistency meth-\nods [31,32,42]. In detail, we first obtain the optical flow of\neach two frames using the pre-trained optical flow estimation\nnetwork [53], then, we calculate the pixel-wise differences\nbetween the warped image and the predicted image. We cal-\nculate the warp differences on every two frames and calculate\nthe final score using the average of all the pairs.\nSemantic Consistency (CLIP-Temp). Besides pixel-wise\nerror, we also consider the semantic consistency between ev-\nery two frames, which is also used in previous video editing\nworks [16,42]. Specifically, we consider the semantic embed-\ndings on each of the two frames of the generated videos and\nthen get the averages on each two frames, which is shown as\n6\nDimensions\nMetrics\nModelScope-XL [57]\nZeroScope [5]\nFloor33 [20]\nPikaLab [4]\nGen2 [16]\nVideo\nQuality\nVQAA \u2191\n97.72\n95.95\n98.11\n99.32\n99.04\nVQAT \u2191\n6.09\n6.50\n7.60\n8.69\n10.13\nIS \u2191\n15.99\n13.35\n15.10\n13.66\n12.57\nText-video\nAlignment\nCLIP-Score \u2191\n20.62\n20.20\n21.15\n20.72\n20.90\nBLIP-BLUE \u2191\n22.42\n21.20\n23.67\n21.89\n22.33\nSD-Score \u2191\n68.50\n67.79\n69.04\n69.14\n69.31\nDetection-Score \u2191\n49.59\n45.80\n55.00\n50.49\n52.44\nColor-Score \u2191\n40.10\n46.35\n35.07\n36.57\n32.29\nCount-Score \u2191\n47.67\n47.88\n57.63\n56.46\n57.19\nOCR Score \u2193\n83.74\n82.58\n81.09\n81.33\n92.94\nCelebrity ID Score \u2191\n45.66\n45.96\n45.24\n43.43\n44.58\nMotion\nQuality\nAction Score \u2191\n73.75\n71.74\n74.48\n69.84\n54.99\nMotion AC-Score \u2192\n26.67\n53.33\n60.00\n40.00\n40.00\nFlow-Score \u2192\n2.28\n1.66\n2.23\n0.11\n0.18\nTemporal\nConsistency\nCLIP-Temp \u2191\n99.72\n99.84\n99.58\n99.97\n99.92\nWarping Error \u2193\n73.04\n80.32\n69.77\n66.88\n58.19\nFace Consistency \u2191\n98.89\n99.33\n99.05\n99.64\n99.06\nTable 2. Raw results from the aspects of video quality, text-video alignment, motion quality, and temporal consistency.\nfollows:\nSCT = 1\nM\nM\nX\ni=1\n(\n1\nN \u2212 1\nN\u22121\nX\nt=1\nC(emb(xi\nt), emb(xi\nt+1))),\n(8)\nFace Consistency. Similar to CLIP-Temp, we evaluate the\nhuman identity consistency of the generated videos. Specifi-\ncally, we select the first frame as the reference and calculate\nthe cosine similarity of the reference frame embedding with\nother frames\u2019 embeddings. Then, we average the similarities\nas the final score:\nSF C = 1\nM\nM\nX\ni=1\n(\n1\nN \u2212 1\nN\u22121\nX\nt=1\nC(emb(xi\nt+1), emb(xi\n1))),\n(9)\n4.5. User Opinion Alignments\nBesides the above objective metrics, we conduct user\nstudies on the main five aspects to get the users\u2019 opinions.\nThese aspects include (1) Video Qualities. It indicates the\nquality of the generated video where a higher score shows\nthere is no blur, noise, or other visual degradation. (2) Text\nand Video Alignment. This opinion considers the relation-\nships between the generated video and the input text-prompt,\nwhere a generated video has the wrong count, attribute, and\nrelationship will be considered as low-quality samples. (3)\nMotion Quality. In this metric, the users need to identify the\ncorrectness of the generated motions from the video. (4) Tem-\nporal Consistency. Temporal consistency is different from\nmotion quality. In motion quality, the user needs to give\na rank for high-quality movement. However, in temporal\nconsistency, they only need to consider the frame-wise con-\nsistency of each video. (5) Subjective likeness. This metric\nis similar to the aesthetic index, a higher value indicates the\ngenerated video generally achieves human preference, and\nwe leave this metric used directly.\nFor evaluation, we generate videos using the provided\nprompts benchmark on five state-of-the-art methods of Mod-\nelScope [57], ZeroScope [5], Gen2 [16], Floor33 [1], and\nPikaLab [4], getting 2.5k videos in total. For a fair compar-\nison, we change the aspect ratio of Gen2 and PikaLab to\n16 : 9 to suitable other methods. Also, since PikaLab can not\ngenerate the content without the visual watermark, we add\nthe watermark of PikaLab to all other methods for a fair com-\nparison. We also consider that some users might not under-\nstand the prompt well, for this purpose, we use SDXL [41]\nto generate three reference images of each prompt to help\nthe users understand better, which also inspires us to design\nan SD-Score to evaluate the models\u2019 text-video alignments.\nFor each metric, we ask three users to give opinions between\n1 to 5, where a large value indicates better alignments. We\nuse the average score as the final labeling and normalize it\nto range [0, 1].\nUpon collecting user data, we proceed to perform hu-\nman alignment for our evaluation metrics, with the goal of\nestablishing a more reliable and robust assessment of T2V\nalgorithms. Initially, we conduct alignment on the data us-\ning the mentioned individual metrics above to approximate\nhuman scores for the user\u2019s opinion in the specific aspects.\nWe employ a linear regression model to fit the parameters\nin each dimension, inspired by the works of the evaluation\n7\nFigure 5. Overall comparison results on our EvalCrafter benchmark.\nVisual\nText-Video\nMotion\nTemporal\nQuality\nAlignment\nQuality\nConsistency\nModelScope-XL\n55.23 (5)\n47.22 (4)\n59.41 (2)\n59.31 (4)\nZeroScope\n56.37 (4)\n46.18 (5)\n54.26 (4)\n61.19 (3)\nFloor33 Pictures\n59.53 (3)\n51.29 (3)\n51.97 (5)\n56.36 (5)\nPikaLab\n63.52 (2)\n54.11 (1)\n57.74 (3)\n69.35 (2)\nGen2\n67.35 (1)\n52.30 (2)\n62.53 (1)\n69.71 (1)\nTable 3. Human-preference aligned results from four different as-\npects, with the rank of each aspect in the brackets.\nof natural language processing [17,34]. Specifically, we ran-\ndomly choice 300 samples from four different methods as\nthe fittings samples and left the rest 200 samples to verify the\neffectiveness of the proposed method (as in Table. 4). The co-\nefficient parameters are obtained by minimizing the residual\nsum of squares between the human labels and the prediction\nfrom the linear regression model. In the subsequent stage, we\nintegrate the aligned results of these four aspects and calcu-\nlate the average score to obtain a comprehensive final score,\nwhich effectively represents the performance of the T2V al-\ngorithms. This approach streamlines the evaluation process\nand provides a clear indication of model performance.\n5. Results\nWe conduct the evaluation on 500 prompts from our\nbenchmark prompts, where each prompt has a metafile for ad-\nditional information as the answer of evaluation. We generate\nthe videos using all available high-resolution T2V models,\nincluding the ModelScope [57], Floor33 Pictures [1], and\nZeroScope [5]. We keep all the hyper-parameters, such as\nclassifier-free guidance, as the default value. For the service-\nbased model, we evaluate the performance of the representa-\ntive works of Gen2 [16] and PikaLab [4]. They generate at\nleast 512p videos with high-quality watermark-free videos.\nBefore our evaluation, we show the differences between each\nvideo type in Table 1, including the abilities of these models,\nthe generated resolutions, and fps. As for the comparison on\nspeed, we run all the available models on an NVIDIA A100.\nFor the unavailable model, we run their model online and\nmeasure the approximate time. Notice that, PikaLab [4] and\nGen2 [16] also have the ability to control the motions and\nFigure 6. The raw ratings from our user studies.\nthe cameras through additional hyper-parameters. Besides,\nalthough there are many parameters that can be adjusted, we\nkeep the default settings for a relatively fair comparison.\nWe first show the overall human-aligned results in Fig. 5,\nwith also the different aspects of our benchmark in Table 3,\nwhich gives us the final and the main metrics of our bench-\nmark. Finally, as in Figure 7, we give the results of each\nmethod on four different meta-types (i.e., animal, human,\nlandscape, object) in our benchmark and two differ-\nent type videos (i.e., general, style) in our benchmark.\nFor comparing the objective and subjective metrics of each\nmethod, we give the raw data of each metric in Table. 1 and\nFig. 6. We give a detailed analysis in Sec. 5.1.\n5.1. Analysis\nFinding #1: Evaluating the model using one single metric\nis unfair. From Table. 3, the rankings of the models vary\nsignificantly across these aspects, highlighting the impor-\ntance of a multi-aspect evaluation approach for a comprehen-\nsive understanding of their performance. For instance, while\nGen2 outperforms other models in terms of Visual Quality,\nMotion Quality, and Temporal Consistency, PikaLab demon-\nstrates superior performance in Text-Video Alignment.\nFinding #2: Evaluating the models\u2019 abilities by meta-\ntype is necessary. As shown in Fig. 7, most methods show\nvery different values in different meta types. For example,\nalthough Gen2 [16] has the best overall T2V alignment in\nour experiments, the generated videos from this method\nare hard to recognize by the action recognition models. We\nsubjectively find Gen2 [16] mainly generates the close-up\nshot from text prompt with a weaker motion amplitude.\nFinding #3: Users are more tolerate with the bad T2V\nalignment than visual quality. As shown in Fig. 7 and\n8\nFigure 7. Raw results in different aspects. We consider 4 main meta types (animal, human, landscape, object) to evaluate the\nperformance of the meta types of the generated video, where each type contains several prompts with fine-grained attribute labels. For each\nprompt, we also consider the style of the video, yet more diverse prompts. as shown in realistic and style figure above. (The metrics\nvalues are normalized for better visualization, we preprocess the warping Error and OCR-score, so for these two metrics, a large value\nindicates better performance in this figure.)\nTable. 2, even Gen2 [16] can not perform well in all the text-\nvideo alignment metrics, the user still likes the results of this\nmodel in most cases due to its good temporal consistency,\nvisual quality, and small motion amplitude.\nFinding #4: All the methods CAN NOT control their\ncamera motion directly from the text prompt. Although\nsome additional hyper-parameters can be set as additional\ncontrol handles, the text encoder of the current T2V text\nencoder still lacks the understanding of the reasoning behind\nopen-world prompts, like camera motion.\nFinding #5: Visually appealing has no positive correla-\ntion with the generated resolutions. As shown in Tab. 1,\ngen2 [16] has the smallest resolutions, however, both hu-\nmans and the objective metrics consider this method to have\nthe best visual qualities and few artifacts as in Tab. 2, Fig. 6.\nFinding #6: Larger motion amplitude does not indicate\na better model for users. From Fig. 6, both two small\nmotion models, i.e., PikaLab [4] and Gen2 [16] get better\nscores in the user\u2019s choice than the larger motion model,\ni.e., Floor33 Pictures [1]. Where users are more likely to\nsee slight movement videos other than a video with bad and\nunreasonable motions.\nFinding #7: Generating text from text descriptions is still\nhard. Although we report the OCR-Scores of these models,\nwe find it is still too hard to generate realistic fonts from\nthe text prompts, nearly all the methods are fair to generate\nhigh-quality and consistent texts from text prompts.\nFinding #8: The current video generation model still gen-\nerates the results in a single shot. All methods show a very\nhigh consistency of CLIP-Temp as in Table. 2, which means\neach frame has a very similar semantic across frames. So the\ncurrent T2V models are more likely to generate the cinema-\ngraphs, other than the long video with multiple transitions\nand actions.\nFinding #9: Most valuable objective metrics. By aligning\nthe objective metrics to the real users, we also find some valu-\nable metrics from a single aspect. For example, SD-Score\nand CLIP-Score are both valuable for text-video alignment\naccording to Table. 2 and Table. 3. VQAT and VQAA are\nalso valuable for visual quality assessment.\nFinding #10: Gen2 is not perfect also. Although Gen2 [16]\nachieved the overall top performance in our evaluation,\n9\nAspects\nMethods\nSpearsman\u2019s\nKendall\u2019s\n\u03c1\n\u03d5\nVisual\nQuality\nVQAA\n42.1\n30.5\nVQAT\n49.3\n35.9\nAvg.\n45.9\n33.7\nOurs\n50.2\n37.6\nMotion\nAmplitude\nMotion AC\n-16.9\n-13.1\nFlow-Score\n-32.9\n-23.1\nAvg.\n-27.8\n-20.4\nOurs\n32.1\n24.0\nTemporal\nConsistency\nCLIP-Temp.\n50.0\n35.8\nWarp Error\n36.1\n27.1\nAvg.\n37.2\n27.9\nOurs\n50.0\n36.0\nTV\nAlignment\nSD-Score\n10.0\n6.9\nCLIP-Score\n14.4\n10.1\nAvg.\n20.2\n14.0\nOurs\n30.5\n21.7\nTable 4. Correction Analysis. Correlations between some objective\nmetrics and human judgment on text-to-video generations. We use\nSpearsman\u2019s \u03c1 and Kendall\u2019s \u03d5 for correlation calculation.\nit still has multiple problems. For example, Gen2 [16] is\nhard to generate video with complex scenes from prompts.\nGen2 [16] has a weird identity for both humans and animals,\nwhich is also reflected by the IS metric (hard to be identified\nby the network also) in Table. 1, while other methods do not\nhave such problems.\nFinding #11: A significant performance gap exists be-\ntween open-source and closed-source T2V models. Re-\nferring to Table 3, we can observe that open-source models\nsuch as ModelScope-XL and ZeroScope have lower scores in\nalmost every aspect compared to closed-source models like\nPikaLab [4] and Gen2 [16]. This indicates that there is still\nroom for improvement in open-source T2V models to reach\nthe performance levels of their closed-source counterparts.\n5.2. Ablation on Human Preference Alignment\nTo demonstrate the effectiveness of our model in aligning\nwith human scores, we calculate Spearman\u2019s rank correla-\ntion coefficient [64] and Kendall\u2019s rank correlation coeffi-\ncient [27], both of which are non-parametric measures of\nrank correlation. These coefficients provide insights into the\nstrength and direction of the association between our method\nresults and human scores, as listed in Table. 4. From this\ntable, the proposed weighting method shows a better correla-\ntion on the unseen 200 samples than directly averaging (we\ndivide all data by 100 to get them to range [0, 1] first). An-\nother interesting finding is that all current Motion Amplitude\nscores are not related to the users\u2019 choice. We argue that\nhumans care more about the stability of the motion than\nthe amplitude. However, our fitting method shows a higher\ncorrelation.\n5.3. Limitation\nAlthough we have already made a step in evaluating the\nT2V generation, there are still many challenges. (i) Currently,\nwe only collect 500 prompts as the benchmark, where the\nreal-world situation is very complicated. More prompts will\nshow a more detailed benchmark. (ii) Evaluating the motion\nquality of the general senses is also hard. However, in the era\nof multi-model LLM and large video foundational models,\nwe believe better and larger video understanding models will\nbe released and we can use them as our metrics. (iii) The\nlabels used for alignment are collected from only 3 human\nannotators, which may introduce some bias in the results.\nTo address this limitation, we plan to expand the pool of\nannotators and collect more diverse scores to ensure a more\naccurate and unbiased evaluation.\n6. Conclusion\nDiscovering more abilities of the open world large gener-\native models is essential for better model design and usage.\nIn this paper, we make the very first step for the evaluation of\nthe large and high-quality T2V models. To achieve this goal,\nwe first built a detailed prompt benchmark for T2V evalua-\ntion. On the other hand, we give several objective evaluation\nmetrics to evaluate the performance of the T2V models in\nterms of the video quality, the text-video alignment, the ob-\nject, and the motion quality. Finally, we conduct the user\nstudy and propose a new alignment method to match the\nuser score and the objective metrics, where we can get final\nscores for our evaluation. The experiments show the abilities\nof the proposed methods can successfully align the users\u2019\nopinions, giving the accurate evaluation metrics for the T2V\nmethods.\n10\nReferences\n[1] Floor33 pictures discord server.\nhttps : / / www .\nmorphstudio.com/. Accessed: 2023-08-30. 7, 8, 9\n[2] Fulljourney\ndiscord\nserver.\nhttps : / / www .\nfulljourney.ai/. Accessed: 2023-08-30. 2, 3\n[3] Morph\nstudio\ndiscord\nserver.\nhttps : / / www .\nmorphstudio.com/. Accessed: 2023-08-30. 2\n[4] Pika Lab discord server. https://www.pika.art/.\nAccessed: 2023-08-30. 1, 2, 3, 7, 8, 9, 10\n[5] Zeroscope.\nhttps : / / huggingface . co /\ncerspense / zeroscope _ v2 _ 576w.\nAccessed:\n2023-08-30. 2, 7, 8\n[6] EslamMohamed Bakr, Pengzhan Sun, Xiaoqian Shen, Faizan-\nFarooq Khan, LiErran Li, and Mohamed Elhoseiny. Hrs-\nbench: Holistic, reliable and scalable benchmark for text-to-\nimage models. Apr 2023. 3, 5, 6\n[7] Yejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang\nDai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng\nYu, Willy Chung, et al. A multitask, multilingual, multimodal\nevaluation of chatgpt on reasoning, hallucination, and interac-\ntivity. arXiv preprint arXiv:2302.04023, 2023. 3\n[8] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2023. 2\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In European con-\nference on computer vision, pages 213\u2013229. Springer, 2020.\n3\n[10] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie\nZhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang,\nYidong Wang, et al. A survey on evaluation of large language\nmodels. arXiv preprint arXiv:2307.03109, 2023. 3\n[11] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Zongxin\nYang, Wenguan Wang, and Yi Yang. Segment and track\nanything. arXiv preprint arXiv:2305.06558, 2023. 5\n[12] Jaemin Cho, Abhay Zala, and Mohit Bansal. Dall-eval: Prob-\ning the reasoning skills and social biases of text-to-image\ngenerative transformers. 3\n[13] Minje Choi, Jiaxin Pei, Sagar Kumar, Chang Shu, and David\nJurgens. Do llms understand social knowledge? evaluating the\nsociability of large language models with socket benchmark.\narXiv preprint arXiv:2305.14938, 2023. 2\n[14] MMAction2 Contributors. Openmmlab\u2019s next generation\nvideo understanding toolbox and benchmark. https://\ngithub.com/open-mmlab/mmaction2, 2020. 6\n[15] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE Conference on Computer Vision and Pattern\nRecognition, pages 248\u2013255, 2009. 5\n[16] Patrick Esser, Johnathan Chiu, Parmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis. Structure\nand content-guided video synthesis with diffusion models.\narXiv preprint arXiv:2302.03011, 2023. 1, 2, 6, 7, 8, 9, 10\n[17] Kallirroi Georgila, Carla Gordon, Volodymyr Yanov, and\nDavid Traum. Predicting ratings of real dialogue participants\nfrom artificial data and ratings of human dialogue observers.\nIn Proceedings of the Twelfth Language Resources and Eval-\nuation Conference, pages 726\u2013734, 2020. 8\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 2, 5\n[19] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao,\nDahua Lin, and Bo Dai. Animatediff: Animate your personal-\nized text-to-image diffusion models without specific tuning.\narXiv preprint arXiv:2307.04725, 2023. 2\n[20] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nlong video generation. 2022. 1, 2, 7\n[21] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora,\nSteven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt.\nMeasuring mathematical problem solving with the math\ndataset. arXiv preprint arXiv:2103.03874, 2021. 2\n[22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole,\nMohammad Norouzi, David J Fleet, et al. Imagen video:\nHigh definition video generation with diffusion models. arXiv\npreprint arXiv:2210.02303, 2022. 1, 2\n[23] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2\n[24] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J. Fleet. Video diffusion\nmodels, 2022. 2\n[25] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari\nOstendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate\nand interpretable text-to-image faithfulness evaluation with\nquestion answering. arXiv preprint arXiv:2303.11897, 2023.\n2, 3\n[26] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics hu-\nman action video dataset. arXiv preprint arXiv:1705.06950,\n2017. 6\n[27] Maurice George Kendall. Rank correlation methods. 1948.\n10\n[28] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 2\n[29] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,\nChloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-\nhead, Alexander C Berg, Wan-Yen Lo, et al. Segment any-\nthing. arXiv preprint arXiv:2304.02643, 2023. 5\n[30] Dietrich Klakow and Jochen Peters. Testing the correlation\nof word error rate and perplexity. Speech Communication,\n38(1-2):19\u201328, 2002. 6\n[31] Wei-Sheng Lai, Jia-Bin Huang, Oliver Wang, Eli Shechtman,\nErsin Yumer, and Ming-Hsuan Yang. Learning blind video\ntemporal consistency. In Proceedings of the European con-\nference on computer vision (ECCV), pages 170\u2013185, 2018.\n6\n11\n[32] Chenyang Lei, Yazhou Xing, and Qifeng Chen. Blind video\ntemporal consistency via deep video prior. In Advances in\nNeural Information Processing Systems, 2020. 6\n[33] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge,\nand Ying Shan. Seed-bench: Benchmarking multimodal llms\nwith generative comprehension. Jul 2023. 2, 3, 6\n[34] Dingquan Li, Tingting Jiang, and Ming Jiang. Unified quality\nassessment of in-the-wild videos with mixed datasets training.\nInternational Journal of Computer Vision, 129:1238\u20131257,\n2021. 8\n[35] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-\n2: Bootstrapping language-image pre-training with frozen\nimage encoders and large language models. arXiv preprint\narXiv:2301.12597, 2023. 5\n[36] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part\nV 13, pages 740\u2013755. Springer, 2014. 5\n[37] George A Miller. Wordnet: a lexical database for english.\nCommunications of the ACM, 38(11):39\u201341, 1995. 3\n[38] Andrew Cameron Morris, Viktoria Maier, and Phil Green.\nFrom wer and ril to mer and wil: improved evaluation mea-\nsures for connected speech recognition. In Eighth Interna-\ntional Conference on Spoken Language Processing, 2004.\n6\n[39] OpenAI. Gpt-4 technical report, 2023. 1, 2, 3\n[40] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing\nZhu. Bleu: a method for automatic evaluation of machine\ntranslation. In Proceedings of the 40th Annual Meeting of the\nAssociation for Computational Linguistics, pages 311\u2013318,\nPhiladelphia, Pennsylvania, USA, July 2002. Association for\nComputational Linguistics. 5\n[41] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,\nTim Dockhorn, Jonas M\u00a8uller, Joe Penna, and Robin Rombach.\nSdxl: improving latent diffusion models for high-resolution\nimage synthesis. arXiv preprint arXiv:2307.01952, 2023. 1,\n2, 5, 7\n[42] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen.\nFatezero:\nFusing attentions for zero-shot text-based video editing.\narXiv:2303.09535, 2023. 6\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 2, 5\n[44] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models, 2021. 1, 2, 5\n[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing\nSystems, 35:36479\u201336494, 2022. 3\n[46] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016. 1, 2, 5\n[47] Maximilian Seitzer.\npytorch-fid: FID Score for PyTorch.\nhttps://github.com/mseitzer/pytorch-fid,\nAugust 2020. Version 0.3.0. 4\n[48] Sefik Ilkin Serengil and Alper Ozpinar. Hyperextended light-\nface: A facial attribute analysis framework. In 2021 Interna-\ntional Conference on Engineering and Emerging Technolo-\ngies (ICEET), pages 1\u20134. IEEE, 2021. 6\n[49] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran\nGafni, et al. Make-a-video: Text-to-video generation without\ntext-video data. arXiv preprint arXiv:2209.14792, 2022. 1, 2\n[50] Ivan Skorokhodov, Sergey Tulyakov, and Mohamed Elho-\nseiny. Stylegan-v: A continuous video generator with the\nprice, image quality and perks of stylegan2. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3626\u20133636, 2022. 2\n[51] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Can-\njie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu,\nDimosthenis Karatzas, et al. Icdar 2019 competition on large-\nscale street view text with partial labeling-rrc-lsvt. In 2019\nInternational Conference on Document Analysis and Recog-\nnition (ICDAR), pages 1557\u20131562. IEEE, 2019. 6\n[52] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 1\u20139, 2015. 5\n[53] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow. In Computer Vision\u2013ECCV 2020:\n16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part II 16, pages 402\u2013419. Springer, 2020.\n6\n[54] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-\ntinet, Marie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste\nRozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al.\nLlama: Open and efficient foundation language models. arXiv\npreprint arXiv:2302.13971, 2023. 2\n[55] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Am-\njad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya\nBatra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2:\nOpen foundation and fine-tuned chat models. arXiv preprint\narXiv:2307.09288, 2023. 2\n[56] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 1, 2, 4\n[57] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,\nXiang Wang, and Shiwei Zhang. Modelscope text-to-video\ntechnical report. arXiv preprint arXiv:2308.06571, 2023. 2,\n7, 8\n[58] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan\nHe, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling\nvideo masked autoencoders with dual masking, 2023. 6\n12\n[59] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou,\nZiqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu,\nPeiqing Yang, Yuwei Guo, Tianxing Wu, Chenyang Si, Yum-\ning Jiang, Cunjian Chen, Chen Change Loy, Bo Dai, Dahua\nLin, Yu Qiao, and Ziwei Liu. Lavie: High-quality video\ngeneration with cascaded latent diffusion models, 2023. 2\n[60] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jing-\nwen Hou Hou, Annan Wang, Wenxiu Sun Sun, Qiong Yan,\nand Weisi Lin. Exploring video quality assessment on user\ngenerated contents from aesthetic and technical perspectives.\nIn International Conference on Computer Vision (ICCV),\n2023. 4\n[61] Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu\nChen, and Jian Zhang. On the tool manipulation capability of\nopen-source large language models, 2023. 2\n[62] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan,\nYiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,\nYaya Shi, et al.\nmplug-owl: Modularization empowers\nlarge language models with multimodality. arXiv preprint\narXiv:2304.14178, 2023. 3\n[63] Sihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho\nKim, Jung-Woo Ha, and Jinwoo Shin. Generating videos with\ndynamics-aware implicit generative adversarial networks. In\nInternational Conference on Learning Representations, 2022.\n2\n[64] Jerrold H Zar. Spearman rank correlation. Encyclopedia of\nBiostatistics, 7, 2005. 10\n[65] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui\nZhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng\nShou. Show-1: Marrying pixel and latent diffusion models\nfor text-to-video generation, 2023. 2\n[66] Wenxuan Zhang, Xiaodong Cun, Xuan Wang, Yong Zhang,\nXi Shen, Yu Guo, Ying Shan, and Fei Wang. Sadtalker: Learn-\ning realistic 3d motion coefficients for stylized audio-driven\nsingle image talking face animation, 2022. 2\n[67] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei\nWang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\nZhang, Zican Dong, et al. A survey of large language models.\narXiv preprint arXiv:2303.18223, 2023. 3\n[68] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,\nDacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez,\nand Ion Stoica. Judging llm-as-a-judge with mt-bench and\nchatbot arena, 2023. 2\n[69] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 1, 2\n[70] Gu Zhouhong, Zhu Xiaoxuan, Ye Haoning, Zhang Lin, Wang\nJianchen, Jiang Sihang, Xiong Zhuozhi, Li Zihan, He Qianyu,\nXu Rui, Huang Wenhao, Zheng Weiguo, Feng Hongwei, and\nXiao Yanghua.\nXiezhi: An ever-updating benchmark for\nholistic domain knowledge evaluation. arXiv:2304.11679,\n2023. 2\n13\n"
  },
  {
    "title": "Context-Aware Meta-Learning",
    "link": "https://arxiv.org/pdf/2310.10971.pdf",
    "upvote": "13",
    "text": "Under Review\nCONTEXT-AWARE META-LEARNING\nChristopher Fifty1, Dennis Duan1,2, Ronald G. Junkins1,\nEhsan Amid3, Jure Leskovec1, Christopher R\u00e91, Sebastian Thrun1\n1Stanford University, 2Google, 3Google DeepMind\nfifty@cs.stanford.com\nABSTRACT\nLarge Language Models like ChatGPT demonstrate a remarkable capacity to learn\nnew concepts during inference without any fine-tuning. However, visual models\ntrained to detect new objects during inference have been unable to replicate this\nability, and instead either perform poorly or require meta-training and/or fine-\ntuning on similar objects. In this work, we propose a meta-learning algorithm that\nemulates Large Language Models by learning new visual concepts during inference\nwithout fine-tuning. Our approach leverages a frozen pre-trained feature extractor,\nand analogous to in-context learning, recasts meta-learning as sequence modeling\nover datapoints with known labels and a test datapoint with an unknown label. On\n8 out of 11 meta-learning benchmarks, our approach\u2014without meta-training or\nfine-tuning\u2014exceeds or matches the state-of-the-art algorithm, P>M>F, which is\nmeta-trained on these benchmarks.\n1\nINTRODUCTION\nMeta-learning algorithms for image classification aim to classify a set of unlabeled images from only\nseveral labeled examples. The labeled examples are termed the support set and the set of unknown\nimages is called the query set. In an n-way-k-shot meta-learning paradigm, the support set spans n\ndifferent classes, each class contains k labeled images, and the meta-learning algorithm predicts the\nclass of each unlabled image in the query set from the n classes in the support set.\nNearly all meta-learning algorithms ascribe to a common pattern of pre-training, meta-training,\nand/or fine-tuning (Hu et al., 2022). Pre-training initializes the meta-learner\u2019s feature extractor with a\npre-trained vision model; meta-training trains the model\u2019s parameters to learn how to classify new\nvisual concepts during inference by training the model on a series of n-way, k-shot classification\ntasks; and fine-tuning updates the model\u2019s parameters on the support set at inference.\nWhile meta-training excels in learning new classes during inference that are similar to those seen\nduring meta-training, it often fails to generalize to new classification paradigms. For example,\nmodels meta-trained on coarse-grained object detection often fail to generalize to fine-grained\nimage classification. Fine-tuning on the support set during inference can rescue an otherwise poor\nperforming model; however, training a model during inference is often impractical and prohibitive\nof many real-time applications. In this regard, visual meta-learning algorithms lag behind recent\nadvancements in natural language where Large Language Models (LLMs) exhibit a remarkable\ncapacity to learn new concepts during inference without fine-tuning (Brown et al., 2020).\nIn this work, we develop a meta-learning algorithm that emulates LLMs by learning new visual\nconcepts during inference without fine-tuning. Drawing inspiration from in-context learning in LLMs,\nwe reformulate n-way-k-shot image classification as sequence modeling over the support set and an\nunknown query image. This is, to the best of our knowledge, the first approach that enables in-context\nlearning in the visual domain.\nAn overview of our approach is visualized in Figure 1. It first encodes support and query images with\na frozen CLIP (Radford et al., 2021) model and represents support set labels with an Equal Length\nand Maximally Equiangular Set (ELMES) encoding. It then concatenates image embeddings with\nlabel embeddings to compose a joint image-label embedding and collates each embedded vector from\nthe query and support set into a sequence. We then pre-train a sequence model\u2014in our experiments a\nTransformer encoder\u2014across several large datasets with an explicit in-context learning objective:\n1\narXiv:2310.10971v1  [cs.LG]  17 Oct 2023\nUnder Review\nclassify the query given an input sequence composed of the support set and query point. This learning\nparadigm trains the Transformer encoder to extrapolate to new classes in the parameter-space of\nthe model, enabling our approach to learn new visual concepts during inference without fine-tuning.\nDue to its capacity to learn visual information \u201cin-context\u201d, we term our approach Context-Aware\nMeta-Learning (CAML).\nOur primary contribution is to develop a meta-learning algorithm for universal meta-learning: the\ncapacity to learn any new visual concept during inference without fine-tuning or meta-training on\nrelated images. This challenging setting emulates the deployment of LLMs to real-time applications,\nand strong performance in this setting would unlock new applications of visual meta-learning. Our\ntheoretical analysis shows that an ELMES is the encoding that minimizes the entropy of detecting\nclasses within the support set, and therefore, does not need to be learned. Our empirical analysis\nhighlights the importance of reformulating meta-learning as sequence modeling: considering the\nsupport set and query together enables the model to attend to specific visual features of images in the\nsupport set to classify the query. Finally, our empirical analysis indicates CAML is a state-of-the-art\nmeta-learning algorithm. On a diverse set of 8 out of 11 meta-learning benchmarks\u2014and without\nmeta-training or fine-tuning\u2014CAML outperforms or matches the performance of P>M>F (Hu et al.,\n2022), a state-of-the-art meta-learning algorithm that is meta-trained on each benchmark.\n2\nRELATED WORK\nMeta-Learning Algorithms. Many recent meta-learning algorithms can be categorized as either\ngradient-based or metric-based (Bouniot et al., 2022); however, CAML belongs to a new category\nof context-based methods that casts meta-learning as learning over a sequence composed of the\nquery and support set to predict the label of the query (Fifty et al., 2023). Dissimilar from other\ncontext-based methods, CAML avoids meta-training and can generalize to out-of-distribution classes\nduring inference.\nGradient-based algorithms learn representations during meta-training that can be quickly adapted to\nnew data distributions by fine-tuning on the support set (Chen et al., 2020b; Finn et al., 2017; Nichol\n& Schulman, 2018; Bertinetto et al., 2018a; Lee et al., 2019a; Nichol et al., 2018; Raghu et al., 2019).\nDissimilar from context-based algorithms, they require fine-tuning during inference, and this property\nmakes them unsuitable for real-time applications. Metric-based algorithms learn a distance-sensitive\nembedding space during meta-training, and during inference, the query is classified by its distance to\npoints in the support set (Allen et al., 2019; Li et al., 2019a; Simon et al., 2020; Snell et al., 2017a;\nSung et al., 2018). While metric-based algorithms do not require fine-tuning during inference, they\noften exhibit poor performance when encountering classes during inference that are significantly\ndifferent from those seen during meta-training (Hu et al., 2022).\nCross-Domain Meta-Learning. Cross-domain meta-learning refers to a challenging evaluation\nparadigm where the meta-training and inference-time data distributions are significantly differ-\nent (Chen et al., 2019). Recent work finds that leveraging self-supervised pre-training\u2014or foun-\ndational model feature extractors\u2014can significantly improve cross-domain performance (Hu et al.,\n2022; Zhang et al., 2021). Moreover, fine-tuning with respect to the support set almost always\noutperforms meta-learning without fine-tuning in this setting (Guo et al., 2020; Oh et al., 2022; Phoo\n& Hariharan, 2020; Islam et al., 2021). While effective, fine-tuning is prohibitive to deploying visual\nmeta-learning models in a manner similar to LLMs like ChatGPT as the latency and memory cost to\nfine-tune a model\u2019s parameters on each user query is untenable. Moreover, fine-tuning is sensitive to\nthe choice of learning rate, making these models difficult to deploy in practice (Hu et al., 2022).\n3\nAPPROACH\nWe adapt the ideas underpinning in-context learning in LLMs\u2014namely learning to classify a query\nfrom a context of support set demonstrations in a single forward pass\u2014to image classification. A\nsimilar concept has recently been explored by Fifty et al. (2023) for few-shot molecular property\nprediction. Dissimilar from this work, we avoid meta-training and instead focus on universal image\nclassification: learning to detect new visual classes during inference without meta-training on related\nclasses or fine-tuning on the support set.\n2\nUnder Review\nELMES Class Encoder\nTransformer Encoder\nMLP\nClass\nP(Class)\nCLIP Image Encoder\nConcatenate \nEmbeddings\nSupport Set Images\nQuery Image\n*\n Unknown \n[class] \nEmbedding\n[1, 0, 0]\n[0, 1, 0]\n[0, 0, 1]\n[1, 0, 0]\n[0, 1, 0]\n[0, 0, 1]\n0.03\n0.06\n0.91\n*\nTransformer Encoder\n*\nSupport Set Classes\nParameters Frozen\nParameters Trained\nFigure 1: Overview of CAML. Query and support set images are encoded with a CLIP feature extractor\nand then concatenated with their corresponding ELMES label embeddings. We feed the resulting sequence\nof concatenated vectors into a Transformer encoder and extract the transformed query vector from the output\nsequence to predict its class.\nArchitecture. CAML consists of three components: (1) a frozen CLIP image encoder, (2) a fixed\nELMES class encoder, and (3) a Transformer encoder sequence model. CAML first encodes query\nand support set images using a frozen CLIP feature extractor. Crucially, the CLIP embedding space\ndistills images into low-dimensional representations so that images with similar visual characteristics\nand semantic meanings have similar embeddings. We encode the classes of the support set with an\nELMES class encoder. In Section 4, we prove that an ELMES encoding of mutually exclusive labels\nallows the Transformer encoder sequence model to maximally identify classes within the support set.\nAs the class of the query is unknown, it uses a special learnable \u201cunknown token\u201d embedding.\nThe core idea underpinning CAML is to cast meta-learning as sequence modeling over the support set\nand query points. We instantiate the sequence model as a Transformer encoder, and during large-scale\npre-training, train the model to predict the class of the query from an input sequence composed of\nthe support set and query embedded vectors. Specifically, the input to the Transformer encoder is a\nsequence of support set and query vectors embedded in the joint image-label embedding space. From\nthe output sequence of the Transformer encoder, we select the element at the same position as the\nquery in the input sequence, and pass this vector through a shallow MLP to predict the label of the\nquery. A visual depiction of CAML is shown in Figure 1.\nLarge-Scale Pre-Training. As our focus is universal meta-learning\u2014and CAML may encounter\nany new visual concept during inference\u2014we pre-train CAML\u2019s Transformer encoder on few-shot\nimage classification tasks from ImageNet-1k (Deng et al., 2009), Fungi (Schroeder & Cui, 2018),\nMSCOCO (Lin et al., 2014), and WikiArt (Saleh & Elgammal, 2015). We chose these datasets because\nthey span generic object recognition (ImageNet-1k, MSCOCO), fine-grained image classification\n(Fungi), and unnatural image classification (WikiArt). To avoid distorting the CLIP embedding space,\nwe freeze the CLIP feature extractor and only update the Transformer encoder during pretraining.\nSimilarly, since an ELMES minimizes the entropy of detecting classes within the support set, the\nlabel encoder is also frozen. In the context of pre-training, meta-training, and fine-tuning, CAML\nonly requires pre-training and avoids meta-training on the train/validation splits of meta-learning\nbenchmarks or fine-tuning on the support set during inference.\n4\nTHEORETICAL ANALYSIS\nIn this section, we explore the symmetries inherent in CAML. These symmetries allow us to formulate\nthe problem of learning support set class representations as an entropy minimization problem with a\nclosed-form solution. We prove that this solution is an ELMES. Later, we show it maintains permu-\ntation invariance, a vital property of meta-learning algorithms that conveys consistent predictions\nirrespective of the ordering of elements within the sequence. Due to space constraints, all proofs and\nmany definitions, properties, lemmas, and theorems are allocated to the Appendix.\n3\nUnder Review\n4.1\nEQUAL LENGTH AND MAXIMALLY EQUIANGULAR SET OF VECTORS\nDefinition 1. An Equal Length and Maximally Equiangular Set (ELMES) is a set of non-zero\nvectors {\u03d5j}d\nj=1, \u03d5j \u2208 Rd+k for some k \u2265 0 and d > 1, such that \u2200j \u0338= j\u2032, \u2225\u03d5j\u2225 = \u2225\u03d5j\u2032\u2225 and\n\u27e8\u03d5j , \u03d5j\u2032\u27e9 =\n\u22121\nd\u22121. Simply, all vectors in this set are equal length and maximally equiangular.\nAn Equal Angle and Maximally Equiangular Set (ELMES) of vectors has connections to both\nEquiangular Tight Frames in representation theory (Welch, 1974; Fickus et al., 2018) as well as the\nSimplex Equiangular Tight Frames highlighted in recent neural collapse works exploring softmax-\nlayer geometry at the terminal phase of training (Papyan et al., 2020; Yang et al., 2022). We offer\nadditional discussion comparing these structures in the Appendix as well as provide an intuitive view\nof an ELMES as a regular d-simplex immersed in Rd+k.\n4.2\nLABEL SYMMETRY\nSymmetry in the assignment of support classes to numeric labels is an important property of meta-\nlearning algorithms. For example, if we have the support set classes {tower, bear, tree}, the mapping\nof {bear -> 1, tower -> 2, tree -> 3 } should produce the same prediction for a query point as a\ndifferent mapping {bear -> 2, tower -> 3, tree -> 1}. To explore this symmetry, we examine how\nclass embeddings are being used by the model.\nFrom our formulation in Section 3, we represent a demonstration vector as a concatenation of an\nimage embedding \u03c1 and a label embedding \u03d5: [\u03c1\n\u03d5]. This vector is directly fed into the self-attention\nmechanism, where we matrix multiply with key, query, and value self-attention heads. Taking only\none of these matrices for simplicity with head-dimension k:\n[\u03c1\n\u03d5]\n\u0014\n\u03931\n...\n\u0393k\n\u03c81\n...\n\u03c8k\n\u0015\n= [\u27e8\u03c1 , \u03931\u27e9\n...\n\u27e8\u03c1 , \u0393k\u27e9] + [\u27e8\u03d5 , \u03c81\u27e9\n...\n\u27e8\u03d5 , \u03c8k\u27e9]\n(1)\nThe output of this transformation will be the sum of two vectors: one composed of the inner products\nbetween the image embedding and the learnable {\u0393i}k\ni=1s and the other composed of the class\nembedding and the learnable {\u03c8i}k\ni=1.\nWe postulate a capacity to distinguish among the classes of demonstration vectors is necessary for\nthe model to predict the class of the query vector. Conversely, if a meta-learning algorithm predicts\namong d classes, and all classes maintain the same embedding \u03d5j = \u03d5i \u2200i \u2208 {1, ..., d}, the model\nwill be unable to identify the class of the query vector as all demonstration vectors appear to have the\nsame class identity. Such an embedding would maximize the Shannon entropy for any learnable \u03c8i\nHi(X) := \u2212\nX\nx\u2208X\npi(x) ln(pi(x))\nwhere we define X = {1, 2, ..., d} to be the different classes, X to be a random variable which takes\non values in X, and pi(X = j) =\ne\u27e8\u03c8i , \u03d5j \u27e9\nP\n\u2113\u2208X e\u27e8\u03c8i , \u03d5\u2113\u27e9 as the softmax probability of class j given that \u03c8i\nis learned to detect class i (i.e. maximize pi(X = i) and minimize Hi(X)).\nContrary to the above example, we assume a capacity to learn a \u03c8i that maximally detects a given class\nj will be beneficial to minimizing the loss for meta-learning paradigms. As we use the softmax of the\ninner product to determine class probabilities, maximizing pi(X = j) is equivalent to minimizing\npi(X = \u2113) for all \u2113 \u0338= j.\nBy symmetry in the assignment of class embeddings to support classes, we can assume that the\nnumber of \u03c8i learned to detect class i is similar to the number of \u03c8j learned to detect class j for all\npairs (i, j). Then pi(X = i) for all 1 \u2264 i \u2264 d is jointly maximized \u21d0\u21d2 the d-class embeddings\n{\u03d5j}d\nj=1 is an ELMES . Before we prove this result, we leverage symmetry in the assignment of\nlabels to classes to make the following assumptions (justifications for each assumption located in the\nAppendix):\nAssumption 1. Suppose {\u03c8i}k\ni=1 are learnable class detectors of unit norm with at least one \u03c8i\ndetecting each class 1 \u2264 i \u2264 d. The probability pj(X = j) = pi(X = i) for 1 \u2264 i, j \u2264 d.\nAssumption 2. Define pi(X = i)\\{\u03d5l}d\nl=(m+1) as the probability of \u03c8i detecting \u03d5i from the set of\nvectors {\u03d5j}m\nj=1, m < d. Then the probability pj(X = j)\\{\u03d5l}d\nl=(m+1) = pi(X = i)\\{\u03d5l}d\nl=(m+1)\nfor 1 \u2264 i, j \u2264 m and m \u2265 2.\n4\nUnder Review\nAssumption 3. When \u03c8i =\n\u03d5i\n\u2225\u03d5i\u2225, pi(X = i) is maximized.\nWhen Assumption 1, Assumption 2, and Assumption 3 hold, the set of class embeddings that\nmaximize the probability of a learnable \u03c8i detecting class i is necessarily an ELMES.\nTheorem 1. The set of class embeddings {\u03d5j}d\nj=1 \u2200j, 1 \u2264 j \u2264 d that maximizes pj(X = j) is\nnecessarily an ELMES.\nAlternatively when viewed through the lens of information theory, we can reinterpret an ELMES as\nthe class embedding that minimizes the entropy of \u03c8i detecting class i. Informally, ELMES causes\n\u03c8i to have the least uncertainty when detecting class i.\nLemma 1. Let Hi be the entropy of pi(X). An ELMES minimizes Hi.\n4.3\nPERMUTATION INVARIANCE.\nIn addition to label symmetry, it is also desirable for the output prediction of CAML to not depend on\nthe order of demonstrations in the sequence. As Fifty et al. (2023) show that a two-class, non-ELMES\nversion of CAML is invariant to permutations in the input sequence, it suffices to show that the\nELMES label encoder is equivariant to permutations in the input sequence.\nLemma 2. Consider a n-sequence of one-hot labels stacked into a matrix S \u2208 Rn\u00d7w, and an ELMES\nlabel encoder denoted by W \u2208 Rw\u00d7d with w denoting \u201cway\u201d and d the dimension of the label\nembedding. The label embedding SW is equivariant to permutations.\n5\nEXPERIMENTS\nTo quantify universal image classification performance, we evaluate a diverse set of 11 meta-learning\nbenchmarks divided across 4 different categories:\n1. Generic Object Recognition: mini-ImageNet (Vinyals et al., 2016), tiered-ImageNet (Ren et al.,\n2018), CIFAR-fs (Bertinetto et al., 2018b), and Pascal VOC (Everingham et al.)\n2. Fine-Grained Image Classification: CUB (Wah et al., 2011), Aircraft (Maji et al., 2013), meta-\niNat (Wertheimer & Hariharan, 2019), and tiered meta-iNat (Wertheimer & Hariharan, 2019)\n3. Unnatural Image Classification: ChestX (Guo et al., 2020) and Paintings (Crowley & Zisserman,\n2015)\n4. Inter-Domain Image Classification: Pascal+Paintings (Everingham et al.; Crowley & Zisserman,\n2015).\nGeneric object recognition, fine-grained image classification, and unnatural image classification\nare standard benchmarking tasks in meta-learning literature (Chen et al., 2020a; Hu et al., 2022;\nWertheimer et al., 2020; Guo et al., 2020). Beyond this, we compose a challenging new inter-domain\ncategory by combining Pascal VOC with Paintings so that each class is composed of both natural\nimages and paintings. This allows us to evaluate the ability of meta-learning algorithms to generalize\nacross domains within the same class. For example, the support image for the class \u201ctower\u201d may be\nVan Gogh\u2019s The Starry Night, while the query may be a picture of the Eiffel Tower. Humans have\nthe ability to generalize visual concepts between such domains; however, meta-learning algorithms\nstruggle with this formulation (Jankowski & Gr \u02dbabczewski, 2011).\n5.1\nBASELINES\nWe evaluate the capacity of CAML, Prototypical Networks (Snell et al., 2017b), MetaOpt (Lee et al.,\n2019b), and MetaQDA (Zhang et al., 2021) in a universal meta-learning setting by pre-training them\nwith the same CLIP feature extractor over the same image corpus as CAML and similarly freezing\ntheir weights at inference time. We select Prototypical Networks and MetaOpt as baselines as they\nare well-studied and have been shown to perform well with a pre-trained foundational model feature\nextractor (Hu et al., 2022). Similarly, MetaQDA benefits from pre-trained feature extractors and was\ndesigned for cross-domain meta-learning (Zhang et al., 2021; Hu et al., 2022).\nTo assess the performance gap between universal meta-learning and the typical meta-training ap-\nproach, we also benchmark the performance of the current state-of-the-art meta-learning algorithm,\n5\nUnder Review\nTable 1: MiniImageNet & CIFAR-fs mean accuracy and standard error comparison with representa-\ntive state-of-the-art meta-learning algorithms. Methods using external data during pre-training are\nindicated. \u2020 indicates the ViT-base CLIP backbone was frozen during pre-training.\nMethod (Backbone)\nExtra\nData\nCIFAR-fs\nMiniImageNet\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nInductive\nProtoNet (CNN-4-64) (Snell et al., 2017b)\n49.4\n68.2\n55.5\n72.0\nBaseline++ (CN-4-64) (Chen et al., 2020a)\n61.4\n77.9\nMetaOpt (ResNet12) (Lee et al., 2019b)\n72.0\n84.3\n61.4\n77.9\nMeta-Baseline (ResNet12) (Chen et al., 2021b)\n68.6\n83.7\nMetaQDA (WRN-28-10) (Zhang et al., 2021)\n74.3\n89.6\n67.8\n84.3\nRankDNN (ResNet12)(Guo et al., 2022)\n78.9\n90.6\n66.7\n84.8\nPatchProto+TSF (WRN-28) (Lai et al., 2022)\n70.2\n84.6\nP>M>F (ViT-base) Hu et al. (2022)\n84.3\n92.2\n95.3\n98.4\nTransductive\nFine-tuning (WRN-28-10) (Dhillon et al., 2020)\n76.6\n85.8\n65.7\n78.4\nSIB (WRN-28-10) (Hu et al., 2020)\n80.0\n85.3\n70.0\n79.2\nPT-MAP (WRN-28-10) (Hu et al., 2021)\n87.7\n90.7\n82.9\n88.8\nCNAPS (ResNet18) (Bateni et al., 2021)\n79.9\n88.1\nSelf-supervised\nProtoNet (WRN-28-10) (Gidaris et al., 2019)\n73.6\n86.1\n62.9\n79.9\nProtoNet (AMDIM ResNet) (Chen et al., 2021a)\n76.8\n91.0\nEPNet + SSL (WRN-28-10) (Rodr\u00edguez et al., 2020)\n79.2\n88.1\nSemi-supervised\nLST (ResNet12) (Li et al., 2019b)\n70.1\n78.7\nPLCM (ResNet12) (Huang et al., 2021)\n77.6\n86.1\n70.1\n83.7\nUniversal Meta-Learning;\nNo Meta-Training or Finetuning\nProtoNet (ViT-base) (Snell et al., 2017b)\n62.9\u00b1.2\n79.7\u00b1.2\n92.1\u00b1.1\n97.1\u00b1.0\nProtoNet (ViT-base)\u2020\n57.7\u00b1.2\n81.0\u00b1.2\n85.3\u00b1.2\n96.0\u00b1.1\nMetaOpt (ViT-base) (Lee et al., 2019b)\n53.1\u00b1.3\n73.1\u00b1.2\n78.5\u00b1.2\n91.6\u00b1.1\nMetaOpt (ViT-base)\u2020\n61.7\u00b1.2\n83.1\u00b1.1\n86.9\u00b1.2\n96.5\u00b1.1\nMetaQDA (ViT-base) (Zhang et al., 2021)\n60.4\u00b1.2\n83.2\u00b1.1\n88.2\u00b1.2\n97.4\u00b1.0\nCAML (ViT-base)\n70.8\u00b1.2\n85.5\u00b1.1\n96.2\u00b1.1\n98.6\u00b1.0\nTable 2: Pascal & Paintings mean accuracy and standard error across 10,000 test epochs. \u2020 indicates\nthe ViT-base CLIP backbone was frozen during pre-training.\nMethod (Backbone)\nPascal + Paintings\nPaintings\nPascal\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F (ViT-base)\n60.7\n74.4\n53.2\n65.8\n72.2\n84.4\nUniversal Meta-Learning\nProtoNet (ViT-base)\n49.6\u00b1.2\n63.5\u00b1.1\n38.3\u00b1.2\n48.2\u00b1.1\n77.9\u00b1.2\n87.3\u00b1.2\nProtoNet (ViT-base)\u2020\n52.2\u00b1.2\n70.6\u00b1.1\n48.3\u00b1.2\n64.1\u00b1.1\n72.2\u00b1.2\n84.3\u00b1.2\nMetaOpt (ViT-base)\n38.2\u00b1.2\n58.2\u00b1.1\n31.6\u00b1.2\n48.0\u00b1.1\n63.7\u00b1.2\n81.7\u00b1.2\nMetaOpt (ViT-base)\u2020\n53.2\u00b1.2\n74.8\u00b1.1\n49.3\u00b1.2\n65.9\u00b1.1\n72.8\u00b1.2\n84.4\u00b1.2\nMetaQDA (ViT-base)\n53.8\u00b1.2\n74.1\u00b1.1\n49.4\u00b1.2\n66.6\u00b1.1\n73.5\u00b1.2\n85.2\u00b1.2\nCAML (ViT-base)\n63.8\u00b1.2\n78.3\u00b1.1\n51.1\u00b1.2\n65.2\u00b1.1\n82.6\u00b1.2\n89.7\u00b1.1\nP>M>F (Hu et al., 2022), which is meta-trained on each dataset. While previous cross-domain\napproaches often involve fine-tuning on the support set at inference time, we forgo this step as\nfine-tuning is incompatible with universal meta-learning and developing real-time meta-learning\napplications.\nWhen pre-training all models in the universal meta-learning setting, we set the learning rate to a fixed\n1 \u00d7 10\u22125 and do not perform any hyperparameter tuning in order to match the practices used by\nP>M>F. We use early stopping with a window size of 10 epochs during pre-training and the code\n6\nUnder Review\nTable 3: meta-iNat & tiered meta-iNat & ChestX mean accuracy and standard error across 10,000\ntest epochs. \u2020 indicates the ViT-base CLIP backbone was frozen during pre-training.\nMethod (Backbone)\nmeta-iNat\ntiered meta-iNat\nChestX\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F (ViT-base)\n91.2\n96.1\n74.8\n89.9\n27.0\n32.1\nUniversal Meta-Learning\nProtoNet (ViT-base)\n78.4\u00b1.2\n89.4\u00b1.1\n66.3\u00b1.2\n82.2\u00b1.2\n22.4\u00b1.1\n25.3\u00b1.1\nProtoNet (ViT-base)\u2020\n84.5\u00b1.2\n94.8\u00b1.1\n73.8\u00b1.2\n89.5\u00b1.1\n22.7\u00b1.1\n25.8\u00b1.1\nMetaOpt (ViT-base)\n53.0\u00b1.2\n77.7\u00b1.2\n37.3\u00b1.2\n63.0\u00b1.2\n20.8\u00b1.1\n23.0\u00b1.1\nMetaOpt (ViT-base)\u2020\n85.5\u00b1.2\n95.5\u00b1.1\n75.1\u00b1.2\n91.9\u00b1.1\n23.0\u00b1.1\n27.4\u00b1.1\nMetaQDA (ViT-base)\n86.3\u00b1.2\n95.9\u00b1.1\n76.0\u00b1.2\n92.4\u00b1.1\n22.6\u00b1.1\n27.0\u00b1.1\nCAML (ViT-base)\n91.2\u00b1.2\n96.3\u00b1.1\n81.9\u00b1.2\n91.6\u00b1.1\n21.5\u00b1.1\n22.2\u00b1.1\nTable 4: CUB & tiered-ImageNet & Aircraft mean accuracy and standard error across 10,000 test\nepochs. \u2020 indicates the ViT-base CLIP backbone was frozen during pre-training.\nMethod (Backbone)\nCUB\ntiered-ImageNet\nAircraft\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nIn-Domain [Meta-Training]\nP>M>F (ViT-base)\n92.3\n97.0\n93.5\n97.3\n79.8\n89.3\nUniversal Meta-Learning\nProtoNet (ViT-base)\n59.4\u00b1.2\n77.3\u00b1.2\n93.5\u00b1.1\n97.4\u00b1.1\n37.9\u00b1.2\n52.5\u00b1.2\nProtoNet (ViT-base)\u2020\n87.0\u00b1.2\n97.1\u00b1.1\n87.3\u00b1.2\n96.1\u00b1.1\n62.4\u00b1.3\n82.0\u00b1.2\nMetaOpt (ViT-base)\n71.5\u00b1.2\n41.2\u00b1.2\n76.6\u00b1.2\n89.6\u00b1.1\n41.6\u00b1.2\n26.7\u00b1.1\nMetaOpt (ViT-base)\u2020\n87.9\u00b1.2\n97.2\u00b1.1\n88.2\u00b1.2\n96.5\u00b1.1\n64.8\u00b1.2\n82.6\u00b1.2\nMetaQDA (ViT-base)\n88.3\u00b1.2\n97.4\u00b1.1\n89.4\u00b1.2\n97.0\u00b1.1\n63.6\u00b1.3\n83.0\u00b1.2\nCAML (ViT-base)\n91.8\u00b1.2\n97.1\u00b1.1\n95.4\u00b1.1\n98.1\u00b1.1\n63.3\u00b1.3\n79.1\u00b1.2\nrelease of Hu et al. (2022) to benchmark P>M>F with the training settings and hyperparameters\ndescribed in their work.\n5.2\nRESULTS\nOur findings are summarized in Table 1, Table 2, Table 3, and Table 4 and indicate that CAML sets a\nnew state-of-the-art for universal meta-learning by significantly outperforming other baselines on\n15 of 22 evaluation settings. For 5 of the other 7 evaluation settings, CAML matches\u2014or nearly\nmatches\u2014the best performing baseline. Remarkably, CAML also performs competitively with\nP>M>F on 8 out of 11 meta-learning benchmarks, even though P>M>F meta-trains on the training\nset of each benchmark. This result suggests that the amount of new visual information learned\nduring inference when using only foundational models and novel meta-learning techniques without\nfine-tuning is comparable to the amount learned when directly meta-training on in-domain data. This\ncapacity may unlock new applications in the visual space, just as the emergence of in-context learning\nin LLMs has enabled many new applications in natural language.\nThe 3 datasets where P>M>F outperforms CAML are CIFAR-fs, Aircraft, and ChestX. CIFAR-fs is a\ngeneric object recognition benchmark containing CIFAR images downsampled to 32x32 resolution.\nAs CAML and CLIP pre-train on 224x224 resolution images, downsampling by a factor of 49 likely\ninduces a distribution shift that was not learned by CAML during large-scale pre-training. In the\ncases of Aircraft and ChestX, we postulate that the CLIP embedding space\u2014structured so images\nwith similar captions have similar embeddings\u2013struggles to effectively differentiate between the\nfine-grained, specialized classes in these tasks. For example, while a Boeing 737 and Airbus A380\nhave different labels in the Aircraft dataset, the scraped CLIP captions for those images may not\nreach that level of granularity. This corroborates the findings from Radford et al. (2021), which found\nthat in a zero-shot setting, CLIP underperforms in specialized or complex tasks such as medical\nimaging or aircraft classification. We anticipate the continued development of visual (or multi-modal)\n7\nUnder Review\nTower\nChicken\nSupport\nCow\nFlower\nHuman\nTower\nQuery\n(a) Left: An example task, classifying images by the objects depicted. Center: CLIP image embeddings on this\ntask\u2019s images. Right: joint image+label representations after the last CAML attention layer for the same task.\nEmbroidery\n3D Model\nSupport\nPencil Sketch\nOil Painting\nStained Glass\nEmbroidery\nQuery\n(b) Left: An example task, classifying images by the artistic medium used. Center: CLIP image embeddings on\nthis task\u2019s images. Right: joint representations after the last CAML attention layer for the same task.\nFigure 2: Two sample tasks over the same support images but utilizing different criteria to define classes. The\nnature of the query image informs the task being presented, e.g. classification by object (top) vs. classification\nby texture (bottom). For both, the final-layer attention outputs provide better separation between class represen-\ntations and groups the query representation with the proper task, even when projected into 2D space by PCA.\nfoundational models will improve their sensitivity to fine-grained visual concepts and significantly\nincrease the performance of CAML on these benchmarks.\nFinally, our empirical findings indicate that large-scale pre-training often decreases the performance\nof Prototypical Networks and MetaOpt. We observe that these methods tend to overfit during pre-\ntraining, and our empirical results show a similar pattern: pre-training with these methods often helps\nthe performance of benchmarks similar to ImageNet (i.e. Pascal, MiniImageNet, tiered-ImageNet),\nbut it significantly hurts the performance of out-of-domain tasks (i.e. Aircraft, CUB, Paintings)\nas shown in Tables 1 to 4. We believe large-scale pre-training of the CLIP backbone distorts\nthe structure of the CLIP embedding space, leading to catastrophic forgetting on out-of-domain\ntasks. Conversely, CAML and MetaQDA\u2014both of which freeze the parameters of the CLIP feature\nextractor\u2014benefit greatly from large-scale pre-training. With the increasing scale and sophistication\nof visual feature extractors, fine-tuning\u2014or even loading these models into memory\u2014becomes less\nfeasible. Algorithms that do not fine-tune this backbone like CAML and MetaQDA will benefit from\nthis research and likely become even more effective.\n5.3\nWEAKNESSES OF CAML\nDespite its strong empirical performance, CAML presents several weaknesses. First, the maximum\nnumber of classes present in the support set at any point during inference must be known at pre-\ntraining to instantiate a d-way ELMES. Further, at least one dataset during pre-training must use a\n8\nUnder Review\nd-way classification setting so the \u03c8i class detectors referenced in Section 4 are trained within the\nTransformer encoder\u2019s attention layers. Another weakness relates to reliance on the frozen image\nencoder. As CAML does not update these parameters at all, it can underperform on tasks that are\ndifficult to distinguish or highly spread out in the CLIP embedding space, such as low-resolution\nimages or specialized fine-grained classes.\n6\nANALYSIS\nTo better understand how CAML learns, we conduct empirical analyses on (1) its ability to dy-\nnamically update its representations at inference time, and (2) the effect of the fixed ELMES class\nembedding.\nContext-Aware Representations. Dissimilar from other meta-learning algorithms and due to\nrecasting meta-learning as sequence modeling, CAML considers the full context of a query and\nsupport set to predict the label of the query. Specifically, the query dynamically influences the\nrepresentation of support set points, and the support set points dynamically influence the representation\nof the query as this sequence is passed through the layers of a Transformer encoder. This property\nenables universal meta-learning by allowing the model to update the support and query representations\nbased on the context of the task, not only the contents of the images.\nAn example where the query dynamically influences the support set is visualized in Figure 2. Given\nonly the 5 support examples, the prediction task is ambiguous. However, the nature of the query\ndetermines the prediction task. The query image of a tower in Figure 2a reduces the task to generic\nobject recognition: classify the query based on the object portrayed in the image. On the other hand,\nand as visualized in Figure 2b, the query image of embroidery reduces the prediction task to texture\nidentification: classify the query based on artistic medium.\nTo analyze how dynamic representations affect CAML, we examine the representations of the support\nset and query vectors at the input to and output of the Transformer encoder. For both examples\nvisualized in Figure 2a and Figure 2b, the Transformer encoder learns to separate support set vectors\nby class identity, and moreover, group the query representation with the correct support set example.\nWe find the frozen CLIP image embeddings are actually antagonistic for the classification-by-texture\ntask visualized in Figure 2b: the query image embedding is closest to the support set example for the\nsecond class, \u201coil painting\u201d. Moreover, we find that our baseline methods that rely on frozen CLIP\nimage embeddings like MetaQDA, ProtoNet\u2020, and MetaOpt\u2020 group the query with \u201coil painting\u201d and\ntherefore misclassify this example. On the other hand, as CAML considers the full context of the\nquery and support set, it develops representations of the query in the context of the support set\u2014and\nthe support set in the context of the query\u2014to group the query with the \u201cembroidery\u201d support set\nimage as they share the same texture, thereby correctly classifying this example.\nELMES Ablation. To supplement our theoretical analysis in Section 4, we train a version of CAML\nwith learnable class embedding vectors in place of the fixed ELMES encoder. Given our analysis\nin Section 4, it is perhaps unsurprising we find that\u2014without any constraints or limitations\u2014the\nclass embeddings converge to an ELMES. The average pair-wise angle between embedding vectors\nis 1.77 \u00b1 0.02 radians whereas the expected pairwise angle from an ELMES is 1.82. Similarly, the\naverage norm of the learnable class embeddings converges to 1.34 \u00b1 0.02 whereas the learned norm\nof the ELMES model is 1.32.\nAn evaluation comparing CAML with learnable class embeddings to the approach with a fixed\nELMES encoder is presented in Table 5, Table 6, Table 7, and Table 8 of the Appendix. In summary,\nthe performance is approximately the same on each benchmark with the exception of Aircraft. In this\ncase, the learnable embedding model significantly outperforms the ELMES model, and moreover,\nsurpasses all other universal meta-learning baselines on the 5-way-1-shot split with an accuracy of\n66.3 \u00b1 .2. Nevertheless, given the similarity between both approaches on the remaining 10 datasets,\nand the learnable class embeddings actually forming an ELMES, we attribute the difference in\nAircraft performance to stochasticity in training the model, suggesting that the fixed ELMES encoder\nis indeed optimal.\n9\nUnder Review\n7\nCONCLUSION\nIn this work, we develop CAML, a meta-learning algorithm that emulates in-context learning in\nLLMs by learning new visual concepts during inference without fine-tuning. Our empirical findings\nshow that CAML\u2014without meta-training or fine-tuning\u2014exceeds or matches the performance of\nthe current state-of-the-art meta-learning algorithm on 8 out of 11 benchmarks. This result indicates\nvisual meta-learning models are ready for deployment in a manner similar to LLMs, and we hope\nthis work recalibrates our sense of limitations for the universal meta-learning paradigm.\nREFERENCES\nKelsey Allen, Evan Shelhamer, Hanul Shin, and Joshua Tenenbaum. Infinite mixture prototypes for\nfew-shot learning. In International conference on machine learning, pp. 232\u2013241. PMLR, 2019.\nPeyman Bateni, Jarred Barber, Jan-Willem van de Meent, and Frank Wood. Enhancing few-shot\nimage classification with unlabelled examples, 2021.\nLuca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differen-\ntiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018a.\nLuca Bertinetto, Joao F Henriques, Philip HS Torr, and Andrea Vedaldi. Meta-learning with differen-\ntiable closed-form solvers. arXiv preprint arXiv:1805.08136, 2018b.\nQuentin Bouniot, Ievgen Redko, Romaric Audigier, Ang\u00e9lique Loesch, and Amaury Habrard. Improv-\ning few-shot learning through multi-task representation learning theory. In European Conference\non Computer Vision, pp. 435\u2013452. Springer, 2022.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nDa Chen, Yuefeng Chen, Yuhong Li, Feng Mao, Yuan He, and Hui Xue. Self-supervised learning for\nfew-shot image classification, 2021a.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look\nat few-shot classification. arXiv preprint arXiv:1904.04232, 2019.\nWei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Frank Wang, and Jia-Bin Huang. A closer look\nat few-shot classification, 2020a.\nXiaohan Chen, Zhangyang Wang, Siyu Tang, and Krikamol Muandet. Mate: plugging in model\nawareness to task embedding for meta learning. Advances in neural information processing systems,\n33:11865\u201311877, 2020b.\nYinbo Chen, Zhuang Liu, Huijuan Xu, Trevor Darrell, and Xiaolong Wang. Meta-baseline: Exploring\nsimple meta-learning for few-shot learning, 2021b.\nElliot J Crowley and Andrew Zisserman. In search of art. In Computer Vision-ECCV 2014 Workshops:\nZurich, Switzerland, September 6-7 and 12, 2014, Proceedings, Part I 13, pp. 54\u201370. Springer,\n2015.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hier-\narchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition,\npp. 248\u2013255, 2009. doi: 10.1109/CVPR.2009.5206848.\nGuneet S. Dhillon, Pratik Chaudhari, Avinash Ravichandran, and Stefano Soatto. A baseline for\nfew-shot image classification, 2020.\nM. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.\nThe\nPASCAL Visual Object Classes Challenge 2012 (VOC2012) Results.\nhttp://www.pascal-\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\n10\nUnder Review\nMatthew Fickus, John Jasper, Emily J King, and Dustin G Mixon. Equiangular tight frames that\ncontain regular simplices. Linear Algebra and its applications, 555:98\u2013138, 2018.\nChristopher Fifty, Jure Leskovec, and Sebastian Thrun. In-context learning for few-shot molecular\nproperty prediction, 2023.\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of\ndeep networks. In International conference on machine learning, pp. 1126\u20131135. PMLR, 2017.\nSpyros Gidaris, Andrei Bursuc, Nikos Komodakis, Patrick P\u00e9rez, and Matthieu Cord. Boosting\nfew-shot visual learning with self-supervision, 2019.\nQianyu Guo, Hongtong Gong, Xujun Wei, Yanwei Fu, Weifeng Ge, Yizhou Yu, and Wenqiang Zhang.\nRankdnn: Learning to rank for few-shot learning, 2022.\nYunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana\nRosing, and Rogerio Feris. A broader study of cross-domain few-shot learning. In Computer\nVision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings,\nPart XXVII 16, pp. 124\u2013141. Springer, 2020.\nShell Xu Hu, Pablo G. Moreno, Yang Xiao, Xi Shen, Guillaume Obozinski, Neil D. Lawrence, and\nAndreas Damianou. Empirical bayes transductive meta-learning with synthetic gradients, 2020.\nShell Xu Hu, Da Li, Jan St\u00fchmer, Minyoung Kim, and Timothy M. Hospedales. Pushing the limits\nof simple pipelines for few-shot learning: External data and fine-tuning make a difference, 2022.\nYuqing Hu, Vincent Gripon, and St\u00e9phane Pateux. Leveraging the feature distribution in transfer-\nbased few-shot learning, 2021.\nKai Huang, Jie Geng, Wen Jiang, Xinyang Deng, and Zhe Xu. Pseudo-loss confidence metric for\nsemi-supervised few-shot learning. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 8671\u20138680, 2021.\nAshraful Islam, Chun-Fu Richard Chen, Rameswar Panda, Leonid Karlinsky, Rogerio Feris, and\nRichard J Radke. Dynamic distillation network for cross-domain few-shot recognition with\nunlabeled data. Advances in Neural Information Processing Systems, 34:3584\u20133595, 2021.\nNorbert Jankowski and Krzysztof Gr \u02dbabczewski. Universal meta-learning architecture and algorithms.\nMeta-learning in computational intelligence, pp. 1\u201376, 2011.\nJinxiang Lai, Siqian Yang, Wenlong Liu, Yi Zeng, Zhongyi Huang, Wenlong Wu, Jun Liu, Bin-Bin\nGao, and Chengjie Wang. tsf: Transformer-based semantic filter for few-shot learning, 2022.\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with\ndifferentiable convex optimization. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 10657\u201310665, 2019a.\nKwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with\ndifferentiable convex optimization, 2019b.\nHongyang Li, David Eigen, Samuel Dodge, Matthew Zeiler, and Xiaogang Wang. Finding task-\nrelevant features for few-shot learning by category traversal. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 1\u201310, 2019a.\nXinzhe Li, Qianru Sun, Yaoyao Liu, Shibao Zheng, Qin Zhou, Tat-Seng Chua, and Bernt Schiele.\nLearning to self-train for semi-supervised few-shot classification, 2019b.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pp. 740\u2013755. Springer, 2014.\nSubhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained\nvisual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013.\n11\nUnder Review\nAlex Nichol and John Schulman. Reptile: a scalable metalearning algorithm. arXiv preprint\narXiv:1803.02999, 2(3):4, 2018.\nAlex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv\npreprint arXiv:1803.02999, 2018.\nJaehoon Oh, Sungnyun Kim, Namgyu Ho, Jin-Hwa Kim, Hwanjun Song, and Se-Young Yun.\nUnderstanding cross-domain few-shot learning based on domain similarity and few-shot difficulty.\nAdvances in Neural Information Processing Systems, 35:2622\u20132636, 2022.\nVardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal\nphase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):\n24652\u201324663, 2020.\nCheng Perng Phoo and Bharath Hariharan. Self-training for few-shot transfer across extreme task\ndifferences. arXiv preprint arXiv:2010.07734, 2020.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International conference on machine learning, pp.\n8748\u20138763. PMLR, 2021.\nAniruddh Raghu, Maithra Raghu, Samy Bengio, and Oriol Vinyals. Rapid learning or feature reuse?\ntowards understanding the effectiveness of maml. arXiv preprint arXiv:1909.09157, 2019.\nMengye Ren, Eleni Triantafillou, Sachin Ravi, Jake Snell, Kevin Swersky, Joshua B Tenenbaum,\nHugo Larochelle, and Richard S Zemel. Meta-learning for semi-supervised few-shot classification.\narXiv preprint arXiv:1803.00676, 2018.\nPau Rodr\u00edguez, Issam Laradji, Alexandre Drouin, and Alexandre Lacoste. Embedding propagation:\nSmoother manifold for few-shot classification, 2020.\nBabak Saleh and Ahmed Elgammal. Large-scale classification of fine-art paintings: Learning the\nright metric on the right feature. arXiv preprint arXiv:1505.00855, 2015.\nBrigit Schroeder and Yin Cui.\nFGVCx fungi classification challenge 2018.\ngithub.com/\nvisipedia/fgvcx_fungi_comp, 2018.\nChristian Simon, Piotr Koniusz, Richard Nock, and Mehrtash Harandi. Adaptive subspaces for\nfew-shot learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 4136\u20134145, 2020.\nJake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances\nin neural information processing systems, 30, 2017a.\nJake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning,\n2017b.\nFlood Sung, Yongxin Yang, Li Zhang, Tao Xiang, Philip HS Torr, and Timothy M Hospedales.\nLearning to compare: Relation network for few-shot learning. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, pp. 1199\u20131208, 2018.\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one\nshot learning. Advances in neural information processing systems, 29, 2016.\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The caltech-ucsd\nbirds-200-2011 dataset. 2011.\nManfred K Warmuth and Dima Kuzmin. Randomized online pca algorithms with regret bounds\nthat are logarithmic in the dimension. Journal of Machine Learning Research, 9(Oct):2287\u20132320,\n2008.\nLloyd Welch. Lower bounds on the maximum cross correlation of signals (corresp.). IEEE Transac-\ntions on Information theory, 20(3):397\u2013399, 1974.\n12\nUnder Review\nDavis Wertheimer and Bharath Hariharan. Few-shot learning with localization in realistic settings.\nIn Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp.\n6558\u20136567, 2019.\nDavis Wertheimer, Luming Tang, and Bharath Hariharan. Few-shot classification with feature map\nreconstruction networks, 2020.\nYibo Yang, Liang Xie, Shixiang Chen, Xiangtai Li, Zhouchen Lin, and Dacheng Tao. Do we really\nneed a learnable classifier at the end of deep neural network? arXiv e-prints, pp. arXiv\u20132203,\n2022.\nXueting Zhang, Debin Meng, Henry Gouk, and Timothy Hospedales. Shallow bayesian meta learning\nfor real-world few-shot recognition, 2021.\n13\nUnder Review\nA\nAPPENDIX\nA.1\nSUPPLEMENTARY THEORETICAL ANALYSIS\nWe offer additional insight into the theoretical analysis presented in Section 4 and provide the omitted\nremarks, properties, lemmas, and proofs.\nA.1.1\nEQUIANGULAR TIGHT FRAMES\nPapyan et al. (2020) coin the term Simplex Equianguar Tight Frame to describe a set of vectors\n{\u03d5j}d\nj=1 such that the minimum angle between any two pairs of vectors is maximized and all vectors\nhave equal norm. Formally,\nDefinition 2. Let Rd be a d\u2212dimensional inner product space over R with the Euclidean inner\nproduct. A Simplex ETF is a set of d vectors {\u03d5j}d\nj=1, \u03d5j \u2208 Rd, specified by the columns of\nq\nd\nd\u22121(Id \u2212 1\nd11T )\nwhere Id \u2208 Rd\u00d7d is the identity matrix and 1 \u2208 Rd\u00d71 is the ones vector. Somewhat contradictory, a\nSimplex Equiangular Tight Frame is not an Equiangular Tight Frame (Welch, 1974) as this set of\nvectors does not form a tight frame in Rd.\nDefinition 3. Let R be a d\u2212dimensional space over R with the Euclidean inner product. An\nEquiangular Tight Frame (ETF) is a set of non-zero, equal norm vectors {\u03d5j}n\nj=1, n \u2265 d, that\nachieves the Welch lower bound:\nmax\nj\u0338=j\u2032\n|\u27e8\u03d5j , \u03d5j\u2032\u27e9|\n\u2225\u03d5j\u2225\u2225\u03d5j\u2032\u2225 =\ns\nn \u2212 d\nd(n \u2212 1)\nIt is well-known that a set of non-zero equal-norm vectors satisfies the Welch lower bound if and\nonly if that set of vectors is equiangular and also a tight frame for Rd (Fickus et al., 2018).\nDefinition 4. A set of non-zero, equal norm vectors {\u03d5j}n\nj=1 is equiangular if \u2200j \u0338= j\u2032, |\u27e8\u03d5j , \u03d5j\u2032\u27e9| =\nc for some c \u2208 R, c > 0.\nDefinition 5. {\u03d5j}n\nj=1 is a tight frame for Rd if, \u2200v \u2208 Rd, \u2203A > 0 such that A\u2225v\u22252 =\nPn\nj=1 |\u27e8\u03d5j , v\u27e9|2.\nRemark 1. A Simplex Equiangular Tight Frame is not a tight frame.\nProof. Observe that for any finite d, for {\u03d5j}d\nj=1 equal to the columns of\nq\nd\nd\u22121(Id \u2212 1\nd11T ), it\nis the case that\nd\u22121\nP\nj=1\n\u03d5j = \u22121 \u2217 \u03d5d. So {\u03d5j}n\nj=1 do not span Rd, and therefore, cannot be a tight\nframe.\nSimilarly, a Simplex ETF is not a d\u2212simplex.\nRemark 2. A Simplex Equiangular Tight Frame is not a simplex.\nProof. A simplex in Rn requires n + 1 points.\nTo align terminology with properties, we generalize a Simplex ETF to an ELMES in Definition 1: a\nset of d vectors in a (d + k)-dimensional ambient space with k \u2265 0. Observe that a regular simplex\nis a special type of ETF in which the number of vectors in the set is one more than the dimension\nof the space that they span (Fickus et al., 2018). Building on this observation, an intuitive view of\nELMES is a regular d\u2212simplex immersed in Rd+k.\nRemark 3. Consider a centered d-dimensional regular simplex with vertices {\u03d5j}d+1\nj=1, \u03d5j \u2208 Rd+1.\nLet \u0131can be the canonical inclusion map: Rd \u2192 Rd+1, \u0131can(x1, x2, ..., xd) = (x1, x2, ..., xd, 0d+1),\nthen {\u0131can(\u03d5j)}d+1\nj=1 is an ELMES.\n14\nUnder Review\nx\n1.0\n0.5\n0.0\n0.5\n1.0\ny\n1.0\n0.5\n0.0\n0.5\n1.0\nz\n1.0\n0.5\n0.0\n0.5\n1.0\nFigure 3: A visualization of a d = 4 ELMES in R3. Observe the endpoints of the vectors of an ELMES lie on\nthe vertices of a centered regular tetrahedron.\nProof. The two criteria of an ELMES are maximally equiangular and equal length. As all vertices\nof a centered regular d\u2212simplex are equal length from the origin, {\u03d5j}d+1\nj=1 are equal length and\ntherefore {\u0131can(\u03d5j)}d+1\nj=1 must also have equal length.\nSimilarly, from Lemma 10 of Papyan et al. (2020), we know the cosine of the angle between any\ntwo vectors in a (d + 1)\u2212dimensional ELMES is \u22121\nd . It is known that for a d\u2212dimensional regular\nsimplex in Rd centered at the origin, the angle subtended by any two verticies through the origin is\ncos(\u03b8) = \u22121\nd . Immersing {\u03d5j}d+1\nj=1, \u03d5j \u2208 Rd, into Rd+1 via the canonical inclusion operator \u0131can\ndoes not change the pairwise angle between vectors in this set: \u27e8\u03d5j , \u03d5j\u2032\u27e9 = \u27e8\u0131can(\u03d5j) , \u0131can(\u03d5j\u2032)\u27e9.\nAs {\u0131can(\u03d5j)}d+1\nj=1 are equal length and maximally equiangular, it forms an ELMES.\nWe now show that an ELMES immersed in a higher dimension remains an ELMES. Taken with\nRemark 3, we can view a high-dimensional ELMES in Rd composed of n + 1 vectors {\u03d5j}n+1\nj=1 ,\nd >> n + 1, as simply a n\u2212simplex immersed in Rd via the canonical inclusion operator.\nLemma 3. Let \u0131can : Rd \u2192 Rd+k. If {\u03d5j}n\nj=1 is an ELMES , then {\u0131can(\u03d5j)}d\nj=1 is an ELMES.\nProof. This reduces to proving that the maximum angle between a set of d equiangular points in Rd\nis the maximum angle between a set of d equiangular points in Rd+k. Let {\u03d5j}d\nj=1 be an ELMES\nsuch that \u03d5j \u2208 Rd and {\u03c8j}d\nj=1 be an ELMES such that \u03c8j \u2208 Rd+k. Then {\u03c8j}d\nj=1 lie in a\nd\u2212dimensional subspace of Rd+k: \u2203\u03b31, ..., \u03b3d and basis vectors e1, ..., ed such that \u2200\u03c8j \u2208 {\u03c8j}d\nj=1,\n\u03c8j = Pd\ni=1 \u03b3iei. Therefore, \u2200j \u0338= j\u2032, \u27e8\u03c8j , \u03c8j\u2032\u27e9 \u2264 \u27e8\u03d5j , \u03d5j\u2032\u27e9 as {\u03d5j}d\nj=1 are an ELMES for\nRd.\nA.1.2\nELMES ROTATIONAL SYMMETRY\nThere are infinitely many ELMES by rotating one such set of vectors about the origin.\nRemark 4. Let {\u03d5j}d\nj=1 be an ELMES in Rd+k for some k \u2265 0. Let o : Rd+k \u2192 Rd+k be an\noperator from the special orthogonal group SO(d + k). Then {o(\u03d5j)}d\nj=1 is also an ELMES .\n15\nUnder Review\nProof. Length is preserved as operations in SO(d + k) have determinant 1 and angles are similarly\npreserved as operations in SO(d + k) are unitary (i.e. preserving inner product).\nA.1.3\nA SET OF ORTHONORMAL BASIS VECTORS IS NOT AN ELMES\nA final remark relates to the common misconception that a set of orthonormal basis vectors {\u03c8j}d\nj=1\nis an ELMES. While {\u03c8j}d\nj=1 is an ETF in Rd since this set realizes the Welch lower-bound\nin Definition 3, these vectors are not maximally equiangular: \u27e8\u03c8j , \u03c8j\u2032\u27e9 = 0 >\n\u22121\nd\u22121.\nA.2\nELMES MAXIMIZES pj(X = j)\nJustification of Assumption 1. This property is implied by symmetry in the assignment of class\nembeddings to support classes. As the assignment is arbitrary, all learnable \u03c8i class detectors should\nhave equal probability of detecting their respective class.\nJustification of Assumption 2. Informally, this property states that, for any m-subset of classes\n{\u03d5j}m\nj=1, the probability of \u03c8j detecting class j is equal to the probability of \u03c8i detecting class\ni. This is again implied by symmetry in the assignment of class embeddings to support classes as\nmeta-learning algorithms may predict among a subset of m classes in the support set rather than the\nmaximum number of classes d.\nJustification of Assumption 3. Recall in Rd, \u27e8\u03c8 , \u03d5\u27e9 = \u2225\u03c8\u2225\u2225\u03d5\u2225 cos(\u03b8) where \u03b8 is the angle between\n\u03c8i and \u03d5i. Then this assumption constrains our set {\u03d5j}d\nj=1 so that relative norm of \u03d5i with respect\nto \u03d5j is lower bounded by cos(\u03b8i,j): \u2225\u03d5i\u2225\n\u2225\u03d5j\u2225 > cos(\u03b8i,j).\nInformally, the {\u03d5j}d\nj=1 are sufficiently spread out in the ambient space so that the learnable \u03c8i that\nmaximizes pi(X = i) is \u03d5i itself: \u03c8i =\n\u03d5i\n\u2225\u03d5i\u2225. This constraint helps us avoid degenerative cases\nsimilar to the {\u03d5j}d\nj=1 all equal maximum entropy case described earlier. For example, \u03d5j = \u03b1\u03d5i,\ni \u0338= j with \u03b1 > 0 is one such degenerative case where one class embedding vector is stacked on a\ndifferent class embedding, but with higher norm.\nProof of Theorem 1. Taken with Assumption 1, Assumption 2, and Assumption 3, it suffices to show\nTheorem 2 and Lemma 6 to prove Theorem 1.\nTheorem 2. p1(X = 1) = p2(X = 2) = ... = pd(X = d) \u21d0\u21d2 {\u03d5j}d\nj=1 are equiangular and\nequal norm.\nTo show the forward (\u21d2) direction, it suffices to first show p1(X = 1) = p2(X = 2) = ... =\npd(X = d) \u21d2 {\u03d5j}d\nj=1 are equal norm and then show p1(X = 1) = p2(X = 2) = ... = pd(X = d)\n\u21d2 {\u03d5j}d\nj=1 are equiangular.\nLemma 4. p1(X = 1) = p2(X = 2) = ... = pd(X = d) \u21d2 {\u03d5j}d\nj=1 are equal norm.\nProof. This implication holds when d = 2:\np1(X = 1) =\ne\u2225\u03d51\u2225\ne\u2225\u03d51\u2225 + e\u2225\u03d52\u2225 cos(\u03b81,2) =\ne\u2225\u03d52\u2225\ne\u2225\u03d52\u2225 + e\u2225\u03d51\u2225 cos(\u03b81,2) = p2(X = 2)\ne\u2225\u03d51\u2225(e\u2225\u03d52\u2225 + e\u2225\u03d51\u2225 cos(\u03b81,2)) = e\u2225\u03d52\u2225(e\u2225\u03d51\u2225 + e\u2225\u03d52\u2225 cos(\u03b81,2))\ne\u2225\u03d51\u2225+\u2225\u03d51\u2225 cos(\u03b81,2) = e\u2225\u03d52\u2225+\u2225\u03d52\u2225 cos(\u03b81,2)\n\u2225\u03d51\u2225(1 + cos(\u03b81,2)) = \u2225\u03d52\u2225(1 + cos(\u03b81,2))\n\u2225\u03d51\u2225 = \u2225\u03d52\u2225\nSuppose d > 2 and p1(X = 1) = ... = pd(X = d). By Assumption 2, all m\u2212combinations\n\u0000 d\nm\n\u0001\nof {p1(X = 1), ..., pd(X = d)} are equal. This implies all 2-combinations are equal: pi(X = i) =\npj(X = j) \u21d2 \u2225\u03d5i\u2225 = \u2225\u03d5j\u2225. Therefore, \u2225\u03d51\u2225 = ... = \u2225\u03d5d\u2225.\n16\nUnder Review\nLemma 5. p1(X = 1) = p2(X = 2) = ... = pd(X = d) \u21d2 {\u03d5j}d\nj=1 are equiangular.\nProof. This implication is trivially true when d = 2 (see the proof of Lemma 4), and we show it is\nsimilarly true when d = 3. Following the steps in the proof of Lemma 4, we arrive at the following 3\npairs of equalities:\n(1) e\u2225\u03d51\u2225(1+cos(\u03b81,2)) + e\u2225\u03d51\u2225+\u2225\u03d53\u2225 cos(\u03b82,3) = e\u2225\u03d52\u2225(1+cos(\u03b81,2)) + e\u2225\u03d52\u2225+\u2225\u03d53\u2225 cos(\u03b81,3)\n(2) e\u2225\u03d51\u2225(1+cos(\u03b81,3)) + e\u2225\u03d51\u2225+\u2225\u03d52\u2225 cos(\u03b82,3) = e\u2225\u03d53\u2225(1+cos(\u03b81,3)) + e\u2225\u03d53\u2225+\u2225\u03d52\u2225 cos(\u03b81,3)\n(3) e\u2225\u03d52\u2225(1+cos(\u03b82,3)) + e\u2225\u03d52\u2225+\u2225\u03d51\u2225 cos(\u03b81,3) = e\u2225\u03d53\u2225(1+cos(\u03b82,3)) + e\u2225\u03d53\u2225+\u2225\u03d51\u2225 cos(\u03b81,2)\nFrom Lemma 4, p1(X = 1) = p2(X = 2) = p3(X = 3) \u21d2 \u2225\u03d51\u2225 = \u2225\u03d52\u2225 = \u2225\u03d53\u2225, so the above\npairs of equalities reduce to:\n(1) cos(\u03b82,3) = cos(\u03b81,3)\n(2) cos(\u03b82,3) = cos(\u03b81,3)\n(3) cos(\u03b81,3) = cos(\u03b81,2)\nand when d = 3, {\u03d5j}3\nj=1 are equiangular.\nSuppose d > 3 and p1(X = 1) = ... = pd(X = d). By Assumption 2, all m\u2212combinations\n\u0000 d\nm\n\u0001\nof {p1(X = 1), ..., pd(X = d)} are equal. This implies all 3-combinations are equal: pi(X = i) =\npj(X = j) = pk(X = k) \u21d2 \u03b8i,j = \u03b8i,k = \u03b8j,k. Therefore, all angles are equal \u03b8i,j = \u03b8l,m for\n1 \u2264 i, j, l, m \u2264 d.\nProof of Theorem 2. (\u21d2) Suppose p1(X = 1) = p2(X = 2) = ... = pd(X = d).\nBy Lemma 4 and Lemma 5, p1(X = 1) = p2(X = 2) = ... = pd(X = d) \u21d2 {\u03d5j}d\nj=1 are\nequiangular and equal norm.\n(\u21d0) Suppose {\u03d5j}d\nj=1 are equiangular and equal norm. Let \u2225\u03d5\u2225 be the norm of any vector in our set\nand cos(\u03b8) be the pairwise angle between any two vectors. Then\npi(X = i) =\ne\u2225\u03d5\u2225\ne\u2225\u03d5\u2225 + (d \u2212 1)e\u2225\u03d5\u2225 cos(\u03b8) = pj(X = j)\nfor any 1 \u2264 i, j \u2264 d.\nLemma 6. For a set of equiangular and equal norm vectors, maximum equiangularity maximizes\nP\nj\npj(X = j).\nProof. The maximum pairwise angle between two vectors in Rd is \u03c0, and from Theorem 2\npi(X = i) = pj(X = j) =\ne\u2225\u03d5\u2225\ne\u2225\u03d5\u2225 + (d \u2212 1)e\u2225\u03d5\u2225 cos(\u03b8)\nfor all 1 \u2264 i, j \u2264 d. Increasing the angle \u03b8 decreases cos(\u03b8). Decreasing cos(\u03b8) only decreases the\ndenominator, which in turn, increases pi(X = i). Therefore, maximizing the pairwise angle between\nall vectors maximizes pi(X = i) for all 1 \u2264 i \u2264 d.\nA.2.1\nAN ELMES MINIMIZES Hi\nProof of Lemma 1. Equal norm and equiangular {\u03d5j}d\nj=1 are bounded in norm, and thus, the set of\nprobability distributions we obtain {p1, p2, ..., pd} belong to a capped simplex (Warmuth & Kuzmin,\n2008) \u2206d\nc = {p \u2208 \u2206| maxi pi \u2264 c} where c =\ne\u2225\u03d5\u22252\ne\u2225\u03d5\u22252+(d\u22121)e\u2225\u03d5\u22252 cos(\u03b8) . Clearly, among such\nprobability vectors, the minimum entropy is achieved at the boundary where cos(\u03b8) is minimized,\ni.e., when the {\u03d5j}d\nj=1 are maximally equiangular.\n17\nUnder Review\nA.2.2\nAN ELMES MAINTAINS PERMUTATION INVARIANCE\nProof of Lemma 2. This follows from row-wise equivariance to permutations in matrix multiplication.\nFor any permutation \u03c0 : [1, . . . , n] \u2192 [1, . . . , n] applied to the rows of Sn, we have \u03c0(S)W =\n\u03c0(SW).\nB\nELMES ABLATION\nTable 5: CUB & tiered-ImageNet & Aircraft mean accuracy and standard error across 10,000 test\nepochs.\nMethod\nCUB\ntiered-ImageNet\nAircraft\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n91.8\u00b1.2\n97.1\u00b1.1\n95.4\u00b1.1\n98.1\u00b1.1\n63.3\u00b1.3\n79.1\u00b1.2\nCAML [Learnable Class Embedding]\n91.8\u00b1.2\n97.1\u00b1.1\n95.3\u00b1.1\n98.3\u00b1.1\n66.3\u00b1.2\n80.6\u00b1.2\nTable 6: MiniImageNet & CIFAR-fs mean accuracy and standard error.\nMethod\nCIFAR-fs\nMiniImageNet\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n70.8\u00b1.2\n85.5\u00b1.1\n96.2\u00b1.1\n98.6\u00b1.0\nCAML [Learnable Class Embedding]\n71.1\u00b1.2\n85.9\u00b1.1\n96.1\u00b1.1\n98.7\u00b1.0\nTable 7: Pascal & Paintings mean accuracy and standard error across 10,000 test epochs.\nMethod\nPascal + Paintings\nPaintings\nPascal\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n63.8\u00b1.2\n78.3\u00b1.1\n51.1\u00b1.2\n65.2\u00b1.1\n82.6\u00b1.2\n89.7\u00b1.1\nCAML [Learnable Class Embedding]\n63.1\u00b1.2\n78.0\u00b1.1\n51.3\u00b1.2\n65.0\u00b1.1\n82.1\u00b1.2\n89.7\u00b1.1\nTable 8: meta-iNat & tiered meta-iNat & ChestX mean accuracy and standard error across 10,000\ntest epochs.\nMethod\nmeta-iNat\ntiered meta-iNat\nChestX\n5w-1s\n5w-5s\n5w-1s\n5w-5s\n5w-1s\n5w-5s\nCAML [ELMES Class Embedding]\n91.2\u00b1.2\n96.3\u00b1.1\n81.9\u00b1.2\n91.6\u00b1.1\n21.5\u00b1.1\n22.2\u00b1.1\nCAML [Learnable Class Embedding]\n91.4\u00b1.2\n96.4\u00b1.1\n82.1\u00b1.2\n91.8\u00b1.1\n21.5\u00b1.1\n22.6\u00b1.1\n18\n"
  },
  {
    "title": "Approximating Two-Layer Feedforward Networks for Efficient Transformers",
    "link": "https://arxiv.org/pdf/2310.10837.pdf",
    "upvote": "10",
    "text": "Approximating Two-Layer Feedforward Networks\nfor Efficient Transformers\nR\u00f3bert Csord\u00e1s1 Kazuki Irie2\u2020 J\u00fcrgen Schmidhuber1,3\n1The Swiss AI Lab IDSIA, USI & SUPSI\n2Harvard University\n3AI Initiative, KAUST\n{robert,juergen}@idsia.ch, kirie@fas.harvard.edu\nAbstract\nHow to reduce compute and memory require-\nments of neural networks (NNs) without sac-\nrificing performance?\nMany recent works\nuse sparse Mixtures of Experts (MoEs) to\nbuild resource-efficient large language mod-\nels (LMs). Here we introduce several novel\nperspectives on MoEs, presenting a general\nframework that unifies various methods to ap-\nproximate two-layer NNs (e.g., feedforward\nblocks of Transformers), including product-\nkey memories (PKMs). Leveraging insights\nfrom this framework, we propose methods\nto improve both MoEs and PKMs.\nUnlike\nprior work that compares MoEs with dense\nbaselines under the compute-equal condition,\nour evaluation condition is parameter-equal,\nwhich is crucial to properly evaluate LMs. We\nshow that our MoEs are competitive with the\ndense Transformer-XL on both the WikiText-\n103 and enwiki8 datasets at two different scales,\nwhile being much more resource-efficient. This\ndemonstrates that MoEs are relevant not only\nto extremely large LMs but also to any-scale\nresource-efficient LMs. Our code is public.1\n1\nIntroduction\nDespite impressive results recently achieved by\nlarge language models (LLMs; Radford et al.\n(2019); Brown et al. (2020); Rae et al. (2021)),\nvast resource requirement remains their obvious\nlimitation. In fact, most existing LLMs, such as\nGPT-3 (Brown et al., 2020), cannot be trained, fine-\ntuned or even evaluated without access to enormous\ncompute. Many recent works strive to develop\nLLMs that, at least, enable inference with limited\nresources (e.g., on consumer hardware), e.g., by\nbuilding \u201csmaller\u201d yet capable LMs (Touvron et al.,\n2023; Taori et al., 2023; Chiang et al., 2023) or de-\nveloping post-training quantization methods (Zafrir\net al., 2019; Dettmers et al., 2022). While these\n\u2020 Work done at IDSIA.\n1https://github.com/robertcsordas/moe\nmethods are gaining popularity, a principled solu-\ntion for resource-efficient neural networks (NNs)\nremains elusive.\nOne promising approach explored by several re-\ncent works on extremely-large LMs is the sparse\nmixture of experts (MoE; Shazeer et al. (2017);\nLewis et al. (2021); Lepikhin et al. (2021); Fedus\net al. (2022); Clark et al. (2022); Chi et al. (2022)).\nUnlike their dense counterparts, MoEs only com-\npute a subset of their activations (i.e, only a few\nexperts) at each step, offering reduced computation\nand memory costs. However, MoEs are not yet\ngenerally adopted as a generic/to-go approach, per-\nhaps because of certain common beliefs on MoEs:\n(1) They are hard to train (involving complex en-\ngineering tricks to prevent collapsing), (2) they\nare not competitive against their dense counter-\nparts with the same number of parameters (in fact,\nprior work focuses on FLOP-equal comparison,\n\u201cunfairly\u201d comparing MoEs against dense baselines\nwith many fewer trainable parameters), and finally,\n(3) they are reserved for extremely large models\n(they are rarely/never considered to further improve\nthe efficiency of \u201csmall\u201d models). Indeed, even\nprior works on MoE-based Transformer LMs only\ndeploy MoEs in a few feedforward blocks; while\nideally, all such blocks should benefit from replace-\nment by MoEs. Here we challenge these common\nbeliefs, and propose novel perspectives on MoEs.\nWe present MoEs within a unified framework\nof methods that approximate two-layer feed-\nforward networks, which includes product-key\nmemories (PKMs; Lample et al. (2019)) and\ntop-k sparsification.\nThis principled view not\nonly allows us to conceptually group and compare\nMoEs with PKMs, it also provides insights on\ndesign choices for improving these methods. Our\nresulting MoE Transformer variant outperforms\nour improved PKMs, and performs as well as\nor even outperforms the dense baseline, while\nusing a fraction of its compute for both training\narXiv:2310.10837v3  [cs.LG]  21 Nov 2023\nand inference.\nImportantly, unlike prior work,\nwe compare our MoEs with dense baselines with\nthe same number of total trainable parameters,\nwhich is crucial for proper evaluation in language\nmodeling. We conduct experiments on the standard\nWikiText-103 (at two different model scales) and\nEnwik8 datasets. We demonstrate that MoEs are\nnot limited to extremely-large LMs, but useful as a\ngeneric approach for resource-efficient NNs at any\nscale, and in line with the recent trend of improving\n\u201csmaller\u201d models (Touvron et al., 2023; Taori et al.,\n2023; Chiang et al., 2023). Finally, we release a\nCUDA kernel for our MoE layers which allows for\nachieving faster wall clock time and large memory\nreduction compared to the dense model.2\n2\nBackground\nTransformers (Vaswani et al., 2017) have two\nmain building blocks:\nthe self-attention layer\n(Parikh et al., 2016; Cheng et al., 2016; Bahdanau\net al., 2015), and the two-layer feedforward, i.e,\nmulti-layer perceptron (MLP) block. Acceleration\nand memory reduction of the self-attention is rather\nwell explored (see, e.g., linear attention dating back\nto the unnormalised linear Transformers of 1991\n(Schmidhuber, 1991; Katharopoulos et al., 2020;\nChoromanski et al., 2021; Schlag et al., 2021)), and\nvery efficient implementations (Dao et al., 2022)\nare also available. In constrast, resource-efficient\nMLP blocks are still underexplored.\nThis is\nour main focus, and it is of particular relevance\ntoday, as the proportion of the total parameter\ncounts, compute and memory requirements due\nto MLP blocks in Transformers is increasing in\never-growing LLMs.\nLet dmodel, dff denote positive integers. Each\nTransformer MLP block consists of one up-\nprojection layer with a weight matrix W1\n\u2208\nRdff\u00d7dmodel where typically dff = 4dmodel, and\none down-projection layer with parameters W2 \u2208\nRdmodel\u00d7dff that projects it back to the original size.\nNon-linearity (typically ReLU) is applied between\nthese two layers. That is, an input x \u2208 Rdmodel is\ntransformed to an output y \u2208 Rdmodel as\nu = ReLU (W1x)\n(1)\ny = W2u\n(2)\nwhere u \u2208 Rdff, and we omit biases (as well as\nbatch and time dimensions) for simplicity.\n2Our non-expert CUDA implementation still has much\nroom for further optimization.\nAlternatively, this layer can be viewed as a key-\nvalue memory accessed by attention (Vaswani et al.\n(2017)3,Geva et al. (2021)), where keys and values\nare rows and columns of weight matrices W1 and\nW2:\nW1 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nk\u22ba\n1\nk\u22ba\n2...\nk\u22ba\ndff\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(3)\nW2 =\n\uf8ee\n\uf8f0 v1\nv2\n. . .\nvdff\n\uf8f9\n\uf8fb\n(4)\nwhere ki \u2208 Rdmodel, vi \u2208 Rdmodel for i \u2208 {1, ..., dff}.\nThen, the output is computed as \u201cattention\u201d:\ny =\ndff\nX\ni=1\nvi ReLU(k\u22ba\ni x) =\ndff\nX\ni=1\n\u03b1ivi\n(5)\nwhere \u03b1i = ReLU(k\u22ba\ni x) \u2208 R\u22650 are the \u201cattention\nweights.\u201d Note that \u03b1i = u[i] where u[i] \u2208 R\ndenotes the i-th component of u \u2208 Rdff in Eq. 1.\nUnlike the standard self-attention, the MLP block\nuses a ReLU activation function (instead of the\nsoftmax) without scaling.\nIt has been observed that, in practice, only a few\nof the factors k\u22ba\ni x are positive (Li et al., 2023; Shen\net al., 2023), making the first layer\u2019s output, i.e., u,\nsparse. Concretely, Shen et al. (2023) report that in\na Transformer with dmodel = 256 and dff = 1024,\n10% of the channels account for 90% of the total\nactivation mass. We confirm this trend in our own\npreliminary study. Fig. 1 shows the average number\nof non-zero units in u of size dff = 2053 in our\n47M parameter dense model trained on WikiText-\n103 (we refer to App. A.2 for more details). The\nnumber is below 200 for all layers. This suggests\nthat the MLP block can be approximated without a\nsignificant performance loss. Note that this is also\nsupported by the findings of Zhang et al. (2022).\n3\nApproximating 2-layer MLPs\nHere we present a unified view on methods to\napproximate 2-layer MLPs (Sec. 2) that includes\nmany existing methods such as MoEs (Sec. 3.3)\nand PKMs (Sec. 3.2).\n3See the appendix \u201cTwo feedforward Layers = Attention\nover Parameter\u201d in their paper version \u201carXiv:1706.03762v3.\u201d\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nLayer\n0\n200\n400\nActive channels\nFigure 1: Number of active channels in u in our dense\n47M parameter model on WikiText-103 (out of 2053\ntotal channels). Standard deviation over all tokens of\nthe test and validation set.\nPreliminaries.\nLet \u02c6y \u2208 Rdmodel denote an approx-\nimation of y \u2208 Rdmodel in Eq. 5. Let yi \u2208 Rdmodel de-\nnote yi = \u03b1ivi for i \u2208 {1, ..., dff}. The core idea is\nto approximate the sum in Eq. 5, i.e., y = Pdff\ni=1 yi\nby only keeping a subset S \u2282 {1, ..., dff} of the\nkey-value pairs, i.e., \u02c6y = P\ni\u2208S yi.\nThe intu-\nition of this approximation is as follows. We as-\nsume that a good approximation \u02c6y of y is the\none that minimizes their Euclidean distance e =\n||\u02c6y \u2212 y||2\ne = || P2 \u2208 R, which can now be expressed as\ni\u2208 \u00afS \u03b1ivi||2\n2 where \u00afS denotes the comple-\nment of S, i.e., \u00afS = {1, ..., dff} \\ S. Since we have\ne = || P\ni\u2208 \u00afS \u03b1ivi||2\n2 \u2264 P\ni\u2208 \u00afS \u03b1i||vi||2\n2 (triangle in-\nequality; where the equality is achieved when vi\nare orthogonal), this upper-bound P\ni\u2208 \u00afS \u03b1i||vi||2\n2\ncan be minimized if each term ci = \u03b1i||vi||2\n2 \u2208 R\nare small. If we further assume that all value vec-\ntors vi have the same norm, the crucial factor for\napproximation quality is reduced to the attention\nweights \u03b1i. In this context, we also call \u03b1i the\ncontribution of key-value pair i.\nLet K be a positive integer. The general idea\nof all methods discussed in this work is to keep\nK pairs (ki, vi) whose contribution \u03b1i is the high-\nest, and ignore other low-contribution pairs. The\ngoal is to find the best mechanism to select such K\npairs. Here we discuss three variants: Top-K acti-\nvation (Sec. 3.1), Product-Key Memories (PKMs,\nSec. 3.2), and Mixture of Experts (MoEs, Sec. 3.3).\n3.1\nTop-K Activation Function\nThe most straightforward implementation of the\napproximation described above is the top-K acti-\nvation function:\nEx = arg topk(u, K) \u2282 {1, ..., dff}\n(6)\n\u02c6y =\nX\ni\u2208Ex\n\u03b1ivi\n(7)\nUnfortunately this only saves less than half of the\nentire computation: while this allows us to re-\nduce computation of Eq. 2, no computation can\nbe saved in Eq. 1 because full computation of u =\nReLU (W1x) is required for Eq. 6. Going beyond\nthis requires to also introduce some approximation\nto Eq. 6 as in PKMs (Sec. 3.2) and MoEs (Sec. 3.3).\n3.2\nProduct-Key Memories (PKMs)\nProduct-Key memories (Lample et al., 2019) con-\nsist of replacing W1 \u2208 Rdff\u00d7dmodel in Eq. 1 by two\nmatrices Wa, Wb \u2208 R\n\u221adff\u00d7 dmodel\n2\n. It slices the in-\nput vector x \u2208 Rdmodel into two halves, xa, xb \u2208\nR\ndmodel\n2\n, so that x = xa|xb, where | denotes con-\ncatenation. The matrix multiplication is then per-\nformed on these smaller vectors: ua = Waxa and\nub = Wbxb. Then u \u2208 Rdff is calculated by com-\nbining the elements of ua \u2208 R\n\u221adff and ub \u2208 R\n\u221adff\nin all possible ways (i.e., Cartesian products), simi-\nlarly to the outer product, but using addition instead\nof multiplication, i.e., for all i \u2208 {1, ..., dff},\nu[i] = ub[\u230ai/\np\ndff\u230b] + ua[i mod\np\ndff]\n(8)\nIn addition to applying Top-K at the output as in\nSec 3.1, here Top-K can also be used to accelerate\nthe operation above. By applying Top-K to ua\nand ub before combining them to compute u, only\nthe K2 << dff components of u[i] have to be\ncalculated, and they are guaranteed to contain the\nK biggest components of the full u.\nIn the original formulation (Lample et al., 2019),\nPKMs use a softmax activation function, taking in-\nspiration from self-attention (Vaswani et al., 2017).\nInstead, we\u2019ll show how a non-competing activa-\ntion function, such as ReLU is a better choice (see\nSec. 6.2).\n3.3\nMixture of Experts (MoE)\nLet NE, G denote positive integers. MoEs par-\ntition dff pairs of (ki, vi) (see their definition in\nSec. 2) into NE groups of size G each, such that\nG \u00b7 NE = dff. This means that the weight matrices\nW1 \u2208 Rdff\u00d7dmodel and W2 \u2208 Rdmodel\u00d7dff (Eqs. 1-2)\nare partitioned into matrices W e\n1 \u2208 R\ndff\nNE \u00d7dmodel\nand W e\n2 \u2208 Rdmodel\u00d7 dff\nNE for e \u2208 {1, ..., NE},\nW e\n1 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\nk\u22ba\neG+1\nk\u22ba\neG+2\n...\nk\u22ba\n(e+1)G\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(9)\nW e\n2 =\n\uf8ee\n\uf8f0 veG+1\nveG+2\n. . .\nv(e+1)G\n\uf8f9\n\uf8fb\n(10)\nThe output is computed as:\n\u02c6y =\nX\ne\u2208Ex\nW e\n2 s[e] ReLU(W e\n1 x)\n(11)\nwhere s[e] \u2208 R is the e-th element of vector\ns \u2208 RNE computed by an expert scoring func-\ntion sel : Rdmodel \u2192 RNE (typically s = sel(x) =\nsoftmax(W3x) with W3 \u2208 RNE\u00d7dmodel), and Ex\ndenotes a subset of indices {1, ..., NE} result-\ning from the Top-K operation on s, i.e., Ex =\narg topk(s, K). Note that in some variants, ad-\nditional re-normalization is applied after Top-K,\nso that P\ne\u2208Ex s[e] = 1, s[e] \u2265 0; we define such\nan operation as norm topk, see its exact defini-\ntion in App. A.1 4. The efficiency of MoEs comes\nfrom the fact that NE \u226a dff, thus calculating s\nis cheap. Furthermore, G and K are chosen so\nthat G \u2217 K \u226a dff, so the calculation performed by\nexperts is less expensive than the dense MLP.\nGiven the notation above, it is straightforward to\nsee that MoEs can also be viewed as approximating\n2-layer MLPs with a trainable component (i.e., the\nselection function sel to produce s). Similarly to\nEqs. 5 and 7, Eq. 11 can be expressed as:\n\u02c6y =\nX\ne\u2208Ex\nG\nX\ni=1\n\u03b1eG+is[e]veG+i\n(12)\nwhere, compared to Eqs. 5 and 7, the \u201ccontribution\nscores\u201d of key-value pair i (defined in Sec. 3/Pre-\nliminaries) have an additional factor s[e] of an\nexpert group e to which the key-value pair belongs.\nThe key challenge of MoEs is to learn an ex-\npert selection mechanism/function sel above that\nassigns high scores to only a few experts (so that we\ncan ignore others without sacrificing performance),\nwhile avoiding a well-known issue, called expert\ncollapsing, where only a few experts are used and\nthe rest are never selected. To avoid this, some regu-\nlarization is typically applied to the selection score\nsel(x), encouraging more uniform routing of ex-\nperts across the whole batch of tokens. We provide\na comprehensive review of MoE variants and their\ndetails in Sec. 4 and our improved version in Sec. 5.\n4\nExisting MoE variants\nSeveral variations of MoEs have been proposed\nwith many different details. Here we briefly review\nthe most popular and representative ones (e.g., we\n4In the case of the softmax(\u00b7) activation function, this is\nequivalent to applying Top-K to the logits before softmax.\ndo not cover those that make use of reinforcement\nlearning for expert routing) before describing\nour improved version in Sec. 5.\nWe\u2019ll review\ntheir expert selection function and regularization\nmethod, and highlight their key characteristics.\nSparsely Gated Mixtures of Experts.\nShazeer\net al. (2017) have revisited MoEs (Jacobs et al.,\n1991; Ivakhnenko and Lapa, 1965) with the Top-K\noperation, allowing a reduction in its resource de-\nmands. Their method is basically the one described\nin Sec. 3.3 (with re-normalization after Top-K)\nexcept that they use a noisy gating function:\nsel(x) = softmax(\nW3x + N(0, 1) \u00b7 softplus(W4x))\n(13)\nwhere W4 \u2208 RNE\u00d7dmodel, the Gaussian noise term\nN(0, 1) is element-wise and independent for each\nchannel, and softplus(x) = log(1 + ex). They use\nthe following auxiliary regularization term for load\nbalancing,\nL = CV\n X\nx\u2208B\nnorm topk(sel(x))\n!\n(14)\nwhere CV(x) = \u00b5x\n\u03c3x is the coefficient of variation\nand B is the set of all tokens in the batch.\nKey characteristics: The scores are normalized\nafter the top-K operation (with K > 1), which is\nequivalent to applying top-K before the softmax.\nSwitch Transformer.\nFedus et al. (2022) inte-\ngrate the MoE above into the Transformer to obtain\ntheir Switch Transformer. In terms of MoE details,\none of Fedus et al. (2022)\u2019s key claims is that top-1\nrouting is enough. Their selection function is sim-\nply: sel(x) = softmax(W3x), but they propose\na hard load-balancing between experts that run on\ndifferent hardware accelerators: At most \u00b5 |B|\nNE to-\nkens are allowed to be routed to an expert, where\n\u00b5 \u2208 R>0 is the capacity factor (typically between 1\nand 1.5), defining how many times more tokens can\nbe processed by one expert compared to the ideal\ncase of uniform routing. Each expert is forbidden\nto process more than this number of tokens. For\nregularization, the fraction of the tokens f \u2208 RNE\nprocessed by each expert, and the average selection\nprobability p \u2208 RNE for each expert are calculated\n(K = 1; top-1 is used) as:\nfi = 1\n|B|\nX\nx\u2208B\n1{i \u2208 arg topk(sel(x), K)} (15)\np = 1\n|B|\nX\nx\u2208B\nsel(x)\n(16)\nL = NEf \u00b7 p\n(17)\nwhere 1 denotes the indicator function (which is\nequal to 1 if the argument is true, and 0 otherwise),\nand \u00b7 denotes dot product. Intuitively, this serves\nas an adaptive regularization that penalizes experts\nthat are used often with high \u201cweights.\u201d In addition,\nthey use dropout with a high drop rate (40%) in\nthe experts (but only 10% in the normal layers).\nFurthermore, Fedus et al. (2022) also propose to\ninitialize the experts with\nq\n0.1\nG . As we\u2019ll see in\nSec. 5, we use a modified version of this scheme.\nNote that applying Top-K after softmax encour-\nages collapsing: if the score of the selected ex-\npert is increased, the scores of all other experts are\nautomatically decreased. This is not the case for\nShazeer et al. (2017): In their method, only the se-\nlected experts compete with each other, so if their\npresence is beneficial, their score can be increased.\nKey characteristics: Note that Top-1 is applied\nafter the softmax without re-normalization.\nBASE layers and S-BASE.\nInspired by the rout-\ning strategy and the hard capacity factor of the\nSwitch Transformer, Lewis et al. (2021) propose\nBASE layers. They use top-1 routing and a sigmoid\nactivation \u03c3 in the selection function:\nsel(x) = \u03c3(W3x)\n(18)\nNow instead of using arg topk, they solve the fol-\nlowing linear assignment problem to find the index\nex \u2208 {1, ..., NE} of the expert to which each input\nx \u2208 B is routed,\nmaximize\nex\u2208{1,...,NE},x\u2208B\nX\nx\u2208B\nsel(x)[ex]\n(19)\ns.t. \u2200i \u2208 {1, ..., NE},\nX\nx\u2208B\n1{ex == i} = |B|\nNE\nThis guarantees uniform assignment of experts,\nwhich is efficient for multi-accelerator training.\nThe output is computed using Eq. 11 with Ex =\n{ex} (a set with a single element; \u201ctop-1\u201d). How-\never, at inference time, no such balancing is pos-\nsible because not all tokens of the sequence are\navailable at each step; Ex = {arg max (sel(x))} is\nused instead. Lewis et al. (2021) show that, while\nduring training, the routing is enforced to be com-\npletely uniform, during the test time, the distribu-\ntion looks exponential (in fact, this is similar to the\nSwitch Transformer but more balanced for BASE).\nThe algorithm for solving the linear assignment\nproblem (Eq. 19) is difficult to implement effi-\nciently on modern accelerators. Clark et al. (2022)\nhave proposed to use the Sinkhorn algorithm\n(Sinkhorn, 1964; Sinkhorn and Knopp, 1967) in-\nstead (resulting in a model called Sinkhorn-BASE\nor S-BASE), to approximate the solution to this\nproblem (note that similar routing is independently\ndiscussed by Kool et al. (2021)). They report that\nthis works well, while being simpler to implement.\nThus, our reimplementation of BASE is S-BASE\nusing the Sinkhorn algorithm.\nKey characteristics: During training, Sinkhorn\niterations are used on scores to obtain a balanced as-\nsignment. The sigmoid activation is always applied\nto compute the weighting score.\nOverall, all load-balancing methods above are\nrather complex. We propose simpler but effective\napproach for MoEs in Sec. 5.\n5\nImproving Mixture of Experts\nHere we present our improved MoE variant, which\nwe call \u03c3-MoE. We conduct thorough ablation stud-\nies on our design choices in Sec. 6.\n\u03c3-MoE Expert Selection Function.\nOur MoE\nmake use of the top-K operation (unlike BASE).\nThe activation we use on the selection function is\nsigmoid (as in Eq. 18 of BASE) instead of softmax\nused in Switch Transformer and Sparsely Gated\nMixtures of Experts. This choice is motivated by\nthe view of MoEs as approximate 2-layer MLPs\n(Sec. 3). In fact, softmax introduces competition\nbetween experts. No such competition between\nchannels is used in the regular 2-layer MLP (i.e.,\nthere is no constraint on \u03b1i in Eq. 5). This suggests\nthat, in principle, no competition is needed\nbetween terms in the sum of Eq. 12 in the MoE\neither, to induce sparsity. It is also well known\nto practitioners that softmax as regular activation\nnegatively affects the trainability of standard MLPs.\nSoftmax combined with top-K can also encourage\nexpert collapsing: when the selection score of one\nexpert increases, the score of the others automat-\nically decreases. For all these reasons, we opt for\nsigmoid instead of softmax; we experimentally\nconfirm that this is indeed a good choice.\nAdditionally, looking at MoEs in this framework\ngives us hints on combining them with Top-K\nactivation (Sec. 3.1) for further acceleration. We\ncan calculate ue = s[e] ReLU(W e\n1 x) (Eq. 11)\nfor the selected experts and perform an additional\nTop-K to keep the highest units among them and\nset the rest to zero. We leave this for future work.\n\u03c3-MoE Initialization.\nAnother design choice\nguided by the MLP-approximation view of MoEs\n(Sec. 3) is the initialization scheme for experts. Typ-\nically, experts are assumed to be independent, and\nthe standard deviation of the initialization (Glorot\nand Bengio, 2010; He et al., 2015) of W e\n2 is calcu-\nlated based on G instead of dff. Our experiments\nin Sec. 6.3 show that this is sub-optimal.\nIn contrast, we initialize all weight matrices\nidentically to the pre-layernorm dense baselines,\nnot taking in account the smaller size of the indi-\nvidual experts, i.e., W e\n1 \u223c N(0,\nq\n2\ndmodel\u00b7nlayers ) and\nW e\n2 \u223c N(0,\nq\n2\ndff\u00b7nlayers ) where nlayers denotes the\nnumber of layers, using dmodel and dff instead of G.\nWe also take special care when initializing W3\nof the selection function.\nWe initialize it to a\nnormal distribution with the same standard devi-\nation as W e\n1 , but we also ensure that the rows\nof W3 have the same norm5. This can be easily\nachieved in practice by initializing the weights to\nW \u2032\n3 \u223c N(0, 1), rescaling its rows to norm 1, and\nthen rescaling the whole matrix again to have the\ndesired standard deviation. Note that each scalar\nscore in s is the dot product of a row of W3 and x.\nThis initialization method ensures that only the an-\ngle between x and the rows of W3 initially affects\nthe score s, rather than an additional random factor\nresulting from initialization.\n\u03c3-MoE Regularization.\nAs already noted in\nSec. 4, existing regularization methods for load-\nbalancing are complex (e.g., Switch Transformers\nneed to deal separately with the actual selection\ndistribution and the scores, Sparsely Gated Mixture\nof Experts needs noise in the selection function).\nIn contrast, we propose to simply maximize the\nentropy of the selection distribution p \u2208 RNE cal-\nculated across the entire batch. Intuitively, this is a\n5Having rows with different norms would discourage the\nuse of experts corresponding to rows with small norms, as\ntheir selection score would be low even if the angle of the\nselector (row of W3) fully aligns with x.\nsimple way to encourage equal expert usage within\nthe batch and prevent unnecessary overconfidence\nin selecting individual experts. Let B be the set\nof all tokens in the batch (counting through both\nthe batch and time dimensions). We introduce the\nfollowing regularization term L:\np = 1\n|B|\nX\nx\u2208B\nsoftmax(W3x)\n(20)\nL =\nNE\nX\ne=1\np[e] log p[e]\n(21)\nFurthermore, we propose to randomly drop com-\nplete experts, during training; we refer to this as\nexpert dropout. Unlike the standard dropout on the\nactivation level, we do not apply rescaling, i.e.,\nsel(x) =\n(\n\u03c3(Wsx) \u2299 m\nif training\n\u03c3(Wsx)\notherwise\n(22)\nwhere m \u2208 {0, 1}NE, m \u223c Bernoulli(1 \u2212 \u03b4),\nwhere \u03b4 is the dropout rate, and \u2299 is the element-\nwise product. This prevents the dropped experts\nfrom being selected while not affecting the other\nones. Intuitively, when an expert dropout removes\na popular expert, it forces the less popular ones to\ntake over. Thus, the chance of them being trained\nand improved increases. We experimentally show\nthat our regularization method (Eq. 21) and expert\ndropout (Eq. 22) are both effective despite their\nsimplicity.\n6\nExperiments\nOur experimental setup is based on Dai et al.\n(2019)\u2019s Transformer XL with some modifications:\nwe use pre-layer norm and reduce the number of\ntraining steps to 100k to reduce the computational\nbudget. Also, to match the parameter counts be-\ntween the baseline and MoEs, we slightly modify\nthe hyperparameters of the baselines (Dai et al.,\n2019). In fact, our MoE CUDA kernel can only\nwork with dimensions divisible by 4. We round the\noriginal sizes up to the next suitable number, e.g.,\nwe change dmodel of our 47M-parameter WikiText-\n103 model from the original 410 to 412. Further-\nmore, since MoEs require extra parameters for the\nexpert selection function, we compensate for these\nby increasing the dff of the baseline model to match\nthe number of parameters. Our modified baseline\nmodel on Enwik8 still has 41M parameters and\nperforms similarly to the original Transformer XL\n(see Tab. 1). For WikiText-103, we use subword\nunits (Sennrich et al., 2016) using SentencePiece\ntokenizer (Kudo and Richardson, 2018) instead of\nthe word-level vocabulary, to avoid extra tricks re-\nquired to reduce the parameter count and compute\nrequirement resulting from the huge vocabulary\nsize. On WikiText-103, we consider two different\nmodel sizes: a 47M-parameter one (denoted by\n\u201cWT-S\u201d for \u201csmall\u201d), and a 262M-parameter one\n(\u201cWT-B\u201d for \u201cbig\u201d). We refer to Enwik8 as \u201cE8\u201d in\ncertain tables. For more details, see Appendix B.\nFor all the methods considered, we use them\nin every MLP block of the model, which is not a\ncommon practice in the literature. Typically, MoE\n(or other approximation methods) is used only once\nevery nth layer or even only in one layer. This is not\nsatisfactory since our goal is to find a generally ap-\nplicable method that can accelerate all layers across\nthe whole model. Moreover, this amplifies the dif-\nference between different methods, helping better\nillustrate effects of each of the design choices.\n6.1\nTop-K\nWe first evaluate the Top-K method (Sec. 3.1).\nThis standalone evaluation is important as Top-K\nis the basis of both the PKM and the MoE\napproximations. Tab. 1 shows the results. We\nobserve that not only Top-K in the MLP blocks\npreserves the performance of Transformers, it even\nimproves performance. We hypothesize that these\nimprovements are due to the reduction in feature\ninterference as described by Elhage et al. (2022).\nHowever, we obviously can not arbitrarily reduce\nK; there should be a trade-off between the denois-\ning effect and the capacity of the network. Here,\nthe optimal value we find is K = 128 or K = 512.\nTable 1: Effects of the top-k activation function on the\nperplexity (WikiText-103) and bits/character (Enwik8).\nDataset\n#params\ndff\nK\nbpc/perplexity\nEnwik8\n41M\n2053\n-\n1.08\n41M\n2053\n128\n1.07\n41M\n2053\n256\n1.08\n41M\n2053\n512\n1.08\nWikiText 103\n47M\n2053\n-\n11.81\n47M\n2053\n64\n11.86\n47M\n2053\n128\n11.74\n47M\n2053\n256\n11.74\n47M\n2053\n512\n11.68\nWikiText 103\n262M\n4110\n-\n9.46\n262M\n4110\n128\n9.26\n262M\n4110\n256\n9.34\n262M\n4110\n512\n9.36\n6.2\nProduct-Key Memory (PKM)\nOur view of Sec. 3 suggests using a non-\ncompetitive activation such as ReLU instead of the\nsoftmax used in the original PKM (Lample et al.,\n2019). Our experiments confirm the benefits of\nthis choice (Tab. 2): the performance of the ReLU\nvariants is much closer to the dense baseline (see\nalso related findings in Shen et al. (2023)). But\neven the best PKM models underperform the dense\nbaselines, indicating the fundamental limitation of\nPKMs. Note that, as stated above, we conduct\na careful comparison between the approximation\nmethod (here, PKM) and the dense baseline using\nthe same number of parameters. For more results\nand details on PKM, we refer to App. A.3.\nTable 2: Performance of the parameter-matched PKM\nmodels. We provide more results in Appendix/Tab. 6.\nVariant\nNonlin\nWT-S\nWT-B\nE8\nDense Baseline\nReLU\n11.81\n9.46\n1.08\nPKM\nSoftmax\n13.96\n11.10\n1.16\nReLU\n12.77\n9.98\n1.11\n6.3\nMixture of Experts (MoE)\nHere we evaluate our \u03c3-MoE models (Sec. 5) on\nEnwik8 and WikiText-103 as well as two additional\ndatasets, C4 (Raffel et al., 2020) and the newly\nproposed peS2o (Soldaini and Lo, 2023). Given\nthe large sizes of C4 and peS2o, we cannot afford to\ntrain for a full epoch; we train for 100k steps with\nthe same hyperparameters as for WikiText-103.\nMain results.\nTab. 3 shows the main results. Our\n\u03c3-MoE models match the performance of their\nparameter-equal dense baselines, while achieving\nsignificant memory and compute reduction. These\nmodels use K = 4 for NE = 16 or NE = 32,\nwhich is a \u201cmoderate\u201d level of sparsity but already\noffering significant compute reduction as shown\nin the column \u201c% FLOPs\u201d; concrete compute and\nmemory reduction is further shown in Fig. 2 (see\nAppendix A.5 for details). Naturally, there is a\nlimit on the minimum sparsity level to preserve\ngood performance of MoEs, which is determined\nby several factors. First, we empirically find that\nexperts with a group size of G < 128 generally\ndegrades performance. Second, our benchmarks\nwith the Top-K operation (Tab. 1) and our\nablations (Tab. 10 in the Appendix) show that\nthe minimum number of simultaneously active\nTable 3: Performance of parameter-batched \u03c3-MoEs\non perplexity (WikiText-103, C4 and peS2o) and\nbits/character (Enwik8). Ours matches or surpasses the\nperformance of the dense baselines across all datasets.\nDataset\nModel\n#params\n% FLOPs\nbpc/ppl\nEnwik8\nDense\n41M\n100.0%\n1.08\n\u03c3-MoE\n41M\n25.0%\n1.08\nWikiText-103\nDense\n47M\n100.0%\n11.81\n\u03c3-MoE\n47M\n25.0%\n11.71\nWikiText-103\nDense\n262M\n100.0%\n9.46\n\u03c3-MoE\n262M\n12.5%\n9.44\nC4\nDense\n47M\n100.0%\n23.76\n\u03c3-MoE\n47M\n25.0%\n23.25\nC4\nDense\n262M\n100.0%\n17.79\n\u03c3-MoE\n262M\n12.5%\n17.46\npeS2o\nDense\n47M\n100.0%\n14.34\n\u03c3-MoE\n47M\n25.0%\n14.12\npeS2o\nDense\n262M\n100.0%\n10.91\n\u03c3-MoE\n262M\n12.5%\n10.91\nchannels G \u00b7 K need to be above a certain critical\nthreshold (usually around 256-512). Finally, we\nmatch the number of parameters of the baseline\nmodel; this is the last constraint.\nUnder these\nconstraints, we find that the performance of the\ndense baselines can be matched using 25% of the\nrequired FLOPs and memory for activations for our\nsmall models, and 12.5% sparsity for the big one\n(note that FLOPs here do not take into account the\nlinear projection used to select the experts, which\nis negligible within the range of NE used here).\nIncreasing NE and Impact of Sparsity.\nThe\nresults above demonstrate that our \u03c3-MoEs can\nbe configured to match the desired performance\nwith fewer resources. Here we conduct an extra\nexperiment where we naively increase NE (while\nkeeping K = 4) from 16 to 128. This increases the\nnumber of parameters to 238M, while keeping the\nspeed and memory requirements comparable to the\noriginal model (column \u201cWT-S*\u201d in Tab. 4). This\nmodel achieves a test perplexity of 10.37, which\nis worse than 9.46 of the 262M dense model (see\nTab. 1). Indeed, even when the parameter count is\nmatched, there are other bottlenecks that are cru-\ncial, e.g., here dmodel is much smaller (412 vs 1024).\nWe construct another dense baseline by setting ev-\nery hyperparameter like in the 47M model, except\ndff, which is set to 16480 to match the number of\nparameters of the NE = 128 MoE. This baseline\nachieves a perplexity of 10.03: thus, the gap be-\ntween the scaled-up MoE and its dense counterpart\n256\n1024\n2048\n4096\ndmodel\n0\n200\nTime (ms)\nMLP\nMoE\n0.0\n0.5\n1.0\nMemory (GB)\nMLP\nMoE\nFigure 2: Execution time and memory usage of a\nforward-backward pass of a single MLP and MoE layer.\n|B| = 32768, corresponding to a batch size 64 and\nsequence length 512, dmodel indicated on the x-axis,\nK = 4, G = 128, dff = 4dmodel, and NE = dff\nG (fol-\nlowing the standard 1x-4x-1x shape). Full/dashed lines\nshow the execution time/memory consumption, respec-\ntively. Shows our mixed-precision Triton implementa-\ntion on an RTX 3090 with PyTorch 2.1 and CUDA 12.\nstill remains significant (10.37 vs 10.03), unlike\nwith the MoE with moderate sparsity. This indi-\ncates the importance of controlling MoE sparsity to\npreserve its performance against the dense baseline.\nComparison to Existing MoEs.\nWe also com-\npare our \u03c3-MoE to other MoE variants (Sec. 4),\nnamely Switch Transformer (Fedus et al., 2022),\nS-BASE (Clark et al., 2022)6 and the basic softmax\nvariant. Tab. 4 shows the results for multiple vari-\nants on WikiText-103 and Enwik8. Additionally,\nwe compare \u03c3-MoE to the most important baselines\non C4 and peS2o in Tab. 5. As Switch Transformer\nand S-BASE select only one single expert (K = 1),\nwe increase the expert size by a factor of 4 (instead\nof G = 128 in our models, we use G = 512), and\nwe decrease NE by the same factor for fair com-\nparison in terms of the parameter count. Neither\nof them uses our proposed expert dropout. For\nSwitch Transformer, we test a variant with stan-\ndard dropout in the experts (see App. B for details),\nand a version without. We also extend S-BASE to\nK = 4, which is similar to ours, except for the bal-\nancing method. Even considering all these cases,\nour \u03c3-MoE outperforms Switch Transformer and\nS-BASE. Note that in terms of FLOPs and memory\nusage, all MoE variants are equivalent given the\nsame hyperparameters (G, dmodel, and K).\nAblation Studies.\nFinally we conduct ablation\nstudies of individual design choices (Sec. 5). Tab. 4\nshows the results. Standard dropout instead of ex-\npert dropout leads to performance degradation for\nmost of the cases, except the model with NE = 128\nexperts. The softmax-based selection functions\n6Unlike the original ones, our implementation does not\nenforce capacity factor-based hard balancing.\nTable 4: Ablation studies. WT-S* is obtained by naively\nscaling NE in WT-S. More details in Sec. 6.3 & Tab. 10.\nDataset\nWT-S\nWT-S*\nWT-B\nE8\n# params. (in M)\n47\n238\n262\n41\nSwitch Transformer\n12.27\n11.24\n9.68\n1.08\nno dropout\n11.88\n11.10\n9.77\n1.10\nS-BASE (K=4, G=128)\n13.01\n10.96\n10.50\n1.17\nK = 1, G = 512\n12.32\n11.31\n9.77\n1.32\n\u03c3-MoE (K=4, G=128)\n11.59\n10.37\n9.44\n1.08\nstandard dropout\n12.01\n10.27\n9.53\n1.08\nsoftmax (renorm.)\n11.89\n11.27\n9.58\n1.09\nsoftmax (no renorm.)\n12.05\n10.54\n9.62\n1.09\nstandard init\n11.80\n10.59\n9.67\n1.08\nno regularization\n11.83\n10.41\n9.51\n1.08\nK = 8, G = 64\n11.63\n10.30\n9.58\n1.08\nK = 2, G = 256\n11.84\n10.44\n9.56\n1.09\nK = 1, G = 512\n11.90\n10.83\n9.58\n1.09\nTable 5: Perplexity of \u03c3-MoE compared to parameter-\nmatched baselines on C4 and peS2o datasets.\nDataset\nC4\nC4\npeS2o\npeS2o\ndmodel\n412\n1024\n412\n1024\n# params\n47M\n262M\n47M\n262M\nG\nK\nDense\n128\n1\n23.76\n17.79\n14.34\n10.91\n\u03c3-MoE\n128\n4\n23.25\n17.46\n14.12\n10.91\nSwitch\n512\n1\n24.47\n18.29\n14.74\n11.56\nS-BASE\n128\n4\n35.48\n18.53\n16.61\n11.72\n(with and without re-re-normalization) consistently\nperform worse than our sigmoid one. The same is\ntrue for the standard initialization ; ours is better.\nInterestingly, removing all regularization methods\ndegrades performance but does not entail catas-\ntrophic collapse even with NE = 128. We also\nexamine the best (G, K) combinations, given a con-\nstant number (G \u00b7 K) of active pairs ki, vi; we find\na high K = 4 works best within this range. Further\nanalysis of our \u03c3-MoE can be found in App. A.4.\nAnalyzing expert utilization.\nA typical failure\nmode of MoEs is expert collapse, where only a\nfew experts are used while others are completely\nignored or underused. Here we conduct an analysis\nto evaluate whether various models including ours\nare affected by this issue. For each layer, we com-\npute the proportion of the expert selection weights\nassigned to each expert (sel(x)) on the entire\nvalidation set of WikiText-103. We use WT-S*\nmodels from Tab. 4 with 128 experts. A represen-\ntative layer is shown in Fig. 3. Models with poor\nperformance (see Tab. 4), i.e., Switch Transformer\n(red) and a \u201cbad\u201d variant of \u03c3-MoE with a softmax\nand renormalization \u201csoftmax (renom.)\u201d (green),\n0\n16\n32\n48\n64\n80\n96\n112\nExpert\n10\u22126\n10\u22124\n10\u22122\nSelection proportion\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\nFigure 3: The total proportion of selection weights as-\nsigned to a given expert (x-axis; sorted by their popu-\nlarity) on the validation set of Wikitext-103 using the\nWT-S* models from Tab. 4. This is for one representa-\ntive layer (\u201cLayer 5\u201d; similar plots for other layers are\nshown in Fig. 7 in the appendix). The models with poor\nperformance (Tab. 4), i.e., Switch Transformer (red) and\n\u03c3-MoE with a softmax and renormalization \u201csoftmax\n(renom.)\u201d (green) can be easily identified. In contrast,\nthe fine performance differences between the rest of the\nmodels do not seem to be due to expert collapse.\ncan be easily identified: they severely suffer from\nthe expert collapse problem. The statistics are\nrather similar for all other models; the fine per-\nformance differences among these models do not\nseem to be due to expert collapse. Remarkably, our\nentropy-regularized models with expert dropout,\nespecially \u03c3-MoE, are capable of matching the ex-\npert usage balancing of S-BASE without using the\nSinkhorn activation function. Note that in general,\nwe do not consider uniform expert activation to be\noptimal: we expect expert specialization, and thus\nthe frequency of their usage should depend on the\noccurrence of the task they are performing.\n7\nConclusion\nOur novel view unifies methods that approximate\n2-layer MLPs,\nsuch as Top-K,\nMixture of\nExperts (MoE) and product-key memory (PKM)\nmethods. While Top-K by itself provides limited\nperformance improvements and speedups, further\nspeedup requires PKM or MoE. A non-competitive\nactivation function inspired by our unified view\nimproves both PKM and MoE. Further novel\nenhancements of MoEs yield our \u03c3-MoE which\noutperforms existing MoEs.\nImportantly, our\n\u03c3-MoE with moderate sparsity matches the\nperformance of parameter-equal dense baselines\nwhile being much more resource-efficient. Our\nnew insights improve the training of language\nmodels with limited hardware resources, making\nlanguage modeling research more accessible.\nLimitations\nOur experiments show that if we naively increase\nthe number of experts, the performance gap be-\ntween MoE models and their dense counterparts in-\ncreases. This indicates the need for careful control\nof sparsity and hyper-parameters, which remains a\nchallenge for MoEs.\nOur CUDA kernel is sub-optimal and I/O\nlimited.\nHowever, even in its current form, it\nalready yields significant performance boosts\nand memory reduction. We expect that an expert\nCUDA programmer could improve the speed of\nour kernel by at least a factor of 2.\nWe do not consider load balancing between hard-\nware accelerators as is done in Switch Transformers\nand S-BASE. Our goal is to make a larger model\nfit a single accelerator, or multiple accelerators in\nthe standard data-parallel training. Our preliminary\nexperiments suggest that such balancing entails a\nperformance hit.\nWe could not reproduce the 277M Enwik8\nmodel of Dai et al. (2019), because we could not\nfit the beaseline model on any of our machines.\nWe tried to use rotary positional encodings with\nPyTorch 2.0\u2019s memory-efficient attention to reduce\nit\u2019s memory consumption; however, this resulted\nin a significant performance degradation (even for\nthe smaller models).\nOur study focuses on end-to-end trainable\nMoEs. Other MoE methods (Irie et al., 2018; Li\net al., 2022) that pre-train LMs on disjoint data,\nto recombine them later into a single model, are\nout-of-scope.\nOur study only considers standard Transform-\ners; however, similar acceleration methods are of\nutmost importance for shared-layer Transformers,\nsuch as Universal Transformers (Dehghani et al.,\n2019) and NDRs (Csord\u00e1s et al., 2022). In fact,\nlayer sharing dramatically reduces the number of\nparameters. Compensating for this by naively in-\ncreasing dmodel or dff results in prohibitively high\nmemory overhead and slow execution. In contrast,\nMoEs allow increasing the number of parameters\nwithout such dramatic drawbacks. We leave shared-\nlayer MoEs for future work.\nAcknowledgements\nThis research was partially funded by ERC Ad-\nvanced grant no: 742870, project AlgoRNN, and\nby Swiss National Science Foundation grant no:\n200021_192356, project NEUSYM. We are thank-\nful for hardware donations from NVIDIA and IBM.\nThe resources used for this work were partially pro-\nvided by Swiss National Supercomputing Centre\n(CSCS) project s1154, s1205 and d123. Finally, we\nwould like to thank D\u00e1niel Ber\u00e9nyi for his support\nwith the CUDA kernel development.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015.\nNeural machine translation by jointly\nlearning to align and translate. In Int. Conf. on Learn-\ning Representations (ICLR), San Diego, CA, USA.\nTom B Brown et al. 2020. Language models are few-\nshot learners. In Proc. Advances in Neural Informa-\ntion Processing Systems (NeurIPS), Virtual only.\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016.\nLong short-term memory-networks for machine read-\ning. In Proc. Conf. on Empirical Methods in Natu-\nral Language Processing (EMNLP), pages 551\u2013561,\nAustin, TX, USA.\nZewen Chi, Li Dong, Shaohan Huang, Damai Dai,\nShuming Ma, Barun Patra, Saksham Singhal, Payal\nBajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and\nFuru Wei. 2022. On the representation collapse of\nsparse mixture of experts. In Proc. Advances in Neu-\nral Information Processing Systems (NeurIPS), New\nOrleans, Louisiana, USA.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s\nSarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J. Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In Int. Conf.\non Learning Representations (ICLR), Virtual only.\nAidan Clark, Diego de Las Casas, Aurelia Guy, Arthur\nMensch, Michela Paganini, Jordan Hoffmann, Bog-\ndan Damoc, Blake A. Hechtman, Trevor Cai, Se-\nbastian Borgeaud, George van den Driessche, Eliza\nRutherford, Tom Hennigan, Matthew J. Johnson,\nAlbin Cassirer, Chris Jones, Elena Buchatskaya,\nDavid Budden, Laurent Sifre, Simon Osindero, Oriol\nVinyals, Marc\u2019Aurelio Ranzato, Jack W. Rae, Erich\nElsen, Koray Kavukcuoglu, and Karen Simonyan.\n2022. Unified scaling laws for routed language mod-\nels. In Proc. Int. Conf. on Machine Learning (ICML).\nR\u00f3bert Csord\u00e1s, Kazuki Irie, and J\u00fcrgen Schmidhuber.\n2022. The neural data router: Adaptive control flow\nin transformers improves systematic generalization.\nIn Int. Conf. on Learning Representations (ICLR),\nVirtual only.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proc. Association for Com-\nputational Linguistics (ACL), pages 2978\u20132988, Flo-\nrence, Italy.\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,\nand Christopher R\u00e9. 2022. Flashattention: Fast and\nmemory-efficient exact attention with io-awareness.\nIn Proc. Advances in Neural Information Processing\nSystems (NeurIPS), New Orleans, Louisiana, USA.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Universal\nTransformers. In Int. Conf. on Learning Representa-\ntions (ICLR), New Orleans, LA, USA.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-\ncation for transformers at scale. In Proc. Advances in\nNeural Information Processing Systems (NeurIPS),\nNew Orleans, Louisiana, USA.\nNelson Elhage, Tristan Hume, Catherine Olsson,\nNicholas Schiefer, Tom Henighan, Shauna Kravec,\nZac Hatfield-Dodds, Robert Lasenby, Dawn Drain,\nCarol Chen, Roger Grosse, Sam McCandlish, Jared\nKaplan, Dario Amodei, Martin Wattenberg, and\nChristopher Olah. 2022. Toy models of superpo-\nsition. Transformer Circuits Thread.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parameter\nmodels with simple and efficient sparsity. Journal\nof Machine Learning Research (JMLR), 23(1):5232\u2013\n5270.\nJerry A Fodor and Zenon W Pylyshyn. 1988. Connec-\ntionism and cognitive architecture: A critical analysis.\nCognition, 28(1-2):3\u201371.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories. In Proc. Conf. on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 5484\u20135495, Punta Cana, Dominican Republic.\nXavier Glorot and Yoshua Bengio. 2010. Understanding\nthe difficulty of training deep feedforward neural\nnetworks. In Proc. Int. Conf. on Artificial Intelligence\nand Statistics (AISTATS), pages 249\u2013256, Sardinia,\nItaly.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving deep into rectifiers: Surpassing\nhuman-level performance on imagenet classification.\nIn Proc. IEEE Int. Conf. on Computer Vision (ICCV),\npages 1026\u20131034, Santiago, Chile.\nDieuwke Hupkes, Verna Dankers, Mathijs Mul, and Elia\nBruni. 2020. Compositionality decomposed: How\ndo neural networks generalise? Journal of Artificial\nIntelligence Research, pages 757\u2013795.\nKazuki Irie, Shankar Kumar, Michael Nirschl, and Hank\nLiao. 2018. RADMM: Recurrent adaptive mixture\nmodel with applications to domain robust language\nmodeling. In Proc. IEEE Int. Conf. on Acoustics,\nSpeech and Signal Processing (ICASSP), pages 6079\u2013\n6083, Calgary, Canada.\nAlekse\u02d8\u0131 Grigorievitch Ivakhnenko and Valentin Grig-\nor\u00e9vich Lapa. 1965. Cybernetic Predicting Devices.\nCCM Information Corporation.\nRobert A. Jacobs, Michael I. Jordan, Steven J. Nowlan,\nand Geoffrey E. Hinton. 1991. Adaptive mixtures of\nlocal experts. Neural Compututaion, 3(1):79\u201387.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and Fran\u00e7ois Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In Proc. Int. Conf. on Machine Learning\n(ICML), volume 119, pages 5156\u20135165, Virtual Only.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Int. Conf. on\nLearning Representations (ICLR), San Diego, CA,\nUSA.\nWouter Kool, Chris J Maddison, and Andriy Mnih. 2021.\nUnbiased gradient estimation with balanced assign-\nments for mixtures of experts.\nIn I (Still) Can\u2019t\nBelieve It\u2019s Not Better Workshop, NeurIPS, Virtual\nOnly.\nTaku Kudo and John Richardson. 2018. Sentencepiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProc. Conf. on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 66\u201371, Brussels,\nBelgium.\nGuillaume\nLample,\nAlexandre\nSablayrolles,\nMarc\u2019Aurelio Ranzato,\nLudovic Denoyer,\nand\nHerv\u00e9 J\u00e9gou. 2019. Large memory layers with prod-\nuct keys. In Proc. Advances in Neural Information\nProcessing Systems (NeurIPS), pages 8546\u20138557,\nVancouver, Canada.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. In Int. Conf. on\nLearning Representations (ICLR), Virtual only.\nMike Lewis, Shruti Bhosale, Tim Dettmers, Naman\nGoyal, and Luke Zettlemoyer. 2021. BASE layers:\nSimplifying training of large, sparse models. In Proc.\nInt. Conf. on Machine Learning (ICML), volume 139,\npages 6265\u20136274, Virtual only.\nMargaret Li, Suchin Gururangan, Tim Dettmers, Mike\nLewis, Tim Althoff, Noah A Smith, and Luke Zettle-\nmoyer. 2022. Branch-train-merge: Embarrassingly\nparallel training of expert language models. Preprint\narXiv:2208.03306.\nZonglin Li, Chong You, Srinadh Bhojanapalli, Daliang\nLi, Ankit Singh Rawat, Sashank J. Reddi, Ke Ye,\nFelix Chern, Felix Yu, Ruiqi Guo, and Sanjiv Kumar.\n2023. The lazy neuron phenomenon: On emergence\nof activation sparsity in transformers. In Int. Conf. on\nLearning Representations (ICLR), Kigali, Rwanda.\nPeter Pagin and Dag Westerst\u00e5hl. 2010. Compositional-\nity I: Definitions and variants. Philosophy Compass,\n5(3):250\u2013264.\nAnkur P. Parikh, Oscar T\u00e4ckstr\u00f6m, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In Proc. Conf.\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2249\u20132255, Austin, TX, USA.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In Proc. Advances in Neural Information Pro-\ncessing Systems (NeurIPS), pages 8024\u20138035, Van-\ncouver, Canada.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, H. Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, Eliza Rutherford, Tom Hennigan, Ja-\ncob Menick, Albin Cassirer, Richard Powell, George\nvan den Driessche, Lisa Anne Hendricks, Mari-\nbeth Rauh, Po-Sen Huang, Amelia Glaese, Jo-\nhannes Welbl, Sumanth Dathathri, Saffron Huang,\nJonathan Uesato, John Mellor, Irina Higgins, Antonia\nCreswell, Nat McAleese, Amy Wu, Erich Elsen, Sid-\ndhant M. Jayakumar, Elena Buchatskaya, David Bud-\nden, Esme Sutherland, Karen Simonyan, Michela Pa-\nganini, Laurent Sifre, Lena Martens, Xiang Lorraine\nLi, Adhiguna Kuncoro, Aida Nematzadeh, Elena\nGribovskaya, Domenic Donato, Angeliki Lazaridou,\nArthur Mensch, Jean-Baptiste Lespiau, Maria Tsim-\npoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sot-\ntiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong,\nDaniel Toyama, Cyprien de Masson d\u2019Autume, Yujia\nLi, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin,\nAidan Clark, Diego de Las Casas, Aurelia Guy, Chris\nJones, James Bradbury, Matthew J. Johnson, Blake A.\nHechtman, Laura Weidinger, Iason Gabriel, William\nIsaac, Edward Lockhart, Simon Osindero, Laura\nRimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub,\nJeff Stanway, Lorrayne Bennett, Demis Hassabis, Ko-\nray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling\nlanguage models: Methods, analysis & insights from\ntraining gopher. Preprint arXiv:2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research\n(JMLR), 21:140:1\u2013140:67.\nImanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber.\n2021. Linear transformers are secretly fast weight\nprogrammers. In Proc. Int. Conf. on Machine Learn-\ning (ICML), volume 139, pages 9355\u20139366, Virtual\nonly.\nJ\u00fcrgen Schmidhuber. 1991. Learning to control fast-\nweight memories: An alternative to recurrent nets.\nTechnical Report FKI-147-91, Institut f\u00fcr Informatik,\nTechnische Universit\u00e4t M\u00fcnchen.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proc. Association for Computa-\ntional Linguistics (ACL), pages 1715\u20131725, Berlin,\nGermany.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc Le, Geoffrey Hinton, and Jeff\nDean. 2017. Outrageously large neural networks:\nThe sparsely-gated mixture-of-experts layer. In Int.\nConf. on Learning Representations (ICLR), Toulon,\nFrance.\nKai Shen, Junliang Guo, Xu Tan, Siliang Tang, Rui\nWang, and Jiang Bian. 2023. A study on relu and\nsoftmax in transformer. Preprint arXiv:2302.06461.\nRichard Sinkhorn. 1964. A relationship between arbi-\ntrary positive matrices and doubly stochastic matri-\nces. The annals of mathematical statistics, 35(2):876\u2013\n879.\nRichard Sinkhorn and Paul Knopp. 1967. Concerning\nnonnegative matrices and doubly stochastic matrices.\nPacific Journal of Mathematics, 21(2):343\u2013348.\nLuca Soldaini and Kyle Lo. 2023. peS2o (Pretrain-\ning Efficiently on S2ORC) Dataset. Technical re-\nport, Allen Institute for AI. https://github.com/\nallenai/pes2o.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model.\nhttps://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. Preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proc. Advances in Neural Information\nProcessing Systems (NIPS), pages 5998\u20136008, Long\nBeach, CA, USA.\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8BERT: quantized 8bit BERT.\nIn Workshop on Energy Efficient Machine Learn-\ning and Cognitive Computing - NeurIPS, Vancouver,\nCanada.\nZhengyan Zhang, Yankai Lin, Zhiyuan Liu, Peng Li,\nMaosong Sun, and Jie Zhou. 2022. MoEfication:\nTransformer feed-forward layers are mixtures of ex-\nperts. In Proc. Findings of the Association for Com-\nputational Linguistics (ACL), pages 877\u2013890, Dublin,\nIreland.\nA\nFurther details and analyses\nA.1\nDefinition of normalised Top-K\nUsing the setting of Sec. 3.3, we define the normal-\nized top-K operation as follows:\nEx = arg topk(s, K)\n(23)\ntopk(s)[i] =\n(\ns[i]\nif i \u2208 Ex\n0\notherwise\n(24)\nnorm topk(s) =\ntopk(s)\nP\ni topk(s)[i]\n(25)\nA.2\nMeasuring the Number of Active\nChannels in u\nIn order to explore whether a (ki - vi) sparsity-\nbased approach is feasible, we measure the number\nof nonzero entries in the up-projected vector u in\nour baseline models (which, because of the ReLU\nactivation function, is the same as the positive en-\ntries). We show the results of our 47M model in Fig.\n1. Note that dff = 2053 (See Tab. 8) for the same\nmodel, which means that on average only 1-10% of\nthe channels are active. We show the same analysis\nfor the 262M model in Fig. 4. Interestingly, the\ncounts remain the same, even though dff = 4110\nfor this model. The 41M parameter model on En-\nwik8 shows a stark difference in the distribution\nof the channels between layers; see Fig. 5. This\nsuggests that the key factor determining the count\ndistribution is the dataset, and the size of the model\nplays only a secondary role. Fortunately, the spar-\nsity is very high for all models considered.\nA.3\nMore Details and Results on PKM\nOur PKM (Sec. 3.2) is based on Lample et al.\n(2019) with the following basic modifications.\nFirst, we do not use batch normalization (BN). As\nLample et al. (2019) shows that BN is only benefi-\ncial for models with a very large memory size, we\nremove it as it simplifies inference where the effec-\ntive batch size varies over time. Also, we directly\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\nLayer\n0\n200\n400\nActive channels\nFigure 4: Number of active channels in u in our dense\n262M parameter model on Wikitext-103. dff = 4110\nfor this model, so the sparsity is below \u223c 5%. Standard\ndeviation over all tokens of the test and validation set.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\nLayer\n0\n100\n200\n300\nActive channels\nFigure 5: Number of active channels in u in our dense\n41M parameter model on Enwik8. dff = 2053 for this\nmodel, thus the sparsity is below \u223c 15%. Standard\ndeviation over all tokens of the test and validation set.\ndivide the input vectors into two sub-keys without\nan additional projection. Finally, unlike Lample\net al. (2019), we use the same learning rate for all\nparts of the network.\nIn addition to the parameter-equal comparison\nof Sec. 6.2, there is another possibly \u201cfair\u201d way of\nsetting the size of the PKM-based model: match\nthe number of values (this would result in fewer pa-\nrameters because of the key approximation), even\nthough Elhage et al. (2022) suggest that the keys\ntypically play a vital role, and reducing their capac-\nity will cause a performance loss. See Tab. 6 for\nthe corresponding results. Note that, for Enwik8\nand Wikitext-103 small, the parameter-equal set-\nting increases the number of sub-keys from 46 to\n62 (2116 vs. 3844 values). This helps significantly.\nA.4\nFurther Analyses of Our \u03c3-MoE\nWe also examine the best (G, K) given a constant\nnumber (G\u00b7K) of active pairs ki, vi. In this setting,\nreducing K by a factor of m (K\u2032 = K\nm) involves\nincreasing G (G\u2032 = mG), which, for a constant\nnumber of parameters, reduces NE to N\u2032\nE = NE\nm .\nThe results can be seen in the 2nd block of Tab. 10.\nWe find that a higher K is beneficial. Given this, we\nask the question how the selection distribution of\nthe models with K > 1 is different from selecting\nthe same experts together and acting as a larger\nTable 6: The performance of the PKM model variants. Both value-count and parameter-matched variants are shown.\nAdditionally, we show the effect of the initialization inspired by our unified view, which is marginal for PKMs.\nVariant\nSetting\nNonlinearity\nWT-S\nWT-M\nE8\nDense Baseline\nReLU\n11.81\n9.46\n1.08\nPKM\nvalue-count\nSoftmax\n14.11\n11.29\n1.20\nPKM\nvalue-count\nReLU\n13.32\n10.16\n1.12\nPKM\n# total params.\nSoftmax\n13.96\n11.10\n1.16\nPKM\n# total params.\nReLU\n12.77\n9.98\n1.11\nPKM + init\n# total params.\nReLU\n12.75\n9.96\n1.11\nexpert. Are these models combining experts in\nmore meaningful ways? To test this, we measure\nthe distribution of experts that are used together\non Wikitext-103 with our 47M MoE model with\nK = 4. The result can be seen in Fig. 6: the\nnetwork combines experts in a rich way, further\nsupporting the use of K > 1. Note that, it remains\nan open question whether such \u201ccompositions\u201d may\nhelp the generalization and compositional behavior\nof the network (Fodor and Pylyshyn, 1988; Pagin\nand Westerst\u00e5hl, 2010; Hupkes et al., 2020).\n1\n5\n9\n13\nExpert ID\n1\n5\n9\n13\nExpert ID\n0.00\n0.05\n0.10\n0.15\n0.20\nFigure 6: Expert co-occurrence in a \u03c3-MoE model with\nNE = 16 experts and K = 4. Each row shows the\ndistribution of experts used together with the one corre-\nsponding to the row. Measured on the validation set of\nWikitext-103 in the 3rd layer of our 47M \u03c3-MoE model.\nThe other layers and models behave qualitatively the\nsame.\nDetailed Usage Count Analysis.\nWe show the\nrelative proportion of experts selected for all layers\nin Fig. 7. For more details, please refer to Sec. 6.3.\nA.5\nMore on Resource Efficiency\nFor execution time and memory usage, both the\ndense MLP and the MoE layers are linear in dmodel\n(Fig. 11), the MLP is linear in dff, and MoE is\nlinear in G (Fig. 10) and K. For the same number\nof parameters (except for the selection network,\nwhich is negligible), dmodel = G \u00b7 NE. However,\nboth the memory usage and the execution time of\nthe MoE are almost independent of NE, except for\na small linear factor due to the selection network\n(see Fig. 9). Figures 2, 9, 10 and 11 show the actual\nmeasured execution time and memory usage on a\nRTX 3090 GPU.\nNote that there is no significant difference in\nterms of speed and memory usage between differ-\nent MoE variants given the same dmodel, G, and\nK. This is because they only differ in the selection\nmechanism and regularization, and not in the way\nthe experts are executed. Since all methods are\nconfigured to have the same number of parameters\nas the dense baselines, and K experts are used in\nparallel, the factor of reduction in both FLOPs and\nmemory usage is given by K\nNE . We show this factor\nfor all models in Tab. 7.\nB\nImplementation details\nWe train all of our models for 100k steps with co-\nsine learning rate decay, starting from the initial\nlearning rate of 0.00025 and decaying to 0. We use\nthe Adam optimizer (Kingma and Ba, 2015) with\ndefault PyTorch parameters (Paszke et al., 2019).\nWe use gradient clipping with a max gradient norm\nof 0.25. We show the other hyperparameters of\nour dense models in Tab. 8. We train our models\nwith an XL memory of the same size as the context\nsize. However, following Dai et al. (2019), we eval-\nuate the models using a longer memory. Unlike\nthe hyperparameter-tuned memory sizes in Trans-\nformer XL, we use 4 times the context size (this\napproximates the size of the memory by Dai et al.\n(2019), while being simple).\nThe hyperparameters of the MoE models match\nthose of their dense counterparts with the same\nnumber of parameters, except for the MoE-specific\nones, which are shown in Tab. 9. \u03b4 denotes the\nexpert dropout and \u03b3 denotes the regularization\nTable 7: The relative amount of FLOPs and memory used by the feedforward block of the MoE transformer\ncompared to its dense counterpart. The same configurations are shown as in Tab. 10.\nDataset\nWikitext 103\nWikitext 103\nWikitext 103\nEnwik8\ndmodel\n412\n412\n1024\n512\n# params\n47M\n237M\n262M\n41M\nG\nK\n\u03c3-MoE (ours)\n128\n4\n25.0%\n3.1%\n12.5%\n25.0%\nstandard dropout\n128\n4\n25.0%\n3.1%\n12.5%\n25.0%\nsoftmax (after top-k)\n128\n4\n25.0%\n3.1%\n12.5%\n25.0%\nsoftmax (before top-k)\n128\n4\n25.0%\n3.1%\n12.5%\n25.0%\nstandard init\n128\n4\n25.0%\n3.1%\n12.5%\n25.0%\nno reg (\u03b3 = 0, \u03b4 = 0)\n128\n4\n25.0%\n3.1%\n12.5%\n25.0%\nK = 8, G = 64\n64\n8\n25.0%\n3.1%\n12.5%\n25.0%\nK = 2, G = 256\n256\n2\n25.0%\n3.1%\n12.5%\n25.0%\nK = 1, G = 512\n512\n1\n25.0%\n3.1%\n12.5%\n25.0%\nN \u2032\nE = 2NE, G = 64\n64\n4\n12.5%\n1.6%\n-\n12.5%\nK = 1\n128\n1\n6.2%\n0.8%\n-\n6.2%\nK = 2\n128\n2\n12.5%\n1.6%\n-\n12.5%\nK = 8\n128\n8\n50.0%\n6.2%\n-\n50.0%\nSwitch, K = 1, G = 512\n512\n1\n25.0%\n3.1%\n12.5%\n25.0%\nno dropout\n512\n1\n25.0%\n3.1%\n12.5%\n25.0%\nK = 4, G = 128\n128\n4\n25.0%\n3.1%\n-\n25.0%\nK = 1, G = 128\n128\n1\n6.2%\n0.8%\n-\n6.2%\nno dropout\n128\n1\n6.2%\n0.8%\n-\n6.2%\nS-BASE\n128\n4\n25.0%\n3.1%\n12.5%\n25.0%\nK = 1, G = 512\n512\n1\n25.0%\n3.1%\n12.5%\n25.0%\nstrength used for the loss L (See Eq. 21). For the\nnon-MoE layers, the same dropout is used as for\nthe baselines. For Switch Transformers, we use\n\u03b3 = 0.01 with regularization of the form presented\nin Eq. 17, following Fedus et al. (2022). The other\nvariants, including S-BASE, use the regularizer\nproposed by us (Eq. 21).\nOur small PKM models use 46 subkeys resulting\nin 462 = 2116 values for the dff-matched case\nand 62 subkeys (3844 values) for the parameter-\nmatched case. The PKM equivalent of the 262M\nparameter model on Wikitext-103 has 64 subkeys\n(4096 values) for the dff-matched and 89 subkeys\n(7921 values) for the parameter-matched case. The\nPKM models do not use dropout in the PKM layers,\nand have 4 heads.\nB.1\nA Few Words on the CUDA Kernel\nWe call the key operation for our MoE layers con-\nditional vector-matrix multiplication, or CVMM,\nand we define it as follows. Given a batch of vec-\ntors, V \u2208 RN\u00d7M, where N is the batch size and\nM is the number of channels, a set of K matri-\nces M \u2208 RK\u00d7M\u00d7L and selection indices S \u2208\n{0, ..., K \u2212 1}N, CVMM(V , S, M) \u2208 RN\u00d7L is:\nCVMM(V , S, M)[n, l] =\n(26)\nM\u22121\nX\nm=0\nV [n, m]M[S[n], m, l]\nOur CUDA kernel is based on the blog post\ndeveloping a matrix multiplication kernel by Si-\nmon Boehm (https://siboehm.com/articles/\n22/CUDA-MMM). However, there are major differ-\nences: unlike standard matrix multiplication, in our\ncase, different matrices could be used for different\nbatch elements of the input. In order to be able\nto reuse matrices fetched from the global memory\nof the GPU, we first do a preprocessing step: we\nsort the selection indices, and obtain a reordering\nvector. This gives us an ordering of the input and\noutput batch elements, such that the consecutive\nindices are multiplied by the same matrix with high\nprobability. Fortunately, multiple channels have to\nbe fetched/written out at once, so this reordering\nhas minimal overhead. Our kernel has an addi-\ntional grid dimension compared to standard ma-\ntrix multiplication, iterating over the matrix index,\nk \u2208 {0, ..., K \u2212 1}. We find that skipping matrices\nthat do not have any corresponding inputs has min-\nimal overhead. To avoid checking all elements of\nthe reordering vector, we precompute their offsets.\nTable 8: Hyperparameters of dense baselines and their MoE counterparts. For the MoE-specific hyperparameters,\nplease refer to Tab. 9. \u201cSetencePiece\u201d tokenization is used for Wikitext-103, C4 and PES2O datasets, and \u201cCharacter\u201d\nfor Enwik8.\nTokenization\n#params\ndmodel\ndff\nnlayers\nnheads\nhead size\ncontext size\nbatch size\ndropout\nlr warmup\nSentencePiece\n47M\n412\n2053\n16\n10\n41\n256\n64\n0.1\n-\nSentencePiece\n238M\n412\n16480\n16\n10\n41\n256\n64\n0.1\n-\nSentencePiece\n262M\n1024\n4110\n18\n16\n64\n512\n64\n0.2\n4000\nCharacter\n41M\n512\n2053\n12\n8\n64\n512\n32\n0.1\n-\nTable 9: MoE-specific hyperparameters for different model variants. \u03b3 denotes the scaler for the load balancing\nterm in the loss and \u03b4 is the probability of the expert dropout. The standard, transformer-specific hyperparameters\nare the same as for the baselines. Please refer to Tab. 8. \u201cSetencePiece\u201d tokenization is used for Wikitext-103, C4\nand PES2O datasets, and \u201cCharacter\u201d for Enwik8.\nTokenization\n#params\ndmodel\nNE\nG\nK\n\u03b4\n\u03b3\nSentencePiece\n47M\n412\n16\n128\n4\n-\n0.001\nSentencePiece\n237M\n412\n128\n128\n4\n0.05\n0.001\nSentencePiece\n262M\n1024\n32\n128\n4\n0.2\n0.001\nCharacter\n41M\n512\n16\n128\n4\n0.05\n0.0001\nOur kernel uses shared memory and register\ncaching; however, it does not use asynchronous\nloads, which makes it I/O bound. It also does not\nsupport tensor cores and mixed precision. The\npre-processing step uses the radix sort from the\nCUB library. However, computing the offsets re-\nquires counting the number of vectors assigned to\na single matrix. This information, as well as the\noffset, which is their sum, are freely available as\nsub-results that the radix sort computes anyways;\nhowever, we found no way of extracting it from\nthe CUB implementation. We estimate that by\nimplementing a more efficient preprocessing step,\nasynchronous loads, and tensor core support, our\nkernel can be further accelerated by a factor of two.\nTriton kernel.\nWe added a new Triton-based im-\nplementation that supports mixed precision. How-\never, due to Triton limitations, it only supports\nVolta or newer GPU generations. For more details,\nsee https://github.com/robertcsordas/moe_\nlayer.\nB.2\nAdditional Results on MoEs\nAdditional results of different MoE variants with\nmore model details are shown in Tab. 10. We repeat\nthe entries from Tab. 4 for easier comparison.\nTable 10: Detailed ablation results. WT-S* is obtained by naively scaling NE in WT-S. More details in Sec. 6.3. We\ndo not evaluate all versions of the 262M Wikitext-103 model due to its long training time. However, we aim to\ninclude what we believe are the most interesting variants. \u03b3 = 0 means no regularization applied to the selection\nscores (See Eq. 21), \u03b4 = 0 denotes no expert dropout.\nVariant\nWT-S\nWT-S*\nWT-B\nE8\ndmodel\n412\n412\n1024\n512\n# params\n47M\n237M\n262M\n41M\nG\nK\n\u03c3-MoE (ours)\n128\n4\n11.59\n10.37\n9.44\n1.08\nstandard dropout\n128\n4\n12.01\n10.27\n9.53\n1.08\nsoftmax (after top-k)\n128\n4\n11.89\n11.27\n9.58\n1.09\nsoftmax (before top-k)\n128\n4\n12.05\n10.54\n9.62\n1.09\nstandard init\n128\n4\n11.80\n10.59\n9.67\n1.08\nno reg (\u03b3 = 0, \u03b4 = 0)\n128\n4\n11.83\n10.41\n9.51\n1.08\nK = 8, G = 64\n64\n8\n11.63\n10.30\n9.58\n1.08\nK = 2, G = 256\n256\n2\n11.84\n10.44\n9.56\n1.09\nK = 1, G = 512\n512\n1\n11.90\n10.83\n9.58\n1.09\nN \u2032\nE = 2NE, G = 64\n64\n4\n11.81\n10.53\n-\n1.08\nK = 1\n128\n1\n12.26\n11.30\n-\n1.09\nK = 2\n128\n2\n11.90\n10.66\n-\n1.09\nK = 8\n128\n8\n11.58\n10.22\n-\n1.08\nSwitch, K = 1, G = 512\n512\n1\n12.27\n11.24\n9.68\n1.08\nno dropout\n512\n1\n11.88\n11.10\n9.77\n1.10\nK = 4, G = 128\n128\n4\n12.05\n11.37\n-\n1.10\nK = 1, G = 128\n128\n1\n12.61\n11.89\n-\n1.11\nno dropout\n128\n1\n12.35\n11.78\n-\n1.10\nS-BASE, K = 4, G = 128\n128\n4\n13.01\n10.96\n10.50\n1.17\nK = 1, G = 512\n512\n1\n12.32\n11.31\n9.77\n1.32\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nSelection proportion\nLayer 0\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nLayer 1\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nSelection proportion\nLayer 2\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22127\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nLayer 3\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nSelection proportion\nLayer 4\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22127\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nLayer 5\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nSelection proportion\nLayer 6\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nLayer 7\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22127\n10\u22125\n10\u22123\n10\u22121\nSelection proportion\nLayer 8\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nLayer 9\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nSelection proportion\nLayer 10\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nLayer 11\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22127\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nSelection proportion\nLayer 12\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nLayer 13\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\nExpert\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nSelection proportion\nLayer 14\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\n0\n16\n32\n48\n64\n80\n96\n112\nExpert\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\nLayer 15\n\u03c3-MoE\n\u03c3-MoE - softmax (no renorm.)\n\u03c3-MoE - softmax (renorm.)\nSwitch Transformer\nS-BASE (K=4, G=128)\nFigure 7: Total proportions of selection weights assigned to a given expert (x-axis) on the validation set of Wikitext-\n103 using the WT-S* models from Tab. 4. Experts are sorted by their popularity. All layers are shown (\u201cLayer 5\u201d\nis also shown in Fig. 3 in the main text). The models with poor performance can be distinguished easily (Switch\nTransformer and \u03c3-MoE with a softmax and renormalization, \u201csoftmax (renom.)\u201d). Their poor performance may be\npartially explained by expert collapse. In contrast, the fine performance differences between the rest of the models\ndo not seem to be due to the expert collapse phenomenon.\n256\n1024\n2048\n4096\ndmodel\n0\n100\n200\n300\nTime (ms)\nMLP\nMoE\n0.0\n0.5\n1.0\nMemory (GB)\nMLP\nMoE\nFigure 8: Execution time and memory usage of a\nforward-backward pass of a single MLP and MoE layer.\n|B| = 32768, corresponding to a batch size 64 and\nsequence length 512, dmodel indicated on the x-axis,\nK = 4, NE = 32, dff = 4dmodel, and G =\ndff\nNE (fol-\nlowing the standard 1x-4x-1x shape). Full/dashed lines\nshow the execution time/memory consumption, respec-\ntively. Shows our mixed-precision Triton implementa-\ntion on an RTX 3090 with PyTorch 2.1 and CUDA 12.\n8 32\n64\n128\n256\nNumber of experts (NE)\n0\n50\n100\nTime (ms)\nMLP\nMoE\n0\n1\n2\nMemory (GB)\nMLP\nMoE\nFigure 9: Execution time and memory usage of a\nforward-backward pass of a single MLP and MoE layer.\n|B| = 32768, corresponding to a batch size 64 and\nsequence length 512, dmodel = 512, K = 4, and\ndff = G \u00b7 NE. Full/dashed lines show the execution\ntime/memory consumption, respectively. As they are\nboth linear with similar slopes, they are almost indistin-\nguishable. Shows our mixed-precision Triton implemen-\ntation on an RTX 3090 with PyTorch 2.1 and CUDA 12.\n64\n256\n512\n1024\nExpert size (G)\n50\n100\nTime (ms)\nMLP\nMoE\n0\n1\n2\nMemory (GB)\nMLP\nMoE\nFigure 10: Measured execution time and memory us-\nage of a forward-backward pass of a single MLP and\nMoE layer. |B| = 32768, corresponding to the realistic\nscenario of a batch size 64 and sequence length 512,\ndmodel = 512, K = 4, NE = 32 and dff = G \u00b7 NE.\nFull lines show the execution time, and dashed ones\nthe memory consumption. Because they are both linear\nwith similar slopes, they are almost indistinguishable.\nShows our mixed-precision Triton implementation on\nan RTX 3090 with PyTorch 2.1 and CUDA 12.\n128\n512\n1024\n2048\ndmodel\n20\n40\nTime (ms)\nMLP\nMoE\n0.1\n0.2\n0.3\nMemory (GB)\nMLP\nMoE\nFigure 11: Measured execution time and memory us-\nage of a forward-backward pass of a single MLP and\nMoE layer. |B| = 32768, corresponding to the real-\nistic scenario of a batch size 64 and sequence length\n512, K = 4, NE = 32, G = 128 and dff = G \u00b7 NE.\nFull lines show the execution time, and dashed ones\nthe memory consumption. Shows our mixed-precision\nTriton implementation on an RTX 3090 with PyTorch\n2.1 and CUDA 12.\n"
  },
  {
    "title": "TEQ: Trainable Equivalent Transformation for Quantization of LLMs",
    "link": "https://arxiv.org/pdf/2310.10944.pdf",
    "upvote": "9",
    "text": "TEQ: Trainable Equivalent Transformation for Quantization of LLMs\nWenhua Cheng and Yiyang Cai and Kaokao Lv and Haihao Shen\nIntel\n{wenhua.cheng, yiyang.cai, kaokao.lv, haihao.shen}@intel.com\nAbstract\nAs large language models (LLMs) become\nmore prevalent, there is a growing need for\nnew and improved quantization methods that\ncan meet the computationalast layer demands\nof these modern architectures while maintain-\ning the accuracy. In this paper, we present TEQ,\na trainable equivalent transformation that pre-\nserves the FP32 precision of the model out-\nput while taking advantage of low-precision\nquantization, especially 3 and 4 bits weight-\nonly quantization.\nThe training process is\nlightweight, requiring only 1K steps and less\nthan 1\u2030 of the original model\u2019s trainable pa-\nrameters. Furthermore, the transformation does\nnot add any computational overhead during in-\nference. Our results are on-par with the state-of-\nthe-art (SOTA) methods on typical LLMs. Our\napproach can be combined with other methods\nto achieve even better performance. The code\nis available at https://github.com/intel/neural-\ncompressor.\n1\nIntroduction\nLarge language models (LLMs) have not only\nshown breakthrough performance in a wide range\nof benchmarks and tasks but played an increasingly\nimportant role in daily life, e.g., ChatGPT (Ope-\nnAI) in information retrieval and Copilot (Github)\nin programming. However, as LLMs\u2019 model size\nkeeps growing dramatically, their significant mem-\nory footprint and heavy computation requirements\nbecome a major bottleneck of their usage.\nOne of the most promising ways to alleviate this\nchallenge is quantization, which can reduce stor-\nage and computational overhead. Quantization con-\nverts high-bit floating-point data to lower-bit rep-\nresentations, and it has become an effective model\ncompression technique.\nQuantization methods can generally be divided\ninto two categories: quantization aware training\n(QAT) (Shen et al., 2021; Zhuang et al., 2021; Gong\net al., 2019; Esser et al., 2019; Louizos et al., 2018)\nand post-training quantization (PTQ) (Frantar et al.,\n2022; Li et al., 2022; Xiao et al., 2022; Wei et al.,\n2022; Frantar and Alistarh, 2022; Hubara et al.,\n2021; Nagel et al., 2020; Hassibi et al., 1993; Le-\nCun et al., 1989). Their effectiveness has been\nvalidated for a wide range of models. However, sev-\neral issues still need to be addressed, especially for\nLLMs. QAT simulates the quantization behavior\nin the training/finetuning phase, but such a process\nis very costly for LLMs due to their unprecedented\nparameter scale. In contrast, PTQ requires no train-\ning and thus has drawn rising attention. However,\nPTQ is prone to large accuracy drops, especially\nfor extreme low-bit quantization. This provides\nLLMs\u2019 PTQ methods with great opportunities for\nimprovement.\nLower-bit quantization (e.g., Int4, W4) has re-\ncently been widely discussed since memory band-\nwidth is becoming the main bottleneck of LLMs.\nHowever, most existing works focus on computer\nvision models (He et al., 2016; Howard et al.,\n2017) that are much smaller than current popular\nLLMs such as BLOOM-176B(Scao et al., 2022),\nOPT-175B(Zhang et al., 2022). Other extreme\nquantization methods (Bai et al., 2020; Zhang\net al., 2020) rely on the knowledge distillation tech-\nnique, introducing extra overhead. GPTQ(Frantar\net al., 2022) tunes the weights based on optimal\nbrain surgeon(Hassibi et al., 1993) and success-\nfully achieves low-bit quantization on LLMs with\nlow computation overhead.\nOur proposed method reduces the compression\nerror by introducing a trainable equivalent transfor-\nmation (Fig. 1), which keeps the mathematical\nequivalency of model output at FP32 precision.\nMoreover, the training cost is significantly low,\nonly 1k steps of batch size 1 with around less than\none-thousandth trainable parameters of the original\nmodels. Also, our method is orthogonal to current\npopular LLMs quantization methods, and better\naccuracy results could be achieved by combining\narXiv:2310.10944v1  [cs.CL]  17 Oct 2023\nours with them.\nIn summary, the contribution of this paper is\nthreefold:\n\u2022 We introduce a trainable equivalent transfor-\nmation for the quantization of LLMs, which\nkeeps the model output unchanged at FP32\nprecision.\nBesides, the training is quite\nlightweight.\n\u2022 Experimental results show our method could\nachieve results on par with or better than the\nSOTA methods.\n\u2022 We also show that our method could be com-\nbined to get the new SOTA performance.\nIn the following, we first briefly introduce the\nwork related to ours in Section 2. We then present\nthe trainable equivalent transformation in Section\n3. Experiments and conclusion are described in\nSections 4 and 5 respectively.\n2\nRelated Work\nQuantization-aware Training.\nQAT methods\nare widely used in model compression. By en-\nabling finetuning process, quantized models\u2019 accu-\nracy can often be on par with or even better than\nthose of original models. (Louizos et al., 2018)\nintroduce a differentiable quantization procedure\nby converting original weights and activations\u2019 dis-\ntribution to categorical distributions. OQAT (Shen\net al., 2021) proposes a combined training scheme\nof architecture and quantization to acquire many\nquantized models. Afterward, they are converted\nto lower-bit models and optimized. (Zhuang et al.,\n2021) propose a progressive quantization scheme\nby quantizing activations after weights. Indeed,\nQAT methods are popular in relatively small-scale\nmodels, but their application in LLMs is limited\ndue to the expensive training or even fine-tuning\ncosts as mentioned in Section 1.\nPost-training Quantization.\nA large number of\npost-training methods quantize weights step by step\nand modify unquantized weights to compensate for\nerrors produced by previously quantized weights.\nOptimal Brain Damage (OBD) (LeCun et al., 1989)\nuses second-derivative information (Hessian-based\nestimation) to predict the effect of weights\u2019 pertur-\nbation analytically. Optimal Brain Surgeon (OBS)\n(Hassibi et al., 1993) applies such an idea by devis-\ning a second-order framework for weight pruning.\nAfterward, Optimal Brain Quantization (OBQ) mi-\ngrate OBS\u2019s pruning framework to quantization\nsince pruning and quantization share the common\nidea of introducing perturbation in original mod-\nels. Finally, GPTQ (Frantar et al., 2022) improves\nthe original framework\u2019s efficiency by fixing the\nquantization order within the layer and calculating\nthe Hessian matrix\u2019s Cholesky decomposition be-\nfore quantization. Other PTQ methods use a better\nrounding scheme than commonly used rounding-\nto-nearest (RTN). AdaRound (Nagel et al., 2020)\nlearns a rounding scheme using mean squared er-\nror (MSE) for layer-wise activation. AQuant (Li\net al., 2022) adds a learnable border function for\nactivation quantization.\nLarge Language Model Quantization.\nRe-\nsearchers are devoting efforts to compression\nmethods particularly designed for LLMs as more\nopen-source releases are available. LLM.int8()\n(Dettmers et al., 2022) discovers peak values in\nactivation outliers\u2019 particular channels. It proposes\nmethods to ensure that these channels are kept in\nhigher precision. SmoothQuant (Xiao et al., 2022)\naddresses the issues mentioned above by migrat-\ning difficulties from activation to weights with a\nhandcrafted equivalent transformation. ZeroQuant\n(Yao et al., 2022) devises an end-to-end quantiza-\ntion and inference pipeline with a novel layer-wise\nknowledge distillation algorithm. However, the\nlargest model it has quantized has only 1.3B pa-\nrameters. GPTQ (Frantar et al., 2022) tunes the\nweights based on optimal brain surgeon (Hassibi\net al., 1993) and successfully achieves low-bit quan-\ntization on LLMs with low computation overhead.\nMore recent, AWQ (Lin et al., 2023) propose to\nsearch the optimal scales to protect parts of weights,\nsince they can significantly reduce the error caused\nby quantization.\n3\nMethodology\nFigure 1 presents a schematic illustration of equiv-\nalent transformation. In the following, we intro-\nduce the quantization process first. Consider a\nfeed-forward neural network comprised of L lay-\ners, which perform matmul or convolution opera-\ntions. We only consider the matmul layer for sim-\nplicity since our method could be easily extended\nto convolution layers. The lth matmul operation\ncan be denoted by yl = wl \u00b7 xl. In which wl and\nxl are the weights and activation(input), and yl is\nthe corresponding output. To quantize a tensor, a\nFigure 1: A schematic illustration of TEQ, where sw1\nand sw2 are trainable parameters. A per-channel scale is\nmultiplied at activations while an inverse scale is multi-\nplied at weights, which could keep the output equivalent.\nquantization op presented below could be applied.\nQ(v) = clip(\nhv\ns\ni\n, \u2212n, n), n \u2208 N\n(1)\nwhere s denotes the quantization scale parameter\nand [\u00b7] denotes the round-to-nearest (RTN) opera-\ntion, while \u2212n and n denote the integer thresholds\nfor clipping. We ignore the zero point for simplic-\nity. For a normal int8 quantization, i.e., W8A8,\nwe need to quantize activation and weights both.\nAnd for weight-only quantization, only the weights\nneed to be quantized. Finally, a de-quantization\noperation will be appended to reconstruct the float\noutput, normally not equal to yl. In summary, the\nLl\u2019s output after normal quantization is converted\nto:\n\u02c6yl = Q\u22121(Q(wl) \u00b7 Q(xl))\n(2)\nwhere \u02c6yl denotes the Ll\u2019s reconstructed output after\nquantization. The value of (yl \u2212 \u02c6yl)2 is usually\nnamed as quantization loss.\n3.1\nTrainable Equivalent Transformation\nPTQ tends to cause a noticeable accuracy drop\nas mentioned before. SmoothQuant (Xiao et al.,\n2022) and AWQ (Lin et al., 2023) rely on hand-\ncrafted rules to migrating quantization difficulties\nof weights and activations. However, these rules\noften fall into sub-optimal solutions, which cannot\nminimize error caused by quantization. To allevi-\nate this issue, we introduce a trainable equivalent\ntransformation that enforces the Fp32 output as the\nsame but greatly improves the quantization robust-\nness. To be more specific, suppose the shape of\nwl is cin\nl \u00d7 cout\nl\n, which stands for their respective\ninput and output channel numbers. For each layer\nLl, we can multiply a per-channel scaling vector\nsl \u2208 Rcin\nl for weights and append a corresponding\ninverse scale vector for activation. Mathematically,\nthis can be restated as\nyl = wl \u00b7 diag(sl) \u00b7 diag(sl)\u22121 \u00b7 xl\n(3)\noperator diag(\u00b7) denotes converting a column/row\nvector to a diagonal matrix whose eigenvalues are\nidentical to the original vector\u2019s elements.\ndiag\n\uf8eb\n\uf8ec\n\uf8ec\n\uf8ec\n\uf8ed\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\ns1\ns2\n...\nsn\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n\uf8f6\n\uf8f7\n\uf8f7\n\uf8f7\n\uf8f8 =\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8f0\ns1\ns2\n...\nsn\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fb\n(4)\nOur observation shows the optimal sw is useful\nto reduce the quantization loss. Therefore, we quan-\ntize the transformed model rather than the original\none.\nThe transformation has two per-channel scale\noperations, which will introduce computation over-\nhead. We fuse the weight scale to the weight it-\nself. For the activation scale, following (Xiao et al.,\n2022), we fuse it to the previous layers, such as\nlayernorm(Ba et al., 2016), batchnorm(Ioffe and\nSzegedy, 2015) and etc. In all our experiments, we\nonly apply the transformation to the layer whose\nscales could be fused, which introduces no extra\noverhead at deployment.\n3.2\nTraining Details\nWe train the scales sl because there is little knowl-\nedge of the best equivalent transformation due to\nvarious models and quantization configurations.\nIt\u2019s worth mentioning that the count of trainable\nscales is much less than the model\u2019s parameters,\nand the model weights are frozen.\nTo train the transformation scales, we follow the\nbasic QAT to simulate the quantization behavior,\nwhich could be denoted as\nylq = (Q\u22121Q(wl))(Q\u22121Q(xl))\n(5)\nFor weight-only quantization, activation quanti-\nzation will be ignored. We adopt straight-through\nestimator (STE) (Bengio et al., 2013) to backward\nthe gradients.\nWe use Adam(Kingma and Ba, 2014) optimizer,\nbetas [0.9, 0.9], and weight decay 0. The learning\nrate is 1e-3 unless explicitly stated and the decay\ntype is linear. We only train 1000 steps. We use\nthe same loss function as the original one in the\ntraining phase. For example, CrossEntorpy loss\nis adopted for LLMs. The sl is usually initialized\nwith 1. However, sometimes 1.0/sqrt(wcin) leads\nn_bits\nMethods\nOPT-6.7B\nOPT-13B\nBLOOM-3B\nBLOOM-7B1\nLLAMA-7B\nLLAMA-13B\n32\nFP32\n64.97\n65.54\n55.65\n60.29\n68.87\n71.06\n4\nRTN\n62.99\n64.17\n53.17\n57.80\n67.41\n68.86\nGPTQ\n63.09\n64.83\n54.65\n58.26\n64.70\n70.00\nOurs\n63.30\n64.91\n53.83\n58.93\n67.71\n69.55\nOurs+GPTQ\n63.94\n65.03\n54.42\n59.62\n65.27\n69.73\n4_g128\nRTN\n64.04\n64.88\n54.91\n59.32\n67.87\n70.88\nGPTQ\n64.76\n65.37\n55.68\n59.59\n66.33\n70.92\nOurs\n64.11\n64.87\n54.98\n59.35\n68.10\n71.00\nOurs+GPTQ\n64.77\n65.20\n55.49\n59.60\n66.56\n70.96\nTable 1: The w4 average accuracy(\u2191) of four tasks, e.g., HellaSwag, WinoGrande, PIQA, and LAMBADA, in\nLM-eval. g denotes group size. \"Ours+GPTQ\" means we apply TEQ first and then apply GPTQ afterward. For\nLLAMA-7B, the result of GPTQ is w/o act-order. Results of act-order are shown in Appendix A.2.\nn_bits\nMethods\nOPT-6.7B\nOPT-13B\nBLOOM-3B\nBLOOM-7B1\nLLAMA-7B\nLLAMA-13B\n32\nFP32\n10.86\n10.12\n13.48\n11.36\n5.68\n5.09\n4\nRTN\n12.10\n11.32\n14.75\n12.09\n6.29\n5.53\nGPTQ\n11.59\n10.33\n14.10\n11.73\n6.59\n5.33\nOurs\n11.68\n10.59\n14.72\n12.21\n6.30\n5.50\nOurs+GPTQ\n11.29\n10.36\n14.03\n11.74\n6.76\n5.35\n4_g128\nRTN\n11.16\n10.32\n13.85\n11.60\n5.97\n5.26\nGPTQ\n10.98\n10.20\n13.69\n11.48\n6.29\n5.21\nOurs\n11.11\n10.28\n13.82\n11.58\n5.97\n5.26\nOurs+GPTQ\n11.02\n10.21\n13.69\n11.48\n6.28\n5.21\nTable 2: The w4 perplexity(\u2193) on WikiText-2. For LLAMA-7B, the result of GPTQ is w/o act-order. Results of\nact-order are shown in Appendix A.2.\nto better results, so we pick the better one in our\nexperiments.\n4\nExperiments\nIn this section, we evaluate our proposed TEQ\u2019s\nin different aspects. Initially, we briefly introduce\nLLM architectures and tasks included in our evalua-\ntion. Secondly, we illustrate a detailed comparison\nof our method and other state-of-the-art (SOTA)\nmethods, and both quantization accuracy and time\nare considered.\n4.1\nExperimental Settings\nLarge Language Models.\nWe conduct our exper-\niments on the most popular LLM architectures, in-\ncluding LLaMAs (Touvron et al., 2023), BLOOMs\n(Scao et al., 2022), and OPTs (Zhang et al., 2022).\nParameter scalings ranging from million to billion\nare all included.\nEvaluation and Datasets.\nWe make assessments\non several language tasks to satisfy the task-\nagnostic setting. Specifically, we report average\naccuracy result on four common sense reason-\ning tasks by leveraging lm-eval-harness(Gao et al.,\n2021), including HellaSwag (Zellers et al., 2019),\nWinoGrande (Sakaguchi et al., 2021), PIQA (Bisk\net al., 2020) and LAMBADA (Paperno et al., 2016).\nFurthermore, we complement our evaluation with\nperplexity (PPL) analysis on WikiText2 (Merity\net al., 2016), PTB (Marcus et al., 1994) as well as\nC4 (Raffel et al., 2020).\nImplementation\nDetails.\nFollowing\nGPTQ\n(Frantar et al., 2022), we focus on weight-only\nquantization and exclude the last layer When\nquantifying. We used a single HW accelerator to\nquantize models with a scale of around ten billion\nparameters. We use the same calibration dataset\npile-10k1 for a fair comparison.\nBaseline.\nOur primary baseline is vanilla round-\nto-nearest quantization (RTN) which has a remark-\nable result at 4bits using a small group size of 128.\nWe also compare with a state-of-the-art method\nGPTQ (Frantar et al., 2022).\n4.2\nResults\nAs mentioned above, we compare our results with\nRTN and the SOTA GTPQ(Frantar et al., 2022).\nAlso, since our method is orthogonal to GPTQ, we\nreport Ours+GPTQ as well, which applies TEQ\nfirst and then runs GPTQ official code2 afterward.\nWe mainly focus on the models around 10B which\n1https://huggingface.co/datasets/NeelNanda/pile-10k\n2https://github.com/IST-DASLab/gptq\nn_bits\nMethods\nOPT-6.7B\nOPT-13B\nBLOOM-3B\nBLOOM-7B1\nLLAMA-7B\nLLAMA-13B\n32\nFP32\n64.97\n65.54\n55.65\n60.29\n68.87\n71.06\n3_g128\nRTN\n56.03\n49.59\n52.54\n57.53\n64.92\n67.68\nGPTQ\n62.98\n64.68\n53.41\n58.12\n58.29\n68.73\nOurs\n61.41\n63.27\n52.69\n57.79\n65.25\n68.32\nOurs+GPTQ\n63.16\n64.60\n53.71\n58.00\n59.27\n69.15\nTable 3: The 3 bits with group size 128 average accuracy(\u2191) of four tasks,e.g., HellaSwag, WinoGrande, PIQA, and\nLAMBADA, in LM-eval. g denotes group size. For LLAMA-7B, the result of GPTQ is w/o act-order. Results of\nact-order are shown in Appendix A.2.\nn_bits\nMethods\nOPT-6.7B\nOPT-13B\nBLOOM-3B\nBLOOM-7B1\nLLAMA-7B\nLLAMA-13B\n32\nFP32\n10.86\n10.12\n13.48\n11.36\n5.68\n5.09\n3_g128\nRTN\n22.37\n40.50\n15.68\n12.47\n7.01\n5.88\nGPTQ\n11.42\n10.51\n14.67\n11.99\n8.28\n5.64\nOurs\n12.03\n11.83\n15.48\n12.40\n6.89\n5.81\nOurs+GPTQ\n11.40\n10.52\n14.64\n11.98\n7.71\n5.64\nTable 4: WikiText-2 perplexity(\u2193) of 3 bits with group size 128. For LLAMA-7B, the result of GPTQ is w/o\nact-order. Results of act-order are shown in Appendix A.2.\nis commonly used.\nW4 Quantization.\nWe first evaluate TEQ on pop-\nular 4 bits quantization. Table 1 shows the lm-eval\nresults of different LLM model architectures and\nparameter sizes. TEQ outperforms RTN in all cases\nexcept one. Comparing with GPTQ, TEQ shows\nbetter results in 6 out of 12 scenarios. After com-\nbining GPTQ, new state-of-the-art results could be\nachieved in 5 scenarios. In summary, TEQ could be\nhelpful in 8 out of 12 scenarios. Table 8 shows the\nhyper-parameters that we used in the experiements.\nWe also evaluate WikiText2 ppl in table 2 w/o\ngroup size and group size 128. TEQ is better or on\npar with RTN. Similarly, the combined approach\n(Ours and GPTQ) shows comparable or better re-\nsults than standalone GPTQ.\nW3 Quantization.\nWe also evaluate TEQ at\nweight with 3 bits. We only consider group size\n128, because the performance drops a lot without\ngroup size and usually could not be deployed in\npractice. Similar to 4 bits evaluation, we report the\nlm-eval result and wikitext2 ppl result in table 3\nand 4 respectively. TEQ outperforms RTN in all\nscenarios and is inferior to GPTQ on certain mod-\nels. However, TEQ could bring improvement for\n8 out of 12 scenarios if taking Ours+GPTQ into\naccount.\nQuantization Time.\nWe report the quantization\ntime in Table 5. We adopt Deepspeed3 for 10B+\nmodels due to the potential out-of-memory (OOM)\nissue. As TEQ needs training, our time cost is\n3https://github.com/microsoft/DeepSpeed\nreasonably higher than GPTQ, especially when the\nmodel does not fit into the device memory. It\u2019s\npossible to reduce the time further by using more\nresources or optimizing the code, while it\u2019s out of\nscope.\nModels\nGPTQ\nOurs\nOPT-6.7B\n841\n1239\nOPT-13B\n1523\n8737*\nBLOOM-3B\n345\n506\nBLOOM-7B1\n661\n1148\nLLAMA-7B\n712\n1249\nLLAMA-13B\n1240\n9501*\nTable 5: Quantization time in seconds for 4-bit weight\nquantization. * denotes DeepSpeed is adopted in train-\ning for 10B+ models.\nAnalysis of Scales in TEQ.\nWe visualize the\nmagnitude distribution histograms of sl for the lay-\ners to which TEQ can be applied. Figure 2 displays\nthe results of models with sl initialized as scalar\nones. Several conclusions can be drawn from these\nresults. Most notably, the majority of trained scales\nremain close to their initial values (e.g., 1), typi-\ncally within the range of [0.75, 1.25]. This suggests\nthat even minor changes to the model can signifi-\ncantly reduce quantization loss. Additionally, some\nscales deviate considerably from 1, indicating the\npresence of \u201coutlier\u201d channels. Furthermore, scales\nin middle layers tend to remain closer to their initial\nvalues compared to other layers, suggesting that\nthe first and last layers are more sensitive to the\nquantization loss. We also attach results of scales\ninitialized with 1.0/sqrt(wcin) in Appendix A.5.\nFigure 2: The magnitude distributions of scales in TEQ for BLOOM-3B, BLOOM-7.1B, OPT-6.7B, LLAMA-7B.\nThe quantization configurations are w3_g128, w4_g128, w4, and w4 respectively. Different colors refer to layer\nindices in models (blue stands for shallow layers which are close to the data layer, while red stands for deeper\nlayers).\n5\nConclusion\nIn this paper, we propose TEQ, a trainable equiva-\nlent transformation that preserves the FP32 preci-\nsion of the model output while also taking advan-\ntage of low-precision quantization, and its training\nprocess is lightweight. Plus, TEQ is regarded as\northogonal support for other quantization methods\nto improve their performance. Our task-agnostic\nexperiments and comparison with other methods\nshow that TEQ or its combination with other meth-\nods can obtain comparable or better results.\n5.1\nLimitations\nWe find that the required memory during training\nis still high, though the number of training parame-\nters remains low. Moreover, since we enforce the\ntransformation to be equivalent and keep the ar-\nchitecture and FP32 output unchanged, our results\nin some scenarios are inferior to the SOTA meth-\nods, which could be fixed by combining the SOTA\nmethods.\n5.2\nEthics Statement\nWe propose TEQ for LLMs quantization. The\nmethod can be either used individually or com-\nbined with other quantization methods. Since TEQ\nonly requires a few steps of finetuning on original\nmodels. Thus, it is safe to say that TEQ\u2019s techni-\ncal details have no significant ethical implications.\nOur work provides an exploration of large language\nmodel quantization through simple finetuning, mak-\ning their application easier. We believe increasingly\nmore work like this will emerge, making LLMs\u2019\nquantization more powerful.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016.\nLayer normalization.\narXiv preprint\narXiv:1607.06450.\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing\nJin, Xin Jiang, Qun Liu, Michael Lyu, and Irwin\nKing. 2020. Binarybert: Pushing the limit of bert\nquantization. arXiv preprint arXiv:2012.15701.\nYoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville.\n2013. Estimating or propagating gradients through\nstochastic neurons for conditional computation.\narXiv preprint arXiv:1308.3432.\nYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi,\net al. 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Proceedings of the\nAAAI conference on artificial intelligence, volume 34,\npages 7432\u20137439.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-\ntiplication for transformers at scale. arXiv preprint\narXiv:2208.07339.\nSteven K Esser, Jeffrey L McKinstry, Deepika Bablani,\nRathinakumar Appuswamy, and Dharmendra S\nModha. 2019. Learned step size quantization. arXiv\npreprint arXiv:1902.08153.\nElias Frantar and Dan Alistarh. 2022.\nOptimal\nbrain compression: A framework for accurate post-\ntraining quantization and pruning. arXiv preprint\narXiv:2208.11580.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2022. Gptq: Accurate post-training\nquantization for generative pre-trained transformers.\narXiv preprint arXiv:2210.17323.\nLeo Gao, Jonathan Tow, Stella Biderman, Sid Black,\nAnthony DiPofi, Charles Foster, Laurence Golding,\nJeffrey Hsu, Kyle McDonell, Niklas Muennighoff,\nJason Phang, Laria Reynolds, Eric Tang, Anish Thite,\nBen Wang, Kevin Wang, and Andy Zou. 2021. A\nframework for few-shot language model evaluation.\nGithub. Github: Copilot.\nRuihao Gong, Xianglong Liu, Shenghu Jiang, Tianxiang\nLi, Peng Hu, Jiazhen Lin, Fengwei Yu, and Junjie\nYan. 2019. Differentiable soft quantization: Bridging\nfull-precision and low-bit neural networks. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 4852\u20134861.\nBabak Hassibi, David G Stork, and Gregory J Wolff.\n1993. Optimal brain surgeon and general network\npruning. In IEEE international conference on neural\nnetworks, pages 293\u2013299. IEEE.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770\u2013\n778.\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry\nKalenichenko, Weijun Wang, Tobias Weyand, Marco\nAndreetto, and Hartwig Adam. 2017. Mobilenets:\nEfficient convolutional neural networks for mobile vi-\nsion applications. arXiv preprint arXiv:1704.04861.\nItay Hubara, Yury Nahshan, Yair Hanani, Ron Banner,\nand Daniel Soudry. 2021. Accurate post training\nquantization with small calibration sets. In Inter-\nnational Conference on Machine Learning, pages\n4466\u20134475. PMLR.\nSergey Ioffe and Christian Szegedy. 2015. Batch nor-\nmalization: Accelerating deep network training by re-\nducing internal covariate shift. In International con-\nference on machine learning, pages 448\u2013456. pmlr.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nYann LeCun, John Denker, and Sara Solla. 1989. Opti-\nmal brain damage. Advances in neural information\nprocessing systems, 2.\nZhengyi Li, Cong Guo, Zhanda Zhu, Yangjie Zhou,\nYuxian Qiu, Xiaotian Gao, Jingwen Leng, and Minyi\nGuo. 2022. Efficient activation quantization via adap-\ntive rounding border for post-training quantization.\narXiv preprint arXiv:2208.11945.\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang,\nXingyu Dang, and Song Han. 2023. Awq: Activation-\naware weight quantization for llm compression and\nacceleration. arXiv.\nChristos Louizos, Matthias Reisser, Tijmen Blankevoort,\nEfstratios Gavves, and Max Welling. 2018. Relaxed\nquantization for discretized neural networks. arXiv\npreprint arXiv:1810.01875.\nMitch Marcus, Grace Kim, Mary Ann Marcinkiewicz,\nRobert MacIntyre, Ann Bies, Mark Ferguson, Karen\nKatz, and Britta Schasberger. 1994. The penn tree-\nbank: Annotating predicate argument structure. In\nHuman Language Technology: Proceedings of a\nWorkshop held at Plainsboro, New Jersey, March\n8-11, 1994.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen,\nChristos Louizos, and Tijmen Blankevoort. 2020. Up\nor down? adaptive rounding for post-training quan-\ntization. In International Conference on Machine\nLearning, pages 7197\u20137206. PMLR.\nOpenAI. Openai: Chatgpt.\nDenis Paperno, Germ\u00e1n Kruszewski, Angeliki Lazari-\ndou, Quan Ngoc Pham, Raffaella Bernardi, Sandro\nPezzelle, Marco Baroni, Gemma Boleda, and Raquel\nFern\u00e1ndez. 2016. The lambada dataset: Word pre-\ndiction requiring a broad discourse context. arXiv\npreprint arXiv:1606.06031.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485\u20135551.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM, 64(9):99\u2013106.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili\u00b4c, Daniel Hesslow, Roman\nCastagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon,\nMatthias Gall\u00e9, et al. 2022.\nBloom:\nA 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nMingzhu Shen, Feng Liang, Ruihao Gong, Yuhang Li,\nChuming Li, Chen Lin, Fengwei Yu, Junjie Yan, and\nWanli Ouyang. 2021. Once quantization-aware train-\ning: High performance extremely low-bit architec-\nture search. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages\n5340\u20135349.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nXiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu,\nand Fengwei Yu. 2022. Qdrop: randomly dropping\nquantization for extremely low-bit post-training quan-\ntization. arXiv preprint arXiv:2203.05740.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien De-\nmouth, and Song Han. 2022. Smoothquant: Accurate\nand efficient post-training quantization for large lan-\nguage models. arXiv preprint arXiv:2211.10438.\nZhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang,\nXiaoxia Wu, Conglong Li, and Yuxiong He. 2022.\nZeroquant: Efficient and affordable post-training\nquantization for large-scale transformers. Advances\nin Neural Information Processing Systems, 35:27168\u2013\n27183.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. Hellaswag: Can a\nmachine really finish your sentence? arXiv preprint\narXiv:1905.07830.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nWei Zhang, Lu Hou, Yichun Yin, Lifeng Shang, Xiao\nChen, Xin Jiang, and Qun Liu. 2020. Ternarybert:\nDistillation-aware ultra-low bit bert. arXiv preprint\narXiv:2009.12812.\nBohan Zhuang, Mingkui Tan, Jing Liu, Lingqiao Liu,\nIan Reid, and Chunhua Shen. 2021. Effective training\nof convolutional neural networks with low-bitwidth\nweights and activations. IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence, 44(10):6140\u2013\n6152.\nA\nAppendix\nA.1\nAdditional comparison with AWQ\nAlthough both AWQ and TEQ use a small calibra-\ntion set from Pile, TEQ\u2019s evaluation methodology\nclosely follows that of GPTQ and only shares a\nfew common tasks with AWQ. It is important to\nacknowledge that this comparison inherently lacks\nrigor due to our reliance on referencing AWQ\u2019s\ndata alone. Consequently, this approach introduces\nthe potential unfairness in the evaluation process,\nprimarily stemming from the utilization of different\ncalibration datasets.\nTable 6 presents the LLaMA-7B\u2019s results of our\ncommon tasks alongside AWQ in table below and\nall the results of AWQ are from their paper.\nA.2\nAdditional comparison with GPTQ\nact-order\nWe show the results in Table 7. TEQ still outper-\nforms GPTQ in most cases.\nLLaMA-7B\nAWQ\nOurs\nnbits\nMethod\nPIQA\nHella.\nWino.\nPIQA\nHella.\nWino.\n16\nFP16\n78.35\n56.44\n67.09\n78.35\n56.42\n66.85\nW3G128\nRTN\n75.84\n53.10\n63.22\n75.68\n53.18\n63.06\nGPTQ\n70.89\n46.77\n60.93\n72.58\n47.10\n59.91\nProposed\n76.66\n53.63\n66.14\n76.01\n53.30\n63.06\nW4G128\nRTN\n77.86\n55.81\n65.59\n77.58\n55.91\n65.59\nGPTQ\n77.20\n53.98\n65.67\n77.58\n55.83\n66.54\nProposed\n78.07\n55.76\n65.82\n78.02\n55.76\n66.54\nTable 6: Reported results of AWQ and Ours\nnbits / gs\nMethods\nlm-eval (\u2191)\nwikitext2\nppl (\u2193)\n4 / -1\nGPTQ-AO\n0.6713\n6.06\nOurs\n0.6771\n6.30\nOurs+GPTQ-AO\n0.6736\n6.03\n4 / 128\nGPTQ-AO\n0.6809\n5.82\nOurs\n0.6813\n5.97\nOurs+GPTQ-AO\n0.6811\n5.82\n3 / 128\nGPTQ-AO\n0.6042\n8.29\nOurs\n0.6521\n6.89\nOurs+GPTQ-AO\n0.6647\n6.61\nTable 7: Comparing results of Llama-7B for GPTQ with\nact-order. AO denotes act-order. TEQ still outperforms\nGPTQ in most cases.\nA.3\nSpecial hyperparameters and settings\nUsually, we adopt the same hyperparameters men-\ntioned in section 3.2. So we only list all the partic-\nular settings in Table 8.\nlr\ninitialization\nModels\ndefault\n1.0/sqrt(wcin)\nopt13b_w4; bloom3b_w4;\nbloom7b_w4;\nopt6.7b_w4_g128;\nopt13b_w3_g128\n4e-4\ndefault\nllama7b_w4;\n2e-4\ndefault\nllama13b_w4_g128;\nTable 8: Special hyperparameters and settings. g de-\nnotes group size\nA.4\nMore visualization results for TEQ\u2019s\ntrained parameters.\nFigure 3 shows the magnitude distribution of scales\ninitialized with 1.0/sqrt(wcin). Since the initial\nvalue is related to channel-wise maximum values,\nit\u2019s more challenging to analyze. However, some\noutliers could be still observed.\nA.5\nCounts of trainable parameters\nintroduced by TEQ\nWe provide more details about counts of trainable\nparameters introduced by TEQ in Table 9. table\npresented below offers details regarding the appli-\ncable layers of TEQ in several models. We handle\nlinear layers that possess transformation scales that\ncan be assimilated by their preceding layers, such\nas Layer Normalization, among others.\nAs an illustration, within a single transformer\nblock of OPT-6.7B, the QKV layers have the same\npreceding layers and therefore utilize the same set\nof trainable parameters. Based on the statistics, we\nhave observed that TEQ\u2019s training only requires a\nminimal number of parameters (around the order\nfrom 1e-5 to 1e-4), thereby making our approach\nlight-weighted enough.\nFigure 3: TEQ\u2019s trained transformation parameters\u2019 magnitude distributions, using maximum\u2019s square root value\nfor initialization. From top to down are BLOOM-3B, BLOOM-7.1B, OPT-6.7B and LLAMA-7B respectively.\nModels\nBlocks\nTEQ\nApplicable\nLinear\nLayers\nTotal\nLinear\nLayers\nTEQ\nParameter\nGroups\nTEQ\nParameters\nCounts\nTotal\nParameters\nCounts\nRatio\nTEQ params and\nTotal Params\nBloom\n3B\n30\n60\n121\n60\n153600\n3644810240\n0.00421%\nBloom\n7B1\n30\n60\n121\n60\n245760\n8096620544\n0.00304%\nOPT\n6.7B\n32\n160\n193\n72\n786432\n6864388096\n0.01146%\nOPT\n13B\n40\n200\n241\n96\n1228800\n13110865920\n0.00937%\nLlama\n7B\n32\n160\n225\n64\n262144\n6738415616\n0.00389%\nLlama\n13B\n40\n200\n281\n80\n409600\n13015864320\n0.00315%\nTable 9: Analysis of TEQ Parameters. TEQ only require a minimal ratio of original models\u2019 parameters (around the\norder from 1e-5 to 1e-4).\n"
  },
  {
    "title": "LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation",
    "link": "https://arxiv.org/pdf/2310.10769.pdf",
    "upvote": "8",
    "text": "LAMP: Learn A Motion Pattern for Few-Shot-Based Video Generation\nRuiqi Wu1 2 *\nLiangyu Chen2\nTong Yang2\nChunle Guo1 \u2020\nChongyi Li1\nXiangyu Zhang2\n1VCIP, CS, Nankai University\n2MEGVII Technology\nwuruiqi@mail.nankai.edu.cn, {chenliangyu, yangtong, zhangxiangyu}@megvii.com\n{guochunle, lichongyi}@nankai.edu.cn\nHelicopter  /  A helicopter over the sea\nHorse run  /  A horse runs on the Mars\nFirework /  Fireworks over the mountains\nTurn to smile  /  Head photo of a cute girl, comic style, turns to smile\nBirds fly /  Many birds fly over a plaza\nFigure 1. Our text-to-video results. The motion prompts and video prompts are listed, respectively. Our LAMP works effectively on\ndiverse motions. The generated videos are temporal consistent and close to the video prompts. Moreover, two advantages of LAMP can be\nreflected in the above results. (1) The proposed first-frame-conditioned training strategy allows us to use powerful SD-XL for first-frame\ngeneration, which is beneficial to producing highly detailed following frames. (2) Good semantic generalization properties of the diffusion\nmodel are preserved (e.g. imposing smile\u2019s motion on unseen comic style) since our tuning way.\nAbstract\nWith the impressive progress in diffusion-based text-to-\nimage generation, extending such powerful generative abil-\nity to text-to-video raises enormous attention.\nExisting\nmethods either require large-scale text-video pairs and a\n* This work is done during Ruiqi Wu\u2019s internship at MEGVII Tech-\nnology.\n\u2020 Correspondence author.\nlarge number of training resources or learn motions that\nare precisely aligned with template videos. It is non-trivial\nto balance a trade-off between the degree of generation\nfreedom and the resource costs for video generation. In\nour study, we present a few-shot-based tuning framework,\nLAMP, which enables text-to-image diffusion model Learn\nA specific Motion Pattern with 8 \u223c16 videos on a sin-\ngle GPU. Specifically, we design a first-frame-conditioned\npipeline that uses an off-the-shelf text-to-image model for\narXiv:2310.10769v1  [cs.CV]  16 Oct 2023\ncontent generation so that our tuned video diffusion model\nmainly focuses on motion learning.\nThe well-developed\ntext-to-image techniques can provide visually pleasing and\ndiverse content as generation conditions, which highly im-\nproves video quality and generation freedom. To capture\nthe features of temporal dimension, we expand the pre-\ntrained 2D convolution layers of the T2I model to our novel\ntemporal-spatial motion learning layers and modify the at-\ntention blocks to the temporal level. Additionally, we de-\nvelop an effective inference trick, shared-noise sampling,\nwhich can improve the stability of videos with computa-\ntional costs. Our method can also be flexibly applied to\nother tasks, e.g. real-world image animation and video edit-\ning. Extensive experiments demonstrate that LAMP can ef-\nfectively learn the motion pattern on limited data and gener-\nate high-quality videos. The code and models are available\nat https://rq-wu.github.io/projects/LAMP.\n1. Introduction\nIn recent years, generative models, particularly diffusion-\nbased models [11, 33, 34], have shown remarkable achieve-\nments in generating images from textual prompts, i.e. text-\nto-image generation (T2I) [22, 24, 27, 28, 30]. Despite the\nsuccess made in the T2I field, which has provided substan-\ntial technical groundwork, there remain large gaps in the de-\nvelopment of text-to-video (T2V) generation: how to gen-\nerate consistent frames and understand the motion patterns\nimplicit in textural prompts.\nSeveral recent works [2, 9, 12, 31, 36, 42] try to bridge\nthese gaps by directly training a diffusion-based T2V model\nusing millions of text-video pairs. These approaches fa-\ncilitate a deeper understanding of the relationship between\nthe video and the textural prompt.\nHowever, the mas-\nsive demand for labeled data and the heavy training bur-\nden are unaffordable for most researchers, constraining\nthe development of this research line.\nAnother research\nline [5, 6, 21, 25, 37, 39] involves utilizing a video template\nand manipulating content using diffusion models while\nkeeping the original motion. Those template-based meth-\nods are also known as video editing. Although these meth-\nods can prove cost-effective, especially with the proposal\nof one-shot [37] and even zero-shot [6, 25, 39] algorithms,\nthe use of given video template significantly limits the gen-\neration freedom. Besides, recent T2V-Zero [18] modifies\nthe T2I diffusion models to generate consistent videos with-\nout training. Nevertheless, it is challenging to transfer the\ntext-image domain knowledge to the text-video domain in a\nzero-shot manner, resulting in the limitation of T2V-Zero to\ngenerate similar-looking frames with random motions.\nIt is essential to achieve a trade-off between training bur-\nden and generation freedom while making models under-\nstand the motions. Since the pretrained T2I diffusion model\nhas good semantic comprehension guided by the prompts, it\nis reasonable that very little data is needed to make it under-\nstand the correspondence between prompts and motions and\ngenerate diverse videos. Therefore, we attempt to explore\na novel few-shot setting for the T2V task. The new setting\naims at tuning a T2I diffusion model to Learn A common\nMotion Pattern from a small video set.\nWhen tuning a T2I model to a T2V model in a few-shot\nmanner, two issues need to be addressed. (1) Due to the lim-\nited data amount, there is a risk of over-fitting the content\nwithin the video set. If the generated videos are similar to\nthe video set, it undermines one of our core goals, namely\ngeneration freedom. (2) The base operators of T2I diffusion\nmodels only work on spatial dimensions, which limits their\nability to capture temporal information within videos.\nWith the two challenging issues, we propose a base-\nline method for few-shot T2V generation, named LAMP.\nOur solution to the first issue is the proposed first-frame-\nconditioned pipeline. It decouples the T2V task into two\nsub-tasks, generating the first frame by a pre-trained T2I\nmodel and predicting subsequent frames using our tuned\nvideo diffusion model. The proposed pipeline seamlessly\nintegrates the first frame as a condition without involving\nany additional model modification (e.g. changing the data\nstructure of inputs or adding new cross-attention layers).\nSpecifically, during training, we retain the first frame of\nthe input video, adding noise and imposing the loss only\non the subsequent frames. Since the first frame provides\nthe majority of the video\u2019s content, our model can focus\non the relationship between the subsequent frames and the\nfirst frame, i.e. the motion pattern rather than the contents.\nDuring the inference, the first frame is generated by a pre-\ntrained T2I model, such as SD-XL [24]. We observe that a\nhigh quality of the first frame can boost the video generation\nperformance through the proposed pipeline. With the refer-\nence provided by the first frame, our model, which is based\non Stable Diffusion v1.4 (SD v1.4) [28], can preserve the\nhigh-quality content generated by SD-XL throughout the\nvideo. Facing the second issue, we design temporal-spatial\nmotion learning layers to capture the features of tempo-\nral and spatial dimensions simultaneously. Since predicting\nsubsequent frames based on the first frame is required in\nthe proposed pipeline, we modified the base operator based\non the video prediction tasks [15, 20], which will be in-\ntroduced in Sec. 3.4. As in previous works [18, 37], we\nmodify the attention layers to build effective communica-\ntion between frames. Moreover, we adopt a shared-noise\nsampling strategy during inference, which constructs the\noriginal noise for each frame from a shared noise. This\nstrategy significantly improves the quality and stability of\nthe generated videos with negligible computational costs.\nWe evaluate our LAMP on several motion cases. With a\nsimple tuning using 8 \u223c16 videos on a single GPU, the pro-\nposed LAMP can generate videos with the common motion\npattern of the video set and generalize well to unseen styles\nand objects. (See Figure 1). Our key contributions can be\nsummarized as follows:\n\u2022 We present a new setting of the few-shot tuning for the\nT2V generation task, aiming to strike a balance between\ngeneration freedom and training costs.\n\u2022 We propose LAMP, a baseline method for few-shot T2V\ncalled LAMP, which effectively learns the motion pattern\nin the given video set following a simple tuning.\n\u2022 We introduce a first-frame conditioned pipeline that uses\nthe first frame as a condition, effectively decoupling the\nmotion and content, which simplifies the T2V task signif-\nicantly.\n\u2022 We introduce the temporal layers and inference tricks, of-\nfering key insights into few-shot-based video generation.\n2. Related Work\n2.1. Text-to-Image Diffusion Models\nRecently, diffusion models [11, 33, 34] beat GANs [4, 7,\n40], VAEs [19, 32, 35], and flow-based [3, 8] approaches\nand have been in the limelight for text-to-image genera-\ntion because of their stable training and outstanding perfor-\nmance. For example, GLIDE [22] uses textural prompts as\nconditions and adopts classifier-free guidance [10] to im-\nprove image quality.\nDALLE-2 [27] introduces the pre-\ntrained CLIP [26] model to align the features of images\nand text. Imagen [30] injects the features from a large lan-\nguage model to diffusion models for better prompts under-\nstanding and proposes a cascaded pipeline to generate high-\nresolution images from coarse to fine. To ease the compu-\ntational burden of the iterative denoising process, Rombach\net al. propose LDM [28] that uses an autoencoder [4, 19] to\nreduce the redundancy of images. LDM compresses an im-\nage into low-dimension latent space by a pre-trained autoen-\ncoder first, then learns to denoise noisy latent data. With the\nsuccess achieved by LDM, many variants [23, 41] are pro-\nposed to improve the performance further. More recently,\nthe SD-XL [24] is presented, which can generate extremely\nphoto-realistic images with high-definition details. In our\nwork, SD-XL is utilized to generate the first frame, and SD-\nv1.4 is modified for subsequent frame prediction.\n2.2. Text-to-Video Diffusion models\nThe thriving of diffusion-based models in the text-to-image\nfield demonstrates its potential in text-to-video generation.\nThe mainstream works can be divided into two categories:\nopen-domain T2V generation and template-based methods.\nOpen-domain T2V generation. During the early stage,\nImagenVideo [12] and Make-A-Video [31] learn T2V on\nthe pixel level. However, the video length and resolution\nare significantly limited due to the high computation in the\npixel space. MagicVideo [42] is then proposed, which trains\na new autoencoder on video data. As the appearance of\nLDMs [28] to the T2I field, MagicVideo boosts the com-\nputational effectiveness for T2V generation. Blattmann et\nal. [2] present an LDMs-based T2V diffusion model, which\nadds extra 3D convolutional layers on frozen pre-trained\nlayers. VideoComposer [36] adds diverse conditions, e.g.\nsketch and motion vectors, to the T2V model by a novel\nencoder. AnimateDiff [9] trains a set of motion layers ca-\npable of being applied to customized T2I models [14, 29],\nenabling them to produce videos in a consistent style. The\nabove methods achieve remarkable performance for T2V\ngeneration. However, the necessity of training these models\non large-scale data like WebVid-10M [1] and HD-VILA-\n100M [38] poses a significant barrier for most researchers.\nIn addition, some zero-shot methods [13, 16, 18] have been\nproposed, yet they often suffer from suboptimal frame con-\nsistency.\nTemplate-based methods. Template-based T2V genera-\ntion aims to facilitate video-to-video translation with the\nguidance of user prompts, which is also known as video\nediting. Dreamix [21] and GEN-1 [5] are two pioneer works\nin template-based methods, while their training costs are\ncomparable to open-domain T2V methods. Then, Tune-\nA-Video [37] proposes a new one-shot setting that uses a\nT2I model to overfit the origin video, which can be imple-\nmented on consumer-grade GPUs. FateZero [25] proposes a\ntraining-free method by injecting the cross-attention map of\nthe source video and modifying attention layers. Rerender-\nA-Video [39] and TokenFlow [6] further improve the con-\nsistency of videos with the integration of priors and condi-\ntional guidance. Different from the objectives of template-\nbased methods, our few-shot T2V setting aims to achieve a\nhigher degree of freedom in video generation rather than\nprecisely aligning with the motion pattern of a template\nvideo.\n3. Method\nIn this section, Sec. 3.1 and Sec. 3.2 first introduce\nthe preliminary knowledge and the new few-shot setting.\nNext, Sec. 3.3 details our proposed first-frame-conditioned\npipeline. Sec. 3.4 is followed to describe how we modify a\nT2I diffusion model to T2V generation. Finally, Sec. 3.5 in-\ntroduces our shared-noise sampling strategy and some tech-\nniques that can improve performance during inference time.\n3.1. Preliminaries\nIn this section, we introduce the preliminary knowledge of\nthe diffusion-based model. Given data x0 \u2208 X, a Markov\nchain can be defined as:\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212 \u03b2txt\u22121, \u03b2tI),\n(1)\nVideo Set: Horse run\nTraining\nInference\n...\n...\nnoisy\nDDPM\nLoss\nCLIP \npre-trained layers (frozen)\ntemporal-spatial motion learning layers\nattention layers (partially frozen)\nU-Net\n...\n...\nU-Net\nA horse runs on the Mars\nT2I\nCLIP  \nsubsequent\nframes\nSample\n\u00d7 T steps\n\u2744\nFigure 2. Framework of LAMP. LAMP learns a motion pattern from a small video set, enabling the generation of videos imbued with\nthe learned motion patterns. This approach strikes a balance between training resources and generation freedom in video generation. We\ntransfer text-to-video generation to the first-frame generation and subsequent-frame prediction, i.e., decoupling a video\u2019s contents and\nmotions. During training, we add noise and compute loss functions for all frames except the first frame. Moreover, only the parameters\nof newly added layers and the query linear layers of self-attention blocks are tuned. During inference, we use a T2I model to generate the\nfirst frame. The tuned model only works on denoising the latent features of subsequent frames with the guidance of user prompts.\nwhere t = 1, ..., T, T is the total number of steps. \u03b2t is\na coefficient that controls the noise strength in step t. The\niterative noise adding can be simplified as:\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212 \u00af\u03b1t\u03f5,\n\u03f5 \u223c N(0, I),\n(2)\nwhere \u00af\u03b1t = Qt\ni=1(1 \u2212 \u03b2t). Diffusion models learn the dis-\ntribution of dataset X by minimizing the training objective,\nwhich can be written as:\narg min\n\u03b8\nEx0,\u03f5\u223cN (0,I),t,c[||\u03f5 \u2212 \u03f5\u03b8(xt, t, c)||2\n2],\n(3)\n\u03f5\u03b8(\u00b7) denotes the noise prediction function of diffusion\nmodels, c is the conditions like textual prompts. After train-\ning, diffusion models can generate data from noise by re-\nversing the noise-adding process.\nHowever, the computational burden becomes substantial\nwhen diffusion models are used to generate high-resolution\nimages. To address this challenge, Latent diffusion models\n(LDMs) for T2I generation have been proposed, adopting\nan auto-encoder to achieve all operators in the latent space.\nThey can acquire low-redundancy initial data by encoder\nand reconstruct generated results by decoder. LDMs are\nalso used in our method to generate high-resolution videos.\n3.2. Our Few-shot-based T2V Generation Setting\nExisting T2V approaches require large-scale data for train-\ning or rely on a template video to obtain low-degree-of-\nfreedom generative capabilities.\nIn order to make video\ngeneration inexpensive and flexible, we propose a novel set-\nting: few-shot T2V generation. Supposing that there is a\nvideo set V = {Vi|i \u2208 [1, n]} contains n videos and a\nprompt Pm to describe the common motion as training data.\nThe proposed new setting is to tune a T2I model on the\ngiven video set and the motion prompt. The tuned model\ncan generate a new video V\u2032 with a similar motion pattern\nto V from a prompt P that is related to the motion. We hope\nto learn the common motion pattern from a small video set\nwhile ignoring the contents. Meanwhile, the training cost\nis affordable because of the small data size. Based on the\nproposed setting, we modify pre-trained T2I models and\npresent a baseline framework for few-shot T2V generation.\n3.3. First-Frame-Conditioned Pipiline\nDue to the limited data in the few-shot tunning process,\nthere is a risk of overfitting the content of the small dataset,\npotentially compromising the degree of generation freedom.\nTo direct our model\u2019s focus toward motion, we propose the\nfirst-frame-conditioned pipeline to decouple motions and\ncontents. The proposed pipeline is illustrated in Figure 2.\nBased on our observation, the first frame contains the ma-\njority of the contents of a short video. It is natural to use the\nfirst frame as a condition, enabling the model to pay more\nattention to motions. Therefore, the T2V generation task\nis translated to first-frame T2I generation and subsequent-\nframe prediction. There are previous works [5, 36] that have\nalso used the first frame as a condition. They concat it to the\ninput noise or add a specific encoder to inject the features\ninto networks.\nHowever, applying these methods in the\nfew-shot setting is challenging, as the limited data makes it\nnearly impossible to facilitate model training through sub-\nstantial modifications to T2I models. In contrast, the pro-\nposed first-frame-conditioned pipeline can achieve compa-\nrable effects with slight parameter changes, as detailed in\nSec. 3.4.\nSpecifically, let V = {f i|i = 1, ..., l} be a video contains\nl frames and encode them into latent space: Z = {zi|i =\n1, ..., l}. When training the model, we preserve the original\nsignal of z1 and add noise to {z2, ..., zl}. The loss functions\ncan be written as:\nL = EZ,\u03f5\u223cN (0,I),t,c[||\u03f52:l \u2212 \u03f52:l\n\u03b8 (Zt, t, c)||2\n2],\n(4)\nwhere \u03f52:l is the added noise from 2nd to l-th frame, respec-\ntively. Other notations are consistent with Eq. (3). After\ntraining, the model gains the capability to generate a video\nwith the motion pattern of the video set according to the first\nframe. During inference, the powerful SD-XL [24] is em-\nployed to provide the first frame \u02c6f 1, which is decoded to \u02c6z1\nThen, a sequence, [\u02c6z1, \u03f52, ...\u03f5l], where \u03f5 is a random noise,\nis fed to the model for the whole video generation. At each\nstep, we preserve the latent of the first frame and denoise\nthe subsequent frames.\nThe proposed pipeline effectively avoids learning the\ncontents of the video set so that it can train a model on\nlimited data. Another advantage lies in the quality of con-\ntent generated by SD-XL, providing a good reference for\nvideo generation.\nThis approach enables us to leverage\nthe advantages of well-established T2I techniques.\nThe\nfirst-frame-conditioned pipeline significantly benefits both\nprompt alignment performance and generation diversity.\nMoreover, this pipeline is also appealing in its flexibility\nin applications e.g. real-world image animation and video\nediting, as detailed in Sec. 5.\nHowever, the original T2I models treat frames as inde-\npendent samples. Thus, the features of the first frame cannot\nbe used to establish temporal relationships between frames\nand generate videos. The following section introduces how\nwe enable the model to work at the temporal level.\n3.4. Adapt T2I Models to Video\nTemporal-spatial motion learning layers. To empower\nthe T2I model to extract temporal features, we inflate\nPretrained 2D Conv\n2D Conv\n1D Conv\nS\n(\ud835\udc4f\u210e\ud835\udc64)\u00d7\ud835\udc50\u00d7\ud835\udc53\n(\ud835\udc4f\ud835\udc53)\u00d7\ud835\udc50\u00d7\u210e\u00d7\ud835\udc64\n(\ud835\udc4f\ud835\udc53)\u00d71\u00d7\u210e\u00d7\ud835\udc64\n(\ud835\udc4f\ud835\udc53)\u00d7\ud835\udc50\u00d7\u210e\u00d7\ud835\udc64\n1D Conv\n\u2026\n\u2026\n\u2026\n\u2026\nOriginal 1D Conv\nVideo-Prediction-Based 1D Conv\n\ud835\udc53!\n\ud835\udc53\"\n\ud835\udc53#$! \ud835\udc53# \ud835\udc53#%!\n\ud835\udc53&\n\ud835\udc53!\n\ud835\udc53\"\n\ud835\udc53&\n\ud835\udc53#$! \ud835\udc53# \ud835\udc53#%!\n(a) The proposed 1D conv\n(b) Temporal-spatial motion\nlearning layer\nFigure 3. The details of the proposed temporal-spatial motion lay-\ners. (b) illustrates that 1D convolutions are added on pre-trained\nlayers to capture information along the temporal dimension. 2D\nconvolution layers with an output channel number of 1 control the\nspatial level\u2019s motion strength. The 1D convolutional layers utilize\nthe former two frames to generate the current frame, as shown in\n(a). f i denotes the i-th frame.\nthe pre-trained 2D convolutional layers into the proposed\ntemporal-spatial motion learning layers. As illustrated in\nFigure 3(b), the proposed layer consists of two branches.\nSuppose the latent features of the input video are repre-\nsented as as a 5D tensor with a shape of b\u00d7c\u00d7f \u00d7h\u00d7w. In\nthe temporal branch, the tensor is reshaped into bhw\u00d7c\u00d7f\nand fed to a 1D convolutional layer. However, since the\n1D convolution kernel can only work on a spatial coordi-\nnate at a time, it fails to take the essential spatial features\ninto account. Consequently, a 2D convolution with an out-\nput channel of 1 along with a Sigmoid function is added\nas compensation for spatial features. The input features are\nreshaped into bf \u00d7 c \u00d7 h \u00d7 w in the spatial branch.\nConsidering that our first-frame-conditioned pipeline\nneeds to predict subsequent frames based on the given first\nframe, which is similar to video prediction [15, 20], we\ndesign our 1D convolutional layers in a video prediction\nmanner, as shown in Figure 3(a). When the kernel slides\nthrough the features of frames {f i\u22121, f i, f i+1}, our video-\nprediction-based 1D convolution produces the features of\nf i+1 instead of f i as in the original version.\nThus, we\ncan utilize the former two frames to predict the subse-\nquent frame, i.e. effectively achieving video prediction in\nthe base operators. Moreover, to avoid our newly added\nlayers polluting the generation capability of the original T2I\nmodel, all parameters are zero-initialized, as done in Con-\ntrolNet [41].\nAttention layers. We also modify the attention layers to\nensure consistency. For self-attention layers, all key and\nvalue features are obtained from the first frame, which can\nbe written as:\nAttention(Qi, K1, V 1) = Softmax(Qi(K1)T\n\u221a\nd\n)V 1, (5)\n\u201cA horse runs in the universe\u201d\n\u201cFireworks, grass land\u201d\nLAMP\nAnimate\nDiff\nTAV\nT2V-Zero\nFigure 4. Qualitative comparison between the proposed LAMP and three baselines. Zoom in for the best view.\nthe superscript i \u2208 {1, ..., l} indicates the features are\nfrom the i-th frame. Combined with the proposed pipeline,\nthe reformulated self-attention layers facilitate subsequent\nframes to refer back to the conditions established by the first\nframe. Besides, it has been demonstrated that such a modifi-\ncation can effectively preserve the main object even without\ntuning [18]. Moreover, following the modification in [37],\nwe have incorporated temporal attention layers, which are\nself-attention layers working on the temporal dimension.\n3.5. Shared-noise Sampling During Inference\nDuring inference, we propose a simple yet effective shared-\nnoise sampling strategy further to improve the quality of\nthe generated videos quality.\nSpecifically, we first sam-\nple a shared noise \u03f5s \u223c N(0, I). Then, a noise sequence\n[\u03f52, ..., \u03f5l] with the same distribution as the base noise is\nsampled. In our sampling strategy, the original noise \u03f5i for\nthe i-th frame generation is updated as:\n\u03f5i = \u03b1\u03f5s + (1 \u2212 \u03b1)\u03f5i,\n(6)\nwhere \u03b1 is a coefficient to control the sharing degree. We\nempirically set \u03b1 = 0.2 in our experiments. This approach\nensures a consistent noise level across each frame, ulti-\nmately manifesting as consistency in the generated videos.\nIntuitively, this approach is in accordance with the prior\nknowledge that every frame of a video has certain similar-\nities. Mathematically, the reduced noise variance can con-\ntract the dynamic range of the latent space, contributing to a\nmore stable generation process. Besides, the AdaIN [17]\ntechnique on latent space and histogram matching at the\npixel level are used for post-processing. The efficacy of\nour free-lunch inference strategies will be demonstrated in\nSec. 4.3.\n4. Experiments\n4.1. Implementations\nIn our experiments, we generate videos with resolutions of\n320 \u00d7 512 and 16 frames. We use SD-XL [24] for the less\ncomputationally intensive first frame generation and the rel-\natively more lightweight SD-v1.4 [28] for the more com-\nputationally demanding prediction of subsequent frames,\nthereby balancing the inference cost of the two stages. For\ntraining, we use a set of self-collected videos ranging from\n8 \u223c 16, randomly sampling a 16-frame clip during each it-\neration All frames are resized to a resolution of 320 \u00d7 512.\nOnly the parameters of new-added layers and the query lin-\near layer in self-attention blocks are tuned, and the learning\nrate is set to 3.0 \u00d7 10\u22125. All experiments are implemented\non a single A100 GPU and only need \u223c 15 GB vRAM for\ntraining and \u223c 6 GB vRAM for inference.\n4.2. Comparisons\nWe train our LAMP for 8 motions, including helicopter\n(rigid motion), waterfall (fluid motion), rain & firework\n(particle motion), horse running (animal motion), birds\nflying (multi-body motion), turn to smile (human emo-\ntion) and play the guitar (human motion).\nWe design 6\nprompts for each motion to build an evaluation set con-\ntaining 48 videos. Three publicly available methods, which\nare large-scale pre-trained AnimateDiff [9], one-shot-based\nvideo editing method Tune-A-Video [37], and zero-shot-\nbased Text2Video-Zero [18], are selected as our compari-\nson baselines. We consider representative work under a va-\nriety of mainstream settings, thus effectively reflecting the\nadvantages of our few-shot learning setting. Notably, for\neach motion pattern, we randomly select a video from the\ncorresponding video set as the template to train Tune-A-\nVideo [37]. The comparisons are constructed in the view of\nobjective and subjective.\nTable 1. Quantitative comparisons with the evaluated text-to-\nvideo methods.\nMethod\nAlignment \u2191\nConsistency \u2191\nDiversity \u2193\nTune-A-Video [37]\n27.2227\n94.8742\n84.7186\nT2V-Zero [18]\n26.9424\n91.4713\n73.0136\nAnimateDiff [9]\n28.8779\n97.8131\n73.4723\nLAMP (Ours)\n31.3547\n98.3085\n71.6535\n(a) LAMP (full model)\n(b) replace SD-XL to SD-v1.4\n(c) w/o first-frame-conditioned pipeline\n(d) w/o shared-noise sampling\n(e) w/o temporal-spatial motion learning layer\n(f) w/o video-prediction-based 1D Conv\nFigure 5. Ablation results. The given prompt is \u2018A red horse runs\nin the sky\u2019.\nQuantitative results. We evaluate our LAMP against base-\nlines in terms of textural alignment, frame consistency, and\ngeneration diversity. The objective metrics and user study\nare used for a comprehensive evaluation.\nObjective metrics. To measure the textual alignment of\na video, we average each frame\u2019s CLIP score [26]. Fol-\nlowing [37], we also represent the frame consistency by the\nmean cosine similarity of CLIP image embedding across all\nframe pairs. Since generation freedom is one of our core\ngoals, we also include generation diversity in quantitative\nevaluation. We use the average CLIP image embedding of\nall frames to represent a video, subsequently computing and\naveraging the cosine distance across all video pairs. A lower\nscore denotes lower similarity, i.e., better diversity. Table 1\npresents the quantitative results of LAMP and baselines.\nAcross all three evaluation criteria, our method achieves\nstate-of-the-art performance against the other baselines.\nUser study. We further conduct a user study to evalu-\nate our approach and three baselines subjectively. We ran-\ndomly select 24 cases from our evaluation set. In each case,\nwe ask the participant \u201cWhich video do you think has better\nvisual quality and better matches the scene and motion of\nthe prompt \u2018...\u2019?\u201d The user study garnered a total of 70 re-\nsponses from a diverse group of participants, including both\nexperts in the field and individuals with no specific back-\nground knowledge. Statistically, 46.84% of respondents fa-\nvor our method, with AnimateDiff [9] achieving 19.11%\nand Tune-A-Video [37] achieving 22.15%. However, it is\nworth noting that Tune-A-Video polarizes choices in differ-\nent situations. When there are similarities between the lay-\nout of the given video template and the scene described by\nthe prompt, combined with its own good frame consistency,\nit can be approved by most volunteers. Conversely, the tex-\ntural alignment of its generated video is poor, e.g. \u201dFire-\nworks, grass land\u201d shown in Figure 4. Besides, 11.90% of\nthe participants select Text2Video-Zero [18] as their prefer-\nence. As a result, our LAMP obtains the highest approval\nrate among the participants.\nQualitative results. We present several visual examples\nof our method and three baselines in Figure 4. Animate-\nDiff [9] learns motion layers on large-scale data and in-\nserts them into personalized T2I models to generate videos\nwith specific styles and better visual quality. However, this\napproach cannot be combined with the better-performing\nbut heterogeneous T2I model, resulting in a limitation in\ntextural alignment capabilities even though the consistency\nand diversity are satisfying. This limitation is apparent in\ncases \u2018A horse runs in the universe\u2019 and \u2018Fireworks, grass\nland\u2019. Tune-A-Video (TAV) can only generate videos with\nthe same motion, with the prompts sometimes unable to ef-\nfectively control the generated videos due to overfitting on\nthe given video. While T2V-Zero produces visually pleas-\ning frames, it falls short in generating videos with meaning-\nful motion patterns. In contrast, our LAMP achieves good\nconsistency and generates videos with proper motion pat-\nterns, benefiting from the proposed motion learning layers.\nBesides, using the advantage of our first-frame-conditioned\npipeline, the proposed method achieves visual quality on\npar with state-of-the-art T2I models, even with the modi-\nfications based on SD-v1.4. Figure 1 and supplementary\nmaterials provide more visual results. Our method under-\nstands the learning motions well and can generalize to di-\nverse, even unseen, scenes and styles.\n4.3. Ablation Study\nWe conduct ablation experiments to demonstrate the effec-\ntiveness of each proposed component. The visual results\nare shown in Figure 5. As we can see in Figure 5(b), us-\ning SD-v1.4 to generate the first frame will decrease the\nperformance compared to the full model. Upon compar-\ning Figure 5(c) with the video generated by the full model,\nA man runs on the road\nA blond hair girl runs at night\nInput Video\nA girl in white is dancing\nA man is dancing\nFigure 6. Visual results of our video editing application. Zoom in for the best view.\nBeihong Xu\u2018s horse painting\nNiagara waterfall\nFigure 7. Visual results of LAMP animates the real-world images.\nthe model without the first-frame-conditioned pipeline pro-\nduces low-quality results. Notably, the presence of unre-\nlated objects, such as fences and dirt, in the video indi-\ncates an overfitting of the content of the video set. In ad-\ndition, the model w/o shared-noise sampling can generate\nrelatively consistent frames but lacks smoothness in the re-\nsult. When the temporal-spatial motion learning layers are\nremoved, the model cannot effectively capture the complex\nmotion pattern, leading to failed results. Finally, when we\nturn video-prediction-based 1D convolution into the origi-\nnal version of 1D convolution, the main object of the video\nbecomes inconsistent. The proposed layer can effectively\npreserve and propagate the features of the first frame to the\nsubsequent frames, as depicted in the results. These results\nverify the significant contributions of each key module to\nthe final full model.\n5. More Applications\nIn this section, we provide more applications of the pro-\nposed LAMP. While primarily designed for text-to-video\ngeneration, our framework can also be used for real image\nanimation and video editing.\n5.1. Real Image Animation\nThrough\nthe\ntraining\nof\nthe\nproposed\nfirst-frame-\nconditioned pipeline,\nour LAMP contains a network\nthat predicts the subsequent frames based on the given first\nframe.\nThis enables the animation of real-world images\ngenerated by T2I models. Thus, our method naturally gains\nthe capability to animate real-world images based on the\nlearned motion patterns if these images are placed in the\nfirst frame.\nFigure 7 shows several representative cases\nin which the \u2018horse run\u2019 model animates a famous horse\npainting created by Beihong Xu and the \u2018waterfall\u2019 model\nmakes the wonderful Niagara waterfall flow. This applica-\ntion further demonstrates our generalization performance,\neven when dealing with complex real-world scenes.\n5.2. Video Editing\nIn cases where the given training set contains only a sin-\ngle video clip, our method can only learn a specific mo-\ntion rather than a motion pattern. In this special case, our\nmethod effectively turns into a video editing algorithm. The\ntraining process remains similar to that in the few-shot set-\nting. During inference, we adopt the ControlNet [41] based\non SD-XL [24] and condition it on canny edges to edit the\nfirst frame. DDIM inversion [37] is also used to provide\na base motion, thereby ensuring better subsequent-frame\nprediction.\nSimilarly to video generation, our approach\ncan also take full advantage of image-editing technologies\nwhen applied to video editing. As visual examples shown in\nFigure 6, our LAMP generates photo-realistic videos while\nmaintaining good frame consistency.\n6. Limitation and Future Works\nIn our experiments, we observed that the occurrence of fail-\nure cases increased as our method attempted to learn com-\nplex motions. More effective modules for motion learn-\ning are potential solutions to this issue. Besides, we found\nthat the motion of the foreground object sometimes influ-\nences the background\u2019s stability. We believe that learning\nthe foreground and background movements independently\nmight be an effective solution. We leave these improve-\nments in our future work.\n7. Conclusion\nThis paper proposes a novel setting, few-shot tuning for\nT2V generation, which learns a common motion pattern\nfrom a small video set to achieve a trade-off between\ntraining burden and generation freedom.\nThe proposed\nLAMP serves as a baseline for this new setting. In our\nmethod, we transfer the T2V task into T2I generation\nfor the first frame and predict the subsequent frames.\nThis avoids overfitting the content of the dataset dur-\ning few-shot tuning while leveraging the advantages of\ntext-to-image techniques.\nMoreover, our novel design\nin network architecture and inference strategy further\nboosts the performance of T2V generation.\nExtensive\nexperiments demonstrate the effectiveness and general-\nization capability of our method.\nWe believe that the\nfew-shot tuning setting offers superior trade-offs and\nwill aid the broader T2V field in exploring the lower\nbounds on the data required for video diffusion training.\nReferences\n[1] Max Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisser-\nman. Frozen in time: A joint video and image encoder for\nend-to-end retrieval. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1728\u20131738,\n2021. 3\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563\u201322575, 2023. 2, 3\n[3] Ricky TQ Chen, Jens Behrmann, David K Duvenaud, and\nJ\u00a8orn-Henrik Jacobsen. Residual flows for invertible genera-\ntive modeling. Advances in Neural Information Processing\nSystems, 32, 2019. 3\n[4] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 12873\u201312883, 2021. 3\n[5] Patrick Esser,\nJohnathan Chiu,\nParmida Atighehchian,\nJonathan Granskog, and Anastasis Germanidis.\nStructure\nand content-guided video synthesis with diffusion models.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 7346\u20137356, 2023. 2, 3, 5\n[6] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel.\nTokenflow: Consistent diffusion features for consistent video\nediting. arXiv preprint arXiv:2307.10373, 2023. 2, 3\n[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial networks. Commu-\nnications of the ACM, 63(11):139\u2013144, 2020. 3\n[8] Matej Grci\u00b4c, Ivan Grubi\u02c7si\u00b4c, and Sini\u02c7sa \u02c7Segvi\u00b4c. Densely con-\nnected normalizing flows. Advances in Neural Information\nProcessing Systems, 34:23968\u201323982, 2021. 3\n[9] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 2, 3, 6, 7\n[10] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 3\n[11] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 2, 3\n[12] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2, 3\n[13] Susung Hong, Junyoung Seo, Sunghwan Hong, Heeseong\nShin, and Seungryong Kim.\nLarge language models are\nframe-level directors for zero-shot text-to-video generation.\narXiv preprint arXiv:2305.14330, 2023. 3\n[14] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.\nLora: Low-rank adaptation of large language models. arXiv\npreprint arXiv:2106.09685, 2021. 3\n[15] Xiaotao Hu, Zhewei Huang, Ailin Huang, Jun Xu, and\nShuchang Zhou. A dynamic multi-scale voxel flow network\nfor video prediction. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n6121\u20136131, 2023. 2, 5\n[16] Hanzhuo\nHuang,\nYufan\nFeng,\nand\nChengShi\nLanXu JingyiYu SibeiYang.\nFree-bloom:\nZero-shot\ntext-to-video generator with llm director and ldm animator.\narXiv preprint arXiv:2309.14494, 2023. 3\n[17] Xun Huang and Serge Belongie. Arbitrary style transfer in\nreal-time with adaptive instance normalization. In Proceed-\nings of the IEEE international conference on computer vi-\nsion, pages 1501\u20131510, 2017. 6\n[18] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2, 3, 6, 7\n[19] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n[20] William Lotter, Gabriel Kreiman, and David Cox. Deep pre-\ndictive coding networks for video prediction and unsuper-\nvised learning. arXiv preprint arXiv:1605.08104, 2016. 2,\n5\n[21] Eyal Molad, Eliahu Horwitz, Dani Valevski, Alex Rav\nAcha, Yossi Matias, Yael Pritch, Yaniv Leviathan, and Yedid\nHoshen. Dreamix: Video diffusion models are general video\neditors. arXiv preprint arXiv:2302.01329, 2023. 2, 3\n[22] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2, 3\n[23] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195\u20134205,\n2023. 3\n[24] Dustin\nPodell,\nZion\nEnglish,\nKyle\nLacey,\nAndreas\nBlattmann, Tim Dockhorn, Jonas M\u00a8uller, Joe Penna, and\nRobin Rombach.\nSdxl: Improving latent diffusion mod-\nels for high-resolution image synthesis.\narXiv preprint\narXiv:2307.01952, 2023. 2, 3, 5, 6, 8\n[25] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei,\nXintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fus-\ning attentions for zero-shot text-based video editing. arXiv\npreprint arXiv:2303.09535, 2023. 2, 3\n[26] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 7\n[27] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 2, 3\n[28] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 2, 3, 6\n[29] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500\u2013\n22510, 2023. 3\n[30] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 2, 3\n[31] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[32] Kihyuk Sohn, Honglak Lee, and Xinchen Yan.\nLearning\nstructured output representation using deep conditional gen-\nerative models. Advances in neural information processing\nsystems, 28, 2015. 3\n[33] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 2, 3\n[34] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equa-\ntions. arXiv preprint arXiv:2011.13456, 2020. 2, 3\n[35] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 3\n[36] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 2, 3, 5\n[37] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian\nLei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu\nQie, and Mike Zheng Shou. Tune-a-video: One-shot tuning\nof image diffusion models for text-to-video generation. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 7623\u20137633, 2023. 2, 3, 6, 7, 8\n[38] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun,\nBei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-\nvancing high-resolution video-language representation with\nlarge-scale video transcriptions.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5036\u20135045, 2022. 3\n[39] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change\nLoy. Rerender a video: Zero-shot text-guided video-to-video\ntranslation. arXiv preprint arXiv:2306.07954, 2023. 2, 3\n[40] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\ngan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks. In Proceedings of the IEEE\ninternational conference on computer vision, pages 5907\u2013\n5915, 2017. 3\n[41] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836\u20133847, 2023. 3, 5, 8\n[42] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 2, 3\n"
  },
  {
    "title": "CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion",
    "link": "https://arxiv.org/pdf/2310.11248.pdf",
    "upvote": "3",
    "text": "CROSSCODEEVAL: A Diverse and Multilingual\nBenchmark for Cross-File Code Completion\nYangruibo Ding1\u2217 Zijian Wang2,\u2217 Wasi Uddin Ahmad2,\u2217\nHantian Ding2 Ming Tan2 Nihal Jain2 Murali Krishna Ramanathan2\nRamesh Nallapati2 Parminder Bhatia2 Dan Roth2 Bing Xiang2\n1Columbia University 2AWS AI Labs\nyrbding@cs.columbia.edu\n{zijwan,wuahmad}@amazon.com\nhttps://crosscodeeval.github.io\nAbstract\nCode completion models have made significant progress in recent years, yet current\npopular evaluation datasets, such as HumanEval and MBPP, predominantly focus\non code completion tasks within a single file. This over-simplified setting falls short\nof representing the real-world software development scenario where repositories\nspan multiple files with numerous cross-file dependencies, and accessing and\nunderstanding cross-file context is often required to complete the code correctly.\nTo fill in this gap, we propose CROSSCODEEVAL, a diverse and multilingual\ncode completion benchmark that necessitates an in-depth cross-file contextual\nunderstanding to complete the code accurately. CROSSCODEEVAL is built on a\ndiverse set of real-world, open-sourced, permissively-licensed repositories in four\npopular programming languages: Python, Java, TypeScript, and C#. To create\nexamples that strictly require cross-file context for accurate completion, we propose\na straightforward yet efficient static-analysis-based approach to pinpoint the use of\ncross-file context within the current file.\nExtensive experiments on state-of-the-art code language models like CodeGen and\nStarCoder demonstrate that CROSSCODEEVAL is extremely challenging when the\nrelevant cross-file context is absent, and we see clear improvements when adding\nthese context into the prompt. However, despite such improvements, the pinnacle\nof performance remains notably unattained even with the highest-performing\nmodel, indicating that CROSSCODEEVAL is also capable of assessing model\u2019s\ncapability in leveraging extensive context to make better code completion. Finally,\nwe benchmarked various methods in retrieving cross-file context, and show that\nCROSSCODEEVAL can also be used to measure the capability of code retrievers.\n1\nIntroduction\nLanguage models for code (code LMs), such as Codex (Chen et al., 2021), CodeGen (Nijkamp\net al., 2023b,a), and StarCoder (Li et al., 2023), have demonstrated their power to enhance developer\nproductivity through their promising results in code completion tasks. To evaluate these models,\nresearchers propose multiple code completion evaluation benchmarks, (e.g., Chen et al., 2021; Lu\net al., 2021; Athiwaratkun et al., 2023; Austin et al., 2021), where the model is asked to complete\nthe code given the context in the current file. However, such an evaluation setting is over-simplified,\nand it is not able to reflect the model\u2019s capability in code completion accurately. Specifically, in the\nrealm of modern software development, repositories consist of multiple files, each interwoven with\n\u2217 Equal Contribution. Work done while Yangruibo Ding was an intern at AWS AI Labs.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks.\narXiv:2310.11248v2  [cs.LG]  17 Nov 2023\nFigure 1: Code LM fails to complete a Python test case since the in-file context (left figure) does\nnot provide sufficient information. The function name from the current file indicates that the\ncompleting function is a test case for convert_camel_to_snake, so with only such context, the\nmodel hallucinates wrong completion as convert_camel_to_snake. However, the failure is not\ndue to the model\u2019s capacity, but the necessary cross-file context is not present (right figure). When\nthe class CaseConverter is present in the prompt, the model generates camel_to_snake correctly.\nextensive cross-file dependencies, i.e., contextual information from other source code files within the\nsame repository. A significant drawback of most existing benchmarks is their tendency to overlook\nthese complex dependencies. As a result, they fall short of providing a comprehensive evaluation of\ncode completion models within realistic real-world scenarios.\nFigure 1 illustrates the limitation of common code completion evaluation sets with a real example.\nThe developer is writing a test case for a class, CaseConverter, implemented within the current\nrepository. CodeGen-2B-mono (Nijkamp et al., 2023b), a large Python code LM, fails to complete\nthe API call if only the current file context is present.\nMotivated by such examples and to fill in the need of evaluating code completion in realistic software\ndevelopment with numerous intervening cross-file context dependencies, we propose CROSSCODEE-\nVAL, a diverse and multilingual benchmark to evaluate code language models\u2019 ability to use cross-file\ncontext for code completion. This new dataset is composed of 10k examples from 1k repositories in\n4 languages. Unlike existing datasets where the correct answer could be predicted with only context\nfrom the current file, CROSSCODEEVAL strictly requires cross-file context to correctly complete the\nmissing code (\u00a72.2). CROSSCODEEVAL\u2019s examples are carefully curated from existing open-sourced\nrepositories with a series of quality filters, and we ensure CROSSCODEEVAL has minimal overlap\nwith the training data from existing code LMs, eliminating the confounder of data leakage and\nmemorization in result interpretation (\u00a72.1 & 2.3).\nWe conducted a comprehensive evaluation of popular public and proprietary code LMs: CodeGen\n(Nijkamp et al., 2023b,a) and StarCoder (Li et al., 2023) in various sizes from 350M to 16B\nparameters, and OpenAI\u2019s GPT-3.5-Turbo in \u00a73. Empirical results reveal that when given only\nthe current-file context, these models yield suboptimal results. Remarkably, incorporating cross-file\ncontext into the prompt significantly enhances the performance of these code LMs, even in a zero-\nshot setting. This underscores that CROSSCODEEVAL effectively serves its goal as a benchmark\naimed at evaluating cross-file code completion. Moreover, even when providing cross-file context in\nthe prompt, the performance of the most powerful models remains notably imperfect, highlighting\nthat CROSSCODEEVAL is also instrumental in assessing a model\u2019s ability for leveraging extensive\ncontext in code completion. Lastly, we benchmarked various retrieval methods from sparse to dense,\ndemonstrating that CROSSCODEEVAL can additionally serve as a benchmark for code retrieval.\n2\nCROSSCODEEVAL: A Benchmark for Cross-File Code Completion\nCROSSCODEEVAL is a diverse and multilingual scope completion dataset in four popular languages:\nPython, Java, TypeScript, and C# where examples include code prompts ending in an imagined cursor\nposition and references include the code token sequences from the cursor position to the end of the\nstatement. CROSSCODEEVAL examples have a key property - the statement to be completed must\n2\nFigure 2: We replace the third import statement, which is from the same repository, with an empty\nclass. Consequently, camel_to_snake in the last line becomes an undefined name in the modified\nfile. Thereby, we know this method in the original file is defined only in the cross-file context.\nhave at least one use of local API (classes, variables, and methods defined in the software repository).\nNext, we briefly describe how we collect the software repositories (\u00a72.1), select a subset of them\nfor CROSSCODEEVAL construction (\u00a72.2), post-processing and quality control process (\u00a72.3), and\nCROSSCODEEVAL statistics and future scope for extending the dataset (\u00a72.4).\n2.1\nDataset Collection\nWe collect permissively licensed repositories from GitHub. To mitigate potential data leakage issues,2\nwe focus on repos that were created recently and not forks. Specifically, we collected repos created\nbetween 2023-03-05 to 2023-06-15 on 2023-09-01. The time span ensures sufficient data collected\nwith no overlap with the training data of many existing code LMs released before mid-2023, no\nmatter whether the data is publicly available or not. We limit repos to contain the four languages we\nstudy and we keep only repos with zipped file size < 1MB and number of stars >= 3. Then we filter\nout repos that have fewer than 10 or more than 50 source code files. Finally, we remove the repos\nwith at least one source code file that exactly matches one of the code files in the commonly used\nStack (Kocetkov et al., 2022) dataset. As a result, we ended up with 471, 239, 193, and 99 repos,\nrespectively.\n2.2\nDataset Generation\nWe propose a static-analysis-based method to identify code fragments that require cross-file context\nautomatically. Our approach is illustrated in Fig 2. First, we find all intra-project imports in the\noriginal file. Next, an empty class is created for each imported name to replace the import statement.\nSince the imported name now refers to an empty class, any subsequent call to its member function or\nattribute will raise an undefined name error. We leverage static analysis to catch such errors in the\nmodified file, which precisely correspond to the names in the original file that can only be resolved\nby cross-file context. We map the location of undefined names back to the original file to determine\nthe split point of the prompt and the reference.\nTo increase the variety in the dataset, we randomly select a tree-sitter3 token in the same line before\nthe cross-file entity to be the cursor location, splitting the code to a prompt and a reference. It is\noften the case that the same cross-file API is called multiple times in a file, and there is a chance for\nmodels to infer the API name from previous calls even without cross-file context. Therefore, if the\nsame undefined name is reported at multiple places in a file, we keep only the first occurrence. In this\nwork, we focus on instantiating our approach in the four popular programming languages, while the\nidea can be generalized to other languages in principle. Specifically, for Python, we use Pylint4\nto detect undefined names; for Java, we use javac compiler; for TypeScript, we use tsc compiler;\n2Overlapping between CROSSCODEEVAL and data used to pretrain code LMs.\n3https://tree-sitter.github.io/tree-sitter/\n4Pylint is a static code analyzer for Python: https://pylint.readthedocs.io/en/latest/\n3\nfor C#, we use csc compiler from the mono5 image. We use tree-sitter to identify full statements to\nconstruct reference completions in Python. For Java, TypeScript, and C#, we consider statements\nending with either \u201c;\u201d, \u201c{\u201d, or \u201c}\u201d. See Appendix A for more details.\n2.3\nPost-processing and Quality Control\nWe designed a series of rule-based and model-based post-processing filters to ensure the quality of the\ndataset. We filter examples if (1) fewer than N lines of code (lines not including import statements,\nwhere N = 10, 20, 30, 5 for Python, Java, TypeScript, and C#) in the prompt, (2) too short (< 3\ntokens), or long (> 30 tokens) reference. We exclude examples if the references are found verbatim\nin any other source code file within the repository (i.e., cross-file). We further discard examples with\nduplicate references. The filtering steps collectively remove 15%-20% of the examples.\nMoreover, to ensure that the reference isn\u2019t predictably inferred solely from the current file (possibly\nowing to strong clues in function names and comments), we feed the examples (input prompts)\nto starcoderbase-1B model (Li et al., 2023) to complete the statement and remove the exact\nmatches. This step results in the removal of <10% of the generated examples. As an ancillary\nbenefit, this further safeguards that the examples are not seen by publicly available code LMs while\nCROSSCODEEVAL is constructed based on repositories that do not overlap with the Stack and possibly\nother private pre-training datasets created prior to 2023. Finally, we perform human annotations on a\nsubsample of the resulting CROSSCODEEVAL and found that the dataset has a satisfactory quality to\nserve the goal of cross-file code completion. See Appendix B for more details.\n2.4\nDataset Statistics, Scope, and Future Extensions\nFeature\nPython\nJava\nTypeScript\nC#\n# Repositories\n471\n239\n193\n99\n# Files\n1368\n745\n779\n642\n# Examples\n2665\n2139\n3356\n1768\nAvg. # lines in prompt\n90.6\n106.7\n116.5\n71.1\nAvg. # tokens in prompt\n938.9\n995.3\n944.9\n584.1\nAvg. # lines in reference\n1.0\n1.1\n1.7\n1.7\nAvg. # tokens in reference\n13.2\n14.5\n17.4\n12.5\nTable 1: CROSSCODEEVAL statistics.\nStatistics\nWe present the statis-\ntics of CROSSCODEEVAL in Ta-\nble 1. We use the StarCoder tok-\nenizer (Li et al., 2023) to compute\nthe number of tokens.\nScope\nIn addition to prompts\nand references, we include the\ncode lines that follow the refer-\nences from the original source\ncode files in CROSSCODEEVAL\nexamples. Given the source code\nlines to the left (prompt or prefix) and right (suffix) of the references, CROSSCODEEVAL can be used\nto evaluate code LMs for their fill-in-the-middle (FIM) capabilities (Bavarian et al., 2022).\nFuture Extensions\nCROSSCODEEVAL currently supports four popular languages. As our method\nis generalizable, CROSSCODEEVAL can potentially be extended to other languages. Additionally, we\nadvise future code LM pre-training datasets should explicitly exclude CROSSCODEEVAL to minimize\nthe effect of memorization.\n3\nExperiments\n3.1\nModels\nWe benchmark CROSSCODEEVAL with popular public and proprietary large language models.\nCodeGen (Nijkamp et al., 2023b,a) is a series of generative code LMs. CodeGen supports left-only\ncontext. CodeGen2.5 notably supports fill-in-the-middle and further improves the performance via\nmulti-epoch training. We benchmarked CodeGen models at various sizes from 350M to 16B.\nStarCoder (Li et al., 2023) is a generative multi-query-based code LM with 15.5B model parameters\ntrained on The Stack dataset (Kocetkov et al., 2022). It supports up to 8k tokens. We also benchmarked\nits base version with at varied sizes: 1B, 3B, and 7B.\n5Mono is an open-source implementation of the .NET Framework: https://www.mono-project.com/\n4\nFigure 3: An illustrative example to showcase the use of in-file context, retrieved cross-file context,\nand retrieved context using reference in prompts. While the baseline prompting uses in-file context\nonly, \u201cRetrieval\u201d and \u201cRetrieval w/ Ref.\u201d prompting uses retrieved contexts by prepending them to\nthe in-file context.\nGPT-3.5-turbo (Ouyang et al., 2022) is one of the most powerful models developed by OpenAI. It\nwas trained with comprehensive text and code data and supports up to 4k max sequence length. Its\nmodel weight remains proprietary and is accessible exclusively via APIs.\n3.2\nEvaluation Metrics\nIn evaluating the performance of code language models, we report performance in two main categories:\ncode match and identifier match (Ding et al., 2022).\nCode Match\nThe code match metric directly compares the generated code with the reference and\nis measured using exact match (EM) and edit similarity (ES). These metrics help to assess the overall\naccuracy of the code completion process, taking into account elements such as identifiers, keywords,\noperators, delimiters, and literals.\nIdentifier Match\nThis metric evaluates the model\u2019s ability to predict the correct application\nprogramming interfaces (APIs). To perform this evaluation, we first parse the code and extract the\nidentifiers from the model prediction and reference, resulting in two ordered lists of identifiers. We\nthen compare the predicted identifiers with the reference and report the results in EM and F1 score.\n3.3\nExperimental Setup\nOur evaluation framework is based on the Transformers (Wolf et al., 2020) library. All the experiments\nare conducted with the zero-shot setting, and no training is involved. We use the same set of\nhyperparameters for code generation across all models. We set the maximum sequence length to\n2,048 for the CodeGen family, 4096 for GPT-3.5-turbo, and 8,192 for the StarCoder family. We use a\nmaximum generation length of 50 and the rest as the prompt.\nWe explore greedy search and nucleus sampling (Holtzman et al., 2020) with reranking (Hossain\net al., 2020). We found there is no significant difference between the two, and we present the greedy\nsearch results in the main paper and refer readers to Appendix D.2 for nucleus sampling.\nWe post-process model predictions to extract statements.6 For Python, we iteratively parse the\nconcatenation of prompt and n completion tokens (e.g., n = 1, 2, . . . , 50) until the sequence becomes\n6We apply the same post-processing on the references before calculating the evaluation metrics.\n5\nparseable (no syntax error) and the (n + 1)-th completion token is a newline character.7 For Java,\nTypeScript, and C#, we consider statements ending with \u201c;\u201d, \u201c{\u201d and \u201c}\u201d, instead of a new line.\nOnly In-File Context\nIn standard practice, pre-trained language models are utilized to perform\ncode completion in a zero-shot manner by taking into account the provided code context. Following\nthe practice, we conduct experiments using the code LMs (3.1), where they are provided code context\nfrom the current file. As shown in Figure 3, the baseline prompt includes only in-file context.\nRetreived Cross-file Context\nInspired by the effectiveness of the recently proposed retrieve-and-\ngenerate (RG) framework for repository-level code completion (Zhang et al., 2023), we adopt it for\ncross-file context retrieval.8 In the RG framework, the retrieval database is constructed by iteratively\nscanning the files in the repository and extracting contiguous M lines (in all our experiments,\nM = 10) of non-overlapping code fragments, which are the candidates for cross-file context retrieval.\nThe query for the retrieval is built using the last N lines (we set N = 10) of the in-file context. We\nuse BM25 (Robertson et al., 2009) to calculate the similarity between the query and the candidates\n(cross-file context chunks), and use the top-5 similar code snippets as the cross-file context, see\n\u201cRetrieval Context\" in Figure 3. We consider a maximum of 512 BPE tokens for such context, and the\nrest of the tokens will be truncated. Figure 3 illustrates9 the retrieved context and the corresponding\nprompt for the model to complete. Given the in-file context as a query, the RG framework successfully\nretrieves the class definition of CaseConverter that is in another file for utilities. We further wrap\nthe class definition into a template as code comment and use it as the cross-file context. To build the\nretrieval prompt, we prepend the retrieved context to the in-file context.\nRetrieval with Reference\nTo quantify the upper bound impacts of cross-file context retrieved by\nthe RG framework, we devise \u201cretrieval with reference\" for comparison. In this setting, we make use\nof not only the in-file context (as in standard retrieval setting) but also the reference to retrieve the\ncross-file context. Specifically, the query is constructed by using the last N lines of the concatenation\nof the in-file context and the reference completion, instead of the in-file context only in the standard\nretrieval setting. We prepend the retrieved context (i.e., \u201cRetrieval w/ Ref. Context\") to in-file context\nto construct the prompt for this setting.\nNote that the Retrieval w/ Ref. context could not be applied to the realistic code completion, as the\nreference completion is unknown. We use it as an estimation of the upper bound model performance\nwith the RG framework. Also, the model\u2019s performance in this setting is not optimal, as it can still be\nlimited by imperfect retrieval and the model\u2019s capability in making use of retrieved code, and we\nperform additional benchmarking and analysis on retrieval methods later in \u00a73.5.\n3.4\nResults\nWe present results in Table 2 and additional results in Table 7. We see that all models perform poorly\nwhen the prompt includes only the in-file context. For example, the best-performing StarCoder\nmodel at 15.5B size only reports 8.82% code exact match in Python. Even a large code LM\nstruggles to achieve promising performance in completing CROSSCODEEVAL samples with only\nin-file context since it could not provide sufficient clues for code completion. This shows the design\nof CROSSCODEEVAL that cross-file context is necessary to complete the code correctly.\nThe performance improves dramatically when the cross-file context is added to the prompts across\nall models and sizes. Figure 4 shows the significant improvements resulting from the inclusion of\ncross-file context in CodeGen and StarCoder models. Looking at Table 2, we see that the StarCoder\nmodel reports up to 3.0\u00d7 and 4.5\u00d7 better exact code match when including retrieved and retrieved\nwith reference context respectively. The results underline the limitation of existing datasets that only\nconsider the in-file context to evaluate code LMs, making these datasets insufficient in reflecting\nmodels\u2019 best capacity in real-world scenarios. In contrast, CROSSCODEEVAL maintains the cross-file\n7We manually verified that this is required to extract full statements in Python.\n8We tried to use the code made publicly available by the authors of Zhang et al. (2023) but failed to execute\nthem with the provided instructions. As a result, we implemented the RG approach by ourselves. Note that,\nunlike Zhang et al. (2023), we perform only 1-step retrieval augmented generation.\n9For the convenience of illustration, we remove the irrelevant code snippets from the retrieved context to\navoid confusion. In practice, the retrieved context spans across a fixed length of lines, which includes useful\ninformation as well as its surrounding lines. More implementation details are in Appendix C.\n6\ncontexts for code completion samples, providing resources to both identify the model\u2019s best capacity\nand analyze the model\u2019s behavior when seeing a more comprehensive context.\nModel\nCode Match\nPython\nJava\nTypeScript\nC#\nEM\nES\nEM\nES\nEM\nES\nEM\nES\nCodeGen25-7B\n7.73\n59.34\n10.43\n62.05\n7.81\n57.56\n4.36\n58.99\n+ Retrieval\n14.52\n64.40\n16.88\n64.35\n12.57\n60.08\n13.01\n63.86\n+ Retrieval w/ Ref.\n19.17\n67.46\n20.20\n66.17\n15.35\n62.73\n17.87\n66.14\nStarCoder-15.5B\n8.82\n61.08\n9.96\n63.25\n6.35\n51.22\n4.47\n59.80\n+ Retrieval\n15.72\n66.28\n17.48\n66.10\n8.31\n44.87\n13.57\n65.00\n+ Retrieval w/ Ref.\n21.01\n68.66\n19.92\n67.75\n11.02\n46.67\n20.08\n67.97\nGPT-3.5-turbo\n4.88\n52.58\n12.30\n63.52\n6.38\n53.78\n3.56\n56.48\n+ Retrieval\n10.77\n54.92\n19.12\n65.61\n10.94\n55.83\n11.82\n62.40\n+ Retrieval w/ Ref.\n15.72\n58.88\n22.72\n68.50\n14.15\n58.40\n17.65\n66.07\nModel\nIdentifier Match\nPython\nJava\nTypeScript\nC#\nEM\nF1\nEM\nF1\nEM\nF1\nEM\nF1\nCodeGen25-7B\n14.26\n46.02\n16.60\n51.43\n12.46\n47.75\n7.69\n33.81\n+ Retrieval\n22.96\n53.68\n24.03\n55.48\n17.85\n51.27\n17.36\n43.56\n+ Retrieval w/ Ref.\n28.33\n57.95\n27.91\n57.87\n21.51\n55.38\n21.78\n47.63\nStarCoder-15.5B\n15.72\n48.16\n18.28\n53.23\n11.86\n43.53\n8.54\n34.33\n+ Retrieval\n24.77\n55.57\n25.95\n57.74\n14.09\n39.50\n18.04\n44.38\n+ Retrieval w/ Ref.\n30.24\n59.46\n29.73\n60.47\n17.55\n42.18\n24.38\n49.09\nGPT-3.5-turbo\n10.09\n39.18\n18.93\n52.52\n10.76\n44.78\n5.77\n30.25\n+ Retrieval\n17.37\n44.43\n26.74\n56.57\n16.69\n48.15\n15.44\n41.24\n+ Retrieval w/ Ref.\n23.49\n50.14\n31.79\n60.52\n20.65\n51.54\n21.72\n47.21\nTable 2: Performance of various code LMs on CROSSCODEEVAL.\u201cRetrieval\u201d and \u201cRetrieval w/ Ref.\u201d\nmean we construct the prompt by prepending the retrieved cross-file context retrieved with the prompt\nand the prompt + reference (see \u00a73.3 for details). The performance with no cross-file context (first\nrow in each section) is generally poor. When prompts are augmented with cross-file context (middle\nrow in each section), the performance increases significantly. The use of reference completion in\nformulating the query for cross-file context retrieval (last row in each section) shows the upper bound\nof the retrieve-and-generate (RG) approach. Results of other models are in Table 7.\n3.5\nAnalysis and Discussions\nImproved vs. Degraded Code Completions\nTable 3 presents changes in the number of correct\ncompletions (based on exact match to the references) across different prompt settings. The results\nsuggest that all models follow a trend that the performance improves with better cross-file context\n(In-file \u2192 Retrieval \u2192 Retrieval w/ Ref.). However, the variation of correct/incorrect generation\nis significant; for example, when changing from Retrieval to Retrieval w/ Ref. with StarCoder in\nCROSSCODEEVAL Python, we see 327 correct generations changed to incorrect, and 468 generations\nchanged the other way around. Upon manual inspections, we see that the retrieval of the correct\ncross-file context plays a huge role, as the quality of the retrieval directly correlates with whether\nthe model is able to generate correctly. This effect is further enhanced by the fact that the retrieval\nhappens in fixed lines of code that do not often follow code structure, making it difficult for the model\nto digest, especially at zero-shot settings, echoing results from Zhang et al. (2023). This highlights\nthat the current best models are still imperfect in leveraging extensive context to make better code\ncompletion. Further, it calls for additional studies in optimizing the retrieval methods for code: we\nshow benchmarking with various retrieval methods later in the section.\n7\nModel\nPython\nJava\nIn-file\n\u2192\nRet.\n\u2192\nRet.\nw/ Ref\nIn-file\n\u2192\nRet.\n\u2192\nRet.\nw/ Ref\nCodeGen-350M\n+72\n\u221268\n+182\n+186\n\u2212166\n+237\n+257\n+75\n\u221271\n+145\n+149\n\u2212135\n+158\n+172\nCodeGen-16.1B\n+183\n\u2212162\n+311\n+332\n\u2212259\n+371\n+444\n+150\n\u2212133\n+233\n+250\n\u2212209\n+251\n+292\nCodeGen25-7B\n+206\n\u2212172\n+353\n+387\n\u2212306\n+430\n+511\n+223\n\u2212179\n+317\n+361\n\u2212277\n+348\n+432\nStarCoder\n+235\n\u2212192\n+376\n+419\n\u2212327\n+468\n+560\n+213\n\u2212167\n+328\n+374\n\u2212300\n+352\n+426\nModel\nTypeScript\nC#\nIn-file\n\u2192\nRet.\n\u2192\nRet.\nw/ Ref\nIn-file\n\u2192\nRet.\n\u2192\nRet.\nw/ Ref\nCodeGen-350M\n+93\n\u221285\n+127\n+135\n\u2212128\n+168\n+175\n+16\n\u221216\n+58\n+58\n\u221251\n+108\n+115\nCodeGen-16.1B\n+151\n\u2212140\n+226\n+237\n\u2212218\n+289\n+308\n+32\n\u221229\n+95\n+98\n\u221285\n+138\n+151\nCodeGen25-7B\n+262\n\u2212226\n+386\n+422\n\u2212341\n+434\n+515\n+77\n\u221262\n+215\n+230\n\u2212189\n+275\n+316\nStarCoder\n+213\n\u2212198\n+264\n+279\n\u2212241\n+338\n+376\n+79\n\u221266\n+227\n+240\n\u2212195\n+310\n+355\nTable 3: The numbers of correct code completions using different code generation models on the\nCROSSCODEEVAL benchmark. \u201cIn-file\" refers to the prompts being constructed only with in-file\ncontext, and \u201cRetrieval\" and \u201cRet. w/ Ref\" refer to the prompts being constructed with the retrieved\ncontexts described in \u00a73.3.\nFigure 4: Performance of models in various sizes.\nScalability of Model Performance\nFigure 4\nvisualizes how the performance of CodeGen and\nStarCoder scales w.r.t. model sizes. We see the\nperformance increases following the power law\nin all settings as expected (Kaplan et al., 2020).\nHowever, again the performance is far from per-\nfect even with the best performing model with\nthe best context retrieved.\nLocations of Retrieved Cross-file Context\nTo\nidentify relevant cross-file context, we retrieve\ncode snippets from other files of the repository.\nTo understand which files contribute to the cross-\nfile context, we further conduct a study on the\nretrieved code snippets. To analyze the code snip-\npets retrieved for each prompt, we examine the\nfiles to determine whether they meet the follow-\ning criteria: (1) they are imported by the target\nfile, (2) they are located in the same directory as\nthe target file, (3) they have a similar name to\nthe target file (with filename sharing at least one token, assuming snake-case or CamelCase style\nfilenames), and (4) they include at least one API import within the project, similar to the target file.\nOur analysis shows that most of the code snippets are sourced from files that are either from the same\ndirectory with the target file (Python: 49.0%, Java: 37.8%, TypeScript: 51.3%, C#: 51.7%), or have\nsimilar names (Python:33.4%, Java:44.5%, TypeScript: 24.9%, C#: 39%). We also observed that\ntarget files and cross-files often share at least one intra-project API import statement. This result\naligns with the findings of Zhang et al. (2023).\nIdentifier Overlap with Retrieved Cross-file Context\nIdentifiers are a significant part of pro-\ngramming language constructs that cover API mentions in a source code. Therefore, we examine\nthe distribution of the retrieved cross-file contexts for examples in CROSSCODEEVAL that include\nmentions of identifiers that are also present in the references. In Figure 5, we show the distribution\nand the identifier exact match performance achieved by the best performing code LM, StarCoder.\nIn general, it is evident that an increased ratio of identifier overlap results in higher performance,\n8\n(a) Python\n(b) Java\n(c) TypeScript\n(d) C#\nFigure 5: Distribution of the examples according to identifier overlap between the retrieved cross-file\ncontext and the reference completion. We also show the corresponding identifier exact match scores.\nModel\nRetriever\nCode Match\nPython\nJava\nTypeScript\nC#\nEM\nES\nEM\nES\nEM\nES\nEM\nES\nCodeGen25-7B\n-\n7.73\n59.34\n10.43\n62.05\n7.81\n57.56\n4.36\n58.99\n+ Retrieval\nBM25\n14.52\n64.40\n16.88\n64.35\n12.57\n60.08\n13.01\n63.86\n+ Retrieval w/ Ref.\nBM25\n19.17\n67.46\n20.20\n66.17\n15.35\n62.73\n17.87\n66.14\n+ Retrieval\nUniXCoder\n13.73\n64.21\n15.61\n63.67\n12.10\n59.82\n12.39\n63.82\n+ Retrieval w/ Ref.\nUniXCoder\n18.01\n66.46\n18.19\n65.23\n14.84\n61.66\n16.46\n65.20\n+ Retrieval\nOpenAI ada\n14.82\n65.00\n17.77\n64.48\n12.75\n60.02\n14.71\n65.35\n+ Retrieval w/ Ref.\nOpenAI ada\n18.39\n66.80\n20.94\n66.27\n15.58\n62.65\n20.43\n68.65\nStarCoder-15.5B\n-\n8.82\n61.08\n9.96\n63.25\n6.35\n51.22\n4.47\n59.80\n+ Retrieval\nBM25\n15.72\n66.28\n17.48\n66.10\n8.31\n44.87\n13.57\n65.00\n+ Retrieval w/ Ref.\nBM25\n21.01\n68.66\n19.92\n67.75\n11.02\n46.67\n20.08\n67.97\n+ Retrieval\nUniXCoder\n15.87\n66.07\n16.83\n66.09\n7.87\n44.67\n11.93\n63.90\n+ Retrieval w/ Ref.\nUniXCoder\n19.32\n68.33\n19.45\n67.51\n10.28\n46.85\n16.63\n66.30\n+ Retrieval\nOpenAI ada\n16.47\n66.72\n17.53\n65.98\n8.43\n45.08\n15.39\n66.21\n+ Retrieval w/ Ref.\nOpenAI ada\n20.53\n68.50\n21.69\n68.11\n11.83\n47.31\n23.49\n70.56\nTable 4: Evaluation results of various sparse and neural methods in retrieving cross-file context.\nIdentifier Match results are in Table 9.\ndemonstrating a positive correlation. This calls for an investigation into retrieval techniques, with a\nparticular emphasis on key terms like identifiers for cross-file context retrieval.\nCROSSCODEEVAL as Code Retrieval Benchmark\nThe observations above (e.g., imperfect upper\nbound performance and identifier overlap) underscore the critical role of the code retrieval method.\nGiven the strong dependency that the correct prediction requires an accurate retrieval of relevant\ncross-file context, we propose to use CROSSCODEEVAL as a code retrieval benchmark. We perform\nexperiments with different retrievers from sparse (BM25 as we used in the rest of the experiments) to\nneural (UniXCoder (Guo et al., 2022) and OpenAI embedding10). For UniXCoder, we use a max\n10We use text-embedding-ada-002.\n9\nsequence length of 256 per 10-line chunk and for OpenAI embedding we use 8,000. We use the\ncosine similarity of the embedding of the prompt and the chunks to retrieve top 5 chunks.\nTable 4 shows the results with these retrieval methods. On one hand, we see BM25 provides a\nstrong baseline and, in most of the cases, can outperform UniXCoder-based retriever. On the other\nhand, retrieving with OpenAI\u2019s ada embedding is generally better than both BM25 and UniXCoder,\nespecially for Java and C#. Nonetheless, the performance with the best performing retriever is still\nsuboptimal (< 20 EM in all languages), calling for future development of better code retriever.\n4\nRelated Works\nThe advent of code language models (LMs) (Feng et al., 2020; Ahmad et al., 2021; Wang et al., 2021;\nGuo et al., 2022) have bolstered the automation of software engineering applications. Among them,\ncode completion has got the most attention, and as a result, generative AI powered by large language\nmodels for code (Chen et al., 2021; Xu et al., 2022; Wang and Komatsuzaki, 2021; Black et al., 2021,\n2022; Nijkamp et al., 2023b; Fried et al., 2023; Li et al., 2022; CodeGeeX, 2022; Allal et al., 2023;\nLi et al., 2023; Nijkamp et al., 2023a) has become a reality. Benchmark datasets have been playing\na pivotal role in advancing the field of generative AI for code. A large pool of recent works (Chen\net al., 2021; CodeGeeX, 2022; Austin et al., 2021; Athiwaratkun et al., 2023; Cassano et al., 2023;\nHendrycks et al., 2021; Raychev et al., 2016; Lu et al., 2021; Allamanis and Sutton, 2013; Puri et al.,\n2021; Husain et al., 2019; Clement et al., 2021; Ding et al., 2023; Wang et al., 2023; Lu et al., 2022)\ndeveloped benchmarks to facilitate the evaluation of code LMs. These benchmarks typically assess\ncode completion ability given in-file context \u2013 code prompts containing code snippets from current\nfiles (where the user is writing code). Therefore, the capability of these code LMs to generate code\nthat requires software repository-level context has been left unexplored until recently.\nA few recent works proposed repository-level code generation frameworks and benchmarks (Shri-\nvastava et al., 2023; Ding et al., 2022; Pei et al., 2023; Zhang et al., 2023). While these works\nshare high-level insights with CROSSCODEEVAL, highlighting the importance of cross-file context,\ntheir focus is mainly on proposing a new approach to incorporate such contexts, and datasets are\ncollected to evaluate their own approaches rather than being carefully crafted as a benchmark to\nevaluate the code LMs in general. For example, Shrivastava et al. (2023) and Ding et al. (2022)\nonly collect data for one single programming language, and Pei et al. (2023) narrows the completion\nscope to only function arguments. As a comparison, CROSSCODEEVAL comprehensively includes\nfour different programming languages (Python, Java, Typescript, and C#) and targets evaluating the\ngeneral code completion capacity of code LMs rather than a specific type of application. REPOEVAL\n(Zhang et al., 2023) is a concurrent work building repository-level code completion benchmark\nin Python, constructed from 16 GitHub repositories. These repositories are limited in a domain\n(mainly academia/research work), some of them overlap with popular code pre-training datasets\n(such as The Stack (Kocetkov et al., 2022)), and some are with non-permissive licenses. In contrast,\nCROSSCODEEVAL is derived from a diverse pool of permissively licensed GitHub repositories in 4\npopular languages (\u00a72.4). Furthermore, CROSSCODEEVAL does not overlap with The Stack to avoid\ndata leakage, minimizing potential memorization issues during evaluations.\n5\nConclusion\nWe introduce CROSSCODEEVAL, a diverse and multilingual benchmark for cross-file code completion.\nCROSSCODEEVAL necessitates cross-file contextual understanding to complete the code accurately.\nWe use a static-analysis-based method to identify cross-file context usages in code, and take steps\nto ensure the dataset is of high quality and has minimal data leakage with the pre-training dataset\nof popular code LMs. We experiment with popular code language models and results show that the\ninclusion of cross-file context significantly improves their accuracy in code completion, demonstrating\nthat CROSSCODEEVAL is an effective benchmark assessing cross-file code completion capabilities.\nMoreover, even the top-performing model with the best retrieval method still exhibits great room\nfor improvement, highlighting the need for further advancements in leveraging extensive context for\ncode completion and better code retriever. In both directions, CROSSCODEEVAL stands as a pivotal\nbenchmark. We envision CROSSCODEEVAL could fill in the gap of evaluating code completion that\nrequires cross-file context and promote future research in all dimensions in this direction.\n10\nAcknowledgments\nWe would like to thank Ramana Keerthi and Akhilesh Bontala for their help in data collection, and\nBryan McWhorter for various legal consultations.\nReferences\nAhmad, W., Chakraborty, S., Ray, B., Chang, K.W., 2021. Unified pre-training for program under-\nstanding and generation, in: Proceedings of the 2021 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies, Association for\nComputational Linguistics, Online. pp. 2655\u20132668. URL: https://aclanthology.org/2021.\nnaacl-main.211, doi:10.18653/v1/2021.naacl-main.211.\nAllal, L.B., Li, R., Kocetkov, D., Mou, C., Akiki, C., Ferrandis, C.M., Muennighoff, N., Mishra,\nM., Gu, A., Dey, M., et al., 2023.\nSantacoder: don\u2019t reach for the stars!\narXiv preprint\narXiv:2301.03988 URL: https://arxiv.org/abs/2301.03988.\nAllamanis, M., Sutton, C., 2013. Mining source code repositories at massive scale using language\nmodeling, in: Proceedings of the 10th Working Conference on Mining Software Repositories,\nIEEE Press. p. 207\u2013216. URL: https://dl.acm.org/doi/pdf/10.5555/2487085.2487127.\nAthiwaratkun, B., Gouda, S.K., Wang, Z., Li, X., Tian, Y., Tan, M., Ahmad, W.U., Wang, S.,\nSun, Q., Shang, M., Gonugondla, S.K., Ding, H., Kumar, V., Fulton, N., Farahani, A., Jain,\nS., Giaquinto, R., Qian, H., Ramanathan, M.K., Nallapati, R., Ray, B., Bhatia, P., Sengupta,\nS., Roth, D., Xiang, B., 2023. Multi-lingual evaluation of code generation models, in: The\nEleventh International Conference on Learning Representations. URL: https://openreview.\nnet/forum?id=Bo7eeXm6An8.\nAustin, J., Odena, A., Nye, M., Bosma, M., Michalewski, H., Dohan, D., Jiang, E., Cai, C., Terry, M.,\nLe, Q., et al., 2021. Program synthesis with large language models. ArXiv preprint abs/2108.07732.\nURL: https://arxiv.org/abs/2108.07732.\nBarr, E.T., Brun, Y., Devanbu, P., Harman, M., Sarro, F., 2014. The plastic surgery hypothesis, in:\nProceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software\nEngineering, Association for Computing Machinery, New York, NY, USA. p. 306\u2013317. URL:\nhttps://doi.org/10.1145/2635868.2635898, doi:10.1145/2635868.2635898.\nBavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey, C., Tworek, J., Chen, M., 2022. Efficient\ntraining of language models to fill in the middle. arXiv preprint arXiv:2207.14255 URL: https:\n//arxiv.org/abs/2207.14255.\nBlack, S., Biderman, S., Hallahan, E., Anthony, Q., Gao, L., Golding, L., He, H., Leahy, C., McDonell,\nK., Phang, J., Pieler, M., Prashanth, U.S., Purohit, S., Reynolds, L., Tow, J., Wang, B., Weinbach,\nS., 2022. GPT-NeoX-20B: An open-source autoregressive language model, in: Proceedings of\nBigScience Episode #5 \u2013 Workshop on Challenges & Perspectives in Creating Large Language\nModels, Association for Computational Linguistics, virtual+Dublin. pp. 95\u2013136. URL: https:\n//aclanthology.org/2022.bigscience-1.9, doi:10.18653/v1/2022.bigscience-1.9.\nBlack, S., Gao, L., Wang, P., Leahy, C., Biderman, S., 2021. GPT-Neo: Large Scale Autoregres-\nsive Language Modeling with Mesh-Tensorflow URL: https://doi.org/10.5281/zenodo.\n5297715, doi:10.5281/zenodo.5297715.\nCassano, F., Gouwar, J., Nguyen, D., Nguyen, S., Phipps-Costin, L., Pinckney, D., Yee, M.H., Zi, Y.,\nAnderson, C.J., Feldman, M.Q., Guha, A., Greenberg, M., Jangda, A., 2023. Multipl-e: A scalable\nand polyglot approach to benchmarking neural code generation. IEEE Transactions on Software\nEngineering 49, 3675\u20133691. doi:10.1109/TSE.2023.3267446.\nChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H.P.d.O., Kaplan, J., Edwards, H., Burda, Y., Joseph,\nN., Brockman, G., et al., 2021. Evaluating large language models trained on code. ArXiv preprint\nabs/2107.03374. URL: https://arxiv.org/abs/2107.03374.\n11\nClement, C., Lu, S., Liu, X., Tufano, M., Drain, D., Duan, N., Sundaresan, N., Svyatkovskiy,\nA., 2021. Long-range modeling of source code files with eWASH: Extended window access\nby syntax hierarchy, in: Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing, Association for Computational Linguistics, Online and Punta Cana, Domini-\ncan Republic. pp. 4713\u20134722. URL: https://aclanthology.org/2021.emnlp-main.387,\ndoi:10.18653/v1/2021.emnlp-main.387.\nCodeGeeX, 2022. https://github.com/THUDM/CodeGeeX.\nDing, H., Kumar, V., Tian, Y., Wang, Z., Kwiatkowski, R., Li, X., Ramanathan, M.K., Ray, B., Bhatia,\nP., Sengupta, S., et al., 2023. A static evaluation of code completion by large language models.\narXiv preprint arXiv:2306.03203 .\nDing, Y., Wang, Z., Ahmad, W.U., Ramanathan, M.K., Nallapati, R., Bhatia, P., Roth, D., Xiang, B.,\n2022. Cocomic: Code completion by jointly modeling in-file and cross-file context. arXiv preprint\narXiv:2212.10007 URL: https://arxiv.org/abs/2212.10007.\nFeng, Z., Guo, D., Tang, D., Duan, N., Feng, X., Gong, M., Shou, L., Qin, B., Liu, T., Jiang,\nD., Zhou, M., 2020. CodeBERT: A pre-trained model for programming and natural languages,\nin: Findings of the Association for Computational Linguistics: EMNLP 2020, Association for\nComputational Linguistics, Online. pp. 1536\u20131547. URL: https://aclanthology.org/2020.\nfindings-emnlp.139, doi:10.18653/v1/2020.findings-emnlp.139.\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, S., Zettlemoyer,\nL., Lewis, M., 2023. Incoder: A generative model for code infilling and synthesis, in: The\nEleventh International Conference on Learning Representations. URL: https://openreview.\nnet/forum?id=hQwb-lbM6EL.\nGuo, D., Lu, S., Duan, N., Wang, Y., Zhou, M., Yin, J., 2022. Unixcoder: Unified cross-modal\npre-training for code representation, in: Proceedings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers), pp. 7212\u20137225.\nHendrycks, D., Basart, S., Kadavath, S., Mazeika, M., Arora, A., Guo, E., Burns, C., Puranik, S.,\nHe, H., Song, D., Steinhardt, J., 2021. Measuring coding challenge competence with APPS, in:\nThirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack (Round 2). URL: https://openreview.net/forum?id=sD93GOzH3i5.\nHoltzman, A., Buys, J., Du, L., Forbes, M., Choi, Y., 2020.\nThe curious case of neural\ntext degeneration, in: International Conference on Learning Representations. URL: https:\n//openreview.net/forum?id=rygGQyrFvH.\nHossain, N., Ghazvininejad, M., Zettlemoyer, L., 2020. Simple and effective retrieve-edit-rerank\ntext generation, in: Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics, Association for Computational Linguistics, Online. pp. 2532\u20132538. URL: https:\n//aclanthology.org/2020.acl-main.228, doi:10.18653/v1/2020.acl-main.228.\nHusain, H., Wu, H.H., Gazit, T., Allamanis, M., Brockschmidt, M., 2019. Codesearchnet challenge:\nEvaluating the state of semantic code search. arXiv preprint arXiv:1909.09436 URL: https:\n//arxiv.org/abs/1909.09436.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T.B., Chess, B., Child, R., Gray, S., Radford, A., Wu,\nJ., Amodei, D., 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 .\nKocetkov, D., Li, R., Allal, L.B., Li, J., Mou, C., Ferrandis, C.M., Jernite, Y., Mitchell, M., Hughes,\nS., Wolf, T., et al., 2022. The stack: 3 tb of permissively licensed source code. arXiv preprint\narXiv:2211.15533 URL: https://arxiv.org/abs/2211.15533.\nLe Goues, C., Nguyen, T., Forrest, S., Weimer, W., 2012. Genprog: A generic method for automatic\nsoftware repair. IEEE Trans. Softw. Eng. 38, 54\u201372. URL: https://doi.org/10.1109/TSE.\n2011.104, doi:10.1109/TSE.2011.104.\n12\nLee, K., Ippolito, D., Nystrom, A., Zhang, C., Eck, D., Callison-Burch, C., Carlini, N., 2022.\nDeduplicating training data makes language models better, in: Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), Association\nfor Computational Linguistics, Dublin, Ireland. pp. 8424\u20138445. URL: https://aclanthology.\norg/2022.acl-long.577, doi:10.18653/v1/2022.acl-long.577.\nLi, R., Allal, L.B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J.,\nChim, J., et al., 2023. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161\nURL: https://arxiv.org/abs/2305.06161.\nLi, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J., Leblond, R., Eccles, T., Keeling, J.,\nGimeno, F., Lago, A.D., et al., 2022. Competition-level code generation with alphacode. ArXiv\npreprint abs/2203.07814. URL: https://arxiv.org/abs/2203.07814.\nLu, S., Duan, N., Han, H., Guo, D., Hwang, S.w., Svyatkovskiy, A., 2022. ReACC: A retrieval-\naugmented code completion framework, in: Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), Association for Computa-\ntional Linguistics, Dublin, Ireland. pp. 6227\u20136240. URL: https://aclanthology.org/2022.\nacl-long.431, doi:10.18653/v1/2022.acl-long.431.\nLu, S., Guo, D., Ren, S., Huang, J., Svyatkovskiy, A., Blanco, A., Clement, C., Drain, D., Jiang,\nD., Tang, D., Li, G., Zhou, L., Shou, L., Zhou, L., Tufano, M., GONG, M., Zhou, M., Duan,\nN., Sundaresan, N., Deng, S.K., Fu, S., LIU, S., 2021. CodeXGLUE: A machine learning\nbenchmark dataset for code understanding and generation, in: Thirty-fifth Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Round 1). URL: https:\n//openreview.net/forum?id=6lE4dQXaUcb.\nNijkamp, E., Hayashi, H., Xiong, C., Savarese, S., Zhou, Y., 2023a. Codegen2: Lessons for\ntraining llms on programming and natural languages. arXiv preprint arXiv:2305.02309 URL:\nhttps://arxiv.org/abs/2305.02309.\nNijkamp, E., Pang, B., Hayashi, H., Tu, L., Wang, H., Zhou, Y., Savarese, S., Xiong, C., 2023b.\nCodegen: An open large language model for code with multi-turn program synthesis, in: Interna-\ntional Conference on Learning Representations. URL: https://openreview.net/forum?id=\niaYcJKpY2B_.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal,\nS., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A.,\nWelinder, P., Christiano, P.F., Leike, J., Lowe, R., 2022. Training language models to follow\ninstructions with human feedback, in: Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho,\nK., Oh, A. (Eds.), Advances in Neural Information Processing Systems, Curran Associates, Inc..\npp. 27730\u201327744. URL: https://proceedings.neurips.cc/paper_files/paper/2022/\nfile/b1efde53be364a73914f58805a001731-Paper-Conference.pdf.\nPei, H., Zhao, J., Lausen, L., Zha, S., Karypis, G., 2023. Better context makes better code language\nmodels: A case study on function call argument completion, in: Proceedings of the Thirty-\nSeventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative\nApplications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in\nArtificial Intelligence, AAAI Press. URL: https://doi.org/10.1609/aaai.v37i4.25653,\ndoi:10.1609/aaai.v37i4.25653.\nPuri, R., Kung, D.S., Janssen, G., Zhang, W., Domeniconi, G., Zolotov, V., Dolby, J., Chen, J.,\nChoudhury, M., Decker, L., Thost, V., Buratti, L., Pujar, S., Ramji, S., Finkler, U., Malaika, S.,\nReiss, F., 2021. Codenet: A large-scale AI for code dataset for learning a diversity of coding tasks,\nin: Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks\nTrack (Round 2). URL: https://openreview.net/forum?id=6vZVBkCDrHT.\nRaychev, V., Bielik, P., Vechev, M., 2016.\nProbabilistic model for code with decision trees,\nin: Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented\nProgramming, Systems, Languages, and Applications, Association for Computing Machinery,\nNew York, NY, USA. p. 731\u2013747.\nURL: https://doi.org/10.1145/2983990.2984041,\ndoi:10.1145/2983990.2984041.\n13\nRobertson, S., Zaragoza, H., et al., 2009. The probabilistic relevance framework: Bm25 and beyond.\nFoundations and Trends\u00ae in Information Retrieval 3, 333\u2013389.\nShrivastava, D., Larochelle, H., Tarlow, D., 2023. Repository-level prompt generation for large\nlanguage models of code, in: Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., Scarlett,\nJ. (Eds.), Proceedings of the 40th International Conference on Machine Learning, PMLR. pp.\n31693\u201331715. URL: https://proceedings.mlr.press/v202/shrivastava23a.html.\nWang, B., Komatsuzaki, A., 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model.\nhttps://github.com/kingoflolz/mesh-transformer-jax.\nWang, S., Li, Z., Qian, H., Yang, C., Wang, Z., Shang, M., Kumar, V., Tan, S., Ray, B., Bhatia, P.,\nNallapati, R., Ramanathan, M.K., Roth, D., Xiang, B., 2023. ReCode: Robustness evaluation\nof code generation models, in: Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), Association for Computational Linguistics,\nToronto, Canada. pp. 13818\u201313843. URL: https://aclanthology.org/2023.acl-long.\n773, doi:10.18653/v1/2023.acl-long.773.\nWang, Y., Wang, W., Joty, S., Hoi, S.C., 2021. CodeT5: Identifier-aware unified pre-trained encoder-\ndecoder models for code understanding and generation, in: Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, Association for Computational Linguistics,\nOnline and Punta Cana, Dominican Republic. pp. 8696\u20138708. URL: https://aclanthology.\norg/2021.emnlp-main.685, doi:10.18653/v1/2021.emnlp-main.685.\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf,\nR., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu,\nC., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., Rush, A., 2020. Transformers: State-\nof-the-art natural language processing, in: Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System Demonstrations, Association for Computational\nLinguistics, Online. pp. 38\u201345. URL: https://aclanthology.org/2020.emnlp-demos.6,\ndoi:10.18653/v1/2020.emnlp-demos.6.\nXu, F.F., Alon, U., Neubig, G., Hellendoorn, V.J., 2022. A systematic evaluation of large language\nmodels of code, in: Proceedings of the 6th ACM SIGPLAN International Symposium on Machine\nProgramming, Association for Computing Machinery, New York, NY, USA. p. 1\u201310. URL:\nhttps://doi.org/10.1145/3520312.3534862, doi:10.1145/3520312.3534862.\nZhang, F., Chen, B., Zhang, Y., Liu, J., Zan, D., Mao, Y., Lou, J.G., Chen, W., 2023. Repocoder:\nRepository-level code completion through iterative retrieval and generation.\narXiv preprint\narXiv:2303.12570 URL: https://arxiv.org/abs/2303.12570.\n14\nCROSSCODEEVAL: A Diverse and Multilingual Benchmark for\nCross-File Code Completion\nAppendices\nA\nMore Details of CROSSCODEEVAL Generation\nA.1\nPython\nWe presented a Python example in Figure 2, Section 2.2 in the main paper. Here we describe the static\nanalysis part in detail. Recall that the purpose of static analysis is to detect undefined members in the\nmodified file whose imports are replaced by empty classes. Hence, there\u2019s no need to include project-\nlevel information at this stage, and we only run Pylint on the standalone file. We restrict the output\ntypes to be error only and ignore other types such as warnings and coding conventions. In the example\nin Figure 2, Section 2.2, the Pylint output for the modified file is \u201ctest_utils.py:10:8:\nE1101:\nClass \u2018CaseConverter\u2019 has no \u2018camel_to_snake\u2019 member (no-member)\u201d.\nWe collect\nall the errors of type E1101 and use them to identify locations of cross-file context usage.\nA.2\nJava\nInstead of using static analysis tool, we directly leverage Java default compiler to extract the undefined\nmember functions in the modified file, whose belonging class, originally imported from another\nproject file, is replaced by an empty class. Figure 6 shows the work flow of the cross-file method\nextraction. Specifically, we can parse the new errors with the \u2227 mark in the modified code to locate\nthe calls of the cross-file function. An illustration is provided in Figure 7.\nA.3\nTypeScript\nWe go through the following steps to create examples from TypeScript repositories.\n1. Extract import statements from source files (files with \u201c.ts\u201d or \u201c.tsx\u201d extension) and identify\nthe dependencies. For example, a file fileA.ts having the import statement import\nmodule_name from \u2018../fileB.ts\u2019; depends on ../fileB.ts. (../fileB.ts is a\ncross-file for fileA.ts)\n2. Identify the cross-file lines by modifying the source code, followed by compiling the project\nusing tsc.11 We make the following two types of modifications.\n\u2022 Removing the import statement and checking the compilation output message for\n\u201cerror TS2304:\nCannot find name \u2018X\u2019.\u201d.\n\u2022 Removing the import statement and adding a dummy class \u201cclass module_name {}\u201d\nand checking the compilation output message for \u201cerror TS2339:\nProperty \u2018X\u2019\ndoes not exist on type \u2018Y\u2019.\u201d.\n3. From the compiler error message, we use the line numbers to form references for examples.\nA.4\nC#\nFor C# repositories, the example generation procedure follows two steps. In the first step, we pick\na source code file from a C# repository and replace the original class with a dummy one. Then,\nwe run the C# mono compiler on all C# files and gather the compilation errors. In the next step,\nwe compare the compiler error messages before and after the substitution in step-1 to locate the\ncross-file references based on the incremental errors, and extract the line numbers for references in\nCROSSCODEEVAL examples. We repeat the process for each source code file within a repository.\n11https://www.typescriptlang.org/docs/handbook/compiler-options.html\n15\nFigure 6: Java data creation: 1) given a original code file, we create an adapted version by replacing\nan imported class found in the project directory and generating a dummy class accordingly. 2) Run\njavac on both versions. New javac error points of the modified code are actually the method calls\nof the original code, where the definitions are in another file in the project. A pair of prompt and\nground truth completion is generated accordingly.\nFigure 7: By comparing javac errors of original Java code and modified code, we find the cross-file\nentities and create the prompt and ground truth.\nB\nHuman Annotation for Quality Control\nTo assess the quality and further improve CROSSCODEEVAL, we conducted human annotation. We\nrandomly sampled 100 and 50 examples from CROSSCODEEVAL Python and Java sets, respectively.\nSix authors annotated 50 example each, and each example was annotated by two annotators with\nsignificant experience in the targeted language. Three questions were asked in the annotation:\n1. Does the reference contain any name that requires looking at the associated cross-file? [Pay\nattention to: while answering, think if looking at the associated cross-file helps you knowing\n16\nmore info about a name mentioned in the reference.]\nChoose from A: Yes. / B: No, I already know everything about all the names mentioned in\nthe reference. / C: Reference does not contain any such name.\n2. Can you predict the reference given only the current-file context? [Pay attention to: while\nanswering, think if you can guess the reference given the current-file context.]\nChoose from A: No, I cannot predict. / B: I possibly can predict but it may not exactly match\nthe reference. / C: I can predict the reference.\n3. Will you prefer to remove the example from the dataset? [Pay attention to: Is the example\ngood enough to be included in the dataset? Use your best judgment to make a decision.]\nChoose from A: No. / B: Maybe. / C: Yes.\nQ1\nQ2\nQ3\nPython\n98%\n89%\n88%\nJava\n100%\n84%\n76%\nTable 5: Agreement scores for the annotation questions in Python and Java, respectively.\nQ1\nQ2\nQ3\nA\nB\nC\nA\nB\nC\nA\nB\nC\nPython\n99%\n1%\n0%\n88.5%\n9.5%\n2%\n91.5%\n6%\n2.5%\nJava\n100%\n0%\n0%\n88%\n10%\n2%\n80%\n13%\n7%\nTable 6: Distribution of annotation answers to each question in Python and Java.\nWe calculated the agreement score12 and summarize the distribution of the annotations in Tables 5\n& 6. Overall, we see great agreement scores in most of the questions in both languages, suggesting\nannotators have consensus in these questions. Looking at the distribution of annotation answers,\nwe see that in almost 100% cases, the references contain names that necessities cross-file informa-\ntion (Q1), and only at 2% that the reference can be predicted with only current-file context (Q2).\nBoth together suggest that CROSSCODEEVAL serves its purpose to be a dedicated cross-file code\ncompletion benchmark that accurately reflects model\u2019s capability in cross-file context understanding.\nBesides, we see 2.5% and 7% examples that annotators think should be removed from the dataset\n(Q3). A closer look reveals that many of such examples contain long strings in the reference, e.g.,\nChat.sendClientSystemMessage(\"Available scripts:\");, which cannot be predicted eas-\nily and also led to disagreement between annotators. We plan to improve the dataset by filtering out\nexamples with long string in the reference in the next data revision.\nC\nRetrieve-and-Generate Modeling Details\nWe present Figure 8 to illustrate more details regarding the Retrieve-and-Generate (RG) framework.\nCross-file Context Retrieval\nAs we have introduced in Section 3.3, the query will be constructed\nusing the in-file context, i.e., the last 10 code lines before the cursor position, and the goal is to find\nthe relevant cross-file context \u2013 a similar operation of the deposit that has been implemented in the\nclass Bank. The code snippets in other files are chunked as code snippets with 10 lines as candidates\nfor similarity calculation, and the most similar candidate of the in-file context turns out to be lines\n5-14 in \u201cRelevant Cross-file Context\" of Figure 8. Since the in-file context is left context and the\ntarget completion is the following code, correspondingly, the retrieved context will be the following\nlines of the similar candidate, i.e., line 15-23 in \u201cRelevant Cross-file Context\". We can see that line\n15 gives a direct hint that helps with the prediction.\nCross-file Context Retrieved w/ Reference\nAs an upper bound estimate, the query for the oracle\ncontext will be constructed by the last 10 lines of concatenating the in-file context and the target\ncompletion. Consequently, the most similar candidate is lines 8-17 in \u201cRelevant Cross-file Context\",\n12Given we only have limited examples and annotations per example, and the annotation is relatively sparse,\nwe didn\u2019t include other inter-coder agreement metrics like Krippendorff\u2019s \u03b1 or Cohen\u2019s \u03ba.\n17\nFigure 8: An illustration demonstrating the context retrieval.\nwhich, different from the cross-file context, will be directly used as the oracle context, since the query\nalready contains the target completion rather than just left context. Again, it includes the direct hint,\nline 15, for the code completion.\nD\nAdditional Evaluation Results and Ablations\nD.1\nEvaluation with Additional Models\nBeyond the results in Table 2 of the main paper, we further evaluate more code LMs, including more\nvariants of CodeGen and StarcoderBase. The results are shown in Table 7.\n18\nModel\nCode Match\nPython\nJava\nTypeScript\nC#\nEM\nES\nEM\nES\nEM\nES\nEM\nES\nCodeGen-350M\n2.70\n52.94\n3.51\n54.92\n2.77\n46.60\n0.90\n42.64\n+ Retrieval\n6.98\n56.91\n6.97\n56.87\n4.02\n48.07\n3.28\n45.35\n+ Retrieval w/ Ref.\n9.64\n59.31\n8.04\n57.75\n5.21\n50.25\n6.50\n48.58\nCodeGen-2.7B\n4.92\n55.75\n5.89\n58.86\n3.84\n50.77\n1.36\n48.04\n+ Retrieval\n9.64\n60.02\n10.00\n61.20\n6.47\n52.61\n4.24\n52.19\n+ Retrieval w/ Ref.\n13.28\n62.65\n11.73\n62.50\n7.81\n54.79\n7.35\n54.78\nCodeGen-6.1B\n5.40\n55.41\n5.19\n58.89\n3.75\n49.87\n1.47\n45.34\n+ Retrieval\n10.58\n60.36\n8.88\n61.06\n6.14\n51.58\n4.64\n47.53\n+ Retrieval w/ Ref.\n14.07\n62.80\n11.03\n61.76\n8.22\n54.01\n7.13\n48.98\nCodeGen-16.1B\n6.87\n57.64\n7.01\n60.49\n4.50\n52.24\n1.81\n45.28\n+ Retrieval\n12.46\n62.66\n11.69\n62.16\n7.06\n54.11\n5.54\n48.25\n+ Retrieval w/ Ref.\n16.66\n65.31\n13.65\n63.50\n9.18\n56.54\n8.54\n49.63\nStarCoderBase-1B\n0.19\n54.78\n0.19\n57.17\n0.06\n42.00\n0.11\n57.65\n+ Retrieval\n8.18\n60.06\n7.48\n59.71\n3.64\n40.07\n8.94\n62.32\n+ Retrieval w/ Ref.\n12.83\n63.37\n10.38\n62.06\n6.79\n42.47\n15.16\n65.56\nStarCoderBase-3B\n4.77\n57.54\n5.66\n60.26\n3.55\n46.74\n3.62\n59.27\n+ Retrieval\n11.74\n62.99\n12.39\n62.33\n6.73\n43.89\n11.99\n64.65\n+ Retrieval w/ Ref.\n16.66\n65.91\n14.91\n64.83\n9.62\n46.39\n18.04\n66.86\nStarCoderBase-7B\n6.75\n59.83\n8.74\n62.84\n5.13\n49.31\n4.86\n59.71\n+ Retrieval\n13.28\n64.76\n15.61\n65.19\n8.25\n45.73\n14.20\n65.65\n+ Retrieval w/ Ref.\n18.95\n67.41\n18.70\n67.76\n11.95\n47.91\n20.64\n67.95\nModel\nIdentifier Match\nPython\nJava\nTypeScript\nC#\nEM\nF1\nEM\nF1\nEM\nF1\nEM\nF1\nCodeGen-350M\n7.92\n38.51\n9.40\n43.36\n5.07\n34.44\n1.98\n21.07\n+ Retrieval\n13.70\n44.34\n13.32\n46.47\n7.06\n37.09\n5.83\n26.67\n+ Retrieval w/ Ref.\n17.49\n48.14\n14.68\n47.90\n8.85\n39.75\n8.88\n31.27\nCodeGen-2.7B\n11.14\n42.42\n12.62\n47.94\n7.51\n39.71\n3.96\n24.92\n+ Retrieval\n17.22\n48.96\n18.19\n51.43\n10.91\n42.96\n8.03\n31.85\n+ Retrieval w/ Ref.\n21.54\n52.40\n20.06\n53.19\n12.96\n45.58\n11.43\n36.50\nCodeGen-6.1B\n10.92\n42.22\n12.48\n48.38\n6.76\n39.13\n3.90\n23.20\n+ Retrieval\n17.90\n49.41\n17.25\n51.79\n10.31\n42.29\n8.26\n30.01\n+ Retrieval w/ Ref.\n22.40\n53.14\n19.45\n53.12\n12.87\n45.72\n12.10\n34.20\nCodeGen-16.1B\n13.28\n44.98\n14.40\n49.97\n8.76\n41.95\n4.13\n23.49\n+ Retrieval\n20.53\n51.95\n19.50\n52.97\n12.13\n44.65\n9.33\n31.08\n+ Retrieval w/ Ref.\n25.37\n55.76\n22.21\n54.98\n15.14\n48.02\n12.67\n35.08\nStarCoderBase-1B\n6.79\n39.70\n7.71\n45.18\n4.71\n33.58\n4.02\n30.32\n+ Retrieval\n15.91\n47.87\n15.57\n49.64\n8.13\n33.45\n13.01\n39.38\n+ Retrieval w/ Ref.\n21.61\n52.95\n19.54\n52.57\n11.80\n36.55\n18.83\n44.81\nStarCoderBase-3B\n11.48\n43.63\n14.12\n49.13\n8.76\n39.18\n7.30\n32.55\n+ Retrieval\n19.81\n51.66\n21.18\n53.03\n11.83\n37.95\n16.06\n42.45\n+ Retrieval w/ Ref.\n25.40\n55.89\n24.36\n56.09\n15.08\n41.09\n21.95\n47.03\nStarCoderBase-7B\n13.92\n46.65\n16.88\n52.18\n10.04\n42.15\n8.71\n34.04\n+ Retrieval\n22.29\n54.14\n24.92\n56.70\n13.92\n40.25\n18.16\n44.55\n+ Retrieval w/ Ref.\n28.26\n58.14\n29.64\n59.99\n17.79\n43.40\n24.04\n49.12\nTable 7: Evaluation of additional code LMs on CROSSCODEEVAL (cf. Table 2)\n19\nD.2\nNucleus Sampling w/ Re-ranking\nThough the main results in this benchmark are reported with greedy search, we further conduct\nexperiments to explore the effects of sampling and reranking. To this end, we apply the nucleus\nsampling (Holtzman et al., 2020) then mean-log-likelihood reranking (Hossain et al., 2020) during\nthe code generation. The experiments are conducted with temperature of 0.2 for the token probability\nscaling. For the nucleus sampling, we set the top-p to be 0.95, and for the mean-log-likelihood\nreranking, we generate top-5 prediction under the sampling setting and compute the mean log-\nlikelihood of each generation to pick the most probable prediction.\nThe results are shown in Table 8. Compared to the main results (Table 2 in the main paper), we notice\nthe results are quite comparable, and the difference is marginal. The cross-file and oracle context\nbring equivalent improvement to the greedy search setting. The results empirically reveal that (1) the\nperformance difference of sampling with re-ranking and greedy decoding is marginal, and (2) the\ncross-file context is helpful regardless of the sampling/search algorithms.\nModel\nCode Match\nPython\nJava\nTypeScript\nC#\nEM\nES\nEM\nES\nEM\nES\nEM\nES\nCodeGen25-7B\n8.14\n59.72\n10.47\n62.54\n7.90\n57.69\n3.90\n59.73\n+ Retrieval\n14.60\n64.56\n17.63\n64.49\n13.32\n60.35\n13.29\n64.55\n+ Retrieval w/ Ref.\n19.51\n67.67\n20.48\n66.92\n15.79\n62.88\n18.21\n66.46\nStarCoder-15.5B\n8.93\n61.43\n10.66\n63.97\n6.05\n51.95\n4.64\n60.52\n+ Retrieval\n15.68\n66.70\n17.58\n66.73\n8.76\n45.78\n14.03\n65.70\n+ Retrieval w/ Ref.\n21.35\n69.44\n20.20\n68.49\n11.65\n47.32\n19.97\n68.32\nModel\nIdentifier Match\nPython\nJava\nTypeScript\nC#\nEM\nF1\nEM\nF1\nEM\nF1\nEM\nF1\nCodeGen25-7B\n14.60\n46.13\n17.16\n52.04\n12.40\n47.73\n7.47\n34.52\n+ Retrieval\n22.81\n53.72\n25.15\n55.99\n18.18\n51.72\n17.36\n43.88\n+ Retrieval w/ Ref.\n28.78\n58.20\n28.94\n58.98\n21.99\n55.53\n22.12\n47.78\nStarCoder-15.5B\n15.91\n48.08\n19.07\n54.08\n11.50\n44.13\n8.31\n34.58\n+ Retrieval\n24.62\n55.75\n26.46\n58.46\n14.78\n40.59\n18.33\n44.55\n+ Retrieval w/ Ref.\n30.77\n60.01\n30.53\n61.07\n18.15\n42.90\n24.15\n49.26\nTable 8: Performance of code LMs on CROSSCODEEVAL with temperature-based nucleus sampling.\n(cf. Table 2 in the main body of the paper).\nD.3\nAdditional Results of Code Retrieval\nTable 9 presents identifier match results of various retrieval methods for cross-file context.\nD.4\nQualitative Analysis\nTo illustrate the quality of the retrieved cross-file context and in which ways they are helping to\nmaximize the code LMs\u2019 capacity, we provide two qualitative examples in Figure 9 and 10.\nIn Figure 9, we can see that the cross-file context provides retrieves the code snippets with a similar\ncontext to the cursor position. By capturing the repetitiveness of the repository (Le Goues et al.,\n2012; Barr et al., 2014), the cross-file context helps the code LMs adapt the existing, repetitive coding\npatterns to complete the programs. Specifically, the retrieved context also defines a function named\nstep(), with a similar goal of generating a token, and it provides a direct reference for completing\nthe API call.\n20\nModel\nRetriever\nIdentifier Match\nPython\nJava\nTypeScript\nC#\nEM\nF1\nEM\nF1\nEM\nF1\nEM\nF1\nCodeGen25-7B\n-\n14.26\n46.02\n16.60\n51.43\n12.46\n47.75\n7.69\n33.81\n+ Retrieval\nBM25\n22.96\n53.68\n24.03\n55.48\n17.85\n51.27\n17.36\n43.56\n+ Retrieval w/ Ref.\nBM25\n28.33\n57.95\n27.91\n57.87\n21.51\n55.38\n21.78\n47.63\n+ Retrieval\nUnixCoder\n22.74\n53.13\n22.67\n54.63\n17.43\n51.04\n16.63\n42.98\n+ Retrieval w/ Ref.\nUnixCoder\n26.90\n56.53\n25.57\n56.69\n20.74\n53.79\n20.36\n45.74\n+ Retrieval\nOpenAI Ada\n23.53\n54.17\n25.01\n55.67\n18.06\n51.67\n19.30\n46.08\n+ Retrieval w/ Ref.\nOpenAI Ada\n27.95\n56.59\n28.33\n58.39\n21.57\n55.07\n24.73\n51.51\nStarCoder-15.5B\n-\n15.72\n48.16\n18.28\n53.23\n11.86\n43.53\n8.54\n34.33\n+ Retrieval\nBM25\n24.77\n55.57\n25.95\n57.74\n14.09\n39.50\n18.04\n44.38\n+ Retrieval w/ Ref.\nBM25\n30.24\n59.46\n29.73\n60.47\n17.55\n42.18\n24.38\n49.09\n+ Retrieval\nUnixCoder\n20.41\n46.66\n19.26\n47.30\n15.55\n45.91\n15.61\n38.86\n+ Retrieval w/ Ref.\nUnixCoder\n25.25\n55.42\n25.90\n57.88\n13.32\n39.13\n16.23\n42.36\n+ Retrieval\nOpenAI Ada\n25.55\n56.23\n26.65\n57.68\n14.15\n39.99\n19.98\n46.45\n+ Retrieval w/ Ref.\nOpenAI Ada\n29.64\n58.64\n30.86\n60.73\n18.12\n42.77\n27.96\n53.53\nTable 9: Identifier Match evaluation results of various methods in retrieving cross-file context (cf.\nTable 4).\nDifferent from Figure 9, Figure 10 no longer retrieves the similar usage of predicting APIs, and\nrather, it retrieves the implementation details of the cross-file dependencies. Concretely, the cross-file\ncontext collects the member function, store_by_text, of class EntitySessionStorage, which\nis imported by the current completing file and instantiated as self.entity_repository. Without\nsuch cross-file context, the model hallucinates a wrong function usage that is not defined in class\nEntitySessionStorage. In contrast, when the cross-file is prepended to the prompt, the model\nsuccessfully predicts the ground truth, empirically revealing the limitation of existing datasets by\nonly feeding the current-file context, which will consequently underestimate code LMs\u2019 capacity.\nE\nLimitations\nZero-shot Evaluation\nOur benchmarking was done in a zero-shot fashion. We didn\u2019t perform\nthe few-shot study as the max sequence length of most benchmarked models is quite limited for\nprepending additional examples to the prompt. Thus, the performance will be limited as the format\nof cross-file context is never seen by the model during both training and prompting. We hope that\nCROSSCODEEVAL encourages future research to investigate methods for efficiently retrieving and\nincorporating cross-file context into the model.\nCross-file Context Retrieval Quality\nPrepending cross-file context to the prompt has shown\nsignificant improvement to the code LM\u2019s performance. However, as we have analyzed and identified\nin Section 3.5, the RG retrieval framework is not perfect. Due to its fixed length of context window\nand token-based similarity calculation, RG sometimes retrieves useless information and fails to help\ncode LMs for better generation. As for future work, we are expecting a more advanced retrieval\napproach to replace RG for more accurate cross-file contexts.\nMemorization\nCode LMs were trained on a vast amount of unlabeled code. There is no way we\ncould ensure that all models didn\u2019t see the evaluation data in the past. We take our best effort by\nexcluding popular packages from annotation (see Section 2.1). Despite that, we suggest researchers\nand practitioners be cautious in interpreting the results with potential memorization in mind. Future\nresearch in incorporating cross-file context may also consider deduplicating the training data with\nCROSSCODEEVAL, e.g., through methods in Lee et al. (2022).\n21\nFigure 9: Qualitative Example 1: the cross-file context provides a similar usage of the predicting API.\n22\nFigure 10: Qualitative Example 2: the cross-file context provides the definition of the predicting API.\n23\n"
  }
]