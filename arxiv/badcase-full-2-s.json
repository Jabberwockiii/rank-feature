[
  {
    "url": "https://arxiv.org/abs/2305.03286",
    "paper_id": "2305.03286",
    "abstract": "\n        We present a deep learning method for composite and task-driven motion control for physically simulated characters. In contrast to existing data-driven approaches using reinforcement learning that imitate full-body motions, we learn decoupled motions for specific body parts from multiple reference motions simultaneously and directly by leveraging the use of multiple discriminators in a GAN-like setup. In this process, there is no need of any manual work to produce composite reference motions for learning. Instead, the control policy explores by itself how the composite motions can be combined automatically. We further account for multiple task-specific rewards and train a single, multi-objective control policy. To this end, we propose a novel framework for multi-objective learning that adaptively balances the learning of disparate motions from multiple sources and multiple goal-directed control objectives. In addition, as composite motions are typically augmentations of simpler behaviors, we introduce a sample-efficient method for training composite control policies in an incremental manner, where we reuse a pre-trained policy as the meta policy and train a cooperative policy that adapts the meta one for new composite tasks. We show the applicability of our approach on a variety of challenging multi-objective tasks involving both composite motion imitation and multiple goal-directed control.\n        \u25b3 Less\n      ",
    "title": "Composite Motion Learning with Task Control",
    "date": "5 May, 2023",
    "authors": [
      "Pei Xu",
      " Xiumin Shang",
      " Victor Zordan",
      " Ioannis Karamouzas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02866",
    "paper_id": "2305.02866",
    "abstract": "\n        Graph Transformer is gaining increasing attention in the field of machine learning and has demonstrated state-of-the-art performance on benchmarks for graph representation learning. However, as current implementations of Graph Transformer primarily focus on learning representations of small-scale graphs, the quadratic complexity of the global self-attention mechanism presents a challenge for full-batch training when applied to larger graphs. Additionally, conventional sampling-based methods fail to capture necessary high-level contextual information, resulting in a significant loss of performance. In this paper, we introduce the Hierarchical Scalable Graph Transformer (HSGT) as a solution to these challenges. HSGT successfully scales the Transformer architecture to node representation learning tasks on large-scale graphs, while maintaining high performance. By utilizing graph hierarchies constructed through coarsening techniques, HSGT efficiently updates and stores multi-scale information in node embeddings at different levels. Together with sampling-based training methods, HSGT effectively captures and aggregates multi-level information on the hierarchical graph using only Transformer blocks. Empirical evaluations demonstrate that HSGT achieves state-of-the-art performance on large-scale benchmarks with graphs containing millions of nodes with high efficiency.\n        \u25b3 Less\n      ",
    "title": "Hierarchical Transformer for Scalable Graph Learning",
    "date": "5 May, 2023",
    "authors": [
      "Wenhao Zhu",
      " Tianyu Wen",
      " Guojie Song",
      " Xiaojun Ma",
      " Liang Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.13216",
    "paper_id": "2304.13216",
    "abstract": "\n        In this paper, we present a comprehensive study on semantic segmentation with the Pascal VOC dataset. Here, we have to label each pixel with a class which in turn segments the entire image based on the objects/entities present. To tackle this, we firstly use a Fully Convolution Network (FCN) baseline which gave 71.31% pixel accuracy and 0.0527 mean IoU. We analyze its performance and working and subsequently address the issues in the baseline with three improvements: a) cosine annealing learning rate scheduler(pixel accuracy: 72.86%, IoU: 0.0529), b) data augmentation(pixel accuracy: 69.88%, IoU: 0.0585) c) class imbalance weights(pixel accuracy: 68.98%, IoU: 0.0596). Apart from these changes in training pipeline, we also explore three different architectures: a) Our proposed model -- Advanced FCN (pixel accuracy: 67.20%, IoU: 0.0602) b) Transfer Learning with ResNet (Best performance) (pixel accuracy: 71.33%, IoU: 0.0926 ) c) U-Net(pixel accuracy: 72.15%, IoU: 0.0649). We observe that the improvements help in greatly improving the performance, as reflected both, in metrics and segmentation maps. Interestingly, we observe that among the improvements, dataset augmentation has the greatest contribution. Also, note that transfer learning model performs the best on the pascal dataset. We analyse the performance of these using loss, accuracy and IoU plots along with segmentation maps, which help us draw valuable insights about the working of the models.\n        \u25b3 Less\n      ",
    "title": "Exploiting CNNs for Semantic Segmentation with Pascal VOC",
    "date": "5 May, 2023",
    "authors": [
      "Sourabh Prakash",
      " Priyanshi Shah",
      " Ashrya Agrawal"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03287",
    "paper_id": "2305.03287",
    "abstract": "\n        Fine-tuning pre-trained language models (PLMs), e.g., SciBERT, generally requires large numbers of annotated data to achieve state-of-the-art performance on a range of NLP tasks in the scientific domain. However, obtaining the fine-tune data for scientific NLP task is still challenging and expensive. Inspired by recent advancement in prompt learning, in this paper, we propose the Mix Prompt Tuning (MPT), which is a semi-supervised method to alleviate the dependence on annotated data and improve the performance of multi-granularity academic function recognition tasks with a small number of labeled examples. Specifically, the proposed method provides multi-perspective representations by combining manual prompt templates with automatically learned continuous prompt templates to help the given academic function recognition task take full advantage of knowledge in PLMs. Based on these prompt templates and the fine-tuned PLM, a large number of pseudo labels are assigned to the unlabeled examples. Finally, we fine-tune the PLM using the pseudo training set. We evaluate our method on three academic function recognition tasks of different granularity including the citation function, the abstract sentence function, and the keyword function, with datasets from computer science domain and biomedical domain. Extensive experiments demonstrate the effectiveness of our method and statistically significant improvements against strong baselines. In particular, it achieves an average increase of 5% in Macro-F1 score compared with fine-tuning, and 6% in Macro-F1 score compared with other semi-supervised method under low-resource settings. In addition, MPT is a general method that can be easily applied to other low-resource scientific classification tasks.\n        \u25b3 Less\n      ",
    "title": "Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge",
    "date": "5 May, 2023",
    "authors": [
      "Jiawei Liu",
      " Zi Xiong",
      " Yi Jiang",
      " Yongqiang Ma",
      " Wei Lu",
      " Yong Huang",
      " Qikai Cheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03289",
    "paper_id": "2305.03289",
    "abstract": "\n        Recently, the Segment Anything Model (SAM) has gained significant attention as an image segmentation foundation model due to its strong performance on various downstream tasks. However, it has been found that SAM does not always perform satisfactorily when faced with challenging downstream tasks. This has led downstream users to demand a customized SAM model that can be adapted to these downstream tasks. In this paper, we present BadSAM, the first backdoor attack on the image segmentation foundation model. Our preliminary experiments on the CAMO dataset demonstrate the effectiveness of BadSAM.\n        \u25b3 Less\n      ",
    "title": "BadSAM: Exploring Security Vulnerabilities of SAM via Backdoor Attacks",
    "date": "5 May, 2023",
    "authors": [
      "Zihan Guan",
      " Mengxuan Hu",
      " Zhongliang Zhou",
      " Jielu Zhang",
      " Sheng Li",
      " Ninghao Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03299",
    "paper_id": "2305.03299",
    "abstract": "\n        Open Information Extraction (OIE) aims to extract relational tuples from open-domain sentences. Existing OIE systems split a sentence into tokens and recognize token spans as tuple relations and arguments. We instead propose Sentence as Chunk sequence (SaC) and recognize chunk spans as tuple relations and arguments. We argue that SaC has better quantitative and qualitative properties for OIE than sentence as token sequence, and evaluate four choices of chunks (i.e., CoNLL chunks, simple phrases, NP chunks, and spans from SpanOIE) against gold OIE tuples. Accordingly, we propose a simple BERT-based model for sentence chunking, and propose Chunk-OIE for tuple extraction on top of SaC. Chunk-OIE achieves state-of-the-art results on multiple OIE datasets, showing that SaC benefits OIE task.\n        \u25b3 Less\n      ",
    "title": "Open Information Extraction via Chunks",
    "date": "5 May, 2023",
    "authors": [
      "Kuicai Dong",
      " Aixin Sun",
      " Jung-Jae Kim",
      " Xiaoli Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.10024",
    "paper_id": "2211.10024",
    "abstract": "\n        This paper considers the problem of helping humans exercise scalable oversight over deep neural networks (DNNs). Adversarial examples can be useful by helping to reveal weaknesses in DNNs, but they can be difficult to interpret or draw actionable conclusions from. Some previous works have proposed using human-interpretable adversarial attacks including copy/paste attacks in which one natural image pasted into another causes an unexpected misclassification. We build on these with two contributions. First, we introduce Search for Natural Adversarial Features Using Embeddings (SNAFUE) which offers a fully automated method for finding copy/paste attacks. Second, we use SNAFUE to red team an ImageNet classifier. We reproduce copy/paste attacks from previous works and find hundreds of other easily-describable vulnerabilities, all without a human in the loop. Code is available at https://github.com/thestephencasper/snafue\n        \u25b3 Less\n      ",
    "title": "Diagnostics for Deep Neural Networks with Automated Copy/Paste Attacks",
    "date": "5 May, 2023",
    "authors": [
      "Stephen Casper",
      " Kaivalya Hariharan",
      " Dylan Hadfield-Menell"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03741",
    "paper_id": "2305.03741",
    "abstract": "\n        Attribute graphs are ubiquitous in multimedia applications, and graph representation learning (GRL) has been successful in analyzing attribute graph data. However, incomplete graph data and missing node attributes can have a negative impact on media knowledge discovery. Existing methods for handling attribute missing graph have limited assumptions or fail to capture complex attribute-graph dependencies. To address these challenges, we propose Attribute missing Graph Contrastive Learning (AmGCL), a framework for handling missing node attributes in attribute graph data. AmGCL leverages Dirichlet energy minimization-based feature precoding to encode in missing attributes and a self-supervised Graph Augmentation Contrastive Learning Structure (GACLS) to learn latent variables from the encoded-in data. Specifically, AmGCL utilizies feature reconstruction based on structure-attribute energy minimization while maximizes the lower bound of evidence for latent representation mutual information. Our experimental results on multiple real-world datasets demonstrate that AmGCL outperforms state-of-the-art methods in both feature imputation and node classification tasks, indicating the effectiveness of our proposed method in real-world attribute graph analysis tasks.\n        \u25b3 Less\n      ",
    "title": "AmGCL: Feature Imputation of Attribute Missing Graph via Self-supervised Contrastive Learning",
    "date": "5 May, 2023",
    "authors": [
      "Xiaochuan Zhang",
      " Mengran Li",
      " Ye Wang",
      " Haojun Fei"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03742",
    "paper_id": "2305.03742",
    "abstract": "\n        Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.\n        \u25b3 Less\n      ",
    "title": "Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming",
    "date": "5 May, 2023",
    "authors": [
      "Hanlin Zhang",
      " Jiani Huang",
      " Ziyang Li",
      " Mayur Naik",
      " Eric Xing"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.03439",
    "paper_id": "2304.03439",
    "abstract": "\n        Harnessing logical reasoning ability is a comprehensive natural language understanding endeavor. With the release of Generative Pretrained Transformer 4 (GPT-4), highlighted as \"advanced\" at reasoning tasks, we are eager to learn the GPT-4 performance on various logical reasoning tasks. This report analyses multiple logical reasoning datasets, with popular benchmarks like LogiQA and ReClor, and newly-released datasets like AR-LSAT. We test the multi-choice reading comprehension and natural language inference tasks with benchmarks requiring logical reasoning. We further construct a logical reasoning out-of-distribution dataset to investigate the robustness of ChatGPT and GPT-4. We also make a performance comparison between ChatGPT and GPT-4. Experiment results show that ChatGPT performs significantly better than the RoBERTa fine-tuning method on most logical reasoning benchmarks. With early access to the GPT-4 API we are able to conduct intense experiments on the GPT-4 model. The results show GPT-4 yields even higher performance on most logical reasoning datasets. Among benchmarks, ChatGPT and GPT-4 do relatively well on well-known datasets like LogiQA and ReClor. However, the performance drops significantly when handling newly released and out-of-distribution datasets. Logical reasoning remains challenging for ChatGPT and GPT-4, especially on out-of-distribution and natural language inference datasets. We release the prompt-style logical reasoning datasets as a benchmark suite and name it LogiEval.\n        \u25b3 Less\n      ",
    "title": "Evaluating the Logical Reasoning Ability of ChatGPT and GPT-4",
    "date": "5 May, 2023",
    "authors": [
      "Hanmeng Liu",
      " Ruoxi Ning",
      " Zhiyang Teng",
      " Jian Liu",
      " Qiji Zhou",
      " Yue Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03336",
    "paper_id": "2305.03336",
    "abstract": "\n        Misinformation spreading in mainstream and social media has been misleading users in different ways. Manual detection and verification efforts by journalists and fact-checkers can no longer cope with the great scale and quick spread of misleading information. This motivated research and industry efforts to develop systems for analyzing and verifying news spreading online. The SemEval-2023 Task 3 is an attempt to address several subtasks under this overarching problem, targeting writing techniques used in news articles to affect readers' opinions. The task addressed three subtasks with six languages, in addition to three ``surprise'' test languages, resulting in 27 different test setups. This paper describes our participating system to this task. Our team is one of the 6 teams that successfully submitted runs for all setups. The official results show that our system is ranked among the top 3 systems for 10 out of the 27 setups.\n        \u25b3 Less\n      ",
    "title": "QCRI at SemEval-2023 Task 3: News Genre, Framing and Persuasion Techniques Detection using Multilingual Models",
    "date": "5 May, 2023",
    "authors": [
      "Maram Hasanain",
      " Ahmed Oumar El-Shangiti",
      " Rabindra Nath Nandi",
      " Preslav Nakov",
      " Firoj Alam"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03340",
    "paper_id": "2305.03340",
    "abstract": "\n        Many major questions in the theory of evolutionary dynamics can in a meaningful sense be mapped to analyses of stochastic trajectories in game theoretic contexts. Often the approach is to analyze small numbers of distinct populations and/or to assume dynamics occur within a regime of population sizes large enough that deterministic trajectories are an excellent approximation of reality. The addition of ecological factors, termed \"eco-evolutionary dynamics\", further complicates the dynamics and results in many problems which are intractable or impractically messy for current theoretical methods. However, an analogous but underexplored approach is to analyze these systems with an eye primarily towards uncertainty in the models themselves. In the language of researchers in Reinforcement Learning and adjacent fields, a Partially Observable Markov Process. Here we introduce a duality which maps the complexity of accounting for both ecology and individual genotypic/phenotypic types onto a problem of accounting solely for underlying information-theoretic computations rather than drawing physical boundaries which do not change the computations. Armed with this equivalence between computation and the relevant biophysics, which we term Taak-duality, we attack the problem of \"directed evolution\" in the form of a Partially Observable Markov Decision Process. This provides a tractable case of studying eco-evolutionary trajectories of a highly general type, and of analyzing questions of potential limits on the efficiency of evolution in the directed case.\n        \u25b3 Less\n      ",
    "title": "Biophysical Cybernetics of Directed Evolution and Eco-evolutionary Dynamics",
    "date": "5 May, 2023",
    "authors": [
      "Bryce Allen Bagley"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.11047",
    "paper_id": "2301.11047",
    "abstract": "\n        With the ever-growing adoption of AI-based systems, the carbon footprint of AI is no longer negligible. AI researchers and practitioners are therefore urged to hold themselves accountable for the carbon emissions of the AI models they design and use. This led in recent years to the appearance of researches tackling AI environmental sustainability, a field referred to as Green AI. Despite the rapid growth of interest in the topic, a comprehensive overview of Green AI research is to date still missing. To address this gap, in this paper, we present a systematic review of the Green AI literature. From the analysis of 98 primary studies, different patterns emerge. The topic experienced a considerable growth from 2020 onward. Most studies consider monitoring AI model footprint, tuning hyperparameters to improve model sustainability, or benchmarking models. A mix of position papers, observational studies, and solution papers are present. Most papers focus on the training phase, are algorithm-agnostic or study neural networks, and use image data. Laboratory experiments are the most common research strategy. Reported Green AI energy savings go up to 115%, with savings over 50% being rather common. Industrial parties are involved in Green AI studies, albeit most target academic readers. Green AI tool provisioning is scarce. As a conclusion, the Green AI research field results to have reached a considerable level of maturity. Therefore, from this review emerges that the time is suitable to adopt other Green AI research strategies, and port the numerous promising academic results to industrial practice.\n        \u25b3 Less\n      ",
    "title": "A Systematic Review of Green AI",
    "date": "5 May, 2023",
    "authors": [
      "Roberto Verdecchia",
      " June Sallou",
      " Lu\u00eds Cruz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.14997",
    "paper_id": "2211.14997",
    "abstract": "\n        Enterprise financial risk analysis aims at predicting the future financial risk of enterprises. Due to its wide and significant application, enterprise financial risk analysis has always been the core research topic in the fields of Finance and Management. Based on advanced computer science and artificial intelligence technologies, enterprise risk analysis research is experiencing rapid developments and making significant progress. Therefore, it is both necessary and challenging to comprehensively review the relevant studies. Although there are already some valuable and impressive surveys on enterprise risk analysis from the perspective of Finance and Management, these surveys introduce approaches in a relatively isolated way and lack recent advances in enterprise financial risk analysis. In contrast, this paper attempts to provide a systematic literature survey of enterprise risk analysis approaches from Big Data perspective, which reviews more than 250 representative articles in the past almost 50 years (from 1968 to 2023). To the best of our knowledge, this is the first and only survey work on enterprise financial risk from Big Data perspective. Specifically, this survey connects and systematizes the existing enterprise financial risk studies, i.e. to summarize and interpret the problems, methods, and spotlights in a comprehensive way. In particular, we first introduce the issues of enterprise financial risks in terms of their types,granularity, intelligence, and evaluation metrics, and summarize the corresponding representative works. Then, we compare the analysis methods used to learn enterprise financial risk, and finally summarize the spotlights of the most representative works. Our goal is to clarify current cutting-edge research and its possible future directions to model enterprise risk, aiming to fully understand the mechanisms of enterprise risk generation and contagion.\n        \u25b3 Less\n      ",
    "title": "A Comprehensive Survey on Enterprise Financial Risk Analysis from Big Data Perspective",
    "date": "5 May, 2023",
    "authors": [
      "Yu Zhao",
      " Huaming Du",
      " Qing Li",
      " Fuzhen Zhuang",
      " Ji Liu",
      " Gang Kou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03352",
    "paper_id": "2305.03352",
    "abstract": "\n        Image/video denoising in low-light scenes is an extremely challenging problem due to limited photon count and high noise. In this paper, we propose a novel approach with contrastive learning to address this issue. Inspired by the success of contrastive learning used in some high-level computer vision tasks, we bring in this idea to the low-level denoising task. In order to achieve this goal, we introduce a new denoising contrastive regularization (DCR) to exploit the information of noisy images and clean images. In the feature space, DCR makes the denoised image closer to the clean image and far away from the noisy image. In addition, we build a new feature embedding network called Wnet, which is more effective to extract high-frequency information. We conduct the experiments on a real low-light dataset that captures still images taken on a moonless clear night in 0.6 millilux and videos under starlight (no moon present, <0.001 lux). The results show that our method can achieve a higher PSNR and better visual quality compared with existing methods\n        \u25b3 Less\n      ",
    "title": "Contrastive Learning for Low-light Raw Denoising",
    "date": "5 May, 2023",
    "authors": [
      "Taoyong Cui",
      " Yuhan Dong"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03356",
    "paper_id": "2305.03356",
    "abstract": "\n        Parsing questions into executable logical forms has showed impressive results for knowledge-base question answering (KBQA). However, complex KBQA is a more challenging task that requires to perform complex multi-step reasoning. Recently, a new semantic parser called KoPL has been proposed to explicitly model the reasoning processes, which achieved the state-of-the-art on complex KBQA. In this paper, we further explore how to unlock the reasoning ability of semantic parsers by a simple proposed parse-execute-refine paradigm. We refine and improve the KoPL parser by demonstrating the executed intermediate reasoning steps to the KBQA model. We show that such simple strategy can significantly improve the ability of complex reasoning. Specifically, we propose three components: a parsing stage, an execution stage and a refinement stage, to enhance the ability of complex reasoning. The parser uses the KoPL to generate the transparent logical forms. Then, the execution stage aligns and executes the logical forms over knowledge base to obtain intermediate reasoning processes. Finally, the intermediate step-by-step reasoning processes are demonstrated to the KBQA model in the refinement stage. With the explicit reasoning processes, it is much easier to answer the complex questions. Experiments on benchmark dataset shows that the proposed PER-KBQA performs significantly better than the stage-of-the-art baselines on the complex KBQA.\n        \u25b3 Less\n      ",
    "title": "From Parse-Execute to Parse-Execute-Refine: Improving Semantic Parser for Complex Question Answering over Knowledge Base",
    "date": "5 May, 2023",
    "authors": [
      "Wangzhen Guo",
      " Linyin Luo",
      " Hanjiang Lai",
      " Jian Yin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03360",
    "paper_id": "2305.03360",
    "abstract": "\n        Model-based approaches are becoming increasingly popular in the field of offline reinforcement learning, with high potential in real-world applications due to the model's capability of thoroughly utilizing the large historical datasets available with supervised learning techniques. This paper presents a literature review of recent work in offline model-based reinforcement learning, a field that utilizes model-based approaches in offline reinforcement learning. The survey provides a brief overview of the concepts and recent developments in both offline reinforcement learning and model-based reinforcement learning, and discuss the intersection of the two fields. We then presents key relevant papers in the field of offline model-based reinforcement learning and discuss their methods, particularly their approaches in solving the issue of distributional shift, the main problem faced by all current offline model-based reinforcement learning methods. We further discuss key challenges faced by the field, and suggest possible directions for future work.\n        \u25b3 Less\n      ",
    "title": "A Survey on Offline Model-Based Reinforcement Learning",
    "date": "5 May, 2023",
    "authors": [
      "Haoyang He"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04927",
    "paper_id": "2305.04927",
    "abstract": "\n        Social media platforms empower us in several ways, from information dissemination to consumption. While these platforms are useful in promoting citizen journalism, public awareness etc., they have misuse potentials. Malicious users use them to disseminate hate-speech, offensive content, rumor etc. to gain social and political agendas or to harm individuals, entities and organizations. Often times, general users unconsciously share information without verifying it, or unintentionally post harmful messages. Some of such content often get deleted either by the platform due to the violation of terms and policies, or users themselves for different reasons, e.g., regrets. There is a wide range of studies in characterizing, understanding and predicting deleted content. However, studies which aims to identify the fine-grained reasons (e.g., posts are offensive, hate speech or no identifiable reason) behind deleted content, are limited. In this study we address this gap, by identifying deleted tweets, particularly within the Arabic context, and labeling them with a corresponding fine-grained disinformation category. We then develop models that can predict the potentiality of tweets getting deleted, as well as the potential reasons behind deletion. Such models can help in moderating social media posts before even posting.\n        \u25b3 Less\n      ",
    "title": "Detecting and Reasoning of Deleted Tweets before they are Posted",
    "date": "5 May, 2023",
    "authors": [
      "Hamdy Mubarak",
      " Samir Abdaljalil",
      " Azza Nassar",
      " Firoj Alam"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03365",
    "paper_id": "2305.03365",
    "abstract": "\n        The increasing use of deep neural networks (DNNs) in safety-critical systems has raised concerns about their potential for exhibiting ill-behaviors. While DNN verification and testing provide post hoc conclusions regarding unexpected behaviors, they do not prevent the erroneous behaviors from occurring. To address this issue, DNN repair/patch aims to eliminate unexpected predictions generated by defective DNNs. Two typical DNN repair paradigms are retraining and fine-tuning. However, existing methods focus on the high-level abstract interpretation or inference of state spaces, ignoring the underlying neurons' outputs. This renders patch processes computationally prohibitive and limited to piecewise linear (PWL) activation functions to great extent. To address these shortcomings, we propose a behavior-imitation based repair framework, BIRDNN, which integrates the two repair paradigms for the first time. BIRDNN corrects incorrect predictions of negative samples by imitating the closest expected behaviors of positive samples during the retraining repair procedure. For the fine-tuning repair process, BIRDNN analyzes the behavior differences of neurons on positive and negative samples to identify the most responsible neurons for the erroneous behaviors. To tackle more challenging domain-wise repair problems (DRPs), we synthesize BIRDNN with a domain behavior characterization technique to repair buggy DNNs in a probably approximated correct style. We also implement a prototype tool based on BIRDNN and evaluate it on ACAS Xu DNNs. Our experimental results show that BIRDNN can successfully repair buggy DNNs with significantly higher efficiency than state-of-the-art repair tools. Additionally, BIRDNN is highly compatible with different activation functions.\n        \u25b3 Less\n      ",
    "title": "Repairing Deep Neural Networks Based on Behavior Imitation",
    "date": "5 May, 2023",
    "authors": [
      "Zhen Liang",
      " Taoran Wu",
      " Changyuan Zhao",
      " Wanwei Liu",
      " Bai Xue",
      " Wenjing Yang",
      " Ji Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03369",
    "paper_id": "2305.03369",
    "abstract": "\n        The MuSe 2023 is a set of shared tasks addressing three different contemporary multimodal affect and sentiment analysis problems: In the Mimicked Emotions Sub-Challenge (MuSe-Mimic), participants predict three continuous emotion targets. This sub-challenge utilises the Hume-Vidmimic dataset comprising of user-generated videos. For the Cross-Cultural Humour Detection Sub-Challenge (MuSe-Humour), an extension of the Passau Spontaneous Football Coach Humour (Passau-SFCH) dataset is provided. Participants predict the presence of spontaneous humour in a cross-cultural setting. The Personalisation Sub-Challenge (MuSe-Personalisation) is based on the Ulm-Trier Social Stress Test (Ulm-TSST) dataset, featuring recordings of subjects in a stressed situation. Here, arousal and valence signals are to be predicted, whereas parts of the test labels are made available in order to facilitate personalisation. MuSe 2023 seeks to bring together a broad audience from different research communities such as audio-visual emotion recognition, natural language processing, signal processing, and health informatics. In this baseline paper, we introduce the datasets, sub-challenges, and provided feature sets. As a competitive baseline system, a Gated Recurrent Unit (GRU)-Recurrent Neural Network (RNN) is employed. On the respective sub-challenges' test datasets, it achieves a mean (across three continuous intensity targets) Pearson's Correlation Coefficient of .4727 for MuSe-Mimic, an Area Under the Curve (AUC) value of .8310 for MuSe-Humor and Concordance Correlation Coefficient (CCC) values of .7482 for arousal and .7827 for valence in the MuSe-Personalisation sub-challenge.\n        \u25b3 Less\n      ",
    "title": "The MuSe 2023 Multimodal Sentiment Analysis Challenge: Mimicked Emotions, Cross-Cultural Humour, and Personalisation",
    "date": "5 May, 2023",
    "authors": [
      "Lukas Christ",
      " Shahin Amiriparian",
      " Alice Baird",
      " Alexander Kathan",
      " Niklas M\u00fcller",
      " Steffen Klug",
      " Chris Gagne",
      " Panagiotis Tzirakis",
      " Eva-Maria Me\u00dfner",
      " Andreas K\u00f6nig",
      " Alan Cowen",
      " Erik Cambria",
      " Bj\u00f6rn W. Schuller"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06157",
    "paper_id": "2305.06157",
    "abstract": "\n        In this paper, we have shown the improvement of English to Bharti Braille machine translation system. We have shown how we can improve a baseline NMT model by adding some linguistic knowledge to it. This was done for five language pairs where English sentences were translated into five Indian languages and then subsequently to corresponding Bharti Braille. This has been demonstrated by adding a sub-module for translating multi-word expressions. The approach shows promising results as across language pairs, we could see improvement in the quality of NMT outputs. The least improvement was observed in English-Nepali language pair with 22.08% and the most improvement was observed in the English-Hindi language pair with 23.30%.\n        \u25b3 Less\n      ",
    "title": "Implications of Multi-Word Expressions on English to Bharti Braille Machine Translation",
    "date": "5 May, 2023",
    "authors": [
      "Nisheeth Joshi",
      " Pragya Katyayan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03375",
    "paper_id": "2305.03375",
    "abstract": "\n        This paper follows calls for critical approaches to computing and conceptualisations of intersectional, feminist, decolonial HCI and AI design and asks what a feminist intersectional perspective in HCXAI research and design might look like. Sketching out initial research directions and implications for explainable AI design, it suggests that explainability from a feminist perspective would include the fostering of response-ability - the capacity to critically evaluate and respond to AI systems - and would centre marginalised perspectives.\n        \u25b3 Less\n      ",
    "title": "Towards Feminist Intersectional XAI: From Explainability to Response-Ability",
    "date": "5 May, 2023",
    "authors": [
      "Goda Klumbyte",
      " Hannah Piehl",
      " Claude Draude"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03376",
    "paper_id": "2305.03376",
    "abstract": "\n        Contemporary automation through AI entails a substantial amount of behind-the-scenes human labour, which is often both invisibilised and underpaid. Since invisible labour, including labelling and maintenance work, is an integral part of contemporary AI systems, it remains important to sensitise users to its role. We suggest that this could be done through explainable AI (XAI) design, particularly feminist intersectional XAI. We propose the method of cartography, which stems from feminist intersectional research, to draw out a systemic perspective of AI and include dimensions of AI that pertain to invisible labour.\n        \u25b3 Less\n      ",
    "title": "Explaining the ghosts: Feminist intersectional XAI and cartography as methods to account for invisible labour",
    "date": "5 May, 2023",
    "authors": [
      "Goda Klumbyte",
      " Hannah Piehl",
      " Claude Draude"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02942",
    "paper_id": "2305.02942",
    "abstract": "\n        Obtaining high-quality data for collaborative training of machine learning models can be a challenging task due to A) the regulatory concerns and B) lack of incentive to participate. The first issue can be addressed through the use of privacy enhancing technologies (PET), one of the most frequently used one being differentially private (DP) training. The second challenge can be addressed by identifying which data points can be beneficial for model training and rewarding data owners for sharing this data. However, DP in deep learning typically adversely affects atypical (often informative) data samples, making it difficult to assess the usefulness of individual contributions. In this work we investigate how to leverage gradient information to identify training samples of interest in private training settings. We show that there exist techniques which are able to provide the clients with the tools for principled data selection even in strictest privacy settings.\n        \u25b3 Less\n      ",
    "title": "Leveraging gradient-derived metrics for data selection and valuation in differentially private training",
    "date": "5 May, 2023",
    "authors": [
      "Dmitrii Usynin",
      " Daniel Rueckert",
      " Georgios Kaissis"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2201.09354",
    "paper_id": "2201.09354",
    "abstract": "\n        Strong demand for autonomous vehicles and the wide availability of 3D sensors are continuously fueling the proposal of novel methods for 3D object detection. In this paper, we provide a comprehensive survey of recent developments from 2012-2021 in 3D object detection covering the full pipeline from input data, over data representation and feature extraction to the actual detection modules. We introduce fundamental concepts, focus on a broad range of different approaches that have emerged over the past decade, and propose a systematization that provides a practical framework for comparing these approaches with the goal of guiding future development, evaluation and application activities. Specifically, our survey and systematization of 3D object detection models and methods can help researchers and practitioners to get a quick overview of the field by decomposing 3DOD solutions into more manageable pieces.\n        \u25b3 Less\n      ",
    "title": "Survey and Systematization of 3D Object Detection Models and Methods",
    "date": "5 May, 2023",
    "authors": [
      "Moritz Drobnitzky",
      " Jonas Friederich",
      " Bernhard Egger",
      " Patrick Zschech"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06475",
    "paper_id": "2305.06475",
    "abstract": "\n        People who are visually impaired face a lot of difficulties while studying. One of the major causes to this is lack of available text in Bharti Braille script. In this paper, we have suggested a scheme to convert text in major Indian languages into Bharti Braille. The system uses a hybrid approach where at first the text in Indian language is given to a rule based system and in case if there is any ambiguity then it is resolved by applying a LSTM based model. The developed model has also been tested and found to have produced near accurate results.\n        \u25b3 Less\n      ",
    "title": "A Model for Translation of Text from Indian Languages to Bharti Braille Characters",
    "date": "5 May, 2023",
    "authors": [
      "Nisheeth Joshi",
      " Pragya Katyayan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03391",
    "paper_id": "2305.03391",
    "abstract": "\n        Convolutional neural networks (CNNs) are commonplace in high-performing solutions to many real-world problems, such as audio classification. CNNs have many parameters and filters, with some having a larger impact on the performance than others. This means that networks may contain many unnecessary filters, increasing a CNN's computation and memory requirements while providing limited performance benefits. To make CNNs more efficient, we propose a pruning framework that eliminates filters with the highest \"commonality\". We measure this commonality using the graph-theoretic concept of \"centrality\". We hypothesise that a filter with a high centrality should be eliminated as it represents commonality and can be replaced by other filters without affecting the performance of a network much. An experimental evaluation of the proposed framework is performed on acoustic scene classification and audio tagging. On the DCASE 2021 Task 1A baseline network, our proposed method reduces computations per inference by 71\\% with 50\\% fewer parameters at less than a two percentage point drop in accuracy compared to the original network. For large-scale CNNs such as PANNs designed for audio tagging, our method reduces 24\\% computations per inference with 41\\% fewer parameters at a slight improvement in performance.\n        \u25b3 Less\n      ",
    "title": "Compressing audio CNNs with graph centrality based filter pruning",
    "date": "5 May, 2023",
    "authors": [
      "James A King",
      " Arshdeep Singh",
      " Mark D. Plumbley"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.03376",
    "paper_id": "2204.03376",
    "abstract": "\n        The widespread adoption of effective hybrid closed loop systems would represent an important milestone of care for people living with type 1 diabetes (T1D). These devices typically utilise simple control algorithms to select the optimal insulin dose for maintaining blood glucose levels within a healthy range. Online reinforcement learning (RL) has been utilised as a method for further enhancing glucose control in these devices. Previous approaches have been shown to reduce patient risk and improve time spent in the target range when compared to classical control algorithms, but are prone to instability in the learning process, often resulting in the selection of unsafe actions. This work presents an evaluation of offline RL for developing effective dosing policies without the need for potentially dangerous patient interaction during training. This paper examines the utility of BCQ, CQL and TD3-BC in managing the blood glucose of the 30 virtual patients available within the FDA-approved UVA/Padova glucose dynamics simulator. When trained on less than a tenth of the total training samples required by online RL to achieve stable performance, this work shows that offline RL can significantly increase time in the healthy blood glucose range from 61.6 +\\- 0.3% to 65.3 +/- 0.5% when compared to the strongest state-of-art baseline (p < 0.001). This is achieved without any associated increase in low blood glucose events. Offline RL is also shown to be able to correct for common and challenging control scenarios such as incorrect bolus dosing, irregular meal timings and compression errors.\n        \u25b3 Less\n      ",
    "title": "Offline Reinforcement Learning for Safer Blood Glucose Control in People with Type 1 Diabetes",
    "date": "5 May, 2023",
    "authors": [
      "Harry Emerson",
      " Matthew Guy",
      " Ryan McConville"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03414",
    "paper_id": "2305.03414",
    "abstract": "\n        Spectral-type subspace clustering algorithms have shown excellent performance in many subspace clustering applications. The existing spectral-type subspace clustering algorithms either focus on designing constraints for the reconstruction coefficient matrix or feature extraction methods for finding latent features of original data samples. In this paper, inspired by graph convolutional networks, we use the graph convolution technique to develop a feature extraction method and a coefficient matrix constraint simultaneously. And the graph-convolutional operator is updated iteratively and adaptively in our proposed algorithm. Hence, we call the proposed method adaptive graph convolutional subspace clustering (AGCSC). We claim that by using AGCSC, the aggregated feature representation of original data samples is suitable for subspace clustering, and the coefficient matrix could reveal the subspace structure of the original data set more faithfully. Finally, plenty of subspace clustering experiments prove our conclusions and show that AGCSC outperforms some related methods as well as some deep models.\n        \u25b3 Less\n      ",
    "title": "Adaptive Graph Convolutional Subspace Clustering",
    "date": "5 May, 2023",
    "authors": [
      "Lai Wei",
      " Zhengwei Chen",
      " Jun Yin",
      " Changming Zhu",
      " Rigui Zhou",
      " Jin Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05543",
    "paper_id": "2305.05543",
    "abstract": "\n        We introduce Walk4Me, a telehealth community mobility assessment system designed to facilitate early diagnosis, severity, and progression identification. Our system achieves this by 1) enabling early diagnosis, 2) identifying early indicators of clinical severity, and 3) quantifying and tracking the progression of the disease across the ambulatory phase of the disease. To accomplish this, we employ an Artificial Intelligence (AI)-based detection of gait characteristics in patients and typically developing peers. Our system remotely and in real-time collects data from device sensors (e.g., acceleration from a mobile device, etc.) using our novel Walk4Me API. Our web application extracts temporal/spatial gait characteristics and raw data signal characteristics and then employs traditional machine learning and deep learning techniques to identify patterns that can 1) identify patients with gait disturbances associated with disease, 2) describe the degree of mobility limitation, and 3) identify characteristics that change over time with disease progression. We have identified several machine learning techniques that differentiate between patients and typically-developing subjects with 100% accuracy across the age range studied, and we have also identified corresponding temporal/spatial gait characteristics associated with each group. Our work demonstrates the potential of utilizing the latest advances in mobile device and machine learning technology to measure clinical outcomes regardless of the point of care, inform early clinical diagnosis and treatment decision-making, and monitor disease progression.\n        \u25b3 Less\n      ",
    "title": "Walk4Me: Telehealth Community Mobility Assessment, An Automated System for Early Diagnosis and Disease Progression",
    "date": "5 May, 2023",
    "authors": [
      "Albara Ah Ramli",
      " Xin Liu",
      " Erik K. Henricson"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03435",
    "paper_id": "2305.03435",
    "abstract": "\n        Modern radio telescopes will daily generate data sets on the scale of exabytes for systems like the Square Kilometre Array (SKA). Massive data sets are a source of unknown and rare astrophysical phenomena that lead to discoveries. Nonetheless, this is only plausible with the exploitation of intensive machine intelligence to complement human-aided and traditional statistical techniques. Recently, there has been a surge in scientific publications focusing on the use of artificial intelligence in radio astronomy, addressing challenges such as source extraction, morphological classification, and anomaly detection. This study presents a succinct, but comprehensive review of the application of machine intelligence techniques on radio images with emphasis on the morphological classification of radio galaxies. It aims to present a detailed synthesis of the relevant papers summarizing the literature based on data complexity, data pre-processing, and methodological novelty in radio astronomy. The rapid advancement and application of computer intelligence in radio astronomy has resulted in a revolution and a new paradigm shift in the automation of daunting data processes. However, the optimal exploitation of artificial intelligence in radio astronomy, calls for continued collaborative efforts in the creation of annotated data sets. Additionally, in order to quickly locate radio galaxies with similar or dissimilar physical characteristics, it is necessary to index the identified radio sources. Nonetheless, this issue has not been adequately addressed in the literature, making it an open area for further study.\n        \u25b3 Less\n      ",
    "title": "Advances on the classification of radio image cubes",
    "date": "5 May, 2023",
    "authors": [
      "Steven Ndung'u",
      " Trienko Grobler",
      " Stefan J. Wijnholds",
      " Dimka Karastoyanova",
      " George Azzopardi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03458",
    "paper_id": "2305.03458",
    "abstract": "\n        Hybrid question answering (HybridQA) over the financial report contains both textual and tabular data, and requires the model to select the appropriate evidence for the numerical reasoning task. Existing methods based on encoder-decoder framework employ a expression tree-based decoder to solve numerical reasoning problems. However, encoders rely more on Machine Reading Comprehension (MRC) methods, which take table serialization and text splicing as input, damaging the granularity relationship between table and text as well as the spatial structure information of table itself. In order to solve these problems, the paper proposes a Multi-View Graph (MVG) Encoder to take the relations among the granularity into account and capture the relations from multiple view. By utilizing MVGE as a module, we constuct Tabular View, Relation View and Numerical View which aim to retain the original characteristics of the hybrid data. We validate our model on the publicly available table-text hybrid QA benchmark (TAT-QA) and outperform the state-of-the-art model.\n        \u25b3 Less\n      ",
    "title": "Multi-View Graph Representation Learning for Answering Hybrid Numerical Reasoning Question",
    "date": "5 May, 2023",
    "authors": [
      "Yifan Wei",
      " Fangyu Lei",
      " Yuanzhe Zhang",
      " Jun Zhao",
      " Kang Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.12173",
    "paper_id": "2302.12173",
    "abstract": "\n        Large Language Models (LLMs) are increasingly being integrated into various applications. The functionalities of recent LLMs can be flexibly modulated via natural language prompts. This renders them susceptible to targeted adversarial prompting, e.g., Prompt Injection (PI) attacks enable attackers to override original instructions and employed controls. So far, it was assumed that the user is directly prompting the LLM. But, what if it is not the user prompting? We argue that LLM-Integrated Applications blur the line between data and instructions. We reveal new attack vectors, using Indirect Prompt Injection, that enable adversaries to remotely (without a direct interface) exploit LLM-integrated applications by strategically injecting prompts into data likely to be retrieved. We derive a comprehensive taxonomy from a computer security perspective to systematically investigate impacts and vulnerabilities, including data theft, worming, information ecosystem contamination, and other novel security risks. We demonstrate our attacks' practical viability against both real-world systems, such as Bing's GPT-4 powered Chat and code-completion engines, and synthetic applications built on GPT-4. We show how processing retrieved prompts can act as arbitrary code execution, manipulate the application's functionality, and control how and if other APIs are called. Despite the increasing integration and reliance on LLMs, effective mitigations of these emerging threats are currently lacking. By raising awareness of these vulnerabilities and providing key insights into their implications, we aim to promote the safe and responsible deployment of these powerful models and the development of robust defenses that protect users and systems from potential attacks.\n        \u25b3 Less\n      ",
    "title": "Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
    "date": "5 May, 2023",
    "authors": [
      "Kai Greshake",
      " Sahar Abdelnabi",
      " Shailesh Mishra",
      " Christoph Endres",
      " Thorsten Holz",
      " Mario Fritz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03572",
    "paper_id": "2305.03572",
    "abstract": "\n        Image-based rendering techniques stand at the core of an immersive experience for the user, as they generate novel views given a set of multiple input images. Since they have shown good performance in terms of objective and subjective quality, the research community devotes great effort to their improvement. However, the large volume of data necessary to render at the receiver's side hinders applications in limited bandwidth environments or prevents their employment in real-time applications. We present LeHoPP, a method for input pixel pruning, where we examine the importance of each input pixel concerning the rendered view, and we avoid the use of irrelevant pixels. Even without retraining the image-based rendering network, our approach shows a good trade-off between synthesis quality and pixel rate. When tested in the general neural rendering framework, compared to other pruning baselines, LeHoPP gains between 0.90.9 dB and 3.63.6 dB on average.\n        \u25b3 Less\n      ",
    "title": "Learn how to Prune Pixels for Multi-view Neural Image-based Synthesis",
    "date": "5 May, 2023",
    "authors": [
      "Marta Milovanovi\u0107",
      " Enzo Tartaglione",
      " Marco Cagnazzo",
      " F\u00e9lix Henry"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03573",
    "paper_id": "2305.03573",
    "abstract": "\n        The phenomena of in-context learning has typically been thought of as \"learning from examples\". In this work which focuses on Machine Translation, we present a perspective of in-context learning as the desired generation task maintaining coherency with its context, i.e., the prompt examples. We first investigate randomly sampled prompts across 4 domains, and find that translation performance improves when shown in-domain prompts. Next, we investigate coherency for the in-domain setting, which uses prompt examples from a moving window. We study this with respect to other factors that have previously been identified in the literature such as length, surface similarity and sentence embedding similarity. Our results across 3 models (GPTNeo2.7B, Bloom3B, XGLM2.9B), and three translation directions (\\texttt{en}\u2192\\rightarrow\\{\\texttt{pt, de, fr}\\}) suggest that the long-term coherency of the prompts and the test sentence is a good indicator of downstream translation performance. In doing so, we demonstrate the efficacy of In-context Machine Translation for on-the-fly adaptation.\n        \u25b3 Less\n      ",
    "title": "In-context Learning as Maintaining Coherency: A Study of On-the-fly Machine Translation Using Large Language Models",
    "date": "5 May, 2023",
    "authors": [
      "Suzanna Sia",
      " Kevin Duh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03601",
    "paper_id": "2305.03601",
    "abstract": "\n        We examined whether embedding human attention knowledge into saliency-based explainable AI (XAI) methods for computer vision models could enhance their plausibility and faithfulness. We first developed new gradient-based XAI methods for object detection models to generate object-specific explanations by extending the current methods for image classification models. Interestingly, while these gradient-based methods worked well for explaining image classification models, when being used for explaining object detection models, the resulting saliency maps generally had lower faithfulness than human attention maps when performing the same task. We then developed Human Attention-Guided XAI (HAG-XAI) to learn from human attention how to best combine explanatory information from the models to enhance explanation plausibility by using trainable activation functions and smoothing kernels to maximize XAI saliency map's similarity to human attention maps. While for image classification models, HAG-XAI enhanced explanation plausibility at the expense of faithfulness, for object detection models it enhanced plausibility and faithfulness simultaneously and outperformed existing methods. The learned functions were model-specific, well generalizable to other databases.\n        \u25b3 Less\n      ",
    "title": "Human Attention-Guided Explainable Artificial Intelligence for Computer Vision Models",
    "date": "5 May, 2023",
    "authors": [
      "Guoyang Liu",
      " Jindi Zhang",
      " Antoni B. Chan",
      " Janet H. Hsiao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2111.08792",
    "paper_id": "2111.08792",
    "abstract": "\n        We present PredProp, a method for optimization of weights and states in predictive coding networks (PCNs) based on the precision of propagated errors and neural activity. PredProp jointly addresses inference and learning via stochastic gradient descent and adaptively weights parameter updates by approximate curvature. Due to the relation between propagated error covariance and the Fisher information matrix, PredProp implements approximate Natural Gradient Descent. We demonstrate PredProp's effectiveness in the context of dense decoder networks and simple image benchmark datasets. We found that PredProp performs favorably over Adam, a widely used adaptive learning rate optimizer in the tested configurations. Furthermore, available optimization methods for weight parameters benefit from using PredProp's error precision during inference. Since hierarchical predictive coding layers are optimised individually using local errors, the required precisions factorize over hierarchical layers. Extending beyond classical PCNs with a single set of decoder layers per hierarchical layer, we also generalize PredProp to deep neural networks in each PCN layer by additionally factorizing over the weights in each PCN layer.\n        \u25b3 Less\n      ",
    "title": "PredProp: Bidirectional Stochastic Optimization with Precision Weighted Predictive Coding",
    "date": "5 May, 2023",
    "authors": [
      "Andr\u00e9 Ofner",
      " Sebastian Stober"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03632",
    "paper_id": "2305.03632",
    "abstract": "\n        This study extends the recently-developed LaCAM algorithm for multi-agent pathfinding (MAPF). LaCAM is a sub-optimal search-based algorithm that uses lazy successor generation to dramatically reduce the planning effort. We present two enhancements. First, we propose its anytime version, called LaCAM*, which eventually converges to optima, provided that solution costs are accumulated transition costs. Second, we improve the successor generation to quickly obtain initial solutions. Exhaustive experiments demonstrate their utility. For instance, LaCAM* sub-optimally solved 99% of the instances retrieved from the MAPF benchmark, where the number of agents varied up to a thousand, within ten seconds on a standard desktop PC, while ensuring eventual convergence to optima; developing a new horizon of MAPF algorithms.\n        \u25b3 Less\n      ",
    "title": "Improving LaCAM for Scalable Eventually Optimal Multi-Agent Pathfinding",
    "date": "5 May, 2023",
    "authors": [
      "Keisuke Okumura"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2105.06903",
    "paper_id": "2105.06903",
    "abstract": "\n        Bayesian hierarchical mixture clustering (BHMC) improves traditionalBayesian hierarchical clustering by replacing conventional Gaussian-to-Gaussian kernels with a Hierarchical Dirichlet Process Mixture Model(HDPMM) for parent-to-child diffusion in the generative process. However,BHMC may produce trees with high nodal variance, indicating weak separation between nodes at higher levels. To address this issue, we employ Posterior Regularization, which imposes max-margin constraints on nodes at every level to enhance cluster separation. We illustrate how to apply PR toBHMC and demonstrate its effectiveness in improving the BHMC model.\n        \u25b3 Less\n      ",
    "title": "Posterior Regularization on Bayesian Hierarchical Mixture Clustering",
    "date": "5 May, 2023",
    "authors": [
      "Weipeng Huang",
      " Tin Lok James Ng",
      " Nishma Laitonjam",
      " Neil J. Hurley"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03660",
    "paper_id": "2305.03660",
    "abstract": "\n        We propose Retrieval Augmented Generation (RAG) as an approach for automated radiology report writing that leverages multimodally aligned embeddings from a contrastively pretrained vision language model for retrieval of relevant candidate radiology text for an input radiology image and a general domain generative model like OpenAI text-davinci-003, gpt-3.5-turbo and gpt-4 for report generation using the relevant radiology text retrieved. This approach keeps hallucinated generations under check and provides capabilities to generate report content in the format we desire leveraging the instruction following capabilities of these generative models. Our approach achieves better clinical metrics with a BERTScore of 0.2865 (\u0394+ 25.88%) and Semb score of 0.4026 (\u0394+ 6.31%). Our approach can be broadly relevant for different clinical settings as it allows to augment the automated radiology report generation process with content relevant for that setting while also having the ability to inject user intents and requirements in the prompts as part of the report generation process to modulate the content and format of the generated reports as applicable for that clinical setting.\n        \u25b3 Less\n      ",
    "title": "Retrieval Augmented Chest X-Ray Report Generation using OpenAI GPT models",
    "date": "5 May, 2023",
    "authors": [
      "Mercy Ranjit",
      " Gopinath Ganapathy",
      " Ranjit Manuel",
      " Tanuja Ganu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03661",
    "paper_id": "2305.03661",
    "abstract": "\n        In this paper we present a novel approach to risk assessment for patients hospitalized with pneumonia or COVID-19 based on their admission reports. We applied a Longformer neural network to admission reports and other textual data available shortly after admission to compute risk scores for the patients. We used patient data of multiple European hospitals to demonstrate that our approach outperforms the Transformer baselines. Our experiments show that the proposed model generalises across institutions and diagnoses. Also, our method has several other advantages described in the paper.\n        \u25b3 Less\n      ",
    "title": "Predicting COVID-19 and pneumonia complications from admission texts",
    "date": "5 May, 2023",
    "authors": [
      "Dmitriy Umerenkov",
      " Oleg Cherkashin",
      " Alexander Nesterov",
      " Victor Gombolevskiy",
      " Irina Demko",
      " Alexander Yalunin",
      " Vladimir Kokh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03662",
    "paper_id": "2305.03662",
    "abstract": "\n        Using observational data to learn causal relationships is essential when randomized experiments are not possible, such as in healthcare. Discovering causal relationships in time-series health data is even more challenging when relationships change over the course of a disease, such as medications that are most effective early on or for individuals with severe disease. Stage variables such as weeks of pregnancy, disease stages, or biomarkers like HbA1c, can influence what causal relationships are true for a patient. However, causal inference within each stage is often not possible due to limited amounts of data, and combining all data risks incorrect or missed inferences. To address this, we propose Causal Discovery with Stage Variables (CDSV), which uses stage variables to reweight data from multiple time-series while accounting for different causal relationships in each stage. In simulated data, CDSV discovers more causes with fewer false discoveries compared to baselines, in eICU it has a lower FDR than baselines, and in MIMIC-III it discovers more clinically relevant causes of high blood pressure.\n        \u25b3 Less\n      ",
    "title": "Causal Discovery with Stage Variables for Health Time Series",
    "date": "5 May, 2023",
    "authors": [
      "Bharat Srikishan",
      " Samantha Kleinberg"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03691",
    "paper_id": "2305.03691",
    "abstract": "\n        Despite significant research efforts, deep neural networks are still vulnerable to biases: this raises concerns about their fairness and limits their generalization. In this paper, we propose a bias-agnostic approach to mitigate the impact of bias in deep neural networks. Unlike traditional debiasing approaches, we rely on a metric to quantify ``bias alignment/misalignment'' on target classes, and use this information to discourage the propagation of bias-target alignment information through the network. We conduct experiments on several commonly used datasets for debiasing and compare our method to supervised and bias-specific approaches. Our results indicate that the proposed method achieves comparable performance to state-of-the-art supervised approaches, although it is bias-agnostic, even in presence of multiple biases in the same sample.\n        \u25b3 Less\n      ",
    "title": "Mining bias-target Alignment from Voronoi Cells",
    "date": "5 May, 2023",
    "authors": [
      "R\u00e9mi Nahon",
      " Van-Tam Nguyen",
      " Enzo Tartaglione"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.01095",
    "paper_id": "2305.01095",
    "abstract": "\n        The development of Adaptive Cruise Control (ACC) systems aims to enhance the safety and comfort of vehicles by automatically regulating the speed of the vehicle to ensure a safe gap from the preceding vehicle. However, conventional ACC systems are unable to adapt themselves to changing driving conditions and drivers' behavior. To address this limitation, we propose a Long Short-Term Memory (LSTM) based ACC system that can learn from past driving experiences and adapt and predict new situations in real time. The model is constructed based on the real-world highD dataset, acquired from German highways with the assistance of camera-equipped drones. We evaluated the ACC system under aggressive lane changes when the side lane preceding vehicle cut off, forcing the targeted driver to reduce speed. To this end, the proposed system was assessed on a simulated driving environment and compared with a feedforward Artificial Neural Network (ANN) model and Model Predictive Control (MPC) model. The results show that the LSTM-based system is 19.25% more accurate than the ANN model and 5.9% more accurate than the MPC model in terms of predicting future values of subject vehicle acceleration. The simulation is done in Matlab/Simulink environment.\n        \u25b3 Less\n      ",
    "title": "LSTM-based Preceding Vehicle Behaviour Prediction during Aggressive Lane Change for ACC Application",
    "date": "5 May, 2023",
    "authors": [
      "Rajmeet Singh",
      " Saeed Mozaffari",
      " Mahdi Rezaei",
      " Shahpour Alirezaee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03724",
    "paper_id": "2305.03724",
    "abstract": "\n        Closing the domain gap between training and deployment and incorporating multiple sensor modalities are two challenging yet critical topics for self-driving. Existing work only focuses on single one of the above topics, overlooking the simultaneous domain and modality shift which pervasively exists in real-world scenarios. A model trained with multi-sensor data collected in Europe may need to run in Asia with a subset of input sensors available. In this work, we propose DualCross, a cross-modality cross-domain adaptation framework to facilitate the learning of a more robust monocular bird's-eye-view (BEV) perception model, which transfers the point cloud knowledge from a LiDAR sensor in one domain during the training phase to the camera-only testing scenario in a different domain. This work results in the first open analysis of cross-domain cross-sensor perception and adaptation for monocular 3D tasks in the wild. We benchmark our approach on large-scale datasets under a wide range of domain shifts and show state-of-the-art results against various baselines.\n        \u25b3 Less\n      ",
    "title": "DualCross: Cross-Modality Cross-Domain Adaptation for Monocular BEV Perception",
    "date": "5 May, 2023",
    "authors": [
      "Yunze Man",
      " Liang-Yan Gui",
      " Yu-Xiong Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00109",
    "paper_id": "2305.00109",
    "abstract": "\n        Segmentation in medical imaging is a critical component for the diagnosis, monitoring, and treatment of various diseases and medical conditions. Presently, the medical segmentation landscape is dominated by numerous specialized deep learning models, each fine-tuned for specific segmentation tasks and image modalities. The recently-introduced Segment Anything Model (SAM) employs the ViT neural architecture and harnesses a massive training dataset to segment nearly any object; however, its suitability to the medical domain has not yet been investigated. In this study, we explore the zero-shot performance of SAM in medical imaging by implementing eight distinct prompt strategies across six datasets from four imaging modalities, including X-ray, ultrasound, dermatoscopy, and colonoscopy. Our findings reveal that SAM's zero-shot performance is not only comparable to, but in certain cases, surpasses the current state-of-the-art. Based on these results, we propose practical guidelines that require minimal interaction while consistently yielding robust outcomes across all assessed contexts. The source code, along with a demonstration of the recommended guidelines, can be accessed at https://github.com/Malta-Lab/SAM-zero-shot-in-Medical-Imaging.\n        \u25b3 Less\n      ",
    "title": "Zero-shot performance of the Segment Anything Model (SAM) in 2D medical imaging: A comprehensive evaluation and practical guidelines",
    "date": "5 May, 2023",
    "authors": [
      "Christian Mattjie",
      " Luis Vinicius de Moura",
      " Rafaela Cappelari Ravazio",
      " Lucas Silveira Kupssinsk\u00fc",
      " Ot\u00e1vio Parraga",
      " Marcelo Mussi Delucis",
      " Rodrigo Coelho Barros"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.01044",
    "paper_id": "2305.01044",
    "abstract": "\n        DFU is a severe complication of diabetes that can lead to amputation of the lower limb if not treated properly. Inspired by the 2021 Diabetic Foot Ulcer Grand Challenge, researchers designed automated multi-class classification of DFU, including infection, ischaemia, both of these conditions, and none of these conditions. However, it remains a challenge as classification accuracy is still not satisfactory. This paper proposes a Venn Diagram interpretation of multi-label CNN-based method, utilizing different image enhancement strategies, to improve the multi-class DFU classification. We propose to reduce the four classes into two since both class wounds can be interpreted as the simultaneous occurrence of infection and ischaemia and none class wounds as the absence of infection and ischaemia. We introduce a novel Venn Diagram representation block in the classifier to interpret all four classes from these two classes. To make our model more resilient, we propose enhancing the perceptual quality of DFU images, particularly blurry or inconsistently lit DFU images, by performing color and sharpness enhancements on them. We also employ a fine-tuned optimization technique, adaptive sharpness aware minimization, to improve the CNN model generalization performance. The proposed method is evaluated on the test dataset of DFUC2021, containing 5,734 images and the results are compared with the top-3 winning entries of DFUC2021. Our proposed approach outperforms these existing approaches and achieves Macro-Average F1, Recall and Precision scores of 0.6592, 0.6593, and 0.6652, respectively.Additionally, We perform ablation studies and image quality measurements to further interpret our proposed method. This proposed method will benefit patients with DFUs since it tackles the inconsistencies in captured images and can be employed for a more robust remote DFU wound classification.\n        \u25b3 Less\n      ",
    "title": "Venn Diagram Multi-label Class Interpretation of Diabetic Foot Ulcer with Color and Sharpness Enhancement",
    "date": "5 May, 2023",
    "authors": [
      "Md Mahamudul Hasan",
      " Moi Hoon Yap",
      " Md Kamrul Hasan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03835",
    "paper_id": "2305.03835",
    "abstract": "\n        Financial markets are an intriguing place that offer investors the potential to gain large profits if timed correctly. Unfortunately, the dynamic, non-linear nature of financial markets makes it extremely hard to predict future price movements. Within the US stock exchange, there are a countless number of factors that play a role in the price of a company's stock, including but not limited to financial statements, social and news sentiment, overall market sentiment, political happenings and trading psychology. Correlating these factors is virtually impossible for a human. Therefore, we propose STST, a novel approach using a Spatiotemporal Transformer-LSTM model for stock movement prediction. Our model obtains accuracies of 63.707 and 56.879 percent against the ACL18 and KDD17 datasets, respectively. In addition, our model was used in simulation to determine its real-life applicability. It obtained a minimum of 10.41% higher profit than the S&P500 stock index, with a minimum annualized return of 31.24%.\n        \u25b3 Less\n      ",
    "title": "Spatiotemporal Transformer for Stock Movement Prediction",
    "date": "5 May, 2023",
    "authors": [
      "Daniel Boyle",
      " Jugal Kalita"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03843",
    "paper_id": "2305.03843",
    "abstract": "\n        This paper introduces a novel code-to-code search technique that enhances the performance of Large Language Models (LLMs) by including both static and dynamic features as well as utilizing both similar and dissimilar examples during training. We present the first-ever code search method that encodes dynamic runtime information during training without the need to execute either the corpus under search or the search query at inference time and the first code search technique that trains on both positive and negative reference samples. To validate the efficacy of our approach, we perform a set of studies demonstrating the capability of enhanced LLMs to perform cross-language code-to-code search.\n  Our evaluation demonstrates that the effectiveness of our approach is consistent across various model architectures and programming languages. We outperform the state-of-the-art cross-language search tool by up to 44.7\\%. Moreover, our ablation studies reveal that even a single positive and negative reference sample in the training process results in substantial performance improvements demonstrating both similar and dissimilar references are important parts of code search. Importantly, we show that enhanced well-crafted, fine-tuned models consistently outperform enhanced larger modern LLMs without fine tuning, even when enhancing the largest available LLMs highlighting the importance for open-sourced models.\n  To ensure the reproducibility and extensibility of our research, we present an open-sourced implementation of our tool and training procedures called Cosco.\n        \u25b3 Less\n      ",
    "title": "On Contrastive Learning of Semantic Similarity forCode to Code Search",
    "date": "5 May, 2023",
    "authors": [
      "Anthony Saieva",
      " Saikat Chakraborty",
      " Gail Kaiser"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03845",
    "paper_id": "2305.03845",
    "abstract": "\n        This paper summarizes the CLaC submission for the MultiCoNER 2 task which concerns the recognition of complex, fine-grained named entities. We compare two popular approaches for NER, namely Sequence Labeling and Span Prediction. We find that our best Span Prediction system performs slightly better than our best Sequence Labeling system on test data. Moreover, we find that using the larger version of XLM RoBERTa significantly improves performance. Post-competition experiments show that Span Prediction and Sequence Labeling approaches improve when they use special input tokens (<s> and </s>) of XLM-RoBERTa. The code for training all models, preprocessing, and post-processing is available at https://github.com/harshshredding/semeval2023-multiconer-paper.\n        \u25b3 Less\n      ",
    "title": "CLaC at SemEval-2023 Task 2: Comparing Span-Prediction and Sequence-Labeling approaches for NER",
    "date": "5 May, 2023",
    "authors": [
      "Harsh Verma",
      " Sabine Bergler"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03852",
    "paper_id": "2305.03852",
    "abstract": "\n        This paper explores the potential for utilizing generative AI models in group-focused co-creative frameworks to enhance problem solving and ideation in business innovation and co-creation contexts, and proposes a novel prompting technique for conversational generative AI agents which employ methods inspired by traditional 'human-to-human' facilitation and instruction to enable active contribution to Design Thinking, a co-creative framework. Through experiments using this prompting technique, we gather evidence that conversational generative transformers (i.e. ChatGPT) have the capability to contribute context-specific, useful, and creative input into Design Thinking activities. We also discuss the potential benefits, limitations, and risks associated with using generative AI models in co-creative ideation and provide recommendations for future research.\n        \u25b3 Less\n      ",
    "title": "CHAI-DT: A Framework for Prompting Conversational Generative AI Agents to Actively Participate in Co-Creation",
    "date": "5 May, 2023",
    "authors": [
      "Brandon Harwood"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.07805",
    "paper_id": "2211.07805",
    "abstract": "\n        In many, if not every realistic sequential decision-making task, the decision-making agent is not able to model the full complexity of the world. The environment is often much larger and more complex than the agent, a setting also known as partial observability. In such settings, the agent must leverage more than just the current sensory inputs; it must construct an agent state that summarizes previous interactions with the world. Currently, a popular approach for tackling this problem is to learn the agent-state function via a recurrent network from the agent's sensory stream as input. Many impressive reinforcement learning applications have instead relied on environment-specific functions to aid the agent's inputs for history summarization. These augmentations are done in multiple ways, from simple approaches like concatenating observations to more complex ones such as uncertainty estimates. Although ubiquitous in the field, these additional inputs, which we term auxiliary inputs, are rarely emphasized, and it is not clear what their role or impact is. In this work we explore this idea further, and relate these auxiliary inputs to prior classic approaches to state construction. We present a series of examples illustrating the different ways of using auxiliary inputs for reinforcement learning. We show that these auxiliary inputs can be used to discriminate between observations that would otherwise be aliased, leading to more expressive features that smoothly interpolate between different states. Finally, we show that this approach is complementary to state-of-the-art methods such as recurrent neural networks and truncated back-propagation through time, and acts as a heuristic that facilitates longer temporal credit assignment, leading to better performance.\n        \u25b3 Less\n      ",
    "title": "Agent-State Construction with Auxiliary Inputs",
    "date": "5 May, 2023",
    "authors": [
      "Ruo Yu Tao",
      " Adam White",
      " Marlos C. Machado"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2201.01811",
    "paper_id": "2201.01811",
    "abstract": "\n        We present CausalSim, a causal framework for unbiased trace-driven simulation. Current trace-driven simulators assume that the interventions being simulated (e.g., a new algorithm) would not affect the validity of the traces. However, real-world traces are often biased by the choices algorithms make during trace collection, and hence replaying traces under an intervention may lead to incorrect results. CausalSim addresses this challenge by learning a causal model of the system dynamics and latent factors capturing the underlying system conditions during trace collection. It learns these models using an initial randomized control trial (RCT) under a fixed set of algorithms, and then applies them to remove biases from trace data when simulating new algorithms.\n  Key to CausalSim is mapping unbiased trace-driven simulation to a tensor completion problem with extremely sparse observations. By exploiting a basic distributional invariance property present in RCT data, CausalSim enables a novel tensor completion method despite the sparsity of observations. Our extensive evaluation of CausalSim on both real and synthetic datasets, including more than ten months of real data from the Puffer video streaming system shows it improves simulation accuracy, reducing errors by 53% and 61% on average compared to expert-designed and supervised learning baselines. Moreover, CausalSim provides markedly different insights about ABR algorithms compared to the biased baseline simulator, which we validate with a real deployment.\n        \u25b3 Less\n      ",
    "title": "CausalSim: A Causal Framework for Unbiased Trace-Driven Simulation",
    "date": "5 May, 2023",
    "authors": [
      "Abdullah Alomar",
      " Pouya Hamadanian",
      " Arash Nasr-Esfahany",
      " Anish Agarwal",
      " Mohammad Alizadeh",
      " Devavrat Shah"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03874",
    "paper_id": "2305.03874",
    "abstract": "\n        We present a numerical framework for learning unknown stochastic dynamical systems using measurement data. Termed stochastic flow map learning (sFML), the new framework is an extension of flow map learning (FML) that was developed for learning deterministic dynamical systems. For learning stochastic systems, we define a stochastic flow map that is a superposition of two sub-flow maps: a deterministic sub-map and a stochastic sub-map. The stochastic training data are used to construct the deterministic sub-map first, followed by the stochastic sub-map. The deterministic sub-map takes the form of residual network (ResNet), similar to the work of FML for deterministic systems. For the stochastic sub-map, we employ a generative model, particularly generative adversarial networks (GANs) in this paper. The final constructed stochastic flow map then defines a stochastic evolution model that is a weak approximation, in term of distribution, of the unknown stochastic system. A comprehensive set of numerical examples are presented to demonstrate the flexibility and effectiveness of the proposed sFML method for various types of stochastic systems.\n        \u25b3 Less\n      ",
    "title": "Learning Stochastic Dynamical System via Flow Map Operator",
    "date": "5 May, 2023",
    "authors": [
      "Yuan Chen",
      " Dongbin Xiu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.05056",
    "paper_id": "2302.05056",
    "abstract": "\n        Neuromorphic computing, commonly understood as a computing approach built upon neurons, synapses, and their dynamics, as opposed to Boolean gates, is gaining large mindshare due to its direct application in solving current and future computing technological problems, such as smart sensing, smart devices, self-hosted and self-contained devices, artificial intelligence (AI) applications, etc. In a largely software-defined implementation of neuromorphic computing, it is possible to throw enormous computational power or optimize models and networks depending on the specific nature of the computational tasks. However, a hardware-based approach needs the identification of well-suited neuronal and synaptic models to obtain high functional and energy efficiency, which is a prime concern in size, weight, and power (SWaP) constrained environments. In this work, we perform a study on the characteristics of hardware neuron models (namely, inference errors, generalizability and robustness, practical implementability, and memory capacity) that have been proposed and demonstrated using a plethora of emerging nano-materials technology-based physical devices, to quantify the performance of such neurons on certain classes of problems that are of great importance in real-time signal processing like tasks in the context of reservoir computing. We find that the answer on which neuron to use for what applications depends on the particulars of the application requirements and constraints themselves, i.e., we need not only a hammer but all sorts of tools in our tool chest for high efficiency and quality neuromorphic computing.\n        \u25b3 Less\n      ",
    "title": "Choose your tools carefully: A Comparative Evaluation of Deterministic vs. Stochastic and Binary vs. Analog Neuron models for Implementing Emerging Computing Paradigms",
    "date": "5 May, 2023",
    "authors": [
      "Md Golam Morshed",
      " Samiran Ganguly",
      " Avik W. Ghosh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.08888",
    "paper_id": "2302.08888",
    "abstract": "\n        With the increasing amount of multimedia data on modern mobile systems and IoT infrastructures, harnessing these rich multimodal data without breaching user privacy becomes a critical issue. Federated learning (FL) serves as a privacy-conscious alternative to centralized machine learning. However, existing FL methods extended to multimodal data all rely on model aggregation on single modality level, which restrains the server and clients to have identical model architecture for each modality. This limits the global model in terms of both model complexity and data capacity, not to mention task diversity. In this work, we propose Contrastive Representation Ensemble and Aggregation for Multimodal FL (CreamFL), a multimodal federated learning framework that enables training larger server models from clients with heterogeneous model architectures and data modalities, while only communicating knowledge on public dataset. To achieve better multimodal representation fusion, we design a global-local cross-modal ensemble strategy to aggregate client representations. To mitigate local model drift caused by two unprecedented heterogeneous factors stemming from multimodal discrepancy (modality gap and task gap), we further propose two inter-modal and intra-modal contrasts to regularize local training, which complements information of the absent modality for uni-modal clients and regularizes local clients to head towards global consensus. Thorough evaluations and ablation studies on image-text retrieval and visual question answering tasks showcase the superiority of CreamFL over state-of-the-art FL methods and its practical value.\n        \u25b3 Less\n      ",
    "title": "Multimodal Federated Learning via Contrastive Representation Ensemble",
    "date": "5 May, 2023",
    "authors": [
      "Qiying Yu",
      " Yang Liu",
      " Yimu Wang",
      " Ke Xu",
      " Jingjing Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03900",
    "paper_id": "2305.03900",
    "abstract": "\n        Imbalance learning is a subfield of machine learning that focuses on learning tasks in the presence of class imbalance. Nearly all existing studies refer to class imbalance as a proportion imbalance, where the proportion of training samples in each class is not balanced. The ignorance of the proportion imbalance will result in unfairness between/among classes and poor generalization capability. Previous literature has presented numerous methods for either theoretical/empirical analysis or new methods for imbalance learning. This study presents a new taxonomy of class imbalance in machine learning with a broader scope. Four other types of imbalance, namely, variance, distance, neighborhood, and quality imbalances between/among classes, which may exist in machine learning tasks, are summarized. Two different levels of imbalance including global and local are also presented. Theoretical analysis is used to illustrate the significant impact of the new imbalance types on learning fairness. Moreover, our taxonomy and theoretical conclusions are used to analyze the shortcomings of several classical methods. As an example, we propose a new logit perturbation-based imbalance learning loss when proportion, variance, and distance imbalances exist simultaneously. Several classical losses become the special case of our proposed method. Meta learning is utilized to infer the hyper-parameters related to the three types of imbalance. Experimental results on several benchmark corpora validate the effectiveness of the proposed method.\n        \u25b3 Less\n      ",
    "title": "Rethinking Class Imbalance in Machine Learning",
    "date": "5 May, 2023",
    "authors": [
      "Ou Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03920",
    "paper_id": "2305.03920",
    "abstract": "\n        Among various region embedding methods, graph-based region relation learning models stand out, owing to their strong structure representation ability for encoding spatial correlations with graph neural networks. Despite their effectiveness, several key challenges have not been well addressed in existing methods: i) Data noise and missing are ubiquitous in many spatio-temporal scenarios due to a variety of factors. ii) Input spatio-temporal data (e.g., mobility traces) usually exhibits distribution heterogeneity across space and time. In such cases, current methods are vulnerable to the quality of the generated region graphs, which may lead to suboptimal performance. In this paper, we tackle the above challenges by exploring the Automated Spatio-Temporal graph contrastive learning paradigm (AutoST) over the heterogeneous region graph generated from multi-view data sources. Our \\model\\ framework is built upon a heterogeneous graph neural architecture to capture the multi-view region dependencies with respect to POI semantics, mobility flow patterns and geographical positions. To improve the robustness of our GNN encoder against data noise and distribution issues, we design an automated spatio-temporal augmentation scheme with a parameterized contrastive view generator. AutoST can adapt to the spatio-temporal heterogeneous graph with multi-view semantics well preserved. Extensive experiments for three downstream spatio-temporal mining tasks on several real-world datasets demonstrate the significant performance gain achieved by our \\model\\ over a variety of baselines. The code is publicly available at https://github.com/HKUDS/AutoST.\n        \u25b3 Less\n      ",
    "title": "Automated Spatio-Temporal Graph Contrastive Learning",
    "date": "5 May, 2023",
    "authors": [
      "Qianru Zhang",
      " Chao Huang",
      " Lianghao Xia",
      " Zheng Wang",
      " Zhonghang Li",
      " Siuming Yiu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03928",
    "paper_id": "2305.03928",
    "abstract": "\n        With the widespread attention and application of artificial intelligence (AI) and blockchain technologies, privacy protection techniques arising from their integration are of notable significance. In addition to protecting privacy of individuals, these techniques also guarantee security and dependability of data. This paper initially presents an overview of AI and blockchain, summarizing their combination along with derived privacy protection technologies. It then explores specific application scenarios in data encryption, de-identification, multi-tier distributed ledgers, and k-anonymity methods. Moreover, the paper evaluates five critical aspects of AI-blockchain-integration privacy protection systems, including authorization management, access control, data protection, network security, and scalability. Furthermore, it analyzes the deficiencies and their actual cause, offering corresponding suggestions. This research also classifies and summarizes privacy protection techniques based on AI-blockchain application scenarios and technical schemes. In conclusion, this paper outlines the future directions of privacy protection technologies emerging from AI and blockchain integration, including enhancing efficiency and security to achieve a more comprehensive privacy protection of privacy.\n        \u25b3 Less\n      ",
    "title": "An Overview of AI and Blockchain Integration for Privacy-Preserving",
    "date": "5 May, 2023",
    "authors": [
      "Zongwei Li",
      " Dechao Kong",
      " Yuanzheng Niu",
      " Hongli Peng",
      " Xiaoqi Li",
      " Wenkai Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2206.00738",
    "paper_id": "2206.00738",
    "abstract": "\n        Relational machine learning programs like those developed in Inductive Logic Programming (ILP) offer several advantages: (1) The ability to model complex relationships amongst data instances; (2) The use of domain-specific relations during model construction; and (3) The models constructed are human-readable, which is often one step closer to being human-understandable. However, these ILP-like methods have not been able to capitalise fully on the rapid hardware, software and algorithmic developments fuelling current developments in deep neural networks. In this paper, we treat relational features as functions and use the notion of generalised composition of functions to derive complex functions from simpler ones. We formulate the notion of a set of M\\text{M}-simple features in a mode language M\\text{M} and identify two composition operators (\u03c11\u03c1_1 and \u03c12\u03c1_2) from which all possible complex features can be derived. We use these results to implement a form of \"explainable neural network\" called Compositional Relational Machines, or CRMs, which are labelled directed-acyclic graphs. The vertex-label for any vertex jj in the CRM contains a feature-function fjf_j and a continuous activation function gjg_j. If jj is a \"non-input\" vertex, then fjf_j is the composition of features associated with vertices in the direct predecessors of jj. Our focus is on CRMs in which input vertices (those without any direct predecessors) all have M\\text{M}-simple features in their vertex-labels. We provide a randomised procedure for constructing and learning such CRMs. Using a notion of explanations based on the compositional structure of features in a CRM, we provide empirical evidence on synthetic data of the ability to identify appropriate explanations; and demonstrate the use of CRMs as 'explanation machines' for black-box models that do not provide explanations for their predictions.\n        \u25b3 Less\n      ",
    "title": "Composition of Relational Features with an Application to Explaining Black-Box Predictors",
    "date": "5 May, 2023",
    "authors": [
      "Ashwin Srinivasan",
      " A Baskar",
      " Tirtharaj Dash",
      " Devanshu Shah"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03937",
    "paper_id": "2305.03937",
    "abstract": "\n        Prompt tuning is one of the successful approaches for parameter-efficient tuning of pre-trained language models. Despite being arguably the most parameter-efficient (tuned soft prompts constitute <0.1% of total parameters), it typically performs worse than other efficient tuning methods and is quite sensitive to hyper-parameters. In this work, we introduce Residual Prompt Tuning - a simple and efficient method that significantly improves the performance and stability of prompt tuning. We propose to reparameterize soft prompt embeddings using a shallow network with a residual connection. Our experiments show that Residual Prompt Tuning significantly outperforms prompt tuning on SuperGLUE benchmark. Notably, our method reaches +7 points improvement over prompt tuning with T5-Base and allows to reduce the prompt length by 10x without hurting performance. In addition, we show that our approach is robust to the choice of learning rate and prompt initialization, and is effective in few-shot settings.\n        \u25b3 Less\n      ",
    "title": "Residual Prompt Tuning: Improving Prompt Tuning with Residual Reparameterization",
    "date": "5 May, 2023",
    "authors": [
      "Anastasia Razdaibiedina",
      " Yuning Mao",
      " Rui Hou",
      " Madian Khabsa",
      " Mike Lewis",
      " Jimmy Ba",
      " Amjad Almahairi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.09245",
    "paper_id": "2210.09245",
    "abstract": "\n        3D grasp synthesis generates grasping poses given an input object. Existing works tackle the problem by learning a direct mapping from objects to the distributions of grasping poses. However, because the physical contact is sensitive to small changes in pose, the high-nonlinear mapping between 3D object representation to valid poses is considerably non-smooth, leading to poor generation efficiency and restricted generality. To tackle the challenge, we introduce an intermediate variable for grasp contact areas to constrain the grasp generation; in other words, we factorize the mapping into two sequential stages by assuming that grasping poses are fully constrained given contact maps: 1) we first learn contact map distributions to generate the potential contact maps for grasps; 2) then learn a mapping from the contact maps to the grasping poses. Further, we propose a penetration-aware optimization with the generated contacts as a consistency constraint for grasp refinement. Extensive validations on two public datasets show that our method outperforms state-of-the-art methods regarding grasp generation on various metrics.\n        \u25b3 Less\n      ",
    "title": "Contact2Grasp: 3D Grasp Synthesis via Hand-Object Contact Constraint",
    "date": "5 May, 2023",
    "authors": [
      "Haoming Li",
      " Xinzhuo Lin",
      " Yang Zhou",
      " Xiang Li",
      " Yuchi Huo",
      " Jiming Chen",
      " Qi Ye"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03987",
    "paper_id": "2305.03987",
    "abstract": "\n        Policy learning (PL) is a module of a task-oriented dialogue system that trains an agent to make actions in each dialogue turn. Imitating human action is a fundamental problem of PL. However, both supervised learning (SL) and reinforcement learning (RL) frameworks cannot imitate humans well. Training RL models require online interactions with user simulators, while simulating complex human policy is hard. Performances of SL-based models are restricted because of the covariate shift problem. Specifically, a dialogue is a sequential decision-making process where slight differences in current utterances and actions will cause significant differences in subsequent utterances. Therefore, the generalize ability of SL models is restricted because statistical characteristics of training and testing dialogue data gradually become different. This study proposed an offline imitation learning model that learns policy from real dialogue datasets and does not require user simulators. It also utilizes state transition information, which alleviates the influence of the covariate shift problem. We introduced a regularization trick to make our model can be effectively optimized. We investigated the performance of our model on four independent public dialogue datasets. The experimental result showed that our model performed better in the action prediction task.\n        \u25b3 Less\n      ",
    "title": "Replicating Complex Dialogue Policy of Humans via Offline Imitation Learning with Supervised Regularization",
    "date": "5 May, 2023",
    "authors": [
      "Zhoujian Sun",
      " Chenyang Zhao",
      " Zhengxing Huang",
      " Nai Ding"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.08639",
    "paper_id": "2211.08639",
    "abstract": "\n        Image harmonization is a critical task in computer vision, which aims to adjust the foreground to make it compatible with the background. Recent works mainly focus on using global transformations (i.e., normalization and color curve rendering) to achieve visual consistency. However, these models ignore local visual consistency and their huge model sizes limit their harmonization ability on edge devices. In this paper, we propose a hierarchical dynamic network (HDNet) to adapt features from local to global view for better feature transformation in efficient image harmonization. Inspired by the success of various dynamic models, local dynamic (LD) module and mask-aware global dynamic (MGD) module are proposed in this paper. Specifically, LD matches local representations between the foreground and background regions based on semantic similarities, then adaptively adjust every foreground local representation according to the appearance of its KK-nearest neighbor background regions. In this way, LD can produce more realistic images at a more fine-grained level, and simultaneously enjoy the characteristic of semantic alignment. The MGD effectively applies distinct convolution to the foreground and background, learning the representations of foreground and background regions as well as their correlations to the global harmonization, facilitating local visual consistency for the images much more efficiently. Experimental results demonstrate that the proposed HDNet significantly reduces the total model parameters by more than 80\\% compared to previous methods, while still attaining state-of-the-art performance on the popular iHarmony4 dataset. Notably, the HDNet achieves a 4\\% improvement in PSNR and a 19\\% reduction in MSE compared to the prior state-of-the-art methods.\n        \u25b3 Less\n      ",
    "title": "Hierarchical Dynamic Image Harmonization",
    "date": "5 May, 2023",
    "authors": [
      "Haoxing Chen",
      " Zhangxuan Gu",
      " Yaohui Li",
      " Jun Lan",
      " Changhua Meng",
      " Weiqiang Wang",
      " Huaxiong Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.12578",
    "paper_id": "2211.12578",
    "abstract": "\n        Federated Learning (FL) is an emerging domain in the broader context of artificial intelligence research. Methodologies pertaining to FL assume distributed model training, consisting of a collection of clients and a server, with the main goal of achieving optimal global model with restrictions on data sharing due to privacy concerns. It is worth highlighting that the diverse existing literature in FL mostly assume stationary data generation processes; such an assumption is unrealistic in real-world conditions where concept drift occurs due to, for instance, seasonal or period observations, faults in sensor measurements. In this paper, we introduce a multiscale algorithmic framework which combines theoretical guarantees of \\textit{FedAvg} and \\textit{FedOMD} algorithms in near stationary settings with a non-stationary detection and adaptation technique to ameliorate FL generalization performance in the presence of concept drifts. We present a multi-scale algorithmic framework leading to $\\Tilde{\\mathcal{O}} ( \\min \\{ \\sqrt{LT} , \u0394^{\\frac{1}{3}}T^{\\frac{2}{3}} + \\sqrt{T} \\})$\\Tilde{\\mathcal{O}} ( \\min \\{ \\sqrt{LT} , \u0394^{\\frac{1}{3}}T^{\\frac{2}{3}} + \\sqrt{T} \\}) \\textit{dynamic regret} for TT rounds with an underlying general convex loss function, where LL is the number of times non-stationary drifts occurred and \u0394\u0394 is the cumulative magnitude of drift experienced within TT rounds.\n        \u25b3 Less\n      ",
    "title": "Online Federated Learning via Non-Stationary Detection and Adaptation amidst Concept Drift",
    "date": "5 May, 2023",
    "authors": [
      "Bhargav Ganguly",
      " Vaneet Aggarwal"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04004",
    "paper_id": "2305.04004",
    "abstract": "\n        In this paper, we present a simple framework of skill transfer learning for robotic ultrasound-guidance procedures. We briefly review challenges in skill transfer learning for robotic ultrasound-guidance procedures. We then identify the need of appropriate sampling techniques, computationally efficient neural networks models that lead to the proposal of a simple framework of skill transfer learning for real-time applications in robotic ultrasound-guidance procedures. We present pilot experiments from two participants (one experienced clinician and one non-clinician) looking for an optimal scanning plane of the four-chamber cardiac view from a fetal phantom. We analysed ultrasound image frames, time series of texture image features and quaternions and found that the experienced clinician performed the procedure in a quicker and smoother way compared to lengthy and non-constant movements from non-clinicians. For future work, we pointed out the need of pruned and quantised neural network models for real-time applications in robotic ultrasound-guidance procedure. The resources to reproduce this work are available at \\url{https://github.com/mxochicale/rami-icra2023}.\n        \u25b3 Less\n      ",
    "title": "Towards a Simple Framework of Skill Transfer Learning for Robotic Ultrasound-guidance Procedures",
    "date": "5 May, 2023",
    "authors": [
      "Tsz Yan Leung",
      " Miguel Xochicale"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04034",
    "paper_id": "2305.04034",
    "abstract": "\n        Answering complex queries on knowledge graphs is important but particularly challenging because of the data incompleteness. Query embedding methods address this issue by learning-based models and simulating logical reasoning with set operators. Previous works focus on specific forms of embeddings, but scoring functions between embeddings are underexplored. In contrast to existing scoring functions motivated by local comparison or global transport, this work investigates the local and global trade-off with unbalanced optimal transport theory. Specifically, we embed sets as bounded measures in $\\real$\\real endowed with a scoring function motivated by the Wasserstein-Fisher-Rao metric. Such a design also facilitates closed-form set operators in the embedding space. Moreover, we introduce a convolution-based algorithm for linear time computation and a block-diagonal kernel to enforce the trade-off. Results show that WFRE can outperform existing query embedding methods on standard datasets, evaluation sets with combinatorially complex queries, and hierarchical knowledge graphs. Ablation study shows that finding a better local and global trade-off is essential for performance improvement.\n        \u25b3 Less\n      ",
    "title": "Wasserstein-Fisher-Rao Embedding: Logical Query Embeddings with Local Comparison and Global Transport",
    "date": "5 May, 2023",
    "authors": [
      "Zihao Wang",
      " Weizhi Fei",
      " Hang Yin",
      " Yangqiu Song",
      " Ginny Y. Wong",
      " Simon See"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04039",
    "paper_id": "2305.04039",
    "abstract": "\n        In this paper, we propose a simple yet efficient approach based on prompt engineering that leverages the large language model itself to optimize its answers without relying on auxiliary models. We introduce an iterative self-evaluating optimization mechanism, with the potential for improved output quality as iterations progress, removing the need for manual intervention. The experiment's findings indicate that utilizing our response refinement framework on the GPT-3.5 model yields results that are on par with, or even surpass, those generated by the cutting-edge GPT-4 model. Detailed implementation strategies and illustrative examples are provided to demonstrate the superiority of our proposed solution.\n        \u25b3 Less\n      ",
    "title": "Refining the Responses of LLMs by Themselves",
    "date": "5 May, 2023",
    "authors": [
      "Tianqiang Yan",
      " Tiansheng Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00233",
    "paper_id": "2305.00233",
    "abstract": "\n        Nowadays, machine learning (ML) is being used in software systems with multiple application fields, from medicine to software engineering (SE). On the one hand, the popularity of ML in the industry can be seen in the statistics showing its growth and adoption. On the other hand, its popularity can also be seen in research, particularly in SE, where not only have multiple studies been published in SE conferences and journals but also in the multiple workshops and co-located conferences in software engineering conferences. At the same time, researchers and practitioners have shown that machine learning has some particular challenges and pitfalls. In particular, research has shown that ML-enabled systems have a different development process than traditional SE, which also describes some of the challenges of ML applications. In order to mitigate some of the identified challenges and pitfalls, white and gray literature has proposed a set of recommendations based on their own experiences and focused on their domain (e.g., biomechanics), but for the best of our knowledge, there is no guideline focused on the SE community. This thesis aims to reduce this gap by answering research questions that help to understand the practices used and discussed by practitioners and researchers in the SE community by analyzing possible sources of practices such as question and answer communities and also previous research studies to present a set of practices with an SE perspective.\n        \u25b3 Less\n      ",
    "title": "Towards machine learning guided by best practices",
    "date": "5 May, 2023",
    "authors": [
      "Anamaria Mojica-Hanke"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.02730",
    "paper_id": "2302.02730",
    "abstract": "\n        In this paper, we study the sampling problem for first-order logic proposed recently by Wang et al. -- how to efficiently sample a model of a given first-order sentence on a finite domain? We extend their result for the universally-quantified subfragment of two-variable logic FO2\\mathbf{FO}^2 (UFO2\\mathbf{UFO}^2) to the entire fragment of FO2\\mathbf{FO}^2. Specifically, we prove the domain-liftability under sampling of FO2\\mathbf{FO}^2, meaning that there exists a sampling algorithm for FO2\\mathbf{FO}^2 that runs in time polynomial in the domain size. We then further show that this result continues to hold even in the presence of counting constraints, such as \u2200x\u2203=ky:\u03c6(x,y)\\forall x\\exists_{=k} y: \\varphi(x,y) and \u2203=kx\u2200y:\u03c6(x,y)\\exists_{=k} x\\forall y: \\varphi(x,y), for some quantifier-free formula \u03c6(x,y)\\varphi(x,y). Our proposed method is constructive, and the resulting sampling algorithms have potential applications in various areas, including the uniform generation of combinatorial structures and sampling in statistical-relational models such as Markov logic networks and probabilistic logic programs.\n        \u25b3 Less\n      ",
    "title": "On Exact Sampling in the Two-Variable Fragment of First-Order Logic",
    "date": "5 May, 2023",
    "authors": [
      "Yuanhong Wang",
      " Juhua Pu",
      " Yuyi Wang",
      " Ond\u0159ej Ku\u017eelka"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04055",
    "paper_id": "2305.04055",
    "abstract": "\n        Ontologies play a critical role in Semantic Web technologies by providing a structured and standardized way to represent knowledge and enabling machines to understand the meaning of data. Several taxonomies and ontologies have been generated, but individuals target one domain, and only some of those have been found expensive in time and manual effort. Also, they need more coverage of unconventional topics representing a more holistic and comprehensive view of the knowledge landscape and interdisciplinary collaborations. Thus, there needs to be an ontology covering Science and Technology and facilitate multidisciplinary research by connecting topics from different fields and domains that may be related or have commonalities. To address these issues, we present an automatic Science and Technology Ontology (S&TO) that covers unconventional topics in different science and technology domains. The proposed S&TO can promote the discovery of new research areas and collaborations across disciplines. The ontology is constructed by applying BERTopic to a dataset of 393,991 scientific articles collected from Semantic Scholar from October 2021 to August 2022, covering four fields of science. Currently, S&TO includes 5,153 topics and 13,155 semantic relations. S&TO model can be updated by running BERTopic on more recent datasets\n        \u25b3 Less\n      ",
    "title": "Science and Technology Ontology: A Taxonomy of Emerging Topics",
    "date": "5 May, 2023",
    "authors": [
      "Mahender Kumar",
      " Ruby Rani",
      " Mirko Botarelli",
      " Gregory Epiophaniou",
      " Carsten Maple"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04062",
    "paper_id": "2305.04062",
    "abstract": "\n        As artificial intelligence (AI) continues to permeate various domains, concerns surrounding trust and transparency in AI-driven inference and training processes have emerged, particularly with respect to potential biases and traceability challenges. Decentralized solutions such as blockchain have been proposed to tackle these issues, but they often struggle when dealing with large-scale models, leading to time-consuming inference and inefficient training verification. To overcome these limitations, we introduce BRAIN, a Blockchain-based Reliable AI Network, a novel platform specifically designed to ensure reliable inference and training of large models. BRAIN harnesses a unique two-phase transaction mechanism, allowing real-time processing via pipelining by separating request and response transactions. Each randomly-selected inference committee commits and reveals the inference results, and upon reaching an agreement through a smart contract, then the requested operation is executed using the consensus result. Additionally, BRAIN carries out training by employing a randomly-selected training committee. They submit commit and reveal transactions along with their respective scores, enabling local model aggregation based on the median value of the scores. Experimental results demonstrate that BRAIN delivers considerably higher inference throughput at reasonable gas fees. In particular, BRAIN's tasks-per-second performance is 454.4293 times greater than that of a naive single-phase implementation.\n        \u25b3 Less\n      ",
    "title": "A Blockchain-based Platform for Reliable Inference and Training of Large-Scale Models",
    "date": "5 May, 2023",
    "authors": [
      "Sanghyeon Park",
      " Junmo Lee",
      " Soo-Mook Moon"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.03144",
    "paper_id": "2304.03144",
    "abstract": "\n        The rapid and accurate identification of bot accounts in online social networks is an ongoing challenge. In this paper, we propose BOTTRINET, a unified embedding framework that leverages the textual content posted by accounts to detect bots. Our approach is based on the premise that account personalities and habits can be revealed through their contextual content. To achieve this, we designed a triplet network that refines raw embeddings using metric learning techniques. The BOTTRINET framework produces word, sentence, and account embeddings, which we evaluate on a real-world dataset, CRESCI2017, consisting of three bot account categories and five bot sample sets. Our approach achieves state-of-the-art performance on two content-intensive bot sets, with an average accuracy of 98.34% and f1score of 97.99%. Moreover, our method makes a significant breakthrough on four content-less bot sets, with an average accuracy improvement of 11.52% and an average f1score increase of 16.70%. Our contribution is twofold: First, we propose a unified and effective framework that combines various embeddings for bot detection. Second, we demonstrate that metric learning techniques can be applied in this context to refine raw embeddings and improve classification performance. Our approach outperforms prior works and sets a new standard for bot detection in social networks.\n        \u25b3 Less\n      ",
    "title": "BotTriNet: A Unified and Efficient Embedding for Social Bots Detection via Metric Learning",
    "date": "5 May, 2023",
    "authors": [
      "Jun Wu",
      " Xuesong Ye",
      " Yanyuet Man"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04095",
    "paper_id": "2305.04095",
    "abstract": "\n        Federated Learning (FL) is a widely adopted privacy-preserving machine learning approach where private data remains local, enabling secure computations and the exchange of local model gradients between local clients and third-party parameter servers. However, recent findings reveal that privacy may be compromised and sensitive information potentially recovered from shared gradients. In this study, we offer detailed analysis and a novel perspective on understanding the gradient leakage problem. These theoretical works lead to a new gradient leakage defense technique that secures arbitrary model architectures using a private key-lock module. Only the locked gradient is transmitted to the parameter server for global model aggregation. Our proposed learning method is resistant to gradient leakage attacks, and the key-lock module is designed and trained to ensure that, without the private information of the key-lock module: a) reconstructing private training data from the shared gradient is infeasible; and b) the global model's inference performance is significantly compromised. We discuss the theoretical underpinnings of why gradients can leak private information and provide theoretical proof of our method's effectiveness. We conducted extensive empirical evaluations with a total of forty-four models on several popular benchmarks, demonstrating the robustness of our proposed approach in both maintaining model performance and defending against gradient leakage attacks.\n        \u25b3 Less\n      ",
    "title": "Gradient Leakage Defense with Key-Lock Module for Federated Learning",
    "date": "5 May, 2023",
    "authors": [
      "Hanchi Ren",
      " Jingjing Deng",
      " Xianghua Xie",
      " Xiaoke Ma",
      " Jianfeng Ma"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04102",
    "paper_id": "2305.04102",
    "abstract": "\n        Additive manufacturing (AM) offers numerous benefits, such as manufacturing complex and customised designs quickly and cost-effectively, reducing material waste, and enabling on-demand production. However, several security challenges are associated with AM, making it increasingly attractive to attackers ranging from individual hackers to organised criminal gangs and nation-state actors. This paper addresses the cyber risk in AM to attackers by proposing a novel semantic-based threat prioritisation system for identifying, extracting and ranking indicators of compromise (IOC). The system leverages the heterogeneous information networks (HINs) that automatically extract high-level IOCs from multi-source threat text and identifies semantic relations among the IOCs. It models IOCs with a HIN comprising different meta-paths and meta-graphs to depict semantic relations among diverse IOCs. We introduce a domain-specific recogniser that identifies IOCs in three domains: organisation-specific, regional source-specific, and regional target-specific. A threat assessment uses similarity measures based on meta-paths and meta-graphs to assess semantic relations among IOCs. It prioritises IOCs by measuring their severity based on the frequency of attacks, IOC lifetime, and exploited vulnerabilities in each domain.\n        \u25b3 Less\n      ",
    "title": "Leveraging Semantic Relationships to Prioritise Indicators of Compromise in Additive Manufacturing Systems",
    "date": "5 May, 2023",
    "authors": [
      "Mahender Kumar",
      " Gregory Epiphaniou",
      " Carsten Maple"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.03904",
    "paper_id": "2301.03904",
    "abstract": "\n        The increasing interest in TinyML, i.e., near-sensor machine learning on power budgets of a few tens of mW, is currently pushing toward enabling TinyML-class training as opposed to inference only. Current training algorithms, based on various forms of error and gradient backpropagation, rely on floating-point matrix operations to meet the precision and dynamic range requirements. So far, the energy and power cost of these operations has been considered too high for TinyML scenarios. This paper addresses the open challenge of near-sensor training on a few mW power budget and presents RedMulE - Reduced-Precision Matrix Multiplication Engine, a low-power specialized accelerator conceived for multi-precision floating-point General Matrix-Matrix Operations (GEMM-Ops) acceleration, supporting FP16, as well as hybrid FP8 formats, with {sign, exponent, mantissa}=({1,4,3}, {1,5,2}). We integrate RedMule into a Parallel Ultra-Low-Power (PULP) cluster containing eight energy-efficient RISC-V cores sharing a tightly-coupled data memory and implement the resulting system in a 22 nm technology. At its best efficiency point (@ 470 MHz, 0.65 V), the RedMulE-augmented PULP cluster achieves 755 GFLOPS/W and 920 GFLOPS/W during regular General Matrix-Matrix Multiplication (GEMM), and up to 1.19 TFLOPS/W and 1.67 TFLOPS/W when executing GEMM-Ops, respectively, for FP16 and FP8 input/output tensors. In its best performance point (@ 613 MHz, 0.8 V), RedMulE achieves up to 58.5 GFLOPS and 117 GFLOPS for FP16 and FP8, respectively, with 99.4% utilization of the array of Computing Elements and consuming less than 60 mW on average, thus enabling on-device training of deep learning models in TinyML application scenarios while retaining the flexibility to tackle other classes of common linear algebra problems efficiently.\n        \u25b3 Less\n      ",
    "title": "RedMule: A Mixed-Precision Matrix-Matrix Operation Engine for Flexible and Energy-Efficient On-Chip Linear Algebra and TinyML Training Acceleration",
    "date": "6 May, 2023",
    "authors": [
      "Yvan Tortorella",
      " Luca Bertaccini",
      " Luca Benini",
      " Davide Rossi",
      " Francesco Conti"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00079",
    "paper_id": "2305.00079",
    "abstract": "\n        In this work, we present a methodology to shape a fisheye-specific representation space that reflects the interaction between distortion and semantic context present in this data modality. Fisheye data has the wider field of view advantage over other types of cameras, but this comes at the expense of high radial distortion. As a result, objects further from the center exhibit deformations that make it difficult for a model to identify their semantic context. While previous work has attempted architectural and training augmentation changes to alleviate this effect, no work has attempted to guide the model towards learning a representation space that reflects this interaction between distortion and semantic context inherent to fisheye data. We introduce an approach to exploit this relationship by first extracting distortion class labels based on an object's distance from the center of the image. We then shape a backbone's representation space with a weighted contrastive loss that constrains objects of the same semantic class and distortion class to be close to each other within a lower dimensional embedding space. This backbone trained with both semantic and distortion information is then fine-tuned within an object detection setting to empirically evaluate the quality of the learnt representation. We show this method leads to performance improvements by as much as 1.1% mean average precision over standard object detection strategies and .6% improvement over other state of the art representation learning approaches.\n        \u25b3 Less\n      ",
    "title": "Exploiting the Distortion-Semantic Interaction in Fisheye Data",
    "date": "6 May, 2023",
    "authors": [
      "Kiran Kokilepersaud",
      " Mohit Prabhushankar",
      " Yavuz Yarici",
      " Ghassan AlRegib",
      " Armin Parchami"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04147",
    "paper_id": "2305.04147",
    "abstract": "\n        Mixed-initiative dialogue tasks involve repeated exchanges of information and conversational control. Conversational agents gain control by generating responses that follow particular dialogue intents or strategies, prescribed by a policy planner. The standard approach has been fine-tuning pre-trained language models to perform generation conditioned on these intents. However, these supervised generation models are limited by the cost and quality of data annotation. We instead prompt large language models as a drop-in replacement to fine-tuning on conditional generation. We formalize prompt construction for controllable mixed-initiative dialogue. Our findings show improvements over fine-tuning and ground truth responses according to human evaluation and automatic metrics for two tasks: PersuasionForGood and Emotional Support Conversations.\n        \u25b3 Less\n      ",
    "title": "Controllable Mixed-Initiative Dialogue Generation through Prompting",
    "date": "6 May, 2023",
    "authors": [
      "Maximillian Chen",
      " Xiao Yu",
      " Weiyan Shi",
      " Urvi Awasthi",
      " Zhou Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.02162",
    "paper_id": "2302.02162",
    "abstract": "\n        Explainable Artificial Intelligence (XAI) encompasses a range of techniques and procedures aimed at elucidating the decision-making processes of AI models. While XAI is valuable in understanding the reasoning behind AI models, the data used for such revelations poses potential security and privacy vulnerabilities. Existing literature has identified privacy risks targeting machine learning models, including membership inference, model inversion, and model extraction attacks. Depending on the settings and parties involved, such attacks may target either the model itself or the training data used to create the model.\n  We have identified that tools providing XAI can particularly increase the vulnerability of model extraction attacks, which can be a significant issue when the owner of an AI model prefers to provide only black-box access rather than sharing the model parameters and architecture with other parties. To explore this privacy risk, we propose AUTOLYCUS, a model extraction attack that leverages the explanations provided by popular explainable AI tools. We particularly focus on white-box machine learning (ML) models such as decision trees and logistic regression models.\n  We have evaluated the performance of AUTOLYCUS on 5 machine learning datasets, in terms of the surrogate model's accuracy and its similarity to the target model. We observe that the proposed attack is highly effective; it requires up to 60x fewer queries to the target model compared to the state-of-the-art attack, while providing comparable accuracy and similarity. We first validate the performance of the proposed algorithm on decision trees, and then show its performance on logistic regression models as an indicator that the proposed algorithm performs well on white-box ML models in general. Finally, we show that the existing countermeasures remain ineffective for the proposed attack.\n        \u25b3 Less\n      ",
    "title": "AUTOLYCUS: Exploiting Explainable AI (XAI) for Model Extraction Attacks against White-Box Models",
    "date": "6 May, 2023",
    "authors": [
      "Abdullah Caglar Oksuz",
      " Anisa Halimi",
      " Erman Ayday"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04154",
    "paper_id": "2305.04154",
    "abstract": "\n        We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of \"smart memory\" that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations.\n  We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone's knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of \"if-then\" production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.\n        \u25b3 Less\n      ",
    "title": "Score: A Rule Engine for the Scone Knowledge Base System",
    "date": "6 May, 2023",
    "authors": [
      "Jeffrey Chen",
      " Scott E. Fahlman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05377",
    "paper_id": "2305.05377",
    "abstract": "\n        The research creates a professional certification survey to test large language models and evaluate their employable skills. It compares the performance of two AI models, GPT-3 and Turbo-GPT3.5, on a benchmark dataset of 1149 professional certifications, emphasizing vocational readiness rather than academic performance. GPT-3 achieved a passing score (>70% correct) in 39% of the professional certifications without fine-tuning or exam preparation. The models demonstrated qualifications in various computer-related fields, such as cloud and virtualization, business analytics, cybersecurity, network setup and repair, and data analytics. Turbo-GPT3.5 scored 100% on the valuable Offensive Security Certified Professional (OSCP) exam. The models also displayed competence in other professional domains, including nursing, licensed counseling, pharmacy, and teaching. Turbo-GPT3.5 passed the Financial Industry Regulatory Authority (FINRA) Series 6 exam with a 70% grade without preparation. Interestingly, Turbo-GPT3.5 performed well on customer service tasks, suggesting potential applications in human augmentation for chatbots in call centers and routine advice services. The models also score well on sensory and experience-based tests such as wine sommelier, beer taster, emotional quotient, and body language reader. The OpenAI model improvement from Babbage to Turbo resulted in a median 60% better-graded performance in less than a few years. This progress suggests that focusing on the latest model's shortcomings could lead to a highly performant AI capable of mastering the most demanding professional certifications. We open-source the benchmark to expand the range of testable professional skills as the models improve or gain emergent capabilities.\n        \u25b3 Less\n      ",
    "title": "Professional Certification Benchmark Dataset: The First 500 Jobs For Large Language Models",
    "date": "6 May, 2023",
    "authors": [
      "David Noever",
      " Matt Ciolino"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.00943",
    "paper_id": "2210.00943",
    "abstract": "\n        Recently, there has been increasing interest in building efficient audio neural networks for on-device scenarios. Most existing approaches are designed to reduce the size of audio neural networks using methods such as model pruning. In this work, we show that instead of reducing model size using complex methods, eliminating the temporal redundancy in the input audio features (e.g., mel-spectrogram) could be an effective approach for efficient audio classification. To do so, we proposed a family of simple pooling front-ends (SimPFs) which use simple non-parametric pooling operations to reduce the redundant information within the mel-spectrogram. We perform extensive experiments on four audio classification tasks to evaluate the performance of SimPFs. Experimental results show that SimPFs can achieve a reduction in more than half of the number of floating point operations (FLOPs) for off-the-shelf audio neural networks, with negligible degradation or even some improvements in audio classification performance.\n        \u25b3 Less\n      ",
    "title": "Simple Pooling Front-ends For Efficient Audio Classification",
    "date": "6 May, 2023",
    "authors": [
      "Xubo Liu",
      " Haohe Liu",
      " Qiuqiang Kong",
      " Xinhao Mei",
      " Mark D. Plumbley",
      " Wenwu Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.03747",
    "paper_id": "2205.03747",
    "abstract": "\n        Boolean MaxSAT, as well as generalized formulations such as Min-MaxSAT and Max-hybrid-SAT, are fundamental optimization problems in Boolean reasoning. Existing methods for MaxSAT have been successful in solving benchmarks in CNF format. They lack, however, the ability to handle 1) (non-CNF) hybrid constraints, such as XORs and 2) generalized MaxSAT problems natively. To address this issue, we propose a novel dynamic-programming approach for solving generalized MaxSAT problems with hybrid constraints -- called \\emph{Dynamic-Programming-MaxSAT} or DPMS for short -- based on Algebraic Decision Diagrams (ADDs). With the power of ADDs and the (graded) project-join-tree builder, our versatile framework admits many generalizations of CNF-MaxSAT, such as MaxSAT, Min-MaxSAT, and MinSAT with hybrid constraints. Moreover, DPMS scales provably well on instances with low width. Empirical results indicate that DPMS is able to solve certain problems quickly, where other algorithms based on various techniques all fail. Hence, DPMS is a promising framework and opens a new line of research that invites more investigation in the future.\n        \u25b3 Less\n      ",
    "title": "DPMS: An ADD-Based Symbolic Approach for Generalized MaxSAT Solving",
    "date": "6 May, 2023",
    "authors": [
      "Anastasios Kyrillidis",
      " Moshe Y. Vardi",
      " Zhiwei Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04177",
    "paper_id": "2305.04177",
    "abstract": "\n        Learning semantically meaningful representations from scientific documents can facilitate academic literature search and improve performance of recommendation systems. Pre-trained language models have been shown to learn rich textual representations, yet they cannot provide powerful document-level representations for scientific articles. We propose MIReAD, a simple method that learns high-quality representations of scientific papers by fine-tuning transformer model to predict the target journal class based on the abstract. We train MIReAD on more than 500,000 PubMed and arXiv abstracts across over 2,000 journal classes. We show that MIReAD produces representations that can be used for similar papers retrieval, topic categorization and literature search. Our proposed approach outperforms six existing models for representation learning on scientific documents across four evaluation standards.\n        \u25b3 Less\n      ",
    "title": "MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents",
    "date": "6 May, 2023",
    "authors": [
      "Anastasia Razdaibiedina",
      " Alexander Brechalov"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04181",
    "paper_id": "2305.04181",
    "abstract": "\n        Open Information Extraction (OIE) aims to extract factual relational tuples from open-domain sentences. Downstream tasks use the extracted OIE tuples as facts, without examining the certainty of these facts. However, uncertainty/speculation is a common linguistic phenomenon. Existing studies on speculation detection are defined at sentence level, but even if a sentence is determined to be speculative, not all tuples extracted from it may be speculative. In this paper, we propose to study speculations in OIE and aim to determine whether an extracted tuple is speculative. We formally define the research problem of tuple-level speculation detection and conduct a detailed data analysis on the LSOIE dataset which contains labels for speculative tuples. Lastly, we propose a baseline model OIE-Spec for this new research task.\n        \u25b3 Less\n      ",
    "title": "Shall We Trust All Relational Tuples by Open Information Extraction? A Study on Speculation Detection",
    "date": "6 May, 2023",
    "authors": [
      "Kuicai Dong",
      " Aixin Sun",
      " Jung-Jae Kim",
      " Xiaoli Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.07098",
    "paper_id": "2211.07098",
    "abstract": "\n        Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete. To solve this problem, we propose a web-based question answering system system with multimodal fusion of unstructured and structured information, to fill in missing information for knowledge bases. To utilize unstructured information from the Web for knowledge base completion, we design a web-based question answering system using multimodal features and question templates to extract missing facts, which can achieve good performance with very few questions. To help improve extraction quality, the question answering system employs structured information from knowledge bases, such as entity types and entity-to-entity relatedness.\n        \u25b3 Less\n      ",
    "title": "Knowledge Base Completion using Web-Based Question Answering and Multimodal Fusion",
    "date": "6 May, 2023",
    "authors": [
      "Yang Peng",
      " Daisy Zhe Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02412",
    "paper_id": "2305.02412",
    "abstract": "\n        Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.\n        \u25b3 Less\n      ",
    "title": "Plan, Eliminate, and Track -- Language Models are Good Teachers for Embodied Agents",
    "date": "6 May, 2023",
    "authors": [
      "Yue Wu",
      " So Yeon Min",
      " Yonatan Bisk",
      " Ruslan Salakhutdinov",
      " Amos Azaria",
      " Yuanzhi Li",
      " Tom Mitchell",
      " Shrimai Prabhumoye"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.02578",
    "paper_id": "2211.02578",
    "abstract": "\n        Camera images are ubiquitous in machine learning research. They also play a central role in the delivery of important services spanning medicine and environmental surveying. However, the application of machine learning models in these domains has been limited because of robustness concerns. A primary failure mode are performance drops due to differences between the training and deployment data. While there are methods to prospectively validate the robustness of machine learning models to such dataset drifts, existing approaches do not account for explicit models of the primary object of interest: the data. This limits our ability to study and understand the relationship between data generation and downstream machine learning model performance in a physically accurate manner. In this study, we demonstrate how to overcome this limitation by pairing traditional machine learning with physical optics to obtain explicit and differentiable data models. We demonstrate how such data models can be constructed for image data and used to control downstream machine learning model performance related to dataset drift. The findings are distilled into three applications. First, drift synthesis enables the controlled generation of physically faithful drift test cases to power model selection and targeted generalization. Second, the gradient connection between machine learning task model and data model allows advanced, precise tolerancing of task model sensitivity to changes in the data generation. These drift forensics can be used to precisely specify the acceptable data environments in which a task model may be run. Third, drift optimization opens up the possibility to create drifts that can help the task model learn better faster, effectively optimizing the data generating process itself. A guide to access the open code and datasets is available at https://github.com/aiaudit-org/raw2logit.\n        \u25b3 Less\n      ",
    "title": "Data Models for Dataset Drift Controls in Machine Learning With Optical Images",
    "date": "6 May, 2023",
    "authors": [
      "Luis Oala",
      " Marco Aversa",
      " Gabriel Nobis",
      " Kurt Willis",
      " Yoan Neuenschwander",
      " Mich\u00e8le Buck",
      " Christian Matek",
      " Jerome Extermann",
      " Enrico Pomarico",
      " Wojciech Samek",
      " Roderick Murray-Smith",
      " Christoph Clausen",
      " Bruno Sanguinetti"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.15073",
    "paper_id": "2210.15073",
    "abstract": "\n        Machine learning with hierarchical quantum circuits, usually referred to as Quantum Convolutional Neural Networks (QCNNs), is a promising prospect for near-term quantum computing. The QCNN is a circuit model inspired by the architecture of Convolutional Neural Networks (CNNs). CNNs are successful because they do not need manual feature design and can learn high-level features from raw data. Neural Architecture Search (NAS) builds on this success by learning network architecture and achieves state-of-the-art performance. However, applying NAS to QCNNs presents unique challenges due to the lack of a well-defined search space. In this work, we propose a novel framework for representing QCNN architectures using techniques from NAS, which enables search space design and architecture search. Using this framework, we generate a family of popular QCNNs, those resembling reverse binary trees. We then evaluate this family of models on a music genre classification dataset, GTZAN, to justify the importance of circuit architecture. Furthermore, we employ a genetic algorithm to perform Quantum Phase Recognition (QPR) as an example of architecture search with our representation. This work provides a way to improve model performance without increasing complexity and to jump around the cost landscape to avoid barren plateaus. Finally, we implement the framework as an open-source Python package to enable dynamic QCNN creation and facilitate QCNN search space design for NAS.\n        \u25b3 Less\n      ",
    "title": "Hierarchical quantum circuit representations for neural architecture search",
    "date": "6 May, 2023",
    "authors": [
      "Matt Lourens",
      " Ilya Sinayskiy",
      " Daniel K. Park",
      " Carsten Blank",
      " Francesco Petruccione"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06155",
    "paper_id": "2305.06155",
    "abstract": "\n        In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the size of the dataset and the number of parameters in the model. We also provide preliminary analysis into whether this boost in performance is linked to ease of optimization or more deterministic nature of the predictions, and whether this paradigm leads to better out-of-distribution performance across different testing domains.\n        \u25b3 Less\n      ",
    "title": "Leveraging Synthetic Targets for Machine Translation",
    "date": "6 May, 2023",
    "authors": [
      "Sarthak Mittal",
      " Oleksii Hrinchuk",
      " Oleksii Kuchaiev"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.12554",
    "paper_id": "2205.12554",
    "abstract": "\n        Goal-oriented dialogue systems aim to help users achieve certain goals. Therefore, how humans perceive their helpfulness is important. However, neither the human-perceived helpfulness of goal-oriented dialogue systems nor its fairness implication has been well studied. In this paper, we study computational measurements of helpfulness. We first formally define a dialogue response as helpful if it is relevant & coherent, useful, and informative to a query. Then, we collect human annotations for the helpfulness of dialogue responses based on our definition and build a classifier to automatically determine the helpfulness of a response. We further propose to use the helpfulness level of a dialogue system towards different user queries to measure the fairness of a dialogue system. Experiments with state-of-the-art dialogue systems under three information-seeking scenarios reveal that existing systems tend to be more helpful for questions regarding concepts from highly-developed countries than less-developed countries, uncovering potential fairness concerns underlying the current goal-oriented dialogue systems.\n        \u25b3 Less\n      ",
    "title": "Helpfulness and Fairness of Task-Oriented Dialogue Systems",
    "date": "6 May, 2023",
    "authors": [
      "Jiao Sun",
      " Yu Hou",
      " Jiin Kim",
      " Nanyun Peng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04217",
    "paper_id": "2305.04217",
    "abstract": "\n        Data mining focuses on discovering interesting, non-trivial and meaningful information from large datasets. Data clustering is one of the unsupervised and descriptive data mining task which group data based on similarity features and physically stored together. As a partitioning clustering method, K-means is widely used due to its simplicity and easiness of implementation. But this method has limitations such as local optimal convergence and initial point sensibility. Due to these impediments, nature inspired Swarm based algorithms such as Artificial Bee Colony Algorithm, Ant Colony Optimization, Firefly Algorithm, Bat Algorithm and etc. are used for data clustering to cope with larger datasets with lack and inconsistency of data. In some cases, those algorithms are used with traditional approaches such as K-means as hybrid approaches to produce better results. This paper reviews the performances of these new approaches and compares which is best for certain problematic situation.\n        \u25b3 Less\n      ",
    "title": "Influence of Swarm Intelligence in Data Clustering Mechanisms",
    "date": "6 May, 2023",
    "authors": [
      "Pitawelayalage Dasun Dileepa Pitawela",
      " Gamage Upeksha Ganegoda"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04237",
    "paper_id": "2305.04237",
    "abstract": "\n        This foundational research provides additional support for using the Fuzzy ARTMAP neural network as a classification algorithm in the TAR domain. While research opportunities exist to improve recall performance and explanation, the robust recall results from this study and the proof-of-concept demonstration of If-Then rules for tf-idf vectorization strongly substantiate that a Fuzzy ARTMAP-based TAR system is a potentially viable explainable alternative to \"black box\" TAR systems.\n        \u25b3 Less\n      ",
    "title": "Opening the TAR Black Box: Developing an Interpretable System for eDiscovery Using the Fuzzy ARTMAP Neural Network",
    "date": "6 May, 2023",
    "authors": [
      "Charles Courchaine",
      " Ricky J. Sethi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13934",
    "paper_id": "2305.13934",
    "abstract": "\n        The emergence of large language models has led to the development of powerful tools such as ChatGPT that can produce text indistinguishable from human-generated work. With the increasing accessibility of such technology, students across the globe may utilize it to help with their school work -- a possibility that has sparked discussions on the integrity of student evaluations in the age of artificial intelligence (AI). To date, it is unclear how such tools perform compared to students on university-level courses. Further, students' perspectives regarding the use of such tools, and educators' perspectives on treating their use as plagiarism, remain unknown. Here, we compare the performance of ChatGPT against students on 32 university-level courses. We also assess the degree to which its use can be detected by two classifiers designed specifically for this purpose. Additionally, we conduct a survey across five countries, as well as a more in-depth survey at the authors' institution, to discern students' and educators' perceptions of ChatGPT's use. We find that ChatGPT's performance is comparable, if not superior, to that of students in many courses. Moreover, current AI-text classifiers cannot reliably detect ChatGPT's use in school work, due to their propensity to classify human-written answers as AI-generated, as well as the ease with which AI-generated text can be edited to evade detection. Finally, we find an emerging consensus among students to use the tool, and among educators to treat this as plagiarism. Our findings offer insights that could guide policy discussions addressing the integration of AI into educational frameworks.\n        \u25b3 Less\n      ",
    "title": "Perception, performance, and detectability of conversational artificial intelligence across 32 university courses",
    "date": "6 May, 2023",
    "authors": [
      "Hazem Ibrahim",
      " Fengyuan Liu",
      " Rohail Asim",
      " Balaraju Battu",
      " Sidahmed Benabderrahmane",
      " Bashar Alhafni",
      " Wifag Adnan",
      " Tuka Alhanai",
      " Bedoor AlShebli",
      " Riyadh Baghdadi",
      " Jocelyn J. B\u00e9langer",
      " Elena Beretta",
      " Kemal Celik",
      " Moumena Chaqfeh",
      " Mohammed F. Daqaq",
      " Zaynab El Bernoussi",
      " Daryl Fougnie",
      " Borja Garcia de Soto",
      " Alberto Gandolfi",
      " Andras Gyorgy",
      " Nizar Habash",
      " J. Andrew Harris",
      " Aaron Kaufman",
      " Lefteris Kirousis",
      " Korhan Kocak ",
      " et al. (14 additional authors not shown)"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.01238",
    "paper_id": "2304.01238",
    "abstract": "\n        This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\u00efve Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 surpasses baseline models and other LLMs in the majority of scenarios, particularly when there are a limited number of training samples available. Our code is publicly available at https://github.com/jpmorganchase/emailspamdetection.\n        \u25b3 Less\n      ",
    "title": "Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection",
    "date": "6 May, 2023",
    "authors": [
      "Maxime Labonne",
      " Sean Moran"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05668",
    "paper_id": "2305.05668",
    "abstract": "\n        In this study, we introduce application of Neurosymbolic Artificial Intelligence (NSAI) for predicting the impact strength of additive manufactured polylactic acid (PLA) components, representing the first-ever use of NSAI in the domain of additive manufacturing. The NSAI model amalgamates the advantages of neural networks and symbolic AI, offering a more robust and accurate prediction than traditional machine learning techniques. Experimental data was collected and synthetically augmented to 1000 data points, enhancing the model's precision. The Neurosymbolic model was developed using a neural network architecture comprising input, two hidden layers, and an output layer, followed by a decision tree regressor representing the symbolic component. The model's performance was benchmarked against a Simple Artificial Neural Network (ANN) model by assessing mean squared error (MSE) and R-squared (R2) values for both training and validation datasets. The results reveal that the Neurosymbolic model surpasses the Simple ANN model, attaining lower MSE and higher R2 values for both training and validation sets. This innovative application of the Neurosymbolic approach in estimating the impact strength of additive manufactured PLA components underscores its potential for optimizing the additive manufacturing process. Future research could investigate further refinements to the Neurosymbolic model, extend its application to other materials and additive manufacturing processes, and incorporate real-time monitoring and control for enhanced process optimization.\n        \u25b3 Less\n      ",
    "title": "Neurosymbolic Artificial Intelligence (NSAI) based Algorithm for predicting the Impact Strength of Additive Manufactured Polylactic Acid (PLA) Specimens",
    "date": "6 May, 2023",
    "authors": [
      "Akshansh Mishra",
      " Vijaykumar S Jatti"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.08741",
    "paper_id": "2301.08741",
    "abstract": "\n        This paper introduces Enactive Artificial Intelligence (eAI) as an intersectional gender-inclusive stance towards AI. AI design is an enacted human sociocultural practice that reflects human culture and values. Unrepresentative AI design could lead to social marginalisation. Section 1, drawing from radical enactivism, outlines embodied cultural practices. In Section 2, explores how intersectional gender intertwines with technoscience as a sociocultural practice. Section 3 focuses on subverting gender norms in the specific case of Robot-Human Interaction in AI. Finally, Section 4 identifies four vectors of ethics: explainability, fairness, transparency, and auditability for adopting an intersectionality-inclusive stance in developing gender-inclusive AI and subverting existing gender norms in robot design.\n        \u25b3 Less\n      ",
    "title": "Enactive Artificial Intelligence: Subverting Gender Norms in Robot-Human Interaction",
    "date": "6 May, 2023",
    "authors": [
      "Ines Hipolito",
      " Katie Winkle",
      " Merete Lie"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04275",
    "paper_id": "2305.04275",
    "abstract": "\n        In recent years, there is an increasing interests in reconstruction based generative models for image One-Class Novelty Detection, most of which only focus on image-level information. While in this paper, we further exploit the latent space of Variational Auto-encoder (VAE), a typical reconstruction based model, and we innovatively divide it into three regions: Normal/Anomalous/Unknown-semantic-region. Based on this hypothesis, we propose a new VAE architecture, Recoding Semantic Consistency Based VAE (RSC-VAE), combining VAE with recoding mechanism and constraining the semantic consistency of two encodings. We come up with three training modes of RSC-VAE: 1. One-Class Training Mode, alleviating False Positive problem of normal samples; 2. Distributionally-Shifted Training Mode, alleviating False Negative problem of anomalous samples; 3. Extremely-Imbalanced Training Mode, introducing a small number of anomalous samples for training to enhance the second mode. The experimental results on multiple datasets demonstrate that our mechanism achieves state-of-the-art performance in various baselines including VAE.\n        \u25b3 Less\n      ",
    "title": "RSC-VAE: Recoding Semantic Consistency Based VAE for One-Class Novelty Detection",
    "date": "6 May, 2023",
    "authors": [
      "Ge Zhang",
      " Wangzhe Du"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04296",
    "paper_id": "2305.04296",
    "abstract": "\n        Neural Radiance Fields has become a prominent method of scene generation via view synthesis. A critical requirement for the original algorithm to learn meaningful scene representation is camera pose information for each image in a data set. Current approaches try to circumnavigate this assumption with moderate success, by learning approximate camera positions alongside learning neural representations of a scene. This requires complicated camera models, causing a long and complicated training process, or results in a lack of texture and sharp details in rendered scenes. In this work we introduce Hash Color Correction (HashCC) -- a lightweight method for improving Neural Radiance Fields rendered image quality, applicable also in situations where camera positions for a given set of images are unknown.\n        \u25b3 Less\n      ",
    "title": "HashCC: Lightweight Method to Improve the Quality of the Camera-less NeRF Scene Generation",
    "date": "6 May, 2023",
    "authors": [
      "Jan Olszewski"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04315",
    "paper_id": "2305.04315",
    "abstract": "\n        To be robust to surprising developments, an intelligent agent must be able to respond to many different types of unexpected change in the world. To date, there are no general frameworks for defining and characterizing the types of environment changes that are possible. We introduce a formal and theoretical framework for defining and categorizing environment transformations, changes to the world an agent inhabits. We introduce two types of environment transformation: R-transformations which modify environment dynamics and T-transformations which modify the generation process that produces scenarios. We present a new language for describing domains, scenario generators, and transformations, called the Transformation and Simulator Abstraction Language (T-SAL), and a logical formalism that rigorously defines these concepts. Then, we offer the first formal and computational set of tests for eight categories of environment transformations. This domain-independent framework paves the way for describing unambiguous classes of novelty, constrained and domain-independent random generation of environment transformations, replication of environment transformation studies, and fair evaluation of agent robustness.\n        \u25b3 Less\n      ",
    "title": "A Framework for Characterizing Novel Environment Transformations in General Environments",
    "date": "6 May, 2023",
    "authors": [
      "Matthew Molineaux",
      " Dustin Dannenhauer",
      " Eric Kildebeck"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.10842",
    "paper_id": "2210.10842",
    "abstract": "\n        Recently, there has been tremendous interest in industry 4.0 infrastructure to address labor shortages in global supply chains. Deploying artificial intelligence-enabled robotic bin picking systems in real world has become particularly important for reducing stress and physical demands of workers while increasing speed and efficiency of warehouses. To this end, artificial intelligence-enabled robotic bin picking systems may be used to automate order picking, but with the risk of causing expensive damage during an abnormal event such as sensor failure. As such, reliability becomes a critical factor for translating artificial intelligence research to real world applications and products. In this paper, we propose a reliable object detection and segmentation system with MultiModal Redundancy (MMRNet) for tackling object detection and segmentation for robotic bin picking using data from different modalities. This is the first system that introduces the concept of multimodal redundancy to address sensor failure issues during deployment. In particular, we realize the multimodal redundancy framework with a gate fusion module and dynamic ensemble learning. Finally, we present a new label-free multi-modal consistency (MC) score that utilizes the output from all modalities to measure the overall system output reliability and uncertainty. Through experiments, we demonstrate that in an event of missing modality, our system provides a much more reliable performance compared to baseline models. We also demonstrate that our MC score is a more reliability indicator for outputs during inference time compared to the model generated confidence scores that are often over-confident.\n        \u25b3 Less\n      ",
    "title": "MMRNet: Improving Reliability for Multimodal Object Detection and Segmentation for Bin Picking via Multimodal Redundancy",
    "date": "7 May, 2023",
    "authors": [
      "Yuhao Chen",
      " Hayden Gunraj",
      " E. Zhixuan Zeng",
      " Robbie Meyer",
      " Maximilian Gilles",
      " Alexander Wong"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05538",
    "paper_id": "2305.05538",
    "abstract": "\n        Many organisations manage service quality and monitor a large set devices and servers where each entity is associated with telemetry or physical sensor data series. Recently, various methods have been proposed to detect behavioural anomalies, however existing approaches focus on multivariate time series and ignore communication between entities. Moreover, we aim to support end-users in not only in locating entities and sensors causing an anomaly at a certain period, but also explain this decision. We propose a scalable approach to detect anomalies using a two-step approach. First, we recover relations between entities in the network, since relations are often dynamic in nature and caused by an unknown underlying process. Next, we report anomalies based on an embedding of sequential patterns. Pattern mining is efficient and supports interpretation, i.e. patterns represent frequent occurring behaviour in time series. We extend pattern mining to filter sequential patterns based on frequency, temporal constraints and minimum description length. We collect and release two public datasets for international broadcasting and X from an Internet company. \\textit{BAD} achieves an overall F1-Score of 0.78 on 9 benchmark datasets, significantly outperforming the best baseline by 3\\%. Additionally, \\textit{BAD} is also an order-of-magnitude faster than state-of-the-art anomaly detection methods.\n        \u25b3 Less\n      ",
    "title": "Efficient pattern-based anomaly detection in a network of multivariate devices",
    "date": "7 May, 2023",
    "authors": [
      "Len Feremans",
      " Boris Cule",
      " Bart Goethals"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.00858",
    "paper_id": "2210.00858",
    "abstract": "\n        In this paper we present a neurosymbolic architecture for coupling language-guided visual reasoning with robot manipulation. A non-expert human user can prompt the robot using unconstrained natural language, providing a referring expression (REF), a question (VQA), or a grasp action instruction. The system tackles all cases in a task-agnostic fashion through the utilization of a shared library of primitive skills. Each primitive handles an independent sub-task, such as reasoning about visual attributes, spatial relation comprehension, logic and enumeration, as well as arm control. A language parser maps the input query to an executable program composed of such primitives, depending on the context. While some primitives are purely symbolic operations (e.g. counting), others are trainable neural functions (e.g. visual grounding), therefore marrying the interpretability and systematic generalization benefits of discrete symbolic approaches with the scalability and representational power of deep networks. We generate a 3D vision-and-language synthetic dataset of tabletop scenes in a simulation environment to train our approach and perform extensive evaluations in both synthetic and real-world scenes. Results showcase the benefits of our approach in terms of accuracy, sample-efficiency, and robustness to the user's vocabulary, while being transferable to real-world scenes with few-shot visual fine-tuning. Finally, we integrate our method with a robot framework and demonstrate how it can serve as an interpretable solution for an interactive object-picking task, both in simulation and with a real robot. We make our datasets available in https://gtziafas.github.io/neurosymbolic-manipulation.\n        \u25b3 Less\n      ",
    "title": "Enhancing Interpretability and Interactivity in Robot Manipulation: A Neurosymbolic Approach",
    "date": "7 May, 2023",
    "authors": [
      "Georgios Tziafas",
      " Hamidreza Kasaei"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04346",
    "paper_id": "2305.04346",
    "abstract": "\n        Nearly all general-purpose neural semantic parsers generate logical forms in a strictly top-down autoregressive fashion. Though such systems have achieved impressive results across a variety of datasets and domains, recent works have called into question whether they are ultimately limited in their ability to compositionally generalize. In this work, we approach semantic parsing from, quite literally, the opposite direction; that is, we introduce a neural semantic parsing generation method that constructs logical forms from the bottom up, beginning from the logical form's leaves. The system we introduce is lazy in that it incrementally builds up a set of potential semantic parses, but only expands and processes the most promising candidate parses at each generation step. Such a parsimonious expansion scheme allows the system to maintain an arbitrarily large set of parse hypotheses that are never realized and thus incur minimal computational overhead. We evaluate our approach on compositional generalization; specifically, on the challenging CFQ dataset and three Text-to-SQL datasets where we show that our novel, bottom-up semantic parsing technique outperforms general-purpose semantic parsers while also being competitive with comparable neural parsers that have been designed for each task.\n        \u25b3 Less\n      ",
    "title": "Laziness Is a Virtue When It Comes to Compositionality in Neural Semantic Parsing",
    "date": "7 May, 2023",
    "authors": [
      "Maxwell Crouse",
      " Pavan Kapanipathi",
      " Subhajit Chaudhury",
      " Tahira Naseem",
      " Ramon Astudillo",
      " Achille Fokoue",
      " Tim Klinger"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.03293",
    "paper_id": "2212.03293",
    "abstract": "\n        With the rising industrial attention to 3D virtual modeling technology, generating novel 3D content based on specified conditions (e.g. text) has become a hot issue. In this paper, we propose a new generative 3D modeling framework called Diffusion-SDF for the challenging task of text-to-shape synthesis. Previous approaches lack flexibility in both 3D data representation and shape generation, thereby failing to generate highly diversified 3D shapes conforming to the given text descriptions. To address this, we propose a SDF autoencoder together with the Voxelized Diffusion model to learn and generate representations for voxelized signed distance fields (SDFs) of 3D shapes. Specifically, we design a novel UinU-Net architecture that implants a local-focused inner network inside the standard U-Net architecture, which enables better reconstruction of patch-independent SDF representations. We extend our approach to further text-to-shape tasks including text-conditioned shape completion and manipulation. Experimental results show that Diffusion-SDF generates both higher quality and more diversified 3D shapes that conform well to given text descriptions when compared to previous approaches. Code is available at: https://github.com/ttlmh/Diffusion-SDF\n        \u25b3 Less\n      ",
    "title": "Diffusion-SDF: Text-to-Shape via Voxelized Diffusion",
    "date": "7 May, 2023",
    "authors": [
      "Muheng Li",
      " Yueqi Duan",
      " Jie Zhou",
      " Jiwen Lu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04357",
    "paper_id": "2305.04357",
    "abstract": "\n        Structural causal models provide a formalism to express causal relations between variables of interest. Models and variables can represent a system at different levels of abstraction, whereby relations may be coarsened and refined according to the need of a modeller. However, switching between different levels of abstraction requires evaluating a trade-off between the consistency and the information loss among different models. In this paper we introduce a family of interventional measures that an agent may use to evaluate such a trade-off. We consider four measures suited for different tasks, analyze their properties, and propose algorithms to evaluate and learn causal abstractions. Finally, we illustrate the flexibility of our setup by empirically showing how different measures and algorithmic choices may lead to different abstractions.\n        \u25b3 Less\n      ",
    "title": "Quantifying Consistency and Information Loss for Causal Abstraction Learning",
    "date": "7 May, 2023",
    "authors": [
      "Fabio Massimo Zennaro",
      " Paolo Turrini",
      " Theodoros Damoulas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.05893",
    "paper_id": "2301.05893",
    "abstract": "\n        An abstraction can be used to relate two structural causal models representing the same system at different levels of resolution. Learning abstractions which guarantee consistency with respect to interventional distributions would allow one to jointly reason about evidence across multiple levels of granularity while respecting the underlying cause-effect relationships. In this paper, we introduce a first framework for causal abstraction learning between SCMs based on the formalization of abstraction recently proposed by Rischel (2020). Based on that, we propose a differentiable programming solution that jointly solves a number of combinatorial sub-problems, and we study its performance and benefits against independent and sequential approaches on synthetic settings and on a challenging real-world problem related to electric vehicle battery manufacturing.\n        \u25b3 Less\n      ",
    "title": "Jointly Learning Consistent Causal Abstractions Over Multiple Interventional Distributions",
    "date": "7 May, 2023",
    "authors": [
      "Fabio Massimo Zennaro",
      " M\u00e1t\u00e9 Dr\u00e1vucz",
      " Geanina Apachitei",
      " W. Dhammika Widanage",
      " Theodoros Damoulas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.07407",
    "paper_id": "2304.07407",
    "abstract": "\n        Motivated by a number of real-world applications from domains like healthcare and sustainable transportation, in this paper we study a scenario of repeated principal-agent games within a multi-armed bandit (MAB) framework, where: the principal gives a different incentive for each bandit arm, the agent picks a bandit arm to maximize its own expected reward plus incentive, and the principal observes which arm is chosen and receives a reward (different than that of the agent) for the chosen arm. Designing policies for the principal is challenging because the principal cannot directly observe the reward that the agent receives for their chosen actions, and so the principal cannot directly learn the expected reward using existing estimation techniques. As a result, the problem of designing policies for this scenario, as well as similar ones, remains mostly unexplored. In this paper, we construct a policy that achieves a low regret (i.e., square-root regret up to a log factor) in this scenario for the case where the agent has perfect-knowledge about its own expected rewards for each bandit arm. We design our policy by first constructing an estimator for the agent's expected reward for each bandit arm. Since our estimator uses as data the sequence of incentives offered and subsequently chosen arms, the principal's estimation can be regarded as an analogy of online inverse optimization in MAB's. Next we construct a policy that we prove achieves a low regret by deriving finite-sample concentration bounds for our estimator. We conclude with numerical simulations demonstrating the applicability of our policy to real-life setting from collaborative transportation planning.\n        \u25b3 Less\n      ",
    "title": "Repeated Principal-Agent Games with Unobserved Agent Rewards and Perfect-Knowledge Agents",
    "date": "7 May, 2023",
    "authors": [
      "Ilgin Dogan",
      " Zuo-Jun Max Shen",
      " Anil Aswani"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04361",
    "paper_id": "2305.04361",
    "abstract": "\n        In Reinforcement Learning (RL), an agent acts in an unknown environment to maximize the expected cumulative discounted sum of an external reward signal, i.e., the expected return. In practice, in many tasks of interest, such as policy optimization, the agent usually spends its interaction budget by collecting episodes of fixed length within a simulator (i.e., Monte Carlo simulation). However, given the discounted nature of the RL objective, this data collection strategy might not be the best option. Indeed, the rewards taken in early simulation steps weigh exponentially more than future rewards. Taking a cue from this intuition, in this paper, we design an a-priori budget allocation strategy that leads to the collection of trajectories of different lengths, i.e., truncated. The proposed approach provably minimizes the width of the confidence intervals around the empirical estimates of the expected return of a policy. After discussing the theoretical properties of our method, we make use of our trajectory truncation mechanism to extend Policy Optimization via Importance Sampling (POIS, Metelli et al., 2018) algorithm. Finally, we conduct a numerical comparison between our algorithm and POIS: the results are consistent with our theory and show that an appropriate truncation of the trajectories can succeed in improving performance.\n        \u25b3 Less\n      ",
    "title": "Truncating Trajectories in Monte Carlo Reinforcement Learning",
    "date": "7 May, 2023",
    "authors": [
      "Riccardo Poiani",
      " Alberto Maria Metelli",
      " Marcello Restelli"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.03997",
    "paper_id": "2304.03997",
    "abstract": "\n        Integrating renewable energy sources into the power grid is becoming increasingly important as the world moves towards a more sustainable energy future in line with SDG 7. However, the intermittent nature of renewable energy sources can make it challenging to manage the power grid and ensure a stable supply of electricity, which is crucial for achieving SDG 9. In this paper, we propose a deep learning-based approach for predicting energy demand in a smart power grid, which can improve the integration of renewable energy sources by providing accurate predictions of energy demand. Our approach aligns with SDG 13 on climate action, enabling more efficient management of renewable energy resources. We use long short-term memory networks, well-suited for time series data, to capture complex patterns and dependencies in energy demand data. The proposed approach is evaluated using four historical short-term energy demand data datasets from different energy distribution companies, including American Electric Power, Commonwealth Edison, Dayton Power and Light, and Pennsylvania-New Jersey-Maryland Interconnection. The proposed model is also compared with three other state-of-the-art forecasting algorithms: Facebook Prophet, Support Vector Regression, and Random Forest Regression. The experimental results show that the proposed REDf model can accurately predict energy demand with a mean absolute error of 1.4%, indicating its potential to enhance the stability and efficiency of the power grid and contribute to achieving SDGs 7, 9, and 13. The proposed model also has the potential to manage the integration of renewable energy sources in an effective manner.\n        \u25b3 Less\n      ",
    "title": "Predicting Short Term Energy Demand in Smart Grid: A Deep Learning Approach for Integrating Renewable Energy Sources in Line with SDGs 7, 9, and 13",
    "date": "7 May, 2023",
    "authors": [
      "Md Saef Ullah Miah",
      " Junaida Sulaiman",
      " Md. Imamul Islam",
      " Md. Masuduzzaman",
      " Nimay Chandra Giri",
      " Siddhartha Bhattacharyya",
      " Segbedji Geraldo Favi",
      " Leo Mrsic"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.03919",
    "paper_id": "2210.03919",
    "abstract": "\n        Recently introduced Contrastive Language-Image Pre-Training (CLIP) bridges images and text by embedding them into a joint latent space. This opens the door to ample literature that aims to manipulate an input image by providing a textual explanation. However, due to the discrepancy between image and text embeddings in the joint space, using text embeddings as the optimization target often introduces undesired artifacts in the resulting images. Disentanglement, interpretability, and controllability are also hard to guarantee for manipulation. To alleviate these problems, we propose to define corpus subspaces spanned by relevant prompts to capture specific image characteristics. We introduce CLIP Projection-Augmentation Embedding (PAE) as an optimization target to improve the performance of text-guided image manipulation. Our method is a simple and general paradigm that can be easily computed and adapted, and smoothly incorporated into any CLIP-based image manipulation algorithm. To demonstrate the effectiveness of our method, we conduct several theoretical and empirical studies. As a case study, we utilize the method for text-guided semantic face editing. We quantitatively and qualitatively demonstrate that PAE facilitates a more disentangled, interpretable, and controllable image manipulation with state-of-the-art quality and accuracy.\n        \u25b3 Less\n      ",
    "title": "CLIP-PAE: Projection-Augmentation Embedding to Extract Relevant Features for a Disentangled, Interpretable, and Controllable Text-Guided Face Manipulation",
    "date": "7 May, 2023",
    "authors": [
      "Chenliang Zhou",
      " Fangcheng Zhong",
      " Cengiz Oztireli"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13927",
    "paper_id": "2305.13927",
    "abstract": "\n        As artificial intelligence and machine learning continue to advance, we must understand their strategic importance in national security. This paper focuses on unique AI applications in the military, emphasizes strategic imperatives for success, and aims to rekindle excitement about AI's role in national security. We will examine the United States progress in AI and ML from a military standpoint, discuss the importance of securing these technologies from adversaries, and explore the challenges and risks associated with their integration. Finally, we will highlight the strategic significance of AI to national security and a set of strategic imperatives for military leaders and policymakers\n        \u25b3 Less\n      ",
    "title": "Optimizing National Security Strategies through LLM-Driven Artificial Intelligence Integration",
    "date": "7 May, 2023",
    "authors": [
      "Dmitry I Mikhailov"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04400",
    "paper_id": "2305.04400",
    "abstract": "\n        A Large Language Model (LLM) is an artificial intelligence system that has been trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. GPT-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics, biases, and other decision effects. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (Anchoring Heuristic, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was erroneously influenced by salient anecdotal information (Representativeness and Availability Heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively - even though both presentations contained identical information (Framing Effect, Study 3); and it valued an owned item more than a newly found item even though the two items were identical (Endowment Effect, Study 4). In each study, human participants showed similar effects. Heuristics and related decision effects in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM - which lacks these processes - also shows such effects invites consideration of the possibility that language may play a role in generating these effects in humans.\n        \u25b3 Less\n      ",
    "title": "Do Large Language Models Show Decision Heuristics Similar to Humans? A Case Study Using GPT-3.5",
    "date": "7 May, 2023",
    "authors": [
      "Gaurav Suri",
      " Lily R. Slater",
      " Ali Ziaee",
      " Morgan Nguyen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04408",
    "paper_id": "2305.04408",
    "abstract": "\n        Anytime search algorithms are useful for planning problems where a solution is desired under a limited time budget. Anytime algorithms first aim to provide a feasible solution quickly and then attempt to improve it until the time budget expires. On the other hand, parallel search algorithms utilize the multithreading capability of modern processors to speed up the search. One such algorithm, ePA*SE (Edge-Based Parallel A* for Slow Evaluations), parallelizes edge evaluations to achieve faster planning and is especially useful in domains with expensive-to-compute edges. In this work, we propose an extension that brings the anytime property to ePA*SE, resulting in A-ePA*SE. We evaluate A-ePA*SE experimentally and show that it is significantly more efficient than other anytime search methods. The open-source code for A-ePA*SE, along with the baselines, is available here: https://github.com/shohinm/parallel_search\n        \u25b3 Less\n      ",
    "title": "A-ePA*SE: Anytime Edge-Based Parallel A* for Slow Evaluations",
    "date": "7 May, 2023",
    "authors": [
      "Hanlan Yang",
      " Shohin Mukherjee",
      " Maxim Likhachev"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04412",
    "paper_id": "2305.04412",
    "abstract": "\n        When autonomous vehicles are deployed on public roads, they will encounter countless and diverse driving situations. Many manually designed driving policies are difficult to scale to the real world. Fortunately, reinforcement learning has shown great success in many tasks by automatic trial and error. However, when it comes to autonomous driving in interactive dense traffic, RL agents either fail to learn reasonable performance or necessitate a large amount of data. Our insight is that when humans learn to drive, they will 1) make decisions over the high-level skill space instead of the low-level control space and 2) leverage expert prior knowledge rather than learning from scratch. Inspired by this, we propose ASAP-RL, an efficient reinforcement learning algorithm for autonomous driving that simultaneously leverages motion skills and expert priors. We first parameterized motion skills, which are diverse enough to cover various complex driving scenarios and situations. A skill parameter inverse recovery method is proposed to convert expert demonstrations from control space to skill space. A simple but effective double initialization technique is proposed to leverage expert priors while bypassing the issue of expert suboptimality and early performance degradation. We validate our proposed method on interactive dense-traffic driving tasks given simple and sparse rewards. Experimental results show that our method can lead to higher learning efficiency and better driving performance relative to previous methods that exploit skills and priors differently. Code is open-sourced to facilitate further research.\n        \u25b3 Less\n      ",
    "title": "Efficient Reinforcement Learning for Autonomous Driving with Parameterized Skills and Priors",
    "date": "7 May, 2023",
    "authors": [
      "Letian Wang",
      " Jie Liu",
      " Hao Shao",
      " Wenshuo Wang",
      " Ruobing Chen",
      " Yu Liu",
      " Steven L. Waslander"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04417",
    "paper_id": "2305.04417",
    "abstract": "\n        We evaluated the capability of a state-of-the-art generative pre-trained transformer (GPT) model to perform semantic annotation of short text snippets (one to few sentences) coming from legal documents of various types. Discussions of potential uses (e.g., document drafting, summarization) of this emerging technology in legal domain have intensified, but to date there has not been a rigorous analysis of these large language models' (LLM) capacity in sentence-level semantic annotation of legal texts in zero-shot learning settings. Yet, this particular type of use could unlock many practical applications (e.g., in contract review) and research opportunities (e.g., in empirical legal studies). We fill the gap with this study. We examined if and how successfully the model can semantically annotate small batches of short text snippets (10-50) based exclusively on concise definitions of the semantic types. We found that the GPT model performs surprisingly well in zero-shot settings on diverse types of documents (F1=.73 on a task involving court opinions, .86 for contracts, and .54 for statutes and regulations). These findings can be leveraged by legal scholars and practicing lawyers alike to guide their decisions in integrating LLMs in wide range of workflows involving semantic annotation of legal texts.\n        \u25b3 Less\n      ",
    "title": "Unlocking Practical Applications in Legal Domain: Evaluation of GPT for Zero-Shot Semantic Annotation of Legal Texts",
    "date": "7 May, 2023",
    "authors": [
      "Jaromir Savelka"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04430",
    "paper_id": "2305.04430",
    "abstract": "\n        Haze usually leads to deteriorated images with low contrast, color shift and structural distortion. We observe that many deep learning based models exhibit exceptional performance on removing homogeneous haze, but they usually fail to address the challenge of non-homogeneous dehazing. Two main factors account for this situation. Firstly, due to the intricate and non uniform distribution of dense haze, the recovery of structural and chromatic features with high fidelity is challenging, particularly in regions with heavy haze. Secondly, the existing small scale datasets for non-homogeneous dehazing are inadequate to support reliable learning of feature mappings between hazy images and their corresponding haze-free counterparts by convolutional neural network (CNN)-based models. To tackle these two challenges, we propose a novel two branch network that leverages 2D discrete wavelete transform (DWT), fast Fourier convolution (FFC) residual block and a pretrained ConvNeXt model. Specifically, in the DWT-FFC frequency branch, our model exploits DWT to capture more high-frequency features. Moreover, by taking advantage of the large receptive field provided by FFC residual blocks, our model is able to effectively explore global contextual information and produce images with better perceptual quality. In the prior knowledge branch, an ImageNet pretrained ConvNeXt as opposed to Res2Net is adopted. This enables our model to learn more supplementary information and acquire a stronger generalization ability. The feasibility and effectiveness of the proposed method is demonstrated via extensive experiments and ablation studies. The code is available at https://github.com/zhouh115/DWT-FFC.\n        \u25b3 Less\n      ",
    "title": "Breaking Through the Haze: An Advanced Non-Homogeneous Dehazing Method based on Fast Fourier Convolution and ConvNeXt",
    "date": "7 May, 2023",
    "authors": [
      "Han Zhou",
      " Wei Dong",
      " Yangyi Liu",
      " Jun Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04432",
    "paper_id": "2305.04432",
    "abstract": "\n        The agent learns to organize decision behavior to achieve a behavioral goal, such as reward maximization, and reinforcement learning is often used for this optimization. Learning an optimal behavioral strategy is difficult under the uncertainty that events necessary for learning are only partially observable, called as Partially Observable Markov Decision Process (POMDP). However, the real-world environment also gives many events irrelevant to reward delivery and an optimal behavioral strategy. The conventional methods in POMDP, which attempt to infer transition rules among the entire observations, including irrelevant states, are ineffective in such an environment. Supposing Redundantly Observable Markov Decision Process (ROMDP), here we propose a method for goal-oriented reinforcement learning to efficiently learn state transition rules among reward-related \"core states'' from redundant observations. Starting with a small number of initial core states, our model gradually adds new core states to the transition diagram until it achieves an optimal behavioral strategy consistent with the Bellman equation. We demonstrate that the resultant inference model outperforms the conventional method for POMDP. We emphasize that our model only containing the core states has high explainability. Furthermore, the proposed method suits online learning as it suppresses memory consumption and improves learning speed.\n        \u25b3 Less\n      ",
    "title": "Goal-oriented inference of environment from redundant observations",
    "date": "7 May, 2023",
    "authors": [
      "Kazuki Takahashi",
      " Tomoki Fukai",
      " Yutaka Sakai",
      " Takashi Takekawa"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.00434",
    "paper_id": "2210.00434",
    "abstract": "\n        In this paper, we consider a novel research problem: music-to-text synaesthesia. Different from the classical music tagging problem that classifies a music recording into pre-defined categories, music-to-text synaesthesia aims to generate descriptive texts from music recordings with the same sentiment for further understanding. As existing music-related datasets do not contain the semantic descriptions on music recordings, we collect a new dataset that contains 1,955 aligned pairs of classical music recordings and text descriptions. Based on this, we build a computational model to generate sentences that can describe the content of the music recording. To tackle the highly non-discriminative classical music, we design a group topology-preservation loss, which considers more samples as a group reference and preserves the relative topology among different samples. Extensive experimental results qualitatively and quantitatively demonstrate the effectiveness of our proposed model over five heuristics or pre-trained competitive methods and their variants on our collected dataset.\n        \u25b3 Less\n      ",
    "title": "Music-to-Text Synaesthesia: Generating Descriptive Text from Music Recordings",
    "date": "7 May, 2023",
    "authors": [
      "Zhihuan Kuang",
      " Shi Zong",
      " Jianbing Zhang",
      " Jiajun Chen",
      " Hongfu Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04446",
    "paper_id": "2305.04446",
    "abstract": "\n        The widespread dissemination of toxic online posts is increasingly damaging to society. However, research on detecting toxic language in Chinese has lagged significantly. Existing datasets lack fine-grained annotation of toxic types and expressions, and ignore the samples with indirect toxicity. In addition, it is crucial to introduce lexical knowledge to detect the toxicity of posts, which has been a challenge for researchers. In this paper, we facilitate the fine-grained detection of Chinese toxic language. First, we built Monitor Toxic Frame, a hierarchical taxonomy to analyze toxic types and expressions. Then, a fine-grained dataset ToxiCN is presented, including both direct and indirect toxic samples. We also build an insult lexicon containing implicit profanity and propose Toxic Knowledge Enhancement (TKE) as a benchmark, incorporating the lexical feature to detect toxic language. In the experimental stage, we demonstrate the effectiveness of TKE. After that, a systematic quantitative and qualitative analysis of the findings is given.\n        \u25b3 Less\n      ",
    "title": "Facilitating Fine-grained Detection of Chinese Toxic Language: Hierarchical Taxonomy, Resources, and Benchmarks",
    "date": "7 May, 2023",
    "authors": [
      "Junyu Lu",
      " Bo Xu",
      " Xiaokun Zhang",
      " Changrong Min",
      " Liang Yang",
      " Hongfei Lin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.06507",
    "paper_id": "2211.06507",
    "abstract": "\n        Unpacking and comprehending how black-box machine learning algorithms make decisions has been a persistent challenge for researchers and end-users. Explaining time-series predictive models is useful for clinical applications with high stakes to understand the behavior of prediction models. However, existing approaches to explain such models are frequently unique to data where the features do not have a time-varying component. In this paper, we introduce WindowSHAP, a model-agnostic framework for explaining time-series classifiers using Shapley values. We intend for WindowSHAP to mitigate the computational complexity of calculating Shapley values for long time-series data as well as improve the quality of explanations. WindowSHAP is based on partitioning a sequence into time windows. Under this framework, we present three distinct algorithms of Stationary, Sliding and Dynamic WindowSHAP, each evaluated against baseline approaches, KernelSHAP and TimeSHAP, using perturbation and sequence analyses metrics. We applied our framework to clinical time-series data from both a specialized clinical domain (Traumatic Brain Injury - TBI) as well as a broad clinical domain (critical care medicine). The experimental results demonstrate that, based on the two quantitative metrics, our framework is superior at explaining clinical time-series classifiers, while also reducing the complexity of computations. We show that for time-series data with 120 time steps (hours), merging 10 adjacent time points can reduce the CPU time of WindowSHAP by 80% compared to KernelSHAP. We also show that our Dynamic WindowSHAP algorithm focuses more on the most important time steps and provides more understandable explanations. As a result, WindowSHAP not only accelerates the calculation of Shapley values for time-series data, but also delivers more understandable explanations with higher quality.\n        \u25b3 Less\n      ",
    "title": "WindowSHAP: An Efficient Framework for Explaining Time-series Classifiers based on Shapley Values",
    "date": "7 May, 2023",
    "authors": [
      "Amin Nayebi",
      " Sindhu Tipirneni",
      " Chandan K Reddy",
      " Brandon Foreman",
      " Vignesh Subbian"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04460",
    "paper_id": "2305.04460",
    "abstract": "\n        Recent works on form understanding mostly employ multimodal transformers or large-scale pre-trained language models. These models need ample data for pre-training. In contrast, humans can usually identify key-value pairings from a form only by looking at layouts, even if they don't comprehend the language used. No prior research has been conducted to investigate how helpful layout information alone is for form understanding. Hence, we propose a unique entity-relation graph parsing method for scanned forms called LAGNN, a language-independent Graph Neural Network model. Our model parses a form into a word-relation graph in order to identify entities and relations jointly and reduce the time complexity of inference. This graph is then transformed by deterministic rules into a fully connected entity-relation graph. Our model simply takes into account relative spacing between bounding boxes from layout information to facilitate easy transfer across languages. To further improve the performance of LAGNN, and achieve isomorphism between entity-relation graphs and word-relation graphs, we use integer linear programming (ILP) based inference. Code is publicly available at https://github.com/Bhanu068/LAGNN\n        \u25b3 Less\n      ",
    "title": "Language Independent Neuro-Symbolic Semantic Parsing for Form Understanding",
    "date": "7 May, 2023",
    "authors": [
      "Bhanu Prakash Voutharoja",
      " Lizhen Qu",
      " Fatemeh Shiri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04468",
    "paper_id": "2305.04468",
    "abstract": "\n        Mechanical defects in real situations affect observation values and cause abnormalities in multivariate time series, such as sensor values or network data. To perceive abnormalities in such data, it is crucial to understand the temporal context and interrelation between variables simultaneously. The anomaly detection task for time series, especially for unlabeled data, has been a challenging problem, and we address it by applying a suitable data degradation scheme to self-supervised model training. We define four types of synthetic outliers and propose the degradation scheme in which a portion of input data is replaced with one of the synthetic outliers. Inspired by the self-attention mechanism, we design a Transformer-based architecture to recognize the temporal context and detect unnatural sequences with high efficiency. Our model converts multivariate data points into temporal representations with relative position bias and yields anomaly scores from these representations. Our method, AnomalyBERT, shows a great capability of detecting anomalies contained in complex time series and surpasses previous state-of-the-art methods on five real-world benchmarks. Our code is available at https://github.com/Jhryu30/AnomalyBERT.\n        \u25b3 Less\n      ",
    "title": "AnomalyBERT: Self-Supervised Transformer for Time Series Anomaly Detection using Data Degradation Scheme",
    "date": "7 May, 2023",
    "authors": [
      "Yungi Jeong",
      " Eunseok Yang",
      " Jung Hyun Ryu",
      " Imseong Park",
      " Myungjoo Kang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04506",
    "paper_id": "2305.04506",
    "abstract": "\n        It is critical for vehicles to prevent any collisions with pedestrians. Current methods for pedestrian collision prevention focus on integrating visual pedestrian detectors with Automatic Emergency Braking (AEB) systems which can trigger warnings and apply brakes as a pedestrian enters a vehicle's path. Unfortunately, pedestrian-detection-based systems can be hindered in certain situations such as night-time or when pedestrians are occluded. Our system addresses such issues using an online, map-based pedestrian detection aggregation system where common pedestrian locations are learned after repeated passes of locations. Using a carefully collected and annotated dataset in La Jolla, CA, we demonstrate the system's ability to learn pedestrian zones and generate advisory notices when a vehicle is approaching a pedestrian despite challenges like dark lighting or pedestrian occlusion. Using the number of correct advisories, false advisories, and missed advisories to define precision and recall performance metrics, we evaluate our system and discuss future positive effects with further data collection. We have made our code available at https://github.com/s7desai/ped-mapping, and a video demonstration of the CHAMP system at https://youtu.be/dxeCrS_Gpkw.\n        \u25b3 Less\n      ",
    "title": "Pedestrian Behavior Maps for Safety Advisories: CHAMP Framework and Real-World Data Analysis",
    "date": "7 May, 2023",
    "authors": [
      "Ross Greer",
      " Samveed Desai",
      " Lulua Rakla",
      " Akshay Gopalkrishnan",
      " Afnan Alofi",
      " Mohan Trivedi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04516",
    "paper_id": "2305.04516",
    "abstract": "\n        One of the most important tasks for ensuring safe autonomous driving systems is accurately detecting road traffic lights and accurately determining how they impact the driver's actions. In various real-world driving situations, a scene may have numerous traffic lights with varying levels of relevance to the driver, and thus, distinguishing and detecting the lights that are relevant to the driver and influence the driver's actions is a critical safety task. This paper proposes a traffic light detection model which focuses on this task by first defining salient lights as the lights that affect the driver's future decisions. We then use this salience property to construct the LAVA Salient Lights Dataset, the first US traffic light dataset with an annotated salience property. Subsequently, we train a Deformable DETR object detection transformer model using Salience-Sensitive Focal Loss to emphasize stronger performance on salient traffic lights, showing that a model trained with this loss function has stronger recall than one trained without.\n        \u25b3 Less\n      ",
    "title": "Robust Traffic Light Detection Using Salience-Sensitive Loss: Computational Framework and Evaluations",
    "date": "7 May, 2023",
    "authors": [
      "Ross Greer",
      " Akshay Gopalkrishnan",
      " Jacob Landgren",
      " Lulua Rakla",
      " Anish Gopalan",
      " Mohan Trivedi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00866",
    "paper_id": "2305.00866",
    "abstract": "\n        Segment Anything Model (SAM) has attracted significant attention recently, due to its impressive performance on various downstream tasks in a zero-short manner. Computer vision (CV) area might follow the natural language processing (NLP) area to embark on a path from task-specific vision models toward foundation models. However, deep vision models are widely recognized as vulnerable to adversarial examples, which fool the model to make wrong predictions with imperceptible perturbation. Such vulnerability to adversarial attacks causes serious concerns when applying deep models to security-sensitive applications. Therefore, it is critical to know whether the vision foundation model SAM can also be fooled by adversarial attacks. To the best of our knowledge, our work is the first of its kind to conduct a comprehensive investigation on how to attack SAM with adversarial examples. With the basic attack goal set to mask removal, we investigate the adversarial robustness of SAM in the full white-box setting and transfer-based black-box settings. Beyond the basic goal of mask removal, we further investigate and find that it is possible to generate any desired mask by the adversarial attack.\n        \u25b3 Less\n      ",
    "title": "Attack-SAM: Towards Attacking Segment Anything Model With Adversarial Examples",
    "date": "8 May, 2023",
    "authors": [
      "Chenshuang Zhang",
      " Chaoning Zhang",
      " Taegoo Kang",
      " Donghun Kim",
      " Sung-Ho Bae",
      " In So Kweon"
    ]
  },
  {
    "url": "https://arxiv.org/abs/1710.00310",
    "paper_id": "1710.00310",
    "abstract": "\n        In this paper we study the personalized book recommender system in a child-robot interactive environment. Firstly, we propose a novel text search algorithm using an inverse filtering mechanism that improves the efficiency. Secondly, we propose a user interest prediction method based on the Bayesian network and a novel feedback mechanism. According to children's fuzzy language input, the proposed method gives the predicted interests. Thirdly, the domain specific synonym association is proposed based on word vectorization, in order to improve the understanding of user intention. Experimental results show that the proposed recommender system has an improved performance and it can operate on embedded consumer devices with limited computational resources.\n        \u25b3 Less\n      ",
    "title": "Personalized Recommender System for Children's Book Recommendation with A Realtime Interactive Robot",
    "date": "8 May, 2023",
    "authors": [
      "Yun Liu",
      " Tianmeng Gao",
      " Baolin Song",
      " Chengwei Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.04553",
    "paper_id": "2304.04553",
    "abstract": "\n        The Transformer is a highly successful deep learning model that has revolutionised the world of artificial neural networks, first in natural language processing and later in computer vision. This model is based on the attention mechanism and is able to capture complex semantic relationships between a variety of patterns present in the input data. Precisely because of these characteristics, the Transformer has recently been exploited for time series forecasting problems, assuming a natural adaptability to the domain of continuous numerical series. Despite the acclaimed results in the literature, some works have raised doubts about the robustness and effectiveness of this approach. In this paper, we further investigate the effectiveness of Transformer-based models applied to the domain of time series forecasting, demonstrate their limitations, and propose a set of alternative models that are better performing and significantly less complex. In particular, we empirically show how simplifying Transformer-based forecasting models almost always leads to an improvement, reaching state of the art performance. We also propose shallow models without the attention mechanism, which compete with the overall state of the art in long time series forecasting, and demonstrate their ability to accurately predict time series over extremely long windows. From a methodological perspective, we show how it is always necessary to use a simple baseline to verify the effectiveness of proposed models, and finally, we conclude the paper with a reflection on recent research paths and the opportunity to follow trends and hypes even where it may not be necessary.\n        \u25b3 Less\n      ",
    "title": "Two Steps Forward and One Behind: Rethinking Time Series Forecasting with Deep Learning",
    "date": "8 May, 2023",
    "authors": [
      "Riccardo Ughi",
      " Eugenio Lomurno",
      " Matteo Matteucci"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04533",
    "paper_id": "2305.04533",
    "abstract": "\n        In this paper, we propose MPC (Modular Prompted Chatbot), a new approach for creating high-quality conversational agents without the need for fine-tuning. Our method utilizes pre-trained large language models (LLMs) as individual modules for long-term consistency and flexibility, by using techniques such as few-shot prompting, chain-of-thought (CoT), and external memory. Our human evaluation results show that MPC is on par with fine-tuned chatbot models in open-domain conversations, making it an effective solution for creating consistent and engaging chatbots.\n        \u25b3 Less\n      ",
    "title": "Prompted LLMs as Chatbot Modules for Long Open-domain Conversation",
    "date": "8 May, 2023",
    "authors": [
      "Gibbeum Lee",
      " Volker Hartmann",
      " Jongho Park",
      " Dimitris Papailiopoulos",
      " Kangwook Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.01618",
    "paper_id": "2303.01618",
    "abstract": "\n        Active inference is a theory of perception, learning and decision making, which can be applied to neuroscience, robotics, and machine learning. Recently, reasearch has been taking place to scale up this framework using Monte-Carlo tree search and deep learning. The goal of this activity is to solve more complicated tasks using deep active inference. First, we review the existing literature, then, we progresively build a deep active inference agent. For two agents, we have experimented with five definitions of the expected free energy and three different action selection strategies. According to our experiments, the models able to solve the dSprites environment are the ones that maximise rewards. Finally, we compare the similarity of the representation learned by the layers of various agents using centered kernel alignment. Importantly, the agent maximising reward and the agent minimising expected free energy learn very similar representations except for the last layer of the critic network (reflecting the difference in learning objective), and the variance layers of the transition and encoder networks. We found that the reward maximising agent is a lot more certain than the agent minimising expected free energy. This is because the agent minimising expected free energy always picks the action down, and does not gather enough data for the other actions. In contrast, the agent maximising reward, keeps on selecting the actions left and right, enabling it to successfully solve the task. The only difference between those two agents is the epistemic value, which aims to make the outputs of the transition and encoder networks as close as possible. Thus, the agent minimising expected free energy picks a single action (down), and becomes an expert at predicting the future when selecting this action. This makes the KL divergence between the output of the transition and encoder networks small.\n        \u25b3 Less\n      ",
    "title": "Deconstructing deep active inference",
    "date": "8 May, 2023",
    "authors": [
      "Th\u00e9ophile Champion",
      " Marek Grze\u015b",
      " Lisa Bonheme",
      " Howard Bowman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05525",
    "paper_id": "2305.05525",
    "abstract": "\n        Explainable artificial intelligence (AI) techniques are increasingly being explored to provide insights into why AI and machine learning (ML) models provide a certain outcome in various applications. However, there has been limited exploration of explainable AI techniques on time-series data, especially in the healthcare context. In this paper, we describe a threshold-based method that utilizes a weakly supervised model and a gradient-based explainable AI technique (i.e. saliency map) and explore its feasibility to identify salient frames of time-series data. Using the dataset from 15 post-stroke survivors performing three upper-limb exercises and labels on whether a compensatory motion is observed or not, we implemented a feed-forward neural network model and utilized gradients of each input on model outcomes to identify salient frames that involve compensatory motions. According to the evaluation using frame-level annotations, our approach achieved a recall of 0.96 and an F2-score of 0.91. Our results demonstrated the potential of a gradient-based explainable AI technique (e.g. saliency map) for time-series data, such as highlighting the frames of a video that therapists should focus on reviewing and reducing the efforts on frame-level labeling for model training.\n        \u25b3 Less\n      ",
    "title": "Exploring a Gradient-based Explainable AI Technique for Time-Series Data: A Case Study of Assessing Stroke Rehabilitation Exercises",
    "date": "8 May, 2023",
    "authors": [
      "Min Hun Lee",
      " Yi Jing Choy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.11007",
    "paper_id": "2207.11007",
    "abstract": "\n        Changes, planned or unexpected, are common during the execution of real-life processes. Detecting these changes is a must for optimizing the performance of organizations running such processes. Most of the algorithms present in the state-of-the-art focus on the detection of sudden changes, leaving aside other types of changes. In this paper, we will focus on the automatic detection of gradual drifts, a special type of change, in which the cases of two models overlap during a period of time. The proposed algorithm relies on conformance checking metrics to carry out the automatic detection of the changes, performing also a fully automatic classification of these changes into sudden or gradual. The approach has been validated with a synthetic dataset consisting of 120 logs with different distributions of changes, getting better results in terms of detection and classification accuracy, delay and change region overlapping than the main state-of-the-art algorithms.\n        \u25b3 Less\n      ",
    "title": "Gradual Drift Detection in Process Models Using Conformance Metrics",
    "date": "8 May, 2023",
    "authors": [
      "Victor Gallego-Fontenla",
      " Juan C. Vidal",
      " Manuel Lama"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04553",
    "paper_id": "2305.04553",
    "abstract": "\n        Evolutionary algorithms are known to be robust to noise in the evaluation of the fitness. In particular, larger offspring population sizes often lead to strong robustness. We analyze to what extent the (1+(\u03bb,\u03bb))(1+(\u03bb,\u03bb)) genetic algorithm is robust to noise. This algorithm also works with larger offspring population sizes, but an intermediate selection step and a non-standard use of crossover as repair mechanism could render this algorithm less robust than, e.g., the simple (1+\u03bb)(1+\u03bb) evolutionary algorithm. Our experimental analysis on several classic benchmark problems shows that this difficulty does not arise. Surprisingly, in many situations this algorithm is even more robust to noise than the (1+\u03bb)(1+\u03bb)~EA.\n        \u25b3 Less\n      ",
    "title": "Larger Offspring Populations Help the \n(1+(\u03bb,\u03bb))\n Genetic Algorithm to Overcome the Noise",
    "date": "8 May, 2023",
    "authors": [
      "Alexandra Ivanova",
      " Denis Antipov",
      " Benjamin Doerr"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04567",
    "paper_id": "2305.04567",
    "abstract": "\n        The field of education has undergone a significant transformation due to the rapid advancements in Artificial Intelligence (AI). Among the various AI technologies, Knowledge Graphs (KGs) using Natural Language Processing (NLP) have emerged as powerful visualization tools for integrating multifaceted information. In the context of university education, the availability of numerous specialized courses and complicated learning resources often leads to inferior learning outcomes for students. In this paper, we propose an automated framework for knowledge extraction, visual KG construction, and graph fusion, tailored for the major of Electronic Information. Furthermore, we perform data analysis to investigate the correlation degree and relationship between courses, rank hot knowledge concepts, and explore the intersection of courses. Our objective is to enhance the learning efficiency of students and to explore new educational paradigms enabled by AI. The proposed framework is expected to enable students to better understand and appreciate the intricacies of their field of study by providing them with a comprehensive understanding of the relationships between the various concepts and courses.\n        \u25b3 Less\n      ",
    "title": "Multi-source Education Knowledge Graph Construction and Fusion for College Curricula",
    "date": "8 May, 2023",
    "authors": [
      "Zeju Li",
      " Linya Cheng",
      " Chunhong Zhang",
      " Xinning Zhu",
      " Hui Zhao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04589",
    "paper_id": "2305.04589",
    "abstract": "\n        For the assignment problem where multiple indivisible items are allocated to a group of agents given their ordinal preferences, we design randomized mechanisms that satisfy first-choice maximality (FCM), i.e., maximizing the number of agents assigned their first choices, together with Pareto efficiency (PE). Our mechanisms also provide guarantees of ex-ante and ex-post fairness. The generalized eager Boston mechanism is ex-ante envy-free, and ex-post envy-free up to one item (EF1). The generalized probabilistic Boston mechanism is also ex-post EF1, and satisfies ex-ante efficiency instead of fairness. We also show that no strategyproof mechanism satisfies ex-post PE, EF1, and FCM simultaneously. In doing so, we expand the frontiers of simultaneously providing efficiency and both ex-ante and ex-post fairness guarantees for the assignment problem.\n        \u25b3 Less\n      ",
    "title": "First-Choice Maximality Meets Ex-ante and Ex-post Fairness",
    "date": "8 May, 2023",
    "authors": [
      "Xiaoxi Guo",
      " Sujoy Sikdar",
      " Lirong Xia",
      " Yongzhi Cao",
      " Hanpin Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05383",
    "paper_id": "2305.05383",
    "abstract": "\n        Code execution is a fundamental aspect of programming language semantics that reflects the exact behavior of the code. However, most pre-trained models for code intelligence ignore the execution trace and only rely on source code and syntactic structures. In this paper, we investigate how well pre-trained models can understand and perform code execution. We develop a mutation-based data augmentation technique to create a large-scale and realistic Python dataset and task for code execution, which challenges existing models such as Codex. We then present CodeExecutor, a Transformer model that leverages code execution pre-training and curriculum learning to enhance its semantic comprehension. We evaluate CodeExecutor on code execution and show its promising performance and limitations. We also demonstrate its potential benefits for code intelligence tasks such as zero-shot code-to-code search and text-to-code generation. Our analysis provides insights into the learning and generalization abilities of pre-trained models for code execution.\n        \u25b3 Less\n      ",
    "title": "Code Execution with Pre-trained Language Models",
    "date": "8 May, 2023",
    "authors": [
      "Chenxiao Liu",
      " Shuai Lu",
      " Weizhu Chen",
      " Daxin Jiang",
      " Alexey Svyatkovskiy",
      " Shengyu Fu",
      " Neel Sundaresan",
      " Nan Duan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.09420",
    "paper_id": "2212.09420",
    "abstract": "\n        The task of generating code from a natural language description, or NL2Code, is considered a pressing and significant challenge in code intelligence. Thanks to the rapid development of pre-training techniques, surging large language models are being proposed for code, sparking the advances in NL2Code. To facilitate further research and applications in this field, in this paper, we present a comprehensive survey of 27 existing large language models for NL2Code, and also review benchmarks and metrics. We provide an intuitive comparison of all existing models on the HumanEval benchmark. Through in-depth observation and analysis, we provide some insights and conclude that the key factors contributing to the success of large language models for NL2Code are \"Large Size, Premium Data, Expert Tuning\". In addition, we discuss challenges and opportunities regarding the gap between models and humans. We also create a website https://nl2code.github.io to track the latest progress through crowd-sourcing. To the best of our knowledge, this is the first survey of large language models for NL2Code, and we believe it will contribute to the ongoing development of the field.\n        \u25b3 Less\n      ",
    "title": "Large Language Models Meet NL2Code: A Survey",
    "date": "8 May, 2023",
    "authors": [
      "Daoguang Zan",
      " Bei Chen",
      " Fengji Zhang",
      " Dianjie Lu",
      " Bingchao Wu",
      " Bei Guan",
      " Yongji Wang",
      " Jian-Guang Lou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.10017",
    "paper_id": "2212.10017",
    "abstract": "\n        Analysis of pre-trained code models also has revealed that they can effectively learn program syntax. However, these works are limited in analyzing code syntax and their distance-based approaches are not accurate due to the curse of high dimensionality. Furthermore, the study of the learnt program semantics of these models is rarely discussed. To further understand the code features learnt by these models, in this paper, we target two well-known representative code pre-trained models (i.e., CodeBERT and GraphCodeBERT) and devise a set of probing tasks for the syntax and semantics analysis. Specifically, on one hand, we design two probing tasks (i.e., syntax pair node prediction and token tagging prediction) to manipulate AST for the understanding of learnt program syntax. On the other hand, we design two tasks (i.e., semantic relationship prediction and semantic propagation prediction(inGraph) ) on the constructed control flow graph (CFG), data dependency graph (DDG) and control dependency graph (CDG) for the learnt program semantic analysis. In addition, to understand which kind of program semantics these pre-trained models can comprehend well, we conduct the statistical analysis for attention weights learnt by different heads and layers. Through extensive analysis in terms of program syntax and semantics, we have the following findings: 1) Both CodeBERT and GraphCodeBERT can learn the program syntax well. 2) Both CodeBERT and GraphCodeBERT can learn program semantics to different extents. GraphCodeBERT is superior to CodeBERT in learning program control flow and data dependency information but has a similar capability to CodeBERT in learning control dependency information. 3) Both CodeBERT and GraphCodeBERT can capture program semantics in the final layer of representation, but different attention heads and layers exhibit different roles in learning program semantics.\n        \u25b3 Less\n      ",
    "title": "Are Code Pre-trained Models Powerful to Learn Code Syntax and Semantics?",
    "date": "8 May, 2023",
    "authors": [
      "Wei Ma",
      " Mengjie Zhao",
      " Xiaofei Xie",
      " Qiang Hu",
      " Shangqing Liu",
      " Jie Zhang",
      " Wenhan Wang",
      " Yang Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07665",
    "paper_id": "2305.07665",
    "abstract": "\n        As the name suggests, affective computing aims to recognize human emotions, sentiments, and feelings. There is a wide range of fields that study affective computing, including languages, sociology, psychology, computer science, and physiology. However, no research has ever been done to determine how machine learning (ML) and mixed reality (XR) interact together. This paper discusses the significance of affective computing, as well as its ideas, conceptions, methods, and outcomes. By using approaches of ML and XR, we survey and discuss recent methodologies in affective computing. We survey the state-of-the-art approaches along with current affective data resources. Further, we discuss various applications where affective computing has a significant impact, which will aid future scholars in gaining a better understanding of its significance and practical relevance.\n        \u25b3 Less\n      ",
    "title": "A Comprehensive Survey on Affective Computing; Challenges, Trends, Applications, and Future Directions",
    "date": "8 May, 2023",
    "authors": [
      "Sitara Afzal",
      " Haseeb Ali Khan",
      " Imran Ullah Khan",
      " Md. Jalil Piran",
      " Jong Weon Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.01997",
    "paper_id": "2305.01997",
    "abstract": "\n        Deep learning-based methods have spearheaded the automatic analysis of echocardiographic images, taking advantage of the publication of multiple open access datasets annotated by experts (CAMUS being one of the largest public databases). However, these models are still considered unreliable by clinicians due to unresolved issues concerning i) the temporal consistency of their predictions, and ii) their ability to generalize across datasets. In this context, we propose a comprehensive comparison between the current best performing methods in medical/echocardiographic image segmentation, with a particular focus on temporal consistency and cross-dataset aspects. We introduce a new private dataset, named CARDINAL, of apical two-chamber and apical four-chamber sequences, with reference segmentation over the full cardiac cycle. We show that the proposed 3D nnU-Net outperforms alternative 2D and recurrent segmentation methods. We also report that the best models trained on CARDINAL, when tested on CAMUS without any fine-tuning, still manage to perform competitively with respect to prior methods. Overall, the experimental results suggest that with sufficient training data, 3D nnU-Net could become the first automated tool to finally meet the standards of an everyday clinical device.\n        \u25b3 Less\n      ",
    "title": "Extraction of volumetric indices from echocardiography: which deep learning solution for clinical use?",
    "date": "8 May, 2023",
    "authors": [
      "Hang Jung Ling",
      " Nathan Painchaud",
      " Pierre-Yves Courand",
      " Pierre-Marc Jodoin",
      " Damien Garcia",
      " Olivier Bernard"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02012",
    "paper_id": "2305.02012",
    "abstract": "\n        eXplainable artificial intelligence (XAI) methods have emerged to convert the black box of machine learning models into a more digestible form. These methods help to communicate how the model works with the aim of making machine learning models more transparent and increasing the trust of end-users into their output. SHapley Additive exPlanations (SHAP) and Local Interpretable Model Agnostic Explanation (LIME) are two widely used XAI methods particularly with tabular data. In this commentary piece, we discuss the way the explainability metrics of these two methods are generated and propose a framework for interpretation of their outputs, highlighting their weaknesses and strengths.\n        \u25b3 Less\n      ",
    "title": "Commentary on explainable artificial intelligence methods: SHAP and LIME",
    "date": "8 May, 2023",
    "authors": [
      "Ahmed Salih",
      " Zahra Raisi-Estabragh",
      " Ilaria Boscolo Galazzo",
      " Petia Radeva",
      " Steffen E. Petersen",
      " Gloria Menegaz",
      " Karim Lekadir"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04631",
    "paper_id": "2305.04631",
    "abstract": "\n        The work covers the development and explainability of machine learning models for predicting political leanings through parliamentary transcriptions. We concentrate on the Slovenian parliament and the heated debate on the European migrant crisis, with transcriptions from 2014 to 2020. We develop both classical machine learning and transformer language models to predict the left- or right-leaning of parliamentarians based on their given speeches on the topic of migrants. With both types of models showing great predictive success, we continue with explaining their decisions. Using explainability techniques, we identify keywords and phrases that have the strongest influence in predicting political leanings on the topic, with left-leaning parliamentarians using concepts such as people and unity and speak about refugees, and right-leaning parliamentarians using concepts such as nationality and focus more on illegal migrants. This research is an example that understanding the reasoning behind predictions can not just be beneficial for AI engineers to improve their models, but it can also be helpful as a tool in the qualitative analysis steps in interdisciplinary research.\n        \u25b3 Less\n      ",
    "title": "XAI in Computational Linguistics: Understanding Political Leanings in the Slovenian Parliament",
    "date": "8 May, 2023",
    "authors": [
      "Bojan Evkoski",
      " Senja Pollak"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04638",
    "paper_id": "2305.04638",
    "abstract": "\n        We study the causal bandit problem that entails identifying a near-optimal intervention from a specified set AA of (possibly non-atomic) interventions over a given causal graph. Here, an optimal intervention in A{A} is one that maximizes the expected value for a designated reward variable in the graph, and we use the standard notion of simple regret to quantify near optimality. Considering Bernoulli random variables and for causal graphs on NN vertices with constant in-degree, prior work has achieved a worst case guarantee of O\u02dc(N/T\u2212\u2212\u221a)\\widetilde{O} (N/\\sqrt{T}) for simple regret. The current work utilizes the idea of covering interventions (which are not necessarily contained within A{A}) and establishes a simple regret guarantee of O\u02dc(N/T\u2212\u2212\u2212\u2212\u221a)\\widetilde{O}(\\sqrt{N/T}). Notably, and in contrast to prior work, our simple regret bound depends only on explicit parameters of the problem instance. We also go beyond prior work and achieve a simple regret guarantee for causal graphs with unobserved variables. Further, we perform experiments to show improvements over baselines in this setting.\n        \u25b3 Less\n      ",
    "title": "Learning Good Interventions in Causal Graphs via Covering",
    "date": "8 May, 2023",
    "authors": [
      "Ayush Sawarni",
      " Rahul Madhavan",
      " Gaurav Sinha",
      " Siddharth Barman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02893",
    "paper_id": "2305.02893",
    "abstract": "\n        For many driving safety applications, it is of great importance to accurately register LiDAR point clouds generated on distant moving vehicles. However, such point clouds have extremely different point density and sensor perspective on the same object, making registration on such point clouds very hard. In this paper, we propose a novel feature extraction framework, called APR, for online distant point cloud registration. Specifically, APR leverages an autoencoder design, where the autoencoder reconstructs a denser aggregated point cloud with several frames instead of the original single input point cloud. Our design forces the encoder to extract features with rich local geometry information based on one single input point cloud. Such features are then used for online distant point cloud registration. We conduct extensive experiments against state-of-the-art (SOTA) feature extractors on KITTI and nuScenes datasets. Results show that APR outperforms all other extractors by a large margin, increasing average registration recall of SOTA extractors by 7.1% on LoKITTI and 4.6% on LoNuScenes. Code is available at https://github.com/liuQuan98/APR.\n        \u25b3 Less\n      ",
    "title": "APR: Online Distant Point Cloud Registration Through Aggregated Point Cloud Reconstruction",
    "date": "8 May, 2023",
    "authors": [
      "Quan Liu",
      " Yunsong Zhou",
      " Hongzi Zhu",
      " Shan Chang",
      " Minyi Guo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04658",
    "paper_id": "2305.04658",
    "abstract": "\n        Graph Contrastive Learning (GCL) is an effective way to learn generalized graph representations in a self-supervised manner, and has grown rapidly in recent years. However, the underlying community semantics has not been well explored by most previous GCL methods. Research that attempts to leverage communities in GCL regards them as having the same influence on the graph, leading to extra representation errors. To tackle this issue, we define ''community strength'' to measure the difference of influence among communities. Under this premise, we propose a Community-Strength-enhanced Graph Contrastive Learning (CSGCL) framework to preserve community strength throughout the learning process. Firstly, we present two novel graph augmentation methods, Communal Attribute Voting (CAV) and Communal Edge Dropping (CED), where the perturbations of node attributes and edges are guided by community strength. Secondly, we propose a dynamic ''Team-up'' contrastive learning scheme, where community strength is used to progressively fine-tune the contrastive objective. We report extensive experiment results on three downstream tasks: node classification, node clustering, and link prediction. CSGCL achieves state-of-the-art performance compared with other GCL methods, validating that community strength brings effectiveness and generality to graph representations. Our code is available at https://github.com/HanChen-HUST/CSGCL.\n        \u25b3 Less\n      ",
    "title": "CSGCL: Community-Strength-Enhanced Graph Contrastive Learning",
    "date": "8 May, 2023",
    "authors": [
      "Han Chen",
      " Ziwen Zhao",
      " Yuhua Li",
      " Yixiong Zou",
      " Ruixuan Li",
      " Rui Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03259",
    "paper_id": "2305.03259",
    "abstract": "\n        Clothes grasping and unfolding is a core step in robotic-assisted dressing. Most existing works leverage depth images of clothes to train a deep learning-based model to recognize suitable grasping points. These methods often utilize physics engines to synthesize depth images to reduce the cost of real labeled data collection. However, the natural domain gap between synthetic and real images often leads to poor performance of these methods on real data. Furthermore, these approaches often struggle in scenarios where grasping points are occluded by the clothing item itself. To address the above challenges, we propose a novel Bi-directional Fractal Cross Fusion Network (BiFCNet) for semantic segmentation, enabling recognition of graspable regions in order to provide more possibilities for grasping. Instead of using depth images only, we also utilize RGB images with rich color features as input to our network in which the Fractal Cross Fusion (FCF) module fuses RGB and depth data by considering global complex features based on fractal geometry. To reduce the cost of real data collection, we further propose a data augmentation method based on an adversarial strategy, in which the color and geometric transformations simultaneously process RGB and depth data while maintaining the label correspondence. Finally, we present a pipeline for clothes grasping and unfolding from the perspective of semantic segmentation, through the addition of a strategy for grasp point selection from segmentation regions based on clothing flatness measures, while taking into account the grasping direction. We evaluate our BiFCNet on the public dataset NYUDv2 and obtained comparable performance to current state-of-the-art models. We also deploy our model on a Baxter robot, running extensive grasping and unfolding experiments as part of our ablation studies, achieving an 84% success rate.\n        \u25b3 Less\n      ",
    "title": "Clothes Grasping and Unfolding Based on RGB-D Semantic Segmentation",
    "date": "8 May, 2023",
    "authors": [
      "Xingyu Zhu",
      " Xin Wang",
      " Jonathan Freer",
      " Hyung Jin Chang",
      " Yixing Gao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.09266",
    "paper_id": "2303.09266",
    "abstract": "\n        Dynamic early exiting has been proven to improve the inference speed of the pre-trained language model like BERT. However, all samples must go through all consecutive layers before early exiting and more complex samples usually go through more layers, which still exists redundant computation. In this paper, we propose a novel dynamic early exiting combined with layer skipping for BERT inference named SmartBERT, which adds a skipping gate and an exiting operator into each layer of BERT. SmartBERT can adaptively skip some layers and adaptively choose whether to exit. Besides, we propose cross-layer contrastive learning and combine it into our training phases to boost the intermediate layers and classifiers which would be beneficial for early exiting. To keep the consistent usage of skipping gates between training and inference phases, we propose a hard weight mechanism during training phase. We conduct experiments on eight classification datasets of the GLUE benchmark. Experimental results show that SmartBERT achieves 2-3x computation reduction with minimal accuracy drops compared with BERT and our method outperforms previous methods in both efficiency and accuracy. Moreover, in some complex datasets like RTE and WNLI, we prove that the early exiting based on entropy hardly works, and the skipping mechanism is essential for reducing computation.\n        \u25b3 Less\n      ",
    "title": "SmartBERT: A Promotion of Dynamic Early Exiting Mechanism for Accelerating BERT Inference",
    "date": "8 May, 2023",
    "authors": [
      "Boren Hu",
      " Yun Zhu",
      " Jiacheng Li",
      " Siliang Tang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04692",
    "paper_id": "2305.04692",
    "abstract": "\n        We consider a service robot in a household environment given a sequence of high-level tasks one at a time. Most existing task planners, lacking knowledge of what they may be asked to do next, solve each task in isolation and so may unwittingly introduce side effects that make subsequent tasks more costly. In order to reduce the overall cost of completing all tasks, we consider that the robot must anticipate the impact its actions could have on future tasks. Thus, we propose anticipatory planning: an approach in which estimates of the expected future cost, from a graph neural network, augment model-based task planning. Our approach guides the robot towards behaviors that encourage preparation and organization, reducing overall costs in long-lived planning scenarios. We evaluate our method on blockworld environments and show that our approach reduces the overall planning costs by 5% as compared to planning without anticipatory planning. Additionally, if given an opportunity to prepare the environment in advance (a special case of anticipatory planning), our planner improves overall cost by 11%.\n        \u25b3 Less\n      ",
    "title": "Anticipatory Planning: Improving Long-Lived Planning by Estimating Expected Cost of Future Tasks",
    "date": "8 May, 2023",
    "authors": [
      "Roshan Dhakal",
      " Md Ridwan Hossain Talukder",
      " Gregory J. Stein"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04699",
    "paper_id": "2305.04699",
    "abstract": "\n        A machine-learned system that is fair in static decision-making tasks may have biased societal impacts in the long-run. This may happen when the system interacts with humans and feedback patterns emerge, reinforcing old biases in the system and creating new biases. While existing works try to identify and mitigate long-run biases through smart system design, we introduce techniques for monitoring fairness in real time. Our goal is to build and deploy a monitor that will continuously observe a long sequence of events generated by the system in the wild, and will output, with each event, a verdict on how fair the system is at the current point in time. The advantages of monitoring are two-fold. Firstly, fairness is evaluated at run-time, which is important because unfair behaviors may not be eliminated a priori, at design-time, due to partial knowledge about the system and the environment, as well as uncertainties and dynamic changes in the system and the environment, such as the unpredictability of human behavior. Secondly, monitors are by design oblivious to how the monitored system is constructed, which makes them suitable to be used as trusted third-party fairness watchdogs. They function as computationally lightweight statistical estimators, and their correctness proofs rely on the rigorous analysis of the stochastic process that models the assumptions about the underlying dynamics of the system. We show, both in theory and experiments, how monitors can warn us (1) if a bank's credit policy over time has created an unfair distribution of credit scores among the population, and (2) if a resource allocator's allocation policy over time has made unfair allocations. Our experiments demonstrate that the monitors introduce very low overhead. We believe that runtime monitoring is an important and mathematically rigorous new addition to the fairness toolbox.\n        \u25b3 Less\n      ",
    "title": "Runtime Monitoring of Dynamic Fairness Properties",
    "date": "8 May, 2023",
    "authors": [
      "Thomas A. Henzinger",
      " Mahyar Karimi",
      " Konstantin Kueffner",
      " Kaushik Mallik"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.14177",
    "paper_id": "2212.14177",
    "abstract": "\n        Artificial Intelligence (AI) has become commonplace to solve routine everyday tasks. Because of the exponential growth in medical imaging data volume and complexity, the workload on radiologists is steadily increasing. We project that the gap between the number of imaging exams and the number of expert radiologist readers required to cover this increase will continue to expand, consequently introducing a demand for AI-based tools that improve the efficiency with which radiologists can comfortably interpret these exams. AI has been shown to improve efficiency in medical-image generation, processing, and interpretation, and a variety of such AI models have been developed across research labs worldwide. However, very few of these, if any, find their way into routine clinical use, a discrepancy that reflects the divide between AI research and successful AI translation. To address the barrier to clinical deployment, we have formed MONAI Consortium, an open-source community which is building standards for AI deployment in healthcare institutions, and developing tools and infrastructure to facilitate their implementation. This report represents several years of weekly discussions and hands-on problem solving experience by groups of industry experts and clinicians in the MONAI Consortium. We identify barriers between AI-model development in research labs and subsequent clinical deployment and propose solutions. Our report provides guidance on processes which take an imaging AI model from development to clinical implementation in a healthcare institution. We discuss various AI integration points in a clinical Radiology workflow. We also present a taxonomy of Radiology AI use-cases. Through this report, we intend to educate the stakeholders in healthcare and AI (AI researchers, radiologists, imaging informaticists, and regulators) about cross-disciplinary challenges and possible solutions.\n        \u25b3 Less\n      ",
    "title": "Current State of Community-Driven Radiological AI Deployment in Medical Imaging",
    "date": "8 May, 2023",
    "authors": [
      "Vikash Gupta",
      " Barbaros Selnur Erdal",
      " Carolina Ramirez",
      " Ralf Floca",
      " Laurence Jackson",
      " Brad Genereaux",
      " Sidney Bryson",
      " Christopher P Bridge",
      " Jens Kleesiek",
      " Felix Nensa",
      " Rickmer Braren",
      " Khaled Younis",
      " Tobias Penzkofer",
      " Andreas Michael Bucher",
      " Ming Melvin Qin",
      " Gigon Bae",
      " Hyeonhoon Lee",
      " M. Jorge Cardoso",
      " Sebastien Ourselin",
      " Eric Kerfoot",
      " Rahul Choudhury",
      " Richard D. White",
      " Tessa Cook",
      " David Bericat",
      " Matthew Lungren ",
      " et al. (2 additional authors not shown)"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04724",
    "paper_id": "2305.04724",
    "abstract": "\n        In the modern world, one of the most severe eye infections brought on by diabetes is known as diabetic retinopathy, which will result in retinal damage, and, thus, lead to blindness. Diabetic retinopathy can be well treated with early diagnosis. Retinal fundus images of humans are used to screen for lesions in the retina. However, detecting DR in the early stages is challenging due to the minimal symptoms. Furthermore, the occurrence of diseases linked to vascular anomalies brought on by DR aids in diagnosing the condition. Nevertheless, the resources required for manually identifying the lesions are high. Similarly, training for Convolutional Neural Networks is more time-consuming. This proposed research aims to improve diabetic retinopathy diagnosis by developing an enhanced deep learning model for timely DR identification that is potentially more accurate than existing CNN-based models. The proposed model will detect various lesions from retinal images in the early stages. First, characteristics are retrieved from the retinal fundus picture and put into the EDLM for classification. For dimensionality reduction, EDLM is used. Additionally, the classification and feature extraction processes are optimized using the stochastic gradient descent optimizer. The EDLM effectiveness is assessed on the KAG GLE dataset with 3459 retinal images, and results are compared over VGG16, VGG19, RESNET18, RESNET34, and RESNET50.\n        \u25b3 Less\n      ",
    "title": "Strategy for Rapid Diabetic Retinopathy Exposure Based on Enhanced Feature Extraction Processing",
    "date": "8 May, 2023",
    "authors": [
      "V. Banupriya",
      " S. Anusuya"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04727",
    "paper_id": "2305.04727",
    "abstract": "\n        Deploying reinforcement learning agents in the real world can be challenging due to the risks associated with learning through trial and error. We propose a task-agnostic method that leverages small sets of safe and unsafe demonstrations to improve the safety of RL agents during learning. The method compares the current trajectory of the agent with both sets of demonstrations at every step, and filters the trajectory if it resembles the unsafe demonstrations. We perform ablation studies on different filtering strategies and investigate the impact of the number of demonstrations on performance. Our method is compatible with any stand-alone RL algorithm and can be applied to any task. We evaluate our method on three tasks from OpenAI Gym's Mujoco benchmark and two state-of-the-art RL algorithms. The results demonstrate that our method significantly reduces the crash rate of the agent while converging to, and in most cases even improving, the performance of the stand-alone agent.\n        \u25b3 Less\n      ",
    "title": "DEFENDER: DTW-Based Episode Filtering Using Demonstrations for Enhancing RL Safety",
    "date": "8 May, 2023",
    "authors": [
      "Andr\u00e9 Correia",
      " Lu\u00eds Alexandre"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04746",
    "paper_id": "2305.04746",
    "abstract": "\n        Randomized smoothing is a technique for providing provable robustness guarantees against adversarial attacks while making minimal assumptions about a classifier. This method relies on taking a majority vote of any base classifier over multiple noise-perturbed inputs to obtain a smoothed classifier, and it remains the tool of choice to certify deep and complex neural network models. Nonetheless, non-trivial performance of such smoothed classifier crucially depends on the base model being trained on noise-augmented data, i.e., on a smoothed input distribution. While widely adopted in practice, it is still unclear how this noisy training of the base classifier precisely affects the risk of the robust smoothed classifier, leading to heuristics and tricks that are poorly understood. In this work we analyze these trade-offs theoretically in a binary classification setting, proving that these common observations are not universal. We show that, without making stronger distributional assumptions, no benefit can be expected from predictors trained with noise-augmentation, and we further characterize distributions where such benefit is obtained. Our analysis has direct implications to the practical deployment of randomized smoothing, and we illustrate some of these via experiments on CIFAR-10 and MNIST, as well as on synthetic datasets.\n        \u25b3 Less\n      ",
    "title": "Understanding Noise-Augmented Training for Randomized Smoothing",
    "date": "8 May, 2023",
    "authors": [
      "Ambar Pal",
      " Jeremias Sulam"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04750",
    "paper_id": "2305.04750",
    "abstract": "\n        Model-based reinforcement learning (MBRL) techniques have recently yielded promising results for real-world autonomous racing using high-dimensional observations. MBRL agents, such as Dreamer, solve long-horizon tasks by building a world model and planning actions by latent imagination. This approach involves explicitly learning a model of the system dynamics and using it to learn the optimal policy for continuous control over multiple timesteps. As a result, MBRL agents may converge to sub-optimal policies if the world model is inaccurate. To improve state estimation for autonomous racing, this paper proposes a self-supervised sensor fusion technique that combines egocentric LiDAR and RGB camera observations collected from the F1TENTH Gym. The zero-shot performance of MBRL agents is empirically evaluated on unseen tracks and against a dynamic obstacle. This paper illustrates that multimodal perception improves robustness of the world model without requiring additional training data. The resulting multimodal Dreamer agent safely avoided collisions and won the most races compared to other tested baselines in zero-shot head-to-head autonomous racing.\n        \u25b3 Less\n      ",
    "title": "Sense, Imagine, Act: Multimodal Perception Improves Model-Based Reinforcement Learning for Head-to-Head Autonomous Racing",
    "date": "8 May, 2023",
    "authors": [
      "Elena Shrestha",
      " Chetan Reddy",
      " Hanxi Wan",
      " Yulun Zhuang",
      " Ram Vasudevan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15323",
    "paper_id": "2305.15323",
    "abstract": "\n        Artificial intelligence (AI) and machine learning have changed the nature of scientific inquiry in recent years. Of these, the development of virtual assistants has accelerated greatly in the past few years, with ChatGPT becoming a prominent AI language model. In this study, we examine the foundations, vision, research challenges of ChatGPT. This article investigates into the background and development of the technology behind it, as well as its popular applications. Moreover, we discuss the advantages of bringing everything together through ChatGPT and Internet of Things (IoT). Further, we speculate on the future of ChatGPT by considering various possibilities for study and development, such as energy-efficiency, cybersecurity, enhancing its applicability to additional technologies (Robotics and Computer Vision), strengthening human-AI communications, and bridging the technological gap. Finally, we discuss the important ethics and current trends of ChatGPT.\n        \u25b3 Less\n      ",
    "title": "ChatGPT: Vision and Challenges",
    "date": "8 May, 2023",
    "authors": [
      "Sukhpal Singh Gill",
      " Rupinder Kaur"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06163",
    "paper_id": "2305.06163",
    "abstract": "\n        Automated feedback as students answer open-ended math questions has significant potential in improving learning outcomes at large scale. A key part of automated feedback systems is an error classification component, which identifies student errors and enables appropriate, predefined feedback to be deployed. Most existing approaches to error classification use a rule-based method, which has limited capacity to generalize. Existing data-driven methods avoid these limitations but specifically require mathematical expressions in student responses to be parsed into syntax trees. This requirement is itself a limitation, since student responses are not always syntactically valid and cannot be converted into trees. In this work, we introduce a flexible method for error classification using pre-trained large language models. We demonstrate that our method can outperform existing methods in algebra error classification, and is able to classify a larger set of student responses. Additionally, we analyze common classification errors made by our method and discuss limitations of automated error classification.\n        \u25b3 Less\n      ",
    "title": "Algebra Error Classification with Large Language Models",
    "date": "8 May, 2023",
    "authors": [
      "Hunter McNichols",
      " Mengxue Zhang",
      " Andrew Lan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.07865",
    "paper_id": "2303.07865",
    "abstract": "\n        This research is aimed to solve the tweet/user geolocation prediction task and provide a flexible methodology for the geotagging of textual big data. The suggested approach implements neural networks for natural language processing (NLP) to estimate the location as coordinate pairs (longitude, latitude) and two-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models has been finetuned on a Twitter dataset using pretrained Bidirectional Encoder Representations from Transformers (BERT) as base models. Performance metrics show a median error of fewer than 30 km on a worldwide-level, and fewer than 15 km on the US-level datasets for the models trained and evaluated on text features of tweets' content and metadata context. Our source code and data are available at https://github.com/K4TEL/geo-twitter.git\n        \u25b3 Less\n      ",
    "title": "Geolocation Predicting of Tweets Using BERT-Based Models",
    "date": "8 May, 2023",
    "authors": [
      "Kateryna Lutsai",
      " Christoph H. Lampert"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.09830",
    "paper_id": "2302.09830",
    "abstract": "\n        Statistical Relational Learning (SRL) integrates First-Order Logic (FOL) and probability theory for learning and inference over relational data. Probabilistic inference and learning in many SRL models can be reduced to Weighted First Order Model Counting (WFOMC). However, WFOMC is known to be intractable (#P1\u2212\\mathrm{\\#P_1-} complete). Hence, logical fragments that admit polynomial time WFOMC are of significant interest. Such fragments are called domain liftable. Recent line of works have shown the two-variable fragment of FOL, extended with counting quantifiers (C2\\mathrm{C^2}) to be domain-liftable. However, many properties of real-world data can not be modelled in C2\\mathrm{C^2}. In fact many ubiquitous properties of real-world data are inexressible in FOL. Acyclicity is one such property, found in citation networks, genealogy data, temporal data e.t.c. In this paper we aim to address this problem by investigating the domain liftability of directed acyclicity constraints. We show that the fragment C2\\mathrm{C^2} with a Directed Acyclic Graph (DAG) axiom, i.e., a predicate in the language is axiomatized to represent a DAG, is domain-liftable. We present a method based on principle of inclusion-exclusion for WFOMC of C2\\mathrm{C^2} formulas extended with DAG axioms.\n        \u25b3 Less\n      ",
    "title": "Weighted First Order Model Counting with Directed Acyclic Graph Axioms",
    "date": "8 May, 2023",
    "authors": [
      "Sagar Malhotra",
      " Luciano Serafini"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.11222",
    "paper_id": "2210.11222",
    "abstract": "\n        When applying differential privacy to sensitive data, we can often improve performance using external information such as other sensitive data, public data, or human priors. We propose to use the learning-augmented algorithms (or algorithms with predictions) framework -- previously applied largely to improve time complexity or competitive ratios -- as a powerful way of designing and analyzing privacy-preserving methods that can take advantage of such external information to improve utility. This idea is instantiated on the important task of multiple quantile release, for which we derive error guarantees that scale with a natural measure of prediction quality while (almost) recovering state-of-the-art prediction-independent guarantees. Our analysis enjoys several advantages, including minimal assumptions about the data, a natural way of adding robustness, and the provision of useful surrogate losses for two novel ``meta\" algorithms that learn predictions from other (potentially sensitive) data. We conclude with experiments on challenging tasks demonstrating that learning predictions across one or more instances can lead to large error reductions while preserving privacy.\n        \u25b3 Less\n      ",
    "title": "Learning-Augmented Private Algorithms for Multiple Quantile Release",
    "date": "8 May, 2023",
    "authors": [
      "Mikhail Khodak",
      " Kareem Amin",
      " Travis Dick",
      " Sergei Vassilvitskii"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.06109",
    "paper_id": "2303.06109",
    "abstract": "\n        We consider the problem of information aggregation in federated decision making, where a group of agents collaborate to infer the underlying state of nature without sharing their private data with the central processor or each other. We analyze the non-Bayesian social learning strategy in which agents incorporate their individual observations into their opinions (i.e., soft-decisions) with Bayes rule, and the central processor aggregates these opinions by arithmetic or geometric averaging. Building on our previous work, we establish that both pooling strategies result in asymptotic normality characterization of the system, which, for instance, can be utilized to derive approximate expressions for the error probability. We verify the theoretical findings with simulations and compare both strategies.\n        \u25b3 Less\n      ",
    "title": "On the Fusion Strategies for Federated Decision Making",
    "date": "8 May, 2023",
    "authors": [
      "Mert Kayaalp",
      " Yunus Inan",
      " Visa Koivunen",
      " Emre Telatar",
      " Ali H. Sayed"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04859",
    "paper_id": "2305.04859",
    "abstract": "\n        Position embeddings, encoding the positional relationships among tokens in text sequences, make great contributions to modeling local context features in Transformer-based pre-trained language models. However, in Extractive Question Answering, position embeddings trained with instances of varied context lengths may not perform well as we expect. Since the embeddings of rear positions are updated fewer times than the front position embeddings, the rear ones may not be properly trained. In this paper, we propose a simple but effective strategy, Random Padding, without any modifications to architectures of existing pre-trained language models. We adjust the token order of input sequences when fine-tuning, to balance the number of updating times of every position embedding. Experiments show that Random Padding can significantly improve model performance on the instances whose answers are located at rear positions, especially when models are trained on short contexts but evaluated on long contexts. Our code and data will be released for future research.\n        \u25b3 Less\n      ",
    "title": "A Frustratingly Easy Improvement for Position Embeddings via Random Padding",
    "date": "8 May, 2023",
    "authors": [
      "Mingxu Tao",
      " Yansong Feng",
      " Dongyan Zhao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.06761",
    "paper_id": "2302.06761",
    "abstract": "\n        Investigating whether pre-trained language models (LMs) can function as knowledge bases (KBs) has raised wide research interests recently. However, existing works focus on simple, triple-based, relational KBs, but omit more sophisticated, logic-based, conceptualised KBs such as OWL ontologies. To investigate an LM's knowledge of ontologies, we propose OntoLAMA, a set of inference-based probing tasks and datasets from ontology subsumption axioms involving both atomic and complex concepts. We conduct extensive experiments on ontologies of different domains and scales, and our results demonstrate that LMs encode relatively less background knowledge of Subsumption Inference (SI) than traditional Natural Language Inference (NLI) but can improve on SI significantly when a small number of samples are given. We will open-source our code and datasets.\n        \u25b3 Less\n      ",
    "title": "Language Model Analysis for Ontology Subsumption Inference",
    "date": "8 May, 2023",
    "authors": [
      "Yuan He",
      " Jiaoyan Chen",
      " Ernesto Jim\u00e9nez-Ruiz",
      " Hang Dong",
      " Ian Horrocks"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2201.09227",
    "paper_id": "2201.09227",
    "abstract": "\n        Language models (LMs) have introduced a major paradigm shift in Natural Language Processing (NLP) modeling where large pre-trained LMs became integral to most of the NLP tasks. The LMs are intelligent enough to find useful and relevant representations of the language without any supervision. Perhaps, these models are used to fine-tune typical NLP tasks with significantly high accuracy as compared to the traditional approaches. Conversely, the training of these models requires a massively large corpus that is a good representation of the language. English LMs generally perform better than their other language counterparts, due to the availability of massive English corpora. This work elaborates on the design and development of a large Arabic corpus. It consists of over 500 GB of Arabic cleaned text targeted at improving cross-domain knowledge and downstream generalization capability of large-scale language models. Moreover, the corpus is utilized in the training of a large Arabic LM. In order to evaluate the effectiveness of the LM, a number of typical NLP tasks are fine-tuned. The tasks demonstrate a significant boost from 4.5 to 8.5% when compared to tasks fine-tuned on multi-lingual BERT (mBERT). To the best of my knowledge, this is currently the largest clean and diverse Arabic corpus ever collected.\n        \u25b3 Less\n      ",
    "title": "A Large and Diverse Arabic Corpus for Language Modeling",
    "date": "8 May, 2023",
    "authors": [
      "Abbas Raza Ali",
      " Muhammad Ajmal Siddiqui",
      " Rema Algunaibet",
      " Hasan Raza Ali"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05531",
    "paper_id": "2305.05531",
    "abstract": "\n        Artificial Intelligence has gained a lot of traction in the recent years, with machine learning notably starting to see more applications across a varied range of fields. One specific machine learning application that is of interest to us is that of software safety and security, especially in the context of parallel programs. The issue of being able to detect concurrency bugs automatically has intrigued programmers for a long time, as the added layer of complexity makes concurrent programs more prone to failure. The development of such automatic detection tools provides considerable benefits to programmers in terms of saving time while debugging, as well as reducing the number of unexpected bugs. We believe machine learning may help achieve this goal by providing additional advantages over current approaches, in terms of both overall tool accuracy as well as programming language flexibility. However, due to the presence of numerous challenges specific to the machine learning approach (correctly labelling a sufficiently large dataset, finding the best model types/architectures and so forth), we have to approach each issue of developing such a tool separately. Therefore, the focus of this project is on comparing both common and recent machine learning approaches. We abstract away the complexity of procuring a labelled dataset of concurrent programs under the form of a synthetic dataset that we define and generate with the scope of simulating real-life (concurrent) programs. We formulate hypotheses about fundamental limits of various machine learning model types which we then validate by running extensive tests on our synthetic dataset. We hope that our findings provide more insight in the advantages and disadvantages of various model types when modelling programs using machine learning, as well as any other related field (e.g. NLP).\n        \u25b3 Less\n      ",
    "title": "Modelling Concurrency Bugs Using Machine Learning",
    "date": "8 May, 2023",
    "authors": [
      "Teodor Rares Begu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06884",
    "paper_id": "2305.06884",
    "abstract": "\n        We introduce the notion of a risk-limiting financial auditing (RLFA): given NN transactions, the goal is to estimate the total misstated monetary fraction~(m\u2217m^*) to a given accuracy \u03b5\u03b5, with confidence 1\u2212\u03b41-\u03b4. We do this by constructing new confidence sequences (CSs) for the weighted average of NN unknown values, based on samples drawn without replacement according to a (randomized) weighted sampling scheme. Using the idea of importance weighting to construct test martingales, we first develop a framework to construct CSs for arbitrary sampling strategies. Next, we develop methods to improve the quality of CSs by incorporating side information about the unknown values associated with each item. We show that when the side information is sufficiently predictive, it can directly drive the sampling. Addressing the case where the accuracy is unknown a priori, we introduce a method that incorporates side information via control variates. Crucially, our construction is adaptive: if the side information is highly predictive of the unknown misstated amounts, then the benefits of incorporating it are significant; but if the side information is uncorrelated, our methods learn to ignore it. Our methods recover state-of-the-art bounds for the special case when the weights are equal, which has already found applications in election auditing. The harder weighted case solves our more challenging problem of AI-assisted financial auditing.\n        \u25b3 Less\n      ",
    "title": "Risk-limiting Financial Audits via Weighted Sampling without Replacement",
    "date": "8 May, 2023",
    "authors": [
      "Shubhanshu Shekhar",
      " Ziyu Xu",
      " Zachary C. Lipton",
      " Pierre J. Liang",
      " Aaditya Ramdas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00050",
    "paper_id": "2305.00050",
    "abstract": "\n        The causal capabilities of large language models (LLMs) is a matter of significant debate, with critical implications for the use of LLMs in societally impactful domains such as medicine, science, law, and policy. We further our understanding of LLMs and their causal implications, considering the distinctions between different types of causal reasoning tasks, as well as the entangled threats of construct and measurement validity. LLM-based methods establish new state-of-the-art accuracies on multiple causal benchmarks. Algorithms based on GPT-3.5 and 4 outperform existing algorithms on a pairwise causal discovery task (97%, 13 points gain), counterfactual reasoning task (92%, 20 points gain), and actual causality (86% accuracy in determining necessary and sufficient causes in vignettes). At the same time, LLMs exhibit unpredictable failure modes and we provide some techniques to interpret their robustness.\n  Crucially, LLMs perform these causal tasks while relying on sources of knowledge and methods distinct from and complementary to non-LLM based approaches. Specifically, LLMs bring capabilities so far understood to be restricted to humans, such as using collected knowledge to generate causal graphs or identifying background causal context from natural language. We envision LLMs to be used alongside existing causal methods, as a proxy for human domain knowledge and to reduce human effort in setting up a causal analysis, one of the biggest impediments to the widespread adoption of causal methods. We also see existing causal methods as promising tools for LLMs to formalize, validate, and communicate their reasoning especially in high-stakes scenarios.\n  In capturing common sense and domain knowledge about causal mechanisms and supporting translation between natural language and formal methods, LLMs open new frontiers for advancing the research, practice, and adoption of causality.\n        \u25b3 Less\n      ",
    "title": "Causal Reasoning and Large Language Models: Opening a New Frontier for Causality",
    "date": "8 May, 2023",
    "authors": [
      "Emre K\u0131c\u0131man",
      " Robert Ness",
      " Amit Sharma",
      " Chenhao Tan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18086",
    "paper_id": "2305.18086",
    "abstract": "\n        The conversational artificial-intelligence (AI) technology ChatGPT has become one of the most widely used natural language processing tools. With thousands of published papers demonstrating its applications across various industries and fields, ChatGPT has sparked significant interest in the research community. Reviews of primary data have also begun to emerge. An overview of the available evidence from multiple reviews and studies could provide further insights, minimize redundancy, and identify areas where further research is needed. Objective: To evaluate the existing reviews and literature related to ChatGPT's applications and its potential impact on different fields by conducting a systematic review of reviews and bibliometric analysis of primary literature. Methods: PubMed, EuropePMC, Dimensions AI, medRxiv, bioRxiv, arXiv, and Google Scholar were searched for ChatGPT-related publications from 2022 to 4/30/2023. Studies including secondary data related to the application of ChatGPT were considered. Reporting and risk of bias assesment was performed using PRISMA guidelines. Results: A total of 305 unique records with potential relevance to the review were identified from a pool of over 2,000 original articles. After multi-step screening process, 11 reviews were selected, consisting of 9 reviews specifically focused on ChatGPT and 2 reviews on broader AI topics that also included discussions on ChatGPT. We also conducted bibliometric analysis of primary data. Conclusions: While AI has the potential to revolutionize various industries, further interdisciplinary research, customized integrations, and ethical innovation are necessary to address existing concerns and ensure its responsible use. Protocol Registration: PROSPERO registration no. CRD42023417336, DOI 10.17605/OSF.IO/87U6Q.\n        \u25b3 Less\n      ",
    "title": "The impact and applications of ChatGPT: a systematic review of literature reviews",
    "date": "8 May, 2023",
    "authors": [
      "Irene S. Gabashvili"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04923",
    "paper_id": "2305.04923",
    "abstract": "\n        Assessing the artness of AI-generated images continues to be a challenge within the realm of image generation. Most existing metrics cannot be used to perform instance-level and reference-free artness evaluation. This paper presents ArtScore, a metric designed to evaluate the degree to which an image resembles authentic artworks by artists (or conversely photographs), thereby offering a novel approach to artness assessment. We first blend pre-trained models for photo and artwork generation, resulting in a series of mixed models. Subsequently, we utilize these mixed models to generate images exhibiting varying degrees of artness with pseudo-annotations. Each photorealistic image has a corresponding artistic counterpart and a series of interpolated images that range from realistic to artistic. This dataset is then employed to train a neural network that learns to estimate quantized artness levels of arbitrary images. Extensive experiments reveal that the artness levels predicted by ArtScore align more closely with human artistic evaluation than existing evaluation metrics, such as Gram loss and ArtFID.\n        \u25b3 Less\n      ",
    "title": "Learning to Evaluate the Artness of AI-generated Images",
    "date": "8 May, 2023",
    "authors": [
      "Junyu Chen",
      " Jie An",
      " Hanjia Lyu",
      " Jiebo Luo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04963",
    "paper_id": "2305.04963",
    "abstract": "\n        Relational pooling is a framework for building more expressive and permutation-invariant graph neural networks. However, there is limited understanding of the exact enhancement in the expressivity of RP and its connection with the Weisfeiler Lehman hierarchy. Starting from RP, we propose to explicitly assign labels to nodes as additional features to improve expressive power of message passing neural networks. The method is then extended to higher dimensional WL, leading to a novel k,lk,l-WL algorithm, a more general framework than kk-WL. Theoretically, we analyze the expressivity of k,lk,l-WL with respect to kk and ll and unifies it with a great number of subgraph GNNs. Complexity reduction methods are also systematically discussed to build powerful and practical k,lk,l-GNN instances. We theoretically and experimentally prove that our method is universally compatible and capable of improving the expressivity of any base GNN model. Our k,lk,l-GNNs achieve superior performance on many synthetic and real-world datasets, which verifies the effectiveness of our framework.\n        \u25b3 Less\n      ",
    "title": "From Relational Pooling to Subgraph GNNs: A Universal Framework for More Expressive Graph Neural Networks",
    "date": "8 May, 2023",
    "authors": [
      "Cai Zhou",
      " Xiyuan Wang",
      " Muhan Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.11970",
    "paper_id": "2301.11970",
    "abstract": "\n        Recently, eXplainable AI (XAI) research has focused on counterfactual explanations as post-hoc justifications for AI-system decisions (e.g. a customer refused a loan might be told: If you asked for a loan with a shorter term, it would have been approved). Counterfactuals explain what changes to the input-features of an AI system change the output-decision. However, there is a sub-type of counterfactual, semi-factuals, that have received less attention in AI (though the Cognitive Sciences have studied them extensively). This paper surveys these literatures to summarise historical and recent breakthroughs in this area. It defines key desiderata for semi-factual XAI and reports benchmark tests of historical algorithms (along with a novel, naieve method) to provide a solid basis for future algorithmic developments.\n        \u25b3 Less\n      ",
    "title": "Even if Explanations: Prior Work, Desiderata & Benchmarks for Semi-Factual XAI",
    "date": "8 May, 2023",
    "authors": [
      "Saugat Aryal",
      " Mark T Keane"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14358",
    "paper_id": "2305.14358",
    "abstract": "\n        The memorialization of mass atrocities such as war crimes and genocides facilitates the remembrance of past suffering, honors those who resisted the perpetrators, and helps prevent the distortion of historical facts. Digital technologies have transformed memorialization practices by enabling less top-down and more creative approaches to remember mass atrocities. At the same time, they may also facilitate the spread of denialism and distortion, attempt to justify past crimes and attack the dignity of victims. The emergence of generative forms of artificial intelligence (AI), which produce textual and visual content, has the potential to revolutionize the field of memorialization even further. AI can identify patterns in training data to create new narratives for representing and interpreting mass atrocities - and do so in a fraction of the time it takes for humans. The use of generative AI in this context raises numerous questions: For example, can the paucity of training data on mass atrocities distort how AI interprets some atrocity-related inquiries? How important is the ability to differentiate between human- and AI-made content concerning mass atrocities? Can AI-made content be used to promote false information concerning atrocities? This article addresses these and other questions by examining the opportunities and risks associated with using generative AIs for memorializing mass atrocities. It also discusses recommendations for AIs integration in memorialization practices to steer the use of these technologies toward a more ethical and sustainable direction.\n        \u25b3 Less\n      ",
    "title": "Shall androids dream of genocides? How generative AI can change the future of memorialization of mass atrocities",
    "date": "8 May, 2023",
    "authors": [
      "Mykola Makhortykh",
      " Eve M. Zucker",
      " David J. Simon",
      " Daniel Bultmann",
      " Roberto Ulloa"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07666",
    "paper_id": "2305.07666",
    "abstract": "\n        Much discussion about large language models and language-and-vision models has focused on whether these models are intelligent agents. We present an alternative perspective. We argue that these artificial intelligence models are cultural technologies that enhance cultural transmission in the modern world, and are efficient imitation engines. We explore what AI models can tell us about imitation and innovation by evaluating their capacity to design new tools and discover novel causal structures, and contrast their responses with those of human children. Our work serves as a first step in determining which particular representations and competences, as well as which kinds of knowledge or skill, can be derived from particular learning techniques and data. Critically, our findings suggest that machines may need more than large scale language and images to achieve what a child can do.\n        \u25b3 Less\n      ",
    "title": "Imitation versus Innovation: What children can do that large language and language-and-vision models cannot (yet)?",
    "date": "8 May, 2023",
    "authors": [
      "Eunice Yiu",
      " Eliza Kosoy",
      " Alison Gopnik"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04989",
    "paper_id": "2305.04989",
    "abstract": "\n        A fundamental question in natural language processing is - what kind of language structure and semantics is the language model capturing? Graph formats such as knowledge graphs are easy to evaluate as they explicitly express language semantics and structure. This study evaluates the semantics encoded in the self-attention transformers by leveraging explicit knowledge graph structures. We propose novel metrics to measure the reconstruction error when providing graph path sequences from a knowledge graph and trying to reproduce/reconstruct the same from the outputs of the self-attention transformer models. The opacity of language models has an immense bearing on societal issues of trust and explainable decision outcomes. Our findings suggest that language models are models of stochastic control processes for plausible language pattern generation. However, they do not ascribe object and concept-level meaning and semantics to the learned stochastic patterns such as those described in knowledge graphs. Furthermore, to enable robust evaluation of concept understanding by language models, we construct and make public an augmented language understanding benchmark built on the General Language Understanding Evaluation (GLUE) benchmark. This has significant application-level user trust implications as stochastic patterns without a strong sense of meaning cannot be trusted in high-stakes applications.\n        \u25b3 Less\n      ",
    "title": "Knowledge Graph Guided Semantic Evaluation of Language Models For User Trust",
    "date": "8 May, 2023",
    "authors": [
      "Kaushik Roy",
      " Tarun Garg",
      " Vedant Palit",
      " Yuxin Zi",
      " Vignesh Narayanan",
      " Amit Sheth"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06223",
    "paper_id": "2305.06223",
    "abstract": "\n        Language models are not accurate in numerical problems. Their architecture does not allow for anything less than a probabilistic next word. This paper introduces ComputeGPT: an approach of creating a chat model able to answer computational problems through running on-demand code. ComputeGPT converts each question to relevant code, runs the code, and returns the computed answer as part of the chat. We combine this approach with a local browser-based Python interpretation and fine-tuned prompts in order to achieve state-of-the-art efficiency on numerical problems and provide a suitable front-end and safe environment for the code to be executed in.\n        \u25b3 Less\n      ",
    "title": "ComputeGPT: A computational chat model for numerical problems",
    "date": "8 May, 2023",
    "authors": [
      "Ryan Hardesty Lewis",
      " Junfeng Jiao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00543",
    "paper_id": "2305.00543",
    "abstract": "\n        Neural network-based decisions tend to be overconfident, where their raw outcome probabilities do not align with the true decision probabilities. Calibration of neural networks is an essential step towards more reliable deep learning frameworks. Prior metrics of calibration error primarily utilize crisp bin membership-based measures. This exacerbates skew in model probabilities and portrays an incomplete picture of calibration error. In this work, we propose a Fuzzy Calibration Error metric (FCE) that utilizes a fuzzy binning approach to calculate calibration error. This approach alleviates the impact of probability skew and provides a tighter estimate while measuring calibration error. We compare our metric with ECE across different data populations and class memberships. Our results show that FCE offers better calibration error estimation, especially in multi-class settings, alleviating the effects of skew in model confidence scores on calibration error estimation. We make our code and supplementary materials available at: https://github.com/bihani-g/fce\n        \u25b3 Less\n      ",
    "title": "Calibration Error Estimation Using Fuzzy Binning",
    "date": "8 May, 2023",
    "authors": [
      "Geetanjali Bihani",
      " Julia Taylor Rayz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05670",
    "paper_id": "2305.05670",
    "abstract": "\n        Car accidents remain a significant public safety issue worldwide, with the majority of them attributed to driver errors stemming from inadequate driving knowledge, non-compliance with regulations, and poor driving habits. To improve road safety, Driving Behavior Detection (DBD) systems have been proposed in several studies to identify safe and unsafe driving behavior. Many of these studies have utilized sensor data obtained from the Controller Area Network (CAN) bus to construct their models. However, the use of publicly available sensors is known to reduce the accuracy of detection models, while incorporating vendor-specific sensors into the dataset increases accuracy. To address the limitations of existing approaches, we present a reliable DBD system based on Graph Convolutional Long Short-Term Memory Networks (GConvLSTM) that enhances the precision and practicality of DBD models using public sensors. Additionally, we incorporate non-public sensors to evaluate the model's effectiveness. Our proposed model achieved a high accuracy of 97.5\\% for public sensors and an average accuracy of 98.1\\% for non-public sensors, indicating its consistency and accuracy in both settings. To enable local driver behavior analysis, we deployed our DBD system on a Raspberry Pi at the network edge, with drivers able to access daily driving condition reports, sensor data, and prediction results through a monitoring dashboard. Furthermore, the dashboard issues voice warnings to alert drivers of hazardous driving conditions. Our findings demonstrate that the proposed system can effectively detect hazardous and unsafe driving behavior, with potential applications in improving road safety and reducing the number of accidents caused by driver errors.\n        \u25b3 Less\n      ",
    "title": "Enhancing Road Safety through Accurate Detection of Hazardous Driving Behaviors with Graph Convolutional Recurrent Networks",
    "date": "8 May, 2023",
    "authors": [
      "Pooyan Khosravinia",
      " Thinagaran Perumal",
      " Javad Zarrin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03509",
    "paper_id": "2305.03509",
    "abstract": "\n        Diffusion-based generative models' impressive ability to create convincing images has captured global attention. However, their complex internal structures and operations often make them difficult for non-experts to understand. We present Diffusion Explainer, the first interactive visualization tool that explains how Stable Diffusion transforms text prompts into images. Diffusion Explainer tightly integrates a visual overview of Stable Diffusion's complex components with detailed explanations of their underlying operations, enabling users to fluidly transition between multiple levels of abstraction through animations and interactive elements. By comparing the evolutions of image representations guided by two related text prompts over refinement timesteps, users can discover the impact of prompts on image generation. Diffusion Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern AI techniques. Our open-sourced tool is available at: https://poloclub.github.io/diffusion-explainer/. A video demo is available at https://youtu.be/Zg4gxdIWDds.\n        \u25b3 Less\n      ",
    "title": "Diffusion Explainer: Visual Explanation for Text-to-image Stable Diffusion",
    "date": "8 May, 2023",
    "authors": [
      "Seongmin Lee",
      " Benjamin Hoover",
      " Hendrik Strobelt",
      " Zijie J. Wang",
      " ShengYun Peng",
      " Austin Wright",
      " Kevin Li",
      " Haekyu Park",
      " Haoyang Yang",
      " Duen Horng Chau"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.04196",
    "paper_id": "2204.04196",
    "abstract": "\n        Proof Blocks is a software tool that allows students to practice writing mathematical proofs by dragging and dropping lines instead of writing proofs from scratch. Proof Blocks offers the capability of assigning partial credit and providing solution quality feedback to students. This is done by computing the edit distance from a student's submission to some predefined set of solutions. In this work, we propose an algorithm for the edit distance problem that significantly outperforms the baseline procedure of exhaustively enumerating over the entire search space. Our algorithm relies on a reduction to the minimum vertex cover problem. We benchmark our algorithm on thousands of student submissions from multiple courses, showing that the baseline algorithm is intractable, and that our proposed algorithm is critical to enable classroom deployment. Our new algorithm has also been used for problems in many other domains where the solution space can be modeled as a DAG, including but not limited to Parsons Problems for writing code, helping students understand packet ordering in networking protocols, and helping students sketch solution steps for physics problems. Integrated into multiple learning management systems, the algorithm serves thousands of students each year.\n        \u25b3 Less\n      ",
    "title": "Efficient Feedback and Partial Credit Grading for Proof Blocks Problems",
    "date": "8 May, 2023",
    "authors": [
      "Seth Poulsen",
      " Shubhang Kulkarni",
      " Geoffrey Herman",
      " Matthew West"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.06675",
    "paper_id": "2302.06675",
    "abstract": "\n        We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, Lion\\textbf{Lion} (EvoLved Sign Momentum\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% zero-shot\\textit{zero-shot} and 91.1% fine-tuning\\textit{fine-tuning} accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.\n        \u25b3 Less\n      ",
    "title": "Symbolic Discovery of Optimization Algorithms",
    "date": "8 May, 2023",
    "authors": [
      "Xiangning Chen",
      " Chen Liang",
      " Da Huang",
      " Esteban Real",
      " Kaiyuan Wang",
      " Yao Liu",
      " Hieu Pham",
      " Xuanyi Dong",
      " Thang Luong",
      " Cho-Jui Hsieh",
      " Yifeng Lu",
      " Quoc V. Le"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09673",
    "paper_id": "2305.09673",
    "abstract": "\n        Application security is an essential part of developing modern software, as lots of attacks depend on vulnerabilities in software. The number of attacks is increasing globally due to technological advancements. Companies must include security in every stage of developing, testing, and deploying their software in order to prevent data breaches. There are several methods to detect software vulnerability Non-AI-based such as Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST). However, these approaches have substantial false-positive and false-negative rates. On the other side, researchers have been interested in developing an AI-based vulnerability detection system employing deep learning models like BERT, BLSTM, etc. In this paper, we proposed a two-stage solution, two deep learning models were proposed for vulnerability detection in C/C++ source codes, the first stage is CNN which detects if the source code contains any vulnerability (binary classification model) and the second stage is CNN-LTSM that classifies this vulnerability into a class of 50 different types of vulnerabilities (multiclass classification model). Experiments were done on SySeVR dataset. Results show an accuracy of 99% for the first and 98% for the second stage.\n        \u25b3 Less\n      ",
    "title": "Vulnerability Detection Using Two-Stage Deep Learning Models",
    "date": "8 May, 2023",
    "authors": [
      "Mohamed Mjd Alhafi",
      " Mohammad Hammade",
      " Khloud Al Jallad"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.13222",
    "paper_id": "2205.13222",
    "abstract": "\n        Federated learning (FL) is a new distributed machine learning framework known for its benefits on data privacy and communication efficiency. Since full client participation in many cases is infeasible due to constrained resources, partial participation FL algorithms have been investigated that proactively select/sample a subset of clients, aiming to achieve learning performance close to the full participation case. This paper studies a passive partial client participation scenario that is much less well understood, where partial participation is a result of external events, namely client dropout, rather than a decision of the FL algorithm. We cast FL with client dropout as a special case of a larger class of FL problems where clients can submit substitute (possibly inaccurate) local model updates. Based on our convergence analysis, we develop a new algorithm FL-FDMS that discovers friends of clients (i.e., clients whose data distributions are similar) on-the-fly and uses friends' local updates as substitutes for the dropout clients, thereby reducing the substitution error and improving the convergence performance. A complexity reduction mechanism is also incorporated into FL-FDMS, making it both theoretically sound and practically useful. Experiments on MNIST and CIFAR-10 confirmed the superior performance of FL-FDMS in handling client dropout in FL.\n        \u25b3 Less\n      ",
    "title": "Combating Client Dropout in Federated Learning via Friend Model Substitution",
    "date": "8 May, 2023",
    "authors": [
      "Heqiang Wang",
      " Jie Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06218",
    "paper_id": "2305.06218",
    "abstract": "\n        In this paper, we analyze the performance of a multitask end-to-end transformer model on the task of conversational recommendations, which aim to provide recommendations based on a user's explicit preferences expressed in dialogue. While previous works in this area adopt complex multi-component approaches where the dialogue management and entity recommendation tasks are handled by separate components, we show that a unified transformer model, based on the T5 text-to-text transformer model, can perform competitively in both recommending relevant items and generating conversation dialogue. We fine-tune our model on the ReDIAL conversational movie recommendation dataset, and create additional training tasks derived from MovieLens (such as the prediction of movie attributes and related movies based on an input movie), in a multitask learning setting. Using a series of probe studies, we demonstrate that the learned knowledge in the additional tasks is transferred to the conversational setting, where each task leads to a 9%-52% increase in its related probe score.\n        \u25b3 Less\n      ",
    "title": "Multi-Task End-to-End Training Improves Conversational Recommendation",
    "date": "8 May, 2023",
    "authors": [
      "Naveen Ram",
      " Dima Kuzmin",
      " Ellie Ka In Chio",
      " Moustafa Farid Alzantot",
      " Santiago Ontanon",
      " Ambarish Jash",
      " Judith Yue Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05080",
    "paper_id": "2305.05080",
    "abstract": "\n        Optimal Transport (OT) is a mathematical framework that first emerged in the eighteenth century and has led to a plethora of methods for answering many theoretical and applied questions. The last decade is a witness of the remarkable contributions of this classical optimization problem to machine learning. This paper is about where and how optimal transport is used in machine learning with a focus on the question of salable optimal transport. We provide a comprehensive survey of optimal transport while ensuring an accessible presentation as permitted by the nature of the topic and the context. First, we explain optimal transport background and introduce different flavors (i.e. mathematical formulations), properties, and notable applications. We then address the fundamental question of how to scale optimal transport to cope with the current demands of big and high dimensional data. We conduct a systematic analysis of the methods used in the literature for scaling OT and present the findings in a unified taxonomy. We conclude with presenting some open challenges and discussing potential future research directions. A live repository of related OT research papers is maintained in https://github.com/abdelwahed/OT_for_big_data.git.\n        \u25b3 Less\n      ",
    "title": "Earth Movers in The Big Data Era: A Review of Optimal Transport in Machine Learning",
    "date": "8 May, 2023",
    "authors": [
      "Abdelwahed Khamis",
      " Russell Tsuchida",
      " Mohamed Tarek",
      " Vivien Rolland",
      " Lars Petersson"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2202.01208",
    "paper_id": "2202.01208",
    "abstract": "\n        Ultrasound b-mode imaging is a qualitative approach and diagnostic quality strongly depends on operators' training and experience. Quantitative approaches can provide information about tissue properties; therefore, can be used for identifying various tissue types, e.g., speed-of-sound in the tissue can be used as a biomarker for tissue malignancy, especially in breast imaging. Recent studies showed the possibility of speed-of-sound reconstruction using deep neural networks that are fully trained on simulated data. However, because of the ever-present domain shift between simulated and measured data, the stability and performance of these models in real setups are still under debate. In prior works, for training data generation, tissue structures were modeled as simplified geometrical structures which does not reflect the complexity of the real tissues. In this study, we proposed a new simulation setup for training data generation based on Tomosynthesis images. We combined our approach with the simplified geometrical model and investigated the impacts of training data diversity on the stability and robustness of an existing network architecture. We studied the sensitivity of the trained network to different simulation parameters, e.g., echogenicity, number of scatterers, noise, and geometry. We showed that the network trained with the joint set of data is more stable on out-of-domain simulated data as well as measured phantom data.\n        \u25b3 Less\n      ",
    "title": "Deep Learning for Ultrasound Speed-of-Sound Reconstruction: Impacts of Training Data Diversity on Stability and Robustness",
    "date": "8 May, 2023",
    "authors": [
      "Farnaz Khun Jush",
      " Markus Biele",
      " Peter M. Dueppenbecker",
      " Andreas Maier"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05092",
    "paper_id": "2305.05092",
    "abstract": "\n        Industries worldwide are being transformed by artificial intelligence (AI), and the telecom industry is no different. Standardization is critical for industry alignment to achieve widespread adoption of AI in telecom. The 3rd generation partnership project (3GPP) Release 18 is the first release of 5G-Advanced, which includes a diverse set of study and work items dedicated to AI. This article provides a holistic overview of the state of the art in the 3GPP work on AI in 5G-Advanced, by presenting the various 3GPP Release-18 activities on AI as an organic whole, explaining in detail the design aspects, and sharing various design rationales influencing standardization.\n        \u25b3 Less\n      ",
    "title": "Artificial Intelligence in 3GPP 5G-Advanced: A Survey",
    "date": "8 May, 2023",
    "authors": [
      "Xingqin Lin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05095",
    "paper_id": "2305.05095",
    "abstract": "\n        The CLIP (Contrastive Language-Image Pre-training) model and its variants are becoming the de facto backbone in many applications. However, training a CLIP model from hundreds of millions of image-text pairs can be prohibitively expensive. Furthermore, the conventional CLIP model doesn't differentiate between the visual semantics and meaning of text regions embedded in images. This can lead to non-robustness when the text in the embedded region doesn't match the image's visual appearance. In this paper, we discuss two effective approaches to improve the efficiency and robustness of CLIP training: (1) augmenting the training dataset while maintaining the same number of optimization steps, and (2) filtering out samples that contain text regions in the image. By doing so, we significantly improve the classification and retrieval accuracy on public benchmarks like ImageNet and CoCo. Filtering out images with text regions also protects the model from typographic attacks. To verify this, we build a new dataset named ImageNet with Adversarial Text Regions (ImageNet-Attr). Our filter-based CLIP model demonstrates a top-1 accuracy of 68.78\\%, outperforming previous models whose accuracy was all below 50\\%.\n        \u25b3 Less\n      ",
    "title": "Less is More: Removing Text-regions Improves CLIP Training Efficiency and Robustness",
    "date": "8 May, 2023",
    "authors": [
      "Liangliang Cao",
      " Bowen Zhang",
      " Chen Chen",
      " Yinfei Yang",
      " Xianzhi Du",
      " Wencong Zhang",
      " Zhiyun Lu",
      " Yantao Zheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06358",
    "paper_id": "2305.06358",
    "abstract": "\n        Humans can collaborate and complete tasks based on visual signals and instruction from the environment. Training such a robot is difficult especially due to the understanding of the instruction and the complicated environment. Previous instruction-following agents are biased to English-centric corpus, making it unrealizable to be applied to users that use multiple languages or even low-resource languages. Nevertheless, the instruction-following agents are pre-trained in a mode that assumes the user can observe the environment, which limits its accessibility. In this work, we're trying to generalize the success of instruction-following agents to non-English languages with little corpus resources, and improve its intractability and accessibility. We introduce UVLN (Universal Vision-Language Navigation), a novel machine-translation instructional augmented framework for cross-lingual vision-language navigation, with a novel composition of state-of-the-art large language model (GPT3) with the image caption model (BLIP). We first collect a multilanguage vision-language navigation dataset via machine translation. Then we extend the standard VLN training objectives to a multilingual setting via a cross-lingual language encoder. The alignment between different languages is captured through a shared vision and action context via a cross-modal transformer, which encodes the inputs of language instruction, visual observation, and action decision sequences. To improve the intractability, we connect our agent with the large language model that informs the situation and current state to the user and also explains the action decisions. Experiments over Room Across Room Dataset prove the effectiveness of our approach. And the qualitative results show the promising intractability and accessibility of our instruction-following agent.\n        \u25b3 Less\n      ",
    "title": "Accessible Instruction-Following Agent",
    "date": "8 May, 2023",
    "authors": [
      "Kairui Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05098",
    "paper_id": "2305.05098",
    "abstract": "\n        State-of-the-art sequence-to-sequence models often require autoregressive decoding, which can be highly expensive. However, for some downstream tasks such as out-of-distribution (OOD) detection and resource allocation, the actual decoding output is not needed just a scalar attribute of this sequence. In these scenarios, where for example knowing the quality of a system's output to predict poor performance prevails over knowing the output itself, is it possible to bypass the autoregressive decoding? We propose Non-Autoregressive Proxy (NAP) models that can efficiently predict general scalar-valued sequence-level attributes. Importantly, NAPs predict these metrics directly from the encodings, avoiding the expensive autoregressive decoding stage. We consider two sequence-to-sequence task: Machine Translation (MT); and Automatic Speech Recognition (ASR). In OOD for MT, NAPs outperform a deep ensemble while being significantly faster. NAPs are also shown to be able to predict performance metrics such as BERTScore (MT) or word error rate (ASR). For downstream tasks, such as data filtering and resource optimization, NAPs generate performance predictions that outperform predictive uncertainty while being highly inference efficient.\n        \u25b3 Less\n      ",
    "title": "Who Needs Decoders? Efficient Estimation of Sequence-level Attributes",
    "date": "8 May, 2023",
    "authors": [
      "Yassir Fathullah",
      " Puria Radmard",
      " Adian Liusie",
      " Mark J. F. Gales"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05101",
    "paper_id": "2305.05101",
    "abstract": "\n        In recent years the development of artificial intelligence (AI) systems for automated medical image analysis has gained enormous momentum. At the same time, a large body of work has shown that AI systems can systematically and unfairly discriminate against certain populations in various application scenarios. These two facts have motivated the emergence of algorithmic fairness studies in this field. Most research on healthcare algorithmic fairness to date has focused on the assessment of biases in terms of classical discrimination metrics such as AUC and accuracy. Potential biases in terms of model calibration, however, have only recently begun to be evaluated. This is especially important when working with clinical decision support systems, as predictive uncertainty is key for health professionals to optimally evaluate and combine multiple sources of information. In this work we study discrimination and calibration biases in models trained for automatic detection of malignant dermatological conditions from skin lesions images. Importantly, we show how several typically employed calibration metrics are systematically biased with respect to sample sizes, and how this can lead to erroneous fairness analysis if not taken into consideration. This is of particular relevance to fairness studies, where data imbalance results in drastic sample size differences between demographic sub-groups, which, if not taken into account, can act as confounders.\n        \u25b3 Less\n      ",
    "title": "Towards unraveling calibration biases in medical image analysis",
    "date": "8 May, 2023",
    "authors": [
      "Mar\u00eda Agustina Ricci Lara",
      " Candelaria Mosquera",
      " Enzo Ferrante",
      " Rodrigo Echeveste"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05111",
    "paper_id": "2305.05111",
    "abstract": "\n        AI methods referred to as interpretable are often discredited as inaccurate by supporters of the existence of a trade-off between interpretability and accuracy. In many problem contexts however this trade-off does not hold. This paper discusses a regression problem context to predict flight take-off delays where the most accurate data regression model was trained via the XGBoost implementation of gradient boosted decision trees. While building an XGB-CBR Twin and converting the XGBoost feature importance into global weights in the CBR model, the resultant CBR model alone provides the most accurate local prediction, maintains the global importance to provide a global explanation of the model, and offers the most interpretable representation for local explanations. This resultant CBR model becomes a benchmark of accuracy and interpretability for this problem context, and hence it is used to evaluate the two additive feature attribute methods SHAP and LIME to explain the XGBoost regression model. The results with respect to local accuracy and feature attribution lead to potentially valuable future work.\n        \u25b3 Less\n      ",
    "title": "When a CBR in Hand is Better than Twins in the Bush",
    "date": "8 May, 2023",
    "authors": [
      "Mobyen Uddin Ahmed",
      " Shaibal Barua",
      " Shahina Begum",
      " Mir Riyanul Islam",
      " Rosina O Weber"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04206",
    "paper_id": "2305.04206",
    "abstract": "\n        Various hand-designed CNN architectures have been developed, such as VGG, ResNet, DenseNet, etc., and achieve State-of-the-Art (SoTA) levels on different tasks. Neural Architecture Search (NAS) now focuses on automatically finding the best CNN architecture to handle the above tasks. However, the verification of a searched architecture is very time-consuming and makes predictor-based methods become an essential and important branch of NAS. Two commonly used techniques to build predictors are graph-convolution networks (GCN) and multilayer perceptron (MLP). In this paper, we consider the difference between GCN and MLP on adjacent operation trails and then propose the Redirected Adjacent Trails NAS (RATs-NAS) to quickly search for the desired neural network architecture. The RATs-NAS consists of two components: the Redirected Adjacent Trails GCN (RATs-GCN) and the Predictor-based Search Space Sampling (P3S) module. RATs-GCN can change trails and their strengths to search for a better neural network architecture. P3S can rapidly focus on tighter intervals of FLOPs in the search space. Based on our observations on cell-based NAS, we believe that architectures with similar FLOPs will perform similarly. Finally, the RATs-NAS consisting of RATs-GCN and P3S beats WeakNAS, Arch-Graph, and others by a significant margin on three sub-datasets of NASBench-201.\n        \u25b3 Less\n      ",
    "title": "RATs-NAS: Redirection of Adjacent Trails on GCN for Neural Architecture Search",
    "date": "8 May, 2023",
    "authors": [
      "Yu-Ming Zhang",
      " Jun-Wei Hsieh",
      " Chun-Chieh Lee",
      " Kuo-Chin Fan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.01118",
    "paper_id": "2305.01118",
    "abstract": "\n        Geo-tagged images are publicly available in large quantities, whereas labels such as object classes are rather scarce and expensive to collect. Meanwhile, contrastive learning has achieved tremendous success in various natural image and language tasks with limited labeled data. However, existing methods fail to fully leverage geospatial information, which can be paramount to distinguishing objects that are visually similar. To directly leverage the abundant geospatial information associated with images in pre-training, fine-tuning, and inference stages, we present Contrastive Spatial Pre-Training (CSP), a self-supervised learning framework for geo-tagged images. We use a dual-encoder to separately encode the images and their corresponding geo-locations, and use contrastive objectives to learn effective location representations from images, which can be transferred to downstream supervised tasks such as image classification. Experiments show that CSP can improve model performance on both iNat2018 and fMoW datasets. Especially, on iNat2018, CSP significantly boosts the model performance with 10-34% relative improvement with various labeled training data sampling ratios.\n        \u25b3 Less\n      ",
    "title": "CSP: Self-Supervised Contrastive Spatial Pre-Training for Geospatial-Visual Representations",
    "date": "8 May, 2023",
    "authors": [
      "Gengchen Mai",
      " Ni Lao",
      " Yutong He",
      " Jiaming Song",
      " Stefano Ermon"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05128",
    "paper_id": "2305.05128",
    "abstract": "\n        A kriging-random forest hybrid model is developed for real-time ground property prediction ahead of the earth pressure balanced shield by integrating Kriging extrapolation and random forest, which can guide shield operating parameter selection thereby mitigate construction risks. The proposed KRF algorithm synergizes two types of information: prior information and real-time information. The previously predicted ground properties with EPB operating parameters are extrapolated via the Kriging algorithm to provide prior information for the prediction of currently being excavated ground properties. The real-time information refers to the real-time operating parameters of the EPB shield, which are input into random forest to provide a real-time prediction of ground properties. The integration of these two predictions is achieved by assigning weights to each prediction according to their uncertainties, ensuring the prediction of KRF with minimum uncertainty. The performance of the KRF algorithm is assessed via a case study of the Changsha Metro Line 4 project. It reveals that the proposed KRF algorithm can predict ground properties with an accuracy of 93%, overperforming the existing algorithms of LightGBM, AdaBoost-CART, and DNN by 29%, 8%, and 12%, respectively. Another dataset from Shenzhen Metro Line 13 project is utilized to further evaluate the model generalization performance, revealing that the model can transfer its learned knowledge from one region to another with an accuracy of 89%.\n        \u25b3 Less\n      ",
    "title": "A Kriging-Random Forest Hybrid Model for Real-time Ground Property Prediction during Earth Pressure Balance Shield Tunneling",
    "date": "8 May, 2023",
    "authors": [
      "Ziheng Geng",
      " Chao Zhang",
      " Yuhao Ren",
      " Minxiang Zhu",
      " Renpeng Chen",
      " Hongzhan Cheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14359",
    "paper_id": "2305.14359",
    "abstract": "\n        Lip-to-Speech (Lip2Speech) synthesis, which predicts corresponding speech from talking face images, has witnessed significant progress with various models and training strategies in a series of independent studies. However, existing studies can not achieve voice control under zero-shot condition, because extra speaker embeddings need to be extracted from natural reference speech and are unavailable when only the silent video of an unseen speaker is given. In this paper, we propose a zero-shot personalized Lip2Speech synthesis method, in which face images control speaker identities. A variational autoencoder is adopted to disentangle the speaker identity and linguistic content representations, which enables speaker embeddings to control the voice characteristics of synthetic speech for unseen speakers. Furthermore, we propose associated cross-modal representation learning to promote the ability of face-based speaker embeddings (FSE) on voice control. Extensive experiments verify the effectiveness of the proposed method whose synthetic utterances are more natural and matching with the personality of input video than the compared methods. To our best knowledge, this paper makes the first attempt on zero-shot personalized Lip2Speech synthesis with a face image rather than reference audio to control voice characteristics.\n        \u25b3 Less\n      ",
    "title": "Zero-shot personalized lip-to-speech synthesis with face image based voice control",
    "date": "8 May, 2023",
    "authors": [
      "Zheng-Yan Sheng",
      " Yang Ai",
      " Zhen-Hua Ling"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05134",
    "paper_id": "2305.05134",
    "abstract": "\n        In the past several decades, the world's economy has become increasingly globalized. On the other hand, there are also ideas advocating the practice of ``buy local'', by which people buy locally produced goods and services rather than those produced farther away. In this paper, we establish a mathematical theory of real price that determines the optimal global versus local spending of an agent which achieves the agent's optimal tradeoff between spending and obtained utility. Our theory of real price depends on the asymptotic analysis of a Markov chain transition probability matrix related to the network of producers and consumers. We show that the real price of a product or service can be determined from the involved Markov chain matrix, and can be dramatically different from the product's label price. In particular, we show that the label prices of products and services are often not ``real'' or directly ``useful'': given two products offering the same myopic utility, the one with lower label price may not necessarily offer better asymptotic utility. This theory shows that the globality or locality of the products and services does have different impacts on the spending-utility tradeoff of a customer. The established mathematical theory of real price can be used to determine whether to adopt or not to adopt certain artificial intelligence (AI) technologies from an economic perspective.\n        \u25b3 Less\n      ",
    "title": "To AI or not to AI, to Buy Local or not to Buy Local: A Mathematical Theory of Real Price",
    "date": "8 May, 2023",
    "authors": [
      "Huan Cai",
      " Catherine Xu",
      " Weiyu Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05159",
    "paper_id": "2305.05159",
    "abstract": "\n        There is a prevalence of multiagent reinforcement learning (MARL) methods that engage in centralized training. But, these methods involve obtaining various types of information from the other agents, which may not be feasible in competitive or adversarial settings. A recent method, the interactive advantage actor critic (IA2C), engages in decentralized training coupled with decentralized execution, aiming to predict the other agents' actions from possibly noisy observations. In this paper, we present the latent IA2C that utilizes an encoder-decoder architecture to learn a latent representation of the hidden state and other agents' actions. Our experiments in two domains -- each populated by many agents -- reveal that the latent IA2C significantly improves sample efficiency by reducing variance and converging faster. Additionally, we introduce open versions of these domains where the agent population may change over time, and evaluate on these instances as well.\n        \u25b3 Less\n      ",
    "title": "Latent Interactive A2C for Improved RL in Open Many-Agent Systems",
    "date": "8 May, 2023",
    "authors": [
      "Keyang He",
      " Prashant Doshi",
      " Bikramjit Banerjee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05163",
    "paper_id": "2305.05163",
    "abstract": "\n        This study explores the vaccine prioritization strategy to reduce the overall burden of the pandemic when the supply is limited. Existing methods conduct macro-level or simplified micro-level vaccine distribution by assuming the homogeneous behavior within subgroup populations and lacking mobility dynamics integration. Directly applying these models for micro-level vaccine allocation leads to sub-optimal solutions due to the lack of behavioral-related details. To address the issue, we first incorporate the mobility heterogeneity in disease dynamics modeling and mimic the disease evolution process using a Trans-vaccine-SEIR model. Then we develop a novel deep reinforcement learning to seek the optimal vaccine allocation strategy for the high-degree spatial-temporal disease evolution system. The graph neural network is used to effectively capture the structural properties of the mobility contact network and extract the dynamic disease features. In our evaluation, the proposed framework reduces 7% - 10% of infections and deaths than the baseline strategies. Extensive evaluation shows that the proposed framework is robust to seek the optimal vaccine allocation with diverse mobility patterns in the micro-level disease evolution system. In particular, we find the optimal vaccine allocation strategy in the transit usage restriction scenario is significantly more effective than restricting cross-zone mobility for the top 10% age-based and income-based zones. These results provide valuable insights for areas with limited vaccines and low logistic efficacy.\n        \u25b3 Less\n      ",
    "title": "Cooperating Graph Neural Networks with Deep Reinforcement Learning for Vaccine Prioritization",
    "date": "8 May, 2023",
    "authors": [
      "Lu Ling",
      " Washim Uddin Mondal",
      " Satish V",
      " Ukkusuri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05172",
    "paper_id": "2305.05172",
    "abstract": "\n        A central quest in explainable AI relates to understanding the decisions made by (learned) classifiers. There are three dimensions of this understanding that have been receiving significant attention in recent years. The first dimension relates to characterizing conditions on instances that are necessary and sufficient for decisions, therefore providing abstractions of instances that can be viewed as the \"reasons behind decisions.\" The next dimension relates to characterizing minimal conditions that are sufficient for a decision, therefore identifying maximal aspects of the instance that are irrelevant to the decision. The last dimension relates to characterizing minimal conditions that are necessary for a decision, therefore identifying minimal perturbations to the instance that yield alternate decisions. We discuss in this tutorial a comprehensive, semantical and computational theory of explainability along these dimensions which is based on some recent developments in symbolic logic. The tutorial will also discuss how this theory is particularly applicable to non-symbolic classifiers such as those based on Bayesian networks, decision trees, random forests and some types of neural networks.\n        \u25b3 Less\n      ",
    "title": "Logic for Explainable AI",
    "date": "8 May, 2023",
    "authors": [
      "Adnan Darwiche"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05176",
    "paper_id": "2305.05176",
    "abstract": "\n        There is a rapidly growing number of large language models (LLMs) that users can query for a fee. We review the cost associated with querying popular LLM APIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have heterogeneous pricing structures, with fees that can differ by two orders of magnitude. In particular, using LLMs on large collections of queries and text can be expensive. Motivated by this, we outline and discuss three types of strategies that users can exploit to reduce the inference cost associated with using LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As an example, we propose FrugalGPT, a simple yet flexible instantiation of LLM cascade which learns which combinations of LLMs to use for different queries in order to reduce cost and improve accuracy. Our experiments show that FrugalGPT can match the performance of the best individual LLM (e.g. GPT-4) with up to 98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost. The ideas and findings presented here lay a foundation for using LLMs sustainably and efficiently.\n        \u25b3 Less\n      ",
    "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
    "date": "8 May, 2023",
    "authors": [
      "Lingjiao Chen",
      " Matei Zaharia",
      " James Zou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05179",
    "paper_id": "2305.05179",
    "abstract": "\n        Hopfield networks are artificial neural networks which store memory patterns on the states of their neurons by choosing recurrent connection weights and update rules such that the energy landscape of the network forms attractors around the memories. How many stable, sufficiently-attracting memory patterns can we store in such a network using NN neurons? The answer depends on the choice of weights and update rule. Inspired by setwise connectivity in biology, we extend Hopfield networks by adding setwise connections and embedding these connections in a simplicial complex. Simplicial complexes are higher dimensional analogues of graphs which naturally represent collections of pairwise and setwise relationships. We show that our simplicial Hopfield networks increase memory storage capacity. Surprisingly, even when connections are limited to a small random subset of equivalent size to an all-pairwise network, our networks still outperform their pairwise counterparts. Such scenarios include non-trivial simplicial topology. We also test analogous modern continuous Hopfield networks, offering a potentially promising avenue for improving the attention mechanism in Transformer models.\n        \u25b3 Less\n      ",
    "title": "Simplicial Hopfield networks",
    "date": "8 May, 2023",
    "authors": [
      "Thomas F Burns",
      " Tomoki Fukai"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05183",
    "paper_id": "2305.05183",
    "abstract": "\n        Recently, much Chinese text error correction work has focused on Chinese Spelling Check (CSC) and Chinese Grammatical Error Diagnosis (CGED). In contrast, little attention has been paid to the complicated problem of Chinese Semantic Error Diagnosis (CSED), which lacks relevant datasets. The study of semantic errors is important because they are very common and may lead to syntactic irregularities or even problems of comprehension. To investigate this, we build the CSED corpus, which includes two datasets. The one is for the CSED-Recognition (CSED-R) task. The other is for the CSED-Correction (CSED-C) task. Our annotation guarantees high-quality data through quality assurance mechanisms. Our experiments show that powerful pre-trained models perform poorly on this corpus. We also find that the CSED task is challenging, as evidenced by the fact that even humans receive a low score. This paper proposes syntax-aware models to specifically adapt to the CSED task. The experimental results show that the introduction of the syntax-aware approach is meaningful.\n        \u25b3 Less\n      ",
    "title": "CSED: A Chinese Semantic Error Diagnosis Corpus",
    "date": "9 May, 2023",
    "authors": [
      "Bo Sun",
      " Baoxin Wang",
      " Yixuan Wang",
      " Wanxiang Che",
      " Dayong Wu",
      " Shijin Wang",
      " Ting Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05187",
    "paper_id": "2305.05187",
    "abstract": "\n        Brain-inspired spiking neural networks (SNNs) replace the multiply-accumulate operations of traditional neural networks by integrate-and-fire neurons, with the goal of achieving greater energy efficiency. Specialized hardware implementations of those neurons clearly have advantages over general-purpose devices in terms of power and performance, but exhibit poor scalability when it comes to accelerating large neural networks. DeepFire2 introduces a hardware architecture which can map large network layers efficiently across multiple super logic regions in a multi-die FPGA. That gives more control over resource allocation and parallelism, benefiting both throughput and energy consumption. Avoiding the use of lookup tables to implement the AND operations of an SNN, prevents the layer size to be limited by logic resources. A deep pipeline does not only lead to an increased clock speed of up to 600 MHz. We double the throughput and power efficiency compared to our previous version of DeepFire, which equates to an almost 10-fold improvement over other previous implementations. Importantly, we are able to deploy a large ImageNet model, while maintaining a throughput of over 1500 frames per second.\n        \u25b3 Less\n      ",
    "title": "DeepFire2: A Convolutional Spiking Neural Network Accelerator on FPGAs",
    "date": "9 May, 2023",
    "authors": [
      "Myat Thu Linn Aung",
      " Daniel Gerlinghoff",
      " Chuping Qu",
      " Liwei Yang",
      " Tian Huang",
      " Rick Siow Mong Goh",
      " Tao Luo",
      " Weng-Fai Wong"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2112.05504",
    "paper_id": "2112.05504",
    "abstract": "\n        Neural radiance fields (NeRF) has achieved outstanding performance in modeling 3D objects and controlled scenes, usually under a single scale. In this work, we focus on multi-scale cases where large changes in imagery are observed at drastically different scales. This scenario vastly exists in real-world 3D environments, such as city scenes, with views ranging from satellite level that captures the overview of a city, to ground level imagery showing complex details of an architecture; and can also be commonly identified in landscape and delicate minecraft 3D models. The wide span of viewing positions within these scenes yields multi-scale renderings with very different levels of detail, which poses great challenges to neural radiance field and biases it towards compromised results. To address these issues, we introduce BungeeNeRF, a progressive neural radiance field that achieves level-of-detail rendering across drastically varied scales. Starting from fitting distant views with a shallow base block, as training progresses, new blocks are appended to accommodate the emerging details in the increasingly closer views. The strategy progressively activates high-frequency channels in NeRF's positional encoding inputs and successively unfolds more complex details as the training proceeds. We demonstrate the superiority of BungeeNeRF in modeling diverse multi-scale scenes with drastically varying views on multiple data sources (city models, synthetic, and drone captured data) and its support for high-quality rendering in different levels of detail.\n        \u25b3 Less\n      ",
    "title": "BungeeNeRF: Progressive Neural Radiance Field for Extreme Multi-scale Scene Rendering",
    "date": "9 May, 2023",
    "authors": [
      "Yuanbo Xiangli",
      " Linning Xu",
      " Xingang Pan",
      " Nanxuan Zhao",
      " Anyi Rao",
      " Christian Theobalt",
      " Bo Dai",
      " Dahua Lin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2108.05660",
    "paper_id": "2108.05660",
    "abstract": "\n        The Reverse Transcription Polymerase Chain Reaction (RTPCR)} test is the silver bullet diagnostic test to discern COVID infection. Rapid antigen detection is a screening test to identify COVID positive patients in little as 15 minutes, but has a lower sensitivity than the PCR tests. Besides having multiple standardized test kits, many people are getting infected and either recovering or dying even before the test due to the shortage and cost of kits, lack of indispensable specialists and labs, time-consuming result compared to bulk population especially in developing and underdeveloped countries. Intrigued by the parametric deviations in immunological and hematological profile of a COVID patient, this research work leveraged the concept of COVID-19 detection by proposing a risk-free and highly accurate Stacked Ensemble Machine Learning model to identify a COVID patient from communally available-widespread-cheap routine blood tests which gives a promising accuracy, precision, recall and F1-score of 100%. Analysis from R-curve also shows the preciseness of the risk-free model to be implemented. The proposed method has the potential for large scale ubiquitous low-cost screening application. This can add an extra layer of protection in keeping the number of infected cases to a minimum and control the pandemic by identifying asymptomatic or pre-symptomatic people early.\n        \u25b3 Less\n      ",
    "title": "Development of a Risk-Free COVID-19 Screening Algorithm from Routine Blood Tests Using Ensemble Machine Learning",
    "date": "9 May, 2023",
    "authors": [
      "Md. Mohsin Sarker Raihan",
      " Md. Mohi Uddin Khan",
      " Laboni Akter",
      " Abdullah Bin Shams"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05191",
    "paper_id": "2305.05191",
    "abstract": "\n        Detecting commonsense causal relations (causation) between events has long been an essential yet challenging task. Given that events are complicated, an event may have different causes under various contexts. Thus, exploiting context plays an essential role in detecting causal relations. Meanwhile, previous works about commonsense causation only consider two events and ignore their context, simplifying the task formulation. This paper proposes a new task to detect commonsense causation between two events in an event sequence (i.e., context), called contextualized commonsense causal reasoning. We also design a zero-shot framework: COLA (Contextualized Commonsense Causality Reasoner) to solve the task from the causal inference perspective. This framework obtains rich incidental supervision from temporality and balances covariates from multiple timestamps to remove confounding effects. Our extensive experiments show that COLA can detect commonsense causality more accurately than baselines.\n        \u25b3 Less\n      ",
    "title": "COLA: Contextualized Commonsense Causal Reasoning from the Causal Inference Perspective",
    "date": "9 May, 2023",
    "authors": [
      "Zhaowei Wang",
      " Quyet V. Do",
      " Hongming Zhang",
      " Jiayao Zhang",
      " Weiqi Wang",
      " Tianqing Fang",
      " Yangqiu Song",
      " Ginny Y. Wong",
      " Simon See"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05200",
    "paper_id": "2305.05200",
    "abstract": "\n        In computer vision, the performance of deep neural networks (DNNs) is highly related to the feature extraction ability, i.e., the ability to recognize and focus on key pixel regions in an image. However, in this paper, we quantitatively and statistically illustrate that DNNs have a serious attention bias problem on many samples from some popular datasets: (1) Position bias: DNNs fully focus on label-independent regions; (2) Range bias: The focused regions from DNN are not completely contained in the ideal region. Moreover, we find that the existing self-attention modules can alleviate these biases to a certain extent, but the biases are still non-negligible. To further mitigate them, we propose a lightweight sub-attention strategy (LSAS), which utilizes high-order sub-attention modules to improve the original self-attention modules. The effectiveness of LSAS is demonstrated by extensive experiments on widely-used benchmark datasets and popular attention networks. We release our code to help other researchers to reproduce the results of LSAS~\\footnote{https://github.com/Qrange-group/LSAS}.\n        \u25b3 Less\n      ",
    "title": "LSAS: Lightweight Sub-attention Strategy for Alleviating Attention Bias Problem",
    "date": "9 May, 2023",
    "authors": [
      "Shanshan Zhong",
      " Wushao Wen",
      " Jinghui Qin",
      " Qiangpu Chen",
      " Zhongzhan Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16158",
    "paper_id": "2305.16158",
    "abstract": "\n        Digital Twins (DTs) are virtual representations of physical objects or processes that can collect information from the real environment to represent, validate, and replicate the physical twin's present and future behavior. The DTs are becoming increasingly prevalent in a variety of fields, including manufacturing, automobiles, medicine, smart cities, and other related areas. In this paper, we presented a systematic reviews on DTs in the autonomous vehicular industry. We addressed DTs and their essential characteristics, emphasized on accurate data collection, real-time analytics, and efficient simulation capabilities, while highlighting their role in enhancing performance and reliability. Next, we explored the technical challenges and central technologies of DTs. We illustrated the comparison analysis of different methodologies that have been used for autonomous vehicles in smart cities. Finally, we addressed the application challenges and limitations of DTs in the autonomous vehicular industry.\n        \u25b3 Less\n      ",
    "title": "A New Era of Mobility: Exploring Digital Twin Applications in Autonomous Vehicular Systems",
    "date": "9 May, 2023",
    "authors": [
      "S M Mostaq Hossain",
      " Sohag Kumar Saha",
      " Shampa Banik",
      " Trapa Banik"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2202.08146",
    "paper_id": "2202.08146",
    "abstract": "\n        Human Activity Recognition (HAR) research has gained significant momentum due to recent technological advancements, artificial intelligence algorithms, the need for smart cities, and socioeconomic transformation. However, existing computer vision and sensor-based HAR solutions have limitations such as privacy issues, memory and power consumption, and discomfort in wearing sensors for which researchers are observing a paradigm shift in HAR research. In response, WiFi-based HAR is gaining popularity due to the availability of more coarse-grained Channel State Information. However, existing WiFi-based HAR approaches are limited to classifying independent and non-concurrent human activities performed within equal time duration. Recent research commonly utilizes a Single Input Multiple Output communication link with a WiFi signal of 5 GHz channel frequency, using two WiFi routers or two Intel 5300 NICs as transmitter-receiver. Our study, on the other hand, utilizes a Multiple Input Multiple Output radio link between a WiFi router and an Intel 5300 NIC, with the time-series Wi-Fi channel state information based on 2.4 GHz channel frequency for mutual human-to-human concurrent interaction recognition. The proposed Self-Attention guided Bidirectional Gated Recurrent Neural Network (Attention-BiGRU) deep learning model can classify 13 mutual interactions with a maximum benchmark accuracy of 94% for a single subject-pair. This has been expanded for ten subject pairs, which secured a benchmark accuracy of 88% with improved classification around the interaction-transition region. An executable graphical user interface (GUI) software has also been developed in this study using the PyQt5 python module to classify, save, and display the overall mutual concurrent human interactions performed within a given time duration. ...\n        \u25b3 Less\n      ",
    "title": "A Prospective Approach for Human-to-Human Interaction Recognition from Wi-Fi Channel Data using Attention Bidirectional Gated Recurrent Neural Network with GUI Application Implementation",
    "date": "9 May, 2023",
    "authors": [
      "Md. Mohi Uddin Khan",
      " Abdullah Bin Shams",
      " Md. Mohsin Sarker Raihan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.11127",
    "paper_id": "2205.11127",
    "abstract": "\n        Recommender systems can strongly influence which information we see online, e.g., on social media, and thus impact our beliefs, decisions, and actions. At the same time, these systems can create substantial business value for different stakeholders. Given the growing potential impact of such AI-based systems on individuals, organizations, and society, questions of fairness have gained increased attention in recent years. However, research on fairness in recommender systems is still a developing area. In this survey, we first review the fundamental concepts and notions of fairness that were put forward in the area in the recent past. Afterward, through a review of more than 160 scholarly publications, we present an overview of how research in this field is currently operationalized, e.g., in terms of general research methodology, fairness measures, and algorithmic approaches. Overall, our analysis of recent works points to certain research gaps. In particular, we find that in many research works in computer science, very abstract problem operationalizations are prevalent and questions of the underlying normative claims and what represents a fair recommendation in the context of a given application are often not discussed in depth. These observations call for more interdisciplinary research to address fairness in recommendation in a more comprehensive and impactful manner.\n        \u25b3 Less\n      ",
    "title": "Fairness in Recommender Systems: Research Landscape and Future Directions",
    "date": "9 May, 2023",
    "authors": [
      "Yashar Deldjoo",
      " Dietmar Jannach",
      " Alejandro Bellogin",
      " Alessandro Difonzo",
      " Dario Zanzonelli"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05239",
    "paper_id": "2305.05239",
    "abstract": "\n        The exploration problem is one of the main challenges in deep reinforcement learning (RL). Recent promising works tried to handle the problem with population-based methods, which collect samples with diverse behaviors derived from a population of different exploratory policies. Adaptive policy selection has been adopted for behavior control. However, the behavior selection space is largely limited by the predefined policy population, which further limits behavior diversity. In this paper, we propose a general framework called Learnable Behavioral Control (LBC) to address the limitation, which a) enables a significantly enlarged behavior selection space via formulating a hybrid behavior mapping from all policies; b) constructs a unified learnable process for behavior selection. We introduce LBC into distributed off-policy actor-critic methods and achieve behavior control via optimizing the selection of the behavior mappings with bandit-based meta-controllers. Our agents have achieved 10077.52% mean human normalized score and surpassed 24 human world records within 1B training frames in the Arcade Learning Environment, which demonstrates our significant state-of-the-art (SOTA) performance without degrading the sample efficiency.\n        \u25b3 Less\n      ",
    "title": "Learnable Behavior Control: Breaking Atari Human World Records via Sample-Efficient Behavior Selection",
    "date": "9 May, 2023",
    "authors": [
      "Jiajun Fan",
      " Yuzheng Zhuang",
      " Yuecheng Liu",
      " Jianye Hao",
      " Bin Wang",
      " Jiangcheng Zhu",
      " Hao Wang",
      " Shu-Tao Xia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05247",
    "paper_id": "2305.05247",
    "abstract": "\n        The widespread adoption of electronic health records and digital healthcare data has created a demand for data-driven insights to enhance patient outcomes, diagnostics, and treatments. However, using real patient data presents privacy and regulatory challenges, including compliance with HIPAA and GDPR. Synthetic data generation, using generative AI models like GANs and VAEs offers a promising solution to balance valuable data access and patient privacy protection. In this paper, we examine generative AI models for creating realistic, anonymized patient data for research and training, explore synthetic data applications in healthcare, and discuss its benefits, challenges, and future research directions. Synthetic data has the potential to revolutionize healthcare by providing anonymized patient data while preserving privacy and enabling versatile applications.\n        \u25b3 Less\n      ",
    "title": "Leveraging Generative AI Models for Synthetic Data Generation in Healthcare: Balancing Research and Privacy",
    "date": "9 May, 2023",
    "authors": [
      "Aryan Jadon",
      " Shashank Kumar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08778",
    "paper_id": "2305.08778",
    "abstract": "\n        We address an important yet challenging problem - modeling high-dimensional dependencies across multivariates such as financial indicators in heterogeneous markets. In reality, a market couples and influences others over time, and the financial variables of a market are also coupled. We make the first attempt to integrate variational sequential neural learning with copula-based dependence modeling to characterize both temporal observable and latent variable-based dependence degrees and structures across non-normal multivariates. Our variational neural network WPVC-VLSTM models variational sequential dependence degrees and structures across multivariate time series by variational long short-term memory networks and regular vine copula. The regular vine copula models nonnormal and long-range distributional couplings across multiple dynamic variables. WPVC-VLSTM is verified in terms of both technical significance and portfolio forecasting performance. It outperforms benchmarks including linear models, stochastic volatility models, deep neural networks, and variational recurrent networks in cross-market portfolio forecasting.\n        \u25b3 Less\n      ",
    "title": "Copula Variational LSTM for High-dimensional Cross-market Multivariate Dependence Modeling",
    "date": "9 May, 2023",
    "authors": [
      "Jia Xu",
      " Longbing Cao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05268",
    "paper_id": "2305.05268",
    "abstract": "\n        In this paper we address the rotation synchronization problem, where the objective is to recover absolute rotations starting from pairwise ones, where the unknowns and the measures are represented as nodes and edges of a graph, respectively. This problem is an essential task for structure from motion and simultaneous localization and mapping. We focus on the formulation of synchronization via neural networks, which has only recently begun to be explored in the literature. Inspired by deep matrix completion, we express rotation synchronization in terms of matrix factorization with a deep neural network. Our formulation exhibits implicit regularization properties and, more importantly, is unsupervised, whereas previous deep approaches are supervised. Our experiments show that we achieve comparable accuracy to the closest competitors in most scenes, while working under weaker assumptions.\n        \u25b3 Less\n      ",
    "title": "Rotation Synchronization via Deep Matrix Factorization",
    "date": "9 May, 2023",
    "authors": [
      "Gk Tejus",
      " Giacomo Zara",
      " Paolo Rota",
      " Andrea Fusiello",
      " Elisa Ricci",
      " Federica Arrigoni"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2206.06119",
    "paper_id": "2206.06119",
    "abstract": "\n        C\u00f4te d'Ivoire and Ghana, the world's largest producers of cocoa, account for two thirds of the global cocoa production. In both countries, cocoa is the primary perennial crop, providing income to almost two million farmers. Yet precise maps of cocoa planted area are missing, hindering accurate quantification of expansion in protected areas, production and yields, and limiting information available for improved sustainability governance. Here, we combine cocoa plantation data with publicly available satellite imagery in a deep learning framework and create high-resolution maps of cocoa plantations for both countries, validated in situ. Our results suggest that cocoa cultivation is an underlying driver of over 37% and 13% of forest loss in protected areas in C\u00f4te d'Ivoire and Ghana, respectively, and that official reports substantially underestimate the planted area, up to 40% in Ghana. These maps serve as a crucial building block to advance understanding of conservation and economic development in cocoa producing regions.\n        \u25b3 Less\n      ",
    "title": "Satellite-based high-resolution maps of cocoa planted area for C\u00f4te d'Ivoire and Ghana",
    "date": "9 May, 2023",
    "authors": [
      "Nikolai Kalischek",
      " Nico Lang",
      " C\u00e9cile Renier",
      " Rodrigo Caye Daudt",
      " Thomas Addoah",
      " William Thompson",
      " Wilma J. Blaser-Hart",
      " Rachael Garrett",
      " Konrad Schindler",
      " Jan D. Wegner"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06158",
    "paper_id": "2305.06158",
    "abstract": "\n        We present a new encoder-decoder generative network dubbed EdgeNet, which introduces a novel encoder-decoder framework for data-driven auction design in online e-commerce advertising. We break the neural auction paradigm of Generalized-Second-Price(GSP), and improve the utilization efficiency of data while ensuring the economic characteristics of the auction mechanism. Specifically, EdgeNet introduces a transformer-based encoder to better capture the mutual influence among different candidate advertisements. In contrast to GSP based neural auction model, we design an autoregressive decoder to better utilize the rich context information in online advertising auctions. EdgeNet is conceptually simple and easy to extend to the existing end-to-end neural auction framework. We validate the efficiency of EdgeNet on a wide range of e-commercial advertising auction, demonstrating its potential in improving user experience and platform revenue.\n        \u25b3 Less\n      ",
    "title": "EdgeNet : Encoder-decoder generative Network for Auction Design in E-commerce Online Advertising",
    "date": "9 May, 2023",
    "authors": [
      "Guangyuan Shen",
      " Shengjie Sun",
      " Dehong Gao",
      " Libin Yang",
      " Yongping Shi",
      " Wei Ning"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.15479",
    "paper_id": "2205.15479",
    "abstract": "\n        We propose a novel method for code summarization utilizing Heterogeneous Code Representations (HCRs) and our specially designed HierarchyNet. HCRs effectively capture essential code features at lexical, syntactic, and semantic levels by abstracting coarse-grained code elements and incorporating fine-grained program elements in a hierarchical structure. Our HierarchyNet method processes each layer of the HCR separately through a unique combination of the Heterogeneous Graph Transformer, a Tree-based CNN, and a Transformer Encoder. This approach preserves dependencies between code elements and captures relations through a novel Hierarchical-Aware Cross Attention layer. Our method surpasses current state-of-the-art techniques, such as PA-Former, CAST, and NeuralCodeSum.\n        \u25b3 Less\n      ",
    "title": "HierarchyNet: Learning to Summarize Source Code with Heterogeneous Representations",
    "date": "9 May, 2023",
    "authors": [
      "Minh Huynh Nguyen",
      " Nghi D. Q. Bui",
      " Truong Son Hy",
      " Long Tran-Thanh",
      " Tien N. Nguyen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05311",
    "paper_id": "2305.05311",
    "abstract": "\n        Structured sentiment analysis (SSA) aims to automatically extract people's opinions from a text in natural language and adequately represent that information in a graph structure. One of the most accurate methods for performing SSA was recently proposed and consists of approaching it as a dependency parsing task. Although we can find in the literature how transition-based algorithms excel in dependency parsing in terms of accuracy and efficiency, all proposed attempts to tackle SSA following that approach were based on graph-based models. In this article, we present the first transition-based method to address SSA as dependency parsing. Specifically, we design a transition system that processes the input text in a left-to-right pass, incrementally generating the graph structure containing all identified opinions. To effectively implement our final transition-based model, we resort to a Pointer Network architecture as a backbone. From an extensive evaluation, we demonstrate that our model offers the best performance to date in practically all cases among prior dependency-based methods, and surpass recent task-specific techniques on the most challenging datasets. We additionally include an in-depth analysis and empirically prove that the overall time-complexity cost of our approach is quadratic in the sentence length, being more efficient than top-performing graph-based parsers.\n        \u25b3 Less\n      ",
    "title": "Structured Sentiment Analysis as Transition-based Dependency Parsing",
    "date": "9 May, 2023",
    "authors": [
      "Daniel Fern\u00e1ndez-Gonz\u00e1lez"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05321",
    "paper_id": "2305.05321",
    "abstract": "\n        Starches are important energy sources found in plants with many uses in the pharmaceutical industry such as binders, disintegrants, bulking agents in drugs and thus require very careful physicochemical analysis for proper identification and verification which includes microscopy. In this work, we applied artificial intelligence techniques (using transfer learning and deep convolution neural network CNNs to microscopical images obtained from 9 starch samples of different botanical sources. Our approach obtained an accuracy of 61% when the machine learning model was pretrained on microscopic images from MicroNet dataset. However the accuracy jumped to 81% for model pretrained on random day to day images obtained from Imagenet dataset. The model pretrained on the imagenet dataset also showed a better precision, recall and f1 score than that pretrained on the imagenet dataset.\n        \u25b3 Less\n      ",
    "title": "Application of Artificial Intelligence in the Classification of Microscopical Starch Images for Drug Formulation",
    "date": "9 May, 2023",
    "authors": [
      "Marvellous Ajala",
      " Blessing Oko",
      " David Oba-Fidelis",
      " Joycelyn Iyasele",
      " Joy I. Odimegwu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05325",
    "paper_id": "2305.05325",
    "abstract": "\n        As the impact of technology on our lives is increasing, we witness increased use of social media that became an essential tool not only for communication but also for sharing information with community about our thoughts and feelings. This can be observed also for people with mental health disorders such as depression where they use social media for expressing their thoughts and asking for help. This opens a possibility to automatically process social media posts and detect signs of depression. We build several large pre-trained language model based classifiers for depression detection from social media posts. Besides fine-tuning BERT, RoBERTA, BERTweet, and mentalBERT were also construct two types of ensembles. We analyze the performance of our models on two data sets of posts from social platforms Reddit and Twitter, and investigate also the performance of transfer learning across the two data sets. The results show that transformer ensembles improve over the single transformer-based classifiers.\n        \u25b3 Less\n      ",
    "title": "Detection of depression on social networks using transformers and ensembles",
    "date": "9 May, 2023",
    "authors": [
      "Ilija Tavchioski",
      " Marko Robnik-\u0160ikonja",
      " Senja Pollak"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.15259",
    "paper_id": "2209.15259",
    "abstract": "\n        Large AI Models (LAIMs), of which large language models are the most prominent recent example, showcase some impressive performance. However they have been empirically found to pose serious security issues. This paper systematizes our knowledge about the fundamental impossibility of building arbitrarily accurate and secure machine learning models. More precisely, we identify key challenging features of many of today's machine learning settings. Namely, high accuracy seems to require memorizing large training datasets, which are often user-generated and highly heterogeneous, with both sensitive information and fake users. We then survey statistical lower bounds that, we argue, constitute a compelling case against the possibility of designing high-accuracy LAIMs with strong security guarantees.\n        \u25b3 Less\n      ",
    "title": "On the Impossible Safety of Large AI Models",
    "date": "9 May, 2023",
    "authors": [
      "El-Mahdi El-Mhamdi",
      " Sadegh Farhadkhani",
      " Rachid Guerraoui",
      " Nirupam Gupta",
      " L\u00ea-Nguy\u00ean Hoang",
      " Rafael Pinot",
      " S\u00e9bastien Rouault",
      " John Stephan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02750",
    "paper_id": "2305.02750",
    "abstract": "\n        Proactive dialogue systems, related to a wide range of real-world conversational applications, equip the conversational agent with the capability of leading the conversation direction towards achieving pre-defined targets or fulfilling certain goals from the system side. It is empowered by advanced techniques to progress to more complicated tasks that require strategical and motivational interactions. In this survey, we provide a comprehensive overview of the prominent problems and advanced designs for conversational agent's proactivity in different types of dialogues. Furthermore, we discuss challenges that meet the real-world application needs but require a greater research focus in the future. We hope that this first survey of proactive dialogue systems can provide the community with a quick access and an overall picture to this practical problem, and stimulate more progresses on conversational AI to the next level.\n        \u25b3 Less\n      ",
    "title": "A Survey on Proactive Dialogue Systems: Problems, Methods, and Prospects",
    "date": "9 May, 2023",
    "authors": [
      "Yang Deng",
      " Wenqiang Lei",
      " Wai Lam",
      " Tat-Seng Chua"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05364",
    "paper_id": "2305.05364",
    "abstract": "\n        In recent years, large pre-trained language models (LLMs) have demonstrated the ability to follow instructions and perform novel tasks from a few examples. The possibility to parameterise an LLM through such in-context examples widens their capability at a much lower cost than finetuning. We extend this line of reasoning and present a method which further expands the capabilities of an LLM by embedding it within an algorithm or program. To demonstrate the benefits of this approach, we present an illustrative example of evidence-supported question-answering. We obtain a 6.4\\% improvement over the chain of thought baseline through a more algorithmic approach without any finetuning. Furthermore, we highlight recent work from this perspective and discuss the advantages and disadvantages in comparison to the standard approaches.\n        \u25b3 Less\n      ",
    "title": "Large Language Model Programs",
    "date": "9 May, 2023",
    "authors": [
      "Imanol Schlag",
      " Sainbayar Sukhbaatar",
      " Asli Celikyilmaz",
      " Wen-tau Yih",
      " Jason Weston",
      " J\u00fcrgen Schmidhuber",
      " Xian Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.17523",
    "paper_id": "2303.17523",
    "abstract": "\n        Although NISQ computers show great promise in accelerating many tasks that are not practically possible using classical computation, useful quantum computing is still a long way off. One important reason is due to the fragile nature of quantum hardware. As the building blocks of a quantum circuit (QC), quantum gates and qubits are susceptible to external interference, and therefore even a simple QC can produce extremely noisy output. Since it is hard to distinguish whether the output represents meaningful computation or just random noise, it raises the question of how much we can rely on the output of a QC, i.e., the fidelity of the QC. In this paper, we purpose a simple yet intuitive metric to measure the fidelity of a QC. By using this metric, we can observe the evolution of fidelity with time as the QC interacts with its external environment. Consequently, we can frame fidelity prediction as a Time Series Forecasting problem and use Long Short-Term Memory (LSTM) neural networks to better estimate the fidelity of a QC. This gives the user better opportunities to optimize the mapping of qubits into the quantum hardware for larger gains. We introduce the LSTM architecture and present a complete workflow to build the training circuit dataset. The trained LSTM system, Q-fid, can predict the output fidelity of a QC running on a specific quantum processor, without the need for any separate input of hardware calibration data or gate error rates. Evaluated on the QASMbench NISQ benchmark suite, Q-fid's prediction achieves an average RMSE of 0.0515, up to 24.7x more accurate than the default Qiskit transpile tool mapomatic. When used to find the high-fidelity circuit layouts from the available circuit transpilations, Q-fid predicts the fidelity for the top 10% layouts with an average RMSE of 0.0252, up to 32.8x more accurate than mapomatic.\n        \u25b3 Less\n      ",
    "title": "Quantum Circuit Fidelity Improvement with Long Short-Term Memory Networks",
    "date": "9 May, 2023",
    "authors": [
      "Yikai Mao",
      " Shaswot Shresthamali",
      " Masaaki Kondo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00984",
    "paper_id": "2305.00984",
    "abstract": "\n        One of the possible representations of three-valued instantaneous noise-based logic is proposed. The third value is an uncertain bit value, which can be useful in artificial intelligence applications. There is a forth value, too, that can represent a non-existing bit (vacuum-state) that is the same (1 numeric value) for all bits, however that is a squeezed state common for all bits. Some logic gates are explored. A ternary Universe has a significant advantage compared to the standard binary one: its amplitude is never zero during any clock period. All the known binary logic gates work for the binary bit values in the same way as earlier therefore the former binary algorithms can be run in the ternary system with no change and without the problems posed by zero values of the Universe.\n        \u25b3 Less\n      ",
    "title": "Ternary Instantaneous Noise-based Logic",
    "date": "9 May, 2023",
    "authors": [
      "Laszlo B. Kish"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05388",
    "paper_id": "2305.05388",
    "abstract": "\n        The VEDLIoT project aims to develop energy-efficient Deep Learning methodologies for distributed Artificial Intelligence of Things (AIoT) applications. During our project, we propose a holistic approach that focuses on optimizing algorithms while addressing safety and security challenges inherent to AIoT systems. The foundation of this approach lies in a modular and scalable cognitive IoT hardware platform, which leverages microserver technology to enable users to configure the hardware to meet the requirements of a diverse array of applications. Heterogeneous computing is used to boost performance and energy efficiency. In addition, the full spectrum of hardware accelerators is integrated, providing specialized ASICs as well as FPGAs for reconfigurable computing. The project's contributions span across trusted computing, remote attestation, and secure execution environments, with the ultimate goal of facilitating the design and deployment of robust and efficient AIoT systems. The overall architecture is validated on use-cases ranging from Smart Home to Automotive and Industrial IoT appliances. Ten additional use cases are integrated via an open call, broadening the range of application areas.\n        \u25b3 Less\n      ",
    "title": "VEDLIoT -- Next generation accelerated AIoT systems and applications",
    "date": "9 May, 2023",
    "authors": [
      "Kevin Mika",
      " Ren\u00e9 Griessl",
      " Nils Kucza",
      " Florian Porrmann",
      " Martin Kaiser",
      " Lennart Tigges",
      " Jens Hagemeyer",
      " Pedro Trancoso",
      " Muhammad Waqar Azhar",
      " Fareed Qararyah",
      " Stavroula Zouzoula",
      " J\u00e4mes M\u00e9n\u00e9trey",
      " Marcelo Pasin",
      " Pascal Felber",
      " Carina Marcus",
      " Oliver Brunnegard",
      " Olof Eriksson",
      " Hans Salomonsson",
      " Daniel \u00d6dman",
      " Andreas Ask",
      " Antonio Casimiro",
      " Alysson Bessani",
      " Tiago Carvalho",
      " Karol Gugala",
      " Piotr Zierhoffer ",
      " et al. (7 additional authors not shown)"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2208.11981",
    "paper_id": "2208.11981",
    "abstract": "\n        Recent advancements in Large Language Models (LLMs) harness linguistic associations in vast natural language data for practical applications. However, their ability to understand the physical world using only language data remains a question. After reviewing existing protocols, we explore this question using a novel and tightly controlled reasoning test (ART) and compare human norms against versions of GPT-3. Our findings highlight the categories of common-sense relations models that could learn directly from data and areas of weakness. GPT-3 offers evidence for verbal reasoning on a par with human subjects for several relations including Synonymy, Antonymy, and Default inheritance, Without reinforcement learning from human judgements, it appears GPT-3 performs at the lower end of the reference interval for Has-part and Contained-in. Weaknesses were observed also in affordance characteristics through Necessary-quality, Order-of-size and Order-of-intensity. Combining LLMs with symbolic world grounding is a promising direction to address associative learning.\n        \u25b3 Less\n      ",
    "title": "On Reality and the Limits of Language Data: Aligning LLMs with Human Norms",
    "date": "9 May, 2023",
    "authors": [
      "Nigel H. Collier",
      " Fangyu Liu",
      " Ehsan Shareghi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05420",
    "paper_id": "2305.05420",
    "abstract": "\n        'Mahabharata' is the most popular among many Indian pieces of literature referred to in many domains for completely different purposes. This text itself is having various dimension and aspects which is useful for the human being in their personal life and professional life. This Indian Epic is originally written in the Sanskrit Language. Now in the era of Natural Language Processing, Artificial Intelligence, Machine Learning, and Human-Computer interaction this text can be processed according to the domain requirement. It is interesting to process this text and get useful insights from Mahabharata. The limitation of the humans while analyzing Mahabharata is that they always have a sentiment aspect towards the story narrated by the author. Apart from that, the human cannot memorize statistical or computational details, like which two words are frequently coming in one sentence? What is the average length of the sentences across the whole literature? Which word is the most popular word across the text, what are the lemmas of the words used across the sentences? Thus, in this paper, we propose an NLP pipeline to get some statistical and computational insights along with the most relevant word searching method from the largest epic 'Mahabharata'. We stacked the different text-processing approaches to articulate the best results which can be further used in the various domain where Mahabharata needs to be referred.\n        \u25b3 Less\n      ",
    "title": "Estimating related words computationally using language model from the Mahabharata -- an Indian epic",
    "date": "9 May, 2023",
    "authors": [
      "Vrunda Gadesha",
      " Keyur D Joshi",
      " Shefali Naik"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.10669",
    "paper_id": "2210.10669",
    "abstract": "\n        Social media platforms are currently the main channel for political messaging, allowing politicians to target specific demographics and adapt based on their reactions. However, making this communication transparent is challenging, as the messaging is tightly coupled with its intended audience and often echoed by multiple stakeholders interested in advancing specific policies. Our goal in this paper is to take a first step towards understanding these highly decentralized settings. We propose a weakly supervised approach to identify the stance and issue of political ads on Facebook and analyze how political campaigns use some kind of demographic targeting by location, gender, or age. Furthermore, we analyze the temporal dynamics of the political ads on election polls.\n        \u25b3 Less\n      ",
    "title": "Weakly Supervised Learning for Analyzing Political Campaigns on Facebook",
    "date": "9 May, 2023",
    "authors": [
      "Tunazzina Islam",
      " Shamik Roy",
      " Dan Goldwasser"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05422",
    "paper_id": "2305.05422",
    "abstract": "\n        We are interested in aligning how people think about objects and what machines perceive, meaning by this the fact that object recognition, as performed by a machine, should follow a process which resembles that followed by humans when thinking of an object associated with a certain concept. The ultimate goal is to build systems which can meaningfully interact with their users, describing what they perceive in the users' own terms. As from the field of Lexical Semantics, humans organize the meaning of words in hierarchies where the meaning of, e.g., a noun, is defined in terms of the meaning of a more general noun, its genus, and of one or more differentiating properties, its differentia. The main tenet of this paper is that object recognition should implement a hierarchical process which follows the hierarchical semantic structure used to define the meaning of words. We achieve this goal by implementing an algorithm which, for any object, recursively recognizes its visual genus and its visual differentia. In other words, the recognition of an object is decomposed in a sequence of steps where the locally relevant visual features are recognized. This paper presents the algorithm and a first evaluation.\n        \u25b3 Less\n      ",
    "title": "Egocentric Hierarchical Visual Semantics",
    "date": "9 May, 2023",
    "authors": [
      "Luca Erculiani",
      " Andrea Bontempelli",
      " Andrea Passerini",
      " Fausto Giunchiglia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.16715",
    "paper_id": "2211.16715",
    "abstract": "\n        Reinforcement learning (RL) problems over general state and action spaces are notoriously challenging. In contrast to the tableau setting, one can not enumerate all the states and then iteratively update the policies for each state. This prevents the application of many well-studied RL methods especially those with provable convergence guarantees. In this paper, we first present a substantial generalization of the recently developed policy mirror descent method to deal with general state and action spaces. We introduce new approaches to incorporate function approximation into this method, so that we do not need to use explicit policy parameterization at all. Moreover, we present a novel policy dual averaging method for which possibly simpler function approximation techniques can be applied. We establish linear convergence rate to global optimality or sublinear convergence to stationarity for these methods applied to solve different classes of RL problems under exact policy evaluation. We then define proper notions of the approximation errors for policy evaluation and investigate their impact on the convergence of these methods applied to general-state RL problems with either finite-action or continuous-action spaces. To the best of our knowledge, the development of these algorithmic frameworks as well as their convergence analysis appear to be new in the literature.\n        \u25b3 Less\n      ",
    "title": "Policy Optimization over General State and Action Spaces",
    "date": "9 May, 2023",
    "authors": [
      "Guanghui Lan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.10952",
    "paper_id": "2210.10952",
    "abstract": "\n        The Work Disability Functional Assessment Battery (WD-FAB) is a multidimensional item response theory (IRT) instrument designed for assessing work-related mental and physical function based on responses to an item bank. In prior iterations it was developed using traditional means -- linear factorization and null hypothesis statistical testing for item partitioning/selection, and finally, posthoc calibration of disjoint unidimensional IRT models. As a result, the WD-FAB, like many other IRT instruments, is a posthoc model. Its item partitioning, based on exploratory factor analysis, is blind to the final nonlinear IRT model and is not performed in a manner consistent with goodness of fit to the final model. In this manuscript, we develop a Bayesian hierarchical model for self-consistently performing the following simultaneous tasks: scale factorization, item selection, parameter identification, and response scoring. This method uses sparsity-based shrinkage to obviate the linear factorization and null hypothesis statistical tests that are usually required for developing multidimensional IRT models, so that item partitioning is consistent with the ultimate nonlinear factor model. We also analogize our multidimensional IRT model to probabilistic autoencoders, specifying an encoder function that amortizes the inference of ability parameters from item responses. The encoder function is equivalent to the \"VBE\" step in a stochastic variational Bayesian expectation maximization (VBEM) procedure that we use for approxiamte Bayesian inference on the entire model. We use the method on a sample of WD-FAB item responses and compare the resulting item discriminations to those obtained using the traditional posthoc method.\n        \u25b3 Less\n      ",
    "title": "Autoencoded sparse Bayesian in-IRT factorization, calibration, and amortized inference for the Work Disability Functional Assessment Battery",
    "date": "9 May, 2023",
    "authors": [
      "Joshua C. Chang",
      " Carson C. Chow",
      " Julia Porcino"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.04362",
    "paper_id": "2209.04362",
    "abstract": "\n        Despite the success of neural networks in computer vision tasks, digital 'neurons' are a very loose approximation of biological neurons. Today's learning approaches are designed to function on digital devices with digital data representations such as image frames. In contrast, biological vision systems are generally much more capable and efficient than state-of-the-art digital computer vision algorithms. Event cameras are an emerging sensor technology which imitates biological vision with asynchronously firing pixels, eschewing the concept of the image frame. To leverage modern learning techniques, many event-based algorithms are forced to accumulate events back to image frames, somewhat squandering the advantages of event cameras.\n  We follow the opposite paradigm and develop a new type of neural network which operates closer to the original event data stream. We demonstrate state-of-the-art performance in angular velocity regression and competitive optical flow estimation, while avoiding difficulties related to training SNN. Furthermore, the processing latency of our proposed approach is less than 1/10 any other implementation, while continuous inference increases this improvement by another order of magnitude.\n        \u25b3 Less\n      ",
    "title": "EDeNN: Event Decay Neural Networks for low latency vision",
    "date": "9 May, 2023",
    "authors": [
      "Celyn Walters",
      " Simon Hadfield"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.13881",
    "paper_id": "2212.13881",
    "abstract": "\n        In recent years neural networks have achieved impressive results on many technological and scientific tasks. Yet, the mechanism through which these models automatically select features, or patterns in data, for prediction remains unclear. Identifying such a mechanism is key to advancing performance and interpretability of neural networks and promoting reliable adoption of these models in scientific applications. In this paper, we identify and characterize the mechanism through which deep fully connected neural networks learn features. We posit the Deep Neural Feature Ansatz, which states that neural feature learning occurs by implementing the average gradient outer product to up-weight features strongly related to model output. Our ansatz sheds light on various deep learning phenomena including emergence of spurious features and simplicity biases and how pruning networks can increase performance, the \"lottery ticket hypothesis.\" Moreover, the mechanism identified in our work leads to a backpropagation-free method for feature learning with any machine learning model. To demonstrate the effectiveness of this feature learning mechanism, we use it to enable feature learning in classical, non-feature learning models known as kernel machines and show that the resulting models, which we refer to as Recursive Feature Machines, achieve state-of-the-art performance on tabular data.\n        \u25b3 Less\n      ",
    "title": "Mechanism of feature learning in deep fully connected networks and kernel machines that recursively learn features",
    "date": "9 May, 2023",
    "authors": [
      "Adityanarayanan Radhakrishnan",
      " Daniel Beaglehole",
      " Parthe Pandit",
      " Mikhail Belkin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.00609",
    "paper_id": "2211.00609",
    "abstract": "\n        Recently, high-performing code generation systems based on large language models have surfaced. They are trained on massive corpora containing much more natural text than actual executable computer code. This work shows that current code generation systems exhibit undesired biases inherited from their large language model backbones, which can reduce the quality of the generated code under specific circumstances.\n  To investigate the effect, we propose the \"block of influence\" concept, which enables a modular decomposition and analysis of the coding challenges. We introduce an automated intervention mechanism reminiscent of adversarial testing that exposes undesired biases through the failure modes of the models under test. Finally, we demonstrate how our framework can be used as a data transformation technique during fine-tuning, acting as a mitigation strategy for these biases.\n        \u25b3 Less\n      ",
    "title": "A Simple, Yet Effective Approach to Finding Biases in Code Generation",
    "date": "9 May, 2023",
    "authors": [
      "Spyridon Mouselinos",
      " Mateusz Malinowski",
      " Henryk Michalewski"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.08176",
    "paper_id": "2302.08176",
    "abstract": "\n        We identify the logic behind the recent theory of coherent sets of desirable (sets of) things, which generalise desirable (sets of) gambles and coherent choice functions, and show that this identification allows us to establish various representation results for such coherent models in terms of simpler ones.\n        \u25b3 Less\n      ",
    "title": "The logic behind desirable sets of things, and its filter representation",
    "date": "9 May, 2023",
    "authors": [
      "Gert de Cooman",
      " Arthur Van Camp",
      " Jasper De Bock"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.03355",
    "paper_id": "2209.03355",
    "abstract": "\n        Single-Image Super-Resolution can support robotic tasks in environments where a reliable visual stream is required to monitor the mission, handle teleoperation or study relevant visual details. In this work, we propose an efficient Generative Adversarial Network model for real-time Super-Resolution, called EdgeSRGAN (code available at https://github.com/PIC4SeR/EdgeSRGAN). We adopt a tailored architecture of the original SRGAN and model quantization to boost the execution on CPU and Edge TPU devices, achieving up to 200 fps inference. We further optimize our model by distilling its knowledge to a smaller version of the network and obtain remarkable improvements compared to the standard training approach. Our experiments show that our fast and lightweight model preserves considerably satisfying image quality compared to heavier state-of-the-art models. Finally, we conduct experiments on image transmission with bandwidth degradation to highlight the advantages of the proposed system for mobile robotic applications.\n        \u25b3 Less\n      ",
    "title": "Generative Adversarial Super-Resolution at the Edge with Knowledge Distillation",
    "date": "9 May, 2023",
    "authors": [
      "Simone Angarano",
      " Francesco Salvetti",
      " Mauro Martini",
      " Marcello Chiaberge"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2202.01666",
    "paper_id": "2202.01666",
    "abstract": "\n        With the increasingly broad deployment of federated learning (FL) systems in the real world, it is critical but challenging to ensure fairness in FL, i.e. reasonably satisfactory performances for each of the numerous diverse clients. In this work, we introduce and study a new fairness notion in FL, called proportional fairness (PF), which is based on the relative change of each client's performance. From its connection with the bargaining games, we propose PropFair, a novel and easy-to-implement algorithm for finding proportionally fair solutions in FL and study its convergence properties. Through extensive experiments on vision and language datasets, we demonstrate that PropFair can approximately find PF solutions, and it achieves a good balance between the average performances of all clients and of the worst 10% clients. Our code is available at \\url{https://github.com/huawei-noah/Federated-Learning/tree/main/FairFL}.\n        \u25b3 Less\n      ",
    "title": "Proportional Fairness in Federated Learning",
    "date": "9 May, 2023",
    "authors": [
      "Guojun Zhang",
      " Saber Malekmohammadi",
      " Xi Chen",
      " Yaoliang Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05549",
    "paper_id": "2305.05549",
    "abstract": "\n        Human social behavior is influenced by individual differences in social preferences. Social value orientation (SVO) is a measurable personality trait which indicates the relative importance an individual places on their own and on others' welfare when making decisions. SVO and other individual difference variables are strong predictors of human behavior and social outcomes. However, there are transient changes human behavior associated with emotions that are not captured by individual differences alone. Integral emotions, the emotions which arise in direct response to a decision-making scenario, have been linked to temporary shifts in decision-making preferences.\n  In this work, we investigated the effects of moderating social preferences with integral emotions in multi-agent societies. We developed Svoie, a method for designing agents which make decisions based on established SVO policies, as well as alternative integral emotion policies in response to task outcomes. We conducted simulation experiments in a resource-sharing task environment, and compared societies of Svoie agents with societies of agents with fixed SVO policies. We find that societies of agents which adapt their behavior through integral emotions achieved similar collective welfare to societies of agents with fixed SVO policies, but with significantly reduced inequality between the welfare of agents with different SVO traits. We observed that by allowing agents to change their policy in response to task outcomes, agents can moderate their behavior to achieve greater social equality. \\end{abstract}\n        \u25b3 Less\n      ",
    "title": "Social Value Orientation and Integral Emotions in Multi-Agent Systems",
    "date": "9 May, 2023",
    "authors": [
      "Daniel Collins",
      " Conor Houghton",
      " Nirav Ajmeri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05562",
    "paper_id": "2305.05562",
    "abstract": "\n        Despite their limited interpretability, weights and biases are still the most popular encoding of the functions learned by ReLU Neural Networks (ReLU NNs). That is why we introduce SkelEx, an algorithm to extract a skeleton of the membership functions learned by ReLU NNs, making those functions easier to interpret and analyze. To the best of our knowledge, this is the first work that considers linear regions from the perspective of critical points. As a natural follow-up, we also introduce BoundEx, which is the first analytical method known to us to extract the decision boundary from the realization of a ReLU NN. Both of those methods introduce very natural visualization tool for ReLU NNs trained on low-dimensional data.\n        \u25b3 Less\n      ",
    "title": "SkelEx and BoundEx: Natural Visualization of ReLU Neural Networks",
    "date": "9 May, 2023",
    "authors": [
      "Pawel Pukowski",
      " Haiping Lu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05566",
    "paper_id": "2305.05566",
    "abstract": "\n        There is a lack of standard benchmarks for Multi-Agent Reinforcement Learning (MARL) algorithms. The Starcraft Multi-Agent Challenge (SMAC) has been widely used in MARL research, but is built on top of a heavy, closed-source computer game, StarCraft II. Thus, SMAC is computationally expensive and requires knowledge and the use of proprietary tools specific to the game for any meaningful alteration or contribution to the environment. We introduce SMAClite -- a challenge based on SMAC that is both decoupled from Starcraft II and open-source, along with a framework which makes it possible to create new content for SMAClite without any special knowledge. We conduct experiments to show that SMAClite is equivalent to SMAC, by training MARL algorithms on SMAClite and reproducing SMAC results. We then show that SMAClite outperforms SMAC in both runtime speed and memory.\n        \u25b3 Less\n      ",
    "title": "SMAClite: A Lightweight Environment for Multi-Agent Reinforcement Learning",
    "date": "9 May, 2023",
    "authors": [
      "Adam Michalski",
      " Filippos Christianos",
      " Stefano V. Albrecht"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05594",
    "paper_id": "2305.05594",
    "abstract": "\n        A signed distance function (SDF) parametrized by an MLP is a common ingredient of neural surface reconstruction. We build on the successful recent method NeuS to extend it by three new components. The first component is to borrow the tri-plane representation from EG3D and represent signed distance fields as a mixture of tri-planes and MLPs instead of representing it with MLPs only. Using tri-planes leads to a more expressive data structure but will also introduce noise in the reconstructed surface. The second component is to use a new type of positional encoding with learnable weights to combat noise in the reconstruction process. We divide the features in the tri-plane into multiple frequency scales and modulate them with sin and cos functions of different frequencies. The third component is to use learnable convolution operations on the tri-plane features using self-attention convolution to produce features with different frequency bands. The experiments show that PET-NeuS achieves high-fidelity surface reconstruction on standard datasets. Following previous work and using the Chamfer metric as the most important way to measure surface reconstruction quality, we are able to improve upon the NeuS baseline by 57% on Nerf-synthetic (0.84 compared to 1.97) and by 15.5% on DTU (0.71 compared to 0.84). The qualitative evaluation reveals how our method can better control the interference of high-frequency noise. Code available at \\url{https://github.com/yiqun-wang/PET-NeuS}.\n        \u25b3 Less\n      ",
    "title": "PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces",
    "date": "9 May, 2023",
    "authors": [
      "Yiqun Wang",
      " Ivan Skorokhodov",
      " Peter Wonka"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05597",
    "paper_id": "2305.05597",
    "abstract": "\n        As voice-based Conversational Assistants (CAs), including Alexa, Siri, Google Home, have become commonly embedded in households, many children now routinely interact with Artificial Intelligence (AI) systems. It is important to research children's experiences with consumer devices which use AI techniques because these shape their understanding of AI and its capabilities. We conducted a mixed-methods study (questionnaires and interviews) with primary-school children aged 6-11 in Scotland to establish children's understanding of how voice-based CAs work, how they perceive their cognitive abilities, agency and other human-like qualities, their awareness and trust of privacy aspects when using CAs and what they perceive as appropriate verbal interactions with CAs. Most children overestimated the CAs' intelligence and were uncertain about the systems' feelings or agency. They also lacked accurate understanding of data privacy and security aspects, and believed it was wrong to be rude to conversational assistants. Exploring children's current understanding of AI-supported technology has educational implications; such findings will enable educators to develop appropriate materials to address the pressing need for AI literacy.\n        \u25b3 Less\n      ",
    "title": "\"Alexa doesn't have that many feelings\": Children's understanding of AI through interactions with smart speakers in their homes",
    "date": "9 May, 2023",
    "authors": [
      "Valentina Andries",
      " Judy Robertson"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05609",
    "paper_id": "2305.05609",
    "abstract": "\n        Background: Artificial intelligence language models have shown promise in various applications, including assisting with clinical decision-making as demonstrated by strong performance of large language models on medical licensure exams. However, their ability to solve complex, open-ended cases, which may be representative of clinical practice, remains unexplored. Methods: In this study, the accuracy of large language AI models GPT4 and GPT3.5 in diagnosing complex clinical cases was investigated using published Case Records of the Massachusetts General Hospital. A total of 50 cases requiring a diagnosis and diagnostic test published from January 1, 2022 to April 16, 2022 were identified. For each case, models were given a prompt requesting the top three specific diagnoses and associated diagnostic tests, followed by case text, labs, and figure legends. Model outputs were assessed in comparison to the final clinical diagnosis and whether the model-predicted test would result in a correct diagnosis. Results: GPT4 and GPT3.5 accurately provided the correct diagnosis in 26% and 22% of cases in one attempt, and 46% and 42% within three attempts, respectively. GPT4 and GPT3.5 provided a correct essential diagnostic test in 28% and 24% of cases in one attempt, and 44% and 50% within three attempts, respectively. No significant differences were found between the two models, and multiple trials with identical prompts using the GPT3.5 model provided similar results. Conclusions: In summary, these models demonstrate potential usefulness in generating differential diagnoses but remain limited in their ability to provide a single unifying diagnosis in complex, open-ended cases. Future research should focus on evaluating model performance in larger datasets of open-ended clinical challenges and exploring potential human-AI collaboration strategies to enhance clinical decision-making.\n        \u25b3 Less\n      ",
    "title": "The Case Records of ChatGPT: Language Models and Complex Clinical Questions",
    "date": "9 May, 2023",
    "authors": [
      "Timothy Poterucha",
      " Pierre Elias",
      " Christopher M. Haggerty"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.01339",
    "paper_id": "2302.01339",
    "abstract": "\n        Can large language models be trained to produce philosophical texts that are difficult to distinguish from texts produced by human philosophers? To address this question, we fine-tuned OpenAI's GPT-3 with the works of philosopher Daniel C. Dennett as additional training data. To explore the Dennett model, we asked the real Dennett ten philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. We recruited 425 participants to distinguish Dennett's answer from the four machine-generated answers. Experts on Dennett's work (N = 25) succeeded 51% of the time, above the chance rate of 20% but short of our hypothesized rate of 80% correct. For two of the ten questions, the language model produced at least one answer that experts selected more frequently than Dennett's own answer. Philosophy blog readers (N = 302) performed similarly to the experts, while ordinary research participants (N = 98) were near chance distinguishing GPT-3's responses from those of an \"actual human philosopher\".\n        \u25b3 Less\n      ",
    "title": "Creating a Large Language Model of a Philosopher",
    "date": "9 May, 2023",
    "authors": [
      "Eric Schwitzgebel",
      " David Schwitzgebel",
      " Anna Strasser"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05648",
    "paper_id": "2305.05648",
    "abstract": "\n        Cardiovascular diseases (CVDs) are responsible for a large proportion of premature deaths in low- and middle-income countries. Early CVD detection and intervention is critical in these populations, yet many existing CVD risk scores require a physical examination or lab measurements, which can be challenging in such health systems due to limited accessibility. Here we investigated the potential to use photoplethysmography (PPG), a sensing technology available on most smartphones that can potentially enable large-scale screening at low cost, for CVD risk prediction. We developed a deep learning PPG-based CVD risk score (DLS) to predict the probability of having major adverse cardiovascular events (MACE: non-fatal myocardial infarction, stroke, and cardiovascular death) within ten years, given only age, sex, smoking status and PPG as predictors. We compared the DLS with the office-based refit-WHO score, which adopts the shared predictors from WHO and Globorisk scores (age, sex, smoking status, height, weight and systolic blood pressure) but refitted on the UK Biobank (UKB) cohort. In UKB cohort, DLS's C-statistic (71.1%, 95% CI 69.9-72.4) was non-inferior to office-based refit-WHO score (70.9%, 95% CI 69.7-72.2; non-inferiority margin of 2.5%, p<0.01). The calibration of the DLS was satisfactory, with a 1.8% mean absolute calibration error. Adding DLS features to the office-based score increased the C-statistic by 1.0% (95% CI 0.6-1.4). DLS predicts ten-year MACE risk comparable with the office-based refit-WHO score. It provides a proof-of-concept and suggests the potential of a PPG-based approach strategies for community-based primary prevention in resource-limited regions.\n        \u25b3 Less\n      ",
    "title": "Predicting Cardiovascular Disease Risk using Photoplethysmography and Deep Learning",
    "date": "9 May, 2023",
    "authors": [
      "Wei-Hung Weng",
      " Sebastien Baur",
      " Mayank Daswani",
      " Christina Chen",
      " Lauren Harrell",
      " Sujay Kakarmath",
      " Mariam Jabara",
      " Babak Behsaz",
      " Cory Y. McLean",
      " Yossi Matias",
      " Greg S. Corrado",
      " Shravya Shetty",
      " Shruthi Prabhakara",
      " Yun Liu",
      " Goodarz Danaei",
      " Diego Ardila"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05653",
    "paper_id": "2305.05653",
    "abstract": "\n        Where is everybody? This phrase distills the foreboding of what has come to be known as the Fermi Paradox - the disquieting idea that, if extraterrestrial life is probable in the Universe, then why have we not encountered it? This conundrum has puzzled scholars for decades, and many hypotheses have been proposed suggesting both naturalistic and sociological explanations. One intriguing hypothesis is known as the Great Filter, which suggests that some event required for the emergence of intelligent life is extremely unlikely, hence the cosmic silence. A logically equivalent version of this hypothesis -- and one that should give us pause -- suggests that some catastrophic event is likely to occur that prevents life's expansion throughout the cosmos. This could be a naturally occurring event, or more disconcertingly, something that intelligent beings do to themselves that leads to their own extinction. From an intelligence perspective, framing global catastrophic risk (particularly risks of anthropogenic origin) within the context of the Great Filter can provide insight into the long-term futures of technologies that we don't fully understand, like artificial intelligence. For the intelligence professional concerned with global catastrophic risk, this has significant implications for how these risks ought to be prioritized.\n        \u25b3 Less\n      ",
    "title": "Could AI be the Great Filter? What Astrobiology can Teach the Intelligence Community about Anthropogenic Risks",
    "date": "9 May, 2023",
    "authors": [
      "Mark M. Bailey"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06159",
    "paper_id": "2305.06159",
    "abstract": "\n        Moderation of social media content is currently a highly manual task, yet there is too much content posted daily to do so effectively. With the advent of a number of multimodal models, there is the potential to reduce the amount of manual labor for this task. In this work, we aim to explore different models and determine what is most effective for the Hateful Memes Challenge, a challenge by Meta designed to further machine learning research in content moderation. Specifically, we explore the differences between early fusion and late fusion models in classifying multimodal memes containing text and images. We first implement a baseline using unimodal models for text and images separately using BERT and ResNet-152, respectively. The outputs from these unimodal models were then concatenated together to create a late fusion model. In terms of early fusion models, we implement ConcatBERT, VisualBERT, ViLT, CLIP, and BridgeTower. It was found that late fusion performed significantly worse than early fusion models, with the best performing model being CLIP which achieved an AUROC of 70.06. The code for this work is available at https://github.com/bzhao18/CS-7643-Project.\n        \u25b3 Less\n      ",
    "title": "A Review of Vision-Language Models and their Performance on the Hateful Memes Challenge",
    "date": "9 May, 2023",
    "authors": [
      "Bryan Zhao",
      " Andrew Zhang",
      " Blake Watson",
      " Gillian Kearney",
      " Isaac Dale"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05661",
    "paper_id": "2305.05661",
    "abstract": "\n        Programs are an increasingly popular representation for visual data, exposing compact, interpretable structure that supports manipulation. Visual programs are usually written in domain-specific languages (DSLs). Finding \"good\" programs, that only expose meaningful degrees of freedom, requires access to a DSL with a \"good\" library of functions, both of which are typically authored by domain experts. We present ShapeCoder, the first system capable of taking a dataset of shapes, represented with unstructured primitives, and jointly discovering (i) useful abstraction functions and (ii) programs that use these abstractions to explain the input shapes. The discovered abstractions capture common patterns (both structural and parametric) across the dataset, so that programs rewritten with these abstractions are more compact, and expose fewer degrees of freedom. ShapeCoder improves upon previous abstraction discovery methods, finding better abstractions, for more complex inputs, under less stringent input assumptions. This is principally made possible by two methodological advancements: (a) a shape to program recognition network that learns to solve sub-problems and (b) the use of e-graphs, augmented with a conditional rewrite scheme, to determine when abstractions with complex parametric expressions can be applied, in a tractable manner. We evaluate ShapeCoder on multiple datasets of 3D shapes, where primitive decompositions are either parsed from manual annotations or produced by an unsupervised cuboid abstraction method. In all domains, ShapeCoder discovers a library of abstractions that capture high-level relationships, remove extraneous degrees of freedom, and achieve better dataset compression compared with alternative approaches. Finally, we investigate how programs rewritten to use discovered abstractions prove useful for downstream tasks.\n        \u25b3 Less\n      ",
    "title": "ShapeCoder: Discovering Abstractions for Visual Programs from Unstructured Primitives",
    "date": "9 May, 2023",
    "authors": [
      "R. Kenny Jones",
      " Paul Guerrero",
      " Niloy J. Mitra",
      " Daniel Ritchie"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05726",
    "paper_id": "2305.05726",
    "abstract": "\n        The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of interest and research in the field of large language models for Artificial General Intelligence (AGI). These models provide us with intelligent solutions that are more similar to human thinking, enabling us to use general artificial intelligence to solve problems in various applications. However, in the field of remote sensing, the scientific literature on the implementation of AGI remains relatively scant. Existing AI-related research primarily focuses on visual understanding tasks while neglecting the semantic understanding of the objects and their relationships. This is where vision-language models excel, as they enable reasoning about images and their associated textual descriptions, allowing for a deeper understanding of the underlying semantics. Vision-language models can go beyond recognizing the objects in an image and can infer the relationships between them, as well as generate natural language descriptions of the image. This makes them better suited for tasks that require both visual and textual understanding, such as image captioning, text-based image retrieval, and visual question answering. This paper provides a comprehensive review of the research on vision-language models in remote sensing, summarizing the latest progress, highlighting the current challenges, and identifying potential research opportunities. Specifically, we review the application of vision-language models in several mainstream remote sensing tasks, including image captioning, text-based image generation, text-based image retrieval, visual question answering, scene classification, semantic segmentation, and object detection. For each task, we briefly describe the task background and review some representative works. Finally, we summarize the limitations of existing work and provide some possible directions for future development.\n        \u25b3 Less\n      ",
    "title": "Vision-Language Models in Remote Sensing: Current Progress and Future Trends",
    "date": "9 May, 2023",
    "authors": [
      "Congcong Wen",
      " Yuan Hu",
      " Xiang Li",
      " Zhenghang Yuan",
      " Xiao Xiang Zhu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05739",
    "paper_id": "2305.05739",
    "abstract": "\n        We study the complexity of reductions for weighted reachability in parametric Markov decision processes. That is, we say a state p is never worse than q if for all valuations of the polynomial indeterminates it is the case that the maximal expected weight that can be reached from p is greater than the same value from q. In terms of computational complexity, we establish that determining whether p is never worse than q is coETR-complete. On the positive side, we give a polynomial-time algorithm to compute the equivalence classes of the order we study for Markov chains. Additionally, we describe and implement two inference rules to under-approximate the never-worse relation and empirically show that it can be used as an efficient preprocessing step for the analysis of large Markov decision processes.\n        \u25b3 Less\n      ",
    "title": "Graph-Based Reductions for Parametric and Weighted MDPs",
    "date": "9 May, 2023",
    "authors": [
      "Kasper Engelen",
      " Guillermo A. P\u00e9rez",
      " Shrisha Rao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.00497",
    "paper_id": "2211.00497",
    "abstract": "\n        Deep learning approaches for black-box modelling of audio effects have shown promise, however, the majority of existing work focuses on nonlinear effects with behaviour on relatively short time-scales, such as guitar amplifiers and distortion. While recurrent and convolutional architectures can theoretically be extended to capture behaviour at longer time scales, we show that simply scaling the width, depth, or dilation factor of existing architectures does not result in satisfactory performance when modelling audio effects such as fuzz and dynamic range compression. To address this, we propose the integration of time-varying feature-wise linear modulation into existing temporal convolutional backbones, an approach that enables learnable adaptation of the intermediate activations. We demonstrate that our approach more accurately captures long-range dependencies for a range of fuzz and compressor implementations across both time and frequency domain metrics. We provide sound examples, source code, and pretrained models to faciliate reproducibility.\n        \u25b3 Less\n      ",
    "title": "Modelling black-box audio effects with time-varying feature modulation",
    "date": "9 May, 2023",
    "authors": [
      "Marco Comunit\u00e0",
      " Christian J. Steinmetz",
      " Huy Phan",
      " Joshua D. Reiss"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11882",
    "paper_id": "2305.11882",
    "abstract": "\n        Teamwork is a critical component of many academic and professional settings. In those contexts, feedback between team members is an important element to facilitate successful and sustainable teamwork. However, in the classroom, as the number of teams and team members and frequency of evaluation increase, the volume of comments can become overwhelming for an instructor to read and track, making it difficult to identify patterns and areas for student improvement. To address this challenge, we explored the use of generative AI models, specifically ChatGPT, to analyze student comments in team based learning contexts. Our study aimed to evaluate ChatGPT's ability to accurately identify topics in student comments based on an existing framework consisting of positive and negative comments. Our results suggest that ChatGPT can achieve over 90\\% accuracy in labeling student comments, providing a potentially valuable tool for analyzing feedback in team projects. This study contributes to the growing body of research on the use of AI models in educational contexts and highlights the potential of ChatGPT for facilitating analysis of student comments.\n        \u25b3 Less\n      ",
    "title": "Exploring the Efficacy of ChatGPT in Analyzing Student Teamwork Feedback with an Existing Taxonomy",
    "date": "9 May, 2023",
    "authors": [
      "Andrew Katz",
      " Siqing Wei",
      " Gaurav Nanda",
      " Christopher Brinton",
      " Matthew Ohland"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05748",
    "paper_id": "2305.05748",
    "abstract": "\n        Representing text into a multidimensional space can be done with sentence embedding models such as Sentence-BERT (SBERT). However, training these models when the data has a complex multilevel structure requires individually trained class-specific models, which increases time and computing costs. We propose a two step approach which enables us to map sentences according to their hierarchical memberships and polarity. At first we teach the upper level sentence space through an AdaCos loss function and then finetune with a novel loss function mainly based on the cosine similarity of intra-level pairs. We apply this method to three different datasets: two weakly supervised Big Five personality dataset obtained from English and Japanese Twitter data and the benchmark MNLI dataset. We show that our single model approach performs better than multiple class-specific classification models.\n        \u25b3 Less\n      ",
    "title": "Multilevel Sentence Embeddings for Personality Prediction",
    "date": "9 May, 2023",
    "authors": [
      "Paolo Tirotta",
      " Akira Yuasa",
      " Masashi Morita"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05750",
    "paper_id": "2305.05750",
    "abstract": "\n        Artificial Intelligence (AI) and, in particular, Machine Learning (ML) have emerged to be utilized in various applications due to their capability to learn how to solve complex problems. Over the last decade, rapid advances in ML have presented Deep Neural Networks (DNNs) consisting of a large number of neurons and layers. DNN Hardware Accelerators (DHAs) are leveraged to deploy DNNs in the target applications. Safety-critical applications, where hardware faults/errors would result in catastrophic consequences, also benefit from DHAs. Therefore, the reliability of DNNs is an essential subject of research. In recent years, several studies have been published accordingly to assess the reliability of DNNs. In this regard, various reliability assessment methods have been proposed on a variety of platforms and applications. Hence, there is a need to summarize the state of the art to identify the gaps in the study of the reliability of DNNs. In this work, we conduct a Systematic Literature Review (SLR) on the reliability assessment methods of DNNs to collect relevant research works as much as possible, present a categorization of them, and address the open challenges. Through this SLR, three kinds of methods for reliability assessment of DNNs are identified including Fault Injection (FI), Analytical, and Hybrid methods. Since the majority of works assess the DNN reliability by FI, we characterize different approaches and platforms of the FI method comprehensively. Moreover, Analytical and Hybrid methods are propounded. Thus, different reliability assessment methods for DNNs have been elaborated on their conducted DNN platforms and reliability evaluation metrics. Finally, we highlight the advantages and disadvantages of the identified methods and address the open challenges in the research area.\n        \u25b3 Less\n      ",
    "title": "A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks",
    "date": "9 May, 2023",
    "authors": [
      "Mohammad Hasan Ahmadilivani",
      " Mahdi Taheri",
      " Jaan Raik",
      " Masoud Daneshtalab",
      " Maksim Jenihhin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05754",
    "paper_id": "2305.05754",
    "abstract": "\n        In collaborative tasks, effective communication is crucial for achieving joint goals. One such task is collaborative building where builders must communicate with each other to construct desired structures in a simulated environment such as Minecraft. We aim to develop an intelligent builder agent to build structures based on user input through dialogue. However, in collaborative building, builders may encounter situations that are difficult to interpret based on the available information and instructions, leading to ambiguity. In the NeurIPS 2022 Competition NLP Task, we address two key research questions, with the goal of filling this gap: when should the agent ask for clarification, and what clarification questions should it ask? We move towards this target with two sub-tasks, a classification task and a ranking task. For the classification task, the goal is to determine whether the agent should ask for clarification based on the current world state and dialogue history. For the ranking task, the goal is to rank the relevant clarification questions from a pool of candidates. In this report, we briefly introduce our methods for the classification and ranking task. For the classification task, our model achieves an F1 score of 0.757, which placed the 3rd on the leaderboard. For the ranking task, our model achieves about 0.38 for Mean Reciprocal Rank by extending the traditional ranking model. Lastly, we discuss various neural approaches for the ranking task and future direction.\n        \u25b3 Less\n      ",
    "title": "When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution",
    "date": "9 May, 2023",
    "authors": [
      "Zhengxiang Shi",
      " Jerome Ramos",
      " To Eun Kim",
      " Xi Wang",
      " Hossein A. Rahmani",
      " Aldo Lipani"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05759",
    "paper_id": "2305.05759",
    "abstract": "\n        Recent work has shown that standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on underrepresented groups due to the prevalence of spurious features. A predominant approach to tackle this group robustness problem minimizes the worst group error (akin to a minimax strategy) on the training data, hoping it will generalize well on the testing data. However, this is often suboptimal, especially when the out-of-distribution (OOD) test data contains previously unseen groups. Inspired by ideas from the information retrieval and learning-to-rank literature, this paper first proposes to use Discounted Cumulative Gain (DCG) as a metric of model quality for facilitating better hyperparameter tuning and model selection. Being a ranking-based metric, DCG weights multiple poorly-performing groups (instead of considering just the group with the worst performance). As a natural next step, we build on our results to propose a ranking-based training method called Discounted Rank Upweighting (DRU), which differentially reweights a ranked list of poorly-performing groups in the training data to learn models that exhibit strong OOD performance on the test data. Results on several synthetic and real-world datasets highlight the superior generalization ability of our group-ranking-based (akin to soft-minimax) approach in selecting and learning models that are robust to group distributional shifts.\n        \u25b3 Less\n      ",
    "title": "Ranking & Reweighting Improves Group Distributional Robustness",
    "date": "9 May, 2023",
    "authors": [
      "Yachuan Liu",
      " Bohan Zhang",
      " Qiaozhu Mei",
      " Paramveer Dhillon"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2108.03899",
    "paper_id": "2108.03899",
    "abstract": "\n        We propose a concise function representation based on deterministic finite state automata for exact most probable explanation and constrained optimization tasks in graphical models. We then exploit our concise representation within Bucket Elimination (BE). We denote our version of BE as FABE. FABE significantly improves the performance of BE in terms of runtime and memory requirements by minimizing redundancy. Results on most probable explanation and weighted constraint satisfaction benchmarks show that FABE often outperforms the state of the art, leading to significant runtime improvements (up to 5 orders of magnitude in our tests).\n        \u25b3 Less\n      ",
    "title": "Faster Exact MPE and Constrained Optimization with Deterministic Finite State Automata",
    "date": "9 May, 2023",
    "authors": [
      "Filippo Bistaffa"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05811",
    "paper_id": "2305.05811",
    "abstract": "\n        This article presents a framework for generating optimisation models using a pre-trained generative transformer. The framework involves specifying the features that the optimisation model should have and using a language model to generate an initial version of the model. The model is then tested and validated, and if it contains build errors, an automatic edition process is triggered. An experiment was performed using MiniZinc as the target language and two GPT-3.5 language models for generation and debugging. The results show that the use of language models for the generation of optimisation models is feasible, with some models satisfying the requested specifications, while others require further refinement. The study provides promising evidence for the use of language models in the modelling of optimisation problems and suggests avenues for future research.\n        \u25b3 Less\n      ",
    "title": "Towards an Automatic Optimisation Model Generator Assisted with Generative Pre-trained Transformer",
    "date": "9 May, 2023",
    "authors": [
      "Boris Almonacid"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05821",
    "paper_id": "2305.05821",
    "abstract": "\n        There is significant evidence that real-world communication cannot be reduced to sending signals with context-independent meaning. In this work, based on a variant of the classical Lewis (1969) signaling model, we explore the conditions for the emergence of context-dependent communication in a situated scenario. In particular, we demonstrate that pressure to minimise the vocabulary size is sufficient for such emergence. At the same time, we study the environmental conditions and cognitive capabilities that enable contextual disambiguation of symbol meanings. We show that environmental constraints on the receiver's referent choice can be unilaterally exploited by the sender, without disambiguation capabilities on the receiver's end. Consistent with common assumptions, the sender's awareness of the context appears to be required for contextual communication. We suggest that context-dependent communication is a situated multilayered phenomenon, crucially influenced by environment properties such as distribution of contexts. The model developed in this work is a demonstration of how signals may be ambiguous out of context, but still allow for near-perfect communication accuracy.\n        \u25b3 Less\n      ",
    "title": "Context-dependent communication under environmental constraints",
    "date": "9 May, 2023",
    "authors": [
      "Krzysztof G\u0142\u00f3wka",
      " Julian Zubek",
      " Joanna R\u0105czaszek-Leonardi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05840",
    "paper_id": "2305.05840",
    "abstract": "\n        In the field of Explainable Artificial Intelligence (XAI), counterfactual examples explain to a user the predictions of a trained decision model by indicating the modifications to be made to the instance so as to change its associated prediction. These counterfactual examples are generally defined as solutions to an optimization problem whose cost function combines several criteria that quantify desiderata for a good explanation meeting user needs. A large variety of such appropriate properties can be considered, as the user needs are generally unknown and differ from one user to another; their selection and formalization is difficult. To circumvent this issue, several approaches propose to generate, rather than a single one, a set of diverse counterfactual examples to explain a prediction. This paper proposes a review of the numerous, sometimes conflicting, definitions that have been proposed for this notion of diversity. It discusses their underlying principles as well as the hypotheses on the user needs they rely on and proposes to categorize them along several dimensions (explicit vs implicit, universe in which they are defined, level at which they apply), leading to the identification of further research challenges on this topic.\n        \u25b3 Less\n      ",
    "title": "Achieving Diversity in Counterfactual Explanations: a Review and Discussion",
    "date": "9 May, 2023",
    "authors": [
      "Thibault Laugel",
      " Adulam Jeyasothy",
      " Marie-Jeanne Lesot",
      " Christophe Marsala",
      " Marcin Detyniecki"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05845",
    "paper_id": "2305.05845",
    "abstract": "\n        The proliferation of video content demands efficient and flexible neural network based approaches for generating new video content. In this paper, we propose a novel approach that combines zero-shot text-to-video generation with ControlNet to improve the output of these models. Our method takes multiple sketched frames as input and generates video output that matches the flow of these frames, building upon the Text-to-Video Zero architecture and incorporating ControlNet to enable additional input conditions. By first interpolating frames between the inputted sketches and then running Text-to-Video Zero using the new interpolated frames video as the control technique, we leverage the benefits of both zero-shot text-to-video generation and the robust control provided by ControlNet. Experiments demonstrate that our method excels at producing high-quality and remarkably consistent video content that more accurately aligns with the user's intended motion for the subject within the video. We provide a comprehensive resource package, including a demo video, project website, open-source GitHub repository, and a Colab playground to foster further research and application of our proposed method.\n        \u25b3 Less\n      ",
    "title": "Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models",
    "date": "9 May, 2023",
    "authors": [
      "Rohan Dhesikan",
      " Vignesh Rajmohan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.01228",
    "paper_id": "2304.01228",
    "abstract": "\n        Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a simple data augmentation framework. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to generate pseudo data, which is then used as training data for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in code-related sequence generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.\n        \u25b3 Less\n      ",
    "title": "Better Language Models of Code through Self-Improvement",
    "date": "9 May, 2023",
    "authors": [
      "Hung Quoc To",
      " Nghi D. Q. Bui",
      " Jin Guo",
      " Tien N. Nguyen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05848",
    "paper_id": "2305.05848",
    "abstract": "\n        Recommender systems are essential to various fields, e.g., e-commerce, e-learning, and streaming media. At present, graph neural networks (GNNs) for session-based recommendations normally can only recommend items existing in users' historical sessions. As a result, these GNNs have difficulty recommending items that users have never interacted with (new items), which leads to a phenomenon of information cocoon. Therefore, it is necessary to recommend new items to users. As there is no interaction between new items and users, we cannot include new items when building session graphs for GNN session-based recommender systems. Thus, it is challenging to recommend new items for users when using GNN-based methods. We regard this challenge as '\\textbf{G}NN \\textbf{S}ession-based \\textbf{N}ew \\textbf{I}tem \\textbf{R}ecommendation (GSNIR)'. To solve this problem, we propose a dual-intent enhanced graph neural network for it. Due to the fact that new items are not tied to historical sessions, the users' intent is difficult to predict. We design a dual-intent network to learn user intent from an attention mechanism and the distribution of historical data respectively, which can simulate users' decision-making process in interacting with a new item. To solve the challenge that new items cannot be learned by GNNs, inspired by zero-shot learning (ZSL), we infer the new item representation in GNN space by using their attributes. By outputting new item probabilities, which contain recommendation scores of the corresponding items, the new items with higher scores are recommended to users. Experiments on two representative real-world datasets show the superiority of our proposed method. The case study from the real-world verifies interpretability benefits brought by the dual-intent module and the new item reasoning module. The code is available at Github: https://github.com/Ee1s/NirGNN\n        \u25b3 Less\n      ",
    "title": "Dual Intent Enhanced Graph Neural Network for Session-based New Item Recommendation",
    "date": "9 May, 2023",
    "authors": [
      "Di Jin",
      " Luzhi Wang",
      " Yizhen Zheng",
      " Guojie Song",
      " Fei Jiang",
      " Xiang Li",
      " Wei Lin",
      " Shirui Pan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04507",
    "paper_id": "2305.04507",
    "abstract": "\n        Federated learning (FL) allows multiple parties to cooperatively learn a federated model without sharing private data with each other. The need of protecting such federated models from being plagiarized or misused, therefore, motivates us to propose a provable secure model ownership verification scheme using zero-knowledge proof, named FedZKP. It is shown that the FedZKP scheme without disclosing credentials is guaranteed to defeat a variety of existing and potential attacks. Both theoretical analysis and empirical studies demonstrate the security of FedZKP in the sense that the probability for attackers to breach the proposed FedZKP is negligible. Moreover, extensive experimental results confirm the fidelity and robustness of our scheme.\n        \u25b3 Less\n      ",
    "title": "FedZKP: Federated Model Ownership Verification with Zero-knowledge Proof",
    "date": "9 May, 2023",
    "authors": [
      "Wenyuan Yang",
      " Yuguo Yin",
      " Gongxi Zhu",
      " Hanlin Gu",
      " Lixin Fan",
      " Xiaochun Cao",
      " Qiang Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05898",
    "paper_id": "2305.05898",
    "abstract": "\n        Adaptive human-agent and agent-agent cooperation are becoming more and more critical in the research area of multi-agent reinforcement learning (MARL), where remarked progress has been made with the help of deep neural networks. However, many established algorithms can only perform well during the learning paradigm but exhibit poor generalization during cooperation with other unseen partners. The personality theory in cognitive psychology describes that humans can well handle the above cooperation challenge by predicting others' personalities first and then their complex actions. Inspired by this two-step psychology theory, we propose a biologically plausible mixture of personality (MoP) improved spiking actor network (SAN), whereby a determinantal point process is used to simulate the complex formation and integration of different types of personality in MoP, and dynamic and spiking neurons are incorporated into the SAN for the efficient reinforcement learning. The benchmark Overcooked task, containing a strong requirement for cooperative cooking, is selected to test the proposed MoP-SAN. The experimental results show that the MoP-SAN can achieve both high performances during not only the learning paradigm but also the generalization test (i.e., cooperation with other unseen agents) paradigm where most counterpart deep actor networks failed. Necessary ablation experiments and visualization analyses were conducted to explain why MoP and SAN are effective in multi-agent reinforcement learning scenarios while DNN performs poorly in the generalization test.\n        \u25b3 Less\n      ",
    "title": "Mixture of personality improved Spiking actor network for efficient multi-agent cooperation",
    "date": "9 May, 2023",
    "authors": [
      "Xiyun Li",
      " Ziyi Ni",
      " Jingqing Ruan",
      " Linghui Meng",
      " Jing Shi",
      " Tielin Zhang",
      " Bo Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05901",
    "paper_id": "2305.05901",
    "abstract": "\n        With the advent of depth-to-image diffusion models, text-guided generation, editing, and transfer of realistic textures are no longer difficult. However, due to the limitations of pre-trained diffusion models, they can only create low-resolution, inconsistent textures. To address this issue, we present the High-definition Consistency Texture Model (HCTM), a novel method that can generate high-definition and consistent textures for 3D meshes according to the text prompts. We achieve this by leveraging a pre-trained depth-to-image diffusion model to generate single viewpoint results based on the text prompt and a depth map. We fine-tune the diffusion model with Parameter-Efficient Fine-Tuning to quickly learn the style of the generated result, and apply the multi-diffusion strategy to produce high-resolution and consistent results from different viewpoints. Furthermore, we propose a strategy that prevents the appearance of noise on the textures caused by backpropagation. Our proposed approach has demonstrated promising results in generating high-definition and consistent textures for 3D meshes, as demonstrated through a series of experiments.\n        \u25b3 Less\n      ",
    "title": "Text-guided High-definition Consistency Texture Model",
    "date": "9 May, 2023",
    "authors": [
      "Zhibin Tang",
      " Tiantong He"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05909",
    "paper_id": "2305.05909",
    "abstract": "\n        Cooperative multi-agent reinforcement learning (CMARL) has shown to be promising for many real-world applications. Previous works mainly focus on improving coordination ability via solving MARL-specific challenges (e.g., non-stationarity, credit assignment, scalability), but ignore the policy perturbation issue when testing in a different environment. This issue hasn't been considered in problem formulation or efficient algorithm design. To address this issue, we firstly model the problem as a limited policy adversary Dec-POMDP (LPA-Dec-POMDP), where some coordinators from a team might accidentally and unpredictably encounter a limited number of malicious action attacks, but the regular coordinators still strive for the intended goal. Then, we propose Robust Multi-Agent Coordination via Evolutionary Generation of Auxiliary Adversarial Attackers (ROMANCE), which enables the trained policy to encounter diversified and strong auxiliary adversarial attacks during training, thus achieving high robustness under various policy perturbations. Concretely, to avoid the ego-system overfitting to a specific attacker, we maintain a set of attackers, which is optimized to guarantee the attackers high attacking quality and behavior diversity. The goal of quality is to minimize the ego-system coordination effect, and a novel diversity regularizer based on sparse action is applied to diversify the behaviors among attackers. The ego-system is then paired with a population of attackers selected from the maintained attacker set, and alternately trained against the constantly evolving attackers. Extensive experiments on multiple scenarios from SMAC indicate our ROMANCE provides comparable or better robustness and generalization ability than other baselines.\n        \u25b3 Less\n      ",
    "title": "Robust multi-agent coordination via evolutionary generation of auxiliary adversarial attackers",
    "date": "9 May, 2023",
    "authors": [
      "Lei Yuan",
      " Zi-Qian Zhang",
      " Ke Xue",
      " Hao Yin",
      " Feng Chen",
      " Cong Guan",
      " Li-He Li",
      " Chao Qian",
      " Yang Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05911",
    "paper_id": "2305.05911",
    "abstract": "\n        In cooperative multi-agent reinforcement learning (MARL), where an agent coordinates with teammate(s) for a shared goal, it may sustain non-stationary caused by the policy change of teammates. Prior works mainly concentrate on the policy change during the training phase or teammates altering cross episodes, ignoring the fact that teammates may suffer from policy change suddenly within an episode, which might lead to miscoordination and poor performance as a result. We formulate the problem as an open Dec-POMDP, where we control some agents to coordinate with uncontrolled teammates, whose policies could be changed within one episode. Then we develop a new framework, fast teammates adaptation (Fastap), to address the problem. Concretely, we first train versatile teammates' policies and assign them to different clusters via the Chinese Restaurant Process (CRP). Then, we train the controlled agent(s) to coordinate with the sampled uncontrolled teammates by capturing their identifications as context for fast adaptation. Finally, each agent applies its local information to anticipate the teammates' context for decision-making accordingly. This process proceeds alternately, leading to a robust policy that can adapt to any teammates during the decentralized execution phase. We show in multiple multi-agent benchmarks that Fastap can achieve superior performance than multiple baselines in stationary and non-stationary scenarios.\n        \u25b3 Less\n      ",
    "title": "Fast Teammate Adaptation in the Presence of Sudden Policy Change",
    "date": "9 May, 2023",
    "authors": [
      "Ziqian Zhang",
      " Lei Yuan",
      " Lihe Li",
      " Ke Xue",
      " Chengxing Jia",
      " Cong Guan",
      " Chao Qian",
      " Yang Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.01923",
    "paper_id": "2212.01923",
    "abstract": "\n        Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete, for example, over 70% of people in Freebase have no known place of birth. To solve this problem, we propose a query-driven knowledge base completion system with multimodal fusion of unstructured and structured information. To effectively fuse unstructured information from the Web and structured information in knowledge bases to achieve good performance, our system builds multimodal knowledge graphs based on question answering and rule inference. We propose a multimodal path fusion algorithm to rank candidate answers based on different paths in the multimodal knowledge graphs, achieving much better performance than question answering, rule inference and a baseline fusion algorithm. To improve system efficiency, query-driven techniques are utilized to reduce the runtime of our system, providing fast responses to user queries. Extensive experiments have been conducted to demonstrate the effectiveness and efficiency of our system.\n        \u25b3 Less\n      ",
    "title": "Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph",
    "date": "9 May, 2023",
    "authors": [
      "Yang Peng",
      " Daisy Zhe Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.09824",
    "paper_id": "2303.09824",
    "abstract": "\n        Intelligent vehicles (IVs) have gained worldwide attention due to their increased convenience, safety advantages, and potential commercial value. Despite predictions of commercial deployment by 2025, implementation remains limited to small-scale validation, with precise tracking controllers and motion planners being essential prerequisites for IVs. This paper reviews state-of-the-art motion planning methods for IVs, including pipeline planning and end-to-end planning methods. The study examines the selection, expansion, and optimization operations in a pipeline method, while it investigates training approaches and validation scenarios for driving tasks in end-to-end methods. Experimental platforms are reviewed to assist readers in choosing suitable training and validation strategies. A side-by-side comparison of the methods is provided to highlight their strengths and limitations, aiding system-level design choices. Current challenges and future perspectives are also discussed in this survey.\n        \u25b3 Less\n      ",
    "title": "Motion Planning for Autonomous Driving: The State of the Art and Future Perspectives",
    "date": "9 May, 2023",
    "authors": [
      "Siyu Teng",
      " Xuemin Hu",
      " Peng Deng",
      " Bai Li",
      " Yuchen Li",
      " Dongsheng Yang",
      " Yunfeng Ai",
      " Lingxi Li",
      " Zhe Xuanyuan",
      " Fenghua Zhu",
      " Long Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05933",
    "paper_id": "2305.05933",
    "abstract": "\n        Federated Learning (FL) is a widely embraced paradigm for distilling artificial intelligence from distributed mobile data. However, the deployment of FL in mobile networks can be compromised by exposure to interference from neighboring cells or jammers. Existing interference mitigation techniques require multi-cell cooperation or at least interference channel state information, which is expensive in practice. On the other hand, power control that treats interference as noise may not be effective due to limited power budgets, and also that this mechanism can trigger countermeasures by interference sources. As a practical approach for protecting FL against interference, we propose Spectrum Breathing, which cascades stochastic-gradient pruning and spread spectrum to suppress interference without bandwidth expansion. The cost is higher learning latency by exploiting the graceful degradation of learning speed due to pruning. We synchronize the two operations such that their levels are controlled by the same parameter, Breathing Depth. To optimally control the parameter, we develop a martingale-based approach to convergence analysis of Over-the-Air FL with spectrum breathing, termed AirBreathing FL. We show a performance tradeoff between gradient-pruning and interference-induced error as regulated by the breathing depth. Given receive SIR and model size, the optimization of the tradeoff yields two schemes for controlling the breathing depth that can be either fixed or adaptive to channels and the learning process. As shown by experiments, in scenarios where traditional Over-the-Air FL fails to converge in the presence of strong interference, AirBreahing FL with either fixed or adaptive breathing depth can ensure convergence where the adaptive scheme achieves close-to-ideal performance.\n        \u25b3 Less\n      ",
    "title": "Spectrum Breathing: Protecting Over-the-Air Federated Learning Against Interference",
    "date": "9 May, 2023",
    "authors": [
      "Zhanwei Wang",
      " Kaibin Huang",
      " Yonina C. Eldar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.02842",
    "paper_id": "2212.02842",
    "abstract": "\n        A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as self- or unsupervised learning. As part of this paper, we present baseline sperm detection performances using the YOLOv5 deep learning (DL) model trained on the VISEM-Tracking dataset. As a result, we show that the dataset can be used to train complex DL models to analyze spermatozoa.\n        \u25b3 Less\n      ",
    "title": "VISEM-Tracking, a human spermatozoa tracking dataset",
    "date": "9 May, 2023",
    "authors": [
      "Vajira Thambawita",
      " Steven A. Hicks",
      " Andrea M. Stor\u00e5s",
      " Thu Nguyen",
      " Jorunn M. Andersen",
      " Oliwia Witczak",
      " Trine B. Haugen",
      " Hugo L. Hammer",
      " P\u00e5l Halvorsen",
      " Michael A. Riegler"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05938",
    "paper_id": "2305.05938",
    "abstract": "\n        Utilizing infrastructure and vehicle-side information to track and forecast the behaviors of surrounding traffic participants can significantly improve decision-making and safety in autonomous driving. However, the lack of real-world sequential datasets limits research in this area. To address this issue, we introduce V2X-Seq, the first large-scale sequential V2X dataset, which includes data frames, trajectories, vector maps, and traffic lights captured from natural scenery. V2X-Seq comprises two parts: the sequential perception dataset, which includes more than 15,000 frames captured from 95 scenarios, and the trajectory forecasting dataset, which contains about 80,000 infrastructure-view scenarios, 80,000 vehicle-view scenarios, and 50,000 cooperative-view scenarios captured from 28 intersections' areas, covering 672 hours of data. Based on V2X-Seq, we introduce three new tasks for vehicle-infrastructure cooperative (VIC) autonomous driving: VIC3D Tracking, Online-VIC Forecasting, and Offline-VIC Forecasting. We also provide benchmarks for the introduced tasks. Find data, code, and more up-to-date information at \\href{https://github.com/AIR-THU/DAIR-V2X-Seq}{https://github.com/AIR-THU/DAIR-V2X-Seq}.\n        \u25b3 Less\n      ",
    "title": "V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting",
    "date": "9 May, 2023",
    "authors": [
      "Haibao Yu",
      " Wenxian Yang",
      " Hongzhi Ruan",
      " Zhenwei Yang",
      " Yingjuan Tang",
      " Xu Gao",
      " Xin Hao",
      " Yifeng Shi",
      " Yifeng Pan",
      " Ning Sun",
      " Juan Song",
      " Jirui Yuan",
      " Ping Luo",
      " Zaiqing Nie"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05948",
    "paper_id": "2305.05948",
    "abstract": "\n        For years the model performance in machine learning obeyed a power-law relationship with the model size. For the consideration of parameter efficiency, recent studies focus on increasing model depth rather than width to achieve better performance. In this paper, we study how model width affects the Transformer model through a parameter-efficient multi-path structure. To better fuse features extracted from different paths, we add three additional operations to each sublayer: a normalization at the end of each path, a cheap operation to produce more features, and a learnable weighted mechanism to fuse all features flexibly. Extensive experiments on 12 WMT machine translation tasks show that, with the same number of parameters, the shallower multi-path model can achieve similar or even better performance than the deeper model. It reveals that we should pay more attention to the multi-path structure, and there should be a balance between the model depth and width to train a better large-scale Transformer.\n        \u25b3 Less\n      ",
    "title": "Multi-Path Transformer is Better: A Case Study on Neural Machine Translation",
    "date": "9 May, 2023",
    "authors": [
      "Ye Lin",
      " Shuhan Zhou",
      " Yanyang Li",
      " Anxiang Ma",
      " Tong Xiao",
      " Jingbo Zhu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.05079",
    "paper_id": "2301.05079",
    "abstract": "\n        The undesired interaction of a quantum system with its environment generally leads to a coherence decay of superposition states in time. A precise knowledge of the spectral content of the noise induced by the environment is crucial to protect qubit coherence and optimize its employment in quantum device applications. We experimentally show that the use of neural networks can highly increase the accuracy of noise spectroscopy, by reconstructing the power spectral density that characterizes an ensemble of carbon impurities around a nitrogen-vacancy (NV) center in diamond. Neural networks are trained over spin coherence functions of the NV center subjected to different Carr-Purcell sequences, typically used for dynamical decoupling (DD). As a result, we determine that deep learning models can be more accurate than standard DD noise-spectroscopy techniques, by requiring at the same time a much smaller number of DD sequences.\n        \u25b3 Less\n      ",
    "title": "Deep learning enhanced noise spectroscopy of a spin qubit environment",
    "date": "9 May, 2023",
    "authors": [
      "Stefano Martina",
      " Santiago Hern\u00e1ndez-G\u00f3mez",
      " Stefano Gherardini",
      " Filippo Caruso",
      " Nicole Fabbri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05964",
    "paper_id": "2305.05964",
    "abstract": "\n        Multimodal misinformation on online social platforms is becoming a critical concern due to increasing credibility and easier dissemination brought by multimedia content, compared to traditional text-only information. While existing multimodal detection approaches have achieved high performance, the lack of interpretability hinders these systems' reliability and practical deployment. Inspired by NeuralSymbolic AI which combines the learning ability of neural networks with the explainability of symbolic learning, we propose a novel logic-based neural model for multimodal misinformation detection which integrates interpretable logic clauses to express the reasoning process of the target task. To make learning effective, we parameterize symbolic logical elements using neural representations, which facilitate the automatic generation and evaluation of meaningful logic clauses. Additionally, to make our framework generalizable across diverse misinformation sources, we introduce five meta-predicates that can be instantiated with different correlations. Results on three public datasets (Twitter, Weibo, and Sarcasm) demonstrate the feasibility and versatility of our model.\n        \u25b3 Less\n      ",
    "title": "Interpretable Multimodal Misinformation Detection with Logic Reasoning",
    "date": "10 May, 2023",
    "authors": [
      "Hui Liu",
      " Wenya Wang",
      " Haoliang Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06934",
    "paper_id": "2305.06934",
    "abstract": "\n        Since the release of ChatGPT, numerous studies have highlighted the remarkable performance of ChatGPT, which often rivals or even surpasses human capabilities in various tasks and domains. However, this paper presents a contrasting perspective by demonstrating an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming. We utilize the IEEExtreme Challenge competition as a benchmark, a prestigious, annual international programming contest encompassing a wide range of problems with different complexities. To conduct a thorough evaluation, we selected and executed a diverse set of 102 challenges, drawn from five distinct IEEExtreme editions, using three major programming languages: Python, Java, and C++. Our empirical analysis provides evidence that contrary to popular belief, human programmers maintain a competitive edge over ChatGPT in certain aspects of problem-solving within the programming context. In fact, we found that the average score obtained by ChatGPT on the set of IEEExtreme programming problems is 3.9 to 5.8 times lower than the average human score, depending on the programming language. This paper elaborates on these findings, offering critical insights into the limitations and potential areas of improvement for AI-based language models like ChatGPT.\n        \u25b3 Less\n      ",
    "title": "Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition",
    "date": "10 May, 2023",
    "authors": [
      "Anis Koubaa",
      " Basit Qureshi",
      " Adel Ammar",
      " Zahid Khan",
      " Wadii Boulila",
      " Lahouari Ghouti"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05982",
    "paper_id": "2305.05982",
    "abstract": "\n        A medical provider's summary of a patient visit serves several critical purposes, including clinical decision-making, facilitating hand-offs between providers, and as a reference for the patient. An effective summary is required to be coherent and accurately capture all the medically relevant information in the dialogue, despite the complexity of patient-generated language. Even minor inaccuracies in visit summaries (for example, summarizing \"patient does not have a fever\" when a fever is present) can be detrimental to the outcome of care for the patient.\n  This paper tackles the problem of medical conversation summarization by discretizing the task into several smaller dialogue-understanding tasks that are sequentially built upon. First, we identify medical entities and their affirmations within the conversation to serve as building blocks. We study dynamically constructing few-shot prompts for tasks by conditioning on relevant patient information and use GPT-3 as the backbone for our experiments. We also develop GPT-derived summarization metrics to measure performance against reference summaries quantitatively. Both our human evaluation study and metrics for medical correctness show that summaries generated using this approach are clinically accurate and outperform the baseline approach of summarizing the dialog in a zero-shot, single-prompt setting.\n        \u25b3 Less\n      ",
    "title": "Generating medically-accurate summaries of patient-provider dialogue: A multi-stage approach using large language models",
    "date": "10 May, 2023",
    "authors": [
      "Varun Nair",
      " Elliot Schumacher",
      " Anitha Kannan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05986",
    "paper_id": "2305.05986",
    "abstract": "\n        Learning causal structure among event types from discrete-time event sequences is a particularly important but challenging task. Existing methods, such as the multivariate Hawkes processes based methods, mostly boil down to learning the so-called Granger causality which assumes that the cause event happens strictly prior to its effect event. Such an assumption is often untenable beyond applications, especially when dealing with discrete-time event sequences in low-resolution; and typical discrete Hawkes processes mainly suffer from identifiability issues raised by the instantaneous effect, i.e., the causal relationship that occurred simultaneously due to the low-resolution data will not be captured by Granger causality. In this work, we propose Structure Hawkes Processes (SHPs) that leverage the instantaneous effect for learning the causal structure among events type in discrete-time event sequence. The proposed method is featured with the minorization-maximization of the likelihood function and a sparse optimization scheme. Theoretical results show that the instantaneous effect is a blessing rather than a curse, and the causal structure is identifiable under the existence of the instantaneous effect. Experiments on synthetic and real-world data verify the effectiveness of the proposed method.\n        \u25b3 Less\n      ",
    "title": "Structural Hawkes Processes for Learning Causal Structure from Discrete-Time Event Sequences",
    "date": "10 May, 2023",
    "authors": [
      "Jie Qiao",
      " Ruichu Cai",
      " Siyu Wu",
      " Yu Xiang",
      " Keli Zhang",
      " Zhifeng Hao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05994",
    "paper_id": "2305.05994",
    "abstract": "\n        Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.\n        \u25b3 Less\n      ",
    "title": "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base",
    "date": "10 May, 2023",
    "authors": [
      "Siyu Yuan",
      " Jiangjie Chen",
      " Changzhi Sun",
      " Jiaqing Liang",
      " Yanghua Xiao",
      " Deqing Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06001",
    "paper_id": "2305.06001",
    "abstract": "\n        The vision of Industry 4.0 introduces new requirements to Operational Technology (OT) systems. Solutions for these requirements already exist in the Information Technology (IT) world, however, due to the different characteristics of both worlds, these solutions often cannot be directly used in the world of OT. We therefore propose an Industrial Business Process Twin (IBPT), allowing to apply methods of one world to another not directly but, instead, to a representation, that is in bidirectional exchange with the other world. The proposed IBPT entity acts as an intermediary, decoupling the worlds of IT and OT, thus allowing for an integration of IT and OT components of different manufacturers and platforms. Using this approach, we demonstrate the four essential Industry 4.0 design principles information transparency, technical assistance, interconnection and decentralized decisions based on the gamified Industry 4.0 scenario of playing the game of Nine Men's Morris. This scenario serves well for agent based Artificial Intelligence (AI)-research and education. We develop an Open Platform Communications Unified Architecture (OPC UA) information and communication model and then evaluate the IBPT component with respect to the different views of the Reference Architecture Model Industry 4.0 (RAMI4.0).\n        \u25b3 Less\n      ",
    "title": "Digital Twins of Business Processes as Enablers for IT / OT Integration",
    "date": "10 May, 2023",
    "authors": [
      "Hannes Waclawek",
      " Georg Sch\u00e4fer",
      " Christoph Binder",
      " Eduard Hirsch",
      " Stefan Huber"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06004",
    "paper_id": "2305.06004",
    "abstract": "\n        We present an approach for safe motion planning under robot state and environment (obstacle and landmark location) uncertainties. To this end, we first develop an approach that accounts for the landmark uncertainties during robot localization. Existing planning approaches assume that the landmark locations are well known or are known with little uncertainty. However, this might not be true in practice. Noisy sensors and imperfect motions compound to the errors originating from the estimate of environment features. Moreover, possible occlusions and dynamic objects in the environment render imperfect landmark estimation. Consequently, not considering this uncertainty can wrongly localize the robot, leading to inefficient plans. Our approach thus incorporates the landmark uncertainty within the Bayes filter estimation framework. We also analyze the effect of considering this uncertainty and delineate the conditions under which it can be ignored. Second, we extend the state-of-the-art by computing an exact expression for the collision probability under Gaussian distributed robot motion, perception and obstacle location uncertainties. We formulate the collision probability process as a quadratic form in random variables. Under Gaussian distribution assumptions, an exact expression for collision probability is thus obtained which is computable in real-time. In contrast, existing approaches approximate the collision probability using upper-bounds that can lead to overly conservative estimate and thereby suboptimal plans. We demonstrate and evaluate our approach using a theoretical example and simulations. We also present a comparison of our approach to different state-of-the-art methods.\n        \u25b3 Less\n      ",
    "title": "Safe motion planning with environment uncertainty",
    "date": "10 May, 2023",
    "authors": [
      "Antony Thomas",
      " Fulvio Mastrogiovanni",
      " Marco Baglietto"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07571",
    "paper_id": "2305.07571",
    "abstract": "\n        We present a simple, sample-efficient algorithm for introducing large but directed learning steps in reinforcement learning (RL), through the use of evolutionary operators. The methodology uses a population of RL agents training with a common experience buffer, with occasional crossovers and mutations of the agents in order to search efficiently through the policy space. Unlike prior literature on combining evolutionary search (ES) with RL, this work does not generate a distribution of agents from a common mean and covariance matrix. Neither does it require the evaluation of the entire population of policies at every time step. Instead, we focus on gradient-based training throughout the life of every policy (individual), with a sparse amount of evolutionary exploration. The resulting algorithm is shown to be robust to hyperparameter variations. As a surprising corollary, we show that simply initialising and training multiple RL agents with a common memory (with no further evolutionary updates) outperforms several standard RL baselines.\n        \u25b3 Less\n      ",
    "title": "Supplementing Gradient-Based Reinforcement Learning with Simple Evolutionary Ideas",
    "date": "10 May, 2023",
    "authors": [
      "Harshad Khadilkar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11061",
    "paper_id": "2305.11061",
    "abstract": "\n        Converting text into the structured query language (Text2SQL) is a research hotspot in the field of natural language processing (NLP), which has broad application prospects. In the era of big data, the use of databases has penetrated all walks of life, in which the collected data is large in scale, diverse in variety, and wide in scope, making the data query cumbersome and inefficient, and putting forward higher requirements for the Text2SQL model. In practical applications, the current mainstream end-to-end Text2SQL model is not only difficult to build due to its complex structure and high requirements for training data, but also difficult to adjust due to massive parameters. In addition, the accuracy of the model is hard to achieve the desired result. Based on this, this paper proposes a pipelined Text2SQL method: SPSQL. This method disassembles the Text2SQL task into four subtasks--table selection, column selection, SQL generation, and value filling, which can be converted into a text classification problem, a sequence labeling problem, and two text generation problems, respectively. Then, we construct data formats of different subtasks based on existing data and improve the accuracy of the overall model by improving the accuracy of each submodel. We also use the named entity recognition module and data augmentation to optimize the overall model. We construct the dataset based on the marketing business data of the State Grid Corporation of China. Experiments demonstrate our proposed method achieves the best performance compared with the end-to-end method and other pipeline methods.\n        \u25b3 Less\n      ",
    "title": "SPSQL: Step-by-step Parsing Based Framework for Text-to-SQL Generation",
    "date": "10 May, 2023",
    "authors": [
      "Ran Shen",
      " Gang Sun",
      " Hao Shen",
      " Yiling Li",
      " Liangfeng Jin",
      " Han Jiang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.02899",
    "paper_id": "2210.02899",
    "abstract": "\n        In recent years, much work has been done on processing of wireless spectrum data involving machine learning techniques in domain-related problems for cognitive radio networks, such as anomaly detection, modulation classification, technology classification and device fingerprinting. Most of the solutions are based on labeled data, created in a controlled manner and processed with supervised learning approaches. However, spectrum data measured in real-world environment is highly nondeterministic, making its labeling a laborious and expensive process, requiring domain expertise, thus being one of the main drawbacks of using supervised learning approaches in this domain. In this paper, we investigate the use of self-supervised learning (SSL) for exploring spectrum activities in a real-world unlabeled data. In particular, we compare the performance of two SSL models, one based on a reference DeepCluster architecture and one adapted for spectrum activity identification and clustering, and a baseline model based on K-means clustering algorithm. We show that SSL models achieve superior performance regarding the quality of extracted features and clustering performance. With SSL models we achieve reduction of the feature vectors size by two orders of magnitude, while improving the performance by a factor of 2 to 2.5 across the evaluation metrics, supported by visual assessment. Additionally we show that adaptation of the reference SSL architecture to the domain data provides reduction of model complexity by one order of magnitude, while preserving or even improving the clustering performance.\n        \u25b3 Less\n      ",
    "title": "Self-supervised Learning for Clustering of Wireless Spectrum Activity",
    "date": "10 May, 2023",
    "authors": [
      "Ljupcho Milosheski",
      " Gregor Cerar",
      " Bla\u017e Bertalani\u010d",
      " Carolina Fortuna",
      " Mihael Mohor\u010di\u010d"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.05135",
    "paper_id": "2207.05135",
    "abstract": "\n        In the last decade, most research in Machine Learning contributed to the improvement of existing models, with the aim of increasing the performance of neural networks for the solution of a variety of different tasks. However, such advancements often come at the cost of an increase of model memory and computational requirements. This represents a significant limitation for the deployability of research output in realistic settings, where the cost, the energy consumption, and the complexity of the framework play a crucial role. To solve this issue, the designer should search for models that maximise the performance while limiting its footprint. Typical approaches to reach this goal rely either on manual procedures, which cannot guarantee the optimality of the final design, or upon Neural Architecture Search algorithms to automatise the process, at the expenses of extremely high computational time. This paper provides a solution for the fast identification of a neural network that maximises the model accuracy while preserving size and computational constraints typical of tiny devices. Our approach, named FreeREA, is a custom cell-based evolution NAS algorithm that exploits an optimised combination of training-free metrics to rank architectures during the search, thus without need of model training. Our experiments, carried out on the common benchmarks NAS-Bench-101 and NATS-Bench, demonstrate that i) FreeREA is a fast, efficient, and effective search method for models automatic design; ii) it outperforms State of the Art training-based and training-free techniques in all the datasets and benchmarks considered, and iii) it can easily generalise to constrained scenarios, representing a competitive solution for fast Neural Architecture Search in generic constrained applications. The code is available at \\url{https://github.com/NiccoloCavagnero/FreeREA}.\n        \u25b3 Less\n      ",
    "title": "FreeREA: Training-Free Evolution-based Architecture Search",
    "date": "10 May, 2023",
    "authors": [
      "Niccol\u00f2 Cavagnero",
      " Luca Robbiano",
      " Barbara Caputo",
      " Giuseppe Averta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.08407",
    "paper_id": "2211.08407",
    "abstract": "\n        Enabled by the emerging industrial agent (IA) technology, swarm intelligence (SI) is envisaged to play an important role in future industrial Internet of Things (IIoT) that is shaped by Sixth Generation (6G) mobile communications and digital twin (DT). However, its fragility against data injection attack may halt it from practical deployment. In this paper we propose an efficient trust approach to address this security concern for SI.\n        \u25b3 Less\n      ",
    "title": "Trust-Awareness to Secure Swarm Intelligence from Data Injection Attack",
    "date": "10 May, 2023",
    "authors": [
      "Bin Han",
      " Dennis Krummacker",
      " Qiuheng Zhou",
      " Hans D. Schotten"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11070",
    "paper_id": "2305.11070",
    "abstract": "\n        A considerable number of texts encountered daily are somehow connected with each other. For example, Wikipedia articles refer to other articles via hyperlinks, scientific papers relate to others via citations or (co)authors, while tweets relate via users that follow each other or reshare content. Hence, a graph-like structure can represent existing connections and be seen as capturing the \"context\" of the texts. The question thus arises if extracting and integrating such context information into a language model might help facilitate a better automated understanding of the text. In this study, we experimentally demonstrate that incorporating graph-based contextualization into BERT model enhances its performance on an example of a classification task. Specifically, on Pubmed dataset, we observed a reduction in error from 8.51% to 7.96%, while increasing the number of parameters just by 1.6%.\n  Our source code: https://github.com/tryptofanik/gc-bert\n        \u25b3 Less\n      ",
    "title": "Enriching language models with graph-based context information to better understand textual data",
    "date": "10 May, 2023",
    "authors": [
      "Albert Roethel",
      " Maria Ganzha",
      " Anna Wr\u00f3blewska"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06058",
    "paper_id": "2305.06058",
    "abstract": "\n        Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including over-fitting, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN by encoding them to multi-layer tensor networks (TN's) that contain exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, and VGG-16) and datasets (MNIST and CIFAR-10), surpassing the state-of-the-art method based on shallow tensor networks. For instance, about 10 million parameters in the three convolutional layers of VGG-16 are compressed in TN's with just 632632 parameters, while the testing accuracy on CIFAR-10 is surprisingly improved from 81.14%81.14\\% by the original NN to 84.36%84.36\\% after compression. Our work suggests TN as an exceptionally efficient mathematical structure for representing the variational parameters of NN's, which superiorly exploits the compressibility than the simple multi-way arrays.\n        \u25b3 Less\n      ",
    "title": "Compressing neural network by tensor network with exponentially fewer variational parameters",
    "date": "10 May, 2023",
    "authors": [
      "Yong Qing",
      " Peng-Fei Zhou",
      " Ke Li",
      " Shi-Ju Ran"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06061",
    "paper_id": "2305.06061",
    "abstract": "\n        Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter tuning, and remapping tuning. Meanwhile, it offers some exciting research directions for prospective pre-training and various interactions in visual tuning.\n        \u25b3 Less\n      ",
    "title": "Visual Tuning",
    "date": "10 May, 2023",
    "authors": [
      "Bruce X. B. Yu",
      " Jianlong Chang",
      " Haixin Wang",
      " Lingbo Liu",
      " Shijie Wang",
      " Zhiyu Wang",
      " Junfan Lin",
      " Lingxi Xie",
      " Haojie Li",
      " Zhouchen Lin",
      " Qi Tian",
      " Chang Wen Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06082",
    "paper_id": "2305.06082",
    "abstract": "\n        We study best arm identification in a variant of the multi-armed bandit problem where the learner has limited precision in arm selection. The learner can only sample arms via certain exploration bundles, which we refer to as boxes. In particular, at each sampling epoch, the learner selects a box, which in turn causes an arm to get pulled as per a box-specific probability distribution. The pulled arm and its instantaneous reward are revealed to the learner, whose goal is to find the best arm by minimising the expected stopping time, subject to an upper bound on the error probability. We present an asymptotic lower bound on the expected stopping time, which holds as the error probability vanishes. We show that the optimal allocation suggested by the lower bound is, in general, non-unique and therefore challenging to track. We propose a modified tracking-based algorithm to handle non-unique optimal allocations, and demonstrate that it is asymptotically optimal. We also present non-asymptotic lower and upper bounds on the stopping time in the simpler setting when the arms accessible from one box do not overlap with those of others.\n        \u25b3 Less\n      ",
    "title": "Best Arm Identification in Bandits with Limited Precision Sampling",
    "date": "10 May, 2023",
    "authors": [
      "Kota Srinivas Reddy",
      " P. N. Karthik",
      " Nikhil Karamchandani",
      " Jayakrishnan Nair"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06085",
    "paper_id": "2305.06085",
    "abstract": "\n        Federated learning allows multiple parties to collaborate in learning a global model without revealing private data. The high cost of training and the significant value of the global model necessitates the need for ownership verification of federated learning. However, the existing ownership verification schemes in federated learning suffer from several limitations, such as inadequate support for a large number of clients and vulnerability to ambiguity attacks. To address these limitations, we propose a cryptographic signature-based federated learning model ownership verification scheme named FedSOV. FedSOV allows numerous clients to embed their ownership credentials and verify ownership using unforgeable digital signatures. The scheme provides theoretical resistance to ambiguity attacks with the unforgeability of the signature. Experimental results on computer vision and natural language processing tasks demonstrate that FedSOV is an effective federated model ownership verification scheme enhanced with provable cryptographic security.\n        \u25b3 Less\n      ",
    "title": "FedSOV: Federated Model Secure Ownership Verification with Unforgeable Signature",
    "date": "10 May, 2023",
    "authors": [
      "Wenyuan Yang",
      " Gongxi Zhu",
      " Yuguo Yin",
      " Hanlin Gu",
      " Lixin Fan",
      " Qiang Yang",
      " Xiaochun Cao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06087",
    "paper_id": "2305.06087",
    "abstract": "\n        Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their development. These models are trained on massive amounts of data and can be used for a wide range of tasks, including language translation, text generation, and question answering. However, the computational resources required to train and run these models are substantial, and the cost of hardware and electricity can be prohibitive for research labs that do not have the funding and resources of the GAFA. In this paper, we will examine the impact of LLMs on AI research. The pace at which such models are generated as well as the range of domains covered is an indication of the trend which not only the public but also the scientific community is currently experiencing. We give some examples on how to use such models in research by focusing on GPT3.5/ChatGPT3.4 and ChatGPT4 at the current state and show that such a range of capabilities in a single system is a strong sign of approaching general intelligence. Innovations integrating such models will also expand along the maturation of such AI systems and exhibit unforeseeable applications that will have important impacts on several aspects of our societies.\n        \u25b3 Less\n      ",
    "title": "A Glimpse in ChatGPT Capabilities and its impact for AI research",
    "date": "10 May, 2023",
    "authors": [
      "Frank Joublin",
      " Antonello Ceravola",
      " Joerg Deigmoeller",
      " Michael Gienger",
      " Mathias Franzius",
      " Julian Eggert"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06088",
    "paper_id": "2305.06088",
    "abstract": "\n        When building a new application we are increasingly confronted with the need of reusing and integrating pre-existing knowledge. Nevertheless, it is a fact that this prior knowledge is virtually impossible to reuse as-is. This is true also in domains, e.g., eHealth, where a lot of effort has been put into developing high-quality standards and reference ontologies, e.g. FHIR1. In this paper, we propose an integrated methodology, called iTelos, which enables data and knowledge reuse towards the construction of Interoperable Electronic Health Records (iEHR). The key intuition is that the data level and the schema level of an application should be developed independently, thus allowing for maximum flexibility in the reuse of the prior knowledge, but under the overall guidance of the needs to be satisfied, formalized as competence queries. This intuition is implemented by codifying all the requirements, including those concerning reuse, as part of a purpose defined a priori, which is then used to drive a middle-out development process where the application schema and data are continuously aligned. The proposed methodology is validated through its application to a large-scale case study.\n        \u25b3 Less\n      ",
    "title": "Building Interoperable Electronic Health Records as Purpose-Driven Knowledge Graphs",
    "date": "10 May, 2023",
    "authors": [
      "Simone Bocca",
      " Alessio Zamboni",
      " Gabor Bella",
      " Yamini Chandrashekar",
      " Mayukh Bagchi",
      " Gabriel Kuper",
      " Paolo Bouquet",
      " Fausto Giunchiglia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.07763",
    "paper_id": "2210.07763",
    "abstract": "\n        Structured knowledge is important for many AI applications. Commonsense knowledge, which is crucial for robust human-centric AI, is covered by a small number of structured knowledge projects. However, they lack knowledge about human traits and behaviors conditioned on socio-cultural contexts, which is crucial for situative AI. This paper presents CANDLE, an end-to-end methodology for extracting high-quality cultural commonsense knowledge (CCSK) at scale. CANDLE extracts CCSK assertions from a huge web corpus and organizes them into coherent clusters, for 3 domains of subjects (geography, religion, occupation) and several cultural facets (food, drinks, clothing, traditions, rituals, behaviors). CANDLE includes judicious techniques for classification-based filtering and scoring of interestingness. Experimental evaluations show the superiority of the CANDLE CCSK collection over prior works, and an extrinsic use case demonstrates the benefits of CCSK for the GPT-3 language model. Code and data can be accessed at https://candle.mpi-inf.mpg.de/.\n        \u25b3 Less\n      ",
    "title": "Extracting Cultural Commonsense Knowledge at Scale",
    "date": "10 May, 2023",
    "authors": [
      "Tuan-Phong Nguyen",
      " Simon Razniewski",
      " Aparna Varde",
      " Gerhard Weikum"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06099",
    "paper_id": "2305.06099",
    "abstract": "\n        The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained named entities in low-context situations and noisy scenarios like the presence of spelling mistakes and typos for multiple languages. The task poses significant challenges due to the scarcity of contextual information, the high granularity of the entities(up to 33 classes), and the interference of noisy data. To address these issues, our team {\\bf PAI} proposes a universal Named Entity Recognition (NER) system that integrates external entity information to improve performance. Specifically, our system retrieves entities with properties from the knowledge base (i.e. Wikipedia) for a given text, then concatenates entity information with the input sentence and feeds it into Transformer-based models. Finally, our system wins 2 first places, 4 second places, and 1 third place out of 13 tracks. The code is publicly available at \\url{https://github.com/diqiuzhuanzhuan/semeval-2023}.\n        \u25b3 Less\n      ",
    "title": "PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information",
    "date": "10 May, 2023",
    "authors": [
      "Long Ma",
      " Kai Lu",
      " Tianbo Che",
      " Hailong Huang",
      " Weiguo Gao",
      " Xuan Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06102",
    "paper_id": "2305.06102",
    "abstract": "\n        Proposing an effective and flexible matrix to represent a graph is a fundamental challenge that has been explored from multiple perspectives, e.g., filtering in Graph Fourier Transforms. In this work, we develop a novel and general framework which unifies many existing GNN models from the view of parameterized decomposition and filtering, and show how it helps to enhance the flexibility of GNNs while alleviating the smoothness and amplification issues of existing models. Essentially, we show that the extensively studied spectral graph convolutions with learnable polynomial filters are constrained variants of this formulation, and releasing these constraints enables our model to express the desired decomposition and filtering simultaneously. Based on this generalized framework, we develop models that are simple in implementation but achieve significant improvements and computational efficiency on a variety of graph learning tasks. Code is available at https://github.com/qslim/PDF.\n        \u25b3 Less\n      ",
    "title": "Towards Better Graph Representation Learning with Parameterized Decomposition & Filtering",
    "date": "10 May, 2023",
    "authors": [
      "Mingqi Yang",
      " Wenjie Feng",
      " Yanming Shen",
      " Bryan Hooi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06114",
    "paper_id": "2305.06114",
    "abstract": "\n        Current few-shot action recognition involves two primary sources of information for classification:(1) intra-video information, determined by frame content within a single video clip, and (2) inter-video information, measured by relationships (e.g., feature similarity) among videos. However, existing methods inadequately exploit these two information sources. In terms of intra-video information, current sampling operations for input videos may omit critical action information, reducing the utilization efficiency of video data. For the inter-video information, the action misalignment among videos makes it challenging to calculate precise relationships. Moreover, how to jointly consider both inter- and intra-video information remains under-explored for few-shot action recognition. To this end, we propose a novel framework, Video Information Maximization (VIM), for few-shot video action recognition. VIM is equipped with an adaptive spatial-temporal video sampler and a spatiotemporal action alignment model to maximize intra- and inter-video information, respectively. The video sampler adaptively selects important frames and amplifies critical spatial regions for each input video based on the task at hand. This preserves and emphasizes informative parts of video clips while eliminating interference at the data level. The alignment model performs temporal and spatial action alignment sequentially at the feature level, leading to more precise measurements of inter-video similarity. Finally, These goals are facilitated by incorporating additional loss terms based on mutual information measurement. Consequently, VIM acts to maximize the distinctiveness of video information from limited video data. Extensive experimental results on public datasets for few-shot action recognition demonstrate the effectiveness and benefits of our framework.\n        \u25b3 Less\n      ",
    "title": "Few-shot Action Recognition via Intra- and Inter-Video Information Maximization",
    "date": "10 May, 2023",
    "authors": [
      "Huabin Liu",
      " Weiyao Lin",
      " Tieyuan Chen",
      " Yuxi Li",
      " Shuyuan Li",
      " John See"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05354",
    "paper_id": "2305.05354",
    "abstract": "\n        Spinal fusion surgery requires highly accurate implantation of pedicle screw implants, which must be conducted in critical proximity to vital structures with a limited view of anatomy. Robotic surgery systems have been proposed to improve placement accuracy, however, state-of-the-art systems suffer from the limitations of open-loop approaches, as they follow traditional concepts of preoperative planning and intraoperative registration, without real-time recalculation of the surgical plan. In this paper, we propose an intraoperative planning approach for robotic spine surgery that leverages real-time observation for drill path planning based on Safe Deep Reinforcement Learning (DRL). The main contributions of our method are (1) the capability to guarantee safe actions by introducing an uncertainty-aware distance-based safety filter; and (2) the ability to compensate for incomplete intraoperative anatomical information, by encoding a-priori knowledge about anatomical structures with a network pre-trained on high-fidelity anatomical models. Planning quality was assessed by quantitative comparison with the gold standard (GS) drill planning. In experiments with 5 models derived from real magnetic resonance imaging (MRI) data, our approach was capable of achieving 90% bone penetration with respect to the GS while satisfying safety requirements, even under observation and motion uncertainty. To the best of our knowledge, our approach is the first safe DRL approach focusing on orthopedic surgeries.\n        \u25b3 Less\n      ",
    "title": "Safe Deep RL for Intraoperative Planning of Pedicle Screw Placement",
    "date": "10 May, 2023",
    "authors": [
      "Yunke Ao",
      " Hooman Esfandiari",
      " Fabio Carrillo",
      " Yarden As",
      " Mazda Farshad",
      " Benjamin F. Grewe",
      " Andreas Krause",
      " Philipp Fuernstahl"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11068",
    "paper_id": "2305.11068",
    "abstract": "\n        The purpose of this work is to describe the Orkg-Leaderboard software designed to extract leaderboards defined as Task-Dataset-Metric tuples automatically from large collections of empirical research papers in Artificial Intelligence (AI). The software can support both the main workflows of scholarly publishing, viz. as LaTeX files or as PDF files. Furthermore, the system is integrated with the Open Research Knowledge Graph (ORKG) platform, which fosters the machine-actionable publishing of scholarly findings. Thus the system output, when integrated within the ORKG's supported Semantic Web infrastructure of representing machine-actionable 'resources' on the Web, enables: 1) broadly, the integration of empirical results of researchers across the world, thus enabling transparency in empirical research with the potential to also being complete contingent on the underlying data source(s) of publications; and 2) specifically, enables researchers to track the progress in AI with an overview of the state-of-the-art (SOTA) across the most common AI tasks and their corresponding datasets via dynamic ORKG frontend views leveraging tables and visualization charts over the machine-actionable data. Our best model achieves performances above 90% F1 on the \\textit{leaderboard} extraction task, thus proving Orkg-Leaderboards a practically viable tool for real-world usage. Going forward, in a sense, Orkg-Leaderboards transforms the leaderboard extraction task to an automated digitalization task, which has been, for a long time in the community, a crowdsourced endeavor.\n        \u25b3 Less\n      ",
    "title": "ORKG-Leaderboards: A Systematic Workflow for Mining Leaderboards as a Knowledge Graph",
    "date": "10 May, 2023",
    "authors": [
      "Salomon Kabongo",
      " Jennifer D'Souza",
      " S\u00f6ren Auer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.10426",
    "paper_id": "2302.10426",
    "abstract": "\n        An accurate and explainable automatic monitoring system is critical for the safety of high efficiency energy conversion plants that operate under extreme working condition. Nonetheless, currently available data-driven monitoring systems often fall short in meeting the requirements for either high-accuracy or interpretability, which hinders their application in practice. To overcome this limitation, a data-driven approach, AttentionMixer, is proposed under a generalized message passing framework, with the goal of establishing an accurate and interpretable radiation monitoring framework for energy conversion plants. To improve the model accuracy, the first technical contribution involves the development of spatial and temporal adaptive message passing blocks, which enable the capture of spatial and temporal correlations, respectively; the two blocks are cascaded through a mixing operator. To enhance the model interpretability, the second technical contribution involves the implementation of a sparse message passing regularizer, which eliminates spurious and noisy message passing routes. The effectiveness of the AttentionMixer approach is validated through extensive evaluations on a monitoring benchmark collected from the national radiation monitoring network for nuclear power plants, resulting in enhanced monitoring accuracy and interpretability in practice.\n        \u25b3 Less\n      ",
    "title": "AttentionMixer: An Accurate and Interpretable Framework for Process Monitoring",
    "date": "10 May, 2023",
    "authors": [
      "Hao Wang",
      " Zhiyu Wang",
      " Yunlong Niu",
      " Zhaoran Liu",
      " Haozhe Li",
      " Yilin Liao",
      " Yuxin Huang",
      " Xinggao Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06178",
    "paper_id": "2305.06178",
    "abstract": "\n        The Multi-Object Navigation (MultiON) task requires a robot to localize an instance (each) of multiple object classes. It is a fundamental task for an assistive robot in a home or a factory. Existing methods for MultiON have viewed this as a direct extension of Object Navigation (ON), the task of localising an instance of one object class, and are pre-sequenced, i.e., the sequence in which the object classes are to be explored is provided in advance. This is a strong limitation in practical applications characterized by dynamic changes. This paper describes a deep reinforcement learning framework for sequence-agnostic MultiON based on an actor-critic architecture and a suitable reward specification. Our framework leverages past experiences and seeks to reward progress toward individual as well as multiple target object classes. We use photo-realistic scenes from the Gibson benchmark dataset in the AI Habitat 3D simulation environment to experimentally show that our method performs better than a pre-sequenced approach and a state of the art ON method extended to MultiON.\n        \u25b3 Less\n      ",
    "title": "Sequence-Agnostic Multi-Object Navigation",
    "date": "10 May, 2023",
    "authors": [
      "Nandiraju Gireesh",
      " Ayush Agrawal",
      " Ahana Datta",
      " Snehasis Banerjee",
      " Mohan Sridharan",
      " Brojeshwar Bhowmick",
      " Madhava Krishna"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06228",
    "paper_id": "2305.06228",
    "abstract": "\n        Although the complete scope of the sixth generation of mobile technologies (6G) is still unclear, the prominence of the Internet of Things (IoT) and Artificial Intelligence (AI) / Machine Learning (ML) in the networking field is undeniable. In this regard, key technology enablers for the previous generation, 5G, such as software-defined networking and network function virtualization, fall short to accomplish the stringent requirements envisioned for 6G verticals. This PhD thesis goes back to basics, by exploring missing functionality gaps in relation to these technologies, in order to provide the ''glue'' for holistic and fully-fledged networking solutions for 6G, aligned with standards and industry recommendations. Although ambitious, and in a very early stage, this PhD thesis illustrates an initial design for in-band control in Software-Defined Networking (SDN) that could facilitate the interoperability among constrained IoT devices. The current design demonstrates promising results in terms of resource-usage and robustness, which are pivotal features for constrained networks. Next steps include the integration of the approach with a real testbed comprised of constrained IoT devices and the implementation of a federated learning environment at the edge.\n        \u25b3 Less\n      ",
    "title": "Enabling Technologies for Programmable and Software-Defined Networks: Bolstering the Path Towards 6G",
    "date": "10 May, 2023",
    "authors": [
      "David Carrascal",
      " Elisa Rojas",
      " Diego Lopez-Pajares"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06236",
    "paper_id": "2305.06236",
    "abstract": "\n        X-ray images are the first steps for diagnosing and further treating dental problems. So, early diagnosis prevents the development and increase of oral and dental diseases. In this paper, we developed a semantic segmentation algorithm based on BEIT adaptor and Mask2Former to detect and identify teeth, roots, and multiple dental diseases and abnormalities such as pulp chamber, restoration, endodontics, crown, decay, pin, composite, bridge, pulpitis, orthodontics, radicular cyst, periapical cyst, cyst, implant, and bone graft material in panoramic, periapical, and bitewing X-ray images. We compared the result of our algorithm to two state-of-the-art algorithms in image segmentation named: Deeplabv3 and Segformer on our own data set. We discovered that Radious outperformed those algorithms by increasing the mIoU scores by 9% and 33% in Deeplabv3+ and Segformer, respectively.\n        \u25b3 Less\n      ",
    "title": "Radious: Unveiling the Enigma of Dental Radiology with BEIT Adaptor and Mask2Former in Semantic Segmentation",
    "date": "10 May, 2023",
    "authors": [
      "Mohammad Mashayekhi",
      " Sara Ahmadi Majd",
      " Arian Amiramjadi",
      " Babak Mashayekhi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06244",
    "paper_id": "2305.06244",
    "abstract": "\n        Automated multi-label chest X-rays (CXR) image classification has achieved substantial progress in clinical diagnosis via utilizing sophisticated deep learning approaches. However, most deep models have high computational demands, which makes them less feasible for compact devices with low computational requirements. To overcome this problem, we propose a knowledge distillation (KD) strategy to create the compact deep learning model for the real-time multi-label CXR image classification. We study different alternatives of CNNs and Transforms as the teacher to distill the knowledge to a smaller student. Then, we employed explainable artificial intelligence (XAI) to provide the visual explanation for the model decision improved by the KD. Our results on three benchmark CXR datasets show that our KD strategy provides the improved performance on the compact student model, thus being the feasible choice for many limited hardware platforms. For instance, when using DenseNet161 as the teacher network, EEEA-Net-C2 achieved an AUC of 83.7%, 87.1%, and 88.7% on the ChestX-ray14, CheXpert, and PadChest datasets, respectively, with fewer parameters of 4.7 million and computational cost of 0.3 billion FLOPS.\n        \u25b3 Less\n      ",
    "title": "Explainable Knowledge Distillation for On-device Chest X-Ray Classification",
    "date": "10 May, 2023",
    "authors": [
      "Chakkrit Termritthikun",
      " Ayaz Umer",
      " Suwichaya Suwanwimolkul",
      " Feng Xia",
      " Ivan Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.08134",
    "paper_id": "2303.08134",
    "abstract": "\n        We present a Non-parametric Network for 3D point cloud analysis, Point-NN, which consists of purely non-learnable components: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions. Surprisingly, it performs well on various 3D tasks, requiring no parameters or training, and even surpasses existing fully trained models. Starting from this basic non-parametric model, we propose two extensions. First, Point-NN can serve as a base architectural framework to construct Parametric Networks by simply inserting linear layers on top. Given the superior non-parametric foundation, the derived Point-PN exhibits a high performance-efficiency trade-off with only a few learnable parameters. Second, Point-NN can be regarded as a plug-and-play module for the already trained 3D models during inference. Point-NN captures the complementary geometric knowledge and enhances existing methods for different 3D benchmarks without re-training. We hope our work may cast a light on the community for understanding 3D point clouds with non-parametric methods. Code is available at https://github.com/ZrrSkywalker/Point-NN.\n        \u25b3 Less\n      ",
    "title": "Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis",
    "date": "10 May, 2023",
    "authors": [
      "Renrui Zhang",
      " Liuhui Wang",
      " Ziyu Guo",
      " Yali Wang",
      " Peng Gao",
      " Hongsheng Li",
      " Jianbo Shi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06297",
    "paper_id": "2305.06297",
    "abstract": "\n        A core assumption of explainable AI systems is that explanations change what users know, thereby enabling them to act within their complex socio-technical environments. Despite the centrality of action, explanations are often organized and evaluated based on technical aspects. Prior work varies widely in the connections it traces between information provided in explanations and resulting user actions. An important first step in centering action in evaluations is understanding what the XAI community collectively recognizes as the range of information that explanations can present and what actions are associated with them. In this paper, we present our framework, which maps prior work on information presented in explanations and user action, and we discuss the gaps we uncovered about the information presented to users.\n        \u25b3 Less\n      ",
    "title": "Why Don't You Do Something About It? Outlining Connections between AI Explanations and User Actions",
    "date": "10 May, 2023",
    "authors": [
      "Gennie Mansi",
      " Mark Riedl"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.02657",
    "paper_id": "2301.02657",
    "abstract": "\n        The general domain of video segmentation is currently fragmented into different tasks spanning multiple benchmarks. Despite rapid progress in the state-of-the-art, current methods are overwhelmingly task-specific and cannot conceptually generalize to other tasks. Inspired by recent approaches with multi-task capability, we propose TarViS: a novel, unified network architecture that can be applied to any task that requires segmenting a set of arbitrarily defined 'targets' in video. Our approach is flexible with respect to how tasks define these targets, since it models the latter as abstract 'queries' which are then used to predict pixel-precise target masks. A single TarViS model can be trained jointly on a collection of datasets spanning different tasks, and can hot-swap between tasks during inference without any task-specific retraining. To demonstrate its effectiveness, we apply TarViS to four different tasks, namely Video Instance Segmentation (VIS), Video Panoptic Segmentation (VPS), Video Object Segmentation (VOS) and Point Exemplar-guided Tracking (PET). Our unified, jointly trained model achieves state-of-the-art performance on 5/7 benchmarks spanning these four tasks, and competitive performance on the remaining two. Code and model weights are available at: https://github.com/Ali2500/TarViS\n        \u25b3 Less\n      ",
    "title": "TarViS: A Unified Approach for Target-based Video Segmentation",
    "date": "10 May, 2023",
    "authors": [
      "Ali Athar",
      " Alexander Hermans",
      " Jonathon Luiten",
      " Deva Ramanan",
      " Bastian Leibe"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06305",
    "paper_id": "2305.06305",
    "abstract": "\n        Instance segmentation is a fundamental skill for many robotic applications. We propose a self-supervised method that uses grasp interactions to collect segmentation supervision for an instance segmentation model. When a robot grasps an item, the mask of that grasped item can be inferred from the images of the scene before and after the grasp. Leveraging this insight, we learn a grasp segmentation model to segment the grasped object from before and after grasp images. Such a model can segment grasped objects from thousands of grasp interactions without costly human annotation. Using the segmented grasped objects, we can \"cut\" objects from their original scenes and \"paste\" them into new scenes to generate instance supervision. We show that our grasp segmentation model provides a 5x error reduction when segmenting grasped objects compared with traditional image subtraction approaches. Combined with our \"cut-and-paste\" generation method, instance segmentation models trained with our method achieve better performance than a model trained with 10x the amount of labeled data. On a real robotic grasping system, our instance segmentation model reduces the rate of grasp errors by over 3x compared to an image subtraction baseline.\n        \u25b3 Less\n      ",
    "title": "Self-Supervised Instance Segmentation by Grasping",
    "date": "10 May, 2023",
    "authors": [
      "YuXuan Liu",
      " Xi Chen",
      " Pieter Abbeel"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.03519",
    "paper_id": "2210.03519",
    "abstract": "\n        Pandas is defined as a software library which is used for data analysis in Python programming language. As pandas is a fast, easy and open source data analysis tool, it is rapidly used in different software engineering projects like software development, machine learning, computer vision, natural language processing, robotics, and others. So a huge interests are shown in software developers regarding pandas and a huge number of discussions are now becoming dominant in online developer forums, like Stack Overflow (SO). Such discussions can help to understand the popularity of pandas library and also can help to understand the importance, prevalence, difficulties of pandas topics. The main aim of this research paper is to find the popularity and difficulty of pandas topics. For this regard, SO posts are collected which are related to pandas topic discussions. Topic modeling are done on the textual contents of the posts. We found 26 topics which we further categorized into 5 board categories. We observed that developers discuss variety of pandas topics in SO related to error and excepting handling, visualization, External support, dataframe, and optimization. In addition, a trend chart is generated according to the discussion of topics in a predefined time series. The finding of this paper can provide a path to help the developers, educators and learners. For example, beginner developers can learn most important topics in pandas which are essential for develop any model. Educators can understand the topics which seem hard to learners and can build different tutorials which can make that pandas topic understandable. From this empirical study it is possible to understand the preferences of developers in pandas topic by processing their SO posts\n        \u25b3 Less\n      ",
    "title": "An Empirical Study on How the Developers Discussed about Pandas Topics",
    "date": "10 May, 2023",
    "authors": [
      "Sajib Kumar Saha Joy",
      " Farzad Ahmed",
      " Al Hasib Mahamud",
      " Nibir Chandra Mandal"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06314",
    "paper_id": "2305.06314",
    "abstract": "\n        Reconstructing semantic 3D building models at the level of detail (LoD) 3 is a long-standing challenge. Unlike mesh-based models, they require watertight geometry and object-wise semantics at the fa\u00e7ade level. The principal challenge of such demanding semantic 3D reconstruction is reliable fa\u00e7ade-level semantic segmentation of 3D input data. We present a novel method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building models by improving fa\u00e7ade-level semantic 3D segmentation. To this end, we leverage laser physics and 3D building model priors to probabilistically identify model conflicts. These probabilistic physical conflicts propose locations of model openings: Their final semantics and shapes are inferred in a Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point clouds, and 2D images. To fulfill demanding LoD3 requirements, we use the estimated shapes to cut openings in 3D building priors and fit semantic 3D objects from a library of fa\u00e7ade objects. Extensive experiments on the TUM city campus datasets demonstrate the superior performance of the proposed Scan2LoD3 over the state-of-the-art methods in fa\u00e7ade-level detection, semantic segmentation, and LoD3 building model reconstruction. We believe our method can foster the development of probability-driven semantic 3D reconstruction at LoD3 since not only the high-definition reconstruction but also reconstruction confidence becomes pivotal for various applications such as autonomous driving and urban simulations.\n        \u25b3 Less\n      ",
    "title": "Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks",
    "date": "10 May, 2023",
    "authors": [
      "Olaf Wysocki",
      " Yan Xia",
      " Magdalena Wysocki",
      " Eleonora Grilli",
      " Ludwig Hoegner",
      " Daniel Cremers",
      " Uwe Stilla"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05279",
    "paper_id": "2305.05279",
    "abstract": "\n          Understanding the customers' high level shopping intent, such as their desire to go camping or hold a birthday party, is critically important for an E-commerce platform; it can help boost the quality of shopping experience by enabling provision of more relevant, explainable, and diversified recommendations. However, such high level shopping intent has been overlooked in the industry due to practical challenges. In this work, we introduce Amazon's new system that explicitly identifies and utilizes each customer's high level shopping intents for personalizing recommendations. We develop a novel technique that automatically identifies various high level goals being pursued by the Amazon customers, such as \"go camping\", and \"preparing for a beach party\". Our solution is in a scalable fashion (in 14 languages across 21 countries). Then a deep learning model maps each customer's online behavior, e.g. product search and individual item engagements, into a subset of high level shopping intents. Finally, a realtime ranker considers both the identified intents as well as the granular engagements to present personalized intent-aware recommendations. Extensive offline analysis ensures accuracy and relevance of the new recommendations and we further observe an 10% improvement in the business metrics. This system is currently serving online traffic at amazon.com, powering several production features, driving significant business impacts\n        \u25b3 Less\n      ",
    "title": "Learning to Personalize Recommendation based on Customers' Shopping Intents",
    "date": "10 May, 2023",
    "authors": [
      "Xin Shen",
      " Jiaying Shi",
      " Sungro Yoon",
      " Jon Katzur",
      " Hanbo Wang",
      " Jim Chan",
      " Jin Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06386",
    "paper_id": "2305.06386",
    "abstract": "\n        We observe that the mapping between an image's representation in one model to its representation in another can be learned surprisingly well with just a linear layer, even across diverse models. Building on this observation, we propose text-to-concept\\textit{text-to-concept}, where features from a fixed pretrained model are aligned linearly to the CLIP space, so that text embeddings from CLIP's text encoder become directly comparable to the aligned features. With text-to-concept, we convert fixed off-the-shelf vision encoders to surprisingly strong zero-shot classifiers for free, with accuracy at times even surpassing that of CLIP, despite being much smaller models and trained on a small fraction of the data compared to CLIP. We show other immediate use-cases of text-to-concept, like building concept bottleneck models with no concept supervision, diagnosing distribution shifts in terms of human concepts, and retrieving images satisfying a set of text-based constraints. Lastly, we demonstrate the feasibility of concept-to-text\\textit{concept-to-text}, where vectors in a model's feature space are decoded by first aligning to the CLIP before being fed to a GPT-based generative model. Our work suggests existing deep models, with presumably diverse architectures and training, represent input samples relatively similarly, and a two-way communication across model representation spaces and to humans (through language) is viable.\n        \u25b3 Less\n      ",
    "title": "Text-To-Concept (and Back) via Cross-Model Alignment",
    "date": "10 May, 2023",
    "authors": [
      "Mazda Moayeri",
      " Keivan Rezaei",
      " Maziar Sanjabi",
      " Soheil Feizi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06398",
    "paper_id": "2305.06398",
    "abstract": "\n        Adaptive learning is an area of educational technology that consists in delivering personalized learning experiences to address the unique needs of each learner. An important subfield of adaptive learning is learning path personalization: it aims at designing systems that recommend sequences of educational activities to maximize students' learning outcomes. Many machine learning approaches have already demonstrated significant results in a variety of contexts related to learning path personalization. However, most of them were designed for very specific settings and are not very reusable. This is accentuated by the fact that they often rely on non-scalable models, which are unable to integrate new elements after being trained on a specific set of educational resources. In this paper, we introduce a flexible and scalable approach towards the problem of learning path personalization, which we formalize as a reinforcement learning problem. Our model is a sequential recommender system based on a graph neural network, which we evaluate on a population of simulated learners. Our results demonstrate that it can learn to make good recommendations in the small-data regime.\n        \u25b3 Less\n      ",
    "title": "Towards Scalable Adaptive Learning with Graph Neural Networks and Reinforcement Learning",
    "date": "10 May, 2023",
    "authors": [
      "Jean Vassoyan",
      " Jill-J\u00eann Vie",
      " Pirmin Lemberger"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06404",
    "paper_id": "2305.06404",
    "abstract": "\n        Text embeddings are useful features for several NLP applications, such as sentence similarity, text clustering, and semantic search. In this paper, we present a Low-rank Adaptation with a Contrastive objective on top of 8-bit Siamese-BLOOM, a multilingual large language model optimized to produce semantically meaningful word embeddings. The innovation is threefold. First, we cast BLOOM weights to 8-bit values. Second, we fine-tune BLOOM with a scalable adapter (LoRA) and 8-bit Adam optimizer for sentence similarity classification. Third, we apply a Siamese architecture on BLOOM model with a contrastive objective to ease the multi-lingual labeled data scarcity. The experiment results show the quality of learned embeddings from LACoS-BLOOM is proportional to the number of model parameters and the amount of unlabeled training data. With the parameter efficient fine-tuning design, we are able to run BLOOM 7.1 billion parameters end-to-end on a single GPU machine with 32GB memory. Compared to previous solution Sentence-BERT, we achieve significant improvement on both English and multi-lingual STS tasks.\n        \u25b3 Less\n      ",
    "title": "LACoS-BLOOM: Low-rank Adaptation with Contrastive objective on 8 bits Siamese-BLOOM",
    "date": "10 May, 2023",
    "authors": [
      "Wen-Yu Hua",
      " Brian Williams",
      " Davood Shamsi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06407",
    "paper_id": "2305.06407",
    "abstract": "\n        Outside-knowledge visual question answering is a challenging task that requires both the acquisition and the use of open-ended real-world knowledge. Some existing solutions draw external knowledge into the cross-modality space which overlooks the much vaster textual knowledge in natural-language space, while others transform the image into a text that further fuses with the textual knowledge into the natural-language space and completely abandons the use of visual features. In this paper, we are inspired to constrain the cross-modality space into the same space of natural-language space which makes the visual features preserved directly, and the model still benefits from the vast knowledge in natural-language space. To this end, we propose a novel framework consisting of a multimodal encoder, a textual encoder and an answer decoder. Such structure allows us to introduce more types of knowledge including explicit and implicit multimodal and textual knowledge. Extensive experiments validate the superiority of the proposed method which outperforms the state-of-the-art by 6.17% accuracy. We also conduct comprehensive ablations of each component, and systematically study the roles of varying types of knowledge. Codes and knowledge data can be found at https://github.com/PhoebusSi/Thinking-while-Observing.\n        \u25b3 Less\n      ",
    "title": "Combo of Thinking and Observing for Outside-Knowledge VQA",
    "date": "10 May, 2023",
    "authors": [
      "Qingyi Si",
      " Yuchen Mo",
      " Zheng Lin",
      " Huishan Ji",
      " Weiping Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07034",
    "paper_id": "2305.07034",
    "abstract": "\n        The Quran is the holy scripture of Islam, and its recitation is an important aspect of the religion. Recognizing the recitation of the Holy Quran automatically is a challenging task due to its unique rules that are not applied in normal speaking speeches. A lot of research has been done in this domain, but previous works have detected recitation errors as a classification task or used traditional automatic speech recognition (ASR). In this paper, we proposed a novel end-to-end deep learning model for recognizing the recitation of the Holy Quran. The proposed model is a CNN-Bidirectional GRU encoder that uses CTC as an objective function, and a character-based decoder which is a beam search decoder. Moreover, all previous works were done on small private datasets consisting of short verses and a few chapters of the Holy Quran. As a result of using private datasets, no comparisons were done. To overcome this issue, we used a public dataset that has recently been published (Ar-DAD) and contains about 37 chapters that were recited by 30 reciters, with different recitation speeds and different types of pronunciation rules. The proposed model performance was evaluated using the most common evaluation metrics in speech recognition, word error rate (WER), and character error rate (CER). The results were 8.34% WER and 2.42% CER. We hope this research will be a baseline for comparisons with future research on this public new dataset (Ar-DAD).\n        \u25b3 Less\n      ",
    "title": "Quran Recitation Recognition using End-to-End Deep Learning",
    "date": "10 May, 2023",
    "authors": [
      "Ahmad Al Harere",
      " Khloud Al Jallad"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06415",
    "paper_id": "2305.06415",
    "abstract": "\n        Studies conducted on Western, Educated, Industrialized, Rich, and Democratic (WEIRD) samples are considered atypical of the world's population and may not accurately represent human behavior. In this study, we aim to quantify the extent to which the ACM FAccT conference, the leading venue in exploring Artificial Intelligence (AI) systems' fairness, accountability, and transparency, relies on WEIRD samples. We collected and analyzed 128 papers published between 2018 and 2022, accounting for 30.8% of the overall proceedings published at FAccT in those years (excluding abstracts, tutorials, and papers without human-subject studies or clear country attribution for the participants). We found that 84% of the analyzed papers were exclusively based on participants from Western countries, particularly exclusively from the U.S. (63%). Only researchers who undertook the effort to collect data about local participants through interviews or surveys added diversity to an otherwise U.S.-centric view of science. Therefore, we suggest that researchers collect data from under-represented populations to obtain an inclusive worldview. To achieve this goal, scientific communities should champion data collection from such populations and enforce transparent reporting of data biases.\n        \u25b3 Less\n      ",
    "title": "WEIRD FAccTs: How Western, Educated, Industrialized, Rich, and Democratic is FAccT?",
    "date": "10 May, 2023",
    "authors": [
      "Ali Akbar Septiandri",
      " Marios Constantinides",
      " Mohammad Tahaei",
      " Daniele Quercia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06426",
    "paper_id": "2305.06426",
    "abstract": "\n        Diabetes is a global health priority, especially in low- and-middle-income countries, where over 50% of premature deaths are attributed to high blood glucose. Several studies have demonstrated the feasibility of using Community Health Worker (CHW) programs to provide affordable and culturally tailored solutions for early detection and management of diabetes. Yet, scalable models to design and implement CHW programs while accounting for screening, management, and patient enrollment decisions have not been proposed. We introduce an optimization framework to determine personalized CHW visits that maximize glycemic control at a community-level. Our framework explicitly models the trade-off between screening new patients and providing management visits to individuals who are already enrolled in treatment. We account for patients' motivational states, which affect their decisions to enroll or drop out of treatment and, therefore, the effectiveness of the intervention. We incorporate these decisions by modeling patients as utility-maximizing agents within a bi-level provider problem that we solve using approximate dynamic programming. By estimating patients' health and motivational states, our model builds visit plans that account for patients' tradeoffs when deciding to enroll in treatment, leading to reduced dropout rates and improved resource allocation. We apply our approach to generate CHW visit plans using operational data from a social enterprise serving low-income neighborhoods in urban areas of India. Through extensive simulation experiments, we find that our framework requires up to 73.4% less capacity than the best naive policy to achieve the same performance in terms of glycemic control. Our experiments also show that our solution algorithm can improve upon naive policies by up to 124.5% using the same CHW capacity.\n        \u25b3 Less\n      ",
    "title": "Planning a Community Approach to Diabetes Care in Low- and Middle-Income Countries Using Optimization",
    "date": "10 May, 2023",
    "authors": [
      "Katherine B. Adams",
      " Justin J. Boutilier",
      " Sarang Deo",
      " Yonatan Mintz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.06100",
    "paper_id": "2302.06100",
    "abstract": "\n        Statutory reasoning is the task of reasoning with facts and statutes, which are rules written in natural language by a legislature. It is a basic legal skill. In this paper we explore the capabilities of the most capable GPT-3 model, text-davinci-003, on an established statutory-reasoning dataset called SARA. We consider a variety of approaches, including dynamic few-shot prompting, chain-of-thought prompting, and zero-shot prompting. While we achieve results with GPT-3 that are better than the previous best published results, we also identify several types of clear errors it makes. We investigate why these errors happen. We discover that GPT-3 has imperfect prior knowledge of the actual U.S. statutes on which SARA is based. More importantly, we create simple synthetic statutes, which GPT-3 is guaranteed not to have seen during training. We find GPT-3 performs poorly at answering straightforward questions about these simple synthetic statutes.\n        \u25b3 Less\n      ",
    "title": "Can GPT-3 Perform Statutory Reasoning?",
    "date": "10 May, 2023",
    "authors": [
      "Andrew Blair-Stanek",
      " Nils Holzenberger",
      " Benjamin Van Durme"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10351",
    "paper_id": "2305.10351",
    "abstract": "\n        Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.\n  To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\\method). The proposed \\method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified \"biosignal sentences\". Specifically, we tokenize each channel into fixed-length segments containing local signal features, flattening them to form consistent \"sentences\". Channel embeddings and {\\em relative} position embeddings are added to preserve spatio-temporal features.\n  The \\method model is versatile and applicable to various biosignal learning settings across different datasets, including joint pre-training for larger models. Comprehensive evaluations on EEG, electrocardiogram (ECG), and human activity sensory signals demonstrate that \\method outperforms robust baselines in common settings and facilitates learning across multiple datasets with different formats. Use CHB-MIT seizure detection task as an example, our vanilla \\method model shows 3\\% improvement over baselines in balanced accuracy, and the pre-trained \\method models (optimized from other data sources) can further bring up to 4\\% improvements.\n        \u25b3 Less\n      ",
    "title": "BIOT: Cross-data Biosignal Learning in the Wild",
    "date": "10 May, 2023",
    "authors": [
      "Chaoqi Yang",
      " M. Brandon Westover",
      " Jimeng Sun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06429",
    "paper_id": "2305.06429",
    "abstract": "\n        In Islam, readers must apply a set of pronunciation rules called Tajweed rules to recite the Quran in the same way that the angel Jibrael taught the Prophet, Muhammad. The traditional process of learning the correct application of these rules requires a human who must have a license and great experience to detect mispronunciation. Due to the increasing number of Muslims around the world, the number of Tajweed teachers is not enough nowadays for daily recitation practice for every Muslim. Therefore, lots of work has been done for automatic Tajweed rules' mispronunciation detection to help readers recite Quran correctly in an easier way and shorter time than traditional learning ways. All previous works have three common problems. First, most of them focused on machine learning algorithms only. Second, they used private datasets with no benchmark to compare with. Third, they did not take into consideration the sequence of input data optimally, although the speech signal is time series. To overcome these problems, we proposed a solution that consists of Mel-Frequency Cepstral Coefficient (MFCC) features with Long Short-Term Memory (LSTM) neural networks which use the time series, to detect mispronunciation in Tajweed rules. In addition, our experiments were performed on a public dataset, the QDAT dataset, which contains more than 1500 voices of the correct and incorrect recitation of three Tajweed rules (Separate stretching , Tight Noon , and Hide ). To the best of our knowledge, the QDAT dataset has not been used by any research paper yet. We compared the performance of the proposed LSTM model with traditional machine learning algorithms used in SoTA. The LSTM model with time series showed clear superiority over traditional machine learning. The accuracy achieved by LSTM on the QDAT dataset was 96%, 95%, and 96% for the three rules (Separate stretching, Tight Noon, and Hide), respectively.\n        \u25b3 Less\n      ",
    "title": "Mispronunciation Detection of Basic Quranic Recitation Rules using Deep Learning",
    "date": "10 May, 2023",
    "authors": [
      "Ahmad Al Harere",
      " Khloud Al Jallad"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06430",
    "paper_id": "2305.06430",
    "abstract": "\n        As IoT devices are becoming widely deployed, there exist many threats to IoT-based systems due to their inherent vulnerabilities. One effective approach to improving IoT security is to deploy IoT honeypot systems, which can collect attack information and reveal the methods and strategies used by attackers. However, building high-interaction IoT honeypots is challenging due to the heterogeneity of IoT devices. Vulnerabilities in IoT devices typically depend on specific device types or firmware versions, which encourages attackers to perform pre-attack checks to gather device information before launching attacks. Moreover, conventional honeypots are easily detected because their replying logic differs from that of the IoT devices they try to mimic. To address these problems, we develop an adaptive high-interaction honeypot for IoT devices, called HoneyIoT. We first build a real device based attack trace collection system to learn how attackers interact with IoT devices. We then model the attack behavior through markov decision process and leverage reinforcement learning techniques to learn the best responses to engage attackers based on the attack trace. We also use differential analysis techniques to mutate response values in some fields to generate high-fidelity responses. HoneyIoT has been deployed on the public Internet. Experimental results show that HoneyIoT can effectively bypass the pre-attack checks and mislead the attackers into uploading malware. Furthermore, HoneyIoT is covert against widely used reconnaissance and honeypot detection tools.\n        \u25b3 Less\n      ",
    "title": "HoneyIoT: Adaptive High-Interaction Honeypot for IoT Devices Through Reinforcement Learning",
    "date": "10 May, 2023",
    "authors": [
      "Chongqi Guan",
      " Heting Liu",
      " Guohong Cao",
      " Sencun Zhu",
      " Thomas La Porta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06437",
    "paper_id": "2305.06437",
    "abstract": "\n        Self-supervised video representation learning aimed at maximizing similarity between different temporal segments of one video, in order to enforce feature persistence over time. This leads to loss of pertinent information related to temporal relationships, rendering actions such as `enter' and `leave' to be indistinguishable. To mitigate this limitation, we propose Latent Time Navigation (LTN), a time-parameterized contrastive learning strategy that is streamlined to capture fine-grained motions. Specifically, we maximize the representation similarity between different video segments from one video, while maintaining their representations time-aware along a subspace of the latent representation code including an orthogonal basis to represent temporal changes. Our extensive experimental analysis suggests that learning video representations by LTN consistently improves performance of action classification in fine-grained and human-oriented tasks (e.g., on Toyota Smarthome dataset). In addition, we demonstrate that our proposed model, when pre-trained on Kinetics-400, generalizes well onto the unseen real world video benchmark datasets UCF101 and HMDB51, achieving state-of-the-art performance in action recognition.\n        \u25b3 Less\n      ",
    "title": "Self-Supervised Video Representation Learning via Latent Time Navigation",
    "date": "10 May, 2023",
    "authors": [
      "Di Yang",
      " Yaohui Wang",
      " Quan Kong",
      " Antitza Dantcheva",
      " Lorenzo Garattoni",
      " Gianpiero Francesca",
      " Francois Bremond"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06439",
    "paper_id": "2305.06439",
    "abstract": "\n        GitHub Copilot is an artificially intelligent programming assistant used by many developers. While a few studies have evaluated the security risks of using Copilot, there has not been any study to show if it aids developers in producing code with better runtime performance. We evaluate the runtime performance of code produced when developers use GitHub Copilot versus when they do not. To this end, we conducted a user study with 32 participants where each participant solved two C++ programming problems, one with Copilot and the other without it and measured the runtime performance of the participants' solutions on our test data. Our results suggest that using Copilot may produce code with a significantly slower runtime performance.\n        \u25b3 Less\n      ",
    "title": "Measuring the Runtime Performance of Code Produced with GitHub Copilot",
    "date": "10 May, 2023",
    "authors": [
      "Daniel Erhabor",
      " Sreeharsha Udayashankar",
      " Meiyappan Nagappan",
      " Samer Al-Kiswany"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2112.07569",
    "paper_id": "2112.07569",
    "abstract": "\n        Advances in autonomy offer the potential for dramatic positive outcomes in a number of domains, yet enabling their safe deployment remains an open problem. This work's motivating question is: In safety-critical settings, can we avoid the need to have one human supervise one machine at all times? The work formalizes this scalable supervision problem by considering remotely located human supervisors and investigating how autonomous agents can cooperate to achieve safety. This article focuses on the safety-critical context of autonomous vehicles (AVs) merging into traffic consisting of a mixture of AVs and human drivers. The analysis establishes high reliability upper bounds on human supervision requirements. It further shows that AV cooperation can improve supervision reliability by orders of magnitude and counterintuitively requires fewer supervisors (per AV) as more AVs are adopted. These analytical results leverage queuing-theoretic analysis, order statistics, and a conservative, reachability-based approach. A key takeaway is the potential value of cooperation in enabling the deployment of autonomy at scale. While this work focuses on AVs, the scalable supervision framework may be of independent interest to a broader array of autonomous control challenges.\n        \u25b3 Less\n      ",
    "title": "Cooperation for Scalable Supervision of Autonomy in Mixed Traffic",
    "date": "10 May, 2023",
    "authors": [
      "Cameron Hickert",
      " Sirui Li",
      " Cathy Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.02560",
    "paper_id": "2212.02560",
    "abstract": "\n        Few-shot relation extraction aims to recognize novel relations with few labeled sentences in each relation. Previous metric-based few-shot relation extraction algorithms identify relationships by comparing the prototypes generated by the few labeled sentences embedding with the embeddings of the query sentences using a trained metric function. However, as these domains always have considerable differences from those in the training dataset, the generalization ability of these approaches on unseen relations in many domains is limited. Since the prototype is necessary for obtaining relationships between entities in the latent space, we suggest learning more interpretable and efficient prototypes from prior knowledge and the intrinsic semantics of relations to extract new relations in various domains more effectively. By exploring the relationships between relations using prior information, we effectively improve the prototype representation of relations. By using contrastive learning to make the classification margins between sentence embedding more distinct, the prototype's geometric interpretability is enhanced. Additionally, utilizing a transfer learning approach for the cross-domain problem allows the generation process of the prototype to account for the gap between other domains, making the prototype more robust and enabling the better extraction of associations across multiple domains. The experiment results on the benchmark FewRel dataset demonstrate the advantages of the suggested method over some state-of-the-art approaches.\n        \u25b3 Less\n      ",
    "title": "Cross-Domain Few-Shot Relation Extraction via Representation Learning and Domain Adaptation",
    "date": "10 May, 2023",
    "authors": [
      "Zhongju Yuan",
      " Zhenkun Wang",
      " Genghui Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.13081",
    "paper_id": "2304.13081",
    "abstract": "\n        Private and public sector structures and norms refine how emerging technology is used in practice. In healthcare, despite a proliferation of AI adoption, the organizational governance surrounding its use and integration is often poorly understood. What the Health AI Partnership (HAIP) aims to do in this research is to better define the requirements for adequate organizational governance of AI systems in healthcare settings and support health system leaders to make more informed decisions around AI adoption. To work towards this understanding, we first identify how the standards for the AI adoption in healthcare may be designed to be used easily and efficiently. Then, we map out the precise decision points involved in the practical institutional adoption of AI technology within specific health systems. Practically, we achieve this through a multi-organizational collaboration with leaders from major health systems across the United States and key informants from related fields. Working with the consultancy IDEO [dot] org, we were able to conduct usability-testing sessions with healthcare and AI ethics professionals. Usability analysis revealed a prototype structured around mock key decision points that align with how organizational leaders approach technology adoption. Concurrently, we conducted semi-structured interviews with 89 professionals in healthcare and other relevant fields. Using a modified grounded theory approach, we were able to identify 8 key decision points and comprehensive procedures throughout the AI adoption lifecycle. This is one of the most detailed qualitative analyses to date of the current governance structures and processes involved in AI adoption by health systems in the United States. We hope these findings can inform future efforts to build capabilities to promote the safe, effective, and responsible adoption of emerging technologies in healthcare.\n        \u25b3 Less\n      ",
    "title": "Organizational Governance of Emerging Technologies: AI Adoption in Healthcare",
    "date": "10 May, 2023",
    "authors": [
      "Jee Young Kim",
      " William Boag",
      " Freya Gulamali",
      " Alifia Hasan",
      " Henry David Jeffry Hogg",
      " Mark Lifson",
      " Deirdre Mulligan",
      " Manesh Patel",
      " Inioluwa Deborah Raji",
      " Ajai Sehgal",
      " Keo Shaw",
      " Danny Tobey",
      " Alexandra Valladares",
      " David Vidal",
      " Suresh Balu",
      " Mark Sendak"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06450",
    "paper_id": "2305.06450",
    "abstract": "\n        Technological advances in the context of digital transformation are the basis for rapid developments in the field of artificial intelligence (AI). Although AI is not a new topic in computer science (CS), recent developments are having an immense impact on everyday life and society. In consequence, everyone needs competencies to be able to adequately and competently analyze, discuss and help shape the impact, opportunities, and limits of artificial intelligence on their personal lives and our society. As a result, an increasing number of CS curricula are being extended to include the topic of AI. However, in order to integrate AI into existing CS curricula, what students can and should learn in the context of AI needs to be clarified. This has proven to be particularly difficult, considering that so far CS education research on central concepts and principles of AI lacks sufficient elaboration. Therefore, in this paper, we present a curriculum of learning objectives that addresses digital literacy and the societal perspective in particular. The learning objectives can be used to comprehensively design curricula, but also allow for analyzing current curricula and teaching materials and provide insights into the central concepts and corresponding competencies of AI.\n        \u25b3 Less\n      ",
    "title": "What Students Can Learn About Artificial Intelligence -- Recommendations for K-12 Computing Education",
    "date": "10 May, 2023",
    "authors": [
      "Tilman Michaeli",
      " Stefan Seegerer",
      " Ralf Romeike"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.12000",
    "paper_id": "2204.12000",
    "abstract": "\n        Technology for open-ended language generation, a key application of artificial intelligence, has advanced to a great extent in recent years. Large-scale language models, which are trained on large corpora of text, are being used in a wide range of applications everywhere, from virtual assistants to conversational bots. While these language models output fluent text, existing research shows that these models can and do capture human biases. Many of these biases, especially those that could potentially cause harm, are being well-investigated. On the other hand, studies that infer and change human personality traits inherited by these models have been scarce or non-existent. Our work seeks to address this gap by exploring the personality traits of several large-scale language models designed for open-ended text generation and the datasets used for training them. We build on the popular Big Five factors and develop robust methods that quantify the personality traits of these models and their underlying datasets. In particular, we trigger the models with a questionnaire designed for personality assessment and subsequently classify the text responses into quantifiable traits using a Zero-shot classifier. Our estimation scheme sheds light on an important anthropomorphic element found in such AI models and can help stakeholders decide how they should be applied as well as how society could perceive them. Additionally, we examined approaches to alter these personalities, adding to our understanding of how AI models can be adapted to specific contexts.\n        \u25b3 Less\n      ",
    "title": "Estimating the Personality of White-Box Language Models",
    "date": "10 May, 2023",
    "authors": [
      "Saketh Reddy Karra",
      " Son The Nguyen",
      " Theja Tulabandhula"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.01568",
    "paper_id": "2211.01568",
    "abstract": "\n        Language models often pre-train on large unsupervised text corpora, then fine-tune on additional task-specific data. However, typical fine-tuning schemes do not prioritize the examples that they tune on. We show that, if you can prioritize informative training data, you can achieve better performance while using fewer labels. To do this we augment a language model with an epinet: a small additional network that helps to estimate model uncertainty and forms an \\textit{epistemic neural network} (ENN). ENNs are neural networks that can know what they don't know. Using an epinet to prioritize uncertain data, we can fine-tune BERT on GLUE tasks to the same performance while using 2x less data than training without prioritization. We also investigate performance in synthetic neural network generative models designed to build understanding. In each setting, using an epinet outperforms heuristic active learning schemes.\n        \u25b3 Less\n      ",
    "title": "Fine-Tuning Language Models via Epistemic Neural Networks",
    "date": "10 May, 2023",
    "authors": [
      "Ian Osband",
      " Seyed Mohammad Asghari",
      " Benjamin Van Roy",
      " Nat McAleese",
      " John Aslanides",
      " Geoffrey Irving"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07035",
    "paper_id": "2305.07035",
    "abstract": "\n        An operation is called covert if it conceals the identity of the actor; it is called clandestine if the very fact that the operation is conducted is concealed. The paper proposes a formal semantics of clandestine operations and introduces a sound and complete logical system that describes the interplay between the distributed knowledge modality and a modality capturing coalition power to conduct clandestine operations.\n        \u25b3 Less\n      ",
    "title": "Shhh! The Logic of Clandestine Operations",
    "date": "10 May, 2023",
    "authors": [
      "Pavel Naumov",
      " Oliver Orejola"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.07412",
    "paper_id": "2302.07412",
    "abstract": "\n        Inspired by the theory of desirable gambles that is used to model uncertainty in the field of imprecise probabilities, I present a theory of desirable things. Its aim is to model a subject's beliefs about which things are desirable. What the things are is not important, nor is what it means for them to be desirable. It can be applied to gambles, calling them desirable if a subject accepts them, but it can just as well be applied to pizzas, calling them desirable if my friend Arthur likes to eat them. Other useful examples of things one might apply this theory to are propositions, horse lotteries, or preferences between any of the above. Regardless of the particular things that are considered, inference rules are imposed by means of an abstract closure operator, and models that adhere to these rules are called coherent. I consider two types of models, each of which can capture a subject's beliefs about which things are desirable: sets of desirable things and sets of desirable sets of things. A crucial result is that the latter type can be represented by a set of the former.\n        \u25b3 Less\n      ",
    "title": "A theory of desirable things",
    "date": "10 May, 2023",
    "authors": [
      "Jasper De Bock"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06485",
    "paper_id": "2305.06485",
    "abstract": "\n        Task planning is an important component of traditional robotics systems enabling robots to compose fine grained skills to perform more complex tasks. Recent work building systems for translating natural language to executable actions for task completion in simulated embodied agents is focused on directly predicting low level action sequences that would be expected to be directly executable by a physical robot. In this work, we instead focus on predicting a higher level plan representation for one such embodied task completion dataset - TEACh, under the assumption that techniques for high-level plan prediction from natural language are expected to be more transferable to physical robot systems. We demonstrate that better plans can be predicted using multimodal context, and that plan prediction and plan execution modules are likely dependent on each other and hence it may not be ideal to fully decouple them. Further, we benchmark execution of oracle plans to quantify the scope for improvement in plan prediction models.\n        \u25b3 Less\n      ",
    "title": "Multimodal Contextualized Plan Prediction for Embodied Task Completion",
    "date": "10 May, 2023",
    "authors": [
      "Mert \u0130nan",
      " Aishwarya Padmakumar",
      " Spandana Gella",
      " Patrick Lange",
      " Dilek Hakkani-Tur"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.14410",
    "paper_id": "2210.14410",
    "abstract": "\n        This work concerns the development of deep networks that are certifiably robust to adversarial attacks. Joint robust classification-detection was recently introduced as a certified defense mechanism, where adversarial examples are either correctly classified or assigned to the \"abstain\" class. In this work, we show that such a provable framework can benefit by extension to networks with multiple explicit abstain classes, where the adversarial examples are adaptively assigned to those. We show that naively adding multiple abstain classes can lead to \"model degeneracy\", then we propose a regularization approach and a training method to counter this degeneracy by promoting full use of the multiple abstain classes. Our experiments demonstrate that the proposed approach consistently achieves favorable standard vs. robust verified accuracy tradeoffs, outperforming state-of-the-art algorithms for various choices of number of abstain classes.\n        \u25b3 Less\n      ",
    "title": "Improving Adversarial Robustness via Joint Classification and Multiple Explicit Detection Classes",
    "date": "10 May, 2023",
    "authors": [
      "Sina Baharlouei",
      " Fatemeh Sheikholeslami",
      " Meisam Razaviyayn",
      " Zico Kolter"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2206.13089",
    "paper_id": "2206.13089",
    "abstract": "\n        Recently, Miller et al. showed that a model's in-distribution (ID) accuracy has a strong linear correlation with its out-of-distribution (OOD) accuracy on several OOD benchmarks -- a phenomenon they dubbed ''accuracy-on-the-line''. While a useful tool for model selection (i.e., the model most likely to perform the best OOD is the one with highest ID accuracy), this fact does not help estimate the actual OOD performance of models without access to a labeled OOD validation set. In this paper, we show a similar but surprising phenomenon also holds for the agreement between pairs of neural network classifiers: whenever accuracy-on-the-line holds, we observe that the OOD agreement between the predictions of any two pairs of neural networks (with potentially different architectures) also observes a strong linear correlation with their ID agreement. Furthermore, we observe that the slope and bias of OOD vs ID agreement closely matches that of OOD vs ID accuracy. This phenomenon, which we call agreement-on-the-line, has important practical applications: without any labeled data, we can predict the OOD accuracy of classifiers}, since OOD agreement can be estimated with just unlabeled data. Our prediction algorithm outperforms previous methods both in shifts where agreement-on-the-line holds and, surprisingly, when accuracy is not on the line. This phenomenon also provides new insights into deep neural networks: unlike accuracy-on-the-line, agreement-on-the-line appears to only hold for neural network classifiers.\n        \u25b3 Less\n      ",
    "title": "Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift",
    "date": "10 May, 2023",
    "authors": [
      "Christina Baek",
      " Yiding Jiang",
      " Aditi Raghunathan",
      " Zico Kolter"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05711",
    "paper_id": "2305.05711",
    "abstract": "\n        Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperforms fine-tuning moderate-size pre-trained models specially designed for IE tasks (e.g., UIE) and prompting NL-LLMs under few-shot settings. We further conduct a series of in-depth analyses to demonstrate the merits of leveraging Code-LLMs for IE tasks.\n        \u25b3 Less\n      ",
    "title": "CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors",
    "date": "10 May, 2023",
    "authors": [
      "Peng Li",
      " Tianxiang Sun",
      " Qiong Tang",
      " Hang Yan",
      " Yuanbin Wu",
      " Xuanjing Huang",
      " Xipeng Qiu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06522",
    "paper_id": "2305.06522",
    "abstract": "\n        Large-scale pre-trained language models have shown outstanding performance in a variety of NLP tasks. However, they are also known to be significantly brittle against specifically crafted adversarial examples, leading to increasing interest in probing the adversarial robustness of NLP systems. We introduce RSMI, a novel two-stage framework that combines randomized smoothing (RS) with masked inference (MI) to improve the adversarial robustness of NLP systems. RS transforms a classifier into a smoothed classifier to obtain robust representations, whereas MI forces a model to exploit the surrounding context of a masked token in an input sequence. RSMI improves adversarial robustness by 2 to 3 times over existing state-of-the-art methods on benchmark datasets. We also perform in-depth qualitative analysis to validate the effectiveness of the different stages of RSMI and probe the impact of its components through extensive ablations. By empirically proving the stability of RSMI, we put it forward as a practical method to robustly train large-scale NLP models. Our code and datasets are available at https://github.com/Han8931/rsmi_nlp\n        \u25b3 Less\n      ",
    "title": "Randomized Smoothing with Masked Inference for Adversarially Robust Text Classifications",
    "date": "10 May, 2023",
    "authors": [
      "Han Cheol Moon",
      " Shafiq Joty",
      " Ruochen Zhao",
      " Megh Thakkar",
      " Xu Chi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07036",
    "paper_id": "2305.07036",
    "abstract": "\n        We propose the GFlowNets with Human Feedback (GFlowHF) framework to improve the exploration ability when training AI models. For tasks where the reward is unknown, we fit the reward function through human evaluations on different trajectories. The goal of GFlowHF is to learn a policy that is strictly proportional to human ratings, instead of only focusing on human favorite ratings like RLHF. Experiments show that GFlowHF can achieve better exploration ability than RLHF.\n        \u25b3 Less\n      ",
    "title": "GFlowNets with Human Feedback",
    "date": "10 May, 2023",
    "authors": [
      "Yinchuan Li",
      " Shuang Luo",
      " Yunfeng Shao",
      " Jianye Hao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06530",
    "paper_id": "2305.06530",
    "abstract": "\n        Recent advancements in Natural Language Processing (NLP) has led to the proliferation of large pretrained language models. These models have been shown to yield good performance, using in-context learning, even on unseen tasks and languages. They have also been exposed as commercial APIs as a form of language-model-as-a-service, with great adoption. However, their performance on African languages is largely unknown. We present a preliminary analysis of commercial large language models on two tasks (machine translation and text classification) across eight African languages, spanning different language families and geographical areas. Our results suggest that commercial language models produce below-par performance on African languages. We also find that they perform better on text classification than machine translation. In general, our findings present a call-to-action to ensure African languages are well represented in commercial large language models, given their growing popularity.\n        \u25b3 Less\n      ",
    "title": "How Good are Commercial Large Language Models on African Languages?",
    "date": "10 May, 2023",
    "authors": [
      "Jessica Ojo",
      " Kelechi Ogueji"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06541",
    "paper_id": "2305.06541",
    "abstract": "\n        Spectral clustering is one of the most popular clustering algorithms that has stood the test of time. It is simple to describe, can be implemented using standard linear algebra, and often finds better clusters than traditional clustering algorithms like kk-means and kk-centers. The foundational algorithm for two-way spectral clustering, by Shi and Malik, creates a geometric graph from data and finds a spectral cut of the graph.\n  In modern machine learning, many data sets are modeled as a large number of points drawn from a probability density function. Little is known about when spectral clustering works in this setting -- and when it doesn't. Past researchers justified spectral clustering by appealing to the graph Cheeger inequality (which states that the spectral cut of a graph approximates the ``Normalized Cut''), but this justification is known to break down on large data sets.\n  We provide theoretically-informed intuition about spectral clustering on large data sets drawn from probability densities, by proving when a continuous form of spectral clustering considered by past researchers (the unweighted spectral cut of a probability density) finds good clusters of the underlying density itself. Our work suggests that Shi-Malik spectral clustering works well on data drawn from mixtures of Laplace distributions, and works poorly on data drawn from certain other densities, such as a density we call the `square-root trough'.\n  Our core theorem proves that weighted spectral cuts have low weighted isoperimetry for all probability densities. Our key tool is a new Cheeger-Buser inequality for all probability densities, including discontinuous ones.\n        \u25b3 Less\n      ",
    "title": "Spectral Clustering on Large Datasets: When Does it Work? Theory from Continuous Clustering and Density Cheeger-Buser",
    "date": "10 May, 2023",
    "authors": [
      "Timothy Chu",
      " Gary Miller",
      " Noel Walkington"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06545",
    "paper_id": "2305.06545",
    "abstract": "\n        With a fast developing pace of geographic applications, automatable and intelligent models are essential to be designed to handle the large volume of information. However, few researchers focus on geographic natural language processing, and there has never been a benchmark to build a unified standard. In this work, we propose a GeoGraphic Language Understanding Evaluation benchmark, named GeoGLUE. We collect data from open-released geographic resources and introduce six natural language understanding tasks, including geographic textual similarity on recall, geographic textual similarity on rerank, geographic elements tagging, geographic composition analysis, geographic where what cut, and geographic entity alignment. We also pro vide evaluation experiments and analysis of general baselines, indicating the effectiveness and significance of the GeoGLUE benchmark.\n        \u25b3 Less\n      ",
    "title": "GeoGLUE: A GeoGraphic Language Understanding Evaluation Benchmark",
    "date": "10 May, 2023",
    "authors": [
      "Dongyang Li",
      " Ruixue Ding",
      " Qiang Zhang",
      " Zheng Li",
      " Boli Chen",
      " Pengjun Xie",
      " Yao Xu",
      " Xin Li",
      " Ning Guo",
      " Fei Huang",
      " Xiaofeng He"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.13836",
    "paper_id": "2304.13836",
    "abstract": "\n        Approaches for appraising feature importance approximations, alternatively referred to as attribution methods, have been established across an extensive array of contexts. The development of resilient techniques for performance benchmarking constitutes a critical concern in the sphere of explainable deep learning. This study scrutinizes the dependability of the RemOve-And-Retrain (ROAR) procedure, which is prevalently employed for gauging the performance of feature importance estimates. The insights gleaned from our theoretical foundation and empirical investigations reveal that attributions containing lesser information about the decision function may yield superior results in ROAR benchmarks, contradicting the original intent of ROAR. This occurrence is similarly observed in the recently introduced variant RemOve-And-Debias (ROAD), and we posit a persistent pattern of blurriness bias in ROAR attribution metrics. Our findings serve as a warning against indiscriminate use on ROAR metrics. The code is available as open source.\n        \u25b3 Less\n      ",
    "title": "On Pitfalls of \nRemOve-And-Retrain\n: Data Processing Inequality Perspective",
    "date": "10 May, 2023",
    "authors": [
      "Junhwa Song",
      " Keumgang Cha",
      " Junghoon Seo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06555",
    "paper_id": "2305.06555",
    "abstract": "\n        Lifelong learning (LL) is an important ability for NLP models to learn new tasks continuously. Architecture-based approaches are reported to be effective implementations for LL models. However, it is non-trivial to extend previous approaches to domain incremental LL scenarios since they either require access to task identities in the testing phase or cannot handle samples from unseen tasks. In this paper, we propose \\textbf{Diana}: a \\underline{d}ynam\\underline{i}c \\underline{a}rchitecture-based lifelo\\underline{n}g le\\underline{a}rning model that tries to learn a sequence of tasks with a prompt-enhanced language model. Four types of hierarchically organized prompts are used in Diana to capture knowledge from different granularities. Specifically, we dedicate task-level prompts to capture task-specific knowledge to retain high LL performances and maintain instance-level prompts to learn knowledge shared across input samples to improve the model's generalization performance. Moreover, we dedicate separate prompts to explicitly model unseen tasks and introduce a set of prompt key vectors to facilitate knowledge sharing between tasks. Extensive experiments demonstrate that Diana outperforms state-of-the-art LL models, especially in handling unseen tasks. We release the code and data at \\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/diana}.\n        \u25b3 Less\n      ",
    "title": "Domain Incremental Lifelong Learning in an Open World",
    "date": "10 May, 2023",
    "authors": [
      "Yi Dai",
      " Hao Lang",
      " Yinhe Zheng",
      " Bowen Yu",
      " Fei Huang",
      " Yongbin Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06557",
    "paper_id": "2305.06557",
    "abstract": "\n        Real-world data often have an open long-tailed distribution, and building a unified QA model supporting various tasks is vital for practical QA applications. However, it is non-trivial to extend previous QA approaches since they either require access to seen tasks of adequate samples or do not explicitly model samples from unseen tasks. In this paper, we define Open Long-Tailed QA (OLTQA) as learning from long-tailed distributed data and optimizing performance over seen and unseen QA tasks. We propose an OLTQA model that encourages knowledge sharing between head, tail and unseen tasks, and explicitly mines knowledge from a large pre-trained language model (LM). Specifically, we organize our model through a pool of fine-grained components and dynamically combine these components for an input to facilitate knowledge sharing. A retrieve-then-rerank frame is further introduced to select in-context examples, which guild the LM to generate text that express knowledge for QA tasks. Moreover, a two-stage training approach is introduced to pre-train the framework by knowledge distillation (KD) from the LM and then jointly train the frame and a QA model through an adaptive mutual KD method. On a large-scale OLTQA dataset we curate from 43 existing QA datasets, our model consistently outperforms the state-of-the-art. We release the code and data at \\url{https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/oltqa}.\n        \u25b3 Less\n      ",
    "title": "Long-Tailed Question Answering in an Open World",
    "date": "10 May, 2023",
    "authors": [
      "Yi Dai",
      " Hao Lang",
      " Yinhe Zheng",
      " Fei Huang",
      " Yongbin Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06574",
    "paper_id": "2305.06574",
    "abstract": "\n        Entity alignment is the task of identifying corresponding entities across different knowledge graphs (KGs). Although recent embedding-based entity alignment methods have shown significant advancements, they still struggle to fully utilize KG structural information. In this paper, we introduce FGWEA, an unsupervised entity alignment framework that leverages the Fused Gromov-Wasserstein (FGW) distance, allowing for a comprehensive comparison of entity semantics and KG structures within a joint optimization framework. To address the computational challenges associated with optimizing FGW, we devise a three-stage progressive optimization algorithm. It starts with a basic semantic embedding matching, proceeds to approximate cross-KG structural and relational similarity matching based on iterative updates of high-confidence entity links, and ultimately culminates in a global structural comparison between KGs. We perform extensive experiments on four entity alignment datasets covering 14 distinct KGs across five languages. Without any supervision or hyper-parameter tuning, FGWEA surpasses 21 competitive baselines, including cutting-edge supervised entity alignment methods. Our code is available at https://github.com/squareRoot3/FusedGW-Entity-Alignment.\n        \u25b3 Less\n      ",
    "title": "A Fused Gromov-Wasserstein Framework for Unsupervised Knowledge Graph Entity Alignment",
    "date": "10 May, 2023",
    "authors": [
      "Jianheng Tang",
      " Kangfei Zhao",
      " Jia Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.11116",
    "paper_id": "2304.11116",
    "abstract": "\n        In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.\n  To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with prompts augmented by ChatGPT to use external graph reasoning API tools. Specifically, we will investigate to teach Graph-ToolFormer to handle various graph data reasoning tasks in this paper, including both (1) very basic graph data loading and graph property reasoning tasks, ranging from simple graph order and size to the graph diameter and periphery, and (2) more advanced reasoning tasks on real-world graph data, such as bibliographic networks, protein molecules, sequential recommender systems, social networks and knowledge graphs.\n        \u25b3 Less\n      ",
    "title": "Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT",
    "date": "10 May, 2023",
    "authors": [
      "Jiawei Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00666",
    "paper_id": "2305.00666",
    "abstract": "\n        In recent years, remarkable results have been achieved in self-supervised action recognition using skeleton sequences with contrastive learning. It has been observed that the semantic distinction of human action features is often represented by local body parts, such as legs or hands, which are advantageous for skeleton-based action recognition. This paper proposes an attention-based contrastive learning framework for skeleton representation learning, called SkeAttnCLR, which integrates local similarity and global features for skeleton-based action representations. To achieve this, a multi-head attention mask module is employed to learn the soft attention mask features from the skeletons, suppressing non-salient local features while accentuating local salient features, thereby bringing similar local features closer in the feature space. Additionally, ample contrastive pairs are generated by expanding contrastive pairs based on salient and non-salient features with global features, which guide the network to learn the semantic representations of the entire skeleton. Therefore, with the attention mask mechanism, SkeAttnCLR learns local features under different data augmentation views. The experiment results demonstrate that the inclusion of local feature similarity significantly enhances skeleton-based action representation. Our proposed SkeAttnCLR outperforms state-of-the-art methods on NTURGB+D, NTU120-RGB+D, and PKU-MMD datasets.\n        \u25b3 Less\n      ",
    "title": "Part Aware Contrastive Learning for Self-Supervised Action Recognition",
    "date": "10 May, 2023",
    "authors": [
      "Yilei Hua",
      " Wenhan Wu",
      " Ce Zheng",
      " Aidong Lu",
      " Mengyuan Liu",
      " Chen Chen",
      " Shiqian Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16163",
    "paper_id": "2305.16163",
    "abstract": "\n        Privacy-preserving cross-domain recommendation (PPCDR) refers to preserving the privacy of users when transferring the knowledge from source domain to target domain for better performance, which is vital for the long-term development of recommender systems. Existing work on cross-domain recommendation (CDR) reaches advanced and satisfying recommendation performance, but mostly neglects preserving privacy. To fill this gap, we propose a privacy-preserving generative cross-domain recommendation (PPGenCDR) framework for PPCDR. PPGenCDR includes two main modules, i.e., stable privacy-preserving generator module, and robust cross-domain recommendation module. Specifically, the former isolates data from different domains with a generative adversarial network (GAN) based model, which stably estimates the distribution of private data in the source domain with Renyi differential privacy (RDP) technique. Then the latter aims to robustly leverage the perturbed but effective knowledge from the source domain with the raw data in target domain to improve recommendation performance. Three key modules, i.e., (1) selective privacy preserver, (2) GAN stabilizer, and (3) robustness conductor, guarantee the cost-effective trade-off between utility and privacy, the stability of GAN when using RDP, and the robustness of leveraging transferable knowledge accordingly. The extensive empirical studies on Douban and Amazon datasets demonstrate that PPGenCDR significantly outperforms the state-of-the-art recommendation models while preserving privacy.\n        \u25b3 Less\n      ",
    "title": "PPGenCDR: A Stable and Robust Framework for Privacy-Preserving Cross-Domain Recommendation",
    "date": "10 May, 2023",
    "authors": [
      "Xinting Liao",
      " Weiming Liu",
      " Xiaolin Zheng",
      " Binhui Yao",
      " Chaochao Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06640",
    "paper_id": "2305.06640",
    "abstract": "\n        Speaker protection algorithm is to leverage the playback signal properties to prevent over excursion while maintaining maximum loudness, especially for the mobile phone with tiny loudspeakers. This paper proposes efficient DL solutions to accurately model and predict the nonlinear excursion, which is challenging for conventional solutions. Firstly, we build the experiment and pre-processing pipeline, where the feedback current and voltage are sampled as input, and laser is employed to measure the excursion as ground truth. Secondly, one FFTNet model is proposed to explore the dominant low-frequency and other unknown harmonics, and compares to a baseline ConvNet model. In addition, BN re-estimation is designed to explore the online adaptation; and INT8 quantization based on AI Model efficiency toolkit (AIMET\\footnote{AIMET is a product of Qualcomm Innovation Center, Inc.}) is applied to further reduce the complexity. The proposed algorithm is verified in two speakers and 3 typical deployment scenarios, and >>99\\% residual DC is less than 0.1 mm, much better than traditional solutions.\n        \u25b3 Less\n      ",
    "title": "Speaker Diaphragm Excursion Prediction: deep attention and online adaptation",
    "date": "10 May, 2023",
    "authors": [
      "Yuwei Ren",
      " Matt Zivney",
      " Yin Huang",
      " Eddie Choy",
      " Chirag Patel",
      " Hao Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.07182",
    "paper_id": "2204.07182",
    "abstract": "\n        Recent advances in Artificial Intelligence (AI) have leveraged promising results in solving complex problems in the area of Natural Language Processing (NLP), being an important tool to help in the expeditious resolution of judicial proceedings in the legal area. In this context, this work targets the problem of detecting the degree of similarity between judicial documents that can be achieved in the inference group, by applying six NLP techniques based on the transformers architecture to a case study of legal proceedings in the Brazilian judicial system. The NLP transformer-based models, namely BERT, GPT-2 and RoBERTa, were pre-trained using a general purpose corpora of the Brazilian Portuguese language, and then were fine-tuned and specialised for the legal sector using 210,000 legal proceedings. Vector representations of each legal document were calculated based on their embeddings, which were used to cluster the lawsuits, calculating the quality of each model based on the cosine of the distance between the elements of the group to its centroid. We noticed that models based on transformers presented better performance when compared to previous traditional NLP techniques, with the RoBERTa model specialised for the Brazilian Portuguese language presenting the best results. This methodology can be also applied to other case studies for different languages, making it possible to advance in the current state of the art in the area of NLP applied to the legal sector.\n        \u25b3 Less\n      ",
    "title": "Analysing similarities between legal court documents using natural language processing approaches based on Transformers",
    "date": "11 May, 2023",
    "authors": [
      "Raphael Souza de Oliveira",
      " Erick Giovani Sperandio Nascimento"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02993",
    "paper_id": "2305.02993",
    "abstract": "\n        This paper describes the results of SemEval 2023 task 7 -- Multi-Evidence Natural Language Inference for Clinical Trial Data (NLI4CT) -- consisting of 2 tasks, a Natural Language Inference (NLI) task, and an evidence selection task on clinical trial data. The proposed challenges require multi-hop biomedical and numerical reasoning, which are of significant importance to the development of systems capable of large-scale interpretation and retrieval of medical evidence, to provide personalized evidence-based care.\n  Task 1, the entailment task, received 643 submissions from 40 participants, and Task 2, the evidence selection task, received 364 submissions from 23 participants. The tasks are challenging, with the majority of submitted systems failing to significantly outperform the majority class baseline on the entailment task, and we observe significantly better performance on the evidence selection task than on the entailment task. Increasing the number of model parameters leads to a direct increase in performance, far more significant than the effect of biomedical pre-training. Future works could explore the limitations of large models for generalization and numerical inference, and investigate methods to augment clinical datasets to allow for more rigorous testing and to facilitate fine-tuning.\n  We envisage that the dataset, models, and results of this task will be useful to the biomedical NLI and evidence retrieval communities. The dataset, competition leaderboard, and website are publicly available.\n        \u25b3 Less\n      ",
    "title": "SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data",
    "date": "11 May, 2023",
    "authors": [
      "Ma\u00ebl Jullien",
      " Marco Valentino",
      " Hannah Frost",
      " Paul O'Regan",
      " Donal Landers",
      " Andr\u00e9 Freitas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06672",
    "paper_id": "2305.06672",
    "abstract": "\n        As of today, 5G is rolling out across the world, but academia and industry have shifted their attention to the sixth generation (6G) cellular technology for a full-digitalized, intelligent society in 2030 and beyond. 6G demands far more bandwidth to support extreme performance, exacerbating the problem of spectrum shortage in mobile communications. In this context, this paper proposes a novel concept coined Full-Spectrum Wireless Communications (FSWC). It makes use of all communication-feasible spectral resources over the whole electromagnetic (EW) spectrum, from microwave, millimeter wave, terahertz (THz), infrared light, visible light, to ultraviolet light. FSWC not only provides sufficient bandwidth but also enables new paradigms taking advantage of peculiarities on different EW bands. This paper will define FSWC, justify its necessity for 6G, and then discuss the opportunities and challenges of exploiting THz and optical bands.\n        \u25b3 Less\n      ",
    "title": "Full-Spectrum Wireless Communications for 6G and Beyond: From Microwave, Millimeter-Wave, Terahertz to Lightwave",
    "date": "11 May, 2023",
    "authors": [
      "Wei Jiang",
      " Hans D. Schotten"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03380",
    "paper_id": "2305.03380",
    "abstract": "\n        Large Language Models (LLMs) have revolutionized natural language processing by generating human-like text and images from textual input. However, their potential to generate complex 2D/3D visualizations has been largely unexplored. We report initial experiments showing that LLMs can generate 2D/3D visualizations that may be used for legal visualization. Further research is needed for complex 2D visualizations and 3D scenes. LLMs can become a powerful tool for many industries and applications, generating complex visualizations with minimal training.\n        \u25b3 Less\n      ",
    "title": "Visualization in the Era of Artificial Intelligence: Experiments for Creating Structural Visualizations by Prompting Large Language Models",
    "date": "11 May, 2023",
    "authors": [
      "Hans-Georg Fill",
      " Fabian Muff"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.16519",
    "paper_id": "2303.16519",
    "abstract": "\n        Several approaches have been developed that generate embeddings for Description Logic ontologies and use these embeddings in machine learning. One approach of generating ontologies embeddings is by first embedding the ontologies into a graph structure, i.e., introducing a set of nodes and edges for named entities and logical axioms, and then applying a graph embedding to embed the graph in Rn\\mathbb{R}^n. Methods that embed ontologies in graphs (graph projections) have different formal properties related to the type of axioms they can utilize, whether the projections are invertible or not, and whether they can be applied to asserted axioms or their deductive closure. We analyze, qualitatively and quantitatively, several graph projection methods that have been used to embed ontologies, and we demonstrate the effect of the properties of graph projections on the performance of predicting axioms from ontology embeddings. We find that there are substantial differences between different projection methods, and both the projection of axioms into nodes and edges as well ontological choices in representing knowledge will impact the success of using ontology embeddings to predict axioms.\n        \u25b3 Less\n      ",
    "title": "From axioms over graphs to vectors, and back again: evaluating the properties of graph-based ontology embeddings",
    "date": "11 May, 2023",
    "authors": [
      "Fernando Zhapa-Camacho",
      " Robert Hoehndorf"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03888",
    "paper_id": "2305.03888",
    "abstract": "\n        In recent years, on-device deep learning has gained attention as a means of developing affordable deep learning applications for mobile devices. However, on-device models are constrained by limited energy and computation resources. In the mean time, a poisoning attack known as sponge poisoning has been developed.This attack involves feeding the model with poisoned examples to increase the energy consumption during inference. As previous work is focusing on server hardware accelerators, in this work, we extend the sponge poisoning attack to an on-device scenario to evaluate the vulnerability of mobile device processors. We present an on-device sponge poisoning attack pipeline to simulate the streaming and consistent inference scenario to bridge the knowledge gap in the on-device setting. Our exclusive experimental analysis with processors and on-device networks shows that sponge poisoning attacks can effectively pollute the modern processor with its built-in accelerator. We analyze the impact of different factors in the sponge poisoning algorithm and highlight the need for improved defense mechanisms to prevent such attacks on on-device deep learning applications.\n        \u25b3 Less\n      ",
    "title": "Energy-Latency Attacks to On-Device Neural Networks via Sponge Poisoning",
    "date": "11 May, 2023",
    "authors": [
      "Zijian Wang",
      " Shuo Huang",
      " Yujin Huang",
      " Helei Cui"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06683",
    "paper_id": "2305.06683",
    "abstract": "\n        This paper introduces a novel worker selection algorithm, enhancing annotation quality and reducing costs in challenging span-based sequence labeling tasks in Natural Language Processing (NLP). Unlike previous studies targeting simpler tasks, this study contends with the complexities of label interdependencies in sequence labeling tasks. The proposed algorithm utilizes a Combinatorial Multi-Armed Bandit (CMAB) approach for worker selection. The challenge of dealing with imbalanced and small-scale datasets, which hinders offline simulation of worker selection, is tackled using an innovative data augmentation method termed shifting, expanding, and shrinking (SES). The SES method is designed specifically for sequence labeling tasks. Rigorous testing on CoNLL 2003 NER and Chinese OEI datasets showcased the algorithm's efficiency, with an increase in F1 score up to 100.04% of the expert-only baseline, alongside cost savings up to 65.97%. The paper also encompasses a dataset-independent test emulating annotation evaluation through a Bernoulli distribution, which still led to an impressive 97.56% F1 score of the expert baseline and 59.88% cost savings. This research addresses and overcomes numerous obstacles in worker selection for complex NLP tasks.\n        \u25b3 Less\n      ",
    "title": "Cost-efficient Crowdsourcing for Span-based Sequence Labeling: Worker Selection and Data Augmentation",
    "date": "11 May, 2023",
    "authors": [
      "Yujie Wang",
      " Chao Huang",
      " Liner Yang",
      " Zhixuan Fang",
      " Yaping Huang",
      " Yang Liu",
      " Erhong Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.12701",
    "paper_id": "2211.12701",
    "abstract": "\n        Continual learning (CL) is a learning paradigm that emulates the human capability of learning and accumulating knowledge continually without forgetting the previously learned knowledge and also transferring the learned knowledge to help learn new tasks better. This survey presents a comprehensive review and analysis of the recent progress of CL in NLP, which has significant differences from CL in computer vision and machine learning. It covers (1) all CL settings with a taxonomy of existing techniques; (2) catastrophic forgetting (CF) prevention, (3) knowledge transfer (KT), which is particularly important for NLP tasks; and (4) some theory and the hidden challenge of inter-task class separation (ICS). (1), (3) and (4) have not been included in the existing survey. Finally, a list of future directions is discussed.\n        \u25b3 Less\n      ",
    "title": "Continual Learning of Natural Language Processing Tasks: A Survey",
    "date": "11 May, 2023",
    "authors": [
      "Zixuan Ke",
      " Bing Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03411",
    "paper_id": "2305.03411",
    "abstract": "\n        As Autonomous Systems (AS) become more ubiquitous in society, more responsible for our safety and our interaction with them more frequent, it is essential that they are trustworthy. Assessing the trustworthiness of AS is a mandatory challenge for the verification and development community. This will require appropriate standards and suitable metrics that may serve to objectively and comparatively judge trustworthiness of AS across the broad range of current and future applications. The meta-expression `trustworthiness' is examined in the context of AS capturing the relevant qualities that comprise this term in the literature. Recent developments in standards and frameworks that support assurance of autonomous systems are reviewed. A list of key challenges are identified for the community and we present an outline of a process that can be used as a trustworthiness assessment framework for AS.\n        \u25b3 Less\n      ",
    "title": "Assessing Trustworthiness of Autonomous Systems",
    "date": "11 May, 2023",
    "authors": [
      "Gregory Chance",
      " Dhaminda B. Abeywickrama",
      " Beckett LeClair",
      " Owen Kerr",
      " Kerstin Eder"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03963",
    "paper_id": "2305.03963",
    "abstract": "\n        The increasing popularity of deep learning (DL) models and the advantages of computing, including low latency and bandwidth savings on smartphones, have led to the emergence of intelligent mobile applications, also known as DL apps, in recent years. However, this technological development has also given rise to several security concerns, including adversarial examples, model stealing, and data poisoning issues. Existing works on attacks and countermeasures for on-device DL models have primarily focused on the models themselves. However, scant attention has been paid to the impact of data processing disturbance on the model inference. This knowledge disparity highlights the need for additional research to fully comprehend and address security issues related to data processing for on-device models. In this paper, we introduce a data processing-based attacks against real-world DL apps. In particular, our attack could influence the performance and latency of the model without affecting the operation of a DL app. To demonstrate the effectiveness of our attack, we carry out an empirical study on 517 real-world DL apps collected from Google Play. Among 320 apps utilizing MLkit, we find that 81.56\\% of them can be successfully attacked.\n  The results emphasize the importance of DL app developers being aware of and taking actions to secure on-device models from the perspective of data processing.\n        \u25b3 Less\n      ",
    "title": "Beyond the Model: Data Pre-processing Attack to Deep Learning Models in Android Apps",
    "date": "11 May, 2023",
    "authors": [
      "Ye Sang",
      " Yujin Huang",
      " Shuo Huang",
      " Helei Cui"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06703",
    "paper_id": "2305.06703",
    "abstract": "\n        Time-to-event modelling, known as survival analysis, differs from standard regression as it addresses censoring in patients who do not experience the event of interest. Despite competitive performances in tackling this problem, machine learning methods often ignore other competing risks that preclude the event of interest. This practice biases the survival estimation. Extensions to address this challenge often rely on parametric assumptions or numerical estimations leading to sub-optimal survival approximations. This paper leverages constrained monotonic neural networks to model each competing survival distribution. This modelling choice ensures the exact likelihood maximisation at a reduced computational cost by using automatic differentiation. The effectiveness of the solution is demonstrated on one synthetic and three medical datasets. Finally, we discuss the implications of considering competing risks when developing risk scores for medical practice.\n        \u25b3 Less\n      ",
    "title": "Neural Fine-Gray: Monotonic neural networks for competing risks",
    "date": "11 May, 2023",
    "authors": [
      "Vincent Jeanselme",
      " Chang Ho Yoon",
      " Brian Tom",
      " Jessica Barrett"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06707",
    "paper_id": "2305.06707",
    "abstract": "\n        Rutting of asphalt pavements is a crucial design criterion in various pavement design guides. A good road transportation base can provide security for the transportation of oil and gas in road transportation. This study attempts to develop a robust artificial intelligence model to estimate different asphalt pavements' rutting depth clips, temperature, and load axes as primary characteristics. The experiment data were obtained from 19 asphalt pavements with different crude oil sources on a 2.038 km long full-scale field accelerated pavement test track (RIOHTrack, Road Track Institute) in Tongzhou, Beijing. In addition, this paper also proposes to build complex networks with different pavement rutting depths through complex network methods and the Louvain algorithm for community detection. The most critical structural elements can be selected from different asphalt pavement rutting data, and similar structural elements can be found. An extreme learning machine algorithm with residual correction (RELM) is designed and optimized using an independent adaptive particle swarm algorithm. The experimental results of the proposed method are compared with several classical machine learning algorithms, with predictions of Average Root Mean Squared Error, Average Mean Absolute Error, and Average Mean Absolute Percentage Error for 19 asphalt pavements reaching 1.742, 1.363, and 1.94\\% respectively. The experiments demonstrate that the RELM algorithm has an advantage over classical machine learning methods in dealing with non-linear problems in road engineering. Notably, the method ensures the adaptation of the simulated environment to different levels of abstraction through the cognitive analysis of the production environment parameters.\n        \u25b3 Less\n      ",
    "title": "A data-driven rutting depth short-time prediction model with metaheuristic optimization for asphalt pavements based on RIOHTrack",
    "date": "11 May, 2023",
    "authors": [
      "Zhuoxuan Li",
      " Iakov Korovin",
      " Xinli Shi",
      " Sergey Gorbachev",
      " Nadezhda Gorbacheva",
      " Wei Huang",
      " Jinde Cao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2112.03740",
    "paper_id": "2112.03740",
    "abstract": "\n        Recent works indicate that convolutional neural networks (CNN) need large receptive fields (RF) to compete with visual transformers and their attention mechanism. In CNNs, RFs can simply be enlarged by increasing the convolution kernel sizes. Yet the number of trainable parameters, which scales quadratically with the kernel's size in the 2D case, rapidly becomes prohibitive, and the training is notoriously difficult. This paper presents a new method to increase the RF size without increasing the number of parameters. The dilated convolution (DC) has already been proposed for the same purpose. DC can be seen as a convolution with a kernel that contains only a few non-zero elements placed on a regular grid. Here we present a new version of the DC in which the spacings between the non-zero elements, or equivalently their positions, are no longer fixed but learnable via backpropagation thanks to an interpolation technique. We call this method \"Dilated Convolution with Learnable Spacings\" (DCLS) and generalize it to the n-dimensional convolution case. However, our main focus here will be on the 2D case. We first tried our approach on ResNet50: we drop-in replaced the standard convolutions with DCLS ones, which increased the accuracy of ImageNet1k classification at iso-parameters, but at the expense of the throughput. Next, we used the recent ConvNeXt state-of-the-art convolutional architecture and drop-in replaced the depthwise convolutions with DCLS ones. This not only increased the accuracy of ImageNet1k classification but also of typical downstream and robustness tasks, again at iso-parameters but this time with negligible cost on throughput, as ConvNeXt uses separable convolutions. Conversely, classic DC led to poor performance with both ResNet50 and ConvNeXt. The code of the method is available at: https://github.com/K-H-Ismail/Dilated-Convolution-with-Learnable-Spacings-PyTorch.\n        \u25b3 Less\n      ",
    "title": "Dilated convolution with learnable spacings",
    "date": "11 May, 2023",
    "authors": [
      "Ismail Khalfaoui-Hassani",
      " Thomas Pellegrini",
      " Timoth\u00e9e Masquelier"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.07697",
    "paper_id": "2210.07697",
    "abstract": "\n        Multi-task learning based video anomaly detection methods combine multiple proxy tasks in different branches to detect video anomalies in different situations. Most existing methods either do not combine complementary tasks to effectively cover all motion patterns, or the class of the objects is not explicitly considered. To address the aforementioned shortcomings, we propose a novel multi-task learning based method that combines complementary proxy tasks to better consider the motion and appearance features. We combine the semantic segmentation and future frame prediction tasks in a single branch to learn the object class and consistent motion patterns, and to detect respective anomalies simultaneously. In the second branch, we added several attention mechanisms to detect motion anomalies with attention to object parts, the direction of motion, and the distance of the objects from the camera. Our qualitative results show that the proposed method considers the object class effectively and learns motion with attention to the aforementioned important factors which results in a precise motion modeling and a better motion anomaly detection. Additionally, quantitative results show the superiority of our method compared with state-of-the-art methods.\n        \u25b3 Less\n      ",
    "title": "Multi-Task Learning based Video Anomaly Detection with Attention",
    "date": "11 May, 2023",
    "authors": [
      "Mohammad Baradaran",
      " Robert Bergevin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.14977",
    "paper_id": "2210.14977",
    "abstract": "\n        Speech emotion recognition (SER) has been a popular research topic in human-computer interaction (HCI). As edge devices are rapidly springing up, applying SER to edge devices is promising for a huge number of HCI applications. Although deep learning has been investigated to improve the performance of SER by training complex models, the memory space and computational capability of edge devices represents a constraint for embedding deep learning models. We propose a neural structured learning (NSL) framework through building synthesized graphs. An SER model is trained on a source dataset and used to build graphs on a target dataset. A relatively lightweight model is then trained with the speech samples and graphs together as the input. Our experiments demonstrate that training a lightweight SER model on the target dataset with speech samples and graphs can not only produce small SER models, but also enhance the model performance compared to models with speech samples only and those using classic transfer learning strategies.\n        \u25b3 Less\n      ",
    "title": "Knowledge Transfer For On-Device Speech Emotion Recognition with Neural Structured Learning",
    "date": "11 May, 2023",
    "authors": [
      "Yi Chang",
      " Zhao Ren",
      " Thanh Tam Nguyen",
      " Kun Qian",
      " Bj\u00f6rn W. Schuller"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06819",
    "paper_id": "2305.06819",
    "abstract": "\n        In most major cities and urban areas, residents form homogeneous neighborhoods along ethnic or socioeconomic lines. This phenomenon is widely known as residential segregation and has been studied extensively. Fifty years ago, Schelling proposed a landmark model that explains residential segregation in an elegant agent-based way. A recent stream of papers analyzed Schelling's model using game-theoretic approaches. However, all these works considered models with a given number of discrete types modeling different ethnic groups.\n  We focus on segregation caused by non-categorical attributes, such as household income or position in a political left-right spectrum. For this, we consider agent types that can be represented as real numbers. This opens up a great variety of reasonable models and, as a proof of concept, we focus on several natural candidates. In particular, we consider agents that evaluate their location by the average type-difference or the maximum type-difference to their neighbors, or by having a certain tolerance range for type-values of neighboring agents. We study the existence and computation of equilibria and provide bounds on the Price of Anarchy and Stability. Also, we present simulation results that compare our models and shed light on the obtained equilibria for our variants.\n        \u25b3 Less\n      ",
    "title": "Schelling Games with Continuous Types",
    "date": "11 May, 2023",
    "authors": [
      "Davide Bil\u00f2",
      " Vittorio Bil\u00f2",
      " Michelle D\u00f6ring",
      " Pascal Lenzner",
      " Louise Molitor",
      " Jonas Schmidt"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.01622",
    "paper_id": "2305.01622",
    "abstract": "\n        There is extensive literature on perceiving road structures by fusing various sensor inputs such as lidar point clouds and camera images using deep neural nets. Leveraging the latest advance of neural architects (such as transformers) and bird-eye-view (BEV) representation, the road cognition accuracy keeps improving. However, how to cognize the ``road'' for automated vehicles where there is no well-defined ``roads'' remains an open problem. For example, how to find paths inside intersections without HD maps is hard since there is neither an explicit definition for ``roads'' nor explicit features such as lane markings. The idea of this paper comes from a proverb: it becomes a way when people walk on it. Although there are no ``roads'' from sensor readings, there are ``roads'' from tracks of other vehicles. In this paper, we propose FlowMap, a path generation framework for automated vehicles based on traffic flows. FlowMap is built by extending our previous work RoadMap, a light-weight semantic map, with an additional traffic flow layer. A path generation algorithm on traffic flow fields (TFFs) is proposed to generate human-like paths. The proposed framework is validated using real-world driving data and is amenable to generating paths for super complicated intersections without using HD maps.\n        \u25b3 Less\n      ",
    "title": "FlowMap: Path Generation for Automated Vehicles in Open Space Using Traffic Flow",
    "date": "11 May, 2023",
    "authors": [
      "Wenchao Ding",
      " Jieru Zhao",
      " Yubin Chu",
      " Haihui Huang",
      " Tong Qin",
      " Chunjing Xu",
      " Yuxiang Guan",
      " Zhongxue Gan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06842",
    "paper_id": "2305.06842",
    "abstract": "\n        Human communication is the vocal and non verbal signal to communicate with others. Human expression is a significant biometric object in picture and record databases of surveillance systems. Face appreciation has a serious role in biometric methods and is good-looking for plentiful applications, including visual scrutiny and security. Facial expressions are a form of nonverbal communication; recognizing them helps improve the human machine interaction. This paper proposes an idea for face and enlightenment invariant credit of facial expressions by the images. In order on, the person's face can be computed. Face expression is used in CNN classifier to categorize the acquired picture into different emotion categories. It is a deep, feed-forward artificial neural network. Outcome surpasses human presentation and shows poses alternate performance. Varying lighting conditions can influence the fitting process and reduce recognition precision. Results illustrate that dependable facial appearance credited with changing lighting conditions for separating reasonable facial terminology display emotions is an efficient representation of clean and assorted moving expressions. This process can also manage the proportions of dissimilar basic affecting expressions of those mixed jointly to produce sensible emotional facial expressions. Our system contains a pre-defined data set, which was residential by a statistics scientist and includes all pure and varied expressions. On average, a data set has achieved 92.4% exact validation of the expressions synthesized by our technique. These facial expressions are compared through the pre-defined data-position inside our system. If it recognizes the person in an abnormal condition, an alert will be passed to the nearby hospital/doctor seeing that a message.\n        \u25b3 Less\n      ",
    "title": "Emotion Recognition for Challenged People Facial Appearance in Social using Neural Network",
    "date": "11 May, 2023",
    "authors": [
      "P. Deivendran",
      " P. Suresh Babu",
      " G. Malathi",
      " K. Anbazhagan",
      " R. Senthil Kumar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06897",
    "paper_id": "2305.06897",
    "abstract": "\n        African languages have far less in-language content available digitally, making it challenging for question answering systems to satisfy the information needs of users. Cross-lingual open-retrieval question answering (XOR QA) systems -- those that retrieve answer content from other languages while serving people in their native language -- offer a means of filling this gap. To this end, we create AfriQA, the first cross-lingual QA dataset with a focus on African languages. AfriQA includes 12,000+ XOR QA examples across 10 African languages. While previous datasets have focused primarily on languages where cross-lingual QA augments coverage from the target language, AfriQA focuses on languages where cross-lingual answer content is the only high-coverage source of answer content. Because of this, we argue that African languages are one of the most important and realistic use cases for XOR QA. Our experiments demonstrate the poor performance of automatic translation and multilingual retrieval methods. Overall, AfriQA proves challenging for state-of-the-art QA models. We hope that the dataset enables the development of more equitable QA technology.\n        \u25b3 Less\n      ",
    "title": "AfriQA: Cross-lingual Open-Retrieval Question Answering for African Languages",
    "date": "11 May, 2023",
    "authors": [
      "Odunayo Ogundepo",
      " Tajuddeen R. Gwadabe",
      " Clara E. Rivera",
      " Jonathan H. Clark",
      " Sebastian Ruder",
      " David Ifeoluwa Adelani",
      " Bonaventure F. P. Dossou",
      " Abdou Aziz DIOP",
      " Claytone Sikasote",
      " Gilles Hacheme",
      " Happy Buzaaba",
      " Ignatius Ezeani",
      " Rooweither Mabuya",
      " Salomey Osei",
      " Chris Emezue",
      " Albert Njoroge Kahira",
      " Shamsuddeen H. Muhammad",
      " Akintunde Oladipo",
      " Abraham Toluwase Owodunni",
      " Atnafu Lambebo Tonja",
      " Iyanuoluwa Shode",
      " Akari Asai",
      " Tunde Oluwaseyi Ajayi",
      " Clemencia Siro",
      " Steven Arthur ",
      " et al. (27 additional authors not shown)"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.14323",
    "paper_id": "2304.14323",
    "abstract": "\n        Standpoint EL is a multi-modal extension of the popular description logic EL that allows for the integrated representation of domain knowledge relative to diverse standpoints or perspectives. Advantageously, its satisfiability problem has recently been shown to be in PTime, making it a promising framework for large-scale knowledge integration.\n  In this paper, we show that we can further push the expressivity of this formalism, arriving at an extended logic, called Standpoint EL+, which allows for axiom negation, role chain axioms, self-loops, and other features, while maintaining tractability. This is achieved by designing a satisfiability-checking deduction calculus, which at the same time addresses the need for practical algorithms. We demonstrate the feasibility of our calculus by presenting a prototypical Datalog implementation of its deduction rules.\n        \u25b3 Less\n      ",
    "title": "Pushing the Boundaries of Tractable Multiperspective Reasoning: A Deduction Calculus for Standpoint EL+",
    "date": "11 May, 2023",
    "authors": [
      "Luc\u00eda G\u00f3mez \u00c1lvarez",
      " Sebastian Rudolph",
      " Hannes Strass"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06951",
    "paper_id": "2305.06951",
    "abstract": "\n        Constraint-based applications attempt to identify a solution that meets all defined user requirements. If the requirements are inconsistent with the underlying constraint set, algorithms that compute diagnoses for inconsistent constraints should be implemented to help users resolve the \"no solution could be found\" dilemma. FastDiag is a typical direct diagnosis algorithm that supports diagnosis calculation without predetermining conflicts. However, this approach faces runtime performance issues, especially when analyzing complex and large-scale knowledge bases. In this paper, we propose a novel algorithm, so-called FastDiagP, which is based on the idea of speculative programming. This algorithm extends FastDiag by integrating a parallelization mechanism that anticipates and pre-calculates consistency checks requested by FastDiag. This mechanism helps to provide consistency checks with fast answers and boosts the algorithm's runtime performance. The performance improvements of our proposed algorithm have been shown through empirical results using the Linux-2.6.3.33 configuration knowledge base.\n        \u25b3 Less\n      ",
    "title": "FastDiagP: An Algorithm for Parallelized Direct Diagnosis",
    "date": "11 May, 2023",
    "authors": [
      "Viet-Man Le",
      " Cristian Vidal Silva",
      " Alexander Felfernig",
      " David Benavides",
      " Jos\u00e9 Galindo",
      " Thi Ngoc Trang Tran"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06967",
    "paper_id": "2305.06967",
    "abstract": "\n        AI systems are not intrinsically neutral and biases trickle in any type of technological tool. In particular when dealing with people, AI algorithms reflect technical errors originating with mislabeled data. As they feed wrong and discriminatory classifications, perpetuating structural racism and marginalization, these systems are not systematically guarded against bias. In this article we consider the problem of bias in AI systems from the point of view of Information Quality dimensions. We illustrate potential improvements of a bias mitigation tool in gender classification errors, referring to two typically difficult contexts: the classification of non-binary individuals and the classification of transgender individuals. The identification of data quality dimensions to implement in bias mitigation tool may help achieve more fairness. Hence, we propose to consider this issue in terms of completeness, consistency, timeliness and reliability, and offer some theoretical results.\n        \u25b3 Less\n      ",
    "title": "Data quality dimensions for fair AI",
    "date": "11 May, 2023",
    "authors": [
      "Camilla Quaresmini",
      " Giuseppe Primiero"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06978",
    "paper_id": "2305.06978",
    "abstract": "\n        Domain shift and label scarcity heavily limit deep learning applications to various medical image analysis tasks. Unsupervised domain adaptation (UDA) techniques have recently achieved promising cross-modality medical image segmentation by transferring knowledge from a label-rich source domain to an unlabeled target domain. However, it is also difficult to collect annotations from the source domain in many clinical applications, rendering most prior works suboptimal with the label-scarce source domain, particularly for few-shot scenarios, where only a few source labels are accessible. To achieve efficient few-shot cross-modality segmentation, we propose a novel transformation-consistent meta-hallucination framework, meta-hallucinator, with the goal of learning to diversify data distributions and generate useful examples for enhancing cross-modality performance. In our framework, hallucination and segmentation models are jointly trained with the gradient-based meta-learning strategy to synthesize examples that lead to good segmentation performance on the target domain. To further facilitate data hallucination and cross-domain knowledge transfer, we develop a self-ensembling model with a hallucination-consistent property. Our meta-hallucinator can seamlessly collaborate with the meta-segmenter for learning to hallucinate with mutual benefits from a combined view of meta-learning and self-ensembling learning. Extensive studies on MM-WHS 2017 dataset for cross-modality cardiac segmentation demonstrate that our method performs favorably against various approaches by a lot in the few-shot UDA scenario.\n        \u25b3 Less\n      ",
    "title": "Meta-hallucinator: Towards Few-Shot Cross-Modality Cardiac Image Segmentation",
    "date": "11 May, 2023",
    "authors": [
      "Ziyuan Zhao",
      " Fangcheng Zhou",
      " Zeng Zeng",
      " Cuntai Guan",
      " S. Kevin Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06993",
    "paper_id": "2305.06993",
    "abstract": "\n        The Smatch metric is a popular method for evaluating graph distances, as is necessary, for instance, to assess the performance of semantic graph parsing systems. However, we observe some issues in the metric that jeopardize meaningful evaluation. E.g., opaque pre-processing choices can affect results, and current graph-alignment solvers do not provide us with upper-bounds. Without upper-bounds, however, fair evaluation is not guaranteed. Furthermore, adaptions of Smatch for extended tasks (e.g., fine-grained semantic similarity) are spread out, and lack a unifying framework.\n  For better inspection, we divide the metric into three modules: pre-processing, alignment, and scoring. Examining each module, we specify its goals and diagnose potential issues, for which we discuss and test mitigation strategies. For pre-processing, we show how to fully conform to annotation guidelines that allow structurally deviating but valid graphs. For safer and enhanced alignment, we show the feasibility of optimal alignment in a standard evaluation setup, and develop a lossless graph compression method that shrinks the search space and significantly increases efficiency. For improved scoring, we propose standardized and extended metric calculation of fine-grained sub-graph meaning aspects. Our code is available at https://github.com/flipz357/smatchpp\n        \u25b3 Less\n      ",
    "title": "SMATCH++: Standardized and Extended Evaluation of Semantic Graphs",
    "date": "11 May, 2023",
    "authors": [
      "Juri Opitz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07069",
    "paper_id": "2305.07069",
    "abstract": "\n        Modern cellular networks are multi-cell and use universal frequency reuse to maximize spectral efficiency. This results in high inter-cell interference. This problem is growing as cellular networks become three-dimensional with the adoption of unmanned aerial vehicles (UAVs). This is because the strength and number of interference links rapidly increase due to the line-of-sight channels in UAV communications. Existing interference management solutions need each transmitter to know the channel information of interfering signals, rendering them impractical due to excessive signaling overhead. In this paper, we propose leveraging deep reinforcement learning for interference management to tackle this shortcoming. In particular, we show that interference can still be effectively mitigated even without knowing its channel information. We then discuss novel approaches to scale the algorithms with linear/sublinear complexity and decentralize them using multi-agent reinforcement learning. By harnessing interference, the proposed solutions enable the continued growth of civilian UAVs.\n        \u25b3 Less\n      ",
    "title": "Deep Reinforcement Learning for Interference Management in UAV-based 3D Networks: Potentials and Challenges",
    "date": "11 May, 2023",
    "authors": [
      "Mojtaba Vaezi",
      " Xingqin Lin",
      " Hongliang Zhang",
      " Walid Saad",
      " H. Vincent Poor"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07095",
    "paper_id": "2305.07095",
    "abstract": "\n        Among the remarkable emergent capabilities of large language models (LMs) is free-text rationalization; beyond a certain scale, large LMs are capable of generating seemingly useful rationalizations, which in turn, can dramatically enhance their performances on leaderboards. This phenomenon raises a question: can machine generated rationales also be useful for humans, especially when lay humans try to answer questions based on those machine rationales? We observe that human utility of existing rationales is far from satisfactory, and expensive to estimate with human studies. Existing metrics like task performance of the LM generating the rationales, or similarity between generated and gold rationales are not good indicators of their human utility. While we observe that certain properties of rationales like conciseness and novelty are correlated with their human utility, estimating them without human involvement is challenging. We show that, by estimating a rationale's helpfulness in answering similar unseen instances, we can measure its human utility to a better extent. We also translate this finding into an automated score, GEN-U, that we propose, which can help improve LMs' ability to generate rationales with better human utility, while maintaining most of its task performance. Lastly, we release all code and collected data with this project.\n        \u25b3 Less\n      ",
    "title": "Are Machine Rationales (Not) Useful to Humans? Measuring and Improving Human Utility of Free-Text Rationales",
    "date": "11 May, 2023",
    "authors": [
      "Brihi Joshi",
      " Ziyi Liu",
      " Sahana Ramnath",
      " Aaron Chan",
      " Zhewei Tong",
      " Shaoliang Nie",
      " Qifan Wang",
      " Yejin Choi",
      " Xiang Ren"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07102",
    "paper_id": "2305.07102",
    "abstract": "\n        Fine-grained visual classification (FGVC) is a challenging computer vision problem, where the task is to automatically recognise objects from subordinate categories. One of its main difficulties is capturing the most discriminative inter-class variances among visually similar classes. Recently, methods with Vision Transformer (ViT) have demonstrated noticeable achievements in FGVC, generally by employing the self-attention mechanism with additional resource-consuming techniques to distinguish potentially discriminative regions while disregarding the rest. However, such approaches may struggle to effectively focus on truly discriminative regions due to only relying on the inherent self-attention mechanism, resulting in the classification token likely aggregating global information from less-important background patches. Moreover, due to the immense lack of the datapoints, classifiers may fail to find the most helpful inter-class distinguishing features, since other unrelated but distinctive background regions may be falsely recognised as being valuable. To this end, we introduce a simple yet effective Salient Mask-Guided Vision Transformer (SM-ViT), where the discriminability of the standard ViT`s attention maps is boosted through salient masking of potentially discriminative foreground regions. Extensive experiments demonstrate that with the standard training procedure our SM-ViT achieves state-of-the-art performance on popular FGVC benchmarks among existing ViT-based approaches while requiring fewer resources and lower input image resolution.\n        \u25b3 Less\n      ",
    "title": "Salient Mask-Guided Vision Transformer for Fine-Grained Classification",
    "date": "11 May, 2023",
    "authors": [
      "Dmitry Demidov",
      " Muhammad Hamza Sharif",
      " Aliakbar Abdurahimov",
      " Hisham Cholakkal",
      " Fahad Shahbaz Khan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.15589",
    "paper_id": "2211.15589",
    "abstract": "\n        Reinforcement Learning (RL) algorithms are known to scale poorly to environments with many available actions, requiring numerous samples to learn an optimal policy. The traditional approach of considering the same fixed action space in every possible state implies that the agent must understand, while also learning to maximize its reward, to ignore irrelevant actions such as \\textit{inapplicable actions}\\textit{inapplicable actions} (i.e. actions that have no effect on the environment when performed in a given state). Knowing this information can help reduce the sample complexity of RL algorithms by masking the inapplicable actions from the policy distribution to only explore actions relevant to finding an optimal policy. While this technique has been formalized for quite some time within the Automated Planning community with the concept of precondition in the STRIPS language, RL algorithms have never formally taken advantage of this information to prune the search space to explore. This is typically done in an ad-hoc manner with hand-crafted domain logic added to the RL algorithm. In this paper, we propose a more systematic approach to introduce this knowledge into the algorithm. We (i) standardize the way knowledge can be manually specified to the agent; and (ii) present a new framework to autonomously learn the partial action model encapsulating the precondition of an action jointly with the policy. We show experimentally that learning inapplicable actions greatly improves the sample efficiency of the algorithm by providing a reliable signal to mask out irrelevant actions. Moreover, we demonstrate that thanks to the transferability of the knowledge acquired, it can be reused in other tasks and domains to make the learning process more efficient.\n        \u25b3 Less\n      ",
    "title": "Inapplicable Actions Learning for Knowledge Transfer in Reinforcement Learning",
    "date": "11 May, 2023",
    "authors": [
      "Leo Ardon",
      " Alberto Pozanco",
      " Daniel Borrajo",
      " Sumitra Ganesh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07124",
    "paper_id": "2305.07124",
    "abstract": "\n        We investigate the difficulty of finding economically efficient solutions to coordination problems on graphs. Our work focuses on two forms of coordination problem: pure-coordination games and anti-coordination games. We consider three objectives in the context of simple binary-action polymatrix games: (i) maximizing welfare, (ii) maximizing potential, and (iii) finding a welfare-maximizing Nash equilibrium. We introduce an intermediate, new graph-partition problem, termed Maximum Weighted Digraph Partition, which is of independent interest, and we provide a complexity dichotomy for it. This dichotomy, among other results, provides as a corollary a dichotomy for Objective (i) for general binary-action polymatrix games. In addition, it reveals that the complexity of achieving these objectives varies depending on the form of the coordination problem. Specifically, Objectives (i) and (ii) can be efficiently solved in pure-coordination games, but are NP-hard in anti-coordination games. Finally, we show that objective (iii) is NP-hard even for simple non-trivial pure-coordination games.\n        \u25b3 Less\n      ",
    "title": "Complexity of Efficient Outcomes in Binary-Action Polymatrix Games with Implications for Coordination Problems",
    "date": "11 May, 2023",
    "authors": [
      "Argyrios Deligkas",
      " Eduard Eiben",
      " Gregory Gutin",
      " Philip R. Neary",
      " Anders Yeo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07135",
    "paper_id": "2305.07135",
    "abstract": "\n        Federated Learning (FL) is a privacy-preserving distributed machine learning approach geared towards applications in edge devices. However, the problem of designing custom neural architectures in federated environments is not tackled from the perspective of overall system efficiency. In this paper, we propose DC-NAS -- a divide-and-conquer approach that performs supernet-based Neural Architecture Search (NAS) in a federated system by systematically sampling the search space. We propose a novel diversified sampling strategy that balances exploration and exploitation of the search space by initially maximizing the distance between the samples and progressively shrinking this distance as the training progresses. We then perform channel pruning to reduce the training complexity at the devices further. We show that our approach outperforms several sampling strategies including Hadamard sampling, where the samples are maximally separated. We evaluate our method on the CIFAR10, CIFAR100, EMNIST, and TinyImagenet benchmarks and show a comprehensive analysis of different aspects of federated learning such as scalability, and non-IID data. DC-NAS achieves near iso-accuracy as compared to full-scale federated NAS with 50% fewer resources.\n        \u25b3 Less\n      ",
    "title": "Divide-and-Conquer the NAS puzzle in Resource Constrained Federated Learning Systems",
    "date": "11 May, 2023",
    "authors": [
      "Yeshwanth Venkatesha",
      " Youngeun Kim",
      " Hyoungseob Park",
      " Priyadarshini Panda"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10436",
    "paper_id": "2305.10436",
    "abstract": "\n        In second language vocabulary learning, existing works have primarily focused on either the learning interface or scheduling personalized retrieval practices to maximize memory retention. However, the learning content, i.e., the information presented on flashcards, has mostly remained constant. Keyword mnemonic is a notable learning strategy that relates new vocabulary to existing knowledge by building an acoustic and imagery link using a keyword that sounds alike. Beyond that, producing verbal and visual cues associated with the keyword to facilitate building these links requires a manual process and is not scalable. In this paper, we explore an opportunity to use large language models to automatically generate verbal and visual cues for keyword mnemonics. Our approach, an end-to-end pipeline for auto-generating verbal and visual cues, can automatically generate highly memorable cues. We investigate the effectiveness of our approach via a human participant experiment by comparing it with manually generated cues.\n        \u25b3 Less\n      ",
    "title": "SmartPhone: Exploring Keyword Mnemonic with Auto-generated Verbal and Visual Cues",
    "date": "11 May, 2023",
    "authors": [
      "Jaewook Lee",
      " Andrew Lan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07141",
    "paper_id": "2305.07141",
    "abstract": "\n        The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture.\n  In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around \"concept groups\" -- sets of problems that focus on specific concepts and that are vary in complexity and level of abstraction. We report results on testing humans on this benchmark as well as three machine solvers: the top two programs from a 2021 ARC competition and OpenAI's GPT-4. Our results show that humans substantially outperform the machine solvers on this benchmark, showing abilities to abstract and generalize concepts that are not yet captured by AI systems. We believe that this benchmark will spur improvements in the development of AI systems for conceptual abstraction and in the effective evaluation of such systems.\n        \u25b3 Less\n      ",
    "title": "The ConceptARC Benchmark: Evaluating Understanding and Generalization in the ARC Domain",
    "date": "11 May, 2023",
    "authors": [
      "Arseny Moskvichev",
      " Victor Vikram Odouard",
      " Melanie Mitchell"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10437",
    "paper_id": "2305.10437",
    "abstract": "\n        The Internet of Things is transforming our society, providing new services that improve the quality of life and resource management. These applications are based on ubiquitous networks of multiple distributed devices, with limited computing resources and power, capable of collecting and storing data from heterogeneous sources in real-time. To avoid network saturation and high delays, new architectures such as fog computing are emerging to bring computing infrastructure closer to data sources. Additionally, new data centers are needed to provide real-time Big Data and data analytics capabilities at the edge of the network, where energy efficiency needs to be considered to ensure a sustainable and effective deployment in areas of human activity. In this research, we present an IoT model based on the principles of Model-Based Systems Engineering defined using the Discrete Event System Specification formalism. The provided mathematical formalism covers the description of the entire architecture, from IoT devices to the processing units in edge data centers. Our work includes the location-awareness of user equipment, network, and computing infrastructures to optimize federated resource management in terms of delay and power consumption. We present an effective framework to assist the dimensioning and the dynamic operation of IoT data stream analytics applications, demonstrating our contributions through a driving assistance use case based on real traces and data.\n        \u25b3 Less\n      ",
    "title": "Bringing AI to the edge: A formal M&S specification to deploy effective IoT architectures",
    "date": "11 May, 2023",
    "authors": [
      "Rom\u00e1n C\u00e1rdenas",
      " Patricia Arroba",
      " Jos\u00e9 L. Risco-Mart\u00edn"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2202.07125",
    "paper_id": "2202.07125",
    "abstract": "\n        Transformers have achieved superior performances in many tasks in natural language processing and computer vision, which also triggered great interest in the time series community. Among multiple advantages of Transformers, the ability to capture long-range dependencies and interactions is especially attractive for time series modeling, leading to exciting progress in various time series applications. In this paper, we systematically review Transformer schemes for time series modeling by highlighting their strengths as well as limitations. In particular, we examine the development of time series Transformers in two perspectives. From the perspective of network structure, we summarize the adaptations and modifications that have been made to Transformers in order to accommodate the challenges in time series analysis. From the perspective of applications, we categorize time series Transformers based on common tasks including forecasting, anomaly detection, and classification. Empirically, we perform robust analysis, model size analysis, and seasonal-trend decomposition analysis to study how Transformers perform in time series. Finally, we discuss and suggest future directions to provide useful research guidance. To the best of our knowledge, this paper is the first work to comprehensively and systematically summarize the recent advances of Transformers for modeling time series data. We hope this survey will ignite further research interests in time series Transformers.\n        \u25b3 Less\n      ",
    "title": "Transformers in Time Series: A Survey",
    "date": "11 May, 2023",
    "authors": [
      "Qingsong Wen",
      " Tian Zhou",
      " Chaoli Zhang",
      " Weiqi Chen",
      " Ziqing Ma",
      " Junchi Yan",
      " Liang Sun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07153",
    "paper_id": "2305.07153",
    "abstract": "\n        A number of leading AI companies, including OpenAI, Google DeepMind, and Anthropic, have the stated goal of building artificial general intelligence (AGI) - AI systems that achieve or exceed human performance across a wide range of cognitive tasks. In pursuing this goal, they may develop and deploy AI systems that pose particularly significant risks. While they have already taken some measures to mitigate these risks, best practices have not yet emerged. To support the identification of best practices, we sent a survey to 92 leading experts from AGI labs, academia, and civil society and received 51 responses. Participants were asked how much they agreed with 50 statements about what AGI labs should do. Our main finding is that participants, on average, agreed with all of them. Many statements received extremely high levels of agreement. For example, 98% of respondents somewhat or strongly agreed that AGI labs should conduct pre-deployment risk assessments, dangerous capabilities evaluations, third-party model audits, safety restrictions on model usage, and red teaming. Ultimately, our list of statements may serve as a helpful foundation for efforts to develop best practices, standards, and regulations for AGI labs.\n        \u25b3 Less\n      ",
    "title": "Towards best practices in AGI safety and governance: A survey of expert opinion",
    "date": "11 May, 2023",
    "authors": [
      "Jonas Schuett",
      " Noemi Dreksler",
      " Markus Anderljung",
      " David McCaffary",
      " Lennart Heim",
      " Emma Bluemke",
      " Ben Garfinkel"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07157",
    "paper_id": "2305.07157",
    "abstract": "\n        Conversational NLU providers often need to scale to thousands of intent-classification models where new customers often face the cold-start problem. Scaling to so many customers puts a constraint on storage space as well. In this paper, we explore four different zero and few-shot intent classification approaches with this low-resource constraint: 1) domain adaptation, 2) data augmentation, 3) zero-shot intent classification using descriptions large language models (LLMs), and 4) parameter-efficient fine-tuning of instruction-finetuned language models. Our results show that all these approaches are effective to different degrees in low-resource settings. Parameter-efficient fine-tuning using T-few recipe (Liu et al., 2022) on Flan-T5 (Chang et al., 2022) yields the best performance even with just one sample per intent. We also show that the zero-shot method of prompting LLMs using intent descriptions\n        \u25b3 Less\n      ",
    "title": "Exploring Zero and Few-shot Techniques for Intent Classification",
    "date": "11 May, 2023",
    "authors": [
      "Soham Parikh",
      " Quaizar Vohra",
      " Prashil Tumbade",
      " Mitul Tiwari"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07163",
    "paper_id": "2305.07163",
    "abstract": "\n        Machine learning with Semantic Web ontologies follows several strategies, one of which involves projecting ontologies into graph structures and applying graph embeddings or graph-based machine learning methods to the resulting graphs. Several methods have been developed that project ontology axioms into graphs. However, these methods are limited in the type of axioms they can project (totality), whether they are invertible (injectivity), and how they exploit semantic information. These limitations restrict the kind of tasks to which they can be applied. Category-theoretical semantics of logic languages formalizes interpretations using categories instead of sets, and categories have a graph-like structure. We developed CatE, which uses the category-theoretical formulation of the semantics of the Description Logic ALC\\mathcal{ALC} to generate a graph representation for ontology axioms. The CatE projection is total and injective, and therefore overcomes limitations of other graph-based ontology embedding methods which are generally not invertible. We apply CatE to a number of different tasks, including deductive and inductive reasoning, and we demonstrate that CatE improves over state of the art ontology embedding methods. Furthermore, we show that CatE can also outperform model-theoretic ontology embedding methods in machine learning tasks in the biomedical domain.\n        \u25b3 Less\n      ",
    "title": "CatE: Embedding \nALC\n ontologies using category-theoretical semantics",
    "date": "11 May, 2023",
    "authors": [
      "Fernando Zhapa-Camacho",
      " Robert Hoehndorf"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07178",
    "paper_id": "2305.07178",
    "abstract": "\n        Pareto optimization using evolutionary multi-objective algorithms has been widely applied to solve constrained submodular optimization problems. A crucial factor determining the runtime of the used evolutionary algorithms to obtain good approximations is the population size of the algorithms which grows with the number of trade-offs that the algorithms encounter. In this paper, we introduce a sliding window speed up technique for recently introduced algorithms. We prove that our technique eliminates the population size as a crucial factor negatively impacting the runtime and achieves the same theoretical performance guarantees as previous approaches within less computation time. Our experimental investigations for the classical maximum coverage problem confirms that our sliding window technique clearly leads to better results for a wide range of instances and constraint settings.\n        \u25b3 Less\n      ",
    "title": "Fast Pareto Optimization Using Sliding Window Selection",
    "date": "11 May, 2023",
    "authors": [
      "Frank Neumann",
      " Carsten Witt"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06924",
    "paper_id": "2305.06924",
    "abstract": "\n        The Nash Equilibrium (NE) estimation in bidding games of electricity markets is the key concern of both generation companies (GENCOs) for bidding strategy optimization and the Independent System Operator (ISO) for market surveillance. However, existing methods for NE estimation in emerging modern electricity markets (FEM) are inaccurate and inefficient because the priori knowledge of bidding strategies before any environment changes, such as load demand variations, network congestion, and modifications of market design, is not fully utilized. In this paper, a Bayes-adaptive Markov Decision Process in FEM (BAMDP-FEM) is therefore developed to model the GENCOs' bidding strategy optimization considering the priori knowledge. A novel Multi-Agent Generative Adversarial Imitation Learning algorithm (MAGAIL-FEM) is then proposed to enable GENCOs to learn simultaneously from priori knowledge and interactions with changing environments. The obtained NE is a Bayesian Nash Equilibrium (BNE) with priori knowledge transferred from the previous environment. In the case study, the superiority of this proposed algorithm in terms of convergence speed compared with conventional methods is verified. It is concluded that the optimal bidding strategies in the obtained BNE can always lead to more profits than NE due to the effective learning from the priori knowledge. Also, BNE is more accurate and consistent with situations in real-world markets.\n        \u25b3 Less\n      ",
    "title": "An Imitation Learning Based Algorithm Enabling Priori Knowledge Transfer in Modern Electricity Markets for Bayesian Nash Equilibrium Estimation",
    "date": "11 May, 2023",
    "authors": [
      "Ziqing Zhu",
      " Ka Wing Chan",
      " Siqi Bu",
      " Ze Hu",
      " Shiwei Xia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06921",
    "paper_id": "2305.06921",
    "abstract": "\n        This two-part paper develops a paradigmatic theory and detailed methods of the joint electricity market design using reinforcement-learning (RL)-based simulation. In Part 2, this theory is further demonstrated by elaborating detailed methods of designing an electricity spot market (ESM), together with a reserved capacity product (RC) in the ancillary service market (ASM) and a virtual bidding (VB) product in the financial market (FM). Following the theory proposed in Part 1, firstly, market design options in the joint market are specified. Then, the Markov game model is developed, in which we show how to incorporate market design options and uncertain risks in model formulation. A multi-agent policy proximal optimization (MAPPO) algorithm is elaborated, as a practical implementation of the generalized market simulation method developed in Part 1. Finally, the case study demonstrates how to pick the best market design options by using some of the market operation performance indicators proposed in Part 1, based on the simulation results generated by implementing the MAPPO algorithm. The impacts of different market design options on market participants' bidding strategy preference are also discussed.\n        \u25b3 Less\n      ",
    "title": "How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 2: Method and Applications",
    "date": "11 May, 2023",
    "authors": [
      "Ziqing Zhu",
      " Siqi Bu",
      " Ka Wing Chan",
      " Bin Zhou",
      " Shiwei Xia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02485",
    "paper_id": "2305.02485",
    "abstract": "\n        In face of the pressing need of decarbonization in the power sector, the re-design of electricity market is necessary as a Marco-level approach to accommodate the high penetration of renewable generations, and to achieve power system operation security, economic efficiency, and environmental friendliness. However, existing market design methodologies suffer from the lack of coordination among energy spot market (ESM), ancillary service market (ASM) and financial market (FM), i.e., the \"joint market\", and the lack of reliable simulation-based verification. To tackle these deficiencies, this two-part paper develops a paradigmatic theory and detailed methods of the joint market design using reinforcement-learning (RL)-based simulation. In Part 1, the theory and framework of this novel market design philosophy are proposed. First, the controversial market design options while designing the joint market are summarized as the targeted research questions. Second, the Markov game model is developed to describe the bidding game in the joint market, incorporating the market design options to be determined. Third, a framework of deploying multiple types of RL algorithms to simulate the market model is developed. Finally, several market operation performance indicators are proposed to validate the market design based on the simulation results.\n        \u25b3 Less\n      ",
    "title": "How to Use Reinforcement Learning to Facilitate Future Electricity Market Design? Part 1: A Paradigmatic Theory",
    "date": "11 May, 2023",
    "authors": [
      "Ziqing Zhu",
      " Siqi Bu",
      " Ka Wing Chan",
      " Bin Zhou",
      " Shiwei Xia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07214",
    "paper_id": "2305.07214",
    "abstract": "\n        In this paper, we study a novel problem in egocentric action recognition, which we term as \"Multimodal Generalization\" (MMG). MMG aims to study how systems can generalize when data from certain modalities is limited or even completely missing. We thoroughly investigate MMG in the context of standard supervised action recognition and the more challenging few-shot setting for learning new action categories. MMG consists of two novel scenarios, designed to support security, and efficiency considerations in real-world applications: (1) missing modality generalization where some modalities that were present during the train time are missing during the inference time, and (2) cross-modal zero-shot generalization, where the modalities present during the inference time and the training time are disjoint. To enable this investigation, we construct a new dataset MMG-Ego4D containing data points with video, audio, and inertial motion sensor (IMU) modalities. Our dataset is derived from Ego4D dataset, but processed and thoroughly re-annotated by human experts to facilitate research in the MMG problem. We evaluate a diverse array of models on MMG-Ego4D and propose new methods with improved generalization ability. In particular, we introduce a new fusion module with modality dropout training, contrastive-based alignment training, and a novel cross-modal prototypical loss for better few-shot performance. We hope this study will serve as a benchmark and guide future research in multimodal generalization problems. The benchmark and code will be available at https://github.com/facebookresearch/MMG_Ego4D.\n        \u25b3 Less\n      ",
    "title": "MMG-Ego4D: Multi-Modal Generalization in Egocentric Action Recognition",
    "date": "11 May, 2023",
    "authors": [
      "Xinyu Gong",
      " Sreyas Mohan",
      " Naina Dhingra",
      " Jean-Charles Bazin",
      " Yilei Li",
      " Zhangyang Wang",
      " Rakesh Ranjan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07233",
    "paper_id": "2305.07233",
    "abstract": "\n        Forgetting is an important concept in knowledge representation and automated reasoning with widespread applications across a number of disciplines. A standard forgetting operator, characterized in [Lin and Reiter'94] in terms of model-theoretic semantics and primarily focusing on the propositional case, opened up a new research subarea. In this paper, a new operator called weak forgetting, dual to standard forgetting, is introduced and both together are shown to offer a new more uniform perspective on forgetting operators in general. Both the weak and standard forgetting operators are characterized in terms of entailment and inference, rather than a model theoretic semantics. This naturally leads to a useful algorithmic perspective based on quantifier elimination and the use of Ackermman's Lemma and its fixpoint generalization. The strong formal relationship between standard forgetting and strongest necessary conditions and weak forgetting and weakest sufficient conditions is also characterized quite naturally through the entailment-based, inferential perspective used. The framework used to characterize the dual forgetting operators is also generalized to the first-order case and includes useful algorithms for computing first-order forgetting operators in special cases. Practical examples are also included to show the importance of both weak and standard forgetting in modeling and representation.\n        \u25b3 Less\n      ",
    "title": "Dual Forgetting Operators in the Context of Weakest Sufficient and Strongest Necessary Conditions",
    "date": "11 May, 2023",
    "authors": [
      "Patrick Doherty",
      " Andrzej Szalas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07248",
    "paper_id": "2305.07248",
    "abstract": "\n        Classical reinforcement learning (RL) aims to optimize the expected cumulative reward. In this work, we consider the RL setting where the goal is to optimize the quantile of the cumulative reward. We parameterize the policy controlling actions by neural networks, and propose a novel policy gradient algorithm called Quantile-Based Policy Optimization (QPO) and its variant Quantile-Based Proximal Policy Optimization (QPPO) for solving deep RL problems with quantile objectives. QPO uses two coupled iterations running at different timescales for simultaneously updating quantiles and policy parameters, whereas QPPO is an off-policy version of QPO that allows multiple updates of parameters during one simulation episode, leading to improved algorithm efficiency. Our numerical results indicate that the proposed algorithms outperform the existing baseline algorithms under the quantile criterion.\n        \u25b3 Less\n      ",
    "title": "Quantile-Based Deep Reinforcement Learning using Two-Timescale Policy Gradient Algorithms",
    "date": "11 May, 2023",
    "authors": [
      "Jinyang Jiang",
      " Jiaqiao Hu",
      " Yijie Peng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.01588",
    "paper_id": "2212.01588",
    "abstract": "\n        Dialogue systems can leverage large pre-trained language models and knowledge to generate fluent and informative responses. However, these models are still prone to produce hallucinated responses not supported by the input source, which greatly hinders their application. The heterogeneity between external knowledge and dialogue context challenges representation learning and source integration, and further contributes to unfaithfulness. To handle this challenge and generate more faithful responses, this paper presents RHO (\u03c1\u03c1) utilizing the representations of linked entities and relation predicates from a knowledge graph (KG). We propose (1) local knowledge grounding to combine textual embeddings with the corresponding KG embeddings; and (2) global knowledge grounding to equip RHO with multi-hop reasoning abilities via the attention mechanism. In addition, we devise a response re-ranking technique based on walks over KG sub-graphs for better conversational reasoning. Experimental results on OpenDialKG show that our approach significantly outperforms state-of-the-art methods on both automatic and human evaluation by a large margin, especially in hallucination reduction (17.54% in FeQA).\n        \u25b3 Less\n      ",
    "title": "RHO (\n\u03c1\n): Reducing Hallucination in Open-domain Dialogues with Knowledge Grounding",
    "date": "11 May, 2023",
    "authors": [
      "Ziwei Ji",
      " Zihan Liu",
      " Nayeon Lee",
      " Tiezheng Yu",
      " Bryan Wilie",
      " Min Zeng",
      " Pascale Fung"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10438",
    "paper_id": "2305.10438",
    "abstract": "\n        Word embeddings, i.e., semantically meaningful vector representation of words, are largely influenced by the distributional hypothesis \"You shall know a word by the company it keeps\" (Harris, 1954), whereas modern prediction-based neural network embeddings rely on design choices and hyperparameter optimization. Word embeddings like Word2Vec, GloVe etc. well capture the contextuality and real-world analogies but contemporary convolution-based image embeddings such as VGGNet, AlexNet, etc. do not capture contextual knowledge. The popular king-queen analogy does not hold true for most commonly used vision embeddings.\n  In this paper, we introduce a pre-trained joint embedding (JE), named IMAGINATOR, trained on 21K distinct image objects level from 1M image+text pairs. JE is a way to encode multimodal data into a vector space where the text modality serves as the ground-ing key, which the complementary modality (in this case, the image) is anchored with. IMAGINATOR encapsulates three individual representations: (i) object-object co-location, (ii) word-object co-location, and (iii) word-object correlation. These three ways capture complementary aspects of the two modalities which are further combined to obtain the final JEs.\n  Generated JEs are intrinsically evaluated to assess how well they capture the contextuality and real-world analogies. We also evaluate pre-trained IMAGINATOR JEs on three downstream tasks: (i) image captioning, (ii) Image2Tweet, and (iii) text-based image retrieval. IMAGINATOR establishes a new standard on the aforementioned down-stream tasks by outperforming the current SoTA on all the selected tasks. IMAGINATOR will be made publicly available. The codes are available at https://github.com/varunakk/IMAGINATOR\n        \u25b3 Less\n      ",
    "title": "IMAGINATOR: Pre-Trained Image+Text Joint Embeddings using Word-Level Grounding of Images",
    "date": "11 May, 2023",
    "authors": [
      "Varuna Krishna",
      " S Suryavardan",
      " Shreyash Mishra",
      " Sathyanarayanan Ramamoorthy",
      " Parth Patwa",
      " Megha Chakraborty",
      " Aman Chadha",
      " Amitava Das",
      " Amit Sheth"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07266",
    "paper_id": "2305.07266",
    "abstract": "\n        Named Entity Recognition (NER) is a well and widely studied task in natural language processing. Recently, the nested NER has attracted more attention since its practicality and difficulty. Existing works for nested NER ignore the recognition order and boundary position relation of nested entities. To address these issues, we propose a novel seq2seq model named GPRL, which formulates the nested NER task as an entity triplet sequence generation process. GPRL adopts the reinforcement learning method to generate entity triplets decoupling the entity order in gold labels and expects to learn a reasonable recognition order of entities via trial and error. Based on statistics of boundary distance for nested entities, GPRL designs a Gaussian prior to represent the boundary distance distribution between nested entities and adjust the output probability distribution of nested boundary tokens. Experiments on three nested NER datasets demonstrate that GPRL outperforms previous nested NER models.\n        \u25b3 Less\n      ",
    "title": "Gaussian Prior Reinforcement Learning for Nested Named Entity Recognition",
    "date": "11 May, 2023",
    "authors": [
      "Yawen Yang",
      " Xuming Hu",
      " Fukun Ma",
      " Shu'ang Li",
      " Aiwei Liu",
      " Lijie Wen",
      " Philip S. Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07309",
    "paper_id": "2305.07309",
    "abstract": "\n        Artificial intelligence (AI) is envisioned to play a key role in future wireless technologies, with deep neural networks (DNNs) enabling digital receivers to learn to operate in challenging communication scenarios. However, wireless receiver design poses unique challenges that fundamentally differ from those encountered in traditional deep learning domains. The main challenges arise from the limited power and computational resources of wireless devices, as well as from the dynamic nature of wireless communications, which causes continual changes to the data distribution. These challenges impair conventional AI based on highly-parameterized DNNs, motivating the development of adaptive, flexible, and light-weight AI for wireless communications, which is the focus of this article. Here, we propose that AI-based design of wireless receivers requires rethinking of the three main pillars of AI: architecture, data, and training algorithms. In terms of architecture, we review how to design compact DNNs via model-based deep learning. Then, we discuss how to acquire training data for deep receivers without compromising spectral efficiency. Finally, we review efficient, reliable, and robust training algorithms via meta-learning and generalized Bayesian learning. Numerical results are presented to demonstrate the complementary effectiveness of each of the surveyed methods. We conclude by presenting opportunities for future research on the development of practical deep receivers\n        \u25b3 Less\n      ",
    "title": "Adaptive and Flexible Model-Based AI for Deep Receivers in Dynamic Channels",
    "date": "11 May, 2023",
    "authors": [
      "Tomer Raviv",
      " Sangwoo Park",
      " Osvaldo Simeone",
      " Yonina C. Eldar",
      " Nir Shlezinger"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07310",
    "paper_id": "2305.07310",
    "abstract": "\n        The multilingual neural machine translation (NMT) model has a promising capability of zero-shot translation, where it could directly translate between language pairs unseen during training. For good transfer performance from supervised directions to zero-shot directions, the multilingual NMT model is expected to learn universal representations across different languages. This paper introduces a cross-lingual consistency regularization, CrossConST, to bridge the representation gap among different languages and boost zero-shot translation performance. The theoretical analysis shows that CrossConST implicitly maximizes the probability distribution for zero-shot translation, and the experimental results on both low-resource and high-resource benchmarks show that CrossConST consistently improves the translation performance. The experimental analysis also proves that CrossConST could close the sentence representation gap and better align the representation space. Given the universality and simplicity of CrossConST, we believe it can serve as a strong baseline for future multilingual NMT research.\n        \u25b3 Less\n      ",
    "title": "Improving Zero-shot Multilingual Neural Machine Translation by Leveraging Cross-lingual Consistency Regularization",
    "date": "11 May, 2023",
    "authors": [
      "Pengzhi Gao",
      " Liwen Zhang",
      " Zhongjun He",
      " Hua Wu",
      " Haifeng Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07340",
    "paper_id": "2305.07340",
    "abstract": "\n        METHODS: First, a set of evaluation criteria is designed based on a comprehensive literature review. Second, existing candidate criteria are optimized for using a Delphi method by five experts in medicine and engineering. Third, three clinical experts design a set of medical datasets to interact with LLMs. Finally, benchmarking experiments are conducted on the datasets. The responses generated by chatbots based on LLMs are recorded for blind evaluations by five licensed medical experts. RESULTS: The obtained evaluation criteria cover medical professional capabilities, social comprehensive capabilities, contextual capabilities, and computational robustness, with sixteen detailed indicators. The medical datasets include twenty-seven medical dialogues and seven case reports in Chinese. Three chatbots are evaluated, ChatGPT by OpenAI, ERNIE Bot by Baidu Inc., and Doctor PuJiang (Dr. PJ) by Shanghai Artificial Intelligence Laboratory. Experimental results show that Dr. PJ outperforms ChatGPT and ERNIE Bot in both multiple-turn medical dialogue and case report scenarios.\n        \u25b3 Less\n      ",
    "title": "MedGPTEval: A Dataset and Benchmark to Evaluate Responses of Large Language Models in Medicine",
    "date": "11 May, 2023",
    "authors": [
      "Jie Xu",
      " Lu Lu",
      " Sen Yang",
      " Bilin Liang",
      " Xinwei Peng",
      " Jiali Pang",
      " Jinru Ding",
      " Xiaoming Shi",
      " Lingrui Yang",
      " Huan Song",
      " Kang Li",
      " Xin Sun",
      " Shaoting Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11891",
    "paper_id": "2305.11891",
    "abstract": "\n        Nowadays, most of the datasets leveraging space-borne Earth Observation (EO) data are based on high-end levels products, which are ortho-rectified, coregistered, calibrated, and further processed to mitigate the impact of noise and distortions. Nevertheless, given the growing interest to apply Artificial Intelligence (AI) onboard satellites for time-critical applications, such as natural disaster response, providing raw satellite images could be useful to foster the research on energy-efficient pre-processing algorithms and AI models for onboard-satellite applications. In this framework, we present THRawS, the first dataset composed of Sentinel-2 (S-2) raw data containing warm temperature hotspots (wildfires and volcanic eruptions). To foster the realisation of robust AI architectures, the dataset gathers data from all over the globe. Furthermore, we designed a custom methodology to identify events in raw data starting from the corresponding Level-1C (L1C) products. Indeed, given the availability of state-of-the-art algorithms for thermal anomalies detection on the L1C tiles, we detect such events on these latter and we then re-project them on the corresponding raw images. Additionally, to deal with unprocessed data, we devise a lightweight coarse coregisteration and georeferencing strategy. The developed dataset is comprehensive of more than 100 samples containing wildfires, volcanic eruptions, and event-free volcanic areas to enable both warm-events detection and general classification applications. Finally, we compare performances between the proposed coarse spatial coregistration technique and the SuperGlue Deep Neural Network method to highlight the different constraints in terms of timing and quality of spatial registration to minimise the spatial displacement error for a specific scene.\n        \u25b3 Less\n      ",
    "title": "THRawS: A Novel Dataset for Thermal Hotspots Detection in Raw Sentinel-2 Data",
    "date": "12 May, 2023",
    "authors": [
      "Gabriele Meoni",
      " Roberto Del Prete",
      " Federico Serva",
      " Alix De Beussche",
      " Olivier Colin",
      " Nicolas Long\u00e9p\u00e9"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07360",
    "paper_id": "2305.07360",
    "abstract": "\n        In this paper, we have shown a method of improving the quality of neural machine translation by translating/transliterating name entities as a preprocessing step. Through experiments we have shown the performance gain of our system. For evaluation we considered three types of name entities viz person names, location names and organization names. The system was able to correctly translate mostly all the name entities. For person names the accuracy was 99.86%, for location names the accuracy was 99.63% and for organization names the accuracy was 99.05%. Overall, the accuracy of the system was 99.52%\n        \u25b3 Less\n      ",
    "title": "Improving the Quality of Neural Machine Translation Through Proper Translation of Name Entities",
    "date": "12 May, 2023",
    "authors": [
      "Radhika Sharma",
      " Pragya Katyayan",
      " Nisheeth Joshi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.08914",
    "paper_id": "2304.08914",
    "abstract": "\n        In this paper, we extend original Neural Collapse Phenomenon by proving Generalized Neural Collapse hypothesis. We obtain Grassmannian Frame structure from the optimization and generalization of classification. This structure maximally separates features of every two classes on a sphere and does not require a larger feature dimension than the number of classes. Out of curiosity about the symmetry of Grassmannian Frame, we conduct experiments to explore if models with different Grassmannian Frames have different performance. As a result, we discover the Symmetric Generalization phenomenon. We provide a theorem to explain Symmetric Generalization of permutation. However, the question of why different directions of features can lead to such different generalization is still open for future investigation.\n        \u25b3 Less\n      ",
    "title": "A Study of Neural Collapse Phenomenon: Grassmannian Frame, Symmetry and Generalization",
    "date": "12 May, 2023",
    "authors": [
      "Peifeng Gao",
      " Qianqian Xu",
      " Peisong Wen",
      " Huiyang Shao",
      " Zhiyong Yang",
      " Qingming Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07365",
    "paper_id": "2305.07365",
    "abstract": "\n        In this paper, we have shown a script conversion (transliteration) technique that converts Sindhi text in the Devanagari script to the Perso-Arabic script. We showed this by incorporating a hybrid approach where some part of the text is converted using a rule base and in case an ambiguity arises then a probabilistic model is used to resolve the same. Using this approach, the system achieved an overall accuracy of 99.64%.\n        \u25b3 Less\n      ",
    "title": "Towards Transliteration between Sindhi Scripts from Devanagari to Perso-Arabic",
    "date": "12 May, 2023",
    "authors": [
      "Shivani Singh Rathore",
      " Bharti Nathani",
      " Nisheeth Joshi",
      " Pragya Katyayan",
      " Chander Prakash Dadlani"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07366",
    "paper_id": "2305.07366",
    "abstract": "\n        Value-alignment in normative multi-agent systems is used to promote a certain value and to ensure the consistent behavior of agents in autonomous intelligent systems with human values. However, the current literature is limited to incorporation of effective norms for single value alignment with no consideration of agents' heterogeneity and the requirement of simultaneous promotion and alignment of multiple values. This research proposes a multi-value promotion model that uses multi-objective evolutionary algorithms to produce the optimum parametric set of norms that is aligned with multiple simultaneous values of heterogeneous agents and the system. To understand various aspects of this complex problem, several evolutionary algorithms were used to find a set of optimised norm parameters considering two toy tax scenarios with two and five values are considered. The results are analysed from different perspectives to show the impact of a selected evolutionary algorithm on the solution, and the importance of understanding the relation between values when prioritising them.\n        \u25b3 Less\n      ",
    "title": "Multi-Value Alignment in Normative Multi-Agent System: Evolutionary Optimisation Approach",
    "date": "12 May, 2023",
    "authors": [
      "Maha Riad",
      " Vinicius Renan de Carvalho",
      " Fatemeh Golpayegani"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07367",
    "paper_id": "2305.07367",
    "abstract": "\n        This paper presents a novel RL algorithm, S-REINFORCE, which is designed to generate interpretable policies for dynamic decision-making tasks. The proposed algorithm leverages two types of function approximators, namely Neural Network (NN) and Symbolic Regressor (SR), to produce numerical and symbolic policies, respectively. The NN component learns to generate a numerical probability distribution over the possible actions using a policy gradient, while the SR component captures the functional form that relates the associated states with the action probabilities. The SR-generated policy expressions are then utilized through importance sampling to improve the rewards received during the learning process. We have tested the proposed S-REINFORCE algorithm on various dynamic decision-making problems with low and high dimensional action spaces, and the results demonstrate its effectiveness and impact in achieving interpretable solutions. By leveraging the strengths of both NN and SR, S-REINFORCE produces policies that are not only well-performing but also easy to interpret, making it an ideal choice for real-world applications where transparency and causality are crucial.\n        \u25b3 Less\n      ",
    "title": "S-REINFORCE: A Neuro-Symbolic Policy Gradient Approach for Interpretable Reinforcement Learning",
    "date": "12 May, 2023",
    "authors": [
      "Rajdeep Dutta",
      " Qincheng Wang",
      " Ankur Singh",
      " Dhruv Kumarjiguda",
      " Li Xiaoli",
      " Senthilnath Jayavelu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06472",
    "paper_id": "2305.06472",
    "abstract": "\n        Prognostics and health management (PHM) technology plays a critical role in industrial production and equipment maintenance by identifying and predicting possible equipment failures and damages, thereby allowing necessary maintenance measures to be taken to enhance equipment service life and reliability while reducing production costs and downtime. In recent years, PHM technology based on artificial intelligence (AI) has made remarkable achievements in the context of the industrial IoT and big data, and it is widely used in various industries, such as railway, energy, and aviation, for condition monitoring, fault prediction, and health management. The emergence of large-scale foundation models (LSF-Models) such as ChatGPT and DALLE-E marks the entry of AI into a new era of AI-2.0 from AI-1.0, where deep models have rapidly evolved from a research paradigm of single-modal, single-task, and limited-data to a multi-modal, multi-task, massive data, and super-large model paradigm. ChatGPT represents a landmark achievement in this research paradigm, offering hope for general artificial intelligence due to its highly intelligent natural language understanding ability. However, the PHM field lacks a consensus on how to respond to this significant change in the AI field, and a systematic review and roadmap is required to elucidate future development directions. To fill this gap, this paper systematically expounds on the key components and latest developments of LSF-Models. Then, we systematically answered how to build the LSF-Model applicable to PHM tasks and outlined the challenges and future development roadmaps for this research paradigm.\n        \u25b3 Less\n      ",
    "title": "ChatGPT-Like Large-Scale Foundation Models for Prognostics and Health Management: A Survey and Roadmaps",
    "date": "12 May, 2023",
    "authors": [
      "Yan-Fu Li",
      " Huan Wang",
      " Muxia Sun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.03796",
    "paper_id": "2211.03796",
    "abstract": "\n        In this review, we explore the historical development and future prospects of artificial intelligence (AI) and deep learning in astronomy. We trace the evolution of connectionism in astronomy through its three waves, from the early use of multilayer perceptrons, to the rise of convolutional and recurrent neural networks, and finally to the current era of unsupervised and generative deep learning methods. With the exponential growth of astronomical data, deep learning techniques offer an unprecedented opportunity to uncover valuable insights and tackle previously intractable problems. As we enter the anticipated fourth wave of astronomical connectionism, we argue for the adoption of GPT-like foundation models fine-tuned for astronomical applications. Such models could harness the wealth of high-quality, multimodal astronomical data to serve state-of-the-art downstream tasks. To keep pace with advancements driven by Big Tech, we propose a collaborative, open-source approach within the astronomy community to develop and maintain these foundation models, fostering a symbiotic relationship between AI and astronomy that capitalizes on the unique strengths of both fields.\n        \u25b3 Less\n      ",
    "title": "Astronomia ex machina: a history, primer, and outlook on neural networks in astronomy",
    "date": "12 May, 2023",
    "authors": [
      "Michael J. Smith",
      " James E. Geach"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07374",
    "paper_id": "2305.07374",
    "abstract": "\n        Question Answering (QA) has proved to be an arduous challenge in the area of natural language processing (NLP) and artificial intelligence (AI). Many attempts have been made to develop complete solutions for QA as well as improving significant sub-modules of the QA systems to improve the overall performance through the course of time. Questions are the most important piece of QA, because knowing the question is equivalent to knowing what counts as an answer (Harrah in Philos Sci, 1961 [1]). In this work, we have attempted to understand questions in a better way by using Quantum Machine Learning (QML). The properties of Quantum Computing (QC) have enabled classically intractable data processing. So, in this paper, we have performed question classification on questions from two classes of SelQA (Selection-based Question Answering) dataset using quantum-based classifier algorithms-quantum support vector machine (QSVM) and variational quantum classifier (VQC) from Qiskit (Quantum Information Science toolKIT) for Python. We perform classification with both classifiers in almost similar environments and study the effects of circuit depths while comparing the results of both classifiers. We also use these classification results with our own rule-based QA system and observe significant performance improvement. Hence, this experiment has helped in improving the quality of QA in general.\n        \u25b3 Less\n      ",
    "title": "Implications of Deep Circuits in Improving Quality of Quantum Question Answering",
    "date": "12 May, 2023",
    "authors": [
      "Pragya Katyayan",
      " Nisheeth Joshi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18305",
    "paper_id": "2305.18305",
    "abstract": "\n        We develop a novel latent-bandit algorithm for tackling the cold-start problem for new users joining a recommender system. This new algorithm significantly outperforms the state of the art, simultaneously achieving both higher accuracy and lower regret.\n        \u25b3 Less\n      ",
    "title": "High Accuracy and Low Regret for User-Cold-Start Using Latent Bandits",
    "date": "12 May, 2023",
    "authors": [
      "David Young",
      " Douglas Leith"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2105.09095",
    "paper_id": "2105.09095",
    "abstract": "\n        A Bayesian treatment of deep learning allows for the computation of uncertainties associated with the predictions of deep neural networks. We show how the concept of Errors-in-Variables can be used in Bayesian deep regression to also account for the uncertainty associated with the input of the employed neural network. The presented approach thereby exploits a relevant, but generally overlooked, source of uncertainty and yields a decomposition of the predictive uncertainty into an aleatoric and epistemic part that is more complete and, in many cases, more consistent from a statistical perspective. We discuss the approach along various simulated and real examples and observe that using an Errors-in-Variables model leads to an increase in the uncertainty while preserving the prediction performance of models without Errors-in-Variables. For examples with known regression function we observe that this ground truth is substantially better covered by the Errors-in-Variables model, indicating that the presented approach leads to a more reliable uncertainty estimation.\n        \u25b3 Less\n      ",
    "title": "Aleatoric uncertainty for Errors-in-Variables models in deep regression",
    "date": "12 May, 2023",
    "authors": [
      "J\u00f6rg Martin",
      " Clemens Elster"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.02337",
    "paper_id": "2302.02337",
    "abstract": "\n        Large generative AI models (LGAIMs), such as ChatGPT, GPT-4 or Stable Diffusion, are rapidly transforming the way we communicate, illustrate, and create. However, AI regulation, in the EU and beyond, has primarily focused on conventional AI models, not LGAIMs. This paper will situate these new generative models in the current debate on trustworthy AI regulation, and ask how the law can be tailored to their capabilities. After laying technical foundations, the legal part of the paper proceeds in four steps, covering (1) direct regulation, (2) data protection, (3) content moderation, and (4) policy proposals. It suggests a novel terminology to capture the AI value chain in LGAIM settings by differentiating between LGAIM developers, deployers, professional and non-professional users, as well as recipients of LGAIM output. We tailor regulatory duties to these different actors along the value chain and suggest strategies to ensure that LGAIMs are trustworthy and deployed for the benefit of society at large. Rules in the AI Act and other direct regulation must match the specificities of pre-trained models. The paper argues for three layers of obligations concerning LGAIMs (minimum standards for all LGAIMs; high-risk obligations for high-risk use cases; collaborations along the AI value chain). In general, regulation should focus on concrete high-risk applications, and not the pre-trained model itself, and should include (i) obligations regarding transparency and (ii) risk management. Non-discrimination provisions (iii) may, however, apply to LGAIM developers. Lastly, (iv) the core of the DSA content moderation rules should be expanded to cover LGAIMs. This includes notice and action mechanisms, and trusted flaggers. In all areas, regulators and lawmakers need to act fast to keep track with the dynamics of ChatGPT et al.\n        \u25b3 Less\n      ",
    "title": "Regulating ChatGPT and other Large Generative AI Models",
    "date": "12 May, 2023",
    "authors": [
      "Philipp Hacker",
      " Andreas Engel",
      " Marco Mauer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07392",
    "paper_id": "2305.07392",
    "abstract": "\n        Video games are one of the richest and most popular forms of human-computer interaction and, hence, their role is critical for our understanding of human behaviour and affect at a large scale. As artificial intelligence (AI) tools are gradually adopted by the game industry a series of ethical concerns arise. Such concerns, however, have so far not been extensively discussed in a video game context. Motivated by the lack of a comprehensive review of the ethics of AI as applied to games, we survey the current state of the art in this area and discuss ethical considerations of these systems from the holistic perspective of the affective loop. Through the components of this loop, we study the ethical challenges that AI faces in video game development. Elicitation highlights the ethical boundaries of artificially induced emotions; sensing showcases the trade-off between privacy and safe gaming spaces; and detection, as utilised during in-game adaptation, poses challenges to transparency and ownership. This paper calls for an open dialogue and action for the games of today and the virtual spaces of the future. By setting an appropriate framework we aim to protect users and to guide developers towards safer and better experiences for their customers.\n        \u25b3 Less\n      ",
    "title": "The Ethics of AI in Games",
    "date": "12 May, 2023",
    "authors": [
      "David Melhart",
      " Julian Togelius",
      " Benedikte Mikkelsen",
      " Christoffer Holmg\u00e5rd",
      " Georgios N. Yannakakis"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07404",
    "paper_id": "2305.07404",
    "abstract": "\n        Breast cancer early detection is crucial for improving patient outcomes. The Institut Catal\u00e0 de la Salut (ICS) has launched the DigiPatICS project to develop and implement artificial intelligence algorithms to assist with the diagnosis of cancer. In this paper, we propose a new approach for facing the color normalization problem in HER2-stained histopathological images of breast cancer tissue, posed as an style transfer problem. We combine the Color Deconvolution technique with the Pix2Pix GAN network to present a novel approach to correct the color variations between different HER2 stain brands. Our approach focuses on maintaining the HER2 score of the cells in the transformed images, which is crucial for the HER2 analysis. Results demonstrate that our final model outperforms the state-of-the-art image style transfer methods in maintaining the cell classes in the transformed images and is as effective as them in generating realistic images.\n        \u25b3 Less\n      ",
    "title": "Color Deconvolution applied to Domain Adaptation in HER2 histopathological images",
    "date": "12 May, 2023",
    "authors": [
      "David Anglada-Rotger",
      " Ferran Marqu\u00e9s",
      " Montse Pard\u00e0s"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07416",
    "paper_id": "2305.07416",
    "abstract": "\n        This work introduces the multidimensional Graph Fourier Transformation Neural Network (GFTNN) for long-term trajectory predictions on highways. Similar to Graph Neural Networks (GNNs), the GFTNN is a novel network architecture that operates on graph structures. While several GNNs lack discriminative power due to suboptimal aggregation schemes, the proposed model aggregates scenario properties through a powerful operation: the multidimensional Graph Fourier Transformation (GFT). The spatio-temporal vehicle interaction graph of a scenario is converted into a spectral scenario representation using the GFT. This beneficial representation is input to the prediction framework composed of a neural network and a descriptive decoder. Even though the proposed GFTNN does not include any recurrent element, it outperforms state-of-the-art models in the task of highway trajectory prediction. For experiments and evaluation, the publicly available datasets highD and NGSIM are used\n        \u25b3 Less\n      ",
    "title": "A Multidimensional Graph Fourier Transformation Neural Network for Vehicle Trajectory Prediction",
    "date": "12 May, 2023",
    "authors": [
      "Marion Neumeier",
      " Andreas Tollk\u00fchn",
      " Michael Botsch",
      " Wolfgang Utschick"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16170",
    "paper_id": "2305.16170",
    "abstract": "\n        We investigate the problem of wireless routing in integrated access backhaul (IAB) networks consisting of fiber-connected and wireless base stations and multiple users. The physical constraints of these networks prevent the use of a central controller, and base stations have limited access to real-time network conditions. We aim to maximize packet arrival ratio while minimizing their latency, for this purpose, we formulate the problem as a multi-agent partially observed Markov decision process (POMDP). To solve this problem, we develop a Relational Advantage Actor Critic (Relational A2C) algorithm that uses Multi-Agent Reinforcement Learning (MARL) and information about similar destinations to derive a joint routing policy on a distributed basis. We present three training paradigms for this algorithm and demonstrate its ability to achieve near-centralized performance. Our results show that Relational A2C outperforms other reinforcement learning algorithms, leading to increased network efficiency and reduced selfish agent behavior. To the best of our knowledge, this work is the first to optimize routing strategy for IAB networks.\n        \u25b3 Less\n      ",
    "title": "Multi-Agent Reinforcement Learning for Network Routing in Integrated Access Backhaul Networks",
    "date": "12 May, 2023",
    "authors": [
      "Shahaf Yamin",
      " Haim Permuter"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11892",
    "paper_id": "2305.11892",
    "abstract": "\n        We propose methods for analysis, design, and evaluation of Meaningful Human Control (MHC) for defense technologies from the perspective of military human-machine teaming (HMT). Our approach is based on three principles. Firstly, MHC should be regarded as a core objective that guides all phases of analysis, design and evaluation. Secondly, MHC affects all parts of the socio-technical system, including humans, machines, AI, interactions, and context. Lastly, MHC should be viewed as a property that spans longer periods of time, encompassing both prior and realtime control by multiple actors. To describe macrolevel design options for achieving MHC, we propose various Team Design Patterns. Furthermore, we present a case study, where we applied some of these methods to envision HMT, involving robots and soldiers in a search and rescue task in a military context.\n        \u25b3 Less\n      ",
    "title": "Designing for Meaningful Human Control in Military Human-Machine Teams",
    "date": "12 May, 2023",
    "authors": [
      "Jurriaan van Diggelen",
      " Karel van den Bosch",
      " Mark Neerincx",
      " Marc Steen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07685",
    "paper_id": "2305.07685",
    "abstract": "\n        Access to individual-level health data is essential for gaining new insights and advancing science. In particular, modern methods based on artificial intelligence rely on the availability of and access to large datasets. In the health sector, access to individual-level data is often challenging due to privacy concerns. A promising alternative is the generation of fully synthetic data, i.e. data generated through a randomised process that have similar statistical properties as the original data, but do not have a one-to-one correspondence with the original individual-level records. In this study, we use a state-of-the-art synthetic data generation method and perform in-depth quality analyses of the generated data for a specific use case in the field of nutrition. We demonstrate the need for careful analyses of synthetic data that go beyond descriptive statistics and provide valuable insights into how to realise the full potential of synthetic datasets. By extending the methods, but also by thoroughly analysing the effects of sampling from a trained model, we are able to largely reproduce significant real-world analysis results in the chosen use case.\n        \u25b3 Less\n      ",
    "title": "Synthetic data generation for a longitudinal cohort study -- Evaluation, method extension and reproduction of published data analysis results",
    "date": "12 May, 2023",
    "authors": [
      "Lisa K\u00fchnel",
      " Julian Schneider",
      " Ines Perrar",
      " Tim Adams",
      " Fabian Prasser",
      " Ute N\u00f6thlings",
      " Holger Fr\u00f6hlich",
      " Juliane Fluck"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07478",
    "paper_id": "2305.07478",
    "abstract": "\n        This paper deals with the importance of developing codes of conduct for practitioners--be it journalists, doctors, attorneys, or other professions--that are encountering ethical issues when using computation, but do not have access to any framework of reference as to how to address those. At the same time, legal and technological developments are calling for establishing such guidelines, as shown in the European Union's and the United States' efforts in regulating a wide array of artificial intelligence systems, and in the resurgence of rule-based models through 'neurosymbolic' AI, a hybrid format that combines them with neural methods. Against this backdrop, we argue for taking a design-inspired approach when encoding professional ethics into a computational form, so as to co-create codes of conduct for computational practice across a wide range of fields.\n        \u25b3 Less\n      ",
    "title": "Professional Ethics by Design: Co-creating Codes of Conduct for Computational Practice",
    "date": "12 May, 2023",
    "authors": [
      "Samuel Danzon-Chambaud",
      " Marguerite Foissac"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07487",
    "paper_id": "2305.07487",
    "abstract": "\n        Deep reinforcement learning (DRL) has emerged as a promising approach for developing more intelligent autonomous vehicles (AVs). A typical DRL application on AVs is to train a neural network-based driving policy. However, the black-box nature of neural networks can result in unpredictable decision failures, making such AVs unreliable. To this end, this work proposes a method to identify and protect unreliable decisions of a DRL driving policy. The basic idea is to estimate and constrain the policy's performance uncertainty, which quantifies potential performance drop due to insufficient training data or network fitting errors. By constraining the uncertainty, the DRL model's performance is always greater than that of a baseline policy. The uncertainty caused by insufficient data is estimated by the bootstrapped method. Then, the uncertainty caused by the network fitting error is estimated using an ensemble network. Finally, a baseline policy is added as the performance lower bound to avoid potential decision failures. The overall framework is called uncertainty-bound reinforcement learning (UBRL). The proposed UBRL is evaluated on DRL policies with different amounts of training data, taking an unprotected left-turn driving case as an example. The result shows that the UBRL method can identify potentially unreliable decisions of DRL policy. The UBRL guarantees to outperform baseline policy even when the DRL policy is not well-trained and has high uncertainty. Meanwhile, the performance of UBRL improves with more training data. Such a method is valuable for the DRL application on real-road driving and provides a metric to evaluate a DRL policy.\n        \u25b3 Less\n      ",
    "title": "Identify, Estimate and Bound the Uncertainty of Reinforcement Learning for Autonomous Driving",
    "date": "12 May, 2023",
    "authors": [
      "Weitao Zhou",
      " Zhong Cao",
      " Nanshan Deng",
      " Kun Jiang",
      " Diange Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10440",
    "paper_id": "2305.10440",
    "abstract": "\n        Multicast communication technology is widely applied in wireless environments with a high device density. Traditional wireless network architectures have difficulty flexibly obtaining and maintaining global network state information and cannot quickly respond to network state changes, thus affecting the throughput, delay, and other QoS requirements of existing multicasting solutions. Therefore, this paper proposes a new multicast routing method based on multiagent deep reinforcement learning (MADRL-MR) in a software-defined wireless networking (SDWN) environment. First, SDWN technology is adopted to flexibly configure the network and obtain network state information in the form of traffic matrices representing global network links information, such as link bandwidth, delay, and packet loss rate. Second, the multicast routing problem is divided into multiple subproblems, which are solved through multiagent cooperation. To enable each agent to accurately understand the current network state and the status of multicast tree construction, the state space of each agent is designed based on the traffic and multicast tree status matrices, and the set of AP nodes in the network is used as the action space. A novel single-hop action strategy is designed, along with a reward function based on the four states that may occur during tree construction: progress, invalid, loop, and termination. Finally, a decentralized training approach is combined with transfer learning to enable each agent to quickly adapt to dynamic network changes and accelerate convergence. Simulation experiments show that MADRL-MR outperforms existing algorithms in terms of throughput, delay, packet loss rate, etc., and can establish more intelligent multicast routes.\n        \u25b3 Less\n      ",
    "title": "Intelligent multicast routing method based on multi-agent deep reinforcement learning in SDWN",
    "date": "12 May, 2023",
    "authors": [
      "Hongwen Hu",
      " Miao Ye",
      " Chenwei Zhao",
      " Qiuxiang Jiang",
      " Yong Wang",
      " Hongbing Qiu",
      " Xiaofang Deng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07495",
    "paper_id": "2305.07495",
    "abstract": "\n        Deep learning methods have been achieved brilliant results in face recognition. One of the important tasks to improve the performance is to collect and label images as many as possible. However, labeling identities and checking qualities of large image data are difficult task and mistakes cannot be avoided in processing large data. Previous works have been trying to deal with the problem only in training domain, however it can cause much serious problem if the mistakes are in gallery data of face identification. We proposed gallery data sampling methods which are robust to outliers including wrong labeled, low quality, and less-informative images and reduce searching time. The proposed sampling-by-pruning and sampling-by-generating methods significantly improved face identification performance on our 5.4M web image dataset of celebrities. The proposed method achieved 0.0975 in terms of FNIR at FPIR=0.01, while conventional method showed 0.3891. The average number of feature vectors for each individual gallery was reduced to 17.1 from 115.9 and it can provide much faster search. We also made experiments on public datasets and our method achieved 0.1314 and 0.0668 FNIRs at FPIR=0.01 on the CASIA-WebFace and MS1MV2, while the convectional method did 0.5446, and 0.1327, respectively.\n        \u25b3 Less\n      ",
    "title": "Gallery Sampling for Robust and Fast Face Identification",
    "date": "12 May, 2023",
    "authors": [
      "Myung-cheol Roh",
      " Pyoung-gang Lim",
      " Jongju Shin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07497",
    "paper_id": "2305.07497",
    "abstract": "\n        Self-driving vehicles (SDVs) are becoming reality but still suffer from \"long-tail\" challenges during natural driving: the SDVs will continually encounter rare, safety-critical cases that may not be included in the dataset they were trained. Some safety-assurance planners solve this problem by being conservative in all possible cases, which may significantly affect driving mobility. To this end, this work proposes a method to automatically adjust the conservative level according to each case's \"long-tail\" rate, named dynamically conservative planner (DCP). We first define the \"long-tail\" rate as an SDV's confidence to pass a driving case. The rate indicates the probability of safe-critical events and is estimated using the statistics bootstrapped method with historical data. Then, a reinforcement learning-based planner is designed to contain candidate policies with different conservative levels. The final policy is optimized based on the estimated \"long-tail\" rate. In this way, the DCP is designed to automatically adjust to be more conservative in low-confidence \"long-tail\" cases while keeping efficient otherwise. The DCP is evaluated in the CARLA simulator using driving cases with \"long-tail\" distributed training data. The results show that the DCP can accurately estimate the \"long-tail\" rate to identify potential risks. Based on the rate, the DCP automatically avoids potential collisions in \"long-tail\" cases using conservative decisions while not affecting the average velocity in other typical cases. Thus, the DCP is safer and more efficient than the baselines with fixed conservative levels, e.g., an always conservative planner. This work provides a technique to guarantee SDV's performance in unexpected driving cases without resorting to a global conservative setting, which contributes to solving the \"long-tail\" problem practically.\n        \u25b3 Less\n      ",
    "title": "Dynamically Conservative Self-Driving Planner for Long-Tail Cases",
    "date": "12 May, 2023",
    "authors": [
      "Weitao Zhou",
      " Zhong Cao",
      " Nanshan Deng",
      " Xiaoyu Liu",
      " Kun Jiang",
      " Diange Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10441",
    "paper_id": "2305.10441",
    "abstract": "\n        Due to the highly dynamic changes in wireless network topologies, efficiently obtaining network status information and flexibly forwarding data to improve communication quality of service are important challenges. This article introduces an intelligent routing algorithm (DRL-PPONSA) based on proximal policy optimization deep reinforcement learning with network situational awareness under a software-defined wireless networking architecture. First, a specific data plane is designed for network topology construction and data forwarding. The control plane collects network traffic information, sends flow tables, and uses a GCN-GRU prediction mechanism to perceive future traffic change trends to achieve network situational awareness. Second, a DRL-based data forwarding mechanism is designed in the knowledge plane. The predicted network traffic matrix and topology information matrix are treated as the environment for DRL agents, while next-hop adjacent nodes are treated as executable actions. Accordingly, action selection strategies are designed for different network conditions to achieve more intelligent, flexible, and efficient routing control. The reward function is designed using network link information and various reward and penalty mechanisms. Additionally, importance sampling and gradient clipping techniques are employed during gradient updating to enhance convergence speed and stability. Experimental results show that DRL-PPONSA outperforms traditional routing methods in network throughput, delay, packet loss rate, and wireless node distance. Compared to value-function-based Dueling DQN routing, the convergence speed is significantly improved, and the convergence effect is more stable. Simultaneously, its consumption of hardware storage space is reduced, and efficient routing decisions can be made in real-time using the current network state information.\n        \u25b3 Less\n      ",
    "title": "An Intelligent SDWN Routing Algorithm Based on Network Situational Awareness and Deep Reinforcement Learning",
    "date": "12 May, 2023",
    "authors": [
      "Jinqiang Li",
      " Miao Ye",
      " Linqiang Huang",
      " Xiaofang Deng",
      " Hongbing Qiu",
      " Yong Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07511",
    "paper_id": "2305.07511",
    "abstract": "\n        Over the last few years, the number of works about deep learning applied to the medical field has increased enormously. The necessity of a rigorous assessment of these models is required to explain these results to all people involved in medical exams. A recent field in the machine learning area is explainable artificial intelligence, also known as XAI, which targets to explain the results of such black box models to permit the desired assessment. This survey analyses several recent studies in the XAI field applied to medical diagnosis research, allowing some explainability of the machine learning results in several different diseases, such as cancers and COVID-19.\n        \u25b3 Less\n      ",
    "title": "eXplainable Artificial Intelligence on Medical Images: A Survey",
    "date": "12 May, 2023",
    "authors": [
      "Matteus Vargas Sim\u00e3o da Silva",
      " Rodrigo Reis Arrais",
      " Jhessica Victoria Santos da Silva",
      " Felipe Souza T\u00e2nios",
      " Mateus Antonio Chinelatto",
      " Natalia Backhaus Pereira",
      " Renata De Paris",
      " Lucas Cesar Ferreira Domingos",
      " Rodrigo D\u00f3ria Villa\u00e7a",
      " Vitor Lopes Fabris",
      " Nayara Rossi Brito da Silva",
      " Ana Claudia Akemi Matsuki de Faria",
      " Jose Victor Nogueira Alves da Silva",
      " Fabiana Cristina Queiroz de Oliveira Marucci",
      " Francisco Alves de Souza Neto",
      " Danilo Xavier Silva",
      " Vitor Yukio Kondo",
      " Claudio Filipi Gon\u00e7alves dos Santos"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07524",
    "paper_id": "2305.07524",
    "abstract": "\n        Current MRI super-resolution (SR) methods only use existing contrasts acquired from typical clinical sequences as input for the neural network (NN). In turbo spin echo sequences (TSE) the sequence parameters can have a strong influence on the actual resolution of the acquired image and have consequently a considera-ble impact on the performance of the NN. We propose a known-operator learning approach to perform an end-to-end optimization of MR sequence and neural net-work parameters for SR-TSE. This MR-physics-informed training procedure jointly optimizes the radiofrequency pulse train of a proton density- (PD-) and T2-weighted TSE and a subsequently applied convolutional neural network to predict the corresponding PDw and T2w super-resolution TSE images. The found radiofrequency pulse train designs generate an optimal signal for the NN to perform the SR task. Our method generalizes from the simulation-based optimi-zation to in vivo measurements and the acquired physics-informed SR images show higher correlation with a time-consuming segmented high-resolution TSE sequence compared to a pure network training approach.\n        \u25b3 Less\n      ",
    "title": "Joint MR sequence optimization beats pure neural network approaches for spin-echo MRI super-resolution",
    "date": "12 May, 2023",
    "authors": [
      "Hoai Nam Dang",
      " Vladimir Golkov",
      " Thomas Wimmer",
      " Daniel Cremers",
      " Andreas Maier",
      " Moritz Zaiss"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07528",
    "paper_id": "2305.07528",
    "abstract": "\n        The open road poses many challenges to autonomous perception, including poor visibility from extreme weather conditions. Models trained on good-weather datasets frequently fail at detection in these out-of-distribution settings. To aid adversarial robustness in perception, we introduce WEDGE (WEather images by DALL-E GEneration): a synthetic dataset generated with a vision-language generative model via prompting. WEDGE consists of 3360 images in 16 extreme weather conditions manually annotated with 16513 bounding boxes, supporting research in the tasks of weather classification and 2D object detection. We have analyzed WEDGE from research standpoints, verifying its effectiveness for extreme-weather autonomous perception. We establish baseline performance for classification and detection with 53.87% test accuracy and 45.41 mAP. Most importantly, WEDGE can be used to fine-tune state-of-the-art detectors, improving SOTA performance on real-world weather benchmarks (such as DAWN) by 4.48 AP for well-generated classes like trucks. WEDGE has been collected under OpenAI's terms of use and is released for public use under the CC BY-NC-SA 4.0 license. The repository for this work and dataset is available at https://infernolia.github.io/WEDGE.\n        \u25b3 Less\n      ",
    "title": "WEDGE: A multi-weather autonomous driving dataset built from generative vision-language models",
    "date": "12 May, 2023",
    "authors": [
      "Aboli Marathe",
      " Deva Ramanan",
      " Rahee Walambe",
      " Ketan Kotecha"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.09015",
    "paper_id": "2304.09015",
    "abstract": "\n        Temporal facts, the facts for characterizing events that hold in specific time periods, are attracting rising attention in the knowledge graph (KG) research communities. In terms of quality management, the introduction of time restrictions brings new challenges to maintaining the temporal consistency of KGs and detecting potential temporal conflicts. Previous studies rely on manually enumerated temporal constraints to detect conflicts, which are labor-intensive and may have granularity issues. We start from the common pattern of temporal facts and constraints and propose a pattern-based temporal constraint mining method, PaTeCon. PaTeCon uses automatically determined graph patterns and their relevant statistical information over the given KG instead of human experts to generate time constraints. Specifically, PaTeCon dynamically attaches class restriction to candidate constraints according to their measuring scores.We evaluate PaTeCon on two large-scale datasets based on Wikidata and Freebase respectively. The experimental results show that pattern-based automatic constraint mining is powerful in generating valuable temporal constraints.\n        \u25b3 Less\n      ",
    "title": "PaTeCon: A Pattern-Based Temporal Constraint Mining Method for Conflict Detection on Knowledge Graphs",
    "date": "12 May, 2023",
    "authors": [
      "Jianhao Chen",
      " Junyang Ren",
      " Wentao Ding",
      " Yuzhong Qu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07546",
    "paper_id": "2305.07546",
    "abstract": "\n        Automatic differentiation, also known as backpropagation, AD, autodiff, or algorithmic differentiation, is a popular technique for computing derivatives of computer programs accurately and efficiently. Sometimes, however, the derivatives computed by AD could be interpreted as incorrect. These pitfalls occur systematically across tools and approaches. In this paper we broadly categorize problematic usages of AD and illustrate each category with examples such as chaos, time-averaged oscillations, discretizations, fixed-point loops, lookup tables, and linear solvers. We also review debugging techniques and their effectiveness in these situations. With this article we hope to help readers avoid unexpected behavior, detect problems more easily when they occur, and have more realistic expectations from AD tools.\n        \u25b3 Less\n      ",
    "title": "Understanding Automatic Differentiation Pitfalls",
    "date": "12 May, 2023",
    "authors": [
      "Jan H\u00fcckelheim",
      " Harshitha Menon",
      " William Moses",
      " Bruce Christianson",
      " Paul Hovland",
      " Laurent Hasco\u00ebt"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07552",
    "paper_id": "2305.07552",
    "abstract": "\n        Diet is central to the epidemic of lifestyle disorders. Accurate and effortless diet logging is one of the significant bottlenecks for effective diet management and calorie restriction. Dish detection from food platters is a challenging problem due to a visually complex food layout. We present an end-to-end computational framework for diet management, from data compilation, annotation, and state-of-the-art model identification to its mobile app implementation. As a case study, we implement the framework in the context of Indian food platters known for their complex presentation that poses a challenge for the automated detection of dishes. Starting with the 61 most popular Indian dishes, we identify the state-of-the-art model through a comparative analysis of deep-learning-based object detection architectures. Rooted in a meticulous compilation of 68,005 platter images with 134,814 manual dish annotations, we first compare ten architectures for multi-label classification to identify ResNet152 (mAP=84.51%) as the best model. YOLOv8x (mAP=87.70%) emerged as the best model architecture for dish detection among the eight deep-learning models implemented after a thorough performance evaluation. By comparing with the state-of-the-art model for the IndianFood10 dataset, we demonstrate the superior object detection performance of YOLOv8x for this subset and establish Resnet152 as the best architecture for multi-label classification. The models thus trained on richly annotated data can be extended to include dishes from across global cuisines. The proposed framework is demonstrated through a proof-of-concept mobile application with diverse applications for diet logging, food recommendation systems, nutritional interventions, and mitigation of lifestyle disorders.\n        \u25b3 Less\n      ",
    "title": "Dish detection in food platters: A framework for automated diet logging and nutrition management",
    "date": "12 May, 2023",
    "authors": [
      "Mansi Goel",
      " Shashank Dargar",
      " Shounak Ghatak",
      " Nidhi Verma",
      " Pratik Chauhan",
      " Anushka Gupta",
      " Nikhila Vishnumolakala",
      " Hareesh Amuru",
      " Ekta Gambhir",
      " Ronak Chhajed",
      " Meenal Jain",
      " Astha Jain",
      " Samiksha Garg",
      " Nitesh Narwade",
      " Nikhilesh Verhwani",
      " Abhuday Tiwari",
      " Kirti Vashishtha",
      " Ganesh Bagler"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07565",
    "paper_id": "2305.07565",
    "abstract": "\n        Existing question answering methods often assume that the input content (e.g., documents or videos) is always accessible to solve the task. Alternatively, memory networks were introduced to mimic the human process of incremental comprehension and compression of the information in a fixed-capacity memory. However, these models only learn how to maintain memory by backpropagating errors in the answers through the entire network. Instead, it has been suggested that humans have effective mechanisms to boost their memorization capacities, such as rehearsal and anticipation. Drawing inspiration from these, we propose a memory model that performs rehearsal and anticipation while processing inputs to memorize important information for solving question answering tasks from streaming data. The proposed mechanisms are applied self-supervised during training through masked modeling tasks focused on coreference information. We validate our model on a short-sequence (bAbI) dataset as well as large-sequence textual (NarrativeQA) and video (ActivityNet-QA) question answering datasets, where it achieves substantial improvements over previous memory network approaches. Furthermore, our ablation study confirms the proposed mechanisms' importance for memory models.\n        \u25b3 Less\n      ",
    "title": "A Memory Model for Question Answering from Streaming Data Supported by Rehearsal and Anticipation of Coreference Information",
    "date": "12 May, 2023",
    "authors": [
      "Vladimir Araujo",
      " Alvaro Soto",
      " Marie-Francine Moens"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07594",
    "paper_id": "2305.07594",
    "abstract": "\n        This paper presents an approach that evaluates best-first search methods to code refactoring. The motivation for code refactoring could be to improve the design, structure, or implementation of an existing program without changing its functionality. To solve a very specific problem of coupling and cohesion, we propose using heuristic search-based techniques on an approximation of the full code refactoring problem, to guide the refactoring process toward solutions that have high cohesion and low coupling. We evaluated our approach by providing demonstrative examples of the effectiveness of this approach on random state problems and created a tool to implement the algorithm on Java projects.\n        \u25b3 Less\n      ",
    "title": "Opti Code Pro: A Heuristic Search-based Approach to Code Refactoring",
    "date": "12 May, 2023",
    "authors": [
      "Sourena Khanzadeh",
      " Samad Alias Nyein Chan",
      " Richard Valenzano",
      " Manar Alalfi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07613",
    "paper_id": "2305.07613",
    "abstract": "\n        Training Generative adversarial networks (GANs) stably is a challenging task. The generator in GANs transform noise vectors, typically Gaussian distributed, into realistic data such as images. In this paper, we propose a novel approach for training GANs with images as inputs, but without enforcing any pairwise constraints. The intuition is that images are more structured than noise, which the generator can leverage to learn a more robust transformation. The process can be made efficient by identifying closely related datasets, or a ``friendly neighborhood'' of the target distribution, inspiring the moniker, Spider GAN. To define friendly neighborhoods leveraging proximity between datasets, we propose a new measure called the signed inception distance (SID), inspired by the polyharmonic kernel. We show that the Spider GAN formulation results in faster convergence, as the generator can discover correspondence even between seemingly unrelated datasets, for instance, between Tiny-ImageNet and CelebA faces. Further, we demonstrate cascading Spider GAN, where the output distribution from a pre-trained GAN generator is used as the input to the subsequent network. Effectively, transporting one distribution to another in a cascaded fashion until the target is learnt -- a new flavor of transfer learning. We demonstrate the efficacy of the Spider approach on DCGAN, conditional GAN, PGGAN, StyleGAN2 and StyleGAN3. The proposed approach achieves state-of-the-art Frechet inception distance (FID) values, with one-fifth of the training iterations, in comparison to their baseline counterparts on high-resolution small datasets such as MetFaces, Ukiyo-E Faces and AFHQ-Cats.\n        \u25b3 Less\n      ",
    "title": "Spider GAN: Leveraging Friendly Neighbors to Accelerate GAN Training",
    "date": "12 May, 2023",
    "authors": [
      "Siddarth Asokan",
      " Chandra Sekhar Seelamantula"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07632",
    "paper_id": "2305.07632",
    "abstract": "\n        Socially assistive robots are increasingly being explored to improve the engagement of older adults and people with disability in health and well-being-related exercises. However, even if people have various physical conditions, most prior work on social robot exercise coaching systems has utilized generic, predefined feedback. The deployment of these systems still remains a challenge. In this paper, we present our work of iteratively engaging therapists and post-stroke survivors to design, develop, and evaluate a social robot exercise coaching system for personalized rehabilitation. Through interviews with therapists, we designed how this system interacts with the user and then developed an interactive social robot exercise coaching system. This system integrates a neural network model with a rule-based model to automatically monitor and assess patients' rehabilitation exercises and can be tuned with individual patient's data to generate real-time, personalized corrective feedback for improvement. With the dataset of rehabilitation exercises from 15 post-stroke survivors, we demonstrated our system significantly improves its performance to assess patients' exercises while tuning with held-out patient's data. In addition, our real-world evaluation study showed that our system can adapt to new participants and achieved 0.81 average performance to assess their exercises, which is comparable to the experts' agreement level. We further discuss the potential benefits and limitations of our system in practice.\n        \u25b3 Less\n      ",
    "title": "Design, Development, and Evaluation of an Interactive Personalized Social Robot to Monitor and Coach Post-Stroke Rehabilitation Exercises",
    "date": "12 May, 2023",
    "authors": [
      "Min Hun Lee",
      " Daniel P. Siewiorek",
      " Asim Smailagic",
      " Alexandre Bernardino",
      " Sergi Berm\u00fadez i Badia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07633",
    "paper_id": "2305.07633",
    "abstract": "\n        Existing recommender systems face difficulties with zero-shot items, i.e. items that have no historical interactions with users during the training stage. Though recent works extract universal item representation via pre-trained language models (PLMs), they ignore the crucial item relationships. This paper presents a novel paradigm for the Zero-Shot Item-based Recommendation (ZSIR) task, which pre-trains a model on product knowledge graph (PKG) to refine the item features from PLMs. We identify three challenges for pre-training PKG, which are multi-type relations in PKG, semantic divergence between item generic information and relations and domain discrepancy from PKG to downstream ZSIR task. We address the challenges by proposing four pre-training tasks and novel task-oriented adaptation (ToA) layers. Moreover, this paper discusses how to fine-tune the model on new recommendation task such that the ToA layers are adapted to ZSIR task. Comprehensive experiments on 18 markets dataset are conducted to verify the effectiveness of the proposed model in both knowledge prediction and ZSIR task.\n        \u25b3 Less\n      ",
    "title": "Zero-shot Item-based Recommendation via Multi-task Product Knowledge Graph Pre-Training",
    "date": "12 May, 2023",
    "authors": [
      "Ziwei Fan",
      " Zhiwei Liu",
      " Shelby Heinecke",
      " Jianguo Zhang",
      " Huan Wang",
      " Caiming Xiong",
      " Philip S. Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07642",
    "paper_id": "2305.07642",
    "abstract": "\n        Meningiomas are the most common primary intracranial tumor in adults and can be associated with significant morbidity and mortality. Radiologists, neurosurgeons, neuro-oncologists, and radiation oncologists rely on multiparametric MRI (mpMRI) for diagnosis, treatment planning, and longitudinal treatment monitoring; yet automated, objective, and quantitative tools for non-invasive assessment of meningiomas on mpMRI are lacking. The BraTS meningioma 2023 challenge will provide a community standard and benchmark for state-of-the-art automated intracranial meningioma segmentation models based on the largest expert annotated multilabel meningioma mpMRI dataset to date. Challenge competitors will develop automated segmentation models to predict three distinct meningioma sub-regions on MRI including enhancing tumor, non-enhancing tumor core, and surrounding nonenhancing T2/FLAIR hyperintensity. Models will be evaluated on separate validation and held-out test datasets using standardized metrics utilized across the BraTS 2023 series of challenges including the Dice similarity coefficient and Hausdorff distance. The models developed during the course of this challenge will aid in incorporation of automated meningioma MRI segmentation into clinical practice, which will ultimately improve care of patients with meningioma.\n        \u25b3 Less\n      ",
    "title": "The ASNR-MICCAI Brain Tumor Segmentation (BraTS) Challenge 2023: Intracranial Meningioma",
    "date": "12 May, 2023",
    "authors": [
      "Dominic LaBella",
      " Maruf Adewole",
      " Michelle Alonso-Basanta",
      " Talissa Altes",
      " Syed Muhammad Anwar",
      " Ujjwal Baid",
      " Timothy Bergquist",
      " Radhika Bhalerao",
      " Sully Chen",
      " Verena Chung",
      " Gian-Marco Conte",
      " Farouk Dako",
      " James Eddy",
      " Ivan Ezhov",
      " Devon Godfrey",
      " Fathi Hilal",
      " Ariana Familiar",
      " Keyvan Farahani",
      " Juan Eugenio Iglesias",
      " Zhifan Jiang",
      " Elaine Johanson",
      " Anahita Fathi Kazerooni",
      " Collin Kent",
      " John Kirkpatrick",
      " Florian Kofler ",
      " et al. (35 additional authors not shown)"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07709",
    "paper_id": "2305.07709",
    "abstract": "\n        This article details the advances made to a system that uses artificial intelligence to identify alarming student responses. This system is built into our assessment platform to assess whether a student's response indicates they are a threat to themselves or others. Such responses may include details concerning threats of violence, severe depression, suicide risks, and descriptions of abuse. Driven by advances in natural language processing, the latest model is a fine-tuned language model trained on a large corpus consisting of student responses and supplementary texts. We demonstrate that the use of a language model delivers a substantial improvement in accuracy over the previous iterations of this system.\n        \u25b3 Less\n      ",
    "title": "Using Language Models to Detect Alarming Student Responses",
    "date": "12 May, 2023",
    "authors": [
      "Christopher M. Ormerod",
      " Milan Patel",
      " Harry Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07716",
    "paper_id": "2305.07716",
    "abstract": "\n        Long-horizon task planning is essential for the development of intelligent assistive and service robots. In this work, we investigate the applicability of a smaller class of large language models (LLMs), specifically GPT-2, in robotic task planning by learning to decompose tasks into subgoal specifications for a planner to execute sequentially. Our method grounds the input of the LLM on the domain that is represented as a scene graph, enabling it to translate human requests into executable robot plans, thereby learning to reason over long-horizon tasks, as encountered in the ALFRED benchmark. We compare our approach with classical planning and baseline methods to examine the applicability and generalizability of LLM-based planners. Our findings suggest that the knowledge stored in an LLM can be effectively grounded to perform long-horizon task planning, demonstrating the promising potential for the future application of neuro-symbolic planning methods in robotics.\n        \u25b3 Less\n      ",
    "title": "Learning to Reason over Scene Graphs: A Case Study of Finetuning GPT-2 into a Robot Language Model for Grounded Task Planning",
    "date": "12 May, 2023",
    "authors": [
      "Georgia Chalvatzaki",
      " Ali Younes",
      " Daljeet Nandha",
      " An Le",
      " Leonardo F. R. Ribeiro",
      " Iryna Gurevych"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08874",
    "paper_id": "2305.08874",
    "abstract": "\n        Quantifying forecast uncertainty is a key aspect of state-of-the-art numerical weather prediction and data assimilation systems. Ensemble-based data assimilation systems incorporate state-dependent uncertainty quantification based on multiple model integrations. However, this approach is demanding in terms of computations and development. In this work a machine learning method is presented based on convolutional neural networks that estimates the state-dependent forecast uncertainty represented by the forecast error covariance matrix using a single dynamical model integration. This is achieved by the use of a loss function that takes into account the fact that the forecast errors are heterodastic. The performance of this approach is examined within a hybrid data assimilation method that combines a Kalman-like analysis update and the machine learning based estimation of a state-dependent forecast error covariance matrix. Observing system simulation experiments are conducted using the Lorenz'96 model as a proof-of-concept. The promising results show that the machine learning method is able to predict precise values of the forecast covariance matrix in relatively high-dimensional states. Moreover, the hybrid data assimilation method shows similar performance to the ensemble Kalman filter outperforming it when the ensembles are relatively small.\n        \u25b3 Less\n      ",
    "title": "Online machine-learning forecast uncertainty estimation for sequential data assimilation",
    "date": "12 May, 2023",
    "authors": [
      "Maximiliano A. Sacco",
      " Manuel Pulido",
      " Juan J. Ruiz",
      " Pierre Tandeo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07763",
    "paper_id": "2305.07763",
    "abstract": "\n        Knowledge representation and reasoning (KRR) systems describe and reason with complex concepts and relations in the form of facts and rules. Unfortunately, wide deployment of KRR systems runs into the problem that domain experts have great difficulty constructing correct logical representations of their domain knowledge. Knowledge engineers can help with this construction process, but there is a deficit of such specialists. The earlier Knowledge Authoring Logic Machine (KALM) based on Controlled Natural Language (CNL) was shown to have very high accuracy for authoring facts and questions. More recently, KALMFL, a successor of KALM, replaced CNL with factual English, which is much less restrictive and requires very little training from users. However, KALMFL has limitations in representing certain types of knowledge, such as authoring rules for multi-step reasoning or understanding actions with timestamps. To address these limitations, we propose KALMRA to enable authoring of rules and actions. Our evaluation using the UTI guidelines benchmark shows that KALMRA achieves a high level of correctness (100%) on rule authoring. When used for authoring and reasoning with actions, KALMRA achieves more than 99.3% correctness on the bAbI benchmark, demonstrating its effectiveness in more sophisticated KRR jobs. Finally, we illustrate the logical reasoning capabilities of KALMRA by drawing attention to the problems faced by the recently made famous AI, ChatGPT.\n        \u25b3 Less\n      ",
    "title": "Knowledge Authoring for Rules and Actions",
    "date": "12 May, 2023",
    "authors": [
      "Yuheng Wang",
      " Paul Fodor",
      " Michael Kifer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2106.01288",
    "paper_id": "2106.01288",
    "abstract": "\n        While Moore's law has driven exponential computing power expectations, its nearing end calls for new avenues for improving the overall system performance. One of these avenues is the exploration of alternative brain-inspired computing architectures that aim at achieving the flexibility and computational efficiency of biological neural processing systems. Within this context, neuromorphic engineering represents a paradigm shift in computing based on the implementation of spiking neural network architectures in which processing and memory are tightly co-located. In this paper, we provide a comprehensive overview of the field, highlighting the different levels of granularity at which this paradigm shift is realized and comparing design approaches that focus on replicating natural intelligence (bottom-up) versus those that aim at solving practical artificial intelligence applications (top-down). First, we present the analog, mixed-signal and digital circuit design styles, identifying the boundary between processing and memory through time multiplexing, in-memory computation, and novel devices. Then, we highlight the key tradeoffs for each of the bottom-up and top-down design approaches, survey their silicon implementations, and carry out detailed comparative analyses to extract design guidelines. Finally, we identify necessary synergies and missing elements required to achieve a competitive advantage for neuromorphic systems over conventional machine-learning accelerators in edge computing applications, and outline the key ingredients for a framework toward neuromorphic intelligence.\n        \u25b3 Less\n      ",
    "title": "Bottom-up and top-down approaches for the design of neuromorphic processing systems: Tradeoffs and synergies between natural and artificial intelligence",
    "date": "12 May, 2023",
    "authors": [
      "Charlotte Frenkel",
      " David Bol",
      " Giacomo Indiveri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07791",
    "paper_id": "2305.07791",
    "abstract": "\n        In this work, we consider the task of automated emphasis detection for spoken language. This problem is challenging in that emphasis is affected by the particularities of speech of the subject, for example the subject accent, dialect or voice. To address this task, we propose to utilize deep fake technology to produce an emphasis devoid speech for this speaker. This requires extracting the text of the spoken voice, and then using a voice sample from the same speaker to produce emphasis devoid speech for this task. By comparing the generated speech with the spoken voice, we are able to isolate patterns of emphasis which are relatively easy to detect.\n        \u25b3 Less\n      ",
    "title": "Using Deepfake Technologies for Word Emphasis Detection",
    "date": "12 May, 2023",
    "authors": [
      "Eran Kaufman",
      " Lee-Ad Gottlieb"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.12948",
    "paper_id": "2302.12948",
    "abstract": "\n        The application of computer vision to nuanced subjective use cases is growing. While crowdsourcing has served the vision community well for most objective tasks (such as labeling a \"zebra\"), it now falters on tasks where there is substantial subjectivity in the concept (such as identifying \"gourmet tuna\"). However, empowering any user to develop a classifier for their concept is technically difficult: users are neither machine learning experts, nor have the patience to label thousands of examples. In reaction, we introduce the problem of Agile Modeling: the process of turning any subjective visual concept into a computer vision model through a real-time user-in-the-loop interactions. We instantiate an Agile Modeling prototype for image classification and show through a user study (N=14) that users can create classifiers with minimal effort under 30 minutes. We compare this user driven process with the traditional crowdsourcing paradigm and find that the crowd's notion often differs from that of the user's, especially as the concepts become more subjective. Finally, we scale our experiments with simulations of users training classifiers for ImageNet21k categories to further demonstrate the efficacy.\n        \u25b3 Less\n      ",
    "title": "Agile Modeling: From Concept to Classifier in Minutes",
    "date": "12 May, 2023",
    "authors": [
      "Otilia Stretcu",
      " Edward Vendrow",
      " Kenji Hata",
      " Krishnamurthy Viswanathan",
      " Vittorio Ferrari",
      " Sasan Tavakkol",
      " Wenlei Zhou",
      " Aditya Avinash",
      " Enming Luo",
      " Neil Gordon Alldrin",
      " MohammadHossein Bateni",
      " Gabriel Berger",
      " Andrew Bunner",
      " Chun-Ta Lu",
      " Javier A Rey",
      " Giulia DeSalvo",
      " Ranjay Krishna",
      " Ariel Fuxman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.08742",
    "paper_id": "2304.08742",
    "abstract": "\n        Enabling robots to learn novel visuomotor skills in a data-efficient manner remains an unsolved problem with myriad challenges. A popular paradigm for tackling this problem is through leveraging large unlabeled datasets that have many behaviors in them and then adapting a policy to a specific task using a small amount of task-specific human supervision (i.e. interventions or demonstrations). However, how best to leverage the narrow task-specific supervision and balance it with offline data remains an open question. Our key insight in this work is that task-specific data not only provides new data for an agent to train on but can also inform the type of prior data the agent should use for learning. Concretely, we propose a simple approach that uses a small amount of downstream expert data to selectively query relevant behaviors from an offline, unlabeled dataset (including many sub-optimal behaviors). The agent is then jointly trained on the expert and queried data. We observe that our method learns to query only the relevant transitions to the task, filtering out sub-optimal or task-irrelevant data. By doing so, it is able to learn more effectively from the mix of task-specific and offline data compared to naively mixing the data or only using the task-specific data. Furthermore, we find that our simple querying approach outperforms more complex goal-conditioned methods by 20% across simulated and real robotic manipulation tasks from images. See https://sites.google.com/view/behaviorretrieval for videos and code.\n        \u25b3 Less\n      ",
    "title": "Behavior Retrieval: Few-Shot Imitation Learning by Querying Unlabeled Datasets",
    "date": "12 May, 2023",
    "authors": [
      "Maximilian Du",
      " Suraj Nair",
      " Dorsa Sadigh",
      " Chelsea Finn"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07814",
    "paper_id": "2305.07814",
    "abstract": "\n        The networks for point cloud tasks are expected to be invariant when the point clouds are affinely transformed such as rotation and reflection. So far, relative to the rotational invariance that has been attracting major research attention in the past years, the reflection invariance is little addressed. Notwithstanding, reflection symmetry can find itself in very common and important scenarios, e.g., static reflection symmetry of structured streets, dynamic reflection symmetry from bidirectional motion of moving objects (such as pedestrians), and left- and right-hand traffic practices in different countries. To the best of our knowledge, unfortunately, no reflection-invariant network has been reported in point cloud analysis till now. To fill this gap, we propose a framework by using quadratic neurons and PCA canonical representation, referred to as Cloud-RAIN, to endow point \\underline{Cloud} models with \\underline{R}eflection\\underline{A}l \\underline{IN}variance. We prove a theorem to explain why Cloud-RAIN can enjoy reflection symmetry. Furthermore, extensive experiments also corroborate the reflection property of the proposed Cloud-RAIN and show that Cloud-RAIN is superior to data augmentation. Our code is available at https://github.com/YimingCuiCuiCui/Cloud-RAIN.\n        \u25b3 Less\n      ",
    "title": "Cloud-RAIN: Point Cloud Analysis with Reflectional Invariance",
    "date": "12 May, 2023",
    "authors": [
      "Yiming Cui",
      " Lecheng Ruan",
      " Hang-Cheng Dong",
      " Qiang Li",
      " Zhongming Wu",
      " Tieyong Zeng",
      " Feng-Lei Fan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07816",
    "paper_id": "2305.07816",
    "abstract": "\n        Pathologic myopia (PM) is a common blinding retinal degeneration suffered by highly myopic population. Early screening of this condition can reduce the damage caused by the associated fundus lesions and therefore prevent vision loss. Automated diagnostic tools based on artificial intelligence methods can benefit this process by aiding clinicians to identify disease signs or to screen mass populations using color fundus photographs as inputs. This paper provides insights about PALM, our open fundus imaging dataset for pathological myopia recognition and anatomical structure annotation. Our databases comprises 1200 images with associated labels for the pathologic myopia category and manual annotations of the optic disc, the position of the fovea and delineations of lesions such as patchy retinal atrophy (including peripapillary atrophy) and retinal detachment. In addition, this paper elaborates on other details such as the labeling process used to construct the database, the quality and characteristics of the samples and provides other relevant usage notes.\n        \u25b3 Less\n      ",
    "title": "PALM: Open Fundus Photograph Dataset with Pathologic Myopia Recognition and Anatomical Structure Annotation",
    "date": "12 May, 2023",
    "authors": [
      "Huihui Fang",
      " Fei Li",
      " Junde Wu",
      " Huazhu Fu",
      " Xu Sun",
      " Jos\u00e9 Ignacio Orlando",
      " Hrvoje Bogunovi\u0107",
      " Xiulan Zhang",
      " Yanwu Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07824",
    "paper_id": "2305.07824",
    "abstract": "\n        Generating proper embedding of sentences through an unsupervised way is beneficial to semantic matching and retrieval problems in real-world scenarios. This paper presents Representation ALchemy (RepAL), an extremely simple post-processing method that enhances sentence representations. The basic idea in RepAL is to de-emphasize redundant information of sentence embedding generated by pre-trained models. Through comprehensive experiments, we show that RepAL is free of training and is a plug-and-play method that can be combined with most existing unsupervised sentence learning models. We also conducted in-depth analysis to understand RepAL.\n        \u25b3 Less\n      ",
    "title": "A Simple and Plug-and-play Method for Unsupervised Sentence Representation Enhancement",
    "date": "12 May, 2023",
    "authors": [
      "Lingfeng Shen",
      " Haiyun Jiang",
      " Lemao Liu",
      " Shuming Shi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.10583",
    "paper_id": "2303.10583",
    "abstract": "\n        In the early stages of the design process, designers explore opportunities by discovering unmet needs and developing innovative concepts as potential solutions. From a human-centered design perspective, designers must develop empathy with people to truly understand their needs. However, developing empathy is a complex and subjective process that relies heavily on the designer's empathic capability. Therefore, the development of empathic understanding is intuitive, and the discovery of underlying needs is often serendipitous. This paper aims to provide insights from artificial intelligence research to indicate the future direction of AI-driven human-centered design, taking into account the essential role of empathy. Specifically, we conduct an interdisciplinary investigation of research areas such as data-driven user studies, empathic understanding development, and artificial empathy. Based on this foundation, we discuss the role that artificial empathy can play in human-centered design and propose an artificial empathy framework for human-centered design. Building on the mechanisms behind empathy and insights from empathic design research, the framework aims to break down the rather complex and subjective concept of empathy into components and modules that can potentially be modeled computationally. Furthermore, we discuss the expected benefits of developing such systems and identify current research gaps to encourage future research efforts.\n        \u25b3 Less\n      ",
    "title": "Toward Artificial Empathy for Human-Centered Design: A Framework",
    "date": "12 May, 2023",
    "authors": [
      "Qihao Zhu",
      " Jianxi Luo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07856",
    "paper_id": "2305.07856",
    "abstract": "\n        Asynchronous action coordination presents a pervasive challenge in Multi-Agent Systems (MAS), which can be represented as a Stackelberg game (SG). However, the scalability of existing Multi-Agent Reinforcement Learning (MARL) methods based on SG is severely constrained by network structures or environmental limitations. To address this issue, we propose the Stackelberg Decision Transformer (STEER), a heuristic approach that resolves the difficulties of hierarchical coordination among agents. STEER efficiently manages decision-making processes in both spatial and temporal contexts by incorporating the hierarchical decision structure of SG, the modeling capability of autoregressive sequence models, and the exploratory learning methodology of MARL. Our research contributes to the development of an effective and adaptable asynchronous action coordination method that can be widely applied to various task types and environmental configurations in MAS. Experimental results demonstrate that our method can converge to Stackelberg equilibrium solutions and outperforms other existing methods in complex scenarios.\n        \u25b3 Less\n      ",
    "title": "Stackelberg Decision Transformer for Asynchronous Action Coordination in Multi-Agent Systems",
    "date": "12 May, 2023",
    "authors": [
      "Bin Zhang",
      " Hangyu Mao",
      " Lijuan Li",
      " Zhiwei Xu",
      " Dapeng Li",
      " Rui Zhao",
      " Guoliang Fan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.08143",
    "paper_id": "2301.08143",
    "abstract": "\n        Federated recommendation is a new Internet service architecture that aims to provide privacy-preserving recommendation services in federated settings. Existing solutions are used to combine distributed recommendation algorithms and privacy-preserving mechanisms. Thus it inherently takes the form of heavyweight models at the server and hinders the deployment of on-device intelligent models to end-users. This paper proposes a novel Personalized Federated Recommendation (PFedRec) framework to learn many user-specific lightweight models to be deployed on smart devices rather than a heavyweight model on a server. Moreover, we propose a new dual personalization mechanism to effectively learn fine-grained personalization on both users and items. The overall learning process is formulated into a unified federated optimization framework. Specifically, unlike previous methods that share exactly the same item embeddings across users in a federated system, dual personalization allows mild finetuning of item embeddings for each user to generate user-specific views for item representations which can be integrated into existing federated recommendation methods to gain improvements immediately. Experiments on multiple benchmark datasets have demonstrated the effectiveness of PFedRec and the dual personalization mechanism. Moreover, we provide visualizations and in-depth analysis of the personalization techniques in item embedding, which shed novel insights on the design of recommender systems in federated settings. The code is available.\n        \u25b3 Less\n      ",
    "title": "Dual Personalization on Federated Recommendation",
    "date": "12 May, 2023",
    "authors": [
      "Chunxu Zhang",
      " Guodong Long",
      " Tianyi Zhou",
      " Peng Yan",
      " Zijian Zhang",
      " Chengqi Zhang",
      " Bo Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07868",
    "paper_id": "2305.07868",
    "abstract": "\n        The rapid proliferation of information in the digital era underscores the importance of accurate historical representation and interpretation. While artificial intelligence has shown promise in various fields, its potential for historical fact-checking and gap-filling remains largely untapped. This study evaluates the performance of three large language models LLMs GPT 3.5, GPT 4, and GoogleBARD in the context of predicting and verifying historical events based on given data. A novel metric, Distance to Reality (DTR), is introduced to assess the models' outputs against established historical facts. The results reveal a substantial potential for AI in historical studies, with GPT 4 demonstrating superior performance. This paper underscores the need for further research into AI's role in enriching our understanding of the past and bridging historical knowledge gaps.\n        \u25b3 Less\n      ",
    "title": "Bridging History with AI A Comparative Evaluation of GPT 3.5, GPT4, and GoogleBARD in Predictive Accuracy and Fact Checking",
    "date": "13 May, 2023",
    "authors": [
      "Davut Emre Tasar",
      " Ceren Ocal Tasar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07871",
    "paper_id": "2305.07871",
    "abstract": "\n        The automatic generation of educational questions will play a key role in scaling online education, enabling self-assessment at scale when a global population is manoeuvring their personalised learning journeys. We develop \\textit{EduQG}, a novel educational question generation model built by adapting a large language model. Our extensive experiments demonstrate that \\textit{EduQG} can produce superior educational questions by further pre-training and fine-tuning a pre-trained language model on the scientific text and science question data.\n        \u25b3 Less\n      ",
    "title": "Scalable Educational Question Generation with Pre-trained Language Models",
    "date": "13 May, 2023",
    "authors": [
      "Sahan Bulathwela",
      " Hamze Muse",
      " Emine Yilmaz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08879",
    "paper_id": "2305.08879",
    "abstract": "\n        In recent years, newly developed methods to train spiking neural networks (SNNs) have rendered them as a plausible alternative to Artificial Neural Networks (ANNs) in terms of accuracy, while at the same time being much more energy efficient at inference and potentially at training time. However, it is still unclear what constitutes a good initialisation for an SNN. We often use initialisation schemes developed for ANN training which are often inadequate and require manual tuning. In this paper, we attempt to tackle this issue by using techniques from the ANN initialisation literature as well as computational neuroscience results. We show that the problem of weight initialisation for ANNs is a more nuanced problem than it is for ANNs due to the spike-and-reset non-linearity of SNNs and the firing rate collapse problem. We firstly identify and propose several solutions to the firing rate collapse problem under different sets of assumptions which successfully solve the issue by leveraging classical random walk and Wiener processes results. Secondly, we devise a general strategy for SNN initialisation which combines variance propagation techniques from ANNs and different methods to obtain the expected firing rate and membrane potential distribution based on diffusion and shot-noise approximations. Altogether, we obtain theoretical results to solve the SNN initialisation which consider the membrane potential distribution in the presence of a threshold. Yet, to what extent can these methods be successfully applied to SNNs on real datasets remains an open question.\n        \u25b3 Less\n      ",
    "title": "Spiking Network Initialisation and Firing Rate Collapse",
    "date": "13 May, 2023",
    "authors": [
      "Nicolas Perez-Nieves",
      " Dan F. M Goodman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09684",
    "paper_id": "2305.09684",
    "abstract": "\n        Typically, foundation models are hosted on cloud servers to meet the high demand for their services. However, this exposes them to security risks, as attackers can modify them after uploading to the cloud or transferring from a local system. To address this issue, we propose an iterative decision-based fragile watermarking algorithm that transforms normal training samples into fragile samples that are sensitive to model changes. We then compare the output of sensitive samples from the original model to that of the compromised model during validation to assess the model's completeness.The proposed fragile watermarking algorithm is an optimization problem that aims to minimize the variance of the predicted probability distribution outputed by the target model when fed with the converted sample.We convert normal samples to fragile samples through multiple iterations. Our method has some advantages: (1) the iterative update of samples is done in a decision-based black-box manner, relying solely on the predicted probability distribution of the target model, which reduces the risk of exposure to adversarial attacks, (2) the small-amplitude multiple iterations approach allows the fragile samples to perform well visually, with a PSNR of 55 dB in TinyImageNet compared to the original samples, (3) even with changes in the overall parameters of the model of magnitude 1e-4, the fragile samples can detect such changes, and (4) the method is independent of the specific model structure and dataset. We demonstrate the effectiveness of our method on multiple models and datasets, and show that it outperforms the current state-of-the-art.\n        \u25b3 Less\n      ",
    "title": "Decision-based iterative fragile watermarking for model integrity verification",
    "date": "13 May, 2023",
    "authors": [
      "Zhaoxia Yin",
      " Heng Yin",
      " Hang Su",
      " Xinpeng Zhang",
      " Zhenzhe Gao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07892",
    "paper_id": "2305.07892",
    "abstract": "\n        Meta learning recently has been heavily researched and helped advance the contemporary machine learning. However, achieving well-performing meta-learning model requires a large amount of training tasks with high-quality meta-data representing the underlying task generalization goal, which is sometimes difficult and expensive to obtain for real applications. Current meta-data-driven meta-learning approaches, however, are fairly hard to train satisfactory meta-models with imperfect training tasks. To address this issue, we suggest a meta-knowledge informed meta-learning (MKIML) framework to improve meta-learning by additionally integrating compensated meta-knowledge into meta-learning process. We preliminarily integrate meta-knowledge into meta-objective via using an appropriate meta-regularization (MR) objective to regularize capacity complexity of the meta-model function class to facilitate better generalization on unseen tasks. As a practical implementation, we introduce data augmentation consistency to encode invariance as meta-knowledge for instantiating MR objective, denoted by DAC-MR. The proposed DAC-MR is hopeful to learn well-performing meta-models from training tasks with noisy, sparse or unavailable meta-data. We theoretically demonstrate that DAC-MR can be treated as a proxy meta-objective used to evaluate meta-model without high-quality meta-data. Besides, meta-data-driven meta-loss objective combined with DAC-MR is capable of achieving better meta-level generalization. 10 meta-learning tasks with different network architectures and benchmarks substantiate the capability of our DAC-MR on aiding meta-model learning. Fine performance of DAC-MR are obtained across all settings, and are well-aligned with our theoretical insights. This implies that our DAC-MR is problem-agnostic, and hopeful to be readily applied to extensive meta-learning problems and tasks.\n        \u25b3 Less\n      ",
    "title": "DAC-MR: Data Augmentation Consistency Based Meta-Regularization for Meta-Learning",
    "date": "13 May, 2023",
    "authors": [
      "Jun Shu",
      " Xiang Yuan",
      " Deyu Meng",
      " Zongben Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.01744",
    "paper_id": "2306.01744",
    "abstract": "\n        The advances in Machine Learning (ML) in recent years have been both impressive and far-reaching. However, the deployment of ML models is still impaired by a lack of trust in how the best-performing ML models make predictions. The issue of lack of trust is even more acute in the uses of ML models in high-risk or safety-critical domains. eXplainable artificial intelligence (XAI) is at the core of ongoing efforts for delivering trustworthy AI. Unfortunately, XAI is riddled with critical misconceptions, that foster distrust instead of building trust. This paper details some of the most visible misconceptions in XAI, and shows how formal methods have been used, both to disprove those misconceptions, but also to devise practically effective alternatives.\n        \u25b3 Less\n      ",
    "title": "Disproving XAI Myths with Formal Methods -- Initial Results",
    "date": "13 May, 2023",
    "authors": [
      "Joao Marques-Silva"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07903",
    "paper_id": "2305.07903",
    "abstract": "\n        We describe a translation from a fragment of SUMO (SUMO-K) into higher-order set theory. The translation provides a formal semantics for portions of SUMO which are beyond first-order and which have previously only had an informal interpretation. It also for the first time embeds a large common-sense ontology into a very secure interactive theorem proving system. We further extend our previous work in finding contradictions in SUMO from first order constructs to include a portion of SUMO's higher order constructs. Finally, using the translation, we can create problems that can be proven using higher-order interactive and automated theorem provers. This is tested in several systems and can be used to form a corpus of higher-order common-sense reasoning problems.\n        \u25b3 Less\n      ",
    "title": "Translating SUMO-K to Higher-Order Set Theory",
    "date": "13 May, 2023",
    "authors": [
      "Chad Brown",
      " Adam Pease",
      " Josef Urban"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07908",
    "paper_id": "2305.07908",
    "abstract": "\n        Hardware implementation of neural network are an essential step to implement next generation efficient and powerful artificial intelligence solutions.\n  Besides the realization of a parallel, efficient and scalable hardware architecture, the optimization of the system's extremely large parameter space with sampling-efficient approaches is essential.\n  Here, we analytically derive the scaling laws for highly efficient Coordinate Descent applied to optimizing the readout layer of a random recurrently connection neural network, a reservoir.\n  We demonstrate that the convergence is exponential and scales linear with the network's number of neurons.\n  Our results perfectly reproduce the convergence and scaling of a large-scale photonic reservoir implemented in a proof-of-concept experiment.\n  Our work therefore provides a solid foundation for such optimization in hardware networks, and identifies future directions that are promising for optimizing convergence speed during learning leveraging measures of a neural network's amplitude statistics and the weight update rule.\n        \u25b3 Less\n      ",
    "title": "Convergence and scaling of Boolean-weight optimization for hardware reservoirs",
    "date": "13 May, 2023",
    "authors": [
      "Louis Andreoli",
      " St\u00e9phane Chr\u00e9tien",
      " Xavier Porte",
      " Daniel Brunner"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06217",
    "paper_id": "2305.06217",
    "abstract": "\n        Machine learning (ML) in healthcare presents numerous opportunities for enhancing patient care, population health, and healthcare providers' workflows. However, the real-world clinical and cost benefits remain limited due to challenges in data privacy, heterogeneous data sources, and the inability to fully leverage multiple data modalities. In this perspective paper, we introduce \"patchwork learning\" (PL), a novel paradigm that addresses these limitations by integrating information from disparate datasets composed of different data modalities (e.g., clinical free-text, medical images, omics) and distributed across separate and secure sites. PL allows the simultaneous utilization of complementary data sources while preserving data privacy, enabling the development of more holistic and generalizable ML models. We present the concept of patchwork learning and its current implementations in healthcare, exploring the potential opportunities and applicable data sources for addressing various healthcare challenges. PL leverages bridging modalities or overlapping feature spaces across sites to facilitate information sharing and impute missing data, thereby addressing related prediction tasks. We discuss the challenges associated with PL, many of which are shared by federated and multimodal learning, and provide recommendations for future research in this field. By offering a more comprehensive approach to healthcare data integration, patchwork learning has the potential to revolutionize the clinical applicability of ML models. This paradigm promises to strike a balance between personalization and generalizability, ultimately enhancing patient experiences, improving population health, and optimizing healthcare providers' workflows.\n        \u25b3 Less\n      ",
    "title": "Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources",
    "date": "13 May, 2023",
    "authors": [
      "Suraj Rajendran",
      " Weishen Pan",
      " Mert R. Sabuncu",
      " Yong Chen",
      " Jiayu Zhou",
      " Fei Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2208.08160",
    "paper_id": "2208.08160",
    "abstract": "\n        Spatial models of preference, in the form of vector embeddings, are learned by many deep learning and multiagent systems, including recommender systems. Often these models are assumed to approximate a Euclidean structure, where an individual prefers alternatives positioned closer to their \"ideal point\", as measured by the Euclidean metric. However, Bogomolnaia and Laslier (2007) showed that there exist ordinal preference profiles that cannot be represented with this structure if the Euclidean space has two fewer dimensions than there are individuals or alternatives. We extend this result, showing that there are situations in which almost all preference profiles cannot be represented with the Euclidean model, and derive a theoretical lower bound on the expected error when using the Euclidean model to approximate non-Euclidean preference profiles. Our results have implications for the interpretation and use of vector embeddings, because in some cases close approximation of arbitrary, true ordinal relationships can be expected only if the dimensionality of the embeddings is a substantial fraction of the number of entities represented.\n        \u25b3 Less\n      ",
    "title": "Error in the Euclidean Preference Model",
    "date": "13 May, 2023",
    "authors": [
      "Luke Thorburn",
      " Maria Polukarov",
      " Carmine Ventre"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07928",
    "paper_id": "2305.07928",
    "abstract": "\n        Knowledge distillation is of key importance to launching multilingual pre-trained language models for real applications. To support cost-effective language inference in multilingual settings, we propose AMTSS, an adaptive multi-teacher single-student distillation framework, which allows distilling knowledge from multiple teachers to a single student. We first introduce an adaptive learning strategy and teacher importance weight, which enables a student to effectively learn from max-margin teachers and easily adapt to new languages. Moreover, we present a shared student encoder with different projection layers in support of multiple languages, which contributes to largely reducing development and machine cost. Experimental results show that AMTSS gains competitive results on the public XNLI dataset and the realistic industrial dataset AliExpress (AE) in the E-commerce scenario.\n        \u25b3 Less\n      ",
    "title": "AMTSS: An Adaptive Multi-Teacher Single-Student Knowledge Distillation Framework For Multilingual Language Inference",
    "date": "13 May, 2023",
    "authors": [
      "Qianglong Chen",
      " Feng Ji",
      " Feng-Lin Li",
      " Guohai Xu",
      " Ming Yan",
      " Ji Zhang",
      " Yin Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07958",
    "paper_id": "2305.07958",
    "abstract": "\n        In an offline reinforcement learning setting, the safe policy improvement (SPI) problem aims to improve the performance of a behavior policy according to which sample data has been generated. State-of-the-art approaches to SPI require a high number of samples to provide practical probabilistic guarantees on the improved policy's performance. We present a novel approach to the SPI problem that provides the means to require less data for such guarantees. Specifically, to prove the correctness of these guarantees, we devise implicit transformations on the data set and the underlying environment model that serve as theoretical foundations to derive tighter improvement bounds for SPI. Our empirical evaluation, using the well-established SPI with baseline bootstrapping (SPIBB) algorithm, on standard benchmarks shows that our method indeed significantly reduces the sample complexity of the SPIBB algorithm.\n        \u25b3 Less\n      ",
    "title": "More for Less: Safe Policy Improvement With Stronger Performance Guarantees",
    "date": "13 May, 2023",
    "authors": [
      "Patrick Wienh\u00f6ft",
      " Marnix Suilen",
      " Thiago D. Sim\u00e3o",
      " Clemens Dubslaff",
      " Christel Baier",
      " Nils Jansen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07970",
    "paper_id": "2305.07970",
    "abstract": "\n        In this study, we investigate the capacity of large language models (LLMs), specifically GPT-3.5, to operationalise natural language descriptions of cooperative, competitive, altruistic, and self-interested behavior in social dilemmas. Our focus is on the iterated Prisoner's Dilemma, a classic example of a non-zero-sum interaction, but our broader research program encompasses a range of experimental economics scenarios, including the ultimatum game, dictator game, and public goods game. Using a within-subject experimental design, we instantiated LLM-generated agents with various prompts that conveyed different cooperative and competitive stances. We then assessed the agents' level of cooperation in the iterated Prisoner's Dilemma, taking into account their responsiveness to the cooperative or defection actions of their partners. Our results provide evidence that LLMs can translate natural language descriptions of altruism and selfishness into appropriate behaviour to some extent, but exhibit limitations in adapting their behavior based on conditioned reciprocity. The observed pattern of increased cooperation with defectors and decreased cooperation with cooperators highlights potential constraints in the LLM's ability to generalize its knowledge about human behavior in social dilemmas. We call upon the research community to further explore the factors contributing to the emergent behavior of LLM-generated agents in a wider array of social dilemmas, examining the impact of model architecture, training parameters, and various partner strategies on agent behavior. As more advanced LLMs like GPT-4 become available, it is crucial to investigate whether they exhibit similar limitations or are capable of more nuanced cooperative behaviors, ultimately fostering the development of AI systems that better align with human values and social norms.\n        \u25b3 Less\n      ",
    "title": "Investigating Emergent Goal-Like Behaviour in Large Language Models Using Experimental Economics",
    "date": "13 May, 2023",
    "authors": [
      "Steve Phelps",
      " Yvan I. Russell"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07973",
    "paper_id": "2305.07973",
    "abstract": "\n        We investigate whether long-run persistent chain Monte Carlo simulation of Langevin dynamics improves the quality of the representations achieved by energy-based models (EBM). We consider a scheme wherein Monte Carlo simulation of a diffusion process using a trained EBM is used to improve the adversarial robustness and the calibration score of an independent classifier network. Our results show that increasing the computational budget of Gibbs sampling in persistent contrastive divergence improves the calibration and adversarial robustness of the model, elucidating the practical merit of realizing new quantum and classical hardware and software for efficient Gibbs sampling from continuous energy potentials.\n        \u25b3 Less\n      ",
    "title": "On the Computational Cost of Stochastic Security",
    "date": "13 May, 2023",
    "authors": [
      "Noah A. Crum",
      " Leanto Sunny",
      " Pooya Ronagh",
      " Raymond Laflamme",
      " Radhakrishnan Balu",
      " George Siopsis"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.03511",
    "paper_id": "2209.03511",
    "abstract": "\n        Robotic arms are widely used in automatic industries. However, with wide applications of deep learning in robotic arms, there are new challenges such as the allocation of grasping computing power and the growing demand for security. In this work, we propose a robotic arm grasping approach based on deep learning and edge-cloud collaboration. This approach realizes the arbitrary grasp planning of the robot arm and considers the grasp efficiency and information security. In addition, the encoder and decoder trained by GAN enable the images to be encrypted while compressing, which ensures the security of privacy. The model achieves 92% accuracy on the OCID dataset, the image compression ratio reaches 0.03%, and the structural difference value is higher than 0.91.\n        \u25b3 Less\n      ",
    "title": "A Secure and Efficient Multi-Object Grasping Detection Approach for Robotic Arms",
    "date": "13 May, 2023",
    "authors": [
      "Hui Wang",
      " Jieren Cheng",
      " Yichen Xu",
      " Sirui Ni",
      " Zaijia Yang",
      " Jiangpeng Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08005",
    "paper_id": "2305.08005",
    "abstract": "\n        The increasing popularity of large language models (LLMs) such as ChatGPT has led to growing concerns about their safety, security risks, and ethical implications. This paper aims to provide an overview of the different types of security risks associated with ChatGPT, including malicious text and code generation, private data disclosure, fraudulent services, information gathering, and producing unethical content. We present an empirical study examining the effectiveness of ChatGPT's content filters and explore potential ways to bypass these safeguards, demonstrating the ethical implications and security risks that persist in LLMs even when protections are in place. Based on a qualitative analysis of the security implications, we discuss potential strategies to mitigate these risks and inform researchers, policymakers, and industry professionals about the complex security challenges posed by LLMs like ChatGPT. This study contributes to the ongoing discussion on the ethical and security implications of LLMs, underscoring the need for continued research in this area.\n        \u25b3 Less\n      ",
    "title": "Beyond the Safeguards: Exploring the Security Risks of ChatGPT",
    "date": "13 May, 2023",
    "authors": [
      "Erik Derner",
      " Kristina Batisti\u010d"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.01246",
    "paper_id": "2211.01246",
    "abstract": "\n        In this paper, we propose a new Self-Supervised Learning (SSL) algorithm called data2vec-aqc, for speech representation learning from unlabeled speech data. Our goal is to improve SSL for speech in domains where both unlabeled and labeled data are limited. Building on the recently introduced data2vec, we introduce additional modules to the data2vec framework that leverage the benefit of data augmentations, quantized representations, and clustering. The interaction between these modules helps solve the cross-contrastive loss as an additional self-supervised objective. data2vec-aqc achieves up to 14.1% and 20.9% relative WER improvement over the existing state-of-the-art data2vec system over the test-clean and test-other sets, respectively of LibriSpeech, without the use of any language model (LM). Our proposed model also achieves up to 17.8\\% relative WER gains over the baseline data2vec when fine-tuned on a subset of the Switchboard dataset. Code: https://github.com/Speech-Lab-IITM/data2vec-aqc.\n        \u25b3 Less\n      ",
    "title": "data2vec-aqc: Search for the right Teaching Assistant in the Teacher-Student training setup",
    "date": "13 May, 2023",
    "authors": [
      "Vasista Sai Lodagala",
      " Sreyan Ghosh",
      " S. Umesh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.08278",
    "paper_id": "2301.08278",
    "abstract": "\n        Solving the problem of cooperation is of fundamental importance to the creation and maintenance of functional societies, with examples of cooperative dilemmas ranging from navigating busy road junctions to negotiating carbon reduction treaties. As the use of AI becomes more pervasive throughout society, the need for socially intelligent agents that are able to navigate these complex cooperative dilemmas is becoming increasingly evident. In the natural world, direct punishment is an ubiquitous social mechanism that has been shown to benefit the emergence of cooperation within populations. However no prior work has investigated its impact on the development of cooperation within populations of artificial learning agents experiencing social dilemmas. Additionally, within natural populations the use of any form of punishment is strongly coupled with the related social mechanisms of partner selection and reputation. However, no previous work has considered the impact of combining multiple social mechanisms on the emergence of cooperation in multi-agent systems. Therefore, in this paper we present a comprehensive analysis of the behaviours and learning dynamics associated with direct punishment in multi-agent reinforcement learning systems and how it compares to third-party punishment, when both are combined with the related social mechanisms of partner selection and reputation. We provide an extensive and systematic evaluation of the impact of these key mechanisms on the dynamics of the strategies learned by agents. Finally, we discuss the implications of the use of these mechanisms on the design of cooperative AI systems.\n        \u25b3 Less\n      ",
    "title": "Investigating the Impact of Direct Punishment on the Emergence of Cooperation in Multi-Agent Reinforcement Learning Systems",
    "date": "13 May, 2023",
    "authors": [
      "Nayana Dasgupta",
      " Mirco Musolesi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.00893",
    "paper_id": "2212.00893",
    "abstract": "\n        Many dynamical systems -- from robots interacting with their surroundings to large-scale multiphysics systems -- involve a number of interacting subsystems. Toward the objective of learning composite models of such systems from data, we present i) a framework for compositional neural networks, ii) algorithms to train these models, iii) a method to compose the learned models, iv) theoretical results that bound the error of the resulting composite models, and v) a method to learn the composition itself, when it is not known a priori. The end result is a modular approach to learning: neural network submodels are trained on trajectory data generated by relatively simple subsystems, and the dynamics of more complex composite systems are then predicted without requiring additional data generated by the composite systems themselves. We achieve this compositionality by representing the system of interest, as well as each of its subsystems, as a port-Hamiltonian neural network (PHNN) -- a class of neural ordinary differential equations that uses the port-Hamiltonian systems formulation as inductive bias. We compose collections of PHNNs by using the system's physics-informed interconnection structure, which may be known a priori, or may itself be learned from data. We demonstrate the novel capabilities of the proposed framework through numerical examples involving interacting spring-mass-damper systems. Models of these systems, which include nonlinear energy dissipation and control inputs, are learned independently. Accurate compositions are learned using an amount of training data that is negligible in comparison with that required to train a new model from scratch. Finally, we observe that the composite PHNNs enjoy properties of port-Hamiltonian systems, such as cyclo-passivity -- a property that is useful for control purposes.\n        \u25b3 Less\n      ",
    "title": "Compositional Learning of Dynamical System Models Using Port-Hamiltonian Neural Networks",
    "date": "13 May, 2023",
    "authors": [
      "Cyrus Neary",
      " Ufuk Topcu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.01105",
    "paper_id": "2207.01105",
    "abstract": "\n        While constructing polar codes for successive-cancellation decoding can be implemented efficiently by sorting the bit-channels, finding optimal polar codes for cyclic-redundancy-check-aided successive-cancellation list (CA-SCL) decoding in an efficient and scalable manner still awaits investigation. This paper first maps a polar code to a unique heterogeneous graph called the polar-code-construction message-passing (PCCMP) graph. Next, a heterogeneous graph-neural-network-based iterative message-passing (IMP) algorithm is proposed which aims to find a PCCMP graph that corresponds to the polar code with minimum frame error rate under CA-SCL decoding. This new IMP algorithm's major advantage lies in its scalability power. That is, the model complexity is independent of the blocklength and code rate, and a trained IMP model over a short polar code can be readily applied to a long polar code's construction. Numerical experiments show that IMP-based polar-code constructions outperform classical constructions under CA-SCL decoding. In addition, when an IMP model trained on a length-128 polar code directly applies to the construction of polar codes with different code rates and blocklengths, simulations show that these polar code constructions deliver comparable performance to the 5G polar codes.\n        \u25b3 Less\n      ",
    "title": "Scalable Polar Code Construction for Successive Cancellation List Decoding: A Graph Neural Network-Based Approach",
    "date": "13 May, 2023",
    "authors": [
      "Yun Liao",
      " Seyyed Ali Hashemi",
      " Hengjie Yang",
      " John M. Cioffi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08031",
    "paper_id": "2305.08031",
    "abstract": "\n        Privacy and confidentiality of medical data are of utmost importance in healthcare settings. ViTs, the SOTA vision model, rely on large amounts of patient data for training, which raises concerns about data security and the potential for unauthorized access. Adversaries may exploit vulnerabilities in ViTs to extract sensitive patient information and compromising patient privacy. This work address these vulnerabilities to ensure the trustworthiness and reliability of ViTs in medical applications. In this work, we introduced a defensive diffusion technique as an adversarial purifier to eliminate adversarial noise introduced by attackers in the original image. By utilizing the denoising capabilities of the diffusion model, we employ a reverse diffusion process to effectively eliminate the adversarial noise from the attack sample, resulting in a cleaner image that is then fed into the ViT blocks. Our findings demonstrate the effectiveness of the diffusion model in eliminating attack-agnostic adversarial noise from images. Additionally, we propose combining knowledge distillation with our framework to obtain a lightweight student model that is both computationally efficient and robust against gray box attacks. Comparison of our method with a SOTA baseline method, SEViT, shows that our work is able to outperform the baseline. Extensive experiments conducted on a publicly available Tuberculosis X-ray dataset validate the computational efficiency and improved robustness achieved by our proposed architecture.\n        \u25b3 Less\n      ",
    "title": "On enhancing the robustness of Vision Transformers: Defensive Diffusion",
    "date": "13 May, 2023",
    "authors": [
      "Raza Imam",
      " Muhammad Huzaifa",
      " Mohammed El-Amine Azz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08034",
    "paper_id": "2305.08034",
    "abstract": "\n        With deep learning deployed in many security-sensitive areas, machine learning security is becoming progressively important. Recent studies demonstrate attackers can exploit system-level techniques exploiting the RowHammer vulnerability of DRAM to deterministically and precisely flip bits in Deep Neural Networks (DNN) model weights to affect inference accuracy. The existing defense mechanisms are software-based, such as weight reconstruction requiring expensive training overhead or performance degradation. On the other hand, generic hardware-based victim-/aggressor-focused mechanisms impose expensive hardware overheads and preserve the spatial connection between victim and aggressor rows. In this paper, we present the first DRAM-based victim-focused defense mechanism tailored for quantized DNNs, named DNN-Defender that leverages the potential of in-DRAM swapping to withstand the targeted bit-flip attacks. Our results indicate that DNN-Defender can deliver a high level of protection downgrading the performance of targeted RowHammer attacks to a random attack level. In addition, the proposed defense has no accuracy drop on CIFAR-10 and ImageNet datasets without requiring any software training or incurring additional hardware overhead.\n        \u25b3 Less\n      ",
    "title": "DNN-Defender: An in-DRAM Deep Neural Network Defense Mechanism for Adversarial Weight Attack",
    "date": "13 May, 2023",
    "authors": [
      "Ranyang Zhou",
      " Sabbir Ahmed",
      " Adnan Siraj Rakin",
      " Shaahin Angizi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.08465",
    "paper_id": "2204.08465",
    "abstract": "\n        The weather phenomenon of frost poses great threats to agriculture. As recent frost prediction methods are based on on-site historical data and sensors, extra development and deployment time are required for data collection in any new site. The aim of this article is to eliminate the dependency on on-site historical data and sensors for frost prediction methods. In this article, a frost prediction method based on spatial interpolation is proposed. The models use climate data from existing weather stations, digital elevation models surveys, and normalized difference vegetation index data to estimate a target site's next hour minimum temperature. The proposed method utilizes ensemble learning to increase the model accuracy. Climate datasets are obtained from 75 weather stations across New South Wales and Australian Capital Territory areas of Australia. The results show that the proposed method reached a detection rate up to 92.55%.\n        \u25b3 Less\n      ",
    "title": "Intelligent Spatial Interpolation-based Frost Prediction Methodology using Artificial Neural Networks with Limited Local Data",
    "date": "13 May, 2023",
    "authors": [
      "Ian Zhou",
      " Justin Lipman",
      " Mehran Abolhasan",
      " Negin Shariati"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.06318",
    "paper_id": "2303.06318",
    "abstract": "\n        Mixture-of-Experts (MoE) is a neural network architecture that adds sparsely activated expert blocks to a base model, increasing the number of parameters without impacting computational costs. However, current distributed deep learning frameworks are limited in their ability to train high-quality MoE models with large base models. In this work, we present DeepSpeed-TED, a novel, three-dimensional, hybrid parallel algorithm that combines data, tensor, and expert parallelism to enable the training of MoE models with 4 to 8x larger base models than the current state-of-the-art. We also describe memory optimizations in the optimizer step, and communication optimizations that eliminate unnecessary data movement. We implement our approach in DeepSpeed and achieve speedups of 26% over a baseline (i.e. without our communication optimizations) when training a 40 billion parameter MoE model (6.7 billion base model with 16 experts) on 128 V100 GPUs.\n        \u25b3 Less\n      ",
    "title": "A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training",
    "date": "13 May, 2023",
    "authors": [
      "Siddharth Singh",
      " Olatunji Ruwase",
      " Ammar Ahmad Awan",
      " Samyam Rajbhandari",
      " Yuxiong He",
      " Abhinav Bhatele"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08048",
    "paper_id": "2305.08048",
    "abstract": "\n        Graph neural networks (GNNs) are the most widely adopted model in graph-structured data oriented learning and representation. Despite their extraordinary success in real-world applications, understanding their working mechanism by theory is still on primary stage. In this paper, we move towards this goal from the perspective of generalization. To be specific, we first establish high probability bounds of generalization gap and gradients in transductive learning with consideration of stochastic optimization. After that, we provide high probability bounds of generalization gap for popular GNNs. The theoretical results reveal the architecture specific factors affecting the generalization gap. Experimental results on benchmark datasets show the consistency between theoretical results and empirical evidence. Our results provide new insights in understanding the generalization of GNNs.\n        \u25b3 Less\n      ",
    "title": "Towards Understanding the Generalization of Graph Neural Networks",
    "date": "13 May, 2023",
    "authors": [
      "Huayi Tang",
      " Yong Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08059",
    "paper_id": "2305.08059",
    "abstract": "\n        Event-Level Video Question Answering (EVQA) requires complex reasoning across video events to obtain the visual information needed to provide optimal answers. However, despite significant progress in model performance, few studies have focused on using the explicit semantic connections between the question and visual information especially at the event level. There is need for using such semantic connections to facilitate complex reasoning across video frames. Therefore, we propose a semantic-aware dynamic retrospective-prospective reasoning approach for video-based question answering. Specifically, we explicitly use the Semantic Role Labeling (SRL) structure of the question in the dynamic reasoning process where we decide to move to the next frame based on which part of the SRL structure (agent, verb, patient, etc.) of the question is being focused on. We conduct experiments on a benchmark EVQA dataset - TrafficQA. Results show that our proposed approach achieves superior performance compared to previous state-of-the-art models. Our code will be made publicly available for research use.\n        \u25b3 Less\n      ",
    "title": "Semantic-aware Dynamic Retrospective-Prospective Reasoning for Event-level Video Question Answering",
    "date": "13 May, 2023",
    "authors": [
      "Chenyang Lyu",
      " Tianbo Ji",
      " Yvette Graham",
      " Jennifer Foster"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08060",
    "paper_id": "2305.08060",
    "abstract": "\n        Simulation-based testing represents an important step to ensure the reliability of autonomous driving software. In practice, when companies rely on third-party general-purpose simulators, either for in-house or outsourced testing, the generalizability of testing results to real autonomous vehicles is at stake.\n  In this paper, we strengthen simulation-based testing by introducing the notion of digital siblings, a novel framework in which the AV is tested on multiple general-purpose simulators, built with different technologies. First, test cases are automatically generated for each individual simulator. Then, tests are migrated between simulators, using feature maps to characterize of the exercised driving conditions. Finally, the joint predicted failure probability is computed and a failure is reported only in cases of agreement among the siblings.\n  We implemented our framework using two open-source simulators and we empirically compared it against a digital twin of a physical scaled autonomous vehicle on a large set of test cases. Our study shows that the ensemble failure predictor by the digital siblings is superior to each individual simulator at predicting the failures of the digital twin. We discuss several ways in which our framework can help researchers interested in automated testing of autonomous driving software.\n        \u25b3 Less\n      ",
    "title": "Two is Better Than One: Digital Siblings to Improve Autonomous Driving Testing",
    "date": "13 May, 2023",
    "authors": [
      "Matteo Biagiola",
      " Andrea Stocco",
      " Vincenzo Riccio",
      " Paolo Tonella"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.05045",
    "paper_id": "2302.05045",
    "abstract": "\n        Parallel training of neural networks at scale is challenging due to significant overheads arising from communication. Recently, deep learning researchers have developed a variety of pruning algorithms that are capable of pruning (i.e. setting to zero) 80-90% of the parameters in a neural network to yield sparse subnetworks that equal the accuracy of the unpruned parent network. In this work, we propose a novel approach that exploits these sparse subnetworks to optimize the memory utilization and communication in two popular algorithms for parallel deep learning namely -- data and inter-layer parallelism. We integrate our approach into AxoNN, a highly scalable framework for parallel deep learning that relies on data and inter-layer parallelism, and demonstrate the reduction in communication time and memory utilization. On 512 NVIDIA V100 GPUs, our optimizations reduce the memory consumption of a 2.7 billion parameter model by 74%, and the total communication time by 40%, thus providing an overall speedup of 34% over AxoNN, 32% over DeepSpeed-3D and 46% over Sputnik, a sparse matrix computation baseline.\n        \u25b3 Less\n      ",
    "title": "Exploiting Sparsity in Pruned Neural Networks to Optimize Large Model Training",
    "date": "13 May, 2023",
    "authors": [
      "Siddharth Singh",
      " Abhinav Bhatele"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2110.13005",
    "paper_id": "2110.13005",
    "abstract": "\n        In the last few years, the memory requirements to train state-of-the-art neural networks have far exceeded the DRAM capacities of modern hardware accelerators. This has necessitated the development of efficient algorithms to train these neural networks in parallel on large-scale GPU-based clusters. Since computation is relatively inexpensive on modern GPUs, designing and implementing extremely efficient communication in these parallel training algorithms is critical for extracting the maximum performance. This paper presents AxoNN, a parallel deep learning framework that exploits asynchrony and message-driven execution to schedule neural network operations on each GPU, thereby reducing GPU idle time and maximizing hardware efficiency. By using the CPU memory as a scratch space for offloading data periodically during training, AxoNN is able to reduce GPU memory consumption by four times. This allows us to increase the number of parameters per GPU by four times, thus reducing the amount of communication and increasing performance by over 13%. When tested against large transformer models with 12-100 billion parameters on 48-384 NVIDIA Tesla V100 GPUs, AxoNN achieves a per-GPU throughput of 49.4-54.78% of theoretical peak and reduces the training time by 22-37 days (15-25% speedup) as compared to the state-of-the-art.\n        \u25b3 Less\n      ",
    "title": "AxoNN: An asynchronous, message-driven parallel framework for extreme-scale deep learning",
    "date": "13 May, 2023",
    "authors": [
      "Siddharth Singh",
      " Abhinav Bhatele"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2206.08406",
    "paper_id": "2206.08406",
    "abstract": "\n        Tweets are the most concise form of communication in online social media, wherein a single tweet has the potential to make or break the discourse of the conversation. Online hate speech is more accessible than ever, and stifling its propagation is of utmost importance for social media companies and users for congenial communication. Most of the research barring a recent few has focused on classifying an individual tweet regardless of the tweet thread/context leading up to that point. One of the classical approaches to curb hate speech is to adopt a reactive strategy after the hate speech postage. The ex-post facto strategy results in neglecting subtle posts that do not show the potential to instigate hate speech on their own but may portend in the subsequent discussion ensuing in the post's replies. In this paper, we propose DRAGNET++, which aims to predict the intensity of hatred that a tweet can bring in through its reply chain in the future. It uses the semantic and propagating structure of the tweet threads to maximize the contextual information leading up to and the fall of hate intensity at each subsequent tweet. We explore three publicly available Twitter datasets -- Anti-Racism contains the reply tweets of a collection of social media discourse on racist remarks during US political and Covid-19 background; Anti-Social presents a dataset of 40 million tweets amidst the COVID-19 pandemic on anti-social behaviours; and Anti-Asian presents Twitter datasets collated based on anti-Asian behaviours during COVID-19 pandemic. All the curated datasets consist of structural graph information of the Tweet threads. We show that DRAGNET++ outperforms all the state-of-the-art baselines significantly. It beats the best baseline by an 11% margin on the Person correlation coefficient and a decrease of 25% on RMSE for the Anti-Racism dataset with a similar performance on the other two datasets.\n        \u25b3 Less\n      ",
    "title": "Predicting Hate Intensity of Twitter Conversation Threads",
    "date": "13 May, 2023",
    "authors": [
      "Qing Meng",
      " Tharun Suresh",
      " Roy Ka-Wei Lee",
      " Tanmoy Chakraborty"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08088",
    "paper_id": "2305.08088",
    "abstract": "\n        Large language models (LLMs) have shown increasing power on various natural language processing (NLP) tasks. However, tuning these models for downstream tasks usually needs exorbitant costs or is unavailable due to commercial considerations. Recently, black-box tuning has been proposed to address this problem by optimizing task-specific prompts without accessing the gradients and hidden representations. However, most existing works have yet fully exploited the potential of gradient-free optimization under the scenario of few-shot learning. In this paper, we describe BBT-RGB, a suite of straightforward and complementary techniques for enhancing the efficiency and performance of black-box optimization. Specifically, our method includes three plug-and-play components: (1) Two-stage derivative-free optimization strategy that facilitates fast convergence and mitigates overfitting; (2) Automatic verbalizer construction with its novel usage under few-shot settings; (3) Better prompt initialization policy based on instruction search and auto-selected demonstration. Extensive experiments across various tasks on natural language understanding and inference demonstrate the effectiveness of our method. Our codes are publicly available at https://github.com/QiushiSun/BBT-RGB.\n        \u25b3 Less\n      ",
    "title": "Make Prompt-based Black-Box Tuning Colorful: Boosting Model Generalization from Three Orthogonal Perspectives",
    "date": "13 May, 2023",
    "authors": [
      "Qiushi Sun",
      " Chengcheng Han",
      " Nuo Chen",
      " Renyu Zhu",
      " Jingyang Gong",
      " Xiang Li",
      " Ming Gao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08883",
    "paper_id": "2305.08883",
    "abstract": "\n        LLMs now exhibit human-like skills in various fields, leading to worries about misuse. Thus, detecting generated text is crucial. However, passive detection methods are stuck in domain specificity and limited adversarial robustness. To achieve reliable detection, a watermark-based method was proposed for white-box LLMs, allowing them to embed watermarks during text generation. The method involves randomly dividing the model vocabulary to obtain a special list and adjusting the probability distribution to promote the selection of words in the list. A detection algorithm aware of the list can identify the watermarked text. However, this method is not applicable in many real-world scenarios where only black-box language models are available. For instance, third-parties that develop API-based vertical applications cannot watermark text themselves because API providers only supply generated text and withhold probability distributions to shield their commercial interests. To allow third-parties to autonomously inject watermarks into generated text, we develop a watermarking framework for black-box language model usage scenarios. Specifically, we first define a binary encoding function to compute a random binary encoding corresponding to a word. The encodings computed for non-watermarked text conform to a Bernoulli distribution, wherein the probability of a word representing bit-1 being approximately 0.5. To inject a watermark, we alter the distribution by selectively replacing words representing bit-0 with context-based synonyms that represent bit-1. A statistical test is then used to identify the watermark. Experiments demonstrate the effectiveness of our method on both Chinese and English datasets. Furthermore, results under re-translation, polishing, word deletion, and synonym substitution attacks reveal that it is arduous to remove the watermark without compromising the original semantics.\n        \u25b3 Less\n      ",
    "title": "Watermarking Text Generated by Black-Box Language Models",
    "date": "13 May, 2023",
    "authors": [
      "Xi Yang",
      " Kejiang Chen",
      " Weiming Zhang",
      " Chang Liu",
      " Yuang Qi",
      " Jie Zhang",
      " Han Fang",
      " Nenghai Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08093",
    "paper_id": "2305.08093",
    "abstract": "\n        This study explores the benefits and challenges of integrating Artificial Intelligence with Agile software development methodologies, focusing on improving continuous integration and delivery. A systematic literature review and longitudinal meta-analysis of the retrieved studies was conducted to analyse the role of Artificial Intelligence and it's future applications within Agile software development. The review helped identify critical challenges, such as the need for specialised socio-technical expertise. While Artificial Intelligence holds promise for improved software development practices, further research is needed to better understand its impact on processes and practitioners, and to address the indirect challenges associated with its implementation.\n        \u25b3 Less\n      ",
    "title": "AI for Agile development: a Meta-Analysis",
    "date": "13 May, 2023",
    "authors": [
      "Beatriz Cabrero-Daniel"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08096",
    "paper_id": "2305.08096",
    "abstract": "\n        Knowledge distillation (KD) is a promising technique for model compression in neural machine translation. However, where the knowledge hides in KD is still not clear, which may hinder the development of KD. In this work, we first unravel this mystery from an empirical perspective and show that the knowledge comes from the top-1 predictions of teachers, which also helps us build a potential connection between word- and sequence-level KD. Further, we point out two inherent issues in vanilla word-level KD based on this finding. Firstly, the current objective of KD spreads its focus to whole distributions to learn the knowledge, yet lacks special treatment on the most crucial top-1 information. Secondly, the knowledge is largely covered by the golden information due to the fact that most top-1 predictions of teachers overlap with ground-truth tokens, which further restricts the potential of KD. To address these issues, we propose a novel method named \\textbf{T}op-1 \\textbf{I}nformation \\textbf{E}nhanced \\textbf{K}nowledge \\textbf{D}istillation (TIE-KD). Specifically, we design a hierarchical ranking loss to enforce the learning of the top-1 information from the teacher. Additionally, we develop an iterative KD procedure to infuse more additional knowledge by distilling on the data without ground-truth targets. Experiments on WMT'14 English-German, WMT'14 English-French and WMT'16 English-Romanian demonstrate that our method can respectively boost Transformerbase_{base} students by +1.04, +0.60 and +1.11 BLEU scores and significantly outperform the vanilla word-level KD baseline. Besides, our method shows higher generalizability on different teacher-student capacity gaps than existing KD techniques.\n        \u25b3 Less\n      ",
    "title": "Towards Understanding and Improving Knowledge Distillation for Neural Machine Translation",
    "date": "13 May, 2023",
    "authors": [
      "Songming Zhang",
      " Yunlong Liang",
      " Shuaibo Wang",
      " Wenjuan Han",
      " Jian Liu",
      " Jinan Xu",
      " Yufeng Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08103",
    "paper_id": "2305.08103",
    "abstract": "\n        Boolean functions and their representation through logics, circuits, machine learning classifiers, or binary decision diagrams (BDDs) play a central role in the design and analysis of computing systems. Quantifying the relative impact of variables on the truth value by means of importance values can provide useful insights to steer system design and debugging. In this paper, we introduce a uniform framework for reasoning about such values, relying on a generic notion of importance value functions (IVFs). The class of IVFs is defined by axioms motivated from several notions of importance values introduced in the literature, including Ben-Or and Linial's influence and Chockler, Halpern, and Kupferman's notion of responsibility and blame. We establish a connection between IVFs and game-theoretic concepts such as Shapley and Banzhaf values, both of which measure the impact of players on outcomes in cooperative games. Exploiting BDD-based symbolic methods and projected model counting, we devise and evaluate practical computation schemes for IVFs.\n        \u25b3 Less\n      ",
    "title": "A Unifying Formal Approach to Importance Values in Boolean Functions",
    "date": "13 May, 2023",
    "authors": [
      "Hans Harder",
      " Simon Jantsch",
      " Christel Baier",
      " Clemens Dubslaff"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08104",
    "paper_id": "2305.08104",
    "abstract": "\n        Federated learning (FL) has recently gained much attention due to its effectiveness in speeding up supervised learning tasks under communication and privacy constraints. However, whether similar speedups can be established for reinforcement learning remains much less understood theoretically. Towards this direction, we study a federated policy evaluation problem where agents communicate via a central aggregator to expedite the evaluation of a common policy. To capture typical communication constraints in FL, we consider finite capacity up-link channels that can drop packets based on a Bernoulli erasure model. Given this setting, we propose and analyze QFedTD - a quantized federated temporal difference learning algorithm with linear function approximation. Our main technical contribution is to provide a finite-sample analysis of QFedTD that (i) highlights the effect of quantization and erasures on the convergence rate; and (ii) establishes a linear speedup w.r.t. the number of agents under Markovian sampling. Notably, while different quantization mechanisms and packet drop models have been extensively studied in the federated learning, distributed optimization, and networked control systems literature, our work is the first to provide a non-asymptotic analysis of their effects in multi-agent and federated reinforcement learning.\n        \u25b3 Less\n      ",
    "title": "Federated TD Learning over Finite-Rate Erasure Channels: Linear Speedup under Markovian Sampling",
    "date": "13 May, 2023",
    "authors": [
      "Nicol\u00f2 Dal Fabbro",
      " Aritra Mitra",
      " George J. Pappas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08112",
    "paper_id": "2305.08112",
    "abstract": "\n        The review analyzes the fundamental principles which Artificial Intelligence should be based on in order to imitate the realistic process of taking decisions by humans experiencing emotions. Two approaches are compared, one based on quantum theory and the other employing classical terms. Both these approaches have a number of similarities, being principally probabilistic. The analogies between quantum measurements under intrinsic noise and affective decision making are elucidated. It is shown that cognitive processes have many features that are formally similar to quantum measurements. This, however, in no way means that for the imitation of human decision making Affective Artificial Intelligence has necessarily to rely on the functioning of quantum systems. Appreciating the common features between quantum measurements and decision making helps for the formulation of an axiomatic approach employing only classical notions. Artificial Intelligence, following this approach, operates similarly to humans, by taking into account the utility of the considered alternatives as well as their emotional attractiveness. Affective Artificial Intelligence, whose operation takes account of the cognition-emotion duality, avoids numerous behavioural paradoxes of traditional decision making. A society of intelligent agents, interacting through the repeated multistep exchange of information, forms a network accomplishing dynamic decision making. The considered intelligent networks can characterize the operation of either a human society of affective decision makers, or the brain composed of neurons, or a typical probabilistic network of an artificial intelligence.\n        \u25b3 Less\n      ",
    "title": "Quantum Operation of Affective Artificial Intelligence",
    "date": "13 May, 2023",
    "authors": [
      "V. I. Yukalov"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2208.08845",
    "paper_id": "2208.08845",
    "abstract": "\n        Empathetic conversation is psychologically supposed to be the result of conscious alignment and interaction between the cognition and affection of empathy. However, existing empathetic dialogue models usually consider only the affective aspect or treat cognition and affection in isolation, which limits the capability of empathetic response generation. In this work, we propose the CASE model for empathetic dialogue generation. It first builds upon a commonsense cognition graph and an emotional concept graph and then aligns the user's cognition and affection at both the coarse-grained and fine-grained levels. Through automatic and manual evaluation, we demonstrate that CASE outperforms state-of-the-art baselines of empathetic dialogues and can generate more empathetic and informative responses.\n        \u25b3 Less\n      ",
    "title": "CASE: Aligning Coarse-to-Fine Cognition and Affection for Empathetic Response Generation",
    "date": "13 May, 2023",
    "authors": [
      "Jinfeng Zhou",
      " Chujie Zheng",
      " Bo Wang",
      " Zheng Zhang",
      " Minlie Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10443",
    "paper_id": "2305.10443",
    "abstract": "\n        Fully autonomous driving has been widely studied and is becoming increasingly feasible. However, such autonomous driving has yet to be achieved on public roads, because of various uncertainties due to surrounding human drivers and pedestrians. In this paper, we present an end-to-end learningbased autonomous driving system named SuperDriver AI, where Deep Neural Networks (DNNs) learn the driving actions and policies from the experienced human drivers and determine the driving maneuvers to take while guaranteeing road safety. In addition, to improve robustness and interpretability, we present a slit model and a visual attention module. We build a datacollection system and emulator with real-world hardware, and we also test the SuperDriver AI system with real-world driving scenarios. Finally, we have collected 150 runs for one driving scenario in Tokyo, Japan, and have shown the demonstration of SuperDriver AI with the real-world vehicle.\n        \u25b3 Less\n      ",
    "title": "SuperDriverAI: Towards Design and Implementation for End-to-End Learning-based Autonomous Driving",
    "date": "13 May, 2023",
    "authors": [
      "Shunsuke Aoki",
      " Issei Yamamoto",
      " Daiki Shiotsuka",
      " Yuichi Inoue",
      " Kento Tokuhiro",
      " Keita Miwa"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08124",
    "paper_id": "2305.08124",
    "abstract": "\n        Credit assignment problems, for example policy evaluation in RL, often require bootstrapping prediction errors through preceding states \\textit{or} maintaining temporally extended memory traces; solutions which are unfavourable or implausible for biological networks of neurons. We propose theta sequences -- chains of neural activity during theta oscillations in the hippocampus, thought to represent rapid playthroughs of awake behaviour -- as a solution. By analysing and simulating a model for theta sequences we show they compress behaviour such that existing but short O(10)\\mathsf{O}(10) ms neuronal memory traces are effectively extended allowing for bootstrap-free credit assignment without long memory traces, equivalent to the use of eligibility traces in TD(\u03bb\u03bb).\n        \u25b3 Less\n      ",
    "title": "Theta sequences as eligibility traces: a biological solution to credit assignment",
    "date": "13 May, 2023",
    "authors": [
      "Tom M George"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11896",
    "paper_id": "2305.11896",
    "abstract": "\n        The extension of legacy business process automation beyond the bounds of specific processes is known as hyperautomation. Hyperautomation provides automation for nearly any repetitive action performed by business users by combining AI tools with RPA. It automates complex IT business processes that a company's top brains might not be able to complete. This is an end-to-end automation of a standard business process deployment. It enables automation to perform task digitalization by combining a brain computer interface (BCI) with AI and RPA automation tools. BCI, in conjunction with automation tools, will advance the detection and generation of automation processes to the next level. It allows enterprises to combine business intelligence systems, address complex requirements, and enhance human expertise and automation experience. Hyperautomation and its importance in today's environment are briefly discussed in this paper. The article then goes on to discuss how BCI and sensors might aid Hyperautomation. The specific sectors of solicitations were examined using a variety of flexible technologies associated to this concept, as well as dedicated workflow techniques, which are also diagrammatically illustrated. Hyperautomation is being utilized to improve the efficiency, accuracy, and human enhancement of automated tasks dramatically. It incorporates a number of automated tools in its discovery, implementation, and automation phases. As a result, it's well-suited to integrating cutting-edge technologies and experimenting with new methods of working. Keywords- Hyperautomation, Brain computer Interface (BCI), Technology, Used case, Sensors, Industries.\n        \u25b3 Less\n      ",
    "title": "Hyper-automation-The next peripheral for automation in IT industries",
    "date": "13 May, 2023",
    "authors": [
      "Ayush Singh Rajput",
      " Richa Gupta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08130",
    "paper_id": "2305.08130",
    "abstract": "\n        In this work, we propose a novel inverse reinforcement learning (IRL) algorithm for constrained Markov decision process (CMDP) problems. In standard IRL problems, the inverse learner or agent seeks to recover the reward function of the MDP, given a set of trajectory demonstrations for the optimal policy. In this work, we seek to infer not only the reward functions of the CMDP, but also the constraints. Using the principle of maximum entropy, we show that the IRL with constraint recovery (IRL-CR) problem can be cast as a constrained non-convex optimization problem. We reduce it to an alternating constrained optimization problem whose sub-problems are convex. We use exponentiated gradient descent algorithm to solve it. Finally, we demonstrate the efficacy of our algorithm for the grid world environment.\n        \u25b3 Less\n      ",
    "title": "Inverse Reinforcement Learning With Constraint Recovery",
    "date": "13 May, 2023",
    "authors": [
      "Nirjhar Das",
      " Arpan Chattopadhyay"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.00717",
    "paper_id": "2306.00717",
    "abstract": "\n        Recently, deep neural networks have emerged as a solution to solve NP-hard wireless resource allocation problems in real-time. However, multi-layer perceptron (MLP) and convolutional neural network (CNN) structures, which are inherited from image processing tasks, are not optimized for wireless network problems. As network size increases, these methods get harder to train and generalize. User pairing is one such essential NP-hard optimization problem in wireless communication systems that entails selecting users to be scheduled together while minimizing interference and maximizing throughput. In this paper, we propose an unsupervised graph neural network (GNN) approach to efficiently solve the user pairing problem. Our proposed method utilizes the Erdos goes neural pipeline to significantly outperform other scheduling methods such as k-means and semi-orthogonal user scheduling (SUS). At 20 dB SNR, our proposed approach achieves a 49% better sum rate than k-means and a staggering 95% better sum rate than SUS while consuming minimal time and resources. The scalability of the proposed method is also explored as our model can handle dynamic changes in network size without experiencing a substantial decrease in performance. Moreover, our model can accomplish this without being explicitly trained for larger or smaller networks facilitating a dynamic functionality that cannot be achieved using CNNs or MLPs.\n        \u25b3 Less\n      ",
    "title": "Graph Neural Networks-Based User Pairing in Wireless Communication Systems",
    "date": "13 May, 2023",
    "authors": [
      "Sharan Mourya",
      " Pavan Reddy",
      " SaiDhiraj Amuru",
      " Kiran Kumar Kuchi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08146",
    "paper_id": "2305.08146",
    "abstract": "\n        Lexical substitution (LS) aims at finding appropriate substitutes for a target word in a sentence. Recently, LS methods based on pretrained language models have made remarkable progress, generating potential substitutes for a target word through analysis of its contextual surroundings. However, these methods tend to overlook the preservation of the sentence's meaning when generating the substitutes. This study explores how to generate the substitute candidates from a paraphraser, as the generated paraphrases from a paraphraser contain variations in word choice and preserve the sentence's meaning. Since we cannot directly generate the substitutes via commonly used decoding strategies, we propose two simple decoding strategies that focus on the variations of the target word during decoding. Experimental results show that our methods outperform state-of-the-art LS methods based on pre-trained language models on three benchmarks.\n        \u25b3 Less\n      ",
    "title": "ParaLS: Lexical Substitution via Pretrained Paraphraser",
    "date": "13 May, 2023",
    "authors": [
      "Jipeng Qiang",
      " Kang Liu",
      " Yun Li",
      " Yunhao Yuan",
      " Yi Zhu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.09868",
    "paper_id": "2304.09868",
    "abstract": "\n        This paper proposes a novel framework for accelerating support vector clustering. The proposed method first computes much smaller compressed data sets while preserving the key cluster properties of the original data sets based on a novel spectral data compression approach. Then, the resultant spectrally-compressed data sets are leveraged for the development of fast and high quality algorithm for support vector clustering. We conducted extensive experiments using real-world data sets and obtained very promising results. The proposed method allows us to achieve 100X and 115X speedups over the state of the art SVC method on the Pendigits and USPS data sets, respectively, while achieving even better clustering quality. To the best of our knowledge, this represents the first practical method for high-quality and fast SVC on large-scale real-world data sets\n        \u25b3 Less\n      ",
    "title": "Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression",
    "date": "13 May, 2023",
    "authors": [
      "Yuxuan Song",
      " Yongyu Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10444",
    "paper_id": "2305.10444",
    "abstract": "\n        Given the importance of forests and their role in maintaining the ecological balance, which directly affects the planet, the climate, and the life on this planet, this research presents the problem of forest fire monitoring using drones. The forest monitoring process is performed continuously to track any changes in the monitored region within the forest. During fires, drones' capture data is used to increase the follow-up speed and enhance the control process of these fires to prevent their spread. The time factor in such problems determines the success rate of the fire extinguishing process, as appropriate data at the right time may be the decisive factor in controlling fires, preventing their spread, extinguishing them, and limiting their losses. Therefore, this research presented the problem of monitoring task scheduling for drones in the forest monitoring system. This problem is solved by developing several algorithms with the aim of minimizing the total completion time required to carry out all the drones' assigned tasks. System performance is measured by using 990 instances of three different classes. The performed experimental results indicated the effectiveness of the proposed algorithms and their ability to act efficiently to achieve the desired goal. The algorithm RIDRID achieved the best performance with a percentage rate of up to 90.3% with a time of 0.088 seconds.\n        \u25b3 Less\n      ",
    "title": "Optimizing Forest Fire Prevention: Intelligent Scheduling Algorithms for Drone-Based Surveillance System",
    "date": "13 May, 2023",
    "authors": [
      "Mahdi Jemmali",
      " Loai Kayed B. Melhim",
      " Wadii Boulila",
      " Hajer Amdouni",
      " Mafawez T. Alharbi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.10302",
    "paper_id": "2303.10302",
    "abstract": "\n        Partially Observable Markov Decision Processes (POMDPs) provide an efficient way to model real-world sequential decision making processes. Motivated by the problem of maintenance and inspection of a group of infrastructure components with independent dynamics, this paper presents an algorithm to find the optimal policy for a multi-component budget-constrained POMDP. We first introduce a budgeted-POMDP model (b-POMDP) which enables us to find the optimal policy for a POMDP while adhering to budget constraints. Next, we prove that the value function or maximal collected reward for a b-POMDP is a concave function of the budget for the finite horizon case. Our second contribution is an algorithm to calculate the optimal policy for a multi-component budget-constrained POMDP by finding the optimal budget split among the individual component POMDPs. The optimal budget split is posed as a welfare maximization problem and the solution is computed by exploiting the concave nature of the value function. We illustrate the effectiveness of the proposed algorithm by proposing a maintenance and inspection policy for a group of real-world infrastructure components with different deterioration dynamics, inspection and maintenance costs. We show that the proposed algorithm vastly outperforms the policy currently used in practice.\n        \u25b3 Less\n      ",
    "title": "Welfare Maximization Algorithm for Solving Budget-Constrained Multi-Component POMDPs",
    "date": "13 May, 2023",
    "authors": [
      "Manav Vora",
      " Pranay Thangeda",
      " Michael N. Grussing",
      " Melkior Ornik"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2109.12265",
    "paper_id": "2109.12265",
    "abstract": "\n        The success of deep learning relies heavily on large labeled datasets, but we often only have access to several small datasets associated with partial labels. To address this problem, we propose a new initiative, \"Label-Assemble\", that aims to unleash the full potential of partial labels from an assembly of public datasets. We discovered that learning from negative examples facilitates both computer-aided disease diagnosis and detection. This discovery will be particularly crucial in novel disease diagnosis, where positive examples are hard to collect, yet negative examples are relatively easier to assemble. For example, assembling existing labels from NIH ChestX-ray14 (available since 2017) significantly improves the accuracy of COVID-19 diagnosis from 96.3% to 99.3%. In addition to diagnosis, assembling labels can also improve disease detection, e.g., the detection of pancreatic ductal adenocarcinoma (PDAC) can greatly benefit from leveraging the labels of Cysts and PanNets (two other types of pancreatic abnormalities), increasing sensitivity from 52.1% to 84.0% while maintaining a high specificity of 98.0%.\n        \u25b3 Less\n      ",
    "title": "Label-Assemble: Leveraging Multiple Datasets with Partial Labels",
    "date": "13 May, 2023",
    "authors": [
      "Mintong Kang",
      " Bowen Li",
      " Zengle Zhu",
      " Yongyi Lu",
      " Elliot K. Fishman",
      " Alan L. Yuille",
      " Zongwei Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08178",
    "paper_id": "2305.08178",
    "abstract": "\n        An innovative sort of mobility platform that can both drive and fly is the air-ground robot. The need for an agile flight cannot be satisfied by traditional path planning techniques for air-ground robots. Prior studies had mostly focused on improving the energy efficiency of paths, seldom taking the seeking speed and optimizing take-off and landing places into account. A robot for the field application environment was proposed, and a lightweight global spatial planning technique for the robot based on the graph-search algorithm taking mode switching point optimization into account, with an emphasis on energy efficiency, searching speed, and the viability of real deployment. The fundamental concept is to lower the computational burden by employing an interchangeable search approach that combines planar and spatial search. Furthermore, to safeguard the health of the power battery and the integrity of the mission execution, a trap escape approach was also provided. Simulations are run to test the effectiveness of the suggested model based on the field DEM map. The simulation results show that our technology is capable of producing finished, plausible 3D paths with a high degree of believability. Additionally, the mode-switching point optimization method efficiently identifies additional acceptable places for mode switching, and the improved paths use less time and energy.\n        \u25b3 Less\n      ",
    "title": "Path Planning for Air-Ground Robot Considering Modal Switching Point Optimization",
    "date": "13 May, 2023",
    "authors": [
      "Xiaoyu Wang",
      " Kangyao Huang",
      " Xinyu Zhang",
      " Honglin Sun",
      " Wenzhuo Liu",
      " Huaping Liu",
      " Jun Li",
      " Pingping Lu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08197",
    "paper_id": "2305.08197",
    "abstract": "\n        The generalisation of Neural Networks (NN) to multiple datasets is often overlooked in literature due to NNs typically being optimised for specific data sources. This becomes especially challenging in time-series-based multi-dataset models due to difficulties in fusing sequential data from different sensors and collection specifications. In a commercial environment, however, generalisation can effectively utilise available data and computational power, which is essential in the context of Green AI, the sustainable development of AI models. This paper introduces \"Dataset Fusion,\" a novel dataset composition algorithm for fusing periodic signals from multiple homogeneous datasets into a single dataset while retaining unique features for generalised anomaly detection. The proposed approach, tested on a case study of 3-phase current data from 2 different homogeneous Induction Motor (IM) fault datasets using an unsupervised LSTMCaps NN, significantly outperforms conventional training approaches with an Average F1 score of 0.879 and effectively generalises across all datasets. The proposed approach was also tested with varying percentages of the training data, in line with the principles of Green AI. Results show that using only 6.25\\% of the training data, translating to a 93.7\\% reduction in computational power, results in a mere 4.04\\% decrease in performance, demonstrating the advantages of the proposed approach in terms of both performance and computational efficiency. Moreover, the algorithm's effectiveness under non-ideal conditions highlights its potential for practical use in real-world applications.\n        \u25b3 Less\n      ",
    "title": "A Dataset Fusion Algorithm for Generalised Anomaly Detection in Homogeneous Periodic Time Series Datasets",
    "date": "13 May, 2023",
    "authors": [
      "Ayman Elhalwagy",
      " Tatiana Kalganova"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.14601",
    "paper_id": "2304.14601",
    "abstract": "\n        Recent works reveal that adversarial augmentation benefits the generalization of neural networks (NNs) if used in an appropriate manner. In this paper, we introduce Temporal Adversarial Augmentation (TA), a novel video augmentation technique that utilizes temporal attention. Unlike conventional adversarial augmentation, TA is specifically designed to shift the attention distributions of neural networks with respect to video clips by maximizing a temporal-related loss function. We demonstrate that TA will obtain diverse temporal views, which significantly affect the focus of neural networks. Training with these examples remedies the flaw of unbalanced temporal information perception and enhances the ability to defend against temporal shifts, ultimately leading to better generalization. To leverage TA, we propose Temporal Video Adversarial Fine-tuning (TAF) framework for improving video representations. TAF is a model-agnostic, generic, and interpretability-friendly training strategy. We evaluate TAF with four powerful models (TSM, GST, TAM, and TPN) over three challenging temporal-related benchmarks (Something-something V1&V2 and diving48). Experimental results demonstrate that TAF effectively improves the test accuracy of these models with notable margins without introducing additional parameters or computational costs. As a byproduct, TAF also improves the robustness under out-of-distribution (OOD) settings. Code is available at https://github.com/jinhaoduan/TAF.\n        \u25b3 Less\n      ",
    "title": "Improve Video Representation with Temporal Adversarial Augmentation",
    "date": "14 May, 2023",
    "authors": [
      "Jinhao Duan",
      " Quanfu Fan",
      " Hao Cheng",
      " Xiaoshuang Shi",
      " Kaidi Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18306",
    "paper_id": "2305.18306",
    "abstract": "\n        In many scenarios, recommender system user interaction data such as clicks or ratings is sparse, and item turnover rates (e.g., new articles, job postings) high. Given this, the integration of contextual \"side\" information in addition to user-item ratings is highly desirable. Whilst there are algorithms that can handle both rating and contextual data simultaneously, these algorithms are typically limited to making only in-sample recommendations, suffer from the curse of dimensionality, and do not incorporate multi-armed bandit (MAB) policies for long-term cumulative reward optimization. We propose multi-view interactive topic regression (MV-ICTR) a novel partially online latent factor recommender algorithm that incorporates both rating and contextual information to model item-specific feature dependencies and users' personal preferences simultaneously, with multi-armed bandit policies for continued online personalization. The result is significantly increased performance on datasets with high percentages of cold-start users and items.\n        \u25b3 Less\n      ",
    "title": "Multi-View Interactive Collaborative Filtering",
    "date": "14 May, 2023",
    "authors": [
      "Maria Lentini",
      " Umashanger Thayasivam"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08246",
    "paper_id": "2305.08246",
    "abstract": "\n        The field of Math-NLP has witnessed significant growth in recent years, motivated by the desire to expand LLM performance to the learning of non-linguistic notions (numerals, and subsequently, arithmetic reasoning). However, non-linguistic skill injection typically comes at a cost for LLMs: it leads to catastrophic forgetting of core linguistic skills, a consequence that often remains unaddressed in the literature. As Math-NLP has been able to create LLMs that can closely approximate the mathematical skills of a grade-schooler or the arithmetic reasoning skills of a calculator, the practicality of these models fail if they concomitantly shed their linguistic capabilities. In this work, we take a closer look into the phenomena of catastrophic forgetting as it pertains to LLMs and subsequently offer a novel framework for non-linguistic skill injection for LLMs based on information theoretic interventions and skill-specific losses that enable the learning of strict arithmetic reasoning. Our model outperforms the state-of-the-art both on injected non-linguistic skills and on linguistic knowledge retention, and does so with a fraction of the non-linguistic training data (1/4) and zero additional synthetic linguistic training data.\n        \u25b3 Less\n      ",
    "title": "Learning Non-linguistic Skills without Sacrificing Linguistic Proficiency",
    "date": "14 May, 2023",
    "authors": [
      "Mandar Sharma",
      " Nikhil Muralidhar",
      " Naren Ramakrishnan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08264",
    "paper_id": "2305.08264",
    "abstract": "\n        We present MatSci-NLP, a natural language benchmark for evaluating the performance of natural language processing (NLP) models on materials science text. We construct the benchmark from publicly available materials science text data to encompass seven different NLP tasks, including conventional NLP tasks like named entity recognition and relation classification, as well as NLP tasks specific to materials science, such as synthesis action retrieval which relates to creating synthesis procedures for materials. We study various BERT-based models pretrained on different scientific text corpora on MatSci-NLP to understand the impact of pretraining strategies on understanding materials science text. Given the scarcity of high-quality annotated data in the materials science domain, we perform our fine-tuning experiments with limited training data to encourage the generalize across MatSci-NLP tasks. Our experiments in this low-resource training setting show that language models pretrained on scientific text outperform BERT trained on general text. MatBERT, a model pretrained specifically on materials science journals, generally performs best for most tasks. Moreover, we propose a unified text-to-schema for multitask learning on \\benchmark and compare its performance with traditional fine-tuning methods. In our analysis of different training methods, we find that our proposed text-to-schema methods inspired by question-answering consistently outperform single and multitask NLP fine-tuning methods. The code and datasets are publicly available at \\url{https://github.com/BangLab-UdeM-Mila/NLP4MatSci-ACL23}.\n        \u25b3 Less\n      ",
    "title": "MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling",
    "date": "14 May, 2023",
    "authors": [
      "Yu Song",
      " Santiago Miret",
      " Bang Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.10881",
    "paper_id": "2204.10881",
    "abstract": "\n        We define a novel notion of ``non-backtracking'' matrix associated to any symmetric matrix, and we prove a ``Ihara-Bass'' type formula for it.\n  We use this theory to prove new results on polynomial-time strong refutations of random constraint satisfaction problems with kk variables per constraints (k-CSPs). For a random k-CSP instance constructed out of a constraint that is satisfied by a pp fraction of assignments, if the instance contains nn variables and nk/2/\u03b52n^{k/2} / \u03b5^2 constraints, we can efficiently compute a certificate that the optimum satisfies at most a p+Ok(\u03b5)p+O_k(\u03b5) fraction of constraints.\n  Previously, this was known for even kk, but for odd kk one needed nk/2(logn)O(1)/\u03b52n^{k/2} (\\log n)^{O(1)} / \u03b5^2 random constraints to achieve the same conclusion.\n  Although the improvement is only polylogarithmic, it overcomes a significant barrier to these types of results. Strong refutation results based on current approaches construct a certificate that a certain matrix associated to the k-CSP instance is quasirandom. Such certificate can come from a Feige-Ofek type argument, from an application of Grothendieck's inequality, or from a spectral bound obtained with a trace argument. The first two approaches require a union bound that cannot work when the number of constraints is o(n\u2308k/2\u2309)o(n^{\\lceil k/2 \\rceil}) and the third one cannot work when the number of constraints is o(nk/2logn\u2212\u2212\u2212\u2212\u221a)o(n^{k/2} \\sqrt{\\log n}).\n  We further apply our techniques to obtain a new PTAS finding assignments for kk-CSP instances with nk/2/\u03b52n^{k/2} / \u03b5^2 constraints in the semi-random settings where the constraints are random, but the sign patterns are adversarial.\n        \u25b3 Less\n      ",
    "title": "A Ihara-Bass Formula for Non-Boolean Matrices and Strong Refutations of Random CSPs",
    "date": "14 May, 2023",
    "authors": [
      "Tommaso d'Orsi",
      " Luca Trevisan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08286",
    "paper_id": "2305.08286",
    "abstract": "\n        This tool demonstration presents a research toolkit for a language model of Java source code. The target audience includes researchers studying problems at the granularity level of subroutines, statements, or variables in Java. In contrast to many existing language models, we prioritize features for researchers including an open and easily-searchable training set, a held out test set with different levels of deduplication from the training set, infrastructure for deduplicating new examples, and an implementation platform suitable for execution on equipment accessible to a relatively modest budget. Our model is a GPT2-like architecture with 350m parameters. Our training set includes 52m Java methods (9b tokens) and 13m StackOverflow threads (10.5b tokens). To improve accessibility of research to more members of the community, we limit local resource requirements to GPUs with 16GB video memory. We provide a test set of held out Java methods that include descriptive comments, including the entire Java projects for those methods. We also provide deduplication tools using precomputed hash tables at various similarity thresholds to help researchers ensure that their own test examples are not in the training set. We make all our tools and data open source and available via Huggingface and Github.\n        \u25b3 Less\n      ",
    "title": "A Language Model of Java Methods with Train/Test Deduplication",
    "date": "14 May, 2023",
    "authors": [
      "Chia-Yi Su",
      " Aakash Bansal",
      " Vijayanta Jain",
      " Sepideh Ghanavati",
      " Collin McMillan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08290",
    "paper_id": "2305.08290",
    "abstract": "\n        We present a simple and generic framework for auditing a given textual conversational system, given some samples of its conversation sessions as its input. The framework computes a SWAN (Schematised Weighted Average Nugget) score based on nugget sequences extracted from the conversation sessions. Following the approaches of S-measure and U-measure, SWAN utilises nugget positions within the conversations to weight the nuggets based on a user model. We also present a schema of twenty (+1) criteria that may be worth incorporating in the SWAN framework. In our future work, we plan to devise conversation sampling methods that are suitable for the various criteria, construct seed user turns for comparing multiple systems, and validate specific instances of SWAN for the purpose of preventing negative impacts of conversational systems on users and society. This paper was written while preparing for the ICTIR 2023 keynote (to be given on July 23, 2023).\n        \u25b3 Less\n      ",
    "title": "SWAN: A Generic Framework for Auditing Textual Conversational Systems",
    "date": "14 May, 2023",
    "authors": [
      "Tetsuya Sakai"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08291",
    "paper_id": "2305.08291",
    "abstract": "\n        In this paper, we introduce the Tree-of-Thought (ToT) framework, a novel approach aimed at improving the problem-solving capabilities of auto-regressive large language models (LLMs). The ToT technique is inspired by the human mind's approach for solving complex reasoning tasks through trial and error. In this process, the human mind explores the solution space through a tree-like thought process, allowing for backtracking when necessary. To implement ToT as a software system, we augment an LLM with additional modules including a prompter agent, a checker module, a memory module, and a ToT controller. In order to solve a given problem, these modules engage in a multi-round conversation with the LLM. The memory module records the conversation and state history of the problem solving process, which allows the system to backtrack to the previous steps of the thought-process and explore other directions from there. To verify the effectiveness of the proposed technique, we implemented a ToT-based solver for the Sudoku Puzzle. Experimental results show that the ToT framework can significantly increase the success rate of Sudoku puzzle solving. Our implementation of the ToT-based Sudoku solver is available on GitHub: \\url{https://github.com/jieyilong/tree-of-thought-puzzle-solver}.\n        \u25b3 Less\n      ",
    "title": "Large Language Model Guided Tree-of-Thought",
    "date": "14 May, 2023",
    "authors": [
      "Jieyi Long"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06784",
    "paper_id": "2305.06784",
    "abstract": "\n        Auction-based Federated Learning (AFL) has attracted extensive research interest due to its ability to motivate data owners to join FL through economic means. Existing works assume that only one data consumer and multiple data owners exist in an AFL marketplace (i.e., a monopoly market). Therefore, data owners bid to join the data consumer for FL. However, this assumption is not realistic in practical AFL marketplaces in which multiple data consumers can compete to attract data owners to join their respective FL tasks. In this paper, we bridge this gap by proposing a first-of-its-kind utility-maximizing bidding strategy for data consumers in federated learning (Fed-Bidder). It enables multiple FL data consumers to compete for data owners via AFL effectively and efficiently by providing with utility estimation capabilities which can accommodate diverse forms of winning functions, each reflecting different market dynamics. Extensive experiments based on six commonly adopted benchmark datasets show that Fed-Bidder is significantly more advantageous compared to four state-of-the-art approaches.\n        \u25b3 Less\n      ",
    "title": "Utility-Maximizing Bidding Strategy for Data Consumers in Auction-based Federated Learning",
    "date": "14 May, 2023",
    "authors": [
      "Xiaoli Tang",
      " Han Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07205",
    "paper_id": "2305.07205",
    "abstract": "\n        Deep learning-based recommendation systems (e.g., DLRMs) are widely used AI models to provide high-quality personalized recommendations. Training data used for modern recommendation systems commonly includes categorical features taking on tens-of-millions of possible distinct values. These categorical tokens are typically assigned learned vector representations, that are stored in large embedding tables, on the order of 100s of GB. Storing and accessing these tables represent a substantial burden in commercial deployments. Our work proposes MEM-REC, a novel alternative representation approach for embedding tables. MEM-REC leverages bloom filters and hashing methods to encode categorical features using two cache-friendly embedding tables. The first table (token embedding) contains raw embeddings (i.e. learned vector representation), and the second table (weight embedding), which is much smaller, contains weights to scale these raw embeddings to provide better discriminative capability to each data point. We provide a detailed architecture, design and analysis of MEM-REC addressing trade-offs in accuracy and computation requirements, in comparison with state-of-the-art techniques. We show that MEM-REC can not only maintain the recommendation quality and significantly reduce the memory footprint for commercial scale recommendation models but can also improve the embedding latency. In particular, based on our results, MEM-REC compresses the MLPerf CriteoTB benchmark DLRM model size by 2900x and performs up to 3.4x faster embeddings while achieving the same AUC as that of the full uncompressed model.\n        \u25b3 Less\n      ",
    "title": "Mem-Rec: Memory Efficient Recommendation System using Alternative Representation",
    "date": "14 May, 2023",
    "authors": [
      "Gopi Krishna Jha",
      " Anthony Thomas",
      " Nilesh Jain",
      " Sameh Gobriel",
      " Tajana Rosing",
      " Ravi Iyer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08296",
    "paper_id": "2305.08296",
    "abstract": "\n        We propose an end-to-end deep-learning approach for automatic rigging and retargeting of 3D models of human faces in the wild. Our approach, called Neural Face Rigging (NFR), holds three key properties:\n  (i) NFR's expression space maintains human-interpretable editing parameters for artistic controls;\n  (ii) NFR is readily applicable to arbitrary facial meshes with different connectivity and expressions;\n  (iii) NFR can encode and produce fine-grained details of complex expressions performed by arbitrary subjects.\n  To the best of our knowledge, NFR is the first approach to provide realistic and controllable deformations of in-the-wild facial meshes, without the manual creation of blendshapes or correspondence. We design a deformation autoencoder and train it through a multi-dataset training scheme, which benefits from the unique advantages of two data sources: a linear 3DMM with interpretable control parameters as in FACS, and 4D captures of real faces with fine-grained details. Through various experiments, we show NFR's ability to automatically produce realistic and accurate facial deformations across a wide range of existing datasets as well as noisy facial scans in-the-wild, while providing artist-controlled, editable parameters.\n        \u25b3 Less\n      ",
    "title": "Neural Face Rigging for Animating and Retargeting Facial Meshes in the Wild",
    "date": "14 May, 2023",
    "authors": [
      "Dafei Qin",
      " Jun Saito",
      " Noam Aigerman",
      " Thibault Groueix",
      " Taku Komura"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08302",
    "paper_id": "2305.08302",
    "abstract": "\n        In the classical supervised learning settings, classifiers are fit with the assumption of balanced label distributions and produce remarkable results on the same. In the real world, however, these assumptions often bend and in turn adversely impact model performance. Identifying bad learners in skewed target distributions is even more challenging. Thus achieving model robustness under these \"label shift\" settings is an important task in autonomous perception. In this paper, we analyze the impact of label shift on the task of multi-weather classification for autonomous vehicles. We use this information as a prior to better assess pedestrian detection in adverse weather. We model the classification performance as an indicator of robustness under 4 label shift scenarios and study the behavior of multiple classes of models. We propose t-RAIN a similarity mapping technique for synthetic data augmentation using large scale generative models and evaluate the performance on DAWN dataset. This mapping boosts model test accuracy by 2.1, 4.4, 1.9, 2.7 % in no-shift, fog, snow, dust shifts respectively. We present state-of-the-art pedestrian detection results on real and synthetic weather domains with best performing 82.69 AP (snow) and 62.31 AP (fog) respectively.\n        \u25b3 Less\n      ",
    "title": "t-RAIN: Robust generalization under weather-aliasing label shift attacks",
    "date": "14 May, 2023",
    "authors": [
      "Aboli Marathe",
      " Sanjana Prabhu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08316",
    "paper_id": "2305.08316",
    "abstract": "\n        Protein-protein interactions (PPIs) are crucial in various biological processes and their study has significant implications for drug development and disease diagnosis. Existing deep learning methods suffer from significant performance degradation under complex real-world scenarios due to various factors, e.g., label scarcity and domain shift. In this paper, we propose a self-ensembling multigraph neural network (SemiGNN-PPI) that can effectively predict PPIs while being both efficient and generalizable. In SemiGNN-PPI, we not only model the protein correlations but explore the label dependencies by constructing and processing multiple graphs from the perspectives of both features and labels in the graph learning process. We further marry GNN with Mean Teacher to effectively leverage unlabeled graph-structured PPI data for self-ensemble graph learning. We also design multiple graph consistency constraints to align the student and teacher graphs in the feature embedding space, enabling the student model to better learn from the teacher model by incorporating more relationships. Extensive experiments on PPI datasets of different scales with different evaluation settings demonstrate that SemiGNN-PPI outperforms state-of-the-art PPI prediction methods, particularly in challenging scenarios such as training with limited annotations and testing on unseen data.\n        \u25b3 Less\n      ",
    "title": "SemiGNN-PPI: Self-Ensembling Multi-Graph Neural Network for Efficient and Generalizable Protein-Protein Interaction Prediction",
    "date": "14 May, 2023",
    "authors": [
      "Ziyuan Zhao",
      " Peisheng Qian",
      " Xulei Yang",
      " Zeng Zeng",
      " Cuntai Guan",
      " Wai Leong Tam",
      " Xiaoli Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08318",
    "paper_id": "2305.08318",
    "abstract": "\n        Relocalization is the basis of map-based localization algorithms. Camera and LiDAR map-based methods are pervasive since their robustness under different scenarios. Generally, mapping and localization using the same sensor have better accuracy since matching features between the same type of data is easier. However, due to the camera's lack of 3D information and the high cost of LiDAR, cross-media methods are developing, which combined live image data and Lidar map. Although matching features between different media is challenging, we believe cross-media is the tendency for AV relocalization since its low cost and accuracy can be comparable to the same-sensor-based methods. In this paper, we propose CMSG, a novel cross-media algorithm for AV relocalization tasks. Semantic features are utilized for better interpretation the correlation between point clouds and image features. What's more, abstracted semantic graph nodes are introduced, and a graph network architecture is integrated to better extract the similarity of semantic features. Validation experiments are conducted on the KITTI odometry dataset. Our results show that CMSG can have comparable or even better accuracy compared to current single-sensor-based methods at a speed of 25 FPS on NVIDIA 1080 Ti GPU.\n        \u25b3 Less\n      ",
    "title": "CMSG Cross-Media Semantic-Graph Feature Matching Algorithm for Autonomous Vehicle Relocalization",
    "date": "14 May, 2023",
    "authors": [
      "Shuhang Tan",
      " Hengyu Liu",
      " Zhiling Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.12971",
    "paper_id": "2302.12971",
    "abstract": "\n        Due to the lack of paired samples and the low signal-to-noise ratio of functional MRI (fMRI) signals, reconstructing perceived natural images or decoding their semantic contents from fMRI data are challenging tasks. In this work, we propose, for the first time, a task-agnostic fMRI-based brain decoding model, BrainCLIP, which leverages CLIP's cross-modal generalization ability to bridge the modality gap between brain activity, image, and text. Our experiments demonstrate that CLIP can act as a pivot for generic brain decoding tasks, including zero-shot visual categories decoding, fMRI-image/text matching, and fMRI-to-image generation. Specifically, BrainCLIP aims to train a mapping network that transforms fMRI patterns into a well-aligned CLIP embedding space by combining visual and textual supervision. Our experiments show that this combination can boost the decoding model's performance on certain tasks like fMRI-text matching and fMRI-to-image generation. On the zero-shot visual category decoding task, BrainCLIP achieves significantly better performance than BraVL, a recently proposed multi-modal method specifically designed for this task. BrainCLIP can also reconstruct visual stimuli with high semantic fidelity and establishes a new state-of-the-art for fMRI-based natural image reconstruction in terms of high-level semantic features.\n        \u25b3 Less\n      ",
    "title": "BrainCLIP: Bridging Brain and Visual-Linguistic Representation Via CLIP for Generic Natural Visual Stimulus Decoding",
    "date": "14 May, 2023",
    "authors": [
      "Yulong Liu",
      " Yongqiang Ma",
      " Wei Zhou",
      " Guibo Zhu",
      " Nanning Zheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08347",
    "paper_id": "2305.08347",
    "abstract": "\n        Generative commonsense question answering (GenCQA) is a task of automatically generating a list of answers given a question. The answer list is required to cover all reasonable answers. This presents the considerable challenges of producing diverse answers and ranking them properly. Incorporating a variety of closely-related background knowledge into the encoding of questions enables the generation of different answers. Meanwhile, learning to distinguish positive answers from negative ones potentially enhances the probabilistic estimation of plausibility, and accordingly, the plausibility-based ranking. Therefore, we propose a Knowledge Enhancement and Plausibility Ranking (KEPR) approach grounded on the Generate-Then-Rank pipeline architecture. Specifically, we expand questions in terms of Wiktionary commonsense knowledge of keywords, and reformulate them with normalized patterns. Dense passage retrieval is utilized for capturing relevant knowledge, and different PLM-based (BART, GPT2 and T5) networks are used for generating answers. On the other hand, we develop an ELECTRA-based answer ranking model, where logistic regression is conducted during training, with the aim of approximating different levels of plausibility in a polar classification scenario. Extensive experiments on the benchmark ProtoQA show that KEPR obtains substantial improvements, compared to the strong baselines. Within the experimental models, the T5-based GenCQA with KEPR obtains the best performance, which is up to 60.91% at the primary canonical metric Inc@3. It outperforms the existing GenCQA models on the current leaderboard of ProtoQA.\n        \u25b3 Less\n      ",
    "title": "KEPR: Knowledge Enhancement and Plausibility Ranking for Generative Commonsense Question Answering",
    "date": "14 May, 2023",
    "authors": [
      "Zhifeng Li",
      " Bowei Zou",
      " Yifan Fan",
      " Yu Hong"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.02733",
    "paper_id": "2212.02733",
    "abstract": "\n        In multi-agent reinforcement learning (MARL), many popular methods, such as VDN and QMIX, are susceptible to a critical multi-agent pathology known as relative overgeneralization (RO), which arises when the optimal joint action's utility falls below that of a sub-optimal joint action in cooperative tasks. RO can cause the agents to get stuck into local optima or fail to solve cooperative tasks that require significant coordination between agents within a given timestep. Recent value-based MARL algorithms such as QPLEX and WQMIX can overcome RO to some extent. However, our experimental results show that they can still fail to solve cooperative tasks that exhibit strong RO. In this work, we propose a novel approach called curriculum learning for relative overgeneralization (CURO) to better overcome RO. To solve a target task that exhibits strong RO, in CURO, we first fine-tune the reward function of the target task to generate source tasks that are tailored to the current ability of the learning agent and train the agent on these source tasks first. Then, to effectively transfer the knowledge acquired in one task to the next, we use a transfer learning method that combines value function transfer with buffer transfer, which enables more efficient exploration in the target task. We demonstrate that, when applied to QMIX, CURO overcomes severe RO problem and significantly improves performance, yielding state-of-the-art results in a variety of cooperative multi-agent tasks, including the challenging StarCraft II micromanagement benchmarks.\n        \u25b3 Less\n      ",
    "title": "Curriculum Learning for Relative Overgeneralization",
    "date": "14 May, 2023",
    "authors": [
      "Lin Shi",
      " Bei Peng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.14616",
    "paper_id": "2210.14616",
    "abstract": "\n        In recent years, spammers are now trying to obfuscate their intents by introducing hybrid spam e-mail combining both image and text parts, which is more challenging to detect in comparison to e-mails containing text or image only. The motivation behind this research is to design an effective approach filtering out hybrid spam e-mails to avoid situations where traditional text-based or image-baesd only filters fail to detect hybrid spam e-mails. To the best of our knowledge, a few studies have been conducted with the goal of detecting hybrid spam e-mails. Ordinarily, Optical Character Recognition (OCR) technology is used to eliminate the image parts of spam by transforming images into text. However, the research questions are that although OCR scanning is a very successful technique in processing text-and-image hybrid spam, it is not an effective solution for dealing with huge quantities due to the CPU power required and the execution time it takes to scan e-mail files. And the OCR techniques are not always reliable in the transformation processes. To address such problems, we propose new late multi-modal fusion training frameworks for a text-and-image hybrid spam e-mail filtering system compared to the classical early fusion detection frameworks based on the OCR method. Convolutional Neural Network (CNN) and Continuous Bag of Words were implemented to extract features from image and text parts of hybrid spam respectively, whereas generated features were fed to sigmoid layer and Machine Learning based classifiers including Random Forest (RF), Decision Tree (DT), Naive Bayes (NB) and Support Vector Machine (SVM) to determine the e-mail ham or spam.\n        \u25b3 Less\n      ",
    "title": "A Late Multi-Modal Fusion Model for Detecting Hybrid Spam E-mail",
    "date": "14 May, 2023",
    "authors": [
      "Zhibo Zhang",
      " Ernesto Damiani",
      " Hussam Al Hamadi",
      " Chan Yeob Yeun",
      " Fatma Taher"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03306",
    "paper_id": "2305.03306",
    "abstract": "\n        The rationale of this work is based on the current user trust discourse of Artificial Intelligence (AI). We aim to produce novel HCI approaches that use trust as a facilitator for the uptake (or appropriation) of current technologies. We propose a framework (HCTFrame) to guide non-experts to unlock the full potential of user trust in AI design. Results derived from a data triangulation of findings from three literature reviews demystify some misconceptions of user trust in computer science and AI discourse, and three case studies are conducted to assess the effectiveness of a psychometric scale in mapping potential users' trust breakdowns and concerns. This work primarily contributes to the fight against the tendency to design technical-centered vulnerable interactions, which can eventually lead to additional real and perceived breaches of trust. The proposed framework can be used to guide system designers on how to map and define user trust and the socioethical and organisational needs and characteristics of AI system design. It can also guide AI system designers on how to develop a prototype and operationalise a solution that meets user trust requirements. The article ends by providing some user research tools that can be employed to measure users' trust intentions and behaviours towards a proposed solution.\n        \u25b3 Less\n      ",
    "title": "Human-centered trust framework: An HCI perspective",
    "date": "14 May, 2023",
    "authors": [
      "Sonia Sousa",
      " Jose Cravino",
      " Paulo Martins",
      " David Lamas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00382",
    "paper_id": "2305.00382",
    "abstract": "\n        Knowledge graphs have shown promise for several cybersecurity tasks, such as vulnerability assessment and threat analysis. In this work, we present a new method for constructing a vulnerability knowledge graph from information in the National Vulnerability Database (NVD). Our approach combines named entity recognition (NER), relation extraction (RE), and entity prediction using a combination of neural models, heuristic rules, and knowledge graph embeddings. We demonstrate how our method helps to fix missing entities in knowledge graphs used for cybersecurity and evaluate the performance.\n        \u25b3 Less\n      ",
    "title": "Constructing a Knowledge Graph from Textual Descriptions of Software Vulnerabilities in the National Vulnerability Database",
    "date": "14 May, 2023",
    "authors": [
      "Anders M\u00f8lmen H\u00f8st",
      " Pierre Lison",
      " Leon Moonen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08413",
    "paper_id": "2305.08413",
    "abstract": "\n        Earth observation (EO) is a prime instrument for monitoring land and ocean processes, studying the dynamics at work, and taking the pulse of our planet. This article gives a bird's eye view of the essential scientific tools and approaches informing and supporting the transition from raw EO data to usable EO-based information. The promises, as well as the current challenges of these developments, are highlighted under dedicated sections. Specifically, we cover the impact of (i) Computer vision; (ii) Machine learning; (iii) Advanced processing and computing; (iv) Knowledge-based AI; (v) Explainable AI and causal inference; (vi) Physics-aware models; (vii) User-centric approaches; and (viii) the much-needed discussion of ethical and societal issues related to the massive use of ML technologies in EO.\n        \u25b3 Less\n      ",
    "title": "Artificial intelligence to advance Earth observation: a perspective",
    "date": "14 May, 2023",
    "authors": [
      "Devis Tuia",
      " Konrad Schindler",
      " Beg\u00fcm Demir",
      " Gustau Camps-Valls",
      " Xiao Xiang Zhu",
      " Mrinalini Kochupillai",
      " Sa\u0161o D\u017eeroski",
      " Jan N. van Rijn",
      " Holger H. Hoos",
      " Fabio Del Frate",
      " Mihai Datcu",
      " Jorge-Arnulfo Quian\u00e9-Ruiz",
      " Volker Markl",
      " Bertrand Le Saux",
      " Rochelle Schneider"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08414",
    "paper_id": "2305.08414",
    "abstract": "\n        In the last five years, there has been a significant focus in Natural Language Processing (NLP) on developing larger Pretrained Language Models (PLMs) and introducing benchmarks such as SuperGLUE and SQuAD to measure their abilities in language understanding, reasoning, and reading comprehension. These PLMs have achieved impressive results on these benchmarks, even surpassing human performance in some cases. This has led to claims of superhuman capabilities and the provocative idea that certain tasks have been solved. In this position paper, we take a critical look at these claims and ask whether PLMs truly have superhuman abilities and what the current benchmarks are really evaluating. We show that these benchmarks have serious limitations affecting the comparison between humans and PLMs and provide recommendations for fairer and more transparent benchmarks.\n        \u25b3 Less\n      ",
    "title": "What's the Meaning of Superhuman Performance in Today's NLU?",
    "date": "14 May, 2023",
    "authors": [
      "Simone Tedeschi",
      " Johan Bos",
      " Thierry Declerck",
      " Jan Hajic",
      " Daniel Hershcovich",
      " Eduard H. Hovy",
      " Alexander Koller",
      " Simon Krek",
      " Steven Schockaert",
      " Rico Sennrich",
      " Ekaterina Shutova",
      " Roberto Navigli"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.13465",
    "paper_id": "2303.13465",
    "abstract": "\n        Traditionally, approximate dynamic programming is employed in dialogue generation with greedy policy improvement through action sampling, as the natural language action space is vast. However, this practice is inefficient for reinforcement learning (RL) due to the sparsity of eligible responses with high action values, which leads to weak improvement sustained by random sampling. This paper presents theoretical analysis and experiments that reveal the performance of the dialogue policy is positively correlated with the sampling size. To overcome this limitation, we introduce a novel dual-granularity Q-function that explores the most promising response category to intervene in the sampling process. Our approach extracts actions based on a grained hierarchy, thereby achieving the optimum with fewer policy iterations. Additionally, we use offline RL and learn from multiple reward functions designed to capture emotional nuances in human interactions. Empirical studies demonstrate that our algorithm outperforms baselines across automatic metrics and human evaluations. Further testing reveals that our algorithm exhibits both explainability and controllability and generates responses with higher expected rewards.\n        \u25b3 Less\n      ",
    "title": "Deep RL with Hierarchical Action Exploration for Dialogue Generation",
    "date": "14 May, 2023",
    "authors": [
      "Itsugun Cho",
      " Ryota Takahashi",
      " Yusaku Yanase",
      " Hiroaki Saito"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08446",
    "paper_id": "2305.08446",
    "abstract": "\n        Multi-Agent Path Finding (MAPF) is an important core problem for many new and emerging industrial applications. Many works appear on this topic each year, and a large number of substantial advancements and performance improvements have been reported. Yet measuring overall progress in MAPF is difficult: there are many potential competitors, and the computational burden for comprehensive experimentation is prohibitively large. Moreover, detailed data from past experimentation is usually unavailable. In this work, we introduce a set of methodological and visualisation tools which can help the community establish clear indicators for state-of-the-art MAPF performance and which can facilitate large-scale comparisons between MAPF solvers. Our objectives are to lower the barrier of entry for new researchers and to further promote the study of MAPF, since progress in the area and the main challenges are made much clearer.\n        \u25b3 Less\n      ",
    "title": "Tracking Progress in Multi-Agent Path Finding",
    "date": "14 May, 2023",
    "authors": [
      "Bojie Shen",
      " Zhe Chen",
      " Muhammad Aamir Cheema",
      " Daniel D. Harabor",
      " Peter J. Stuckey"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.10848",
    "paper_id": "2304.10848",
    "abstract": "\n        The Metropolis algorithm (MA) is a classic stochastic local search heuristic. It avoids getting stuck in local optima by occasionally accepting inferior solutions. To better and in a rigorous manner understand this ability, we conduct a mathematical runtime analysis of the MA on the CLIFF benchmark. Apart from one local optimum, cliff functions are monotonically increasing towards the global optimum. Consequently, to optimize a cliff function, the MA only once needs to accept an inferior solution. Despite seemingly being an ideal benchmark for the MA to profit from its main working principle, our mathematical runtime analysis shows that this hope does not come true. Even with the optimal temperature (the only parameter of the MA), the MA optimizes most cliff functions less efficiently than simple elitist evolutionary algorithms (EAs), which can only leave the local optimum by generating a superior solution possibly far away. This result suggests that our understanding of why the MA is often very successful in practice is not yet complete. Our work also suggests to equip the MA with global mutation operators, an idea supported by our preliminary experiments.\n        \u25b3 Less\n      ",
    "title": "How Well Does the Metropolis Algorithm Cope With Local Optima?",
    "date": "14 May, 2023",
    "authors": [
      "Benjamin Doerr",
      " Taha El Ghazi El Houssaini",
      " Amirhossein Rajabi",
      " Carsten Witt"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08462",
    "paper_id": "2305.08462",
    "abstract": "\n        Semantic segmentation has recently witnessed great progress. Despite the impressive overall results, the segmentation performance in some hard areas (e.g., small objects or thin parts) is still not promising. A straightforward solution is hard sample mining, which is widely used in object detection. Yet, most existing hard pixel mining strategies for semantic segmentation often rely on pixel's loss value, which tends to decrease during training. Intuitively, the pixel hardness for segmentation mainly depends on image structure and is expected to be stable. In this paper, we propose to learn pixel hardness for semantic segmentation, leveraging hardness information contained in global and historical loss values. More precisely, we add a gradient-independent branch for learning a hardness level (HL) map by maximizing hardness-weighted segmentation loss, which is minimized for the segmentation head. This encourages large hardness values in difficult areas, leading to appropriate and stable HL map. Despite its simplicity, the proposed method can be applied to most segmentation methods with no and marginal extra cost during inference and training, respectively. Without bells and whistles, the proposed method achieves consistent/significant improvement (1.37% mIoU on average) over most popular semantic segmentation methods on Cityscapes dataset, and demonstrates good generalization ability across domains. The source codes are available at https://github.com/Menoly-xin/Hardness-Level-Learning .\n        \u25b3 Less\n      ",
    "title": "Not All Pixels Are Equal: Learning Pixel Hardness for Semantic Segmentation",
    "date": "15 May, 2023",
    "authors": [
      "Xin Xiao",
      " Daiguo Zhou",
      " Jiagao Hu",
      " Yi Hu",
      " Yongchao Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11898",
    "paper_id": "2305.11898",
    "abstract": "\n        In recent years, Deep Convolutional Neural Networks (DCNNs) have outreached the performance of classical algorithms for image restoration tasks. However most of these methods are not suited for computational efficiency and are therefore too expensive to be executed on embedded and mobile devices. In this work we investigate Spiking Neural Networks (SNNs) for Gaussian denoising, with the goal of approaching the performance of conventional DCNN while reducing the computational load. We propose a formal analysis of the information conversion processing carried out by the Leaky Integrate and Fire (LIF) neurons and we compare its performance with the classical rate-coding mechanism. The neural coding schemes are then evaluated through experiments in terms of denoising performance and computation efficiency for a state-of-the-art deep convolutional neural network. Our results show that SNNs with LIF neurons can provide competitive denoising performance but at a reduced computational cost.\n        \u25b3 Less\n      ",
    "title": "Neural information coding for efficient spike-based image denoising",
    "date": "15 May, 2023",
    "authors": [
      "Andrea Castagnetti",
      " Alain Pegatoquet",
      " Beno\u00eet Miramond"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.08508",
    "paper_id": "2302.08508",
    "abstract": "\n        In this work, we perform an in-depth analysis of the visualisation methods implemented in two popular self-explaining models for visual classification based on prototypes - ProtoPNet and ProtoTree. Using two fine-grained datasets (CUB-200-2011 and Stanford Cars), we first show that such methods do not correctly identify the regions of interest inside of the images, and therefore do not reflect the model behaviour. Secondly, using a deletion metric, we demonstrate quantitatively that saliency methods such as Smoothgrads or PRP provide more faithful image patches. We also propose a new relevance metric based on the segmentation of the object provided in some datasets (e.g. CUB-200-2011) and show that the imprecise patch visualisations generated by ProtoPNet and ProtoTree can create a false sense of bias that can be mitigated by the use of more faithful methods. Finally, we discuss the implications of our findings for other prototype-based models sharing the same visualisation method.\n        \u25b3 Less\n      ",
    "title": "Sanity checks and improvements for patch visualisation in prototype-based image classification",
    "date": "15 May, 2023",
    "authors": [
      "Romain Xu-Darme",
      " Georges Qu\u00e9not",
      " Zakaria Chihani",
      " Marie-Christine Rousset"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.17137",
    "paper_id": "2305.17137",
    "abstract": "\n        This paper aims to serve as a comprehensive guide for researchers and practitioners, offering insights into the current state, potential applications, and future research directions for generative artificial intelligence and foundation models within the context of intelligent vehicles. As the automotive industry progressively integrates AI, generative artificial intelligence technologies hold the potential to revolutionize user interactions, delivering more immersive, intuitive, and personalised in-car experiences. We provide an overview of current applications of generative artificial intelligence in the automotive domain, emphasizing speech, audio, vision, and multimodal interactions. We subsequently outline critical future research areas, including domain adaptability, alignment, multimodal integration and others, as well as, address the challenges and risks associated with ethics. By fostering collaboration and addressing these research areas, generative artificial intelligence can unlock its full potential, transforming the driving experience and shaping the future of intelligent vehicles.\n        \u25b3 Less\n      ",
    "title": "Integrating Generative Artificial Intelligence in Intelligent Vehicle Systems",
    "date": "15 May, 2023",
    "authors": [
      "Lukas Stappen",
      " Jeremy Dillmann",
      " Serena Striegel",
      " Hans-J\u00f6rg V\u00f6gel",
      " Nicolas Flores-Herr",
      " Bj\u00f6rn W. Schuller"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.00759",
    "paper_id": "2211.00759",
    "abstract": "\n        The Adaptive Large Neighborhood Search (ALNS) algorithm has shown considerable success in solving complex combinatorial optimization problems (COPs). ALNS selects various heuristics adaptively during the search process, leveraging their strengths to find good solutions for optimization problems. However, the effectiveness of ALNS depends on the proper configuration of its selection and acceptance parameters. To address this limitation, we propose a Deep Reinforcement Learning (DRL) approach that selects heuristics, adjusts parameters, and controls the acceptance criteria during the search process. The proposed method aims to learn, based on the state of the search, how to configure the next iteration of the ALNS to obtain good solutions to the underlying optimization problem. We evaluate the proposed method on a time-dependent orienteering problem with stochastic weights and time windows, used in an IJCAI competition. The results show that our approach outperforms vanilla ALNS and ALNS tuned with Bayesian Optimization. In addition, it obtained better solutions than two state-of-the-art DRL approaches, which are the winning methods of the competition, with much fewer observations required for training. The implementation of our approach will be made publicly available.\n        \u25b3 Less\n      ",
    "title": "Online Control of Adaptive Large Neighborhood Search using Deep Reinforcement Learning",
    "date": "15 May, 2023",
    "authors": [
      "Robbert Reijnen",
      " Yingqian Zhang",
      " Hoong Chuin Lau",
      " Zaharah Bukhsh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.02462",
    "paper_id": "2301.02462",
    "abstract": "\n        We introduce new power indices to measure the a priori voting power of voters in liquid democracy elections where an underlying network restricts delegations. We argue that our power indices are natural extensions of the standard Penrose-Banzhaf index in simple voting games. We show that computing the criticality of a voter is #P-hard even when voting weights are polynomially-bounded in the size of the instance. However, for specific settings, such as when the underlying network is a bipartite or complete graph, recursive formulas can compute these indices for weighted voting games in pseudo-polynomial time. We highlight their theoretical properties and provide numerical results to illustrate how restricting the possible delegations can alter voters' voting power.\n        \u25b3 Less\n      ",
    "title": "Measuring a Priori Voting Power -- Taking Delegations Seriously",
    "date": "15 May, 2023",
    "authors": [
      "Rachael Colley",
      " Th\u00e9o Delemazure",
      " Hugo Gilbert"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.05280",
    "paper_id": "2305.05280",
    "abstract": "\n        Compared to news and chat summarization, the development of meeting summarization is hugely decelerated by the limited data. To this end, we introduce a versatile Chinese meeting summarization dataset, dubbed VCSum, consisting of 239 real-life meetings, with a total duration of over 230 hours. We claim our dataset is versatile because we provide the annotations of topic segmentation, headlines, segmentation summaries, overall meeting summaries, and salient sentences for each meeting transcript. As such, the dataset can adapt to various summarization tasks or methods, including segmentation-based summarization, multi-granularity summarization and retrieval-then-generate summarization. Our analysis confirms the effectiveness and robustness of VCSum. We also provide a set of benchmark models regarding different downstream summarization tasks on VCSum to facilitate further research. The dataset and code will be released at https://github.com/hahahawu/VCSum.\n        \u25b3 Less\n      ",
    "title": "VCSUM: A Versatile Chinese Meeting Summarization Dataset",
    "date": "15 May, 2023",
    "authors": [
      "Han Wu",
      " Mingjie Zhan",
      " Haochen Tan",
      " Zhaohui Hou",
      " Ding Liang",
      " Linqi Song"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18307",
    "paper_id": "2305.18307",
    "abstract": "\n        Auditing plays a pivotal role in the development of trustworthy AI. However, current research primarily focuses on creating auditable AI documentation, which is intended for regulators and experts rather than end-users affected by AI decisions. How to communicate to members of the public that an AI has been audited and considered trustworthy remains an open challenge. This study empirically investigated certification labels as a promising solution. Through interviews (N = 12) and a census-representative survey (N = 302), we investigated end-users' attitudes toward certification labels and their effectiveness in communicating trustworthiness in low- and high-stakes AI scenarios. Based on the survey results, we demonstrate that labels can significantly increase end-users' trust and willingness to use AI in both low- and high-stakes scenarios. However, end-users' preferences for certification labels and their effect on trust and willingness to use AI were more pronounced in high-stake scenarios. Qualitative content analysis of the interviews revealed opportunities and limitations of certification labels, as well as facilitators and inhibitors for the effective use of labels in the context of AI. For example, while certification labels can mitigate data-related concerns expressed by end-users (e.g., privacy and data protection), other concerns (e.g., model performance) are more challenging to address. Our study provides valuable insights and recommendations for designing and implementing certification labels as a promising constituent within the trustworthy AI ecosystem.\n        \u25b3 Less\n      ",
    "title": "Certification Labels for Trustworthy AI: Insights From an Empirical Mixed-Method Study",
    "date": "15 May, 2023",
    "authors": [
      "Nicolas Scharowski",
      " Michaela Benk",
      " Swen J. K\u00fchne",
      " L\u00e9ane Wettstein",
      " Florian Br\u00fchlmann"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.06049",
    "paper_id": "2209.06049",
    "abstract": "\n        NLP in the legal domain has seen increasing success with the emergence of Transformer-based Pre-trained Language Models (PLMs) pre-trained on legal text. PLMs trained over European and US legal text are available publicly; however, legal text from other domains (countries), such as India, have a lot of distinguishing characteristics. With the rapidly increasing volume of Legal NLP applications in various countries, it has become necessary to pre-train such LMs over legal text of other countries as well. In this work, we attempt to investigate pre-training in the Indian legal domain. We re-train (continue pre-training) two popular legal PLMs, LegalBERT and CaseLawBERT, on Indian legal data, as well as train a model from scratch with a vocabulary based on Indian legal text. We apply these PLMs over three benchmark legal NLP tasks -- Legal Statute Identification from facts, Semantic Segmentation of Court Judgment Documents, and Court Appeal Judgment Prediction -- over both Indian and non-Indian (EU, UK) datasets. We observe that our approach not only enhances performance on the new domain (Indian texts) but also over the original domain (European and UK texts). We also conduct explainability experiments for a qualitative comparison of all these different PLMs.\n        \u25b3 Less\n      ",
    "title": "Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law",
    "date": "15 May, 2023",
    "authors": [
      "Shounak Paul",
      " Arpan Mandal",
      " Pawan Goyal",
      " Saptarshi Ghosh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09686",
    "paper_id": "2305.09686",
    "abstract": "\n        Due to the widespread use of data-powered systems in our everyday lives, concepts like bias and fairness gained significant attention among researchers and practitioners, in both industry and academia. Such issues typically emerge from the data, which comes with varying levels of quality, used to train supervised machine learning systems. With the commercialization and deployment of such systems that are sometimes delegated to make life-changing decisions, significant efforts are being made towards the identification and removal of possible sources of data bias that may resurface to the final end user or in the decisions being made. In this paper, we present research results that show how bias in data affects end users, where bias is originated, and provide a viewpoint about what we should do about it. We argue that data bias is not something that should necessarily be removed in all cases, and that research attention should instead shift from bias removal towards the identification, measurement, indexing, surfacing, and adapting for bias, which we name bias management.\n        \u25b3 Less\n      ",
    "title": "Data Bias Management",
    "date": "15 May, 2023",
    "authors": [
      "Gianluca Demartini",
      " Kevin Roitero",
      " Stefano Mizzaro"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08504",
    "paper_id": "2305.08504",
    "abstract": "\n        Intelligent, large-scale IoT ecosystems have become possible due to recent advancements in sensing technologies, distributed learning, and low-power inference in embedded devices. In traditional cloud-centric approaches, raw data is transmitted to a central server for training and inference purposes. On the other hand, Federated Learning migrates both tasks closer to the edge nodes and endpoints. This allows for a significant reduction in data exchange while preserving the privacy of users. Trained models, though, may under-perform in dynamic environments due to changes in the data distribution, affecting the model's ability to infer accurately; this is referred to as concept drift. Such drift may also be adversarial in nature. Therefore, it is of paramount importance to detect such behaviours promptly. In order to simultaneously reduce communication traffic and maintain the integrity of inference models, we introduce FLARE, a novel lightweight dual-scheduler FL framework that conditionally transfers training data, and deploys models between edge and sensor endpoints based on observing the model's training behaviour and inference statistics, respectively. We show that FLARE can significantly reduce the amount of data exchanged between edge and sensor nodes compared to fixed-interval scheduling methods (over 5x reduction), is easily scalable to larger systems, and can successfully detect concept drift reactively with at least a 16x reduction in latency.\n        \u25b3 Less\n      ",
    "title": "FLARE: Detection and Mitigation of Concept Drift for Federated Learning based IoT Deployments",
    "date": "15 May, 2023",
    "authors": [
      "Theo Chow",
      " Usman Raza",
      " Ioannis Mavromatis",
      " Aftab Khan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08506",
    "paper_id": "2305.08506",
    "abstract": "\n        Global crises and regulatory developments require increased supply chain transparency and resilience. Companies do not only need to react to a dynamic environment but have to act proactively and implement measures to prevent production delays and reduce risks in the supply chains. However, information about supply chains, especially at the deeper levels, is often intransparent and incomplete, making it difficult to obtain precise predictions about prospective risks. By connecting different data sources, we model the supply network as a knowledge graph and achieve transparency up to tier-3 suppliers. To predict missing information in the graph, we apply state-of-the-art knowledge graph completion methods and attain a mean reciprocal rank of 0.4377 with the best model. Further, we apply graph analysis algorithms to identify critical entities in the supply network, supporting supply chain managers in automated risk identification.\n        \u25b3 Less\n      ",
    "title": "A Knowledge Graph Perspective on Supply Chain Resilience",
    "date": "15 May, 2023",
    "authors": [
      "Yushan Liu",
      " Bailan He",
      " Marcel Hildebrandt",
      " Maximilian Buchner",
      " Daniela Inzko",
      " Roger Wernert",
      " Emanuel Weigel",
      " Dagmar Beyer",
      " Martin Berbalk",
      " Volker Tresp"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08511",
    "paper_id": "2305.08511",
    "abstract": "\n        We propose bounded fitting as a scheme for learning description logic concepts in the presence of ontologies. A main advantage is that the resulting learning algorithms come with theoretical guarantees regarding their generalization to unseen examples in the sense of PAC learning. We prove that, in contrast, several other natural learning algorithms fail to provide such guarantees. As a further contribution, we present the system SPELL which efficiently implements bounded fitting for the description logic ELHr\\mathcal{ELH}^r based on a SAT solver, and compare its performance to a state-of-the-art learner.\n        \u25b3 Less\n      ",
    "title": "SAT-Based PAC Learning of Description Logic Concepts",
    "date": "15 May, 2023",
    "authors": [
      "Balder ten Cate",
      " Maurice Funk",
      " Jean Christoph Jung",
      " Carsten Lutz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08518",
    "paper_id": "2305.08518",
    "abstract": "\n        The progress of Natural Language Processing (NLP), although fast in recent years, is not at the same pace for all languages. African languages in particular are still behind and lack automatic processing tools. Some of these tools are very important for the development of these languages but also have an important role in many NLP applications. This is particularly the case for automatic spell checkers. Several approaches have been studied to address this task and the one modeling spelling correction as a translation task from misspelled (noisy) text to well-spelled (correct) text shows promising results. However, this approach requires a parallel corpus of noisy data on the one hand and correct data on the other hand, whereas Wolof is a low-resource language and does not have such a corpus. In this paper, we present a way to address the constraint related to the lack of data by generating synthetic data and we present sequence-to-sequence models using Deep Learning for spelling correction in Wolof. We evaluated these models in three different scenarios depending on the subwording method applied to the data and showed that the latter had a significant impact on the performance of the models, which opens the way for future research in Wolof spelling correction.\n        \u25b3 Less\n      ",
    "title": "Beqi: Revitalize the Senegalese Wolof Language with a Robust Spelling Corrector",
    "date": "15 May, 2023",
    "authors": [
      "Derguene Mbaye",
      " Moussa Diallo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08544",
    "paper_id": "2305.08544",
    "abstract": "\n        Neural networks have achieved impressive breakthroughs in both industry and academia. How to effectively develop neural networks on quantum computing devices is a challenging open problem. Here, we propose a new quantum neural network model for quantum neural computing using (classically-controlled) single-qubit operations and measurements on real-world quantum systems with naturally occurring environment-induced decoherence, which greatly reduces the difficulties of physical implementations. Our model circumvents the problem that the state-space size grows exponentially with the number of neurons, thereby greatly reducing memory requirements and allowing for fast optimization with traditional optimization algorithms. We benchmark our model for handwritten digit recognition and other nonlinear classification tasks. The results show that our model has an amazing nonlinear classification ability and robustness to noise. Furthermore, our model allows quantum computing to be applied in a wider context and inspires the earlier development of a quantum neural computer than standard quantum computers.\n        \u25b3 Less\n      ",
    "title": "Quantum Neural Network for Quantum Neural Computing",
    "date": "15 May, 2023",
    "authors": [
      "Min-Gang Zhou",
      " Zhi-Ping Liu",
      " Hua-Lei Yin",
      " Chen-Long Li",
      " Tong-Kai Xu",
      " Zeng-Bing Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08551",
    "paper_id": "2305.08551",
    "abstract": "\n        Vision transformers (ViTs) achieve remarkable performance on large datasets, but tend to perform worse than convolutional neural networks (CNNs) when trained from scratch on smaller datasets, possibly due to a lack of local inductive bias in the architecture. Recent studies have therefore added locality to the architecture and demonstrated that it can help ViTs achieve performance comparable to CNNs in the small-size dataset regime. Existing methods, however, are architecture-specific or have higher computational and memory costs. Thus, we propose a module called Local InFormation Enhancer (LIFE) that extracts patch-level local information and incorporates it into the embeddings used in the self-attention block of ViTs. Our proposed module is memory and computation efficient, as well as flexible enough to process auxiliary tokens such as the classification and distillation tokens. Empirical results show that the addition of the LIFE module improves the performance of ViTs on small image classification datasets. We further demonstrate how the effect can be extended to downstream tasks, such as object detection and semantic segmentation. In addition, we introduce a new visualization method, Dense Attention Roll-Out, specifically designed for dense prediction tasks, allowing the generation of class-specific attention maps utilizing the attention maps of all tokens.\n        \u25b3 Less\n      ",
    "title": "Enhancing Performance of Vision Transformers on Small Datasets through Local Inductive Bias Incorporation",
    "date": "15 May, 2023",
    "authors": [
      "Ibrahim Batuhan Akkaya",
      " Senthilkumar S. Kathiresan",
      " Elahe Arani",
      " Bahram Zonooz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10446",
    "paper_id": "2305.10446",
    "abstract": "\n        Emotion regulation is a crucial element in dealing with emotional events and has positive effects on mental health. This paper aims to provide a more comprehensive understanding of emotional events by introducing a new French corpus of emotional narratives collected using a questionnaire for emotion regulation. We follow the theoretical framework of the Component Process Model which considers emotions as dynamic processes composed of four interrelated components (behavior, feeling, thinking and territory). Each narrative is related to a discrete emotion and is structured based on all emotion components by the writers. We study the interaction of components and their impact on emotion classification with machine learning methods and pre-trained language models. Our results show that each component improves prediction performance, and that the best results are achieved by jointly considering all components. Our results also show the effectiveness of pre-trained language models in predicting discrete emotion from certain components, which reveal differences in how emotion components are expressed.\n        \u25b3 Less\n      ",
    "title": "Emotion Recognition based on Psychological Components in Guided Narratives for Emotion Regulation",
    "date": "15 May, 2023",
    "authors": [
      "Gustave Cortal",
      " Alain Finkel",
      " Patrick Paroubek",
      " Lina Ye"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.16865",
    "paper_id": "2211.16865",
    "abstract": "\n        A temporal knowledge graph (TKG) stores the events derived from the data involving time. Predicting events is extremely challenging due to the time-sensitive property of events. Besides, the previous TKG completion (TKGC) approaches cannot represent both the timeliness and the causality properties of events, simultaneously. To address these challenges, we propose a Logic and Commonsense-Guided Embedding model (LCGE) to jointly learn the time-sensitive representation involving timeliness and causality of events, together with the time-independent representation of events from the perspective of commonsense. Specifically, we design a temporal rule learning algorithm to construct a rule-guided predicate embedding regularization strategy for learning the causality among events. Furthermore, we could accurately evaluate the plausibility of events via auxiliary commonsense knowledge. The experimental results of TKGC task illustrate the significant performance improvements of our model compared with the existing approaches. More interestingly, our model is able to provide the explainability of the predicted results in the view of causal inference. The source code and datasets of this paper are available at https://github.com/ngl567/LCGE.\n        \u25b3 Less\n      ",
    "title": "Logic and Commonsense-Guided Temporal Knowledge Graph Completion",
    "date": "15 May, 2023",
    "authors": [
      "Guanglin Niu",
      " Bo Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08598",
    "paper_id": "2305.08598",
    "abstract": "\n        Trust is an important factor in people's interactions with AI systems. However, there is a lack of empirical studies examining how real end-users trust or distrust the AI system they interact with. Most research investigates one aspect of trust in lab settings with hypothetical end-users. In this paper, we provide a holistic and nuanced understanding of trust in AI through a qualitative case study of a real-world computer vision application. We report findings from interviews with 20 end-users of a popular, AI-based bird identification app where we inquired about their trust in the app from many angles. We find participants perceived the app as trustworthy and trusted it, but selectively accepted app outputs after engaging in verification behaviors, and decided against app adoption in certain high-stakes scenarios. We also find domain knowledge and context are important factors for trust-related assessment and decision-making. We discuss the implications of our findings and provide recommendations for future research on trust in AI.\n        \u25b3 Less\n      ",
    "title": "Humans, AI, and Context: Understanding End-Users' Trust in a Real-World Computer Vision Application",
    "date": "15 May, 2023",
    "authors": [
      "Sunnie S. Y. Kim",
      " Elizabeth Anne Watkins",
      " Olga Russakovsky",
      " Ruth Fong",
      " Andr\u00e9s Monroy-Hern\u00e1ndez"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06854",
    "paper_id": "2305.06854",
    "abstract": "\n        Datalog reasoning based on the semina\u00efve evaluation strategy evaluates rules using traditional join plans, which often leads to redundancy and inefficiency in practice, especially when the rules are complex. Hypertree decompositions help identify efficient query plans and reduce similar redundancy in query answering. However, it is unclear how this can be applied to materialisation and incremental reasoning with recursive Datalog programs. Moreover, hypertree decompositions require additional data structures and thus introduce nonnegligible overhead in both runtime and memory consumption. In this paper, we provide algorithms that exploit hypertree decompositions for the materialisation and incremental evaluation of Datalog programs. Furthermore, we combine this approach with standard Datalog reasoning algorithms in a modular fashion so that the overhead caused by the decompositions is reduced. Our empirical evaluation shows that, when the program contains complex rules, the combined approach is usually significantly faster than the baseline approach, sometimes by orders of magnitude.\n        \u25b3 Less\n      ",
    "title": "Enhancing Datalog Reasoning with Hypertree Decompositions",
    "date": "15 May, 2023",
    "authors": [
      "Xinyue Zhang",
      " Pan Hu",
      " Yavor Nenov",
      " Ian Horrocks"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2107.11972",
    "paper_id": "2107.11972",
    "abstract": "\n        Price movement forecasting aims at predicting the future trends of financial assets based on the current market conditions and other relevant information. Recently, machine learning (ML) methods have become increasingly popular and achieved promising results for price movement forecasting in both academia and industry. Most existing ML solutions formulate the forecasting problem as a classification (to predict the direction) or a regression (to predict the return) problem over the entire set of training data. However, due to the extremely low signal-to-noise ratio and stochastic nature of financial data, good trading opportunities are extremely scarce. As a result, without careful selection of potentially profitable samples, such ML methods are prone to capture the patterns of noises instead of real signals. To address this issue, we propose a novel price movement forecasting framework named LARA consisting of two main components: Locality-Aware Attention (LA-Attention) and Iterative Refinement Labeling (RA-Labeling). (1) LA-Attention automatically extracts the potentially profitable samples by attending to label information. Moreover, equipped with metric learning techniques, LA-Attention enjoys task-specific distance metrics and effectively distributes attention to potentially profitable samples. (2) RA-Labeling further iteratively refines the noisy labels of potentially profitable samples, and combines the learned predictors robust to the unseen and noisy samples. In a set of experiments on three real-world financial markets: stocks, cryptocurrencies, and ETFs, LARA significantly outperforms several machine learning based methods on the Qlib quantitative investment platform. Extensive ablation studies and experiments also demonstrate that LARA indeed captures more reliable trading opportunities.\n        \u25b3 Less\n      ",
    "title": "Trade When Opportunity Comes: Price Movement Forecasting via Locality-Aware Attention and Iterative Refinement Labeling",
    "date": "15 May, 2023",
    "authors": [
      "Liang Zeng",
      " Lei Wang",
      " Hui Niu",
      " Ruchen Zhang",
      " Ling Wang",
      " Jian Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08624",
    "paper_id": "2305.08624",
    "abstract": "\n        Gaussian Process based Bayesian Optimization is a well-known sample efficient sequential strategy for globally optimizing black-box, expensive, and multi-extremal functions. The role of the Gaussian Process is to provide a probabilistic approximation of the unknown function, depending on the sequentially collected observations, while an acquisition function drives the choice of the next solution to evaluate, balancing between exploration and exploitation, depending on the current Gaussian Process model. Despite the huge effort of the scientific community in defining effective exploration-exploitation mechanisms, we are still far away from the master acquisition function. This paper merges the most relevant results and insights from both algorithmic and human search strategies to propose a novel acquisition function, mastering the trade-off between explorative and exploitative choices, adaptively. We compare the proposed acquisition function on a number of test functions and against different state-of-the-art ones, which are instead based on prefixed or random scheduling between exploration and exploitation. A Pareto analysis is performed with respect to two (antagonistic) goals: convergence to the optimum and exploration capability. Results empirically prove that the proposed acquisition function is almost always Pareto optimal and also the most balanced trade-off between the two goals.\n        \u25b3 Less\n      ",
    "title": "Mastering the exploration-exploitation trade-off in Bayesian Optimization",
    "date": "15 May, 2023",
    "authors": [
      "Antonio Candelieri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08625",
    "paper_id": "2305.08625",
    "abstract": "\n        This paper presents the best-performing approach alias \"Adam Smith\" for the SemEval-2023 Task 4: \"Identification of Human Values behind Arguments\". The goal of the task was to create systems that automatically identify the values within textual arguments. We train transformer-based models until they reach their loss minimum or f1-score maximum. Ensembling the models by selecting one global decision threshold that maximizes the f1-score leads to the best-performing system in the competition. Ensembling based on stacking with logistic regressions shows the best performance on an additional dataset provided to evaluate the robustness (\"Nahj al-Balagha\"). Apart from outlining the submitted system, we demonstrate that the use of the large ensemble model is not necessary and that the system size can be significantly reduced.\n        \u25b3 Less\n      ",
    "title": "Adam-Smith at SemEval-2023 Task 4: Discovering Human Values in Arguments with Ensembles of Transformer-based Models",
    "date": "15 May, 2023",
    "authors": [
      "Daniel Schroter",
      " Daryna Dementieva",
      " Georg Groh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08633",
    "paper_id": "2305.08633",
    "abstract": "\n        Deep learning techniques have gained a lot of traction in the field of NLP research. The aim of this paper is to predict the age and gender of an individual by inspecting their written text. We propose a supervised BERT-based classification technique in order to predict the age and gender of bloggers. The dataset used contains 681284 rows of data, with the information of the blogger's age, gender, and text of the blog written by them. We compare our algorithm to previous works in the same domain and achieve a better accuracy and F1 score. The accuracy reported for the prediction of age group was 84.2%, while the accuracy for the prediction of gender was 86.32%. This study relies on the raw capabilities of BERT to predict the classes of textual data efficiently. This paper shows promising capability in predicting the demographics of the author with high accuracy and can have wide applicability across multiple domains.\n        \u25b3 Less\n      ",
    "title": "Text2Gender: A Deep Learning Architecture for Analysis of Blogger's Age and Gender",
    "date": "15 May, 2023",
    "authors": [
      "Vishesh Thakur",
      " Aneesh Tickoo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08636",
    "paper_id": "2305.08636",
    "abstract": "\n        The Explainable Detection of Online Sexism task presents the problem of explainable sexism detection through fine-grained categorisation of sexist cases with three subtasks. Our team experimented with different ways to combat class imbalance throughout the tasks using data augmentation and loss alteration techniques. We tackled the challenge by utilising ensembles of Transformer models trained on different datasets, which are tested to find the balance between performance and interpretability. This solution ranked us in the top 40\\% of teams for each of the tracks.\n        \u25b3 Less\n      ",
    "title": "AdamR at SemEval-2023 Task 10: Solving the Class Imbalance Problem in Sexism Detection with Ensemble Learning",
    "date": "15 May, 2023",
    "authors": [
      "Adam Rydelek",
      " Daryna Dementieva",
      " Georg Groh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11301",
    "paper_id": "2305.11301",
    "abstract": "\n        While Knowledge Graph Completion (KGC) on static facts is a matured field, Temporal Knowledge Graph Completion (TKGC), that incorporates validity time into static facts is still in its nascent stage. The KGC methods fall into multiple categories including embedding-based, rule-based, GNN-based, pretrained Language Model based approaches. However, such dimensions have not been explored in TKG. To that end, we propose a novel temporal neuro-symbolic model, NeuSTIP, that performs link prediction and time interval prediction in a TKG. NeuSTIP learns temporal rules in the presence of the Allen predicates that ensure the temporal consistency between neighboring predicates in a given rule. We further design a unique scoring function that evaluates the confidence of the candidate answers while performing link prediction and time interval prediction by utilizing the learned rules. Our empirical evaluation on two time interval based TKGC datasets suggests that our model outperforms state-of-the-art models for both link prediction and the time interval prediction task.\n        \u25b3 Less\n      ",
    "title": "NeuSTIP: A Novel Neuro-Symbolic Model for Link and Time Prediction in Temporal Knowledge Graphs",
    "date": "15 May, 2023",
    "authors": [
      "Ishaan Singh",
      " Navdeep Kaur",
      " Garima Gaur",
      " Mausam"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08661",
    "paper_id": "2305.08661",
    "abstract": "\n        In this paper, our goal is to design a simple learning paradigm for long-tail visual recognition, which not only improves the robustness of the feature extractor but also alleviates the bias of the classifier towards head classes while reducing the training skills and overhead. We propose an efficient one-stage training strategy for long-tailed visual recognition called Global and Local Mixture Consistency cumulative learning (GLMC). Our core ideas are twofold: (1) a global and local mixture consistency loss improves the robustness of the feature extractor. Specifically, we generate two augmented batches by the global MixUp and local CutMix from the same batch data, respectively, and then use cosine similarity to minimize the difference. (2) A cumulative head tail soft label reweighted loss mitigates the head class bias problem. We use empirical class frequencies to reweight the mixed label of the head-tail class for long-tailed data and then balance the conventional loss and the rebalanced loss with a coefficient accumulated by epochs. Our approach achieves state-of-the-art accuracy on CIFAR10-LT, CIFAR100-LT, and ImageNet-LT datasets. Additional experiments on balanced ImageNet and CIFAR demonstrate that GLMC can significantly improve the generalization of backbones. Code is made publicly available at https://github.com/ynu-yangpeng/GLMC.\n        \u25b3 Less\n      ",
    "title": "Global and Local Mixture Consistency Cumulative Learning for Long-tailed Visual Recognitions",
    "date": "15 May, 2023",
    "authors": [
      "Fei Du",
      " Peng Yang",
      " Qi Jia",
      " Fengtao Nan",
      " Xiaoting Chen",
      " Yun Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08664",
    "paper_id": "2305.08664",
    "abstract": "\n        Being able to infer ground truth from the responses of multiple imperfect advisors is a problem of crucial importance in many decision-making applications, such as lending, trading, investment, and crowd-sourcing. In practice, however, gathering answers from a set of advisors has a cost. Therefore, finding an advisor selection strategy that retrieves a reliable answer and maximizes the overall utility is a challenging problem. To address this problem, we propose a novel strategy for optimally selecting a set of advisers in a sequential binary decision-making setting, where multiple decisions need to be made over time. Crucially, we assume no access to ground truth and no prior knowledge about the reliability of advisers. Specifically, our approach considers how to simultaneously (1) select advisors by balancing the advisors' costs and the value of making correct decisions, (2) learn the trustworthiness of advisers dynamically without prior information by asking multiple advisers, and (3) make optimal decisions without access to the ground truth, improving this over time. We evaluate our algorithm through several numerical experiments. The results show that our approach outperforms two other methods that combine state-of-the-art models.\n        \u25b3 Less\n      ",
    "title": "MADDM: Multi-Advisor Dynamic Binary Decision-Making by Maximizing the Utility",
    "date": "15 May, 2023",
    "authors": [
      "Zhaori Guo",
      " Timothy J. Norman",
      " Enrico H. Gerding"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08676",
    "paper_id": "2305.08676",
    "abstract": "\n        Using reinforcement learning for automated theorem proving has recently received much attention. Current approaches use representations of logical statements that often rely on the names used in these statements and, as a result, the models are generally not transferable from one domain to another. The size of these representations and whether to include the whole theory or part of it are other important decisions that affect the performance of these approaches as well as their runtime efficiency. In this paper, we present NIAGRA; an ensemble Name InvAriant Graph RepresentAtion. NIAGRA addresses this problem by using 1) improved Graph Neural Networks for learning name-invariant formula representations that is tailored for their unique characteristics and 2) an efficient ensemble approach for automated theorem proving. Our experimental evaluation shows state-of-the-art performance on multiple datasets from different domains with improvements up to 10% compared to the best learning-based approaches. Furthermore, transfer learning experiments show that our approach significantly outperforms other learning-based approaches by up to 28%.\n        \u25b3 Less\n      ",
    "title": "An Ensemble Approach for Automated Theorem Proving Based on Efficient Name Invariant Graph Neural Representations",
    "date": "15 May, 2023",
    "authors": [
      "Achille Fokoue",
      " Ibrahim Abdelaziz",
      " Maxwell Crouse",
      " Shajith Ikbal",
      " Akihiro Kishimoto",
      " Guilherme Lima",
      " Ndivhuwo Makondo",
      " Radu Marinescu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.11383",
    "paper_id": "2304.11383",
    "abstract": "\n        Deep learning and symbolic learning are two frequently employed methods in Sequential Recommendation (SR). Recent neural-symbolic SR models demonstrate their potential to enable SR to be equipped with concurrent perception and cognition capacities. However, neural-symbolic SR remains a challenging problem due to open issues like representing users and items in logical reasoning. In this paper, we combine the Deep Neural Network (DNN) SR models with logical reasoning and propose a general framework named Sequential Recommendation with Probabilistic Logical Reasoning (short for SR-PLR). This framework allows SR-PLR to benefit from both similarity matching and logical reasoning by disentangling feature embedding and logic embedding in the DNN and probabilistic logic network. To better capture the uncertainty and evolution of user tastes, SR-PLR embeds users and items with a probabilistic method and conducts probabilistic logical reasoning on users' interaction patterns. Then the feature and logic representations learned from the DNN and logic network are concatenated to make the prediction. Finally, experiments on various sequential recommendation models demonstrate the effectiveness of the SR-PLR.\n        \u25b3 Less\n      ",
    "title": "Sequential Recommendation with Probabilistic Logical Reasoning",
    "date": "15 May, 2023",
    "authors": [
      "Huanhuan Yuan",
      " Pengpeng Zhao",
      " Xuefeng Xian",
      " Guanfeng Liu",
      " Victor S. Sheng",
      " Lei Zhao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08694",
    "paper_id": "2305.08694",
    "abstract": "\n        Recently, Carlini et al. demonstrated the widely used model Stable Diffusion can regurgitate real training samples, which is troublesome from a copyright perspective. In this work, we provide an efficient extraction attack on par with the recent attack, with several order of magnitudes less network evaluations. In the process, we expose a new phenomena, which we dub template verbatims, wherein a diffusion model will regurgitate a training sample largely in tact. Template verbatims are harder to detect as they require retrieval and masking to correctly label. Furthermore, they are still generated by newer systems, even those which de-duplicate their training set, and we give insight into why they still appear during generation. We extract training images from several state of the art systems, including Stable Diffusion 2.0, Deep Image Floyd, and finally Midjourney v4. We release code to verify our extraction attack, perform the attack, as well as all extracted prompts at \\url{https://github.com/ryanwebster90/onestep-extraction}.\n        \u25b3 Less\n      ",
    "title": "A Reproducible Extraction of Training Images from Diffusion Models",
    "date": "15 May, 2023",
    "authors": [
      "Ryan Webster"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08702",
    "paper_id": "2305.08702",
    "abstract": "\n        Continual pre-training is the paradigm where pre-trained language models (PLMs) continually acquire fresh knowledge from growing data and gradually get upgraded. Before an upgraded PLM is released, we may have tuned the original PLM for various tasks and stored the adapted weights. However, when tuning the upgraded PLM, these outdated adapted weights will typically be ignored and discarded, causing a potential waste of resources. We bring this issue to the forefront and contend that proper algorithms for recycling outdated adapted weights should be developed. To this end, we formulate the task of recyclable tuning for continual pre-training. In pilot studies, we find that after continual pre-training, the upgraded PLM remains compatible with the outdated adapted weights to some extent. Motivated by this finding, we analyze the connection between continually pre-trained PLMs from two novel aspects, i.e., mode connectivity, and functional similarity. Based on the corresponding findings, we propose both an initialization-based method and a distillation-based method for our task. We demonstrate their feasibility in improving the convergence and performance for tuning the upgraded PLM. We also show that both methods can be combined to achieve better performance. The source codes are publicly available at https://github.com/thunlp/RecyclableTuning.\n        \u25b3 Less\n      ",
    "title": "Recyclable Tuning for Continual Pre-training",
    "date": "15 May, 2023",
    "authors": [
      "Yujia Qin",
      " Cheng Qian",
      " Xu Han",
      " Yankai Lin",
      " Huadong Wang",
      " Ruobing Xie",
      " Zhiyuan Liu",
      " Maosong Sun",
      " Jie Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.10541",
    "paper_id": "2207.10541",
    "abstract": "\n        Many deep generative models are defined as a push-forward of a Gaussian measure by a continuous generator, such as Generative Adversarial Networks (GANs) or Variational Auto-Encoders (VAEs). This work explores the latent space of such deep generative models. A key issue with these models is their tendency to output samples outside of the support of the target distribution when learning disconnected distributions. We investigate the relationship between the performance of these models and the geometry of their latent space. Building on recent developments in geometric measure theory, we prove a sufficient condition for optimality in the case where the dimension of the latent space is larger than the number of modes. Through experiments on GANs, we demonstrate the validity of our theoretical results and gain new insights into the latent space geometry of these models. Additionally, we propose a truncation method that enforces a simplicial cluster structure in the latent space and improves the performance of GANs.\n        \u25b3 Less\n      ",
    "title": "Unveiling the Latent Space Geometry of Push-Forward Generative Models",
    "date": "15 May, 2023",
    "authors": [
      "Thibaut Issenhuth",
      " Ugo Tanielian",
      " J\u00e9r\u00e9mie Mary",
      " David Picard"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.09593",
    "paper_id": "2204.09593",
    "abstract": "\n        Vision outlooker improves the performance of vision transformers, which implements a self-attention mechanism by adding an outlook attention, a form of local attention.\n  In natural language processing, as has been the case in computer vision and other domains, transformer-based models constitute the state-of-the-art for most processing tasks. In this domain, too, many authors have argued and demonstrated the importance of local context.\n  We present an outlook attention mechanism, COOL, for natural language processing. COOL, added on top of the self-attention layers of a transformer-based model, encodes local syntactic context considering word proximity and more pair-wise constraints than dynamic convolution used by existing approaches.\n  A comparative empirical performance evaluation of an implementation of COOL with different transformer-based models confirms the opportunity for improvement over a baseline using the original models alone for various natural language processing tasks, including question answering. The proposed approach achieves competitive performance with existing state-of-the-art methods on some tasks.\n        \u25b3 Less\n      ",
    "title": "COOL, a Context Outlooker, and its Application to Question Answering and other Natural Language Processing Tasks",
    "date": "15 May, 2023",
    "authors": [
      "Fangyi Zhu",
      " See-Kiong Ng",
      " St\u00e9phane Bressan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06588",
    "paper_id": "2305.06588",
    "abstract": "\n        Link Prediction on Hyper-relational Knowledge Graphs (HKG) is a worthwhile endeavor. HKG consists of hyper-relational facts (H-Facts), composed of a main triple and several auxiliary attribute-value qualifiers, which can effectively represent factually comprehensive information. The internal structure of HKG can be represented as a hypergraph-based representation globally and a semantic sequence-based representation locally. However, existing research seldom simultaneously models the graphical and sequential structure of HKGs, limiting HKGs' representation. To overcome this limitation, we propose a novel Hierarchical Attention model for HKG Embedding (HAHE), including global-level and local-level attention. The global-level attention can model the graphical structure of HKG using hypergraph dual-attention layers, while the local-level attention can learn the sequential structure inside H-Facts via heterogeneous self-attention layers. Experiment results indicate that HAHE achieves state-of-the-art performance in link prediction tasks on HKG standard datasets. In addition, HAHE addresses the issue of HKG multi-position prediction for the first time, increasing the applicability of the HKG link prediction task. Our code is publicly available.\n        \u25b3 Less\n      ",
    "title": "HAHE: Hierarchical Attention for Hyper-Relational Knowledge Graphs in Global and Local Level",
    "date": "15 May, 2023",
    "authors": [
      "Haoran Luo",
      " Haihong E",
      " Yuhao Yang",
      " Yikai Guo",
      " Mingzhi Sun",
      " Tianyu Yao",
      " Zichen Tang",
      " Kaiyang Wan",
      " Meina Song",
      " Wei Lin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08767",
    "paper_id": "2305.08767",
    "abstract": "\n        Load forecasting is a crucial topic in energy management systems (EMS) due to its vital role in optimizing energy scheduling and enabling more flexible and intelligent power grid systems. As a result, these systems allow power utility companies to respond promptly to demands in the electricity market. Deep learning (DL) models have been commonly employed in load forecasting problems supported by adaptation mechanisms to cope with the changing pattern of consumption by customers, known as concept drift. A drift magnitude threshold should be defined to design change detection methods to identify drifts. While the drift magnitude in load forecasting problems can vary significantly over time, existing literature often assumes a fixed drift magnitude threshold, which should be dynamically adjusted rather than fixed during system evolution. To address this gap, in this paper, we propose a dynamic drift-adaptive Long Short-Term Memory (DA-LSTM) framework that can improve the performance of load forecasting models without requiring a drift threshold setting. We integrate several strategies into the framework based on active and passive adaptation approaches. To evaluate DA-LSTM in real-life settings, we thoroughly analyze the proposed framework and deploy it in a real-world problem through a cloud-based environment. Efficiency is evaluated in terms of the prediction performance of each approach and computational cost. The experiments show performance improvements on multiple evaluation metrics achieved by our framework compared to baseline methods from the literature. Finally, we present a trade-off analysis between prediction performance and computational costs.\n        \u25b3 Less\n      ",
    "title": "DA-LSTM: A Dynamic Drift-Adaptive Learning Framework for Interval Load Forecasting with LSTM Networks",
    "date": "15 May, 2023",
    "authors": [
      "Firas Bayram",
      " Phil Aupke",
      " Bestoun S. Ahmed",
      " Andreas Kassler",
      " Andreas Theocharis",
      " Jonas Forsman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2208.09554",
    "paper_id": "2208.09554",
    "abstract": "\n        Autonomous agents are able to draw on a wide variety of potential sources of task knowledge; however current approaches invariably focus on only one or two. Here we investigate the challenges and impact of exploiting diverse knowledge sources to learn online, in one-shot, new tasks for a simulated office mobile robot. The resulting agent, developed in the Soar cognitive architecture, uses the following sources of domain and task knowledge: interaction with the environment, task execution and search knowledge, human natural language instruction, and responses retrieved from a large language model (GPT-3). We explore the distinct contributions of these knowledge sources and evaluate the performance of different combinations in terms of learning correct task knowledge and human workload. Results show that an agent's online integration of diverse knowledge sources improves one-shot task learning overall, reducing human feedback needed for rapid and reliable task learning.\n        \u25b3 Less\n      ",
    "title": "Integrating Diverse Knowledge Sources for Online One-shot Learning of Novel Tasks",
    "date": "15 May, 2023",
    "authors": [
      "James R. Kirk",
      " Robert E. Wray",
      " Peter Lindes",
      " John E. Laird"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.07689",
    "paper_id": "2304.07689",
    "abstract": "\n        Deep metric learning techniques have been used for visual representation in various supervised and unsupervised learning tasks through learning embeddings of samples with deep networks. However, classic approaches, which employ a fixed distance metric as a similarity function between two embeddings, may lead to suboptimal performance for capturing the complex data distribution. The Bregman divergence generalizes measures of various distance metrics and arises throughout many fields of deep metric learning. In this paper, we first show how deep metric learning loss can arise from the Bregman divergence. We then introduce a novel method for learning empirical Bregman divergence directly from data based on parameterizing the convex function underlying the Bregman divergence with a deep learning setting. We further experimentally show that our approach performs effectively on five popular public datasets compared to other SOTA deep metric learning methods, particularly for pattern recognition problems.\n        \u25b3 Less\n      ",
    "title": "Learning Empirical Bregman Divergence for Uncertain Distance Representation",
    "date": "15 May, 2023",
    "authors": [
      "Zhiyuan Li",
      " Ziru Liu",
      " Anna Zou",
      " Anca L. Ralescu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10447",
    "paper_id": "2305.10447",
    "abstract": "\n        Neural networks and in particular the attention mechanism have brought significant advances to the field of Automated Essay Scoring. Many of these systems use a regression-based model which may be prone to underfitting when the model only predicts the mean of the training data. In this paper, we present a dynamic loss function that creates an incentive for the model to predict with the correct distribution, as well as predicting the correct values. Our loss function achieves this goal without sacrificing any performance achieving a Quadratic Weighted Kappa score of 0.752 on the Automated Student Assessment Prize Automated Essay Scoring dataset.\n        \u25b3 Less\n      ",
    "title": "The Effectiveness of a Dynamic Loss Function in Neural Network Based Automated Essay Scoring",
    "date": "15 May, 2023",
    "authors": [
      "Oscar Morris"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.12986",
    "paper_id": "2304.12986",
    "abstract": "\n        The development of large-scale Chinese language models is flourishing, yet there is a lack of corresponding capability assessments. Therefore, we propose a test to measure the multitask accuracy of large Chinese language models. This test encompasses four major domains, including medicine, law, psychology, and education, with 15 subtasks in medicine and 8 subtasks in education. We found that the best-performing models in the zero-shot setting outperformed the worst-performing models by nearly 18.6 percentage points on average. Across the four major domains, the highest average zero-shot accuracy of all models is 0.512. In the subdomains, only the GPT-3.5-turbo model achieved a zero-shot accuracy of 0.693 in clinical medicine, which was the highest accuracy among all models across all subtasks. All models performed poorly in the legal domain, with the highest zero-shot accuracy reaching only 0.239. By comprehensively evaluating the breadth and depth of knowledge across multiple disciplines, this test can more accurately identify the shortcomings of the models.\n        \u25b3 Less\n      ",
    "title": "Measuring Massive Multitask Chinese Understanding",
    "date": "15 May, 2023",
    "authors": [
      "Hui Zeng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2106.01263",
    "paper_id": "2106.01263",
    "abstract": "\n        Sample-and-rank is a key decoding strategy for modern generation-based dialogue systems. It helps achieve diverse and high-quality responses by selecting an answer from a small pool of generated candidates. The current state-of-the-art ranking methods mainly use an encoding paradigm called Cross-Encoder, which separately encodes each context-candidate pair and ranks the candidates according to their fitness scores. However, Cross-Encoder repeatedly encodes the same lengthy context for each candidate, resulting in high computational costs. Poly-Encoder addresses the above problems by reducing the interaction between context and candidates, but with a price of performance drop. In this work, we develop a new paradigm called Uni-Encoder, that keeps the full attention over each pair as in Cross-Encoder while only encoding the context once, as in Poly-Encoder. Uni-Encoder encodes all the candidates with the context in one forward pass. We use the same positional embedding for all candidates to ensure they are treated equally and design a new attention mechanism to avoid confusion. Our Uni-Encoder can simulate other ranking paradigms using different attention and response concatenation methods. Extensive experiments show that our proposed paradigm achieves new state-of-the-art results on four benchmark datasets with high computational efficiency. For instance, it improves R10@1 by 2.9% with an approximately 4X faster inference speed on the Ubuntu V2 dataset.\n        \u25b3 Less\n      ",
    "title": "Uni-Encoder: A Fast and Accurate Response Selection Paradigm for Generation-Based Dialogue Systems",
    "date": "15 May, 2023",
    "authors": [
      "Chiyu Song",
      " Hongliang He",
      " Haofei Yu",
      " Pengfei Fang",
      " Leyang Cui",
      " Zhenzhong Lan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.07836",
    "paper_id": "2301.07836",
    "abstract": "\n        Self supervision and natural language supervision have emerged as two exciting ways to train general purpose image encoders which excel at a variety of downstream tasks. Recent works such as M3AE and SLIP have suggested that these approaches can be effectively combined, but most notably their results use small pre-training datasets (<50M samples) and don't effectively reflect the large-scale regime (>100M examples) that is commonly used for these approaches. Here we investigate whether a similar approach can be effective when trained with a much larger amount of data. We find that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit (as evaluated on a suite of common vision tasks) over CLIP when trained on a large corpus of 1.4B images. Our work provides some much needed clarity into the effectiveness (or lack thereof) of self supervision for large-scale image-text training.\n        \u25b3 Less\n      ",
    "title": "Masked Autoencoding Does Not Help Natural Language Supervision at Scale",
    "date": "15 May, 2023",
    "authors": [
      "Floris Weers",
      " Vaishaal Shankar",
      " Angelos Katharopoulos",
      " Yinfei Yang",
      " Tom Gunter"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08842",
    "paper_id": "2305.08842",
    "abstract": "\n        This work examines the challenges of training neural networks using vector quantization using straight-through estimation. We find that a primary cause of training instability is the discrepancy between the model embedding and the code-vector distribution. We identify the factors that contribute to this issue, including the codebook gradient sparsity and the asymmetric nature of the commitment loss, which leads to misaligned code-vector assignments. We propose to address this issue via affine re-parameterization of the code vectors. Additionally, we introduce an alternating optimization to reduce the gradient error introduced by the straight-through estimation. Moreover, we propose an improvement to the commitment loss to ensure better alignment between the codebook representation and the model embedding. These optimization methods improve the mathematical approximation of the straight-through estimation and, ultimately, the model performance. We demonstrate the effectiveness of our methods on several common model architectures, such as AlexNet, ResNet, and ViT, across various tasks, including image classification and generative modeling.\n        \u25b3 Less\n      ",
    "title": "Straightening Out the Straight-Through Estimator: Overcoming Optimization Challenges in Vector Quantized Networks",
    "date": "15 May, 2023",
    "authors": [
      "Minyoung Huh",
      " Brian Cheung",
      " Pulkit Agrawal",
      " Phillip Isola"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08848",
    "paper_id": "2305.08848",
    "abstract": "\n        Large language models (LLMs) such as GPT-3 and GPT-4 are powerful but their weights are often publicly unavailable and their immense sizes make the models difficult to be tuned with common hardware. As a result, effectively tuning these models with large-scale supervised data can be challenging. As an alternative, In-Context Learning (ICL) can only use a small number of supervised examples due to context length limits. In this paper, we propose Super In-Context Learning (SuperICL) which allows black-box LLMs to work with locally fine-tuned smaller models, resulting in superior performance on supervised tasks. Our experiments demonstrate that SuperICL can improve performance beyond state-of-the-art fine-tuned models while addressing the instability problem of in-context learning. Furthermore, SuperICL can enhance the capabilities of smaller models, such as multilinguality and interpretability.\n        \u25b3 Less\n      ",
    "title": "Small Models are Valuable Plug-ins for Large Language Models",
    "date": "15 May, 2023",
    "authors": [
      "Canwen Xu",
      " Yichong Xu",
      " Shuohang Wang",
      " Yang Liu",
      " Chenguang Zhu",
      " Julian McAuley"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08852",
    "paper_id": "2305.08852",
    "abstract": "\n        Hyperparameter optimization is crucial to achieving high performance in deep learning. On top of the performance, other criteria such as inference time or memory requirement often need to be optimized due to some practical reasons. This motivates research on multi-objective optimization (MOO). However, Pareto fronts of MOO methods are often shown without considering the variability caused by random seeds and this makes the performance stability evaluation difficult. Although there is a concept named empirical attainment surface to enable the visualization with uncertainty over multiple runs, there is no major Python package for empirical attainment surface. We, therefore, develop a Python package for this purpose and describe the usage. The package is available at https://github.com/nabenabe0928/empirical-attainment-func.\n        \u25b3 Less\n      ",
    "title": "Python Tool for Visualizing Variability of Pareto Fronts over Multiple Runs",
    "date": "15 May, 2023",
    "authors": [
      "Shuhei Watanabe"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08929",
    "paper_id": "2305.08929",
    "abstract": "\n        Deep learning-based approaches, such as AlphaFold2 (AF2), have significantly advanced protein tertiary structure prediction, achieving results comparable to real biological experimental methods. While AF2 has shown limitations in predicting the effects of mutations, its robustness against sequence mutations remains to be determined. Starting with the wild-type (WT) sequence, we investigate adversarial sequences generated via an evolutionary approach, which AF2 predicts to be substantially different from WT. Our experiments on CASP14 reveal that by modifying merely three residues in the protein sequence using a combination of replacement, deletion, and insertion strategies, the alteration in AF2's predictions, as measured by the Local Distance Difference Test (lDDT), reaches 46.61. Moreover, when applied to a specific protein, SPNS2, our proposed algorithm successfully identifies biologically meaningful residues critical to protein structure determination and potentially indicates alternative conformations, thus significantly expediting the experimental process.\n        \u25b3 Less\n      ",
    "title": "AF2-Mutation: Adversarial Sequence Mutations against AlphaFold2 on Protein Tertiary Structure Prediction",
    "date": "15 May, 2023",
    "authors": [
      "Zhongju Yuan",
      " Tao Shen",
      " Sheng Xu",
      " Leiye Yu",
      " Ruobing Ren",
      " Siqi Sun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08932",
    "paper_id": "2305.08932",
    "abstract": "\n        Exploring in environments with high-dimensional observations is hard. One promising approach for exploration is to use intrinsic rewards, which often boils down to estimating \"novelty\" of states, transitions, or trajectories with deep networks. Prior works have shown that conditional prediction objectives such as masked autoencoding can be seen as stochastic estimation of pseudo-likelihood. We show how this perspective naturally leads to a unified view on existing intrinsic reward approaches: they are special cases of conditional prediction, where the estimation of novelty can be seen as pseudo-likelihood estimation with different mask distributions. From this view, we propose a general framework for deriving intrinsic rewards -- Masked Input Modeling for Exploration (MIMEx) -- where the mask distribution can be flexibly tuned to control the difficulty of the underlying conditional prediction task. We demonstrate that MIMEx can achieve superior results when compared against competitive baselines on a suite of challenging sparse-reward visuomotor tasks.\n        \u25b3 Less\n      ",
    "title": "MIMEx: Intrinsic Rewards from Masked Input Modeling",
    "date": "15 May, 2023",
    "authors": [
      "Toru Lin",
      " Allan Jabri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.05777",
    "paper_id": "2211.05777",
    "abstract": "\n        Cancer is one of the leading causes of death worldwide. It is caused by a variety of genetic mutations, which makes every instance of the disease unique. Since chemotherapy can have extremely severe side effects, each patient requires a personalized treatment plan. Finding the dosages that maximize the beneficial effects of the drugs and minimize their adverse side effects is vital. Deep neural networks automate and improve drug selection. However, they require a lot of data to be trained on. Therefore, there is a need for machine-learning approaches that require less data. Hybrid quantum neural networks were shown to provide a potential advantage in problems where training data availability is limited. We propose a novel hybrid quantum neural network for drug response prediction, based on a combination of convolutional, graph convolutional, and deep quantum neural layers of 8 qubits with 363 layers. We test our model on the reduced Genomics of Drug Sensitivity in Cancer dataset and show that the hybrid quantum model outperforms its classical analog by 15% in predicting IC50 drug effectiveness values. The proposed hybrid quantum machine learning model is a step towards deep quantum data-efficient algorithms with thousands of quantum gates for solving problems in personalized medicine, where data collection is a challenge.\n        \u25b3 Less\n      ",
    "title": "Hybrid quantum neural network for drug response prediction",
    "date": "15 May, 2023",
    "authors": [
      "Asel Sagingalieva",
      " Mohammad Kordzanganeh",
      " Nurbolat Kenbayev",
      " Daria Kosichkina",
      " Tatiana Tomashuk",
      " Alexey Melnikov"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.15613",
    "paper_id": "2211.15613",
    "abstract": "\n        Translating training data into many languages has emerged as a practical solution for improving cross-lingual transfer. For tasks that involve span-level annotations, such as information extraction or question answering, an additional label projection step is required to map annotated spans onto the translated texts. Recently, a few efforts have utilized a simple mark-then-translate method to jointly perform translation and projection by inserting special markers around the labeled spans in the original sentence. However, as far as we are aware, no empirical analysis has been conducted on how this approach compares to traditional annotation projection based on word alignment. In this paper, we present an extensive empirical study across 57 languages and three tasks (QA, NER, and Event Extraction) to evaluate the effectiveness and limitations of both methods, filling an important gap in the literature. Experimental results show that our optimized version of mark-then-translate, which we call EasyProject, is easily applied to many languages and works surprisingly well, outperforming the more complex word alignment-based methods. We analyze several key factors that affect the end-task performance, and show EasyProject works well because it can accurately preserve label span boundaries after translation. We will publicly release all our code and data.\n        \u25b3 Less\n      ",
    "title": "Frustratingly Easy Label Projection for Cross-lingual Transfer",
    "date": "15 May, 2023",
    "authors": [
      "Yang Chen",
      " Chao Jiang",
      " Alan Ritter",
      " Wei Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.14211",
    "paper_id": "2304.14211",
    "abstract": "\n        The goal of the linear law-based feature space transformation (LLT) algorithm is to assist with the classification of univariate and multivariate time series. The presented R package, called LLT, implements this algorithm in a flexible yet user-friendly way. This package first splits the instances into training and test sets. It then utilizes time-delay embedding and spectral decomposition techniques to identify the governing patterns (called linear laws) of each input sequence (initial feature) within the training set. Finally, it applies the linear laws of the training set to transform the initial features of the test set. These steps are performed by three separate functions called trainTest, trainLaw, and testTrans. Their application requires a predefined data structure; however, for fast calculation, they use only built-in functions. The LLT R package and a sample dataset with the appropriate data structure are publicly available on GitHub.\n        \u25b3 Less\n      ",
    "title": "LLT: An R package for Linear Law-based Feature Space Transformation",
    "date": "15 May, 2023",
    "authors": [
      "Marcell T. Kurbucz",
      " P\u00e9ter P\u00f3sfay",
      " Antal Jakov\u00e1c"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14364",
    "paper_id": "2305.14364",
    "abstract": "\n        Music therapy has emerged recently as a successful intervention that improves patient's outcome in a large range of neurological and mood disorders without adverse effects. Brain networks are entrained to music in ways that can be explained both via top-down and bottom-up processes. In particular, the direct interaction of auditory with the motor and the reward system via a predictive framework explains the efficacy of music-based interventions in motor rehabilitation. In this manuscript, we provide a brief overview of current theories of music perception and processing. Subsequently, we summarise evidence of music-based interventions primarily in motor, emotional and cardiovascular regulation. We highlight opportunities to improve quality of life and reduce stress beyond the clinic environment and in healthy individuals. This relatively unexplored area requires an understanding of how we can personalise and automate music selection processes to fit individuals needs and tasks via feedback loops mediated by measurements of neuro-physiological responses.\n        \u25b3 Less\n      ",
    "title": "Towards personalised music-therapy; a neurocomputational modelling perspective",
    "date": "15 May, 2023",
    "authors": [
      "Nicole Lai",
      " Marios Philiastides",
      " Fahim Kawsar",
      " Fani Deligianni"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08985",
    "paper_id": "2305.08985",
    "abstract": "\n        Federated Learning is a distributed machine learning approach that enables geographically distributed data silos to collaboratively learn a joint machine learning model without sharing data. Most of the existing work operates on unstructured data, such as images or text, or on structured data assumed to be consistent across the different sites. However, sites often have different schemata, data formats, data values, and access patterns. The field of data integration has developed many methods to address these challenges, including techniques for data exchange and query rewriting using declarative schema mappings, and for entity linkage. Therefore, we propose an architectural vision for an end-to-end Federated Learning and Integration system, incorporating the critical steps of data harmonization and data imputation, to spur further research on the intersection of data management information systems and machine learning.\n        \u25b3 Less\n      ",
    "title": "Federated Learning over Harmonized Data Silos",
    "date": "15 May, 2023",
    "authors": [
      "Dimitris Stripelis",
      " Jose Luis Ambite"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15420",
    "paper_id": "2305.15420",
    "abstract": "\n        Building Information Modeling (BIM) technology is a key component of modern construction engineering and project management workflows. As-is BIM models that represent the spatial reality of a project site can offer crucial information to stakeholders for construction progress monitoring, error checking, and building maintenance purposes. Geometric methods for automatically converting raw scan data into BIM models (Scan-to-BIM) often fail to make use of higher-level semantic information in the data. Whereas, semantic segmentation methods only output labels at the point level without creating object level models that is necessary for BIM. To address these issues, this research proposes a hybrid semantic-geometric approach for clutter-resistant floorplan generation from laser-scanned building point clouds. The input point clouds are first pre-processed by normalizing the coordinate system and removing outliers. Then, a semantic segmentation network based on PointNet++ is used to label each point as ceiling, floor, wall, door, stair, and clutter. The clutter points are removed whereas the wall, door, and stair points are used for 2D floorplan generation. A region-growing segmentation algorithm paired with geometric reasoning rules is applied to group the points together into individual building elements. Finally, a 2-fold Random Sample Consensus (RANSAC) algorithm is applied to parameterize the building elements into 2D lines which are used to create the output floorplan. The proposed method is evaluated using the metrics of precision, recall, Intersection-over-Union (IOU), Betti error, and warping error.\n        \u25b3 Less\n      ",
    "title": "A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds",
    "date": "15 May, 2023",
    "authors": [
      "Seongyong Kim",
      " Yosuke Yajima",
      " Jisoo Park",
      " Jingdao Chen",
      " Yong K. Cho"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.05274",
    "paper_id": "2209.05274",
    "abstract": "\n        In machine learning, training data often capture the behaviour of multiple subgroups of some underlying human population. This behaviour can often be modelled as observations of an unknown dynamical system with an unobserved state. When the training data for the subgroups are not controlled carefully, however, under-representation bias arises. To counter under-representation bias, we introduce two natural notions of fairness in time-series forecasting problems: subgroup fairness and instantaneous fairness. These notions extend predictive parity to the learning of dynamical systems. We also show globally convergent methods for the fairness-constrained learning problems using hierarchies of convexifications of non-commutative polynomial optimisation problems. We also show that by exploiting sparsity in the convexifications, we can reduce the run time of our methods considerably. Our empirical results on a biased data set motivated by insurance applications and the well-known COMPAS data set demonstrate the efficacy of our methods.\n        \u25b3 Less\n      ",
    "title": "Fairness in Forecasting of Observations of Linear Dynamical Systems",
    "date": "15 May, 2023",
    "authors": [
      "Quan Zhou",
      " Jakub Marecek",
      " Robert N. Shorten"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.03789",
    "paper_id": "2204.03789",
    "abstract": "\n        Incorporating interdisciplinary perspectives is seen as an essential step towards enhancing artificial intelligence (AI) ethics. In this regard, the field of arts is perceived to play a key role in elucidating diverse historical and cultural narratives, serving as a bridge across research communities. Most of the works that examine the interplay between the field of arts and AI ethics concern digital artworks, largely exploring the potential of computational tools in being able to surface biases in AI systems. In this paper, we investigate a complementary direction--that of uncovering the unique socio-cultural perspectives embedded in human-made art, which in turn, can be valuable in expanding the horizon of AI ethics. Through semi-structured interviews across sixteen artists, art scholars, and researchers of diverse Indian art forms like music, sculpture, painting, floor drawings, dance, etc., we explore how {\\it non-Western} ethical abstractions, methods of learning, and participatory practices observed in Indian arts, one of the most ancient yet perpetual and influential art traditions, can shed light on aspects related to ethical AI systems. Through a case study concerning the Indian dance system (i.e. the {\\it `Natyashastra'}), we analyze potential pathways towards enhancing ethics in AI systems. Insights from our study outline the need for (1) incorporating empathy in ethical AI algorithms, (2) integrating multimodal data formats for ethical AI system design and development, (3) viewing AI ethics as a dynamic, diverse, cumulative, and shared process rather than as a static, self-contained framework to facilitate adaptability without annihilation of values (4) consistent life-long learning to enhance AI accountability\n        \u25b3 Less\n      ",
    "title": "Broadening AI Ethics Narratives: An Indic Art View",
    "date": "15 May, 2023",
    "authors": [
      "Ajay Divakaran",
      " Aparna Sridhar",
      " Ramya Srinivasan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09018",
    "paper_id": "2305.09018",
    "abstract": "\n        Exploiting the recent advancements in artificial intelligence, showcased by ChatGPT and DALL-E, in real-world applications necessitates vast, domain-specific, and publicly accessible datasets. Unfortunately, the scarcity of such datasets poses a significant challenge for researchers aiming to apply these breakthroughs in engineering design. Synthetic datasets emerge as a viable alternative. However, practitioners are often uncertain about generating high-quality datasets that accurately represent real-world data and are suitable for the intended downstream applications. This study aims to fill this knowledge gap by proposing comprehensive guidelines for generating, annotating, and validating synthetic datasets. The trade-offs and methods associated with each of these aspects are elaborated upon. Further, the practical implications of these guidelines are illustrated through the creation of a turbo-compressors dataset. The study underscores the importance of thoughtful sampling methods to ensure the appropriate size, diversity, utility, and realism of a dataset. It also highlights that design diversity does not equate to performance diversity or realism. By employing test sets that represent uniform, real, or task-specific samples, the influence of sample size and sampling strategy is scrutinized. Overall, this paper offers valuable insights for researchers intending to create and publish synthetic datasets for engineering design, thereby paving the way for more effective applications of AI advancements in the field. The code and data for the dataset and methods are made publicly accessible at https://github.com/cyrilpic/radcomp .\n        \u25b3 Less\n      ",
    "title": "DATED: Guidelines for Creating Synthetic Datasets for Engineering Design Applications",
    "date": "15 May, 2023",
    "authors": [
      "Cyril Picard",
      " J\u00fcrg Schiffmann",
      " Faez Ahmed"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.04543",
    "paper_id": "2207.04543",
    "abstract": "\n        Building learning agents that can progressively learn and accumulate knowledge is the core goal of the continual learning (CL) research field. Unfortunately, training a model on new data usually compromises the performance on past data. In the CL literature, this effect is referred to as catastrophic forgetting (CF). CF has been largely studied, and a plethora of methods have been proposed to address it on short sequences of non-overlapping tasks. In such setups, CF always leads to a quick and significant drop in performance in past tasks. Nevertheless, despite CF, recent work showed that SGD training on linear models accumulates knowledge in a CL regression setup. This phenomenon becomes especially visible when tasks reoccur. We might then wonder if DNNs trained with SGD or any standard gradient-based optimization accumulate knowledge in such a way. Such phenomena would have interesting consequences for applying DNNs to real continual scenarios. Indeed, standard gradient-based optimization methods are significantly less computationally expensive than existing CL algorithms. In this paper, we study the progressive knowledge accumulation (KA) in DNNs trained with gradient-based algorithms in long sequences of tasks with data re-occurrence. We propose a new framework, SCoLe (Scaling Continual Learning), to investigate KA and discover that catastrophic forgetting has a limited effect on DNNs trained with SGD. When trained on long sequences with data sparsely re-occurring, the overall accuracy improves, which might be counter-intuitive given the CF phenomenon. We empirically investigate KA in DNNs under various data occurrence frequencies and propose simple and scalable strategies to increase knowledge accumulation in DNNs.\n        \u25b3 Less\n      ",
    "title": "Challenging Common Assumptions about Catastrophic Forgetting",
    "date": "15 May, 2023",
    "authors": [
      "Timoth\u00e9e Lesort",
      " Oleksiy Ostapenko",
      " Diganta Misra",
      " Md Rifat Arefin",
      " Pau Rodr\u00edguez",
      " Laurent Charlin",
      " Irina Rish"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09064",
    "paper_id": "2305.09064",
    "abstract": "\n        Improving our understanding of how humans perceive AI teammates is an important foundation for our general understanding of human-AI teams. Extending relevant work from cognitive science, we propose a framework based on item response theory for modeling these perceptions. We apply this framework to real-world experiments, in which each participant works alongside another person or an AI agent in a question-answering setting, repeatedly assessing their teammate's performance. Using this experimental data, we demonstrate the use of our framework for testing research questions about people's perceptions of both AI agents and other people. We contrast mental models of AI teammates with those of human teammates as we characterize the dimensionality of these mental models, their development over time, and the influence of the participants' own self-perception. Our results indicate that people expect AI agents' performance to be significantly better on average than the performance of other humans, with less variation across different types of problems. We conclude with a discussion of the implications of these findings for human-AI interaction.\n        \u25b3 Less\n      ",
    "title": "Capturing Humans' Mental Models of AI: An Item Response Theory Approach",
    "date": "15 May, 2023",
    "authors": [
      "Markelle Kelly",
      " Aakriti Kumar",
      " Padhraic Smyth",
      " Mark Steyvers"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.11261",
    "paper_id": "2212.11261",
    "abstract": "\n        Nine language-vision AI models trained on web scrapes with the Contrastive Language-Image Pretraining (CLIP) objective are evaluated for evidence of a bias studied by psychologists: the sexual objectification of girls and women, which occurs when a person's human characteristics, such as emotions, are disregarded and the person is treated as a body. We replicate three experiments in psychology quantifying sexual objectification and show that the phenomena persist in AI. A first experiment uses standardized images of women from the Sexual OBjectification and EMotion Database, and finds that human characteristics are disassociated from images of objectified women: the model's recognition of emotional state is mediated by whether the subject is fully or partially clothed. Embedding association tests (EATs) return significant effect sizes for both anger (d >0.80) and sadness (d >0.50), associating images of fully clothed subjects with emotions. GRAD-CAM saliency maps highlight that CLIP gets distracted from emotional expressions in objectified images. A second experiment measures the effect in a representative application: an automatic image captioner (Antarctic Captions) includes words denoting emotion less than 50% as often for images of partially clothed women than for images of fully clothed women. A third experiment finds that images of female professionals (scientists, doctors, executives) are likely to be associated with sexual descriptions relative to images of male professionals. A fourth experiment shows that a prompt of \"a [age] year old girl\" generates sexualized images (as determined by an NSFW classifier) up to 73% of the time for VQGAN-CLIP and Stable Diffusion; the corresponding rate for boys never surpasses 9%. The evidence indicates that language-vision AI models trained on web scrapes learn biases of sexual objectification, which propagate to downstream applications.\n        \u25b3 Less\n      ",
    "title": "Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias",
    "date": "15 May, 2023",
    "authors": [
      "Robert Wolfe",
      " Yiwei Yang",
      " Bill Howe",
      " Aylin Caliskan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09691",
    "paper_id": "2305.09691",
    "abstract": "\n        Recent algorithms of time-series anomaly detection have been evaluated by applying a Point Adjustment (PA) protocol. However, the PA protocol has a problem of overestimating the performance of the detection algorithms because it only depends on the number of detected abnormal segments and their size. We propose a novel evaluation protocol called the Point-Adjusted protocol with decay function (PAdf) to evaluate the time-series anomaly detection algorithm by reflecting the following ideal requirements: detect anomalies quickly and accurately without false alarms. This paper theoretically and experimentally shows that the PAdf protocol solves the over- and under-estimation problems of existing protocols such as PA and PA\\%K. By conducting re-evaluations of SOTA models in benchmark datasets, we show that the PA protocol only focuses on finding many anomalous segments, whereas the score of the PAdf protocol considers not only finding many segments but also detecting anomalies quickly without delay.\n        \u25b3 Less\n      ",
    "title": "Evaluation Strategy of Time-series Anomaly Detection with Decay Function",
    "date": "15 May, 2023",
    "authors": [
      "Yongwan Gim",
      " Kyushik Min"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.16944",
    "paper_id": "2211.16944",
    "abstract": "\n        Biomedical named entity recognition (BioNER) seeks to automatically recognize biomedical entities in natural language text, serving as a necessary foundation for downstream text mining tasks and applications such as information extraction and question answering. Manually labeling training data for the BioNER task is costly, however, due to the significant domain expertise required for accurate annotation. The resulting data scarcity causes current BioNER approaches to be prone to overfitting, to suffer from limited generalizability, and to address a single entity type at a time (e.g., gene or disease). We therefore propose a novel all-in-one (AIO) scheme that uses external data from existing annotated resources to enhance the accuracy and stability of BioNER models. We further present AIONER, a general-purpose BioNER tool based on cutting-edge deep learning and our AIO schema. We evaluate AIONER on 14 BioNER benchmark tasks and show that AIONER is effective, robust, and compares favorably to other state-of-the-art approaches such as multi-task learning. We further demonstrate the practical utility of AIONER in three independent tasks to recognize entity types not previously seen in training data, as well as the advantages of AIONER over existing methods for processing biomedical text at a large scale (e.g., the entire PubMed data).\n        \u25b3 Less\n      ",
    "title": "AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning",
    "date": "15 May, 2023",
    "authors": [
      "Ling Luo",
      " Chih-Hsuan Wei",
      " Po-Ting Lai",
      " Robert Leaman",
      " Qingyu Chen",
      " Zhiyong Lu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09107",
    "paper_id": "2305.09107",
    "abstract": "\n        Conventional Transformer-based Video Question Answering (VideoQA) approaches generally encode frames independently through one or more image encoders followed by interaction between frames and question. However, such schema would incur significant memory use and inevitably slow down the training and inference speed. In this work, we present a highly efficient approach for VideoQA based on existing vision-language pre-trained models where we concatenate video frames to a n\u00d7nn\\times n matrix and then convert it to one image. By doing so, we reduce the use of the image encoder from n2n^{2} to 11 while maintaining the temporal structure of the original video. Experimental results on MSRVTT and TrafficQA show that our proposed approach achieves state-of-the-art performance with nearly 4\u00d74\\times faster speed and only 30% memory use. We show that by integrating our approach into VideoQA systems we can achieve comparable, even superior, performance with a significant speed up for training and inference. We believe the proposed approach can facilitate VideoQA-related research by reducing the computational requirements for those who have limited access to budgets and resources. Our code will be made publicly available for research use.\n        \u25b3 Less\n      ",
    "title": "Is a Video worth \nn\u00d7n\n Images? A Highly Efficient Approach to Transformer-based Video Question Answering",
    "date": "15 May, 2023",
    "authors": [
      "Chenyang Lyu",
      " Tianbo Ji",
      " Yvette Graham",
      " Jennifer Foster"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09111",
    "paper_id": "2305.09111",
    "abstract": "\n        The recent popularity of Wordle has revived interest in guessing games. We develop a general method for finding optimal strategies for guessing games while avoiding an exhaustive search. Our main contributions are several theorems that build towards a general theory to prove the optimality of a strategy for a guessing game. This work is developed to apply to any guessing game, but we use Wordle as an example to present concrete results.\n        \u25b3 Less\n      ",
    "title": "On Optimal Strategies for Wordle and General Guessing Games",
    "date": "15 May, 2023",
    "authors": [
      "Michael Cunanan",
      " Michael Thielscher"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.15206",
    "paper_id": "2209.15206",
    "abstract": "\n        Current methods for prompt learning in zeroshot scenarios widely rely on a development set with sufficient human-annotated data to select the best-performing prompt template a posteriori. This is not ideal because in a realworld zero-shot scenario of practical relevance, no labelled data is available. Thus, we propose a simple yet effective method for screening reasonable prompt templates in zero-shot text classification: Perplexity Selection (Perplection). We hypothesize that language discrepancy can be used to measure the efficacy of prompt templates, and thereby develop a substantiated perplexity-based scheme allowing for forecasting the performance of prompt templates in advance. Experiments show that our method leads to improved prediction performance in a realistic zero-shot setting, eliminating the need for any labelled examples.\n        \u25b3 Less\n      ",
    "title": "What Makes Pre-trained Language Models Better Zero-shot Learners?",
    "date": "15 May, 2023",
    "authors": [
      "Jinghui Lu",
      " Dongsheng Zhu",
      " Weidong Han",
      " Rui Zhao",
      " Brian Mac Namee",
      " Fei Tan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.00266",
    "paper_id": "2211.00266",
    "abstract": "\n        As the excellent tools for aiding communication,intelligent reflecting surface (IRS) and unmanned aerial vehicle (UAV) can extend the coverage area, remove blind area, and achieve a dramatic rate improvement. In this paper, we improve the secrecy rate (SR) performance at directional modulation (DM) networks using IRS and UAV in combination. To fully explore the benefits of IRS and UAV, two efficient methods are proposed to enhance SR performance. The first approach computes the confidential message (CM) beamforming vector by maximizing the SR, and the signal-to-leakage-noise ratio (SLNR) method is used to optimize the IRS phase shift matrix, which is called Max-SR-SLNR. Here, Eve is maximally interfered by transmitting artificial noise (AN) along the direct path and null-space projection (NSP) on the remaining two channels. To reduce the computational complexity, the CM, AN beamforming and IRS phase shift design are independently designed in the following methods. The CM beamforming vector is constructed based on maximum ratio transmission (MRT) criteria along the channel from Alice-to-IRS, and phase shift matrix of IRS is directly given by phase alignment (PA) method. This method is called MRT-NSP-PA. Simulation results show that the SR performance of the Max-SR-SLNR method outperforms the MRT-NSP-PA method in the cases of small-scale and medium-scale IRSs, and the latter approaches the former in performance as IRS tends to lager-scale.\n        \u25b3 Less\n      ",
    "title": "Two Low-complexity Efficient Beamformers for IRS-and-UAV-aided Directional Modulation Networks",
    "date": "15 May, 2023",
    "authors": [
      "Yeqing Lin",
      " Feng Shu",
      " Yuxiang Zheng",
      " Jing Liu",
      " Rongen Dong",
      " Xun Chen",
      " Yue Wu",
      " Feng Shu",
      " Jiangzhou Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.10372",
    "paper_id": "2209.10372",
    "abstract": "\n        Large Language Models pre-trained with self-supervised learning have demonstrated impressive zero-shot generalization capabilities on a wide spectrum of tasks. In this work, we present WeLM: a well-read pre-trained language model for Chinese that is able to seamlessly perform different types of tasks with zero or few-shot demonstrations. WeLM is trained with 10B parameters by \"reading\" a curated high-quality corpus covering a wide range of topics. We show that WeLM is equipped with broad knowledge on various domains and languages. On 18 monolingual (Chinese) tasks, WeLM can significantly outperform existing pre-trained models with similar sizes and match the performance of models up to 25 times larger. WeLM also exhibits strong capabilities in multi-lingual and code-switching understanding, outperforming existing multilingual language models pre-trained on 30 languages. Furthermore, We collected human-written prompts for a large set of supervised datasets in Chinese and fine-tuned WeLM with multi-prompted training. The resulting model can attain strong generalization on unseen types of tasks and outperform the unsupervised WeLM in zero-shot learning. Finally, we demonstrate that WeLM has basic skills at explaining and calibrating the decisions from itself, which can be promising directions for future research. Our models can be applied from https://welm.weixin.qq.com/docs/api/.\n        \u25b3 Less\n      ",
    "title": "WeLM: A Well-Read Pre-trained Language Model for Chinese",
    "date": "15 May, 2023",
    "authors": [
      "Hui Su",
      " Xiao Zhou",
      " Houjin Yu",
      " Xiaoyu Shen",
      " Yuwen Chen",
      " Zilin Zhu",
      " Yang Yu",
      " Jie Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09145",
    "paper_id": "2305.09145",
    "abstract": "\n        A ReLU network is a piecewise linear function over polytopes. Figuring out the properties of such polytopes is of fundamental importance for the research and development of neural networks. So far, either theoretical or empirical studies on polytopes only stay at the level of counting their number, which is far from a complete characterization of polytopes. To upgrade the characterization to a new level, here we propose to study the shapes of polytopes via the number of simplices obtained by triangulating the polytope. Then, by computing and analyzing the histogram of simplices across polytopes, we find that a ReLU network has relatively simple polytopes under both initialization and gradient descent, although these polytopes theoretically can be rather diverse and complicated. This finding can be appreciated as a novel implicit bias. Next, we use nontrivial combinatorial derivation to theoretically explain why adding depth does not create a more complicated polytope by bounding the average number of faces of polytopes with a function of the dimensionality. Our results concretely reveal what kind of simple functions a network learns and its space partition property. Also, by characterizing the shape of polytopes, the number of simplices be a leverage for other problems, \\textit{e.g.}, serving as a generic functional complexity measure to explain the power of popular shortcut networks such as ResNet and analyzing the impact of different regularization strategies on a network's space partition.\n        \u25b3 Less\n      ",
    "title": "Deep ReLU Networks Have Surprisingly Simple Polytopes",
    "date": "15 May, 2023",
    "authors": [
      "Feng-Lei Fan",
      " Wei Huang",
      " Xiangru Zhong",
      " Lecheng Ruan",
      " Tieyong Zeng",
      " Huan Xiong",
      " Fei Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09147",
    "paper_id": "2305.09147",
    "abstract": "\n        Trajectory prediction is one of the key components of the autonomous driving software stack. Accurate prediction for the future movement of surrounding traffic participants is an important prerequisite for ensuring the driving efficiency and safety of intelligent vehicles. Trajectory prediction algorithms based on artificial intelligence have been widely studied and applied in recent years and have achieved remarkable results. However, complex artificial intelligence models are uncertain and difficult to explain, so they may face unintended failures when applied in the real world. In this paper, a self-aware trajectory prediction method is proposed. By introducing a self-awareness module and a two-stage training process, the original trajectory prediction module's performance is estimated online, to facilitate the system to deal with the possible scenario of insufficient prediction function in time, and create conditions for the realization of safe and reliable autonomous driving. Comprehensive experiments and analysis are performed, and the proposed method performed well in terms of self-awareness, memory footprint, and real-time performance, showing that it may serve as a promising paradigm for safe autonomous driving.\n        \u25b3 Less\n      ",
    "title": "Self-Aware Trajectory Prediction for Safe Autonomous Driving",
    "date": "15 May, 2023",
    "authors": [
      "Wenbo Shao",
      " Jun Li",
      " Hong Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09148",
    "paper_id": "2305.09148",
    "abstract": "\n        Recent studies have shown that dual encoder models trained with the sentence-level translation ranking task are effective methods for cross-lingual sentence embedding. However, our research indicates that token-level alignment is also crucial in multilingual scenarios, which has not been fully explored previously. Based on our findings, we propose a dual-alignment pre-training (DAP) framework for cross-lingual sentence embedding that incorporates both sentence-level and token-level alignment. To achieve this, we introduce a novel representation translation learning (RTL) task, where the model learns to use one-side contextualized token representation to reconstruct its translation counterpart. This reconstruction objective encourages the model to embed translation information into the token representation. Compared to other token-level alignment methods such as translation language modeling, RTL is more suitable for dual encoder architectures and is computationally efficient. Extensive experiments on three sentence-level cross-lingual benchmarks demonstrate that our approach can significantly improve sentence embedding. Our code is available at https://github.com/ChillingDream/DAP.\n        \u25b3 Less\n      ",
    "title": "Dual-Alignment Pre-training for Cross-lingual Sentence Embedding",
    "date": "15 May, 2023",
    "authors": [
      "Ziheng Li",
      " Shaohan Huang",
      " Zihan Zhang",
      " Zhi-Hong Deng",
      " Qiang Lou",
      " Haizhen Huang",
      " Jian Jiao",
      " Furu Wei",
      " Weiwei Deng",
      " Qi Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.04746",
    "paper_id": "2301.04746",
    "abstract": "\n        To replace data augmentation, this paper proposed a method called SLAP to intensify experience to speed up machine learning and reduce the sample size. SLAP is a model-independent protocol/function to produce the same output given different transformation variants. SLAP improved the convergence speed of convolutional neural network learning by 83% in the experiments with Gomoku game states, with only one eighth of the sample size compared with data augmentation. In reinforcement learning for Gomoku, using AlphaGo Zero/AlphaZero algorithm with data augmentation as baseline, SLAP reduced the number of training samples by a factor of 8 and achieved similar winning rate against the same evaluator, but it was not yet evident that it could speed up reinforcement learning. The benefits should at least apply to domains that are invariant to symmetry or certain transformations. As future work, SLAP may aid more explainable learning and transfer learning for domains that are not invariant to symmetry, as a small step towards artificial general intelligence.\n        \u25b3 Less\n      ",
    "title": "Switchable Lightweight Anti-symmetric Processing (SLAP) with CNN Outspeeds Data Augmentation by Smaller Sample -- Application in Gomoku Reinforcement Learning",
    "date": "15 May, 2023",
    "authors": [
      "Chi-Hang Suen",
      " Eduardo Alonso"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09161",
    "paper_id": "2305.09161",
    "abstract": "\n        Diffusion models (DM) can gradually learn to remove noise, which have been widely used in artificial intelligence generated content (AIGC) in recent years. The property of DM for removing noise leads us to wonder whether DM can be applied to wireless communications to help the receiver eliminate the channel noise. To address this, we propose channel denoising diffusion models (CDDM) for wireless communications in this paper. CDDM can be applied as a new physical layer module after the channel equalization to learn the distribution of the channel input signal, and then utilizes this learned knowledge to remove the channel noise. We design corresponding training and sampling algorithms for the forward diffusion process and the reverse sampling process of CDDM. Moreover, we apply CDDM to a semantic communications system based on joint source-channel coding (JSCC). Experimental results demonstrate that CDDM can further reduce the mean square error (MSE) after minimum mean square error (MMSE) equalizer, and the joint CDDM and JSCC system achieves better performance than the JSCC system and the traditional JPEG2000 with low-density parity-check (LDPC) code approach.\n        \u25b3 Less\n      ",
    "title": "CDDM: Channel Denoising Diffusion Models for Wireless Communications",
    "date": "15 May, 2023",
    "authors": [
      "Tong Wu",
      " Zhiyong Chen",
      " Dazhi He",
      " Liang Qian",
      " Yin Xu",
      " Meixia Tao",
      " Wenjun Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09179",
    "paper_id": "2305.09179",
    "abstract": "\n        Neural Ordinary Differential Equations (NODEs) probed the usage of numerical solvers to solve the differential equation characterized by a Neural Network (NN), therefore initiating a new paradigm of deep learning models with infinite depth. NODEs were designed to tackle the irregular time series problem. However, NODEs have demonstrated robustness against various noises and adversarial attacks. This paper is about the natural robustness of NODEs and examines the cause behind such surprising behaviour. We show that by controlling the Lipschitz constant of the ODE dynamics the robustness can be significantly improved. We derive our approach from Grownwall's inequality. Further, we draw parallels between contractivity theory and Grownwall's inequality. Experimentally we corroborate the enhanced robustness on numerous datasets - MNIST, CIFAR-10, and CIFAR 100. We also present the impact of adaptive and non-adaptive solvers on the robustness of NODEs.\n        \u25b3 Less\n      ",
    "title": "Ortho-ODE: Enhancing Robustness and of Neural ODEs against Adversarial Attacks",
    "date": "15 May, 2023",
    "authors": [
      "Vishal Purohit"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09204",
    "paper_id": "2305.09204",
    "abstract": "\n        Feature attribution aims to explain the reasoning behind a black-box model's prediction by identifying the impact of each feature on the prediction. Recent work has extended feature attribution to interactions between multiple features. However, the lack of a unified framework has led to a proliferation of methods that are often not directly comparable. This paper introduces a parameterized attribution framework -- the Weighted M\u00f6bius Score -- and (i) shows that many different attribution methods for both individual features and feature interactions are special cases and (ii) identifies some new methods. By studying the vector space of attribution methods, our framework utilizes standard linear algebra tools and provides interpretations in various fields, including cooperative game theory and causal mediation analysis. We empirically demonstrate the framework's versatility and effectiveness by applying these attribution methods to feature interactions in sentiment analysis and chain-of-thought prompting.\n        \u25b3 Less\n      ",
    "title": "The Weighted M\u00f6bius Score: A Unified Framework for Feature Attribution",
    "date": "15 May, 2023",
    "authors": [
      "Yifan Jiang",
      " Shane Steinert-Threlkeld"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09206",
    "paper_id": "2305.09206",
    "abstract": "\n        We study the problem of designing truthful and fair mechanisms when allocating a mixture of divisible and indivisible goods. We first show that there does not exist an EFM (envy-free for mixed goods) and truthful mechanism in general. This impossibility result holds even if there is only one indivisible good and one divisible good and there are only two agents. Thus, we focus on some more restricted settings. Under the setting where agents have binary valuations on indivisible goods and identical valuations on a single divisible good (e.g., money), we design an EFM and truthful mechanism. When agents have binary valuations over both divisible and indivisible goods, we first show there exist EFM and truthful mechanisms when there are only two agents or when there is a single divisible good. On the other hand, we show that the mechanism maximizing Nash welfare cannot ensure EFM and truthfulness simultaneously.\n        \u25b3 Less\n      ",
    "title": "Truthful Fair Mechanisms for Allocating Mixed Divisible and Indivisible Goods",
    "date": "15 May, 2023",
    "authors": [
      "Zihao Li",
      " Shengxin Liu",
      " Xinhang Lu",
      " Biaoshuai Tao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09207",
    "paper_id": "2305.09207",
    "abstract": "\n        Counterfactual outcome prediction in longitudinal data has recently gained attention due to its potential applications in healthcare and social sciences. In this paper, we explore the use of the state space model, a popular sequence model, for this task. Specifically, we compare the performance of two models: Treatment Effect Neural Controlled Differential Equation (TE-CDE) and structured state space model (S4Model). While TE-CDE uses controlled differential equations to address time-dependent confounding, it suffers from optimization issues and slow training. In contrast, S4Model is more efficient at modeling long-range dependencies and easier to train. We evaluate the models on a simulated lung tumor growth dataset and find that S4Model outperforms TE-CDE with 1.63x reduction in per epoch training time and 10x better normalized mean squared error. Additionally, S4Model is more stable during training and less sensitive to weight initialization than TE-CDE. Our results suggest that the state space model may be a promising approach for counterfactual outcome prediction in longitudinal data, with S4Model offering a more efficient and effective alternative to TE-CDE.\n        \u25b3 Less\n      ",
    "title": "Counterfactual Outcome Prediction using Structured State Space Model",
    "date": "15 May, 2023",
    "authors": [
      "Vishal Purohit"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.10545",
    "paper_id": "2212.10545",
    "abstract": "\n        In this paper, we propose DimonGen, which aims to generate diverse sentences describing concept relationships in various everyday scenarios. To support this, we first create a benchmark dataset for this task by adapting the existing CommonGen dataset. We then propose a two-stage model called MoREE to generate the target sentences. MoREE consists of a mixture of retrievers model that retrieves diverse context sentences related to the given concepts, and a mixture of generators model that generates diverse sentences based on the retrieved contexts. We conduct experiments on the DimonGen task and show that MoREE outperforms strong baselines in terms of both the quality and diversity of the generated sentences. Our results demonstrate that MoREE is able to generate diverse sentences that reflect different relationships between concepts, leading to a comprehensive understanding of concept relationships.\n        \u25b3 Less\n      ",
    "title": "DimonGen: Diversified Generative Commonsense Reasoning for Explaining Concept Relationships",
    "date": "16 May, 2023",
    "authors": [
      "Chenzhengyi Liu",
      " Jie Huang",
      " Kerui Zhu",
      " Kevin Chen-Chuan Chang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09696",
    "paper_id": "2305.09696",
    "abstract": "\n        Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data, including privacy protection, low resource regime, missing value imputation, and imbalanced classification. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios. Meanwhile, it can be easily combined with various backbone models, including LightGBM, Multilayer Perceptron (MLP) and Transformer. Moreover, with the aid of table pre-training, models trained using synthetic data generated by TapTap can even compete with models using the original dataset on half of the experimental datasets, marking a milestone in the development of synthetic tabular data generation. The codes are available at https://github.com/ZhangTP1996/TapTap.\n        \u25b3 Less\n      ",
    "title": "Generative Table Pre-training Empowers Models for Tabular Prediction",
    "date": "16 May, 2023",
    "authors": [
      "Tianping Zhang",
      " Shaowen Wang",
      " Shuicheng Yan",
      " Jian Li",
      " Qian Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09209",
    "paper_id": "2305.09209",
    "abstract": "\n        The advancement of Internet and Communication Technologies (ICTs) has led to the era of Industry 4.0. This shift is followed by healthcare industries creating the term Healthcare 4.0. In Healthcare 4.0, the use of IoT-enabled medical imaging devices for early disease detection has enabled medical practitioners to increase healthcare institutions' quality of service. However, Healthcare 4.0 is still lagging in Artificial Intelligence and big data compared to other Industry 4.0 due to data privacy concerns. In addition, institutions' diverse storage and computing capabilities restrict institutions from incorporating the same training model structure. This paper presents a secure multi-party computation-based ensemble federated learning with blockchain that enables heterogeneous models to collaboratively learn from healthcare institutions' data without violating users' privacy. Blockchain properties also allow the party to enjoy data integrity without trust in a centralized server while also providing each healthcare institution with auditability and version control capability.\n        \u25b3 Less\n      ",
    "title": "Trustworthy Privacy-preserving Hierarchical Ensemble and Federated Learning in Healthcare 4.0 with Blockchain",
    "date": "16 May, 2023",
    "authors": [
      "Veronika Stephanie",
      " Ibrahim Khalil",
      " Mohammed Atiquzzaman",
      " Xun Yi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09220",
    "paper_id": "2305.09220",
    "abstract": "\n        To adapt text summarization to the multilingual world, previous work proposes multi-lingual summarization (MLS) and cross-lingual summarization (CLS). However, these two tasks have been studied separately due to the different definitions, which limits the compatible and systematic research on both of them. In this paper, we aim to unify MLS and CLS into a more general setting, i.e., many-to-many summarization (M2MS), where a single model could process documents in any language and generate their summaries also in any language. As the first step towards M2MS, we conduct preliminary studies to show that M2MS can better transfer task knowledge across different languages than MLS and CLS. Furthermore, we propose Pisces, a pre-trained M2MS model that learns language modeling, cross-lingual ability and summarization ability via three-stage pre-training. Experimental results indicate that our Pisces significantly outperforms the state-of-the-art baselines, especially in the zero-shot directions, where there is no training data from the source-language documents to the target-language summaries.\n        \u25b3 Less\n      ",
    "title": "Towards Unifying Multi-Lingual and Cross-Lingual Summarization",
    "date": "16 May, 2023",
    "authors": [
      "Jiaan Wang",
      " Fandong Meng",
      " Duo Zheng",
      " Yunlong Liang",
      " Zhixu Li",
      " Jianfeng Qu",
      " Jie Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09224",
    "paper_id": "2305.09224",
    "abstract": "\n        We propose a privacy-preserving ensemble infused enhanced Deep Neural Network (DNN) based learning framework in this paper for Internet-of-Things (IoT), edge, and cloud convergence in the context of healthcare. In the convergence, edge server is used for both storing IoT produced bioimage and hosting DNN algorithm for local model training. The cloud is used for ensembling local models. The DNN-based training process of a model with a local dataset suffers from low accuracy, which can be improved by the aforementioned convergence and Ensemble Learning. The ensemble learning allows multiple participants to outsource their local model for producing a generalized final model with high accuracy. Nevertheless, Ensemble Learning elevates the risk of leaking sensitive private data from the final model. The proposed framework presents a Differential Privacy-based privacy-preserving DNN with Transfer Learning for a local model generation to ensure minimal loss and higher efficiency at edge server. We conduct several experiments to evaluate the performance of our proposed framework.\n        \u25b3 Less\n      ",
    "title": "Privacy-Preserving Ensemble Infused Enhanced Deep Neural Network Framework for Edge Cloud Convergence",
    "date": "16 May, 2023",
    "authors": [
      "Veronika Stephanie",
      " Ibrahim Khalil",
      " Mohammad Saidur Rahman",
      " Mohammed Atiquzzaman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.12669",
    "paper_id": "2212.12669",
    "abstract": "\n        The pervasive uncertainty and dynamic nature of real-world environments present significant challenges for the widespread implementation of machine-driven Intelligent Decision-Making (IDM) systems. Consequently, IDM should possess the ability to continuously acquire new skills and effectively generalize across a broad range of applications. The advancement of Artificial General Intelligence (AGI) that transcends task and application boundaries is critical for enhancing IDM. Recent studies have extensively investigated the Transformer neural architecture as a foundational model for various tasks, including computer vision, natural language processing, and reinforcement learning. We propose that a Foundation Decision Model (FDM) can be developed by formulating diverse decision-making tasks as sequence decoding tasks using the Transformer architecture, offering a promising solution for expanding IDM applications in complex real-world situations. In this paper, we discuss the efficiency and generalization improvements offered by a foundation decision model for IDM and explore its potential applications in multi-agent game AI, production scheduling, and robotics tasks. Lastly, we present a case study demonstrating our FDM implementation, DigitalBrain (DB1) with 1.3 billion parameters, achieving human-level performance in 870 tasks, such as text generation, image captioning, video game playing, robotic control, and traveling salesman problems. As a foundation decision model, DB1 represents an initial step toward more autonomous and efficient real-world IDM applications.\n        \u25b3 Less\n      ",
    "title": "On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective",
    "date": "16 May, 2023",
    "authors": [
      "Ying Wen",
      " Ziyu Wan",
      " Ming Zhou",
      " Shufang Hou",
      " Zhe Cao",
      " Chenyang Le",
      " Jingxiao Chen",
      " Zheng Tian",
      " Weinan Zhang",
      " Jun Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2203.02901",
    "paper_id": "2203.02901",
    "abstract": "\n        Chromosomes carry the genetic information of humans. They exhibit non-rigid and non-articulated nature with varying degrees of curvature. Chromosome straightening is an important step for subsequent karyotype construction, pathological diagnosis and cytogenetic map development. However, robust chromosome straightening remains challenging, due to the unavailability of training images, distorted chromosome details and shapes after straightening, as well as poor generalization capability. In this paper, we propose a novel architecture, ViT-Patch GAN, consisting of a self-learned motion transformation generator and a Vision Transformer-based patch (ViT-Patch) discriminator. The generator learns the motion representation of chromosomes for straightening. With the help of the ViT-Patch discriminator, the straightened chromosomes retain more shape and banding pattern details. The experimental results show that the proposed method achieves better performance on Fr\u00e9chet Inception Distance (FID), Learned Perceptual Image Patch Similarity (LPIPS) and downstream chromosome classification accuracy, and shows excellent generalization capability on a large dataset.\n        \u25b3 Less\n      ",
    "title": "A Robust Framework of Chromosome Straightening with ViT-Patch GAN",
    "date": "16 May, 2023",
    "authors": [
      "Sifan Song",
      " Jinfeng Wang",
      " Fengrui Cheng",
      " Qirui Cao",
      " Yihan Zuo",
      " Yongteng Lei",
      " Ruomai Yang",
      " Chunxiao Yang",
      " Frans Coenen",
      " Jia Meng",
      " Kang Dang",
      " Jionglong Su"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.00020",
    "paper_id": "2304.00020",
    "abstract": "\n        The prevalence of memes on social media has created the need to sentiment analyze their underlying meanings for censoring harmful content. Meme censoring systems by machine learning raise the need for a semi-supervised learning solution to take advantage of the large number of unlabeled memes available on the internet and make the annotation process less challenging. Moreover, the approach needs to utilize multimodal data as memes' meanings usually come from both images and texts. This research proposes a multimodal semi-supervised learning approach that outperforms other multimodal semi-supervised learning and supervised learning state-of-the-art models on two datasets, the Multimedia Automatic Misogyny Identification and Hateful Memes dataset. Building on the insights gained from Contrastive Language-Image Pre-training, which is an effective multimodal learning technique, this research introduces SemiMemes, a novel training method that combines auto-encoder and classification task to make use of the resourceful unlabeled data.\n        \u25b3 Less\n      ",
    "title": "SemiMemes: A Semi-supervised Learning Approach for Multimodal Memes Analysis",
    "date": "16 May, 2023",
    "authors": [
      "Pham Thai Hoang Tung",
      " Nguyen Tan Viet",
      " Ngo Tien Anh",
      " Phan Duy Hung"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09246",
    "paper_id": "2305.09246",
    "abstract": "\n        Instruction tuning for large language models (LLMs) has gained attention from researchers due to its ability to unlock the potential of LLMs in following instructions. While instruction tuning offers advantages for facilitating the adaptation of large language models (LLMs) to downstream tasks as a fine-tuning approach, training models with tens of millions or even billions of parameters on large amounts of data results in unaffordable computational costs. To address this, we focus on reducing the data used in LLM instruction tuning to decrease training costs and improve data efficiency, dubbed as Low Training Data Instruction Tuning (LTD Instruction Tuning). Specifically, this paper conducts a preliminary exploration into reducing the data used in LLM training and identifies several observations regarding task specialization for LLM training, such as the optimization of performance for a specific task, the number of instruction types required for instruction tuning, and the amount of data required for task-specific models. The results suggest that task-specific models can be trained using less than 0.5% of the original dataset, with a 2% improvement in performance over those trained on full task-related data.\n        \u25b3 Less\n      ",
    "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning",
    "date": "16 May, 2023",
    "authors": [
      "Hao Chen",
      " Yiming Zhang",
      " Qi Zhang",
      " Hantao Yang",
      " Xiaomeng Hu",
      " Xuetao Ma",
      " Yifan Yanggong",
      " Junbo Zhao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09247",
    "paper_id": "2305.09247",
    "abstract": "\n        The problem of model counting, also known as #SAT, is to compute the number of models or satisfying assignments of a given Boolean formula FF. Model counting is a fundamental problem in computer science with a wide range of applications. In recent years, there has been a growing interest in using hashing-based techniques for approximate model counting that provide (\u03b5,\u03b4)(\\varepsilon, \u03b4)-guarantees: i.e., the count returned is within a (1+\u03b5)(1+\\varepsilon)-factor of the exact count with confidence at least 1\u2212\u03b41-\u03b4. While hashing-based techniques attain reasonable scalability for large enough values of \u03b4\u03b4, their scalability is severely impacted for smaller values of \u03b4\u03b4, thereby preventing their adoption in application domains that require estimates with high confidence.\n  The primary contribution of this paper is to address the Achilles heel of hashing-based techniques: we propose a novel approach based on rounding that allows us to achieve a significant reduction in runtime for smaller values of \u03b4\u03b4. The resulting counter, called RoundMC, achieves a substantial runtime performance improvement over the current state-of-the-art counter, ApproxMC. In particular, our extensive evaluation over a benchmark suite consisting of 1890 instances shows that RoundMC solves 204 more instances than ApproxMC, and achieves a 4\u00d74\\times speedup over ApproxMC.\n        \u25b3 Less\n      ",
    "title": "Rounding Meets Approximate Model Counting",
    "date": "16 May, 2023",
    "authors": [
      "Jiong Yang",
      " Kuldeep S. Meel"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09257",
    "paper_id": "2305.09257",
    "abstract": "\n        This paper presents a new genetic algorithm encoding representation to solve the travelling salesman problem. To assess the performance of the proposed chromosome structure, we compare it with state-of-the-art encoding representations. For that purpose, we use 14 benchmarks of different sizes taken from TSPLIB. Finally, after conducting the experimental study, we report the obtained results and draw our conclusion.\n        \u25b3 Less\n      ",
    "title": "A new node-shift encoding representation for the travelling salesman problem",
    "date": "16 May, 2023",
    "authors": [
      "Menouar Boulif",
      " Aghiles Gharbi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11901",
    "paper_id": "2305.11901",
    "abstract": "\n        Stagnant weather condition is one of the major contributors to air pollution as it is favorable for the formation and accumulation of pollutants. To measure the atmosphere's ability to dilute air pollutants, Air Stagnation Index (ASI) has been introduced as an important meteorological index. Therefore, making long-lead ASI forecasts is vital to make plans in advance for air quality management. In this study, we found that autumn Ni\u00f1o indices derived from sea surface temperature (SST) anomalies show a negative correlation with wintertime ASI in southern China, offering prospects for a prewinter forecast. We developed an LSTM-based model to predict the future wintertime ASI. Results demonstrated that multivariate inputs (past ASI and Ni\u00f1o indices) achieve better forecast performance than univariate input (only past ASI). The model achieves a correlation coefficient of 0.778 between the actual and predicted ASI, exhibiting a high degree of consistency.\n        \u25b3 Less\n      ",
    "title": "Long-lead forecasts of wintertime air stagnation index in southern China using oceanic memory effects",
    "date": "16 May, 2023",
    "authors": [
      "Chenhong Zhou",
      " Xiaorui Zhang",
      " Meng Gao",
      " Shanshan Liu",
      " Yike Guo",
      " Jie Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.05948",
    "paper_id": "2301.05948",
    "abstract": "\n        The HuggingFace Datasets Hub hosts thousands of datasets, offering exciting opportunities for language model training and evaluation. However, datasets for a specific task type often have different schemas, making harmonization challenging. Multi-task training or evaluation necessitates manual work to fit data into task templates. Several initiatives independently tackle this issue by releasing harmonized datasets or providing harmonization codes to preprocess datasets into a consistent format. We identify patterns across previous preprocessing efforts, such as column name mapping and extracting specific sub-fields from structured data in a column. We then propose a structured annotation framework that ensures our annotations are fully exposed and not hidden within unstructured code. We release a dataset annotation framework and dataset annotations for more than 500 English tasks\\footnote{\\url{https://github.com/sileod/tasksource}}. These annotations include metadata, such as the names of columns to be used as input or labels for all datasets, which can save time for future dataset preprocessing, regardless of whether our framework is utilized. We fine-tune a multi-task text encoder on all tasksource tasks, outperforming every publicly available text encoder of comparable size in an external evaluation.\n        \u25b3 Less\n      ",
    "title": "tasksource: A Dataset Harmonization Framework for Streamlined NLP Multi-Task Learning and Evaluation",
    "date": "16 May, 2023",
    "authors": [
      "Damien Sileo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09275",
    "paper_id": "2305.09275",
    "abstract": "\n        We revisit the common practice of evaluating adaptation of Online Continual Learning (OCL) algorithms through the metric of online accuracy, which measures the accuracy of the model on the immediate next few samples. However, we show that this metric is unreliable, as even vacuous blind classifiers, which do not use input images for prediction, can achieve unrealistically high online accuracy by exploiting spurious label correlations in the data stream. Our study reveals that existing OCL algorithms can also achieve high online accuracy, but perform poorly in retaining useful information, suggesting that they unintentionally learn spurious label correlations. To address this issue, we propose a novel metric for measuring adaptation based on the accuracy on the near-future samples, where spurious correlations are removed. We benchmark existing OCL approaches using our proposed metric on large-scale datasets under various computational budgets and find that better generalization can be achieved by retaining and reusing past seen information. We believe that our proposed metric can aid in the development of truly adaptive OCL methods. We provide code to reproduce our results at https://github.com/drimpossible/EvalOCL.\n        \u25b3 Less\n      ",
    "title": "Rapid Adaptation in Online Continual Learning: Are We Evaluating It Right?",
    "date": "16 May, 2023",
    "authors": [
      "Hasan Abed Al Kader Hammoud",
      " Ameya Prabhu",
      " Ser-Nam Lim",
      " Philip H. S. Torr",
      " Adel Bibi",
      " Bernard Ghanem"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09276",
    "paper_id": "2305.09276",
    "abstract": "\n        In which we propose neural network architecture (dune neural network) for recognizing general noisy image without adding any artificial noise in the training data. By representing each free parameter of the network as an uncertainty interval, and applying a linear transformation to each input element, we show that the resulting architecture achieves decent noise robustness when faced with input data with white noise. We apply simple dune neural networks for MNIST dataset and demonstrate that even for very noisy input images which are hard for human to recognize, our approach achieved better test set accuracy than human without dataset augmentation. We also find that our method is robust for many other examples with various background patterns added.\n        \u25b3 Less\n      ",
    "title": "Noise robust neural network architecture",
    "date": "16 May, 2023",
    "authors": [
      "Xiong Yunuo",
      " Xiong Hongwei"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08747",
    "paper_id": "2305.08747",
    "abstract": "\n        Users are often overwhelmed by privacy decisions to manage their personal data, which can happen on the web, in mobile, and in IoT environments. These decisions can take various forms -- such as decisions for setting privacy permissions or privacy preferences, decisions responding to consent requests, or to intervene and ``reject'' processing of one's personal data --, and each can have different legal impacts. In all cases and for all types of decisions, scholars and industry have been proposing tools to better automate the process of privacy decisions at different levels, in order to enhance usability. We provide in this paper an overview of the main challenges raised by the automation of privacy decisions, together with a classification scheme of the existing and envisioned work and proposals addressing automation of privacy decisions.\n        \u25b3 Less\n      ",
    "title": "Automating privacy decisions -- where to draw the line?",
    "date": "16 May, 2023",
    "authors": [
      "Victor Morel",
      " Simone Fischer-H\u00fcbner"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09302",
    "paper_id": "2305.09302",
    "abstract": "\n        We introduce a novel dataset consisting of images depicting pink eggs that have been identified as Pomacea canaliculata eggs, accompanied by corresponding bounding box annotations. The purpose of this dataset is to aid researchers in the analysis of the spread of Pomacea canaliculata species by utilizing deep learning techniques, as well as supporting other investigative pursuits that require visual data pertaining to the eggs of Pomacea canaliculata. It is worth noting, however, that the identity of the eggs in question is not definitively established, as other species within the same taxonomic family have been observed to lay similar-looking eggs in regions of the Americas. Therefore, a crucial prerequisite to any decision regarding the elimination of these eggs would be to establish with certainty whether they are exclusively attributable to invasive Pomacea canaliculata or if other species are also involved. The dataset is available at https://www.kaggle.com/datasets/deeshenzhen/pinkeggs\n        \u25b3 Less\n      ",
    "title": "Pink-Eggs Dataset V1: A Step Toward Invasive Species Management Using Deep Learning Embedded Solutions",
    "date": "16 May, 2023",
    "authors": [
      "Di Xu",
      " Yang Zhao",
      " Xiang Hao",
      " Xin Meng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09304",
    "paper_id": "2305.09304",
    "abstract": "\n        AI systems empowered by reinforcement learning (RL) algorithms harbor the immense potential to catalyze societal advancement, yet their deployment is often impeded by significant safety concerns. Particularly in safety-critical applications, researchers have raised concerns about unintended harms or unsafe behaviors of unaligned RL agents. The philosophy of safe reinforcement learning (SafeRL) is to align RL agents with harmless intentions and safe behavioral patterns. In SafeRL, agents learn to develop optimal policies by receiving feedback from the environment, while also fulfilling the requirement of minimizing the risk of unintended harm or unsafe behavior. However, due to the intricate nature of SafeRL algorithm implementation, combining methodologies across various domains presents a formidable challenge. This had led to an absence of a cohesive and efficacious learning framework within the contemporary SafeRL research milieu. In this work, we introduce a foundational framework designed to expedite SafeRL research endeavors. Our comprehensive framework encompasses an array of algorithms spanning different RL domains and places heavy emphasis on safety elements. Our efforts are to make the SafeRL-related research process more streamlined and efficient, therefore facilitating further research in AI safety. Our project is released at: https://github.com/PKU-Alignment/omnisafe.\n        \u25b3 Less\n      ",
    "title": "OmniSafe: An Infrastructure for Accelerating Safe Reinforcement Learning Research",
    "date": "16 May, 2023",
    "authors": [
      "Jiaming Ji",
      " Jiayi Zhou",
      " Borong Zhang",
      " Juntao Dai",
      " Xuehai Pan",
      " Ruiyang Sun",
      " Weidong Huang",
      " Yiran Geng",
      " Mickel Liu",
      " Yaodong Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09313",
    "paper_id": "2305.09313",
    "abstract": "\n        In passage retrieval system, the initial passage retrieval results may be unsatisfactory, which can be refined by a reranking scheme. Existing solutions to passage reranking focus on enriching the interaction between query and each passage separately, neglecting the context among the top-ranked passages in the initial retrieval list. To tackle this problem, we propose a Hybrid and Collaborative Passage Reranking (HybRank) method, which leverages the substantial similarity measurements of upstream retrievers for passage collaboration and incorporates the lexical and semantic properties of sparse and dense retrievers for reranking. Besides, built on off-the-shelf retriever features, HybRank is a plug-in reranker capable of enhancing arbitrary passage lists including previously reranked ones. Extensive experiments demonstrate the stable improvements of performance over prevalent retrieval and reranking methods, and verify the effectiveness of the core components of HybRank.\n        \u25b3 Less\n      ",
    "title": "Hybrid and Collaborative Passage Reranking",
    "date": "16 May, 2023",
    "authors": [
      "Zongmeng Zhang",
      " Wengang Zhou",
      " Jiaxin Shi",
      " Houqiang Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09316",
    "paper_id": "2305.09316",
    "abstract": "\n        In this study, we investigate using graph neural network (GNN) representations to enhance contextualized representations of pre-trained language models (PLMs) for keyphrase extraction from lengthy documents. We show that augmenting a PLM with graph embeddings provides a more comprehensive semantic understanding of words in a document, particularly for long documents. We construct a co-occurrence graph of the text and embed it using a graph convolutional network (GCN) trained on the task of edge prediction. We propose a graph-enhanced sequence tagging architecture that augments contextualized PLM embeddings with graph representations. Evaluating on benchmark datasets, we demonstrate that enhancing PLMs with graph embeddings outperforms state-of-the-art models on long documents, showing significant improvements in F1 scores across all the datasets. Our study highlights the potential of GNN representations as a complementary approach to improve PLM performance for keyphrase extraction from long documents.\n        \u25b3 Less\n      ",
    "title": "Enhancing Keyphrase Extraction from Long Scientific Documents using Graph Embeddings",
    "date": "16 May, 2023",
    "authors": [
      "Roberto Mart\u00ednez-Cruz",
      " Debanjan Mahata",
      " Alvaro J. L\u00f3pez-L\u00f3pez",
      " Jos\u00e9 Portela"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09319",
    "paper_id": "2305.09319",
    "abstract": "\n        Among the seven key requirements to achieve trustworthy AI proposed by the High-Level Expert Group on Artificial Intelligence (AI-HLEG) established by the European Commission (EC), the fifth requirement (\"Diversity, non-discrimination and fairness\") declares: \"In order to achieve Trustworthy AI, we must enable inclusion and diversity throughout the entire AI system's life cycle. [...] This requirement is closely linked with the principle of fairness\". In this paper, we try to shed light on how closely these two distinct concepts, diversity and fairness, may be treated by focusing on information access systems and ranking literature. These concepts should not be used interchangeably because they do represent two different values, but what we argue is that they also cannot be considered totally unrelated or divergent. Having diversity does not imply fairness, but fostering diversity can effectively lead to fair outcomes, an intuition behind several methods proposed to mitigate the disparate impact of information access systems, i.e. recommender systems and search engines.\n        \u25b3 Less\n      ",
    "title": "Fairness and Diversity in Information Access Systems",
    "date": "16 May, 2023",
    "authors": [
      "Lorenzo Porcaro",
      " Carlos Castillo",
      " Emilia G\u00f3mez",
      " Jo\u00e3o Vinagre"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.12668",
    "paper_id": "2302.12668",
    "abstract": "\n        Quality-Diversity (QD) algorithms have recently gained traction as optimisation methods due to their effectiveness at escaping local optima and capability of generating wide-ranging and high-performing solutions. Recently, Multi-Objective MAP-Elites (MOME) extended the QD paradigm to the multi-objective setting by maintaining a Pareto front in each cell of a map-elites grid. MOME achieved a global performance that competed with NSGA-II and SPEA2, two well-established Multi-Objective Evolutionary Algorithms (MOEA), while also acquiring a diverse repertoire of solutions. However, MOME is limited by non-directed genetic search mechanisms which struggle in high-dimensional search spaces. In this work, we present Multi-Objective MAP-Elites with Policy-Gradient Assistance and Crowding-based Exploration (MOME-PGX): a new QD algorithm that extends MOME to improve its data efficiency and performance. MOME-PGX uses gradient-based optimisation to efficiently drive solutions towards higher performance. It also introduces crowding-based mechanisms to create an improved exploration strategy and to encourage uniformity across Pareto fronts. We evaluate MOME-PGX in four simulated robot locomotion tasks and demonstrate that it converges faster and to a higher performance than all other baselines. We show that MOME-PGX is between 4.3 and 42 times more data-efficient than MOME and doubles the performance of MOME, NSGA-II and SPEA2 in challenging environments.\n        \u25b3 Less\n      ",
    "title": "Improving the Data Efficiency of Multi-Objective Quality-Diversity through Gradient Assistance and Crowding Exploration",
    "date": "16 May, 2023",
    "authors": [
      "Hannah Janmohamed",
      " Thomas Pierrot",
      " Antoine Cully"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09330",
    "paper_id": "2305.09330",
    "abstract": "\n        In the current landscape of ever-increasing levels of digitalization, we are facing major challenges pertaining to scalability. Recommender systems have become irreplaceable both for helping users navigate the increasing amounts of data and, conversely, aiding providers in marketing products to interested users. The growing awareness of discrimination in machine learning methods has recently motivated both academia and industry to research how fairness can be ensured in recommender systems. For recommender systems, such issues are well exemplified by occupation recommendation, where biases in historical data may lead to recommender systems relating one gender to lower wages or to the propagation of stereotypes. In particular, consumer-side fairness, which focuses on mitigating discrimination experienced by users of recommender systems, has seen a vast number of diverse approaches for addressing different types of discrimination. The nature of said discrimination depends on the setting and the applied fairness interpretation, of which there are many variations. This survey serves as a systematic overview and discussion of the current research on consumer-side fairness in recommender systems. To that end, a novel taxonomy based on high-level fairness interpretation is proposed and used to categorize the research and their proposed fairness evaluation metrics. Finally, we highlight some suggestions for the future direction of the field.\n        \u25b3 Less\n      ",
    "title": "Consumer-side Fairness in Recommender Systems: A Systematic Survey of Methods and Evaluation",
    "date": "16 May, 2023",
    "authors": [
      "Bj\u00f8rnar Vass\u00f8y",
      " Helge Langseth"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09333",
    "paper_id": "2305.09333",
    "abstract": "\n        Multi-modal visual understanding of images with prompts involves using various visual and textual cues to enhance the semantic understanding of images. This approach combines both vision and language processing to generate more accurate predictions and recognition of images. By utilizing prompt-based techniques, models can learn to focus on certain features of an image to extract useful information for downstream tasks. Additionally, multi-modal understanding can improve upon single modality models by providing more robust representations of images. Overall, the combination of visual and textual information is a promising area of research for advancing image recognition and understanding. In this paper we will try an amount of prompt design methods and propose a new method for better extraction of semantic information\n        \u25b3 Less\n      ",
    "title": "Multi-modal Visual Understanding with Prompts for Semantic Information Disentanglement of Image",
    "date": "16 May, 2023",
    "authors": [
      "Yuzhou Peng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.01746",
    "paper_id": "2306.01746",
    "abstract": "\n        Maji et al. introduced in 2002 a method of parametric decision making using soft sets as tools and representing their tabular form as a binary matrix. In cases, however, where some or all of the parameters used for the characterization of the elements of the universal set are of fuzzy texture, their method does not give always the best decision making solution. In order to tackle this problem, we modified in earlier works the method of Maji et al. by replacing the binary elements in the tabular form of the corresponding soft set either by grey numbers or by triangular fuzzy numbers. In this work, in order to tackle more efficiently cases in which the decision maker has doubts about the correctness of the fuzzy/qualitative characterizations assigned to some or all of the elements of the universal set, we replace the binary elements of the tabular form by neutrosophic triplets. Our new, neutrosophic decision making method is illustrated by an application concerning the choice of a new player by a soccer club.\n        \u25b3 Less\n      ",
    "title": "An Application of Neutrosophic Sets to Decision Making",
    "date": "16 May, 2023",
    "authors": [
      "Michael Gr. Voskoglou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09348",
    "paper_id": "2305.09348",
    "abstract": "\n        Neural networks (NNs) are capable of learning complex patterns and relationships in data to make predictions with high accuracy, making them useful for various tasks. However, NNs are both computation-intensive and memory-intensive methods, making them challenging for edge applications. To accelerate the most common operations (matrix-vector multiplication) in NNs, hardware accelerator architectures such as computation-in-memory (CiM) with non-volatile memristive crossbars are utilized. Although they offer benefits such as power efficiency, parallelism, and nonvolatility, they suffer from various faults and variations, both during manufacturing and lifetime operations. This can lead to faulty computations and, in turn, degradation of post-mapping inference accuracy, which is unacceptable for many applications, including safety-critical applications. Therefore, proper testing of NN hardware accelerators is required. In this paper, we propose a \\emph{one-shot} testing approach that can test NNs accelerated on memristive crossbars with only one test vector, making it very suitable for online testing applications. Our approach can consistently achieve 100%100\\% fault coverage across several large topologies with up to 201201 layers and challenging tasks like semantic segmentation. Nevertheless, compared to existing methods, the fault coverage is improved by up to 24%24\\%, the memory overhead is only 0.01230.0123 MB, a reduction of up to 19980\u00d719980\\times and the number of test vectors is reduced by 10000\u00d710000\\times.\n        \u25b3 Less\n      ",
    "title": "One-Shot Online Testing of Deep Neural Networks Based on Distribution Shift Detection",
    "date": "16 May, 2023",
    "authors": [
      "Soyed Tuhin Ahmed",
      " Mehdi B. Tahoori"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09349",
    "paper_id": "2305.09349",
    "abstract": "\n        We propose a method that allows to develop shared understanding between two agents for the purpose of performing a task that requires cooperation. Our method focuses on efficiently establishing successful task-oriented communication in an open multi-agent system, where the agents do not know anything about each other and can only communicate via grounded interaction. The method aims to assist researchers that work on human-machine interaction or scenarios that require a human-in-the-loop, by defining interaction restrictions and efficiency metrics. To that end, we point out the challenges and limitations of such a (diverse) setup, while also restrictions and requirements which aim to ensure that high task performance truthfully reflects the extent to which the agents correctly understand each other. Furthermore, we demonstrate a use-case where our method can be applied for the task of cooperative query answering. We design the experiments by modifying an established ontology alignment benchmark. In this example, the agents want to query each other, while representing different databases, defined in their own ontologies that contain different and incomplete knowledge. Grounded interaction here has the form of examples that consists of common instances, for which the agents are expected to have similar knowledge. Our experiments demonstrate successful communication establishment under the required restrictions, and compare different agent policies that aim to solve the task in an efficient manner.\n        \u25b3 Less\n      ",
    "title": "Establishing Shared Query Understanding in an Open Multi-Agent System",
    "date": "16 May, 2023",
    "authors": [
      "Nikolaos Kondylidis",
      " Ilaria Tiddi",
      " Annette ten Teije"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.08635",
    "paper_id": "2212.08635",
    "abstract": "\n        Open-Domain Question Answering (ODQA) aims at answering factoid questions without explicitly providing specific background documents. In a zero-shot setting, this task is more challenging since no data is available to train customized models like Retriever-Readers. Recently, Large Language Models (LLMs) like GPT-3 have shown their power in zero-shot ODQA with direct prompting methods, but these methods are still far from releasing the full powerfulness of LLMs only in an implicitly invoking way. In this paper, we propose a Self-Prompting framework to explicitly utilize the massive knowledge stored in the parameters of LLMs and their strong instruction understanding abilities. Concretely, we prompt LLMs step by step to generate multiple pseudo QA pairs with background passages and explanations from scratch and then use those generated elements for in-context learning. Experimental results show our method surpasses previous SOTA methods significantly on three widely-used ODQA datasets, and even achieves comparable performance with some Retriever-Reader models fine-tuned on full training data.\n        \u25b3 Less\n      ",
    "title": "Self-Prompting Large Language Models for Zero-Shot Open-Domain QA",
    "date": "16 May, 2023",
    "authors": [
      "Junlong Li",
      " Zhuosheng Zhang",
      " Hai Zhao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09703",
    "paper_id": "2305.09703",
    "abstract": "\n        Graph neural networks (GNNs), especially dynamic GNNs, have become a research hotspot in spatio-temporal forecasting problems. While many dynamic graph construction methods have been developed, relatively few of them explore the causal relationship between neighbour nodes. Thus, the resulting models lack strong explainability for the causal relationship between the neighbour nodes of the dynamically generated graphs, which can easily lead to a risk in subsequent decisions. Moreover, few of them consider the uncertainty and noise of dynamic graphs based on the time series datasets, which are ubiquitous in real-world graph structure networks. In this paper, we propose a novel Dynamic Diffusion-Variational Graph Neural Network (DVGNN) for spatio-temporal forecasting. For dynamic graph construction, an unsupervised generative model is devised. Two layers of graph convolutional network (GCN) are applied to calculate the posterior distribution of the latent node embeddings in the encoder stage. Then, a diffusion model is used to infer the dynamic link probability and reconstruct causal graphs in the decoder stage adaptively. The new loss function is derived theoretically, and the reparameterization trick is adopted in estimating the probability distribution of the dynamic graphs by Evidence Lower Bound during the backpropagation period. After obtaining the generated graphs, dynamic GCN and temporal attention are applied to predict future states. Experiments are conducted on four real-world datasets of different graph structures in different domains. The results demonstrate that the proposed DVGNN model outperforms state-of-the-art approaches and achieves outstanding Root Mean Squared Error result while exhibiting higher robustness. Also, by F1-score and probability distribution analysis, we demonstrate that DVGNN better reflects the causal relationship and uncertainty of dynamic graphs.\n        \u25b3 Less\n      ",
    "title": "Dynamic Causal Explanation Based Diffusion-Variational Graph Neural Network for Spatio-temporal Forecasting",
    "date": "16 May, 2023",
    "authors": [
      "Guojun Liang",
      " Prayag Tiwari",
      " S\u0142awomir Nowaczyk",
      " Stefan Byttner",
      " Fernando Alonso-Fernandez"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09401",
    "paper_id": "2305.09401",
    "abstract": "\n        We propose a method that augments a simulated dataset using diffusion models to improve the performance of pedestrian detection in real-world data. The high cost of collecting and annotating data in the real-world has motivated the use of simulation platforms to create training datasets. While simulated data is inexpensive to collect and annotate, it unfortunately does not always closely match the distribution of real-world data, which is known as the sim2real gap. In this paper we propose a novel method of synthetic data creation meant to close the sim2real gap for the challenging pedestrian detection task. Our method uses a diffusion-based architecture to learn a real-world distribution which, once trained, is used to generate datasets. We mix this generated data with simulated data as a form of augmentation and show that training on a combination of generated and simulated data increases average precision by as much as 27.3% for pedestrian detection models in real-world data, compared against training on purely simulated data.\n        \u25b3 Less\n      ",
    "title": "Diffusion Dataset Generation: Towards Closing the Sim2Real Gap for Pedestrian Detection",
    "date": "16 May, 2023",
    "authors": [
      "Andrew Farley",
      " Mohsen Zand",
      " Michael Greenspan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07039",
    "paper_id": "2305.07039",
    "abstract": "\n        In this paper, we address the challenges faced by Value Iteration Networks (VIN) in handling larger input maps and mitigating the impact of accumulated errors caused by increased iterations. We propose a novel approach, Value Iteration Networks with Gated Summarization Module (GS-VIN), which incorporates two main improvements: (1) employing an Adaptive Iteration Strategy in the Value Iteration module to reduce the number of iterations, and (2) introducing a Gated Summarization module to summarize the iterative process. The adaptive iteration strategy uses larger convolution kernels with fewer iteration times, reducing network depth and increasing training stability while maintaining the accuracy of the planning process. The gated summarization module enables the network to emphasize the entire planning process, rather than solely relying on the final global planning outcome, by temporally and spatially resampling the entire planning process within the VI module. We conduct experiments on 2D grid world path-finding problems and the Atari Mr. Pac-man environment, demonstrating that GS-VIN outperforms the baseline in terms of single-step accuracy, planning success rate, and overall performance across different map sizes. Additionally, we provide an analysis of the relationship between input size, kernel size, and the number of iterations in VI-based models, which is applicable to a majority of VI-based models and offers valuable insights for researchers and industrial deployment.\n        \u25b3 Less\n      ",
    "title": "Value Iteration Networks with Gated Summarization Module",
    "date": "16 May, 2023",
    "authors": [
      "Jinyu Cai",
      " Jialong Li",
      " Mingyue Zhang",
      " Kenji Tei"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09402",
    "paper_id": "2305.09402",
    "abstract": "\n        This paper evaluates the capability of two state-of-the-art artificial intelligence (AI) models, GPT-3.5 and Bard, in generating Java code given a function description. We sourced the descriptions from CodingBat.com, a popular online platform that provides practice problems to learn programming. We compared the Java code generated by both models based on correctness, verified through the platform's own test cases. The results indicate clear differences in the capabilities of the two models. GPT-3.5 demonstrated superior performance, generating correct code for approximately 90.6% of the function descriptions, whereas Bard produced correct code for 53.1% of the functions. While both models exhibited strengths and weaknesses, these findings suggest potential avenues for the development and refinement of more advanced AI-assisted code generation tools. The study underlines the potential of AI in automating and supporting aspects of software development, although further research is required to fully realize this potential.\n        \u25b3 Less\n      ",
    "title": "A Preliminary Analysis on the Code Generation Capabilities of GPT-3.5 and Bard AI Models for Java Functions",
    "date": "16 May, 2023",
    "authors": [
      "Giuseppe Destefanis",
      " Silvia Bartolucci",
      " Marco Ortu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09410",
    "paper_id": "2305.09410",
    "abstract": "\n        This document contains a discussion of the F1 score evaluation used in the article 'Relation Classification with Entity Type Restriction' by Shengfei Lyu, Huanhuan Chen published on Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. The authors created a system named RECENT and claim it achieves (then) a new state-of-the-art result 75.2 (previous 74.8) on the TACRED dataset, while after correcting errors and reevaluation the final result is 65.16\n        \u25b3 Less\n      ",
    "title": "About Evaluation of F1 Score for RECENT Relation Extraction System",
    "date": "16 May, 2023",
    "authors": [
      "Micha\u0142 Olek"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.12816",
    "paper_id": "2209.12816",
    "abstract": "\n        Transformer-based language models utilize the attention mechanism for substantial performance improvements in almost all natural language processing (NLP) tasks. Similar attention structures are also extensively studied in several other areas. Although the attention mechanism enhances the model performances significantly, its quadratic complexity prevents efficient processing of long sequences. Recent works focused on eliminating the disadvantages of computational inefficiency and showed that transformer-based models can still reach competitive results without the attention layer. A pioneering study proposed the FNet, which replaces the attention layer with the Fourier Transform (FT) in the transformer encoder architecture. FNet achieves competitive performances concerning the original transformer encoder model while accelerating training process by removing the computational burden of the attention mechanism. However, the FNet model ignores essential properties of the FT from the classical signal processing that can be leveraged to increase model efficiency further. We propose different methods to deploy FT efficiently in transformer encoder models. Our proposed architectures have smaller number of model parameters, shorter training times, less memory usage, and some additional performance improvements. We demonstrate these improvements through extensive experiments on common benchmarks.\n        \u25b3 Less\n      ",
    "title": "Fast-FNet: Accelerating Transformer Encoder Models via Efficient Fourier Layers",
    "date": "16 May, 2023",
    "authors": [
      "Nurullah Sevim",
      " Ege Ozan \u00d6zyedek",
      " Furkan \u015eahinu\u00e7",
      " Aykut Ko\u00e7"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2208.01375",
    "paper_id": "2208.01375",
    "abstract": "\n        Recommending points of interest (POIs) is a challenging task that requires extracting comprehensive location data from location-based social media platforms. To provide effective location-based recommendations, it's important to analyze users' historical behavior and preferences. In this study, we present a sophisticated location-aware recommendation system that uses Bidirectional Encoder Representations from Transformers (BERT) to offer personalized location-based suggestions. Our model combines location information and user preferences to provide more relevant recommendations compared to models that predict the next POI in a sequence. Our experiments on two benchmark dataset show that our BERT-based model outperforms various state-of-the-art sequential models. Moreover, we see the effectiveness of the proposed model for quality through additional experiments.\n        \u25b3 Less\n      ",
    "title": "BERT4Loc: BERT for Location -- POI Recommender System",
    "date": "16 May, 2023",
    "authors": [
      "Syed Raza Bashir",
      " Shaina Raza",
      " Vojislav Misic"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.09667",
    "paper_id": "2304.09667",
    "abstract": "\n        While large language models (LLMs) have been successfully applied to various tasks, they still face challenges with hallucinations. Augmenting LLMs with domain-specific tools such as database utilities can facilitate easier and more precise access to specialized knowledge. In this paper, we present GeneGPT, a novel method for teaching LLMs to use the Web APIs of the National Center for Biotechnology Information (NCBI) for answering genomics questions. Specifically, we prompt Codex to solve the GeneTuring tests with NCBI Web APIs by in-context learning and an augmented decoding algorithm that can detect and execute API calls. Experimental results show that GeneGPT achieves state-of-the-art performance on eight tasks in the GeneTuring benchmark with an average score of 0.83, largely surpassing retrieval-augmented LLMs such as the new Bing (0.44), biomedical LLMs such as BioMedLM (0.08) and BioGPT (0.04), as well as GPT-3 (0.16) and ChatGPT (0.12). Our further analyses suggest that: (1) API demonstrations have good cross-task generalizability and are more useful than documentations for in-context learning; (2) GeneGPT can generalize to longer chains of API calls and answer multi-hop questions in GeneHop, a novel dataset introduced in this work; (3) Different types of errors are enriched in different tasks, providing valuable insights for future improvements.\n        \u25b3 Less\n      ",
    "title": "GeneGPT: Augmenting Large Language Models with Domain Tools for Improved Access to Biomedical Information",
    "date": "16 May, 2023",
    "authors": [
      "Qiao Jin",
      " Yifan Yang",
      " Qingyu Chen",
      " Zhiyong Lu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09463",
    "paper_id": "2305.09463",
    "abstract": "\n        In this technical report, a low-complexity deep learning system for acoustic scene classification (ASC) is presented. The proposed system comprises two main phases: (Phase I) Training a teacher network; and (Phase II) training a student network using distilled knowledge from the teacher. In the first phase, the teacher, which presents a large footprint model, is trained. After training the teacher, the embeddings, which are the feature map of the second last layer of the teacher, are extracted. In the second phase, the student network, which presents a low complexity model, is trained with the embeddings extracted from the teacher. Our experiments conducted on DCASE 2023 Task 1 Development dataset have fulfilled the requirement of low-complexity and achieved the best classification accuracy of 57.4%, improving DCASE baseline by 14.5%.\n        \u25b3 Less\n      ",
    "title": "Low-complexity deep learning frameworks for acoustic scene classification using teacher-student scheme and multiple spectrograms",
    "date": "16 May, 2023",
    "authors": [
      "Lam Pham",
      " Dat Ngo",
      " Cam Le",
      " Anahid Jalali",
      " Alexander Schindler"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09464",
    "paper_id": "2305.09464",
    "abstract": "\n        Applications of large open-domain knowledge graphs (KGs) to real-world problems pose many unique challenges. In this paper, we present extensions to Saga our platform for continuous construction and serving of knowledge at scale. In particular, we describe a pipeline for training knowledge graph embeddings that powers key capabilities such as fact ranking, fact verification, a related entities service, and support for entity linking. We then describe how our platform, including graph embeddings, can be leveraged to create a Semantic Annotation service that links unstructured Web documents to entities in our KG. Semantic annotation of the Web effectively expands our knowledge graph with edges to open-domain Web content which can be used in various search and ranking problems. Finally, we leverage annotated Web documents to drive Open-domain Knowledge Extraction. This targeted extraction framework identifies important coverage issues in the KG, then finds relevant data sources for target entities on the Web and extracts missing information to enrich the KG. Finally, we describe adaptations to our knowledge platform needed to construct and serve private personal knowledge on-device. This includes private incremental KG construction, cross-device knowledge sync, and global knowledge enrichment.\n        \u25b3 Less\n      ",
    "title": "Growing and Serving Large Open-domain Knowledge Graphs",
    "date": "16 May, 2023",
    "authors": [
      "Ihab F. Ilyas",
      " JP Lacerda",
      " Yunyao Li",
      " Umar Farooq Minhas",
      " Ali Mousavi",
      " Jeffrey Pound",
      " Theodoros Rekatsinas",
      " Chiraag Sumanth"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.12671",
    "paper_id": "2301.12671",
    "abstract": "\n        Constrained clustering is a semi-supervised task that employs a limited amount of labelled data, formulated as constraints, to incorporate domain-specific knowledge and to significantly improve clustering accuracy. Previous work has considered exact optimization formulations that can guarantee optimal clustering while satisfying all constraints, however these approaches lack interpretability. Recently, decision-trees have been used to produce inherently interpretable clustering solutions, however existing approaches do not support clustering constraints and do not provide strong theoretical guarantees on solution quality. In this work, we present a novel SAT-based framework for interpretable clustering that supports clustering constraints and that also provides strong theoretical guarantees on solution quality. We also present new insight into the trade-off between interpretability and satisfaction of such user-provided constraints. Our framework is the first approach for interpretable and constrained clustering. Experiments with a range of real-world and synthetic datasets demonstrate that our approach can produce high-quality and interpretable constrained clustering solutions.\n        \u25b3 Less\n      ",
    "title": "Optimal Decision Trees For Interpretable Clustering with Constraints (Extended Version)",
    "date": "16 May, 2023",
    "authors": [
      "Pouya Shati",
      " Eldan Cohen",
      " Sheila McIlraith"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08116",
    "paper_id": "2305.08116",
    "abstract": "\n        Large knowledge graphs combine human knowledge garnered from projects ranging from academia and institutions to enterprises and crowdsourcing. Within such graphs, each relationship between two nodes represents a basic fact involving these two entities. The diversity of the semantics of relationships constitutes the richness of knowledge graphs, leading to the emergence of singular topologies, sometimes chaotic in appearance. However, this complex characteristic can be modeled in a simple way by introducing the concept of superficiality, which controls the overlap between relationships whose facts are generated independently. Superficiality also regulates the balance of the global distribution of knowledge by determining the proportion of misdescribed entities. This is the first model for the structure and dynamics of knowledge graphs. It leads to a better understanding of formal knowledge acquisition and organization.\n        \u25b3 Less\n      ",
    "title": "The Structure and Dynamics of Knowledge Graphs, with Superficiality",
    "date": "16 May, 2023",
    "authors": [
      "Lo\u00efck Lhote",
      " B\u00e9atrice Markhoff",
      " Arnaud Soulet"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09489",
    "paper_id": "2305.09489",
    "abstract": "\n        Denoising Diffusion Probabilistic Models (DDPMs) have made great strides in generating high-quality samples in both discrete and continuous domains. However, Discrete DDPMs (D3PMs) have yet to be applied to the domain of Symbolic Music. This work presents the direct generation of Polyphonic Symbolic Music using D3PMs. Our model exhibits state-of-the-art sample quality, according to current quantitative evaluation metrics, and allows for flexible infilling at the note level. We further show, that our models are accessible to post-hoc classifier guidance, widening the scope of possible applications. However, we also cast a critical view on quantitative evaluation of music sample quality via statistical metrics, and present a simple algorithm that can confound our metrics with completely spurious, non-musical samples.\n        \u25b3 Less\n      ",
    "title": "Discrete Diffusion Probabilistic Models for Symbolic Music Generation",
    "date": "16 May, 2023",
    "authors": [
      "Matthias Plasser",
      " Silvan Peter",
      " Gerhard Widmer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.13020",
    "paper_id": "2209.13020",
    "abstract": "\n        We are currently unable to specify human goals and societal values in a way that reliably directs AI behavior. Law-making and legal interpretation form a computational engine that converts opaque human values into legible directives. \"Law Informs Code\" is the research agenda embedding legal knowledge and reasoning in AI. Similar to how parties to a legal contract cannot foresee every potential contingency of their future relationship, and legislators cannot predict all the circumstances under which their proposed bills will be applied, we cannot ex ante specify rules that provably direct good AI behavior. Legal theory and practice have developed arrays of tools to address these specification problems. For instance, legal standards allow humans to develop shared understandings and adapt them to novel situations. In contrast to more prosaic uses of the law (e.g., as a deterrent of bad behavior through the threat of sanction), leveraged as an expression of how humans communicate their goals, and what society values, Law Informs Code.\n  We describe how data generated by legal processes (methods of law-making, statutory interpretation, contract drafting, applications of legal standards, legal reasoning, etc.) can facilitate the robust specification of inherently vague human goals. This increases human-AI alignment and the local usefulness of AI. Toward society-AI alignment, we present a framework for understanding law as the applied philosophy of multi-agent alignment. Although law is partly a reflection of historically contingent political power - and thus not a perfect aggregation of citizen preferences - if properly parsed, its distillation offers the most legitimate computational comprehension of societal values available. If law eventually informs powerful AI, engaging in the deliberative political process to improve law takes on even more meaning.\n        \u25b3 Less\n      ",
    "title": "Law Informs Code: A Legal Informatics Approach to Aligning Artificial Intelligence with Humans",
    "date": "16 May, 2023",
    "authors": [
      "John J. Nay"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09503",
    "paper_id": "2305.09503",
    "abstract": "\n        We present a method for extracting general modules for ontologies formulated in the description logic ALC. A module for an ontology is an ideally substantially smaller ontology that preserves all entailments for a user-specified set of terms. As such, it has applications such as ontology reuse and ontology analysis. Different from classical modules, general modules may use axioms not explicitly present in the input ontology, which allows for additional conciseness. So far, general modules have only been investigated for lightweight description logics. We present the first work that considers the more expressive description logic ALC. In particular, our contribution is a new method based on uniform interpolation supported by some new theoretical results. Our evaluation indicates that our general modules are often smaller than classical modules and uniform interpolants computed by the state-of-the-art, and compared with uniform interpolants, can be computed in a significantly shorter time. Moreover, our method can be used for, and in fact improves, the computation of uniform interpolants and classical modules.\n        \u25b3 Less\n      ",
    "title": "Efficient Computation of General Modules for ALC Ontologies (Extended Version)",
    "date": "16 May, 2023",
    "authors": [
      "Hui Yang",
      " Patrick Koopmann",
      " Yue Ma",
      " Nicole Bidoit"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.14131",
    "paper_id": "2212.14131",
    "abstract": "\n        Purpose: Tracking the 3D motion of the surgical tool and the patient anatomy is a fundamental requirement for computer-assisted skull-base surgery. The estimated motion can be used both for intra-operative guidance and for downstream skill analysis. Recovering such motion solely from surgical videos is desirable, as it is compliant with current clinical workflows and instrumentation.\n  Methods: We present Tracker of Anatomy and Tool (TAToo). TAToo jointly tracks the rigid 3D motion of patient skull and surgical drill from stereo microscopic videos. TAToo estimates motion via an iterative optimization process in an end-to-end differentiable form. For robust tracking performance, TAToo adopts a probabilistic formulation and enforces geometric constraints on the object level.\n  Results: We validate TAToo on both simulation data, where ground truth motion is available, as well as on anthropomorphic phantom data, where optical tracking provides a strong baseline. We report sub-millimeter and millimeter inter-frame tracking accuracy for skull and drill, respectively, with rotation errors below 1\u00b0. We further illustrate how TAToo may be used in a surgical navigation setting.\n  Conclusion: We present TAToo, which simultaneously tracks the surgical tool and the patient anatomy in skull-base surgery. TAToo directly predicts the motion from surgical videos, without the need of any markers. Our results show that the performance of TAToo compares favorably to competing approaches. Future work will include fine-tuning of our depth network to reach a 1 mm clinical accuracy goal desired for surgical applications in the skull base.\n        \u25b3 Less\n      ",
    "title": "TAToo: Vision-based Joint Tracking of Anatomy and Tool for Skull-base Surgery",
    "date": "16 May, 2023",
    "authors": [
      "Zhaoshuo Li",
      " Hongchao Shu",
      " Ruixing Liang",
      " Anna Goodridge",
      " Manish Sahu",
      " Francis X. Creighton",
      " Russell H. Taylor",
      " Mathias Unberath"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09508",
    "paper_id": "2305.09508",
    "abstract": "\n        We study formal languages which are capable of fully expressing quantitative probabilistic reasoning and do-calculus reasoning for causal effects, from a computational complexity perspective. We focus on satisfiability problems whose instance formulas allow expressing many tasks in probabilistic and causal inference. The main contribution of this work is establishing the exact computational complexity of these satisfiability problems. We introduce a new natural complexity class, named succ\u2203\\existsR, which can be viewed as a succinct variant of the well-studied class \u2203\\existsR, and show that the problems we consider are complete for succ\u2203\\existsR. Our results imply even stronger algorithmic limitations than were proven by Fagin, Halpern, and Megiddo (1990) and Moss\u00e9, Ibeling, and Icard (2022) for some variants of the standard languages used commonly in probabilistic and causal inference.\n        \u25b3 Less\n      ",
    "title": "The Hardness of Reasoning about Probabilities and Causality",
    "date": "16 May, 2023",
    "authors": [
      "Benito van der Zander",
      " Markus Bl\u00e4ser",
      " Maciej Li\u015bkiewicz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09510",
    "paper_id": "2305.09510",
    "abstract": "\n        Robotic manipulation systems operating in complex environments rely on perception systems that provide information about the geometry (pose and 3D shape) of the objects in the scene along with other semantic information such as object labels. This information is then used for choosing the feasible grasps on relevant objects. In this paper, we present a novel method to provide this geometric and semantic information of all objects in the scene as well as feasible grasps on those objects simultaneously. The main advantage of our method is its speed as it avoids sequential perception and grasp planning steps. With detailed quantitative analysis, we show that our method delivers competitive performance compared to the state-of-the-art dedicated methods for object shape, pose, and grasp predictions while providing fast inference at 30 frames per second speed.\n        \u25b3 Less\n      ",
    "title": "Real-time Simultaneous Multi-Object 3D Shape Reconstruction, 6DoF Pose Estimation and Dense Grasp Prediction",
    "date": "16 May, 2023",
    "authors": [
      "Shubham Agrawal",
      " Nikhil Chavan-Dafle",
      " Isaac Kasahara",
      " Selim Engin",
      " Jinwook Huh",
      " Volkan Isler"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.02127",
    "paper_id": "2211.02127",
    "abstract": "\n        We consider the problem of multi-agent navigation and collision avoidance when observations are limited to the local neighborhood of each agent. We propose InforMARL, a novel architecture for multi-agent reinforcement learning (MARL) which uses local information intelligently to compute paths for all the agents in a decentralized manner. Specifically, InforMARL aggregates information about the local neighborhood of agents for both the actor and the critic using a graph neural network and can be used in conjunction with any standard MARL algorithm. We show that (1) in training, InforMARL has better sample efficiency and performance than baseline approaches, despite using less information, and (2) in testing, it scales well to environments with arbitrary numbers of agents and obstacles. We illustrate these results using four task environments, including one with predetermined goals for each agent, and one in which the agents collectively try to cover all goals. Code available at https://github.com/nsidn98/InforMARL.\n        \u25b3 Less\n      ",
    "title": "Scalable Multi-Agent Reinforcement Learning through Intelligent Information Aggregation",
    "date": "16 May, 2023",
    "authors": [
      "Siddharth Nayak",
      " Kenneth Choi",
      " Wenqi Ding",
      " Sydney Dolan",
      " Karthik Gopalakrishnan",
      " Hamsa Balakrishnan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10448",
    "paper_id": "2305.10448",
    "abstract": "\n        This paper presents GenDoc, a general sequence-to-sequence document understanding model pre-trained with unified masking across three modalities: text, image, and layout. The proposed model utilizes an encoder-decoder architecture, which allows for increased adaptability to a wide range of downstream tasks with diverse output formats, in contrast to the encoder-only models commonly employed in document understanding. In addition to the traditional text infilling task used in previous encoder-decoder models, our pre-training extends to include tasks of masked image token prediction and masked layout prediction. We also design modality-specific instruction and adopt both disentangled attention and the mixture-of-modality-experts strategy to effectively capture the information leveraged by each modality. Evaluation of the proposed model through extensive experiments on several downstream tasks in document understanding demonstrates its ability to achieve superior or competitive performance compared to state-of-the-art approaches. Our analysis further suggests that GenDoc is more robust than the encoder-only models in scenarios where the OCR quality is imperfect.\n        \u25b3 Less\n      ",
    "title": "Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding",
    "date": "16 May, 2023",
    "authors": [
      "Shuwei Feng",
      " Tianyang Zhan",
      " Zhanming Jie",
      " Trung Quoc Luong",
      " Xiaoran Jin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14365",
    "paper_id": "2305.14365",
    "abstract": "\n        Artificial limbs are sophisticated devices to assist people with tasks of daily living. Despite advanced robotic prostheses demonstrating similar motion capabilities to biological limbs, users report them difficult and non-intuitive to use. Providing more effective feedback from the device to the user has therefore become a topic of increased interest. In particular, prediction learning methods from the field of reinforcement learning -- specifically, an approach termed Pavlovian signalling -- have been proposed as one approach for better modulating feedback in prostheses since they can adapt during continuous use. One challenge identified in these learning methods is that they can forget previously learned predictions when a user begins to successfully act upon delivered feedback. The present work directly addresses this challenge, contributing new evidence on the impact of algorithmic choices, such as on- or off-policy methods and representation choices, on the Pavlovian signalling from a machine to a user during their control of a robotic arm. Two conditions of algorithmic differences were studied using different scenarios of controlling a robotic arm: an automated motion system and human participant piloting. Contrary to expectations, off-policy learning did not provide the expected solution to the forgetting problem. We instead identified beneficial properties of a look-ahead state representation that made existing approaches able to learn (and not forget) predictions in support of Pavlovian signalling. This work therefore contributes new insight into the challenges of providing learned predictive feedback from a prosthetic device, and demonstrates avenues for more dynamic signalling in future human-machine interactions.\n        \u25b3 Less\n      ",
    "title": "Continually Learned Pavlovian Signalling Without Forgetting for Human-in-the-Loop Robotic Control",
    "date": "16 May, 2023",
    "authors": [
      "Adam S. R. Parker",
      " Michael R. Dawson",
      " Patrick M. Pilarski"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09543",
    "paper_id": "2305.09543",
    "abstract": "\n        Sleep staging is critical for assessing sleep quality and diagnosing sleep disorders. However, capturing both the spatial and temporal relationships within electroencephalogram (EEG) signals during different sleep stages remains challenging. In this paper, we propose a novel framework called the Hybrid Attention EEG Sleep Staging (HASS) Framework. Specifically, we propose a well-designed spatio-temporal attention mechanism to adaptively assign weights to inter-channels and intra-channel EEG segments based on the spatio-temporal relationship of the brain during different sleep stages. Experiment results on the MASS and ISRUC datasets demonstrate that HASS can significantly improve typical sleep staging networks. Our proposed framework alleviates the difficulties of capturing the spatial-temporal relationship of EEG signals during sleep staging and holds promise for improving the accuracy and reliability of sleep assessment in both clinical and research settings.\n        \u25b3 Less\n      ",
    "title": "EEG-based Sleep Staging with Hybrid Attention",
    "date": "16 May, 2023",
    "authors": [
      "Xinliang Zhou",
      " Chenyu Liu",
      " Jiaping Xiao",
      " Yang Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09573",
    "paper_id": "2305.09573",
    "abstract": "\n        Amidst decline in public trust in technology, computing ethics have taken center stage, and critics have raised questions about corporate ethics washing. Yet few studies examine the actual implementation of AI ethics values in technology companies. Based on a qualitative analysis of technology workers tasked with integrating AI ethics into product development, we find that workers experience an environment where policies, practices, and outcomes are decoupled. We analyze AI ethics workers as ethics entrepreneurs who work to institutionalize new ethics-related practices within organizations. We show that ethics entrepreneurs face three major barriers to their work. First, they struggle to have ethics prioritized in an environment centered around software product launches. Second, ethics are difficult to quantify in a context where company goals are incentivized by metrics. Third, the frequent reorganization of teams makes it difficult to access knowledge and maintain relationships central to their work. Consequently, individuals take on great personal risk when raising ethics issues, especially when they come from marginalized backgrounds. These findings shed light on complex dynamics of institutional change at technology companies.\n        \u25b3 Less\n      ",
    "title": "Walking the Walk of AI Ethics: Organizational Challenges and the Individualization of Risk among Ethics Entrepreneurs",
    "date": "16 May, 2023",
    "authors": [
      "Sanna J. Ali",
      " Ang\u00e8le Christin",
      " Andrew Smart",
      " Riitta Katila"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09574",
    "paper_id": "2305.09574",
    "abstract": "\n        Backdoors implanted in pre-trained language models (PLMs) can be transferred to various downstream tasks, which exposes a severe security threat. However, most existing backdoor attacks against PLMs are un-targeted and task-specific. Few targeted and task-agnostic methods use manually pre-defined triggers and output representations, which prevent the attacks from being more effective and general. In this paper, we first summarize the requirements that a more threatening backdoor attack against PLMs should satisfy, and then propose a new backdoor attack method called UOR, which breaks the bottleneck of the previous approach by turning manual selection into automatic optimization. Specifically, we define poisoned supervised contrastive learning which can automatically learn the more uniform and universal output representations of triggers for various PLMs. Moreover, we use gradient search to select appropriate trigger words which can be adaptive to different PLMs and vocabularies. Experiments show that our method can achieve better attack performance on various text classification tasks compared to manual methods. Further, we tested our method on PLMs with different architectures, different usage paradigms, and more difficult tasks, which demonstrated the universality of our method.\n        \u25b3 Less\n      ",
    "title": "UOR: Universal Backdoor Attacks on Pre-trained Language Models",
    "date": "16 May, 2023",
    "authors": [
      "Wei Du",
      " Peixuan Li",
      " Boqun Li",
      " Haodong Zhao",
      " Gongshen Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09584",
    "paper_id": "2305.09584",
    "abstract": "\n        Robots that assist humans will need to interact with articulated objects such as cabinets or microwaves. Early work on creating systems for doing so used proprioceptive sensing to estimate joint mechanisms during contact. However, nowadays, almost all systems use only vision and no longer consider proprioceptive information during contact. We believe that proprioceptive information during contact is a valuable source of information and did not find clear motivation for not using it in the literature. Therefore, in this paper, we create a system that, starting from a given grasp, uses proprioceptive sensing to open cabinets with a position-controlled robot and a parallel gripper. We perform a qualitative evaluation of this system, where we find that slip between the gripper and handle limits the performance. Nonetheless, we find that the system already performs quite well. This poses the question: should we make more use of proprioceptive information during contact in articulated object manipulation systems, or is it not worth the added complexity, and can we manage with vision alone? We do not have an answer to this question, but we hope to spark some discussion on the matter. The codebase and videos of the system are available at https://tlpss.github.io/revisiting-proprioception-for-articulated-manipulation/.\n        \u25b3 Less\n      ",
    "title": "Revisiting Proprioceptive Sensing for Articulated Object Manipulation",
    "date": "16 May, 2023",
    "authors": [
      "Thomas Lips",
      " Francis wyffels"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10449",
    "paper_id": "2305.10449",
    "abstract": "\n        Going beyond 'dendritic democracy', we introduce a 'democracy of local processors', termed Cooperator. Here we compare their capabilities when used in permutation-invariant neural networks for reinforcement learning (RL), with machine learning algorithms based on Transformers, such as ChatGPT. Transformers are based on the long-standing conception of integrate-and-fire 'point' neurons, whereas Cooperator is inspired by recent neurobiological breakthroughs suggesting that the cellular foundations of mental life depend on context-sensitive pyramidal neurons in the neocortex which have two functionally distinct points. We show that when used for RL, an algorithm based on Cooperator learns far quicker than that based on Transformer, even while having the same number of parameters.\n        \u25b3 Less\n      ",
    "title": "Cooperation Is All You Need",
    "date": "16 May, 2023",
    "authors": [
      "Ahsan Adeel",
      " Junaid Muzaffar",
      " Khubaib Ahmed",
      " Mohsin Raza"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09600",
    "paper_id": "2305.09600",
    "abstract": "\n        Collisions, crashes, and other incidents on road networks, if left unmitigated, can potentially cause cascading failures that can affect large parts of the system. Timely handling such extreme congestion scenarios is imperative to reduce emissions, enhance productivity, and improve the quality of urban living. In this work, we propose a Deep Reinforcement Learning (DRL) approach to reduce traffic congestion on multi-lane freeways during extreme congestion. The agent is trained to learn adaptive detouring strategies for congested freeway traffic such that the freeway lanes along with the local arterial network in proximity are utilized optimally, with rewards being congestion reduction and traffic speed improvement. The experimental setup is a 2.6-mile-long 4-lane freeway stretch in Shoreline, Washington, USA with two exits and associated arterial roads simulated on a microscopic and continuous multi-modal traffic simulator SUMO (Simulation of Urban MObility) while using parameterized traffic profiles generated using real-world traffic data. Our analysis indicates that DRL-based controllers can improve average traffic speed by 21\\% when compared to no-action during steep congestion. The study further discusses the trade-offs involved in the choice of reward functions, the impact of human compliance on agent performance, and the feasibility of knowledge transfer from one agent to other to address data sparsity and scaling issues.\n        \u25b3 Less\n      ",
    "title": "Deep Reinforcement Learning to Maximize Arterial Usage during Extreme Congestion",
    "date": "16 May, 2023",
    "authors": [
      "Ashutosh Dutta",
      " Milan Jain",
      " Arif Khan",
      " Arun Sathanur"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.10902",
    "paper_id": "2302.10902",
    "abstract": "\n        The imputation of missing values in multivariate time series (MTS) data is critical in ensuring data quality and producing reliable data-driven predictive models. Apart from many statistical approaches, a few recent studies have proposed state-of-the-art deep learning methods to impute missing values in MTS data. However, the evaluation of these deep methods is limited to one or two data sets, low missing rates, and completely random missing value types. This survey performs six data-centric experiments to benchmark state-of-the-art deep imputation methods on five time series health data sets. Our extensive analysis reveals that no single imputation method outperforms the others on all five data sets. The imputation performance depends on data types, individual variable statistics, missing value rates, and types. Deep learning methods that jointly perform cross-sectional (across variables) and longitudinal (across time) imputations of missing values in time series data yield statistically better data quality than traditional imputation methods. Although computationally expensive, deep learning methods are practical given the current availability of high-performance computing resources, especially when data quality and sample size are highly important in healthcare informatics. Our findings highlight the importance of data-centric selection of imputation methods to optimize data-driven predictive models.\n        \u25b3 Less\n      ",
    "title": "Deep Imputation of Missing Values in Time Series Health Data: A Review with Benchmarking",
    "date": "16 May, 2023",
    "authors": [
      "Maksims Kazijevs",
      " Manar D. Samad"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11999",
    "paper_id": "2305.11999",
    "abstract": "\n        There is an ever-present need for shared memory parallelization schemes to exploit the full potential of multi-core architectures. The most common parallelization API addressing this need today is OpenMP. Nevertheless, writing parallel code manually is complex and effort-intensive. Thus, many deterministic source-to-source (S2S) compilers have emerged, intending to automate the process of translating serial to parallel code. However, recent studies have shown that these compilers are impractical in many scenarios. In this work, we combine the latest advancements in the field of AI and natural language processing (NLP) with the vast amount of open-source code to address the problem of automatic parallelization. Specifically, we propose a novel approach, called OMPify, to detect and predict the OpenMP pragmas and shared-memory attributes in parallel code, given its serial version. OMPify is based on a Transformer-based model that leverages a graph-based representation of source code that exploits the inherent structure of code. We evaluated our tool by predicting the parallelization pragmas and attributes of a large corpus of (over 54,000) snippets of serial code written in C and C++ languages (Open-OMP-Plus). Our results demonstrate that OMPify outperforms existing approaches, the general-purposed and popular ChatGPT and targeted PragFormer models, in terms of F1 score and accuracy. Specifically, OMPify achieves up to 90% accuracy on commonly-used OpenMP benchmark tests such as NAS, SPEC, and PolyBench. Additionally, we performed an ablation study to assess the impact of different model components and present interesting insights derived from the study. Lastly, we also explored the potential of using data augmentation and curriculum learning techniques to improve the model's robustness and generalization capabilities.\n        \u25b3 Less\n      ",
    "title": "Advising OpenMP Parallelization via a Graph-Based Approach with Transformers",
    "date": "16 May, 2023",
    "authors": [
      "Tal Kadosh",
      " Nadav Schneider",
      " Niranjan Hasabnis",
      " Timothy Mattson",
      " Yuval Pinter",
      " Gal Oren"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09610",
    "paper_id": "2305.09610",
    "abstract": "\n        Recent semantic segmentation models accurately classify test-time examples that are similar to a training dataset distribution. However, their discriminative closed-set approach is not robust in practical data setups with distributional shifts and out-of-distribution (OOD) classes. As a result, the predicted probabilities can be very imprecise when used as confidence scores at test time. To address this, we propose a generative model for concurrent in-distribution misclassification (IDM) and OOD detection that relies on a normalizing flow framework. The proposed flow-based detector with an energy-based inputs (FlowEneDet) can extend previously deployed segmentation models without their time-consuming retraining. Our FlowEneDet results in a low-complexity architecture with marginal increase in the memory footprint. FlowEneDet achieves promising results on Cityscapes, Cityscapes-C, FishyScapes and SegmentMeIfYouCan benchmarks in IDM/OOD detection when applied to pretrained DeepLabV3+ and SegFormer semantic segmentation models.\n        \u25b3 Less\n      ",
    "title": "Concurrent Misclassification and Out-of-Distribution Detection for Semantic Segmentation via Energy-Based Normalizing Flow",
    "date": "16 May, 2023",
    "authors": [
      "Denis Gudovskiy",
      " Tomoyuki Okuno",
      " Yohei Nakata"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09617",
    "paper_id": "2305.09617",
    "abstract": "\n        Recent artificial intelligence (AI) systems have reached milestones in \"grand challenges\" ranging from Go to protein-folding. The capability to retrieve medical knowledge, reason over it, and answer medical questions comparably to physicians has long been viewed as one such grand challenge.\n  Large language models (LLMs) have catalyzed significant progress in medical question answering; Med-PaLM was the first model to exceed a \"passing\" score in US Medical Licensing Examination (USMLE) style questions with a score of 67.2% on the MedQA dataset. However, this and other prior work suggested significant room for improvement, especially when models' answers were compared to clinicians' answers. Here we present Med-PaLM 2, which bridges these gaps by leveraging a combination of base LLM improvements (PaLM 2), medical domain finetuning, and prompting strategies including a novel ensemble refinement approach.\n  Med-PaLM 2 scored up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19% and setting a new state-of-the-art. We also observed performance approaching or exceeding state-of-the-art across MedMCQA, PubMedQA, and MMLU clinical topics datasets.\n  We performed detailed human evaluations on long-form questions along multiple axes relevant to clinical applications. In pairwise comparative ranking of 1066 consumer medical questions, physicians preferred Med-PaLM 2 answers to those produced by physicians on eight of nine axes pertaining to clinical utility (p < 0.001). We also observed significant improvements compared to Med-PaLM on every evaluation axis (p < 0.001) on newly introduced datasets of 240 long-form \"adversarial\" questions to probe LLM limitations.\n  While further studies are necessary to validate the efficacy of these models in real-world settings, these results highlight rapid progress towards physician-level performance in medical question answering.\n        \u25b3 Less\n      ",
    "title": "Towards Expert-Level Medical Question Answering with Large Language Models",
    "date": "16 May, 2023",
    "authors": [
      "Karan Singhal",
      " Tao Tu",
      " Juraj Gottweis",
      " Rory Sayres",
      " Ellery Wulczyn",
      " Le Hou",
      " Kevin Clark",
      " Stephen Pfohl",
      " Heather Cole-Lewis",
      " Darlene Neal",
      " Mike Schaekermann",
      " Amy Wang",
      " Mohamed Amin",
      " Sami Lachgar",
      " Philip Mansfield",
      " Sushant Prakash",
      " Bradley Green",
      " Ewa Dominowska",
      " Blaise Aguera y Arcas",
      " Nenad Tomasev",
      " Yun Liu",
      " Renee Wong",
      " Christopher Semturs",
      " S. Sara Mahdavi",
      " Joelle Barral ",
      " et al. (6 additional authors not shown)"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.07175",
    "paper_id": "2209.07175",
    "abstract": "\n        Fuzzy rule based systems (FRBSs) is a rule-based system which uses linguistic fuzzy variables as antecedents and consequent to represent human understandable knowledge. They have been applied to various applications and areas throughout the soft computing literature. However, FRBSs suffers from many drawbacks such as uncertainty representation, high number of rules, interpretability loss, high computational time for learning etc. To overcome these issues with FRBSs, there exists many extensions of FRBSs. This paper presents an overview and literature review of recent trends on various types and prominent areas of fuzzy systems (FRBSs) namely genetic fuzzy system (GFS), hierarchical fuzzy system (HFS), neuro fuzzy system (NFS), evolving fuzzy system (eFS), FRBSs for big data, FRBSs for imbalanced data, interpretability in FRBSs and FRBSs which use cluster centroids as fuzzy rules. The review is for years 2010-2021. This paper also highlights important contributions, publication statistics and current trends in the field. The paper also addresses several open research areas which need further attention from the FRBSs research community.\n        \u25b3 Less\n      ",
    "title": "Literature Review of the Recent Trends and Applications in various Fuzzy Rule based systems",
    "date": "16 May, 2023",
    "authors": [
      "Ayush K. Varshney",
      " Vicen\u00e7 Torra"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09648",
    "paper_id": "2305.09648",
    "abstract": "\n        Prompt-tuning has emerged as a promising method for adapting pre-trained models to downstream tasks or aligning with human preferences. Prompt learning is widely used in NLP but has limited applicability to RL due to the complex physical meaning and environment-specific information contained within RL prompts. These factors require supervised learning to imitate the demonstrations and may result in a loss of meaning after learning. Additionally, directly extending prompt-tuning approaches to RL is challenging because RL prompts guide agent behavior based on environmental modeling and analysis, rather than filling in missing information, making it unlikely that adjustments to the prompt format for downstream tasks, as in NLP, can yield significant improvements. In this work, we propose the Prompt-Tuning DT algorithm to address these challenges by using trajectory segments as prompts to guide RL agents in acquiring environmental information and optimizing prompts via black-box tuning to enhance their ability to contain more relevant information, thereby enabling agents to make better decisions. Our approach involves randomly sampling a Gaussian distribution to fine-tune the elements of the prompt trajectory and using preference ranking function to find the optimization direction, thereby providing more informative prompts and guiding the agent towards specific preferences in the target environment. Extensive experiments show that with only 0.03% of the parameters learned, Prompt-Tuning DT achieves comparable or even better performance than full-model fine-tuning in low-data scenarios. Our work contributes to the advancement of prompt-tuning approaches in RL, providing a promising direction for optimizing large RL agents for specific preference tasks.\n        \u25b3 Less\n      ",
    "title": "Prompt-Tuning Decision Transformer with Preference Ranking",
    "date": "16 May, 2023",
    "authors": [
      "Shengchao Hu",
      " Li Shen",
      " Ya Zhang",
      " Dacheng Tao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09662",
    "paper_id": "2305.09662",
    "abstract": "\n        Text-guided human motion generation has drawn significant interest because of its impactful applications spanning animation and robotics. Recently, application of diffusion models for motion generation has enabled improvements in the quality of generated motions. However, existing approaches are limited by their reliance on relatively small-scale motion capture data, leading to poor performance on more diverse, in-the-wild prompts. In this paper, we introduce Make-An-Animation, a text-conditioned human motion generation model which learns more diverse poses and prompts from large-scale image-text datasets, enabling significant improvement in performance over prior works. Make-An-Animation is trained in two stages. First, we train on a curated large-scale dataset of (text, static pseudo-pose) pairs extracted from image-text datasets. Second, we fine-tune on motion capture data, adding additional layers to model the temporal dimension. Unlike prior diffusion models for motion generation, Make-An-Animation uses a U-Net architecture similar to recent text-to-video generation models. Human evaluation of motion realism and alignment with input text shows that our model reaches state-of-the-art performance on text-to-motion generation.\n        \u25b3 Less\n      ",
    "title": "Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation",
    "date": "16 May, 2023",
    "authors": [
      "Samaneh Azadi",
      " Akbar Shah",
      " Thomas Hayes",
      " Devi Parikh",
      " Sonal Gupta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09729",
    "paper_id": "2305.09729",
    "abstract": "\n        Heterogeneous graph neural networks (HGNNs) can learn from typed and relational graph data more effectively than conventional GNNs. With larger parameter spaces, HGNNs may require more training data, which is often scarce in real-world applications due to privacy regulations (e.g., GDPR). Federated graph learning (FGL) enables multiple clients to train a GNN collaboratively without sharing their local data. However, existing FGL methods mainly focus on homogeneous GNNs or knowledge graph embeddings; few have considered heterogeneous graphs and HGNNs. In federated heterogeneous graph learning, clients may have private graph schemas. Conventional FL/FGL methods attempting to define a global HGNN model would violate schema privacy. To address these challenges, we propose FedHGN, a novel and general FGL framework for HGNNs. FedHGN adopts schema-weight decoupling to enable schema-agnostic knowledge sharing and employs coefficients alignment to stabilize the training process and improve HGNN performance. With better privacy preservation, FedHGN consistently outperforms local training and conventional FL methods on three widely adopted heterogeneous graph datasets with varying client numbers. The code is available at https://github.com/cynricfu/FedHGN .\n        \u25b3 Less\n      ",
    "title": "FedHGN: A Federated Framework for Heterogeneous Graph Neural Networks",
    "date": "16 May, 2023",
    "authors": [
      "Xinyu Fu",
      " Irwin King"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09736",
    "paper_id": "2305.09736",
    "abstract": "\n        For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository https://github.com/s4nyam/pvt-addsl.\n        \u25b3 Less\n      ",
    "title": "ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language",
    "date": "16 May, 2023",
    "authors": [
      "Sanyam Jain"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09738",
    "paper_id": "2305.09738",
    "abstract": "\n        Training machine learning models in an incremental fashion is not only important but also an efficient way to achieve artificial general intelligence. The ability that humans possess of continuous or lifelong learning helps them to not forget previously learned tasks. However, current neural network models are prone to catastrophic forgetting when it comes to continual learning. Many researchers have come up with several techniques in order to reduce the effect of forgetting from neural networks, however, all techniques are studied classically with a very less focus on changing the machine learning model architecture. In this research paper, we show that it is not only possible to circumvent catastrophic forgetting in continual learning with novel hybrid classical-quantum neural networks, but also explains what features are most important to learn for classification. In addition, we also claim that if the model is trained with these explanations, it tends to give better performance and learn specific features that are far from the decision boundary. Finally, we present the experimental results to show comparisons between classical and classical-quantum hybrid architectures on benchmark MNIST and CIFAR-10 datasets. After successful runs of learning procedure, we found hybrid neural network outperforms classical one in terms of remembering the right evidences of the class-specific features.\n        \u25b3 Less\n      ",
    "title": "CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning",
    "date": "16 May, 2023",
    "authors": [
      "Sanyam Jain"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09773",
    "paper_id": "2305.09773",
    "abstract": "\n        Neural source code summarization is the task of generating natural language descriptions of source code behavior using neural networks. A fundamental component of most neural models is an attention mechanism. The attention mechanism learns to connect features in source code to specific words to use when generating natural language descriptions. Humans also pay attention to some features in code more than others. This human attention reflects experience and high-level cognition well beyond the capability of any current neural model. In this paper, we use data from published eye-tracking experiments to create a model of this human attention. The model predicts which words in source code are the most important for code summarization. Next, we augment a baseline neural code summarization approach using our model of human attention. We observe an improvement in prediction performance of the augmented approach in line with other bio-inspired neural models.\n        \u25b3 Less\n      ",
    "title": "Towards Modeling Human Attention from Eye Movements for Neural Source Code Summarization",
    "date": "16 May, 2023",
    "authors": [
      "Aakash Bansal",
      " Bonita Sharif",
      " Collin McMillan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09785",
    "paper_id": "2305.09785",
    "abstract": "\n        Learning vectors that capture the meaning of concepts remains a fundamental challenge. Somewhat surprisingly, perhaps, pre-trained language models have thus far only enabled modest improvements to the quality of such concept embeddings. Current strategies for using language models typically represent a concept by averaging the contextualised representations of its mentions in some corpus. This is potentially sub-optimal for at least two reasons. First, contextualised word vectors have an unusual geometry, which hampers downstream tasks. Second, concept embeddings should capture the semantic properties of concepts, whereas contextualised word vectors are also affected by other factors. To address these issues, we propose two contrastive learning strategies, based on the view that whenever two sentences reveal similar properties, the corresponding contextualised vectors should also be similar. One strategy is fully unsupervised, estimating the properties which are expressed in a sentence from the neighbourhood structure of the contextualised word embeddings. The second strategy instead relies on a distant supervision signal from ConceptNet. Our experimental results show that the resulting vectors substantially outperform existing concept embeddings in predicting the semantic properties of concepts, with the ConceptNet-based strategy achieving the best results. These findings are furthermore confirmed in a clustering task and in the downstream task of ontology completion.\n        \u25b3 Less\n      ",
    "title": "Distilling Semantic Concept Embeddings from Contrastively Fine-Tuned Language Models",
    "date": "16 May, 2023",
    "authors": [
      "Na Li",
      " Hanane Kteich",
      " Zied Bouraoui",
      " Steven Schockaert"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09793",
    "paper_id": "2305.09793",
    "abstract": "\n        Reinforcement learning (RL) exhibits impressive performance when managing complicated control tasks for robots. However, its wide application to physical robots is limited by the absence of strong safety guarantees. To overcome this challenge, this paper explores the control Lyapunov barrier function (CLBF) to analyze the safety and reachability solely based on data without explicitly employing a dynamic model. We also proposed the Lyapunov barrier actor-critic (LBAC), a model-free RL algorithm, to search for a controller that satisfies the data-based approximation of the safety and reachability conditions. The proposed approach is demonstrated through simulation and real-world robot control experiments, i.e., a 2D quadrotor navigation task. The experimental findings reveal this approach's effectiveness in reachability and safety, surpassing other model-free RL methods.\n        \u25b3 Less\n      ",
    "title": "Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions",
    "date": "16 May, 2023",
    "authors": [
      "Desong Du",
      " Shaohang Han",
      " Naiming Qi",
      " Haitham Bou Ammar",
      " Jun Wang",
      " Wei Pan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.07143",
    "paper_id": "2204.07143",
    "abstract": "\n        We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9% ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding-window attention, we open source our project and release our checkpoints at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer .\n        \u25b3 Less\n      ",
    "title": "Neighborhood Attention Transformer",
    "date": "16 May, 2023",
    "authors": [
      "Ali Hassani",
      " Steven Walton",
      " Jiachen Li",
      " Shen Li",
      " Humphrey Shi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.13844",
    "paper_id": "2302.13844",
    "abstract": "\n        One of the main challenges of multi-agent learning lies in establishing convergence of the algorithms, as, in general, a collection of individual, self-serving agents is not guaranteed to converge with their joint policy, when learning concurrently. This is in stark contrast to most single-agent environments, and sets a prohibitive barrier for deployment in practical applications, as it induces uncertainty in long term behavior of the system. In this work, we apply the concept of trapping regions, known from qualitative theory of dynamical systems, to create safety sets in the joint strategy space for decentralized learning. We propose a binary partitioning algorithm for verification that candidate sets form trapping regions in systems with known learning dynamics, and a heuristic sampling algorithm for scenarios where learning dynamics are not known. We demonstrate the applications to a regularized version of Dirac Generative Adversarial Network, a four-intersection traffic control scenario run in a state of the art open-source microscopic traffic simulator SUMO, and a mathematical model of economic competition.\n        \u25b3 Less\n      ",
    "title": "Safe Multi-agent Learning via Trapping Regions",
    "date": "16 May, 2023",
    "authors": [
      "Aleksander Czechowski",
      " Frans A. Oliehoek"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09815",
    "paper_id": "2305.09815",
    "abstract": "\n        The last few years have significantly increased global interest in generative artificial intelligence. Deepfakes, which are synthetically created videos, emerged as an application of generative artificial intelligence. Fake news and pornographic content have been the two most prevalent negative use cases of deepfakes in the digital ecosystem. Deepfakes have some advantageous applications that experts in the subject have thought of in the areas of filmmaking, teaching, etc. Research on the potential of deepfakes among people with disabilities is, however, scarce or nonexistent. This workshop paper explores the potential of deepfakes as an assistive technology. We examined Reddit conversations regarding Nvdia's new videoconferencing feature which allows participants to maintain eye contact during online meetings. Through manual web scraping and qualitative coding, we found 162 relevant comments discussing the relevance and appropriateness of the technology for people with Autism. The themes identified from the qualitative codes indicate a number of concerns for technology among the autistic community. We suggest that developing generative AI-based assistive solutions will have ramifications for human-computer interaction (HCI), and present open questions that should be investigated further in this space.\n        \u25b3 Less\n      ",
    "title": "Exploring outlooks towards generative AI-based assistive technologies for people with Autism",
    "date": "16 May, 2023",
    "authors": [
      "Deepak Giri",
      " Erin Brady"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09817",
    "paper_id": "2305.09817",
    "abstract": "\n        The current state-of-the-art Diffusion model has demonstrated excellent results in generating images. However, the images are monotonous and are mostly the result of the distribution of images of people in the training set, making it challenging to generate multiple images for a fixed number of individuals. This problem can often only be solved by fine-tuning the training of the model. This means that each individual/animated character image must be trained if it is to be drawn, and the hardware and cost of this training is often beyond the reach of the average user, who accounts for the largest number of people. To solve this problem, the Character Image Feature Encoder model proposed in this paper enables the user to use the process by simply providing a picture of the character to make the image of the character in the generated image match the expectation. In addition, various details can be adjusted during the process using prompts. Unlike traditional Image-to-Image models, the Character Image Feature Encoder extracts only the relevant image features, rather than information about the model's composition or movements. In addition, the Character Image Feature Encoder can be adapted to different models after training. The proposed model can be conveniently incorporated into the Stable Diffusion generation process without modifying the model's ontology or used in combination with Stable Diffusion as a joint model.\n        \u25b3 Less\n      ",
    "title": "A Method for Training-free Person Image Picture Generation",
    "date": "16 May, 2023",
    "authors": [
      "Tianyu Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09828",
    "paper_id": "2305.09828",
    "abstract": "\n        It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they \"look\" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique \"mimetic initialization\".\n        \u25b3 Less\n      ",
    "title": "Mimetic Initialization of Self-Attention Layers",
    "date": "16 May, 2023",
    "authors": [
      "Asher Trockman",
      " J. Zico Kolter"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09832",
    "paper_id": "2305.09832",
    "abstract": "\n        Cellular-Vehicle-to-Everything (C-V2X) is currently at the forefront of the digital transformation of our society. By enabling vehicles to communicate with each other and with the traffic environment using cellular networks, we redefine transportation, improving road safety and transportation services, increasing efficiency of traffic flows, and reducing environmental impact. This paper proposes a decentralized approach for provisioning Cellular Vehicular-to-Network (C-V2N) services, addressing the coupled problems of service task placement and scaling of edge resources. We formalize the joint problem and prove its complexity. We propose an approach to tackle it, linking the two problems, employing decentralized decision-making using (i) a greedy approach for task placement and (ii) a Deep Deterministic Policy Gradient (DDPG) based approach for scaling. We benchmark the performance of our approach, focusing on the scaling agent, against several State-of-the-Art (SoA) scaling approaches via simulations using a real C-V2N traffic data set. The results show that DDPG-based solutions outperform SoA solutions, keeping the latency experienced by the C-V2N service below the target delay while optimizing the use of computing resources. By conducting a complexity analysis, we prove that DDPG-based solutions achieve runtimes in the range of sub-milliseconds, meeting the strict latency requirements of C-V2N services.\n        \u25b3 Less\n      ",
    "title": "A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular Vehicle-to-Network Service Provisioning",
    "date": "16 May, 2023",
    "authors": [
      "Cyril Shih-Huan Hsu",
      " Jorge Mart\u00edn-P\u00e9rez",
      " Danny De Vleeschauwer",
      " Koteswararao Kondepu",
      " Luca Valcarenghi",
      " Xi Li",
      " Chrysa Papagianni"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09838",
    "paper_id": "2305.09838",
    "abstract": "\n        Coagent networks for reinforcement learning (RL) [Thomas and Barto, 2011] provide a powerful and flexible framework for deriving principled learning rules for arbitrary stochastic neural networks. The coagent framework offers an alternative to backpropagation-based deep learning (BDL) that overcomes some of backpropagation's main limitations. For example, coagent networks can compute different parts of the network \\emph{asynchronously} (at different rates or at different times), can incorporate non-differentiable components that cannot be used with backpropagation, and can explore at levels higher than their action spaces (that is, they can be designed as hierarchical networks for exploration and/or temporal abstraction). However, the coagent framework is not just an alternative to BDL; the two approaches can be blended: BDL can be combined with coagent learning rules to create architectures with the advantages of both approaches. This work generalizes the coagent theory and learning rules provided by previous works; this generalization provides more flexibility for network architecture design within the coagent framework. This work also studies one of the chief disadvantages of coagent networks: high variance updates for networks that have many coagents and do not use backpropagation. We show that a coagent algorithm with a policy network that does not use backpropagation can scale to a challenging RL domain with a high-dimensional state and action space (the MuJoCo Ant environment), learning reasonable (although not state-of-the-art) policies. These contributions motivate and provide a more general theoretical foundation for future work that studies coagent networks.\n        \u25b3 Less\n      ",
    "title": "Coagent Networks: Generalized and Scaled",
    "date": "16 May, 2023",
    "authors": [
      "James E. Kostas",
      " Scott M. Jordan",
      " Yash Chandak",
      " Georgios Theocharous",
      " Dhawal Gupta",
      " Martha White",
      " Bruno Castro da Silva",
      " Philip S. Thomas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09856",
    "paper_id": "2305.09856",
    "abstract": "\n        Federated learning (FL), as an emerging artificial intelligence (AI) approach, enables decentralized model training across multiple devices without exposing their local training data. FL has been increasingly gaining popularity in both academia and industry. While research works have been proposed to improve the fault tolerance of FL, the real impact of unreliable devices (e.g., dropping out, misconfiguration, poor data quality) in real-world applications is not fully investigated. We carefully chose two representative, real-world classification problems with a limited numbers of clients to better analyze FL fault tolerance. Contrary to the intuition, simple FL algorithms can perform surprisingly well in the presence of unreliable clients.\n        \u25b3 Less\n      ",
    "title": "Keep It Simple: Fault Tolerance Evaluation of Federated Learning with Unreliable Clients",
    "date": "16 May, 2023",
    "authors": [
      "Victoria Huang",
      " Shaleeza Sohail",
      " Michael Mayo",
      " Tania Lorido Botran",
      " Mark Rodrigues",
      " Chris Anderson",
      " Melanie Ooi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09858",
    "paper_id": "2305.09858",
    "abstract": "\n        Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation labeling tasks using just 1 to 5 labeled examples per relation. Additionally, we experiment with different prompt engineering techniques to examine their impact on model performance. Our results show that LLMs significantly outperform existing KG completion models in relation labeling for e-commerce KGs and exhibit performance strong enough to replace human labeling.\n        \u25b3 Less\n      ",
    "title": "Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs",
    "date": "16 May, 2023",
    "authors": [
      "Jiao Chen",
      " Luyi Ma",
      " Xiaohan Li",
      " Nikhil Thakurdesai",
      " Jianpeng Xu",
      " Jason H. D. Cho",
      " Kaushiki Nag",
      " Evren Korpeoglu",
      " Sushant Kumar",
      " Kannan Achan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09874",
    "paper_id": "2305.09874",
    "abstract": "\n        Unmanned ground vehicles (UGVs) in unstructured environments mostly operate through teleoperation. To enable stable teleoperated driving in unstructured environments, some research has suggested driver assistance and evaluation methods that involve user studies, which can be costly and require lots of time and effort. A simulation model-based approach has been proposed to complement the user study; however, the models on teleoperated driving do not account for unstructured environments. Our proposed solution involves simulation models of teleoperated driving for drivers that utilize a deep generative model. Initially, we build a teleoperated driving simulator to imitate unstructured environments based on previous research and collect driving data from drivers. Then, we design and implement the simulation models based on a conditional variational autoencoder (CVAE). Our evaluation results demonstrate that the proposed teleoperated driving model can generate data by simulating the driver appropriately in unstructured canyon terrains.\n        \u25b3 Less\n      ",
    "title": "Generative Model-based Simulation of Driver Behavior when Using Control Input Interface for Teleoperated Driving in Unstructured Canyon Terrains",
    "date": "16 May, 2023",
    "authors": [
      "Hyeonggeun Yun",
      " Younggeol Cho",
      " Jinwon Lee",
      " Arim Ha",
      " Jihyeok Yun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09875",
    "paper_id": "2305.09875",
    "abstract": "\n        This research seeks to explore how Augmented Reality (AR) can support learning psychomotor tasks that involve complex manipulation and reasoning processes. The AR prototype was created using Unity and used on HoloLens 2 headsets. Here, we explore the potential of AR as a training or assistive tool for spatial tasks and the need for intelligent mechanisms to enable adaptive and personalized interactions between learners and AR. The paper discusses how integrating AR with Artificial Intelligence (AI) can adaptably scaffold the learning of complex tasks to accelerate the development of expertise in psychomotor domains.\n        \u25b3 Less\n      ",
    "title": "Augmenting Learning with Augmented Reality: Exploring the Affordances of AR in Supporting Mastery of Complex Psychomotor Tasks",
    "date": "16 May, 2023",
    "authors": [
      "Dong Woo Yoo",
      " Sakib Reza",
      " Nicholas Wilson",
      " Kemi Jona",
      " Mohsen Moghaddam"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09877",
    "paper_id": "2305.09877",
    "abstract": "\n        Initiated by the University Consortium of Geographic Information Science (UCGIS), GIS&T Body of Knowledge (BoK) is a community-driven endeavor to define, develop, and document geospatial topics related to geographic information science and technologies (GIS&T). In recent years, GIS&T BoK has undergone rigorous development in terms of its topic re-organization and content updating, resulting in a new digital version of the project. While the BoK topics provide useful materials for researchers and students to learn about GIS, the semantic relationships among the topics, such as semantic similarity, should also be identified so that a better and automated topic navigation can be achieved. Currently, the related topics are either defined manually by editors or authors, which may result in an incomplete assessment of topic relationship. To address this challenge, our research evaluates the effectiveness of multiple natural language processing (NLP) techniques in extracting semantics from text, including both deep neural networks and traditional machine learning approaches. Besides, a novel text summarization - KACERS (Keyword-Aware Cross-Encoder-Ranking Summarizer) - is proposed to generate a semantic summary of scientific publications. By identifying the semantic linkages among key topics, this work provides guidance for future development and content organization of the GIS&T BoK project. It also offers a new perspective on the use of machine learning techniques for analyzing scientific publications, and demonstrate the potential of KACERS summarizer in semantic understanding of long text documents.\n        \u25b3 Less\n      ",
    "title": "Semantic Similarity Measure of Natural Language Text through Machine Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study Using UCGIS GIS&T Body of Knowledge",
    "date": "16 May, 2023",
    "authors": [
      "Yuanyuan Tian",
      " Wenwen Li",
      " Sizhe Wang",
      " Zhining Gu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.11719",
    "paper_id": "2301.11719",
    "abstract": "\n        Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.\n        \u25b3 Less\n      ",
    "title": "The Exploration of Knowledge-Preserving Prompts for Document Summarisation",
    "date": "16 May, 2023",
    "authors": [
      "Chen Chen",
      " Wei Emma Zhang",
      " Alireza Seyed Shakeri",
      " Makhmoor Fiza"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09892",
    "paper_id": "2305.09892",
    "abstract": "\n        Contrastive learning has been widely studied in sentence representation learning. However, earlier works mainly focus on the construction of positive examples, while in-batch samples are often simply treated as negative examples. This approach overlooks the importance of selecting appropriate negative examples, potentially leading to a scarcity of hard negatives and the inclusion of false negatives. To address these issues, we propose ClusterNS (Clustering-aware Negative Sampling), a novel method that incorporates cluster information into contrastive learning for unsupervised sentence representation learning. We apply a modified K-means clustering algorithm to supply hard negatives and recognize in-batch false negatives during training, aiming to solve the two issues in one unified framework. Experiments on semantic textual similarity (STS) tasks demonstrate that our proposed ClusterNS compares favorably with baselines in unsupervised sentence representation learning. Our code has been made publicly available.\n        \u25b3 Less\n      ",
    "title": "Clustering-Aware Negative Sampling for Unsupervised Sentence Representation",
    "date": "16 May, 2023",
    "authors": [
      "Jinghao Deng",
      " Fanqi Wan",
      " Tao Yang",
      " Xiaojun Quan",
      " Rui Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09913",
    "paper_id": "2305.09913",
    "abstract": "\n        Just-in-Time Adaptive Interventions (JITAIs) are a class of personalized health interventions developed within the behavioral science community. JITAIs aim to provide the right type and amount of support by iteratively selecting a sequence of intervention options from a pre-defined set of components in response to each individual's time varying state. In this work, we explore the application of reinforcement learning methods to the problem of learning intervention option selection policies. We study the effect of context inference error and partial observability on the ability to learn effective policies. Our results show that the propagation of uncertainty from context inferences is critical to improving intervention efficacy as context uncertainty increases, while policy gradient algorithms can provide remarkable robustness to partially observed behavioral state information.\n        \u25b3 Less\n      ",
    "title": "Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions",
    "date": "16 May, 2023",
    "authors": [
      "Karine Karine",
      " Predrag Klasnja",
      " Susan A. Murphy",
      " Benjamin M. Marlin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09922",
    "paper_id": "2305.09922",
    "abstract": "\n        Reinforcement learning (RL) is experiencing a resurgence in research interest, where Learning Classifier Systems (LCSs) have been applied for many years. However, traditional Michigan approaches tend to evolve large rule bases that are difficult to interpret or scale to domains beyond standard mazes. A Pittsburgh Genetic Fuzzy System (dubbed Fuzzy MoCoCo) is proposed that utilises both multiobjective and cooperative coevolutionary mechanisms to evolve fuzzy rule-based policies for RL environments. Multiobjectivity in the system is concerned with policy performance vs. complexity. The continuous state RL environment Mountain Car is used as a testing bed for the proposed system. Results show the system is able to effectively explore the trade-off between policy performance and complexity, and learn interpretable, high-performing policies that use as few rules as possible.\n        \u25b3 Less\n      ",
    "title": "A Genetic Fuzzy System for Interpretable and Parsimonious Reinforcement Learning Policies",
    "date": "16 May, 2023",
    "authors": [
      "Jordan T. Bishop",
      " Marcus Gallagher",
      " Will N. Browne"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.02180",
    "paper_id": "2302.02180",
    "abstract": "\n        Value decomposition methods have gained popularity in the field of cooperative multi-agent reinforcement learning. However, almost all existing methods follow the principle of Individual Global Max (IGM) or its variants, which limits their problem-solving capabilities. To address this, we propose a dual self-awareness value decomposition framework, inspired by the notion of dual self-awareness in psychology, that entirely rejects the IGM premise. Each agent consists of an ego policy for action selection and an alter ego value function to solve the credit assignment problem. The value function factorization can ignore the IGM assumption by utilizing an explicit search procedure. On the basis of the above, we also suggest a novel anti-ego exploration mechanism to avoid the algorithm becoming stuck in a local optimum. As the first fully IGM-free value decomposition method, our proposed framework achieves desirable performance in various cooperative tasks.\n        \u25b3 Less\n      ",
    "title": "Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative Multi-Agent Reinforcement Learning",
    "date": "16 May, 2023",
    "authors": [
      "Zhiwei Xu",
      " Bin Zhang",
      " Dapeng Li",
      " Guangchong Zhou",
      " Zeren Zhang",
      " Guoliang Fan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09945",
    "paper_id": "2305.09945",
    "abstract": "\n        Interest in reinforcement learning (RL) has recently surged due to the application of deep learning techniques, but these connectionist approaches are opaque compared with symbolic systems. Learning Classifier Systems (LCSs) are evolutionary machine learning systems that can be categorised as eXplainable AI (XAI) due to their rule-based nature. Michigan LCSs are commonly used in RL domains as the alternative Pittsburgh systems (e.g. SAMUEL) suffer from complex algorithmic design and high computational requirements; however they can produce more compact/interpretable solutions than Michigan systems. We aim to develop two novel Pittsburgh LCSs to address RL domains: PPL-DL and PPL-ST. The former acts as a \"zeroth-level\" system, and the latter revisits SAMUEL's core Monte Carlo learning mechanism for estimating rule strength. We compare our two Pittsburgh systems to the Michigan system XCS across deterministic and stochastic FrozenLake environments. Results show that PPL-ST performs on-par or better than PPL-DL and outperforms XCS in the presence of high levels of environmental uncertainty. Rulesets evolved by PPL-ST can achieve higher performance than those evolved by XCS, but in a more parsimonious and therefore more interpretable fashion, albeit with higher computational cost. This indicates that PPL-ST is an LCS well-suited to producing explainable policies in RL domains.\n        \u25b3 Less\n      ",
    "title": "Pittsburgh Learning Classifier Systems for Explainable Reinforcement Learning: Comparing with XCS",
    "date": "16 May, 2023",
    "authors": [
      "Jordan T. Bishop",
      " Marcus Gallagher",
      " Will N. Browne"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09368",
    "paper_id": "2305.09368",
    "abstract": "\n        This study proposes an unsupervised sequence-to-sequence learning approach that automatically assesses the motion-induced reliability degradation of the cardiac volume signal (CVS) in multi-channel electrical impedance-based hemodynamic monitoring. The proposed method attempts to tackle shortcomings in existing learning-based assessment approaches, such as the requirement of manual annotation for motion influence and the lack of explicit mechanisms for realizing motion-induced abnormalities under contextual variations in CVS over time. By utilizing long-short term memory and variational auto-encoder structures, an encoder--decoder model is trained not only to self-reproduce an input sequence of the CVS but also to extrapolate the future in a parallel fashion. By doing so, the model can capture contextual knowledge lying in a temporal CVS sequence while being regularized to explore a general relationship over the entire time-series. A motion-influenced CVS of low-quality is detected, based on the residual between the input sequence and its neural representation with a cut--off value determined from the two-sigma rule of thumb over the training set. Our experimental observations validated two claims: (i) in the learning environment of label-absence, assessment performance is achievable at a competitive level to the supervised setting, and (ii) the contextual information across a time series of CVS is advantageous for effectively realizing motion-induced unrealistic distortions in signal amplitude and morphology. We also investigated the capability as a pseudo-labeling tool to minimize human-craft annotation by preemptively providing strong candidates for motion-induced anomalies. Empirical evidence has shown that machine-guided annotation can reduce inevitable human-errors during manual assessment while minimizing cumbersome and time-consuming processes.\n        \u25b3 Less\n      ",
    "title": "Unsupervised sequence-to-sequence learning for automatic signal quality assessment in multi-channel electrical impedance-based hemodynamic monitoring",
    "date": "16 May, 2023",
    "authors": [
      "Chang Min Hyun",
      " Tae-Geun Kim",
      " Kyounghun Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09974",
    "paper_id": "2305.09974",
    "abstract": "\n        We study Graph Neural Networks (GNNs)-based embedding techniques for knowledge graph (KG) reasoning. For the first time, we link the path redundancy issue in the state-of-the-art KG reasoning models based on path encoding and message passing to the transformation error in model training, which brings us new theoretical insights into KG reasoning, as well as high efficacy in practice. On the theoretical side, we analyze the entropy of transformation error in KG paths and point out query-specific redundant paths causing entropy increases. These findings guide us to maintain the shortest paths and remove redundant paths for minimized-entropy message passing. To achieve this goal, on the practical side, we propose an efficient Graph Percolation Process motivated by the percolation model in Fluid Mechanics, and design a lightweight GNN-based KG reasoning framework called Graph Percolation Embeddings (GraPE). GraPE outperforms previous state-of-the-art methods in both transductive and inductive reasoning tasks while requiring fewer training parameters and less inference time.\n        \u25b3 Less\n      ",
    "title": "River of No Return: Graph Percolation Embeddings for Efficient Knowledge Graph Reasoning",
    "date": "16 May, 2023",
    "authors": [
      "Kai Wang",
      " Siqiang Luo",
      " Dan Lin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09978",
    "paper_id": "2305.09978",
    "abstract": "\n        Many machine learning applications and tasks rely on the stochastic gradient descent (SGD) algorithm and its variants. Effective step length selection is crucial for the success of these algorithms, which has motivated the development of algorithms such as ADAM or AdaGrad. In this paper, we propose a novel algorithm for adaptive step length selection in the classical SGD framework, which can be readily adapted to other stochastic algorithms. Our proposed algorithm is inspired by traditional nonlinear optimization techniques and is supported by analytical findings. We show that under reasonable conditions, the algorithm produces step lengths in line with well-established theoretical requirements, and generates iterates that converge to a stationary neighborhood of a solution in expectation. We test the proposed algorithm on logistic regressions and deep neural networks and demonstrate that the algorithm can generate step lengths comparable to the best step length obtained from manual tuning.\n        \u25b3 Less\n      ",
    "title": "Stochastic Ratios Tracking Algorithm for Large Scale Machine Learning Problems",
    "date": "16 May, 2023",
    "authors": [
      "Shigeng Sun",
      " Yuchen Xie"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09992",
    "paper_id": "2305.09992",
    "abstract": "\n        Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), digital twin, Metaverse and other related digital technologies have attracted much attention in recent years. These new emerging technologies are changing the world significantly. This research introduces a fusion model, i.e. Fusion Universe (FU), where the virtual, physical, and cognitive worlds are merged together. Therefore, it is crucial to establish a set of principles for the fusion model that is compatible with our physical universe laws and principles. This paper investigates several aspects that could affect immersive and interactive experience; and proposes the fundamental principles for Fusion Universe that can integrate physical and virtual world seamlessly.\n        \u25b3 Less\n      ",
    "title": "A Fusion Model: Towards a Virtual, Physical and Cognitive Integration and its Principles",
    "date": "16 May, 2023",
    "authors": [
      "Hao Lan Zhang",
      " Yun Xue",
      " Yifan Lu",
      " Sanghyuk Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09993",
    "paper_id": "2305.09993",
    "abstract": "\n        We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, and human-written CoT baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts.\n        \u25b3 Less\n      ",
    "title": "Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling",
    "date": "16 May, 2023",
    "authors": [
      "Weijia Xu",
      " Andrzej Banburski-Fahey",
      " Nebojsa Jojic"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09994",
    "paper_id": "2305.09994",
    "abstract": "\n        Time-domain single-channel speech enhancement (SE) still remains challenging to extract the target speaker without any prior information on multi-talker conditions. It has been shown via auditory attention decoding that the brain activity of the listener contains the auditory information of the attended speaker. In this paper, we thus propose a novel time-domain brain-assisted SE network (BASEN) incorporating electroencephalography (EEG) signals recorded from the listener for extracting the target speaker from monaural speech mixtures. The proposed BASEN is based on the fully-convolutional time-domain audio separation network. In order to fully leverage the complementary information contained in the EEG signals, we further propose a convolutional multi-layer cross attention module to fuse the dual-branch features. Experimental results on a public dataset show that the proposed model outperforms the state-of-the-art method in several evaluation metrics. The reproducible code is available at https://github.com/jzhangU/Basen.git.\n        \u25b3 Less\n      ",
    "title": "BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions",
    "date": "16 May, 2023",
    "authors": [
      "Jie Zhang",
      " Qing-Tian Xu",
      " Qiu-Shi Zhu",
      " Zhen-Hua Ling"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14366",
    "paper_id": "2305.14366",
    "abstract": "\n        This study demonstrates that the soft biological tissues of humans can be used as a type of soft body in physical reservoir computing. Soft biological tissues possess characteristics such as stress-strain nonlinearity and viscoelasticity that satisfy the requirements for physical reservoir computing, including nonlinearity and memory. The aim of this study was to utilize the dynamics of human soft tissues as a physical reservoir for the emulation of nonlinear dynamical systems. To demonstrate this concept, joint angle data during motion in the flexion-extension direction of the wrist joint, and ultrasound images of the muscles associated with that motion, were acquired from human participants. The input to the system was the angle of the wrist joint, while the deformation field within the muscle (obtained from ultrasound images) represented the state of the reservoir. The results indicate that the dynamics of soft tissue have a positive impact on the computational task of emulating nonlinear dynamical systems. This research suggests that the soft tissue of humans can be used as a potential computational resource.\n        \u25b3 Less\n      ",
    "title": "Information processing via human soft tissue",
    "date": "16 May, 2023",
    "authors": [
      "Yo Kobayashi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.02847",
    "paper_id": "2205.02847",
    "abstract": "\n        In medical imaging analysis, deep learning has shown promising results. We frequently rely on volumetric data to segment medical images, necessitating the use of 3D architectures, which are commended for their capacity to capture interslice context. However, because of the 3D convolutions, max pooling, up-convolutions, and other operations utilized in these networks, these architectures are often more inefficient in terms of time and computation than their 2D equivalents. Furthermore, there are few 3D pretrained model weights, and pretraining is often difficult. We present a simple yet effective 2D method to handle 3D data while efficiently embedding the 3D knowledge during training. We propose transforming volumetric data into 2D super images and segmenting with 2D networks to solve these challenges. Our method generates a super-resolution image by stitching slices side by side in the 3D image. We expect deep neural networks to capture and learn these properties spatially despite losing depth information. This work aims to present a novel perspective when dealing with volumetric data, and we test the hypothesis using CNN and ViT networks as well as self-supervised pretraining. While attaining equal, if not superior, results to 3D networks utilizing only 2D counterparts, the model complexity is reduced by around threefold. Because volumetric data is relatively scarce, we anticipate that our approach will entice more studies, particularly in medical imaging analysis.\n        \u25b3 Less\n      ",
    "title": "Super Images -- A New 2D Perspective on 3D Medical Imaging Analysis",
    "date": "16 May, 2023",
    "authors": [
      "Ikboljon Sobirov",
      " Numan Saeed",
      " Mohammad Yaqub"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.08168",
    "paper_id": "2304.08168",
    "abstract": "\n        As the rapid development of Intelligent Tutoring Systems (ITS) in the past decade, tracing the students' knowledge state has become more and more important in order to provide individualized learning guidance. This is the main idea of Knowledge Tracing (KT), which models students' mastery of knowledge concepts (KCs, skills needed to solve a question) based on their past interactions on platforms. Plenty of KT models have been proposed and have shown remarkable performance recently. However, the majority of these models use concepts to index questions, which means the predefined skill tags for each question are required in advance to indicate the KCs needed to answer that question correctly. This makes it pretty hard to apply on large-scale online education platforms where questions are often not well-organized by skill tags. In this paper, we propose Q-matrix-based Attentive Knowledge Tracing (QAKT), an end-to-end style model that is able to apply the attentive method to scenes where no predefined skill tags are available without sacrificing its performance. With a novel hybrid embedding method based on the q-matrix and Rasch model, QAKT is capable of modeling problems hierarchically and learning the q-matrix efficiently based on students' sequences. Meanwhile, the architecture of QAKT ensures that it is friendly to questions associated with multiple skills and has outstanding interpretability. After conducting experiments on a variety of open datasets, we empirically validated that our model shows similar or even better performance than state-of-the-art KT methods. Results of further experiments suggest that the q-matrix learned by QAKT is highly model-agnostic and more information-sufficient than the one labeled by human experts, which could help with the data mining tasks in existing ITSs.\n        \u25b3 Less\n      ",
    "title": "Attentive Q-Matrix Learning for Knowledge Tracing",
    "date": "16 May, 2023",
    "authors": [
      "Zhongfeng Jia",
      " Wei Su",
      " Jiamin Liu",
      " Wenli Yue"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2106.10811",
    "paper_id": "2106.10811",
    "abstract": "\n        Shape implicit neural representations (INRs) have recently shown to be effective in shape analysis and reconstruction tasks. Existing INRs require point coordinates to learn the implicit level sets of the shape. When a normal vector is available for each point, a higher fidelity representation can be learned, however normal vectors are often not provided as raw data. Furthermore, the method's initialization has been shown to play a crucial role for surface reconstruction. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal INRs that further improves convergence to the desired solution. We evaluate the effectiveness of our approach on the task of surface reconstruction and shape space learning and show SOTA performance compared to other unoriented methods. Code and model parameters available at our project page https://chumbyte.github.io/DiGS-Site/.\n        \u25b3 Less\n      ",
    "title": "DiGS : Divergence guided shape implicit neural representation for unoriented point clouds",
    "date": "16 May, 2023",
    "authors": [
      "Yizhak Ben-Shabat",
      " Chamin Hewa Koneputugodage",
      " Stephen Gould"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10013",
    "paper_id": "2305.10013",
    "abstract": "\n        Large pre-trained language models (PLMs) have garnered significant attention for their versatility and potential for solving a wide spectrum of natural language processing (NLP) tasks. However, the cost of running these PLMs may be prohibitive. Furthermore, PLMs may not be open-sourced due to commercial considerations and potential risks of misuse, such as GPT-3. The parameters and gradients of PLMs are unavailable in this scenario. To solve the issue, black-box tuning has been proposed, which utilizes derivative-free optimization (DFO), instead of gradient descent, for training task-specific continuous prompts. However, these gradient-free methods still exhibit a significant gap compared to gradient-based methods. In this paper, we introduce gradient descent into black-box tuning scenario through knowledge distillation. Furthermore, we propose a novel method GDFO, which integrates gradient descent and derivative-free optimization to optimize task-specific continuous prompts in a harmonized manner. Experimental results show that GDFO can achieve significant performance gains over previous state-of-the-art methods.\n        \u25b3 Less\n      ",
    "title": "When Gradient Descent Meets Derivative-Free Optimization: A Match Made in Black-Box Scenario",
    "date": "16 May, 2023",
    "authors": [
      "Chengcheng Han",
      " Liqing Cui",
      " Renyu Zhu",
      " Jianing Wang",
      " Nuo Chen",
      " Qiushi Sun",
      " Xiang Li",
      " Ming Gao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10018",
    "paper_id": "2305.10018",
    "abstract": "\n        Fine-grained classification is a challenging task that involves identifying subtle differences between objects within the same category. This task is particularly challenging in scenarios where data is scarce. Visual transformers (ViT) have recently emerged as a powerful tool for image classification, due to their ability to learn highly expressive representations of visual data using self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine tuned using semi-supervised learning techniques, suitable for situations where we have lack of annotated data. This is particularly common in e-commerce, where images are readily available but labels are noisy, nonexistent, or expensive to obtain. Our results demonstrate that Semi-ViT outperforms traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned with limited annotated data. These findings indicate that Semi-ViTs hold significant promise for applications that require precise and fine-grained classification of visual data.\n        \u25b3 Less\n      ",
    "title": "Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers",
    "date": "16 May, 2023",
    "authors": [
      "Manuel Lagunas",
      " Brayan Impata",
      " Victor Martinez",
      " Virginia Fernandez",
      " Christos Georgakis",
      " Sofia Braun",
      " Felipe Bertrand"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10021",
    "paper_id": "2305.10021",
    "abstract": "\n        Answer Set Programming with Quantifiers ASP(Q) extends Answer Set Programming (ASP) to allow for declarative and modular modeling of problems from the entire polynomial hierarchy. The first implementation of ASP(Q), called qasp, was based on a translation to Quantified Boolean Formulae (QBF) with the aim of exploiting the well-developed and mature QBF-solving technology. However, the implementation of the QBF encoding employed in qasp is very general and might produce formulas that are hard to evaluate for existing QBF solvers because of the large number of symbols and sub-clauses. In this paper, we present a new implementation that builds on the ideas of qasp and features both a more efficient encoding procedure and new optimized encodings of ASP(Q) programs in QBF. The new encodings produce smaller formulas (in terms of the number of quantifiers, variables, and clauses) and result in a more efficient evaluation process. An algorithm selection strategy automatically combines several QBF-solving back-ends to further increase performance. An experimental analysis, conducted on known benchmarks, shows that the new system outperforms qasp.\n        \u25b3 Less\n      ",
    "title": "An efficient solver for ASP(Q)",
    "date": "16 May, 2023",
    "authors": [
      "Wolfgang Faber",
      " Giuseppe Mazzotta",
      " Francesco Ricca"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10032",
    "paper_id": "2305.10032",
    "abstract": "\n        Understanding the laws that govern a phenomenon is the core of scientific progress. This is especially true when the goal is to model the interplay between different aspects in a causal fashion. Indeed, causal inference itself is specifically designed to quantify the underlying relationships that connect a cause to its effect. Causal discovery is a branch of the broader field of causality in which causal graphs is recovered from data (whenever possible), enabling the identification and estimation of causal effects. In this paper, we explore recent advancements in a unified manner, provide a consistent overview of existing algorithms developed under different settings, report useful tools and data, present real-world applications to understand why and how these methods can be fruitfully exploited.\n        \u25b3 Less\n      ",
    "title": "A Survey on Causal Discovery: Theory and Practice",
    "date": "16 May, 2023",
    "authors": [
      "Alessio Zanga",
      " Fabio Stella"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10041",
    "paper_id": "2305.10041",
    "abstract": "\n        Assessing the pre-operative risk of lymph node metastases in endometrial cancer patients is a complex and challenging task. In principle, machine learning and deep learning models are flexible and expressive enough to capture the dynamics of clinical risk assessment. However, in this setting we are limited to observational data with quality issues, missing values, small sample size and high dimensionality: we cannot reliably learn such models from limited observational data with these sources of bias. Instead, we choose to learn a causal Bayesian network to mitigate the issues above and to leverage the prior knowledge on endometrial cancer available from clinicians and physicians. We introduce a causal discovery algorithm for causal Bayesian networks based on bootstrap resampling, as opposed to the single imputation used in related works. Moreover, we include a context variable to evaluate whether selection bias results in learning spurious associations. Finally, we discuss the strengths and limitations of our findings in light of the presence of missing data that may be missing-not-at-random, which is common in real-world clinical settings.\n        \u25b3 Less\n      ",
    "title": "Risk Assessment of Lymph Node Metastases in Endometrial Cancer Patients: A Causal Approach",
    "date": "17 May, 2023",
    "authors": [
      "Alessio Zanga",
      " Alice Bernasconi",
      " Peter J. F. Lucas",
      " Hanny Pijnenborg",
      " Casper Reijnen",
      " Marco Scutari",
      " Fabio Stella"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10051",
    "paper_id": "2305.10051",
    "abstract": "\n        This paper addresses the \u03b5\u03b5-close parameter tuning problem for Bayesian Networks (BNs): find a minimal \u03b5\u03b5-close amendment of probability entries in a given set of (rows in) conditional probability tables that make a given quantitative constraint on the BN valid. Based on the state-of-the-art \"region verification\" techniques for parametric Markov chains, we propose an algorithm whose capabilities go beyond any existing techniques. Our experiments show that \u03b5\u03b5-close tuning of large BN benchmarks with up to 8 parameters is feasible. In particular, by allowing (i) varied parameters in multiple CPTs and (ii) inter-CPT parameter dependencies, we treat subclasses of parametric BNs that have received scant attention so far.\n        \u25b3 Less\n      ",
    "title": "Finding an \n\u03b5\n-close Variation of Parameters in Bayesian Networks",
    "date": "17 May, 2023",
    "authors": [
      "Bahare Salmani",
      " Joost-Pieter Katoen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07348",
    "paper_id": "2305.07348",
    "abstract": "\n        Estimating the pose of an uncooperative spacecraft is an important computer vision problem for enabling the deployment of automatic vision-based systems in orbit, with applications ranging from on-orbit servicing to space debris removal. Following the general trend in computer vision, more and more works have been focusing on leveraging Deep Learning (DL) methods to address this problem. However and despite promising research-stage results, major challenges preventing the use of such methods in real-life missions still stand in the way. In particular, the deployment of such computation-intensive algorithms is still under-investigated, while the performance drop when training on synthetic and testing on real images remains to mitigate. The primary goal of this survey is to describe the current DL-based methods for spacecraft pose estimation in a comprehensive manner. The secondary goal is to help define the limitations towards the effective deployment of DL-based spacecraft pose estimation solutions for reliable autonomous vision-based applications. To this end, the survey first summarises the existing algorithms according to two approaches: hybrid modular pipelines and direct end-to-end regression methods. A comparison of algorithms is presented not only in terms of pose accuracy but also with a focus on network architectures and models' sizes keeping potential deployment in mind. Then, current monocular spacecraft pose estimation datasets used to train and test these methods are discussed. The data generation methods: simulators and testbeds, the domain gap and the performance drop between synthetically generated and lab/space collected images and the potential solutions are also discussed. Finally, the paper presents open research questions and future directions in the field, drawing parallels with other computer vision applications.\n        \u25b3 Less\n      ",
    "title": "A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects",
    "date": "17 May, 2023",
    "authors": [
      "Leo Pauly",
      " Wassim Rharbaoui",
      " Carl Shneider",
      " Arunkumar Rathinam",
      " Vincent Gaudilliere",
      " Djamila Aouada"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10057",
    "paper_id": "2305.10057",
    "abstract": "\n        Coronal Mass Ejections (CMEs) correspond to dramatic expulsions of plasma and magnetic field from the solar corona into the heliosphere. CMEs are scientifically relevant because they are involved in the physical mechanisms characterizing the active Sun. However, more recently CMEs have attracted attention for their impact on space weather, as they are correlated to geomagnetic storms and may induce the generation of Solar Energetic Particles streams. In this space weather framework, the present paper introduces a physics-driven artificial intelligence (AI) approach to the prediction of CMEs travel time, in which the deterministic drag-based model is exploited to improve the training phase of a cascade of two neural networks fed with both remote sensing and in-situ data. This study shows that the use of physical information in the AI architecture significantly improves both the accuracy and the robustness of the travel time prediction.\n        \u25b3 Less\n      ",
    "title": "Physics-driven machine learning for the prediction of coronal mass ejections' travel times",
    "date": "17 May, 2023",
    "authors": [
      "Sabrina Guastavino",
      " Valentina Candiani",
      " Alessandro Bemporad",
      " Francesco Marchetti",
      " Federico Benvenuto",
      " Anna Maria Massone",
      " Roberto Susino",
      " Daniele Telloni",
      " Silvano Fineschi",
      " Michele Piana"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10059",
    "paper_id": "2305.10059",
    "abstract": "\n        Predictive Maintenance (PdM) methods aim to facilitate the scheduling of maintenance work before equipment failure. In this context, detecting early faults in automated teller machines (ATMs) has become increasingly important since these machines are susceptible to various types of unpredictable failures. ATMs track execution status by generating massive event-log data that collect system messages unrelated to the failure event. Predicting machine failure based on event logs poses additional challenges, mainly in extracting features that might represent sequences of events indicating impending failures. Accordingly, feature learning approaches are currently being used in PdM, where informative features are learned automatically from minimally processed sensor data. However, a gap remains to be seen on how these approaches can be exploited for deriving relevant features from event-log-based data. To fill this gap, we present a predictive model based on a convolutional kernel (MiniROCKET and HYDRA) to extract features from the original event-log data and a linear classifier to classify the sample based on the learned features. The proposed methodology is applied to a significant real-world collected dataset. Experimental results demonstrated how one of the proposed convolutional kernels (i.e. HYDRA) exhibited the best classification performance (accuracy of 0.759 and AUC of 0.693). In addition, statistical analysis revealed that the HYDRA and MiniROCKET models significantly overcome one of the established state-of-the-art approaches in time series classification (InceptionTime), and three non-temporal ML methods from the literature. The predictive model was integrated into a container-based decision support system to support operators in the timely maintenance of ATMs.\n        \u25b3 Less\n      ",
    "title": "A hybrid feature learning approach based on convolutional kernels for ATM fault prediction using event-log data",
    "date": "17 May, 2023",
    "authors": [
      "V\u00edctor Manuel Vargas",
      " Riccardo Rosati",
      " C\u00e9sar Herv\u00e1s-Mart\u00ednez",
      " Adriano Mancini",
      " Luca Romeo",
      " Pedro Antonio Guti\u00e9rrez"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10069",
    "paper_id": "2305.10069",
    "abstract": "\n        In eXplainable Artificial Intelligence (XAI), counterfactual explanations are known to give simple, short, and comprehensible justifications for complex model decisions. However, we are yet to see more applied studies in which they are applied in real-world cases. To fill this gap, this study focuses on showing how counterfactuals are applied to employability-related problems which involve complex machine learning algorithms. For these use cases, we use real data obtained from a public Belgian employment institution (VDAB). The use cases presented go beyond the mere application of counterfactuals as explanations, showing how they can enhance decision support, comply with legal requirements, guide controlled changes, and analyze novel insights.\n        \u25b3 Less\n      ",
    "title": "Unveiling the Potential of Counterfactuals Explanations in Employability",
    "date": "17 May, 2023",
    "authors": [
      "Raphael Mazzine Barbosa de Oliveira",
      " Sofie Goethals",
      " Dieter Brughmans",
      " David Martens"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10091",
    "paper_id": "2305.10091",
    "abstract": "\n        Multi-agent reinforcement learning (MARL) is a widely used Artificial Intelligence (AI) technique. However, current studies and applications need to address its scalability, non-stationarity, and trustworthiness. This paper aims to review methods and applications and point out research trends and visionary prospects for the next decade. First, this paper summarizes the basic methods and application scenarios of MARL. Second, this paper outlines the corresponding research methods and their limitations on safety, robustness, generalization, and ethical constraints that need to be addressed in the practical applications of MARL. In particular, we believe that trustworthy MARL will become a hot research topic in the next decade. In addition, we suggest that considering human interaction is essential for the practical application of MARL in various societies. Therefore, this paper also analyzes the challenges while MARL is applied to human-machine interaction.\n        \u25b3 Less\n      ",
    "title": "Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges",
    "date": "17 May, 2023",
    "authors": [
      "Ziyuan Zhou",
      " Guanjun Liu",
      " Ying Tang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10096",
    "paper_id": "2305.10096",
    "abstract": "\n        A recent trend in the domain of open-domain conversational agents is enabling them to converse empathetically to emotional prompts. Current approaches either follow an end-to-end approach or condition the responses on similar emotion labels to generate empathetic responses. But empathy is a broad concept that refers to the cognitive and emotional reactions of an individual to the observed experiences of another and it is more complex than mere mimicry of emotion. Hence, it requires identifying complex human conversational strategies and dynamics in addition to generic emotions to control and interpret empathetic responding capabilities of chatbots. In this work, we make use of a taxonomy of eight empathetic response intents in addition to generic emotion categories in building a dialogue response generation model capable of generating empathetic responses in a controllable and interpretable manner. It consists of two modules: 1) a response emotion/intent prediction module; and 2) a response generation module. We propose several rule-based and neural approaches to predict the next response's emotion/intent and generate responses conditioned on these predicted emotions/intents. Automatic and human evaluation results emphasize the importance of the use of the taxonomy of empathetic response intents in producing more diverse and empathetically more appropriate responses than end-to-end models.\n        \u25b3 Less\n      ",
    "title": "Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots",
    "date": "17 May, 2023",
    "authors": [
      "Anuradha Welivita",
      " Pearl Pu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10103",
    "paper_id": "2305.10103",
    "abstract": "\n        Social Networks represent one of the most important online sources to share content across a world-scale audience. In this context, predicting whether a post will have any impact in terms of engagement is of crucial importance to drive the profitable exploitation of these media. In the literature, several studies address this issue by leveraging direct features of the posts, typically related to the textual content and the user publishing it. In this paper, we argue that the rise of engagement is also related to another key component, which is the semantic connection among posts published by users in social media. Hence, we propose TweetGage, a Graph Neural Network solution to predict the user engagement based on a novel graph-based model that represents the relationships among posts. To validate our proposal, we focus on the Twitter platform and perform a thorough experimental campaign providing evidence of its quality.\n        \u25b3 Less\n      ",
    "title": "Predicting Tweet Engagement with Graph Neural Networks",
    "date": "17 May, 2023",
    "authors": [
      "Marco Arazzi",
      " Marco Cotogni",
      " Antonino Nocera",
      " Luca Virgili"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.13443",
    "paper_id": "2304.13443",
    "abstract": "\n        In the realm of urban transportation, metro systems serve as crucial and sustainable means of public transit. However, their substantial energy consumption poses a challenge to the goal of sustainability. Disturbances such as delays and passenger flow changes can further exacerbate this issue by negatively affecting energy efficiency in metro systems. To tackle this problem, we propose a policy-based reinforcement learning approach that reschedules the metro timetable and optimizes energy efficiency in metro systems under disturbances by adjusting the dwell time and cruise speed of trains. Our experiments conducted in a simulation environment demonstrate the superiority of our method over baseline methods, achieving a traction energy consumption reduction of up to 10.9% and an increase in regenerative braking energy utilization of up to 47.9%. This study provides an effective solution to the energy-saving problem of urban rail transit.\n        \u25b3 Less\n      ",
    "title": "Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning",
    "date": "17 May, 2023",
    "authors": [
      "Haiqin Xie",
      " Cheng Wang",
      " Shicheng Li",
      " Yue Zhang",
      " Shanshan Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10113",
    "paper_id": "2305.10113",
    "abstract": "\n        Artificial Intelligence plays a main role in supporting and improving smart manufacturing and Industry 4.0, by enabling the automation of different types of tasks manually performed by domain experts. In particular, assessing the compliance of a product with the relative schematic is a time-consuming and prone-to-error process. In this paper, we address this problem in a specific industrial scenario. In particular, we define a Neuro-Symbolic approach for automating the compliance verification of the electrical control panels. Our approach is based on the combination of Deep Learning techniques with Answer Set Programming (ASP), and allows for identifying possible anomalies and errors in the final product even when a very limited amount of training data is available. The experiments conducted on a real test case provided by an Italian Company operating in electrical control panel production demonstrate the effectiveness of the proposed approach.\n        \u25b3 Less\n      ",
    "title": "Neuro-Symbolic AI for Compliance Checking of Electrical Control Panels",
    "date": "17 May, 2023",
    "authors": [
      "Vito Barbara",
      " Massimo Guarascio",
      " Nicola Leone",
      " Giuseppe Manco",
      " Alessandro Quarta",
      " Francesco Ricca",
      " Ettore Ritacco"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.09513",
    "paper_id": "2304.09513",
    "abstract": "\n        All data on the Internet are transferred by network traffic, thus accurately modeling network traffic can help improve network services quality and protect data privacy. Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as application classification, attack detection and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.\n  To tackle these challenges, in this paper, we make the first attempt to provide a generative pretrained model NetGPT for both traffic understanding and generation tasks. We propose the multi-pattern network traffic modeling to construct unified text inputs and support both traffic understanding and generation tasks. We further optimize the adaptation effect of the pretrained model to diversified tasks by shuffling header fields, segmenting packets in flows, and incorporating diverse task labels with prompts. With diverse traffic datasets from encrypted software, DNS, private industrial protocols and cryptocurrency mining, expensive experiments demonstrate the effectiveness of our NetGPT in a range of traffic understanding and generation tasks on traffic datasets, and outperform state-of-the-art baselines by a wide margin.\n        \u25b3 Less\n      ",
    "title": "NetGPT: Generative Pretrained Transformer for Network Traffic",
    "date": "17 May, 2023",
    "authors": [
      "Xuying Meng",
      " Chungang Lin",
      " Yequan Wang",
      " Yujun Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.13616",
    "paper_id": "2301.13616",
    "abstract": "\n        Despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning. In this paper, we revisit these results and show that, with a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue. We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL benchmark, showing that it is capable of achieving performance comparable to ensemble-based methods and outperforming ensemble-free approaches by a wide margin.\n        \u25b3 Less\n      ",
    "title": "Anti-Exploration by Random Network Distillation",
    "date": "17 May, 2023",
    "authors": [
      "Alexander Nikulin",
      " Vladislav Kurenkov",
      " Denis Tarasov",
      " Sergey Kolesnikov"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10161",
    "paper_id": "2305.10161",
    "abstract": "\n        Wind power is becoming an increasingly important source of renewable energy worldwide. However, wind farm power control faces significant challenges due to the high system complexity inherent in these farms. A novel communication-based multi-agent deep reinforcement learning large-scale wind farm multivariate control is proposed to handle this challenge and maximize power output. A wind farm multivariate power model is proposed to study the influence of wind turbines (WTs) wake on power. The multivariate model includes axial induction factor, yaw angle, and tilt angle controllable variables. The hierarchical communication multi-agent proximal policy optimization (HCMAPPO) algorithm is proposed to coordinate the multivariate large-scale wind farm continuous controls. The large-scale wind farm is divided into multiple wind turbine aggregators (WTAs), and neighboring WTAs can exchange information through hierarchical communication to maximize the wind farm power output. Simulation results demonstrate that the proposed multivariate HCMAPPO can significantly increase wind farm power output compared to the traditional PID control, coordinated model-based predictive control, and multi-agent deep deterministic policy gradient algorithm. Particularly, the HCMAPPO algorithm can be trained with the environment based on the thirteen-turbine wind farm and effectively applied to larger wind farms. At the same time, there is no significant increase in the fatigue damage of the wind turbine blade from the wake control as the wind farm scale increases. The multivariate HCMAPPO control can realize the collective large-scale wind farm maximum power output.\n        \u25b3 Less\n      ",
    "title": "Collective Large-scale Wind Farm Multivariate Power Output Control Based on Hierarchical Communication Multi-Agent Proximal Policy Optimization",
    "date": "17 May, 2023",
    "authors": [
      "Yubao Zhang",
      " Xin Chen",
      " Sumei Gong",
      " Haojie Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.11636",
    "paper_id": "2212.11636",
    "abstract": "\n        Adequately assigning credit to actions for future outcomes based on their contributions is a long-standing open challenge in Reinforcement Learning. The assumptions of the most commonly used credit assignment method are disadvantageous in tasks where the effects of decisions are not immediately evident. Furthermore, this method can only evaluate actions that have been selected by the agent, making it highly inefficient. Still, no alternative methods have been widely adopted in the field. Hindsight Credit Assignment is a promising, but still unexplored candidate, which aims to solve the problems of both long-term and counterfactual credit assignment. In this thesis, we empirically investigate Hindsight Credit Assignment to identify its main benefits, and key points to improve. Then, we apply it to factored state representations, and in particular to state representations based on the causal structure of the environment. In this setting, we propose a variant of Hindsight Credit Assignment that effectively exploits a given causal structure. We show that our modification greatly decreases the workload of Hindsight Credit Assignment, making it more efficient and enabling it to outperform the baseline credit assignment method on various tasks. This opens the way to other methods based on given or learned causal structures.\n        \u25b3 Less\n      ",
    "title": "Towards Causal Credit Assignment",
    "date": "17 May, 2023",
    "authors": [
      "M\u00e1ty\u00e1s Schubert"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10465",
    "paper_id": "2305.10465",
    "abstract": "\n        Estimating the 3DoF rotation from a single RGB image is an important yet challenging problem. As a popular approach, probabilistic rotation modeling additionally carries prediction uncertainty information, compared to single-prediction rotation regression. For modeling probabilistic distribution over SO(3), it is natural to use Gaussian-like Bingham distribution and matrix Fisher, however they are shown to be sensitive to outlier predictions, e.g. 180\u2218180^\\circ error and thus are unlikely to converge with optimal performance. In this paper, we draw inspiration from multivariate Laplace distribution and propose a novel rotation Laplace distribution on SO(3). Our rotation Laplace distribution is robust to the disturbance of outliers and enforces much gradient to the low-error region that it can improve. In addition, we show that our method also exhibits robustness to small noises and thus tolerates imperfect annotations. With this benefit, we demonstrate its advantages in semi-supervised rotation regression, where the pseudo labels are noisy. To further capture the multi-modal rotation solution space for symmetric objects, we extend our distribution to rotation Laplace mixture model and demonstrate its effectiveness. Our extensive experiments show that our proposed distribution and the mixture model achieve state-of-the-art performance in all the rotation regression experiments over both probabilistic and non-probabilistic baselines.\n        \u25b3 Less\n      ",
    "title": "Towards Robust Probabilistic Modeling on SO(3) via Rotation Laplace Distribution",
    "date": "17 May, 2023",
    "authors": [
      "Yingda Yin",
      " Jiangran Lyu",
      " Yang Wang",
      " He Wang",
      " Baoquan Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2208.06178",
    "paper_id": "2208.06178",
    "abstract": "\n        Identifying, classifying, and analyzing arguments in legal discourse has been a prominent area of research since the inception of the argument mining field. However, there has been a major discrepancy between the way natural language processing (NLP) researchers model and annotate arguments in court decisions and the way legal experts understand and analyze legal argumentation. While computational approaches typically simplify arguments into generic premises and claims, arguments in legal research usually exhibit a rich typology that is important for gaining insights into the particular case and applications of law in general. We address this problem and make several substantial contributions to move the field forward. First, we design a new annotation scheme for legal arguments in proceedings of the European Court of Human Rights (ECHR) that is deeply rooted in the theory and practice of legal argumentation research. Second, we compile and annotate a large corpus of 373 court decisions (2.3M tokens and 15k annotated argument spans). Finally, we train an argument mining model that outperforms state-of-the-art models in the legal NLP domain and provide a thorough expert-based evaluation. All datasets and source codes are available under open lincenses at https://github.com/trusthlt/mining-legal-arguments.\n        \u25b3 Less\n      ",
    "title": "Mining Legal Arguments in Court Decisions",
    "date": "17 May, 2023",
    "authors": [
      "Ivan Habernal",
      " Daniel Faber",
      " Nicola Recchia",
      " Sebastian Bretthauer",
      " Iryna Gurevych",
      " Indra Spiecker genannt D\u00f6hmann",
      " Christoph Burchard"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10167",
    "paper_id": "2305.10167",
    "abstract": "\n        In this work we introduce a structured signaling game, an extension of the classical signaling game with a similarity structure between meanings in the context, along with a variant of the Rational Speech Act (RSA) framework which we call structured-RSA (sRSA) for pragmatic reasoning in structured domains. We explore the behavior of the sRSA in the domain of color and show that pragmatic agents using sRSA on top of semantic representations, derived from the World Color Survey, attain efficiency very close to the information theoretic limit after only 1 or 2 levels of recursion. We also explore the interaction between pragmatic reasoning and learning in multi-agent reinforcement learning framework. Our results illustrate that artificial agents using sRSA develop communication closer to the information theoretic frontier compared to agents using RSA and just reinforcement learning. We also find that the ambiguity of the semantic representation increases as the pragmatic agents are allowed to perform deeper reasoning about each other during learning.\n        \u25b3 Less\n      ",
    "title": "Pragmatic Reasoning in Structured Signaling Games",
    "date": "17 May, 2023",
    "authors": [
      "Emil Carlsson",
      " Devdatt Dubhashi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10466",
    "paper_id": "2305.10466",
    "abstract": "\n        Lung cancer(LC) is a type of malignant neoplasm that originates in the bronchial mucosa or glands.As a clinically common nodule,solitary pulmonary nodules(SPNs) have a significantly higher probability of malignancy when they are larger than 8 mm in diameter.But there is also a risk of lung cancer when the diameter is less than 8mm,the purpose of this study was to create a nomogram for estimating the likelihood of lung cancer in patients with SPNs of 8 mm or smaller using computed tomography(CT) scans and biomarker information.Use CT scans and various biomarkers as input to build predictive models for the likelihood of lung cancer in patients with SPNs of 8 mm or less.The age,precursor gastrin-releasing peptide (ProGRP),gender,Carcinoembryonic Antigen(CEA),and stress corrosion cracking(SCC) were independent key tumor markers and were entered into the nomogram.The developed nomogram demonstrated strong accuracy in predicting lung cancer risk,with an internal validation area under the receiver operating characteristics curve(ROC) of 0.8474.The calibration curves plotted showed that the nomogram predicted the probability of lung cancer with good agreement with the actual probability.In this study,we finally succeeded in constructing a suitable nomogram that could predict the risk of lung cancer in patients with SPNs<=8 mm in diameter.The model has a high level of accuracy and is able to accurately distinguish between different patients,allowing clinicians to develop personalized treatment plans for individuals with SPNs.\n        \u25b3 Less\n      ",
    "title": "Solitary pulmonary nodules prediction for lung cancer patients using nomogram and machine learning",
    "date": "17 May, 2023",
    "authors": [
      "Hailan Zhang",
      " Gongjin Song"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10192",
    "paper_id": "2305.10192",
    "abstract": "\n        Solving job shop scheduling problems (JSSPs) with a fixed strategy, such as a priority dispatching rule, may yield satisfactory results for several problem instances but, nevertheless, insufficient results for others. From this single-strategy perspective finding a near optimal solution to a specific JSSP varies in difficulty even if the machine setup remains the same. A recent intensively researched and promising method to deal with difficulty variability is Deep Reinforcement Learning (DRL), which dynamically adjusts an agent's planning strategy in response to difficult instances not only during training, but also when applied to new situations. In this paper, we further improve DLR as an underlying method by actively incorporating the variability of difficulty within the same problem size into the design of the learning process. We base our approach on a state-of-the-art methodology that solves JSSP by means of DRL and graph neural network embeddings. Our work supplements the training routine of the agent by a curriculum learning strategy that ranks the problem instances shown during training by a new metric of problem instance difficulty. Our results show that certain curricula lead to significantly better performances of the DRL solutions. Agents trained on these curricula beat the top performance of those trained on randomly distributed training data, reaching 3.2% shorter average makespans.\n        \u25b3 Less\n      ",
    "title": "Curriculum Learning in Job Shop Scheduling using Reinforcement Learning",
    "date": "17 May, 2023",
    "authors": [
      "Constantin Waubert de Puiseau",
      " Hasan Tercan",
      " Tobias Meisen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10196",
    "paper_id": "2305.10196",
    "abstract": "\n        Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution, so that researchers can recognise the current state and future directions of this field. We provide an organisation of the literature based on evolution, dataset, method and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation causes learning bias in languages and domains; 3) performance improvements are often reported on single benchmarks, but advanced methods are still far from real-world use; 4) general-purpose metrics are not reliable on nuances and complexities of ZPT, emphasizing the necessity of targeted metrics; 5) apart from commonly-cited errors, ZPs will cause risks of gender bias.\n        \u25b3 Less\n      ",
    "title": "A Survey on Zero Pronoun Translation",
    "date": "17 May, 2023",
    "authors": [
      "Longyue Wang",
      " Siyou Liu",
      " Mingzhou Xu",
      " Linfeng Song",
      " Shuming Shi",
      " Zhaopeng Tu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10197",
    "paper_id": "2305.10197",
    "abstract": "\n        We present a machine learning approach for efficiently computing order independent transparency (OIT). Our method is fast, requires a small constant amount of memory (depends only on the screen resolution and not on the number of triangles or transparent layers), is more accurate as compared to previous approximate methods, works for every scene without setup and is portable to all platforms running even with commodity GPUs. Our method requires a rendering pass to extract all features that are subsequently used to predict the overall OIT pixel color with a pre-trained neural network. We provide a comparative experimental evaluation and shader source code of all methods for reproduction of the experiments.\n        \u25b3 Less\n      ",
    "title": "Deep and Fast Approximate Order Independent Transparency",
    "date": "17 May, 2023",
    "authors": [
      "Grigoris Tsopouridis",
      " Andreas-Alexandros Vasilakis",
      " Ioannis Fudos"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10204",
    "paper_id": "2305.10204",
    "abstract": "\n        Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model's representations. However, current methods are restricted to removing only linearly encoded information. In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. We evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. Our results demonstrate that IGBP is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.\n        \u25b3 Less\n      ",
    "title": "Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection",
    "date": "17 May, 2023",
    "authors": [
      "Shadi Iskander",
      " Kira Radinsky",
      " Yonatan Belinkov"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08876",
    "paper_id": "2305.08876",
    "abstract": "\n        Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications.\n        \u25b3 Less\n      ",
    "title": "Neurosymbolic AI and its Taxonomy: a survey",
    "date": "17 May, 2023",
    "authors": [
      "Wandemberg Gibaut",
      " Leonardo Pereira",
      " Fabio Grassiotto",
      " Alexandre Osorio",
      " Eder Gadioli",
      " Amparo Munoz",
      " Sildolfo Gomes",
      " Claudio dos Santos"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10219",
    "paper_id": "2305.10219",
    "abstract": "\n        Support Vector Machine (SVM) is a robust machine learning algorithm with broad applications in classification, regression, and outlier detection. SVM requires tuning the regularization parameter (RP) which controls the model capacity and the generalization performance. Conventionally, the optimum RP is found by comparison of a range of values through the Cross-Validation (CV) procedure. In addition, for non-linearly separable data, the SVM uses kernels where a set of kernels, each with a set of parameters, denoted as a grid of kernels, are considered. The optimal choice of RP and the grid of kernels is through the grid-search of CV. By stochastically analyzing the behavior of the regularization parameter, this work shows that the SVM performance can be modeled as a function of separability and scatteredness (S&S) of the data. Separability is a measure of the distance between classes, and scatteredness is the ratio of the spread of data points. In particular, for the hinge loss cost function, an S&S ratio-based table provides the optimum RP. The S&S ratio is a powerful value that can automatically detect linear or non-linear separability before using the SVM algorithm. The provided S&S ratio-based table can also provide the optimum kernel and its parameters before using the SVM algorithm. Consequently, the computational complexity of the CV grid-search is reduced to only one time use of the SVM. The simulation results on the real dataset confirm the superiority and efficiency of the proposed approach in the sense of computational complexity over the grid-search CV method.\n        \u25b3 Less\n      ",
    "title": "Separability and Scatteredness (S&S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection",
    "date": "17 May, 2023",
    "authors": [
      "Mahdi Shamsi",
      " Soosan Beheshti"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10222",
    "paper_id": "2305.10222",
    "abstract": "\n        Human Activity Recognition (HAR) has become a spotlight in recent scientific research because of its applications in various domains such as healthcare, athletic competitions, smart cities, and smart home. While researchers focus on the methodology of processing data, users wonder if the Artificial Intelligence (AI) methods used for HAR can be trusted. Trust depends mainly on the reliability or robustness of the system. To investigate the robustness of HAR systems, we analyzed several suitable current public datasets and selected WISDM for our investigation of Deep Learning approaches. While the published specification of WISDM matched our fundamental requirements (e.g., large, balanced, multi-hardware), several hidden issues were found in the course of our analysis. These issues reduce the performance and the overall trust of the classifier. By identifying the problems and repairing the dataset, the performance of the classifier was increased. This paper presents the methods by which other researchers may identify and correct similar problems in public datasets. By fixing the issues dataset veracity is improved, which increases the overall trust in the trained HAR system.\n        \u25b3 Less\n      ",
    "title": "rWISDM: Repaired WISDM, a Public Dataset for Human Activity Recognition",
    "date": "17 May, 2023",
    "authors": [
      "Mohammadreza Heydarian",
      " Thomas E. Doyle"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.08264",
    "paper_id": "2302.08264",
    "abstract": "\n        Ethical principles for algorithms are gaining importance as more and more stakeholders are affected by \"high-risk\" algorithmic decision-making (ADM) systems. Understanding how these systems work enables stakeholders to make informed decisions and to assess the systems' adherence to ethical values. Explanations are a promising way to create understanding, but current explainable artificial intelligence (XAI) research does not always consider existent theories on how understanding is formed and evaluated. In this work, we aim to contribute to a better understanding of understanding by conducting a qualitative task-based study with 30 participants, including users and affected stakeholders. We use three explanation modalities (textual, dialogue, and interactive) to explain a \"high-risk\" ADM system to participants and analyse their responses both inductively and deductively, using the \"six facets of understanding\" framework by Wiggins & McTighe. Our findings indicate that the \"six facets\" framework is a promising approach to analyse participants' thought processes in understanding, providing categories for both rational and emotional understanding. We further introduce the \"dialogue\" modality as a valid explanation approach to increase participant engagement and interaction with the \"explainer\", allowing for more insight into their understanding in the process. Our analysis further suggests that individuality in understanding affects participants' perceptions of algorithmic fairness, demonstrating the interdependence between understanding and ADM assessment that previous studies have outlined. We posit that drawing from theories on learning and understanding like the \"six facets\" and leveraging explanation modalities can guide XAI research to better suit explanations to learning processes of individuals and consequently enable their assessment of ethical values of ADM systems.\n        \u25b3 Less\n      ",
    "title": "On the Impact of Explanations on Understanding of Algorithmic Decision-Making",
    "date": "17 May, 2023",
    "authors": [
      "Timoth\u00e9e Schmude",
      " Laura Koesten",
      " Torsten M\u00f6ller",
      " Sebastian Tschiatschek"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.02865",
    "paper_id": "2302.02865",
    "abstract": "\n        Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given query, subject to its uncertainty. Code is available at https://github.com/mkirchhof/Probabilistic_Contrastive_Learning\n        \u25b3 Less\n      ",
    "title": "Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs",
    "date": "17 May, 2023",
    "authors": [
      "Michael Kirchhof",
      " Enkelejda Kasneci",
      " Seong Joon Oh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10254",
    "paper_id": "2305.10254",
    "abstract": "\n        In recent years, the agricultural industry has witnessed significant advancements in artificial intelligence (AI), particularly with the development of large-scale foundational models. Among these foundation models, the Segment Anything Model (SAM), introduced by Meta AI Research, stands out as a groundbreaking solution for object segmentation tasks. While SAM has shown success in various agricultural applications, its potential in the poultry industry, specifically in the context of cage-free hens, remains relatively unexplored. This study aims to assess the zero-shot segmentation performance of SAM on representative chicken segmentation tasks, including part-based segmentation and the use of infrared thermal images, and to explore chicken-tracking tasks by using SAM as a segmentation tool. The results demonstrate SAM's superior performance compared to SegFormer and SETR in both whole and part-based chicken segmentation. SAM-based object tracking also provides valuable data on the behavior and movement patterns of broiler birds. The findings of this study contribute to a better understanding of SAM's potential in poultry science and lay the foundation for future advancements in chicken segmentation and tracking.\n        \u25b3 Less\n      ",
    "title": "SAM for Poultry Science",
    "date": "17 May, 2023",
    "authors": [
      "Xiao Yang",
      " Haixing Dai",
      " Zihao Wu",
      " Ramesh Bist",
      " Sachin Subedi",
      " Jin Sun",
      " Guoyu Lu",
      " Changying Li",
      " Tianming Liu",
      " Lilong Chai"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10257",
    "paper_id": "2305.10257",
    "abstract": "\n        Link prediction problem has increasingly become prominent in many domains such as social network analyses, bioinformatics experiments, transportation networks, criminal investigations and so forth. A variety of techniques has been developed for link prediction problem, categorized into 1) similarity based approaches which study a set of features to extract similar nodes; 2) learning based approaches which extract patterns from the input data; 3) probabilistic statistical approaches which optimize a set of parameters to establish a model which can best compute formation probability. However, existing literatures lack approaches which utilize strength of each approach by integrating them to achieve a much more productive one. To tackle the link prediction problem, we propose an approach based on the combination of first and second group methods; the existing studied works use just one of these categories. Our two-phase developed method firstly determines new features related to the position and dynamic behavior of nodes, which enforce the approach more efficiency compared to approaches using mere measures. Then, a subspace clustering algorithm is applied to group social objects based on the computed similarity measures which differentiate the strength of clusters; basically, the usage of local and global indices and the clustering information plays an imperative role in our link prediction process. Some extensive experiments held on real datasets including Facebook, Brightkite and HepTh indicate good performances of our proposal method. Besides, we have experimentally verified our approach with some previous techniques in the area to prove the supremacy of ours.\n        \u25b3 Less\n      ",
    "title": "Improving Link Prediction in Social Networks Using Local and Global Features: A Clustering-based Approach",
    "date": "17 May, 2023",
    "authors": [
      "Safiye Ghasemi",
      " Amin Zarei"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10284",
    "paper_id": "2305.10284",
    "abstract": "\n        The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate our methods and demonstrate their effectiveness in addressing the challenge of missing system evaluation on an entire task. This work highlights the need for more comprehensive benchmarking approaches that can handle real-world scenarios where not all systems are evaluated on the entire task.\n        \u25b3 Less\n      ",
    "title": "Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks",
    "date": "17 May, 2023",
    "authors": [
      "Anas Himmi",
      " Ekhine Irurozki",
      " Nathan Noiry",
      " Stephan Clemencon",
      " Pierre Colombo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10289",
    "paper_id": "2305.10289",
    "abstract": "\n        EXplainable AI (XAI) is an essential topic to improve human understanding of deep neural networks (DNNs) given their black-box internals. For computer vision tasks, mainstream pixel-based XAI methods explain DNN decisions by identifying important pixels, and emerging concept-based XAI explore forming explanations with concepts (e.g., a head in an image). However, pixels are generally hard to interpret and sensitive to the imprecision of XAI methods, whereas \"concepts\" in prior works require human annotation or are limited to pre-defined concept sets. On the other hand, driven by large-scale pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promotable framework for performing precise and comprehensive instance segmentation, enabling automatic preparation of concept sets from a given image. This paper for the first time explores using SAM to augment concept-based XAI. We offer an effective and flexible concept-based explanation method, namely Explain Any Concept (EAC), which explains DNN decisions with any concept. While SAM is highly effective and offers an \"out-of-the-box\" instance segmentation, it is costly when being integrated into defacto XAI pipelines. We thus propose a lightweight per-input equivalent (PIE) scheme, enabling efficient explanation with a surrogate model. Our evaluation over two popular datasets (ImageNet and COCO) illustrate the highly encouraging performance of EAC over commonly-used XAI methods.\n        \u25b3 Less\n      ",
    "title": "Explain Any Concept: Segment Anything Meets Concept-Based Explanation",
    "date": "17 May, 2023",
    "authors": [
      "Ao Sun",
      " Pingchuan Ma",
      " Yuanyuan Yuan",
      " Shuai Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.10197",
    "paper_id": "2302.10197",
    "abstract": "\n        Neural Cellular Automata (NCA) models have shown remarkable capacity for pattern formation and complex global behaviors stemming from local coordination. However, in the original implementation of NCA, cells are incapable of adjusting their own orientation, and it is the responsibility of the model designer to orient them externally. A recent isotropic variant of NCA (Growing Isotropic Neural Cellular Automata) makes the model orientation-independent - cells can no longer tell up from down, nor left from right - by removing its dependency on perceiving the gradient of spatial states in its neighborhood. In this work, we revisit NCA with a different approach: we make each cell responsible for its own orientation by allowing it to \"turn\" as determined by an adjustable internal state. The resulting Steerable NCA contains cells of varying orientation embedded in the same pattern. We observe how, while Isotropic NCA are orientation-agnostic, Steerable NCA have chirality: they have a predetermined left-right symmetry. We therefore show that we can train Steerable NCA in similar but simpler ways than their Isotropic variant by: (1) breaking symmetries using only two seeds, or (2) introducing a rotation-invariant training objective and relying on asynchronous cell updates to break the up-down symmetry of the system.\n        \u25b3 Less\n      ",
    "title": "Growing Steerable Neural Cellular Automata",
    "date": "17 May, 2023",
    "authors": [
      "Ettore Randazzo",
      " Alexander Mordvintsev",
      " Craig Fouts"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10298",
    "paper_id": "2305.10298",
    "abstract": "\n        Lithium-ion batteries are widely used in various applications, including portable electronic devices, electric vehicles, and renewable energy storage systems. Accurately estimating the remaining useful life of these batteries is crucial for ensuring their optimal performance, preventing unexpected failures, and reducing maintenance costs. In this paper, we present a comprehensive review of the existing approaches for estimating the remaining useful life of lithium-ion batteries, including data-driven methods, physics-based models, and hybrid approaches. We also propose a novel approach based on machine learning techniques for accurately predicting the remaining useful life of lithium-ion batteries. Our approach utilizes various battery performance parameters, including voltage, current, and temperature, to train a predictive model that can accurately estimate the remaining useful life of the battery. We evaluate the performance of our approach on a dataset of lithium-ion battery cycles and compare it with other state-of-the-art methods. The results demonstrate the effectiveness of our proposed approach in accurately estimating the remaining useful life of lithium-ion batteries.\n        \u25b3 Less\n      ",
    "title": "Estimation of Remaining Useful Life and SOH of Lithium Ion Batteries (For EV Vehicles)",
    "date": "17 May, 2023",
    "authors": [
      "Ganesh Kumar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.09081",
    "paper_id": "2207.09081",
    "abstract": "\n        As a pivotal component to attaining generalizable solutions in human intelligence, reasoning provides great potential for reinforcement learning (RL) agents' generalization towards varied goals by summarizing part-to-whole arguments and discovering cause-and-effect relations. However, how to discover and represent causalities remains a huge gap that hinders the development of causal RL. In this paper, we augment Goal-Conditioned RL (GCRL) with Causal Graph (CG), a structure built upon the relation between objects and events. We novelly formulate the GCRL problem into variational likelihood maximization with CG as latent variables. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventional data to estimate the posterior of CG; using CG to learn generalizable models and interpretable policies. Due to the lack of public benchmarks that verify generalization capability under reasoning, we design nine tasks and then empirically show the effectiveness of the proposed method against five baselines on these tasks. Further theoretical analysis shows that our performance improvement is attributed to the virtuous cycle of causal discovery, transition modeling, and policy training, which aligns with the experimental evidence in extensive ablation studies.\n        \u25b3 Less\n      ",
    "title": "Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning",
    "date": "17 May, 2023",
    "authors": [
      "Wenhao Ding",
      " Haohong Lin",
      " Bo Li",
      " Ding Zhao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10349",
    "paper_id": "2305.10349",
    "abstract": "\n        We present a system for interpretable, symbolic, interactive task learning from dialog using a GPT model as a conversational front-end. The learned tasks are represented as hierarchical decompositions of predicate-argument structures with scoped variable arguments. By using a GPT model to convert interactive dialog into a semantic representation, and then recursively asking for definitions of unknown steps, we show that hierarchical task knowledge can be acquired and re-used in a natural and unrestrained conversational environment. We compare our system to a similar architecture using a more conventional parser and show that our system tolerates a much wider variety of linguistic variance.\n        \u25b3 Less\n      ",
    "title": "Interactive Learning of Hierarchical Tasks from Dialog with GPT",
    "date": "17 May, 2023",
    "authors": [
      "Lane Lawley",
      " Christopher J. MacLellan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.02744",
    "paper_id": "2211.02744",
    "abstract": "\n        The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the standard fine-tuning phase sets a new state-of-the-art performance for the link prediction task on the benchmark datasets.\n        \u25b3 Less\n      ",
    "title": "KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction",
    "date": "17 May, 2023",
    "authors": [
      "Jason Youn",
      " Ilias Tagkopoulos"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10378",
    "paper_id": "2305.10378",
    "abstract": "\n        As multi-agent reinforcement learning (MARL) systems are increasingly deployed throughout society, it is imperative yet challenging for users to understand the emergent behaviors of MARL agents in complex environments. This work presents an approach for generating policy-level contrastive explanations for MARL to answer a temporal user query, which specifies a sequence of tasks completed by agents with possible cooperation. The proposed approach encodes the temporal query as a PCTL logic formula and checks if the query is feasible under a given MARL policy via probabilistic model checking. Such explanations can help reconcile discrepancies between the actual and anticipated multi-agent behaviors. The proposed approach also generates correct and complete explanations to pinpoint reasons that make a user query infeasible. We have successfully applied the proposed approach to four benchmark MARL domains (up to 9 agents in one domain). Moreover, the results of a user study show that the generated explanations significantly improve user performance and satisfaction.\n        \u25b3 Less\n      ",
    "title": "Explainable Multi-Agent Reinforcement Learning for Temporal Queries",
    "date": "17 May, 2023",
    "authors": [
      "Kayla Boggess",
      " Sarit Kraus",
      " Lu Feng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08953",
    "paper_id": "2305.08953",
    "abstract": "\n        In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.\n        \u25b3 Less\n      ",
    "title": "Motion Question Answering via Modular Motion Programs",
    "date": "17 May, 2023",
    "authors": [
      "Mark Endo",
      " Joy Hsu",
      " Jiaman Li",
      " Jiajun Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.10517",
    "paper_id": "2304.10517",
    "abstract": "\n        Training segmentation models for medical images continues to be challenging due to the limited availability of data annotations. Segment Anything Model (SAM) is a foundation model that is intended to segment user-defined objects of interest in an interactive manner. While the performance on natural images is impressive, medical image domains pose their own set of challenges. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 19 medical imaging datasets from various modalities and anatomies. We report the following findings: (1) SAM's performance based on single prompts highly varies depending on the dataset and the task, from IoU=0.1135 for spine MRI to IoU=0.8650 for hip X-ray. (2) Segmentation performance appears to be better for well-circumscribed objects with prompts with less ambiguity and poorer in various other scenarios such as the segmentation of brain tumors. (3) SAM performs notably better with box prompts than with point prompts. (4) SAM outperforms similar methods RITM, SimpleClick, and FocalClick in almost all single-point prompt settings. (5) When multiple-point prompts are provided iteratively, SAM's performance generally improves only slightly while other methods' performance improves to the level that surpasses SAM's point-based performance. We also provide several illustrations for SAM's performance on all tested datasets, iterative segmentation, and SAM's behavior given prompt ambiguity. We conclude that SAM shows impressive zero-shot segmentation performance for certain medical imaging datasets, but moderate to poor performance for others. SAM has the potential to make a significant impact in automated medical image segmentation in medical imaging, but appropriate care needs to be applied when using it.\n        \u25b3 Less\n      ",
    "title": "Segment Anything Model for Medical Image Analysis: an Experimental Study",
    "date": "17 May, 2023",
    "authors": [
      "Maciej A. Mazurowski",
      " Haoyu Dong",
      " Hanxue Gu",
      " Jichen Yang",
      " Nicholas Konz",
      " Yixin Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.00808",
    "paper_id": "2302.00808",
    "abstract": "\n        Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other state-of-the-art algorithms adapted for the average CMDP setting.\n        \u25b3 Less\n      ",
    "title": "Average-Constrained Policy Optimization",
    "date": "17 May, 2023",
    "authors": [
      "Akhil Agnihotri",
      " Rahul Jain",
      " Haipeng Luo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10412",
    "paper_id": "2305.10412",
    "abstract": "\n        What role can AI play in supporting and constraining creative coding by families? To investigate these questions, we built a Wizard of Oz platform to help families engage in creative coding in partnership with a researcher-operated AI Friend. We designed a 3 week series of programming activities with ten children, 7 to 12 years old, and nine parents. Using a creative self efficacy lens, we observe that families found it easier to generate game ideas when prompted with questions by AI Friend; parents played a unique role in guiding children in more complex programming tasks when the AI Friend failed to help, and children were more encouraged to write code for novel ideas using the AI friend help. These findings suggest that AI supported platforms should highlight unique family AI interactions focused on children's agency and creative self-efficacy.\n        \u25b3 Less\n      ",
    "title": "AI Friends: A Design Framework for AI-Powered Creative Programming for Youth",
    "date": "17 May, 2023",
    "authors": [
      "Stefania Druga",
      " Amy J. Ko"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10417",
    "paper_id": "2305.10417",
    "abstract": "\n        How can AI enhance creative coding experiences for families? This study explores the potential of large language models (LLMs) in helping families with creative coding using Scratch. Based on our previous user study involving a prototype AI assistant, we devised three evaluation scenarios to determine if LLMs could help families comprehend game code, debug programs, and generate new ideas for future projects. We utilized 22 Scratch projects for each scenario and generated responses from LLMs with and without practice tasks, resulting in 120 creative coding support scenario datasets. In addition, the authors independently evaluated their precision, pedagogical value, and age-appropriate language. Our findings show that LLMs achieved an overall success rate of more than 80\\% on the different tasks and evaluation criteria. This research offers valuable information on using LLMs for creative family coding and presents design guidelines for future AI-supported coding applications. Our evaluation framework, together with our labeled evaluation data, is publicly available.\n        \u25b3 Less\n      ",
    "title": "Scratch Copilot Evaluation: Assessing AI-Assisted Creative Coding for Families",
    "date": "17 May, 2023",
    "authors": [
      "Stefania Druga",
      " Nancy Otero"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10421",
    "paper_id": "2305.10421",
    "abstract": "\n        Du e to rapid population growth and the need to use artificial intelligence to make quick decisions, developing a machine learning-based disease detection model and abnormality identification system has greatly improved the level of medical diagnosis Since COVID-19 has become one of the most severe diseases in the world, developing an automatic COVID-19 detection framework helps medical doctors in the diagnostic process of disease and provides correct and fast results. In this paper, we propose a machine lear ning based framework for the detection of Covid 19. The proposed model employs a Tsukamoto Neuro Fuzzy Inference network to identify and distinguish Covid 19 disease from normal and pneumonia cases. While the traditional training methods tune the parameters of the neuro-fuzzy model by gradient-based algorithms and recursive least square method, we use an evolutionary-based optimization, the Cat swarm algorithm to update the parameters. In addition, six texture features extracted from chest X-ray images are give n as input to the model. Finally, the proposed model is conducted on the chest X-ray dataset to detect Covid 19. The simulation results indicate that the proposed model achieves an accuracy of 98.51%, sensitivity of 98.35%, specificity of 98.08%, and F1 score of 98.17%.\n        \u25b3 Less\n      ",
    "title": "Evolving Tsukamoto Neuro Fuzzy Model for Multiclass Covid 19 Classification with Chest X Ray Images",
    "date": "17 May, 2023",
    "authors": [
      "Marziyeh Rezaei",
      " Sevda Molani",
      " Negar Firoozeh",
      " Hossein Abbasi",
      " Farzan Vahedifard",
      " Maysam Orouskhani"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10425",
    "paper_id": "2305.10425",
    "abstract": "\n        Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.\n        \u25b3 Less\n      ",
    "title": "SLiC-HF: Sequence Likelihood Calibration with Human Feedback",
    "date": "17 May, 2023",
    "authors": [
      "Yao Zhao",
      " Rishabh Joshi",
      " Tianqi Liu",
      " Misha Khalman",
      " Mohammad Saleh",
      " Peter J. Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10427",
    "paper_id": "2305.10427",
    "abstract": "\n        Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling the method on parallel resources. Finally, we introduce a decoding dependency graph visualizer (DDGviz) that let us see how the model has learned the conditional dependence between tokens and inspect the decoding procedure.\n        \u25b3 Less\n      ",
    "title": "Accelerating Transformer Inference for Translation via Parallel Decoding",
    "date": "17 May, 2023",
    "authors": [
      "Andrea Santilli",
      " Silvio Severino",
      " Emilian Postolache",
      " Valentino Maiorca",
      " Michele Mancusi",
      " Riccardo Marin",
      " Emanuele Rodol\u00e0"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10496",
    "paper_id": "2305.10496",
    "abstract": "\n        Feature attribution methods (FAs) are popular approaches for providing insights into the model reasoning process of making predictions. The more faithful a FA is, the more accurately it reflects which parts of the input are more important for the prediction. Widely used faithfulness metrics, such as sufficiency and comprehensiveness use a hard erasure criterion, i.e. entirely removing or retaining the top most important tokens ranked by a given FA and observing the changes in predictive likelihood. However, this hard criterion ignores the importance of each individual token, treating them all equally for computing sufficiency and comprehensiveness. In this paper, we propose a simple yet effective soft erasure criterion. Instead of entirely removing or retaining tokens from the input, we randomly mask parts of the token vector representations proportionately to their FA importance. Extensive experiments across various natural language processing tasks and different FAs show that our soft-sufficiency and soft-comprehensiveness metrics consistently prefer more faithful explanations compared to hard sufficiency and comprehensiveness. Our code: https://github.com/casszhao/SoftFaith\n        \u25b3 Less\n      ",
    "title": "Incorporating Attribution Importance for Improving Faithfulness Metrics",
    "date": "17 May, 2023",
    "authors": [
      "Zhixue Zhao",
      " Nikolaos Aletras"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.10438",
    "paper_id": "2209.10438",
    "abstract": "\n        In neural networks, task-relevant information is represented jointly by groups of neurons. However, the specific way in which this mutual information about the classification label is distributed among the individual neurons is not well understood: While parts of it may only be obtainable from specific single neurons, other parts are carried redundantly or synergistically by multiple neurons. We show how Partial Information Decomposition (PID), a recent extension of information theory, can disentangle these different contributions. From this, we introduce the measure of \"Representational Complexity\", which quantifies the difficulty of accessing information spread across multiple neurons. We show how this complexity is directly computable for smaller layers. For larger layers, we propose subsampling and coarse-graining procedures and prove corresponding bounds on the latter. Empirically, for quantized deep neural networks solving the MNIST and CIFAR10 tasks, we observe that representational complexity decreases both through successive hidden layers and over training, and compare the results to related measures. Overall, we propose representational complexity as a principled and interpretable summary statistic for analyzing the structure and evolution of neural representations and complex systems in general.\n        \u25b3 Less\n      ",
    "title": "A Measure of the Complexity of Neural Representations based on Partial Information Decomposition",
    "date": "17 May, 2023",
    "authors": [
      "David A. Ehrlich",
      " Andreas C. Schneider",
      " Viola Priesemann",
      " Michael Wibral",
      " Abdullah Makkeh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10504",
    "paper_id": "2305.10504",
    "abstract": "\n        Robust Markov decision processes (MDPs) address the challenge of model uncertainty by optimizing the worst-case performance over an uncertainty set of MDPs. In this paper, we focus on the robust average-reward MDPs under the model-free setting. We first theoretically characterize the structure of solutions to the robust average-reward Bellman equation, which is essential for our later convergence analysis. We then design two model-free algorithms, robust relative value iteration (RVI) TD and robust RVI Q-learning, and theoretically prove their convergence to the optimal solution. We provide several widely used uncertainty sets as examples, including those defined by the contamination model, total variation, Chi-squared divergence, Kullback-Leibler (KL) divergence and Wasserstein distance.\n        \u25b3 Less\n      ",
    "title": "Model-Free Robust Average-Reward Reinforcement Learning",
    "date": "17 May, 2023",
    "authors": [
      "Yue Wang",
      " Alvaro Velasquez",
      " George Atia",
      " Ashley Prater-Bennette",
      " Shaofeng Zou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10507",
    "paper_id": "2305.10507",
    "abstract": "\n        The large-scale deployment of autonomous vehicles is yet to come, and one of the major remaining challenges lies in urban dense traffic scenarios. In such cases, it remains challenging to predict the future evolution of the scene and future behaviors of objects, and to deal with rare adverse events such as the sudden appearance of occluded objects. In this paper, we present ReasonNet, a novel end-to-end driving framework that extensively exploits both temporal and global information of the driving scene. By reasoning on the temporal behavior of objects, our method can effectively process the interactions and relationships among features in different frames. Reasoning about the global information of the scene can also improve overall perception performance and benefit the detection of adverse events, especially the anticipation of potential danger from occluded objects. For comprehensive evaluation on occlusion events, we also release publicly a driving simulation benchmark DriveOcclusionSim consisting of diverse occlusion events. We conduct extensive experiments on multiple CARLA benchmarks, where our model outperforms all prior methods, ranking first on the sensor track of the public CARLA Leaderboard.\n        \u25b3 Less\n      ",
    "title": "ReasonNet: End-to-End Driving with Temporal and Global Reasoning",
    "date": "17 May, 2023",
    "authors": [
      "Hao Shao",
      " Letian Wang",
      " Ruobing Chen",
      " Steven L. Waslander",
      " Hongsheng Li",
      " Yu Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10510",
    "paper_id": "2305.10510",
    "abstract": "\n        In this multicultural age, language translation is one of the most performed tasks, and it is becoming increasingly AI-moderated and automated. As a novel AI system, ChatGPT claims to be proficient in such translation tasks and in this paper, we put that claim to the test. Specifically, we examine ChatGPT's accuracy in translating between English and languages that exclusively use gender-neutral pronouns. We center this study around Bengali, the 7^{th}^{th} most spoken language globally, but also generalize our findings across five other languages: Farsi, Malay, Tagalog, Thai, and Turkish. We find that ChatGPT perpetuates gender defaults and stereotypes assigned to certain occupations (e.g. man = doctor, woman = nurse) or actions (e.g. woman = cook, man = go to work), as it converts gender-neutral pronouns in languages to `he' or `she'. We also observe ChatGPT completely failing to translate the English gender-neutral pronoun `they' into equivalent gender-neutral pronouns in other languages, as it produces translations that are incoherent and incorrect. While it does respect and provide appropriately gender-marked versions of Bengali words when prompted with gender information in English, ChatGPT appears to confer a higher respect to men than to women in the same occupation. We conclude that ChatGPT exhibits the same gender biases which have been demonstrated for tools like Google Translate or MS Translator, as we provide recommendations for a human centered approach for future designers of AIs that perform language translation to better accommodate such low-resource languages.\n        \u25b3 Less\n      ",
    "title": "ChatGPT Perpetuates Gender Bias in Machine Translation and Ignores Non-Gendered Pronouns: Findings across Bengali and Five other Low-Resource Languages",
    "date": "17 May, 2023",
    "authors": [
      "Sourojit Ghosh",
      " Aylin Caliskan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.00477",
    "paper_id": "2305.00477",
    "abstract": "\n        Despite remarkable successes, deep reinforcement learning algorithms remain sample inefficient: they require an enormous amount of trial and error to find good policies. Model-based algorithms promise sample efficiency by building an environment model that can be used for planning. Posterior Sampling for Reinforcement Learning is such a model-based algorithm that has attracted significant interest due to its performance in the tabular setting. This paper introduces Posterior Sampling for Deep Reinforcement Learning (PSDRL), the first truly scalable approximation of Posterior Sampling for Reinforcement Learning that retains its model-based essence. PSDRL combines efficient uncertainty quantification over latent state space models with a specially tailored continual planning algorithm based on value-function approximation. Extensive experiments on the Atari benchmark show that PSDRL significantly outperforms previous state-of-the-art attempts at scaling up posterior sampling while being competitive with a state-of-the-art (model-based) reinforcement learning method, both in sample efficiency and computational efficiency.\n        \u25b3 Less\n      ",
    "title": "Posterior Sampling for Deep Reinforcement Learning",
    "date": "17 May, 2023",
    "authors": [
      "Remo Sasso",
      " Michelangelo Conserva",
      " Paulo Rauber"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2201.02018",
    "paper_id": "2201.02018",
    "abstract": "\n        The notion of reparametrizations of Weighted CSPs (WCSPs) (also known as equivalence-preserving transformations of WCSPs) is well-known and finds its use in many algorithms to approximate or bound the optimal WCSP value. In contrast, the concept of super-reparametrizations (which are changes of the weights that keep or increase the WCSP objective for every assignment) was already proposed but never studied in detail. To fill this gap, we present a number of theoretical properties of super-reparametrizations and compare them to those of reparametrizations. Furthermore, we propose a framework for computing upper bounds on the optimal value of the (maximization version of) WCSP using super-reparametrizations. We show that it is in principle possible to employ arbitrary (under some technical conditions) constraint propagation rules to improve the bound. For arc consistency in particular, the method reduces to the known Virtual AC (VAC) algorithm. We implemented the method for singleton arc consistency (SAC) and compared it to other strong local consistencies in WCSPs on a public benchmark. The results show that the bounds obtained from SAC are superior for many instance groups.\n        \u25b3 Less\n      ",
    "title": "Super-Reparametrizations of Weighted CSPs: Properties and Optimization Perspective",
    "date": "17 May, 2023",
    "authors": [
      "Tom\u00e1\u0161 Dlask",
      " Tom\u00e1\u0161 Werner",
      " Simon de Givry"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10528",
    "paper_id": "2305.10528",
    "abstract": "\n        Off-Policy reinforcement learning has been a driving force for the state-of-the-art conversational AIs leading to more natural humanagent interactions and improving the user satisfaction for goal-oriented agents. However, in large-scale commercial settings, it is often challenging to balance between policy improvements and experience continuity on the broad spectrum of applications handled by such system. In the literature, off-policy evaluation and guard-railing on aggregate statistics has been commonly used to address this problem. In this paper, we propose a method for curating and leveraging high-precision samples sourced from historical regression incident reports to validate, safe-guard, and improve policies prior to the online deployment. We conducted extensive experiments using data from a real-world conversational system and actual regression incidents. The proposed method is currently deployed in our production system to protect customers against broken experiences and enable long-term policy improvements.\n        \u25b3 Less\n      ",
    "title": "Scalable and Safe Remediation of Defective Actions in Self-Learning Conversational Systems",
    "date": "17 May, 2023",
    "authors": [
      "Sarthak Ahuja",
      " Mohammad Kachuee",
      " Fateme Sheikholeslami",
      " Weiqing Liu",
      " Jaeyoung Do"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10538",
    "paper_id": "2305.10538",
    "abstract": "\n        Bayesian networks (BN) are directed acyclic graphical (DAG) models that have been adopted into many fields for their strengths in transparency, interpretability, probabilistic reasoning, and causal modeling. Given a set of data, one hurdle towards using BNs is in building the network graph from the data that properly handles dependencies, whether correlated or causal. In this paper, we propose an initial methodology for discovering network structures using Tsetlin Machines.\n        \u25b3 Less\n      ",
    "title": "Generating Bayesian Network Models from Data Using Tsetlin Machines",
    "date": "17 May, 2023",
    "authors": [
      "Christian D. Blakely"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2307.11688",
    "paper_id": "2307.11688",
    "abstract": "\n        The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA) -- one of the fields laying the foundations of modern mathematics -- is still completely unexplored. This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhance interpretability without sacrificing task accuracy, (ii) strongly generalize when predicting universal algebra's properties, (iii) generate simple explanations that empirically validate existing conjectures, and (iv) identify subgraphs suggesting the formulation of novel conjectures.\n        \u25b3 Less\n      ",
    "title": "Interpretable Graph Networks Formulate Universal Algebra Conjectures",
    "date": "17 May, 2023",
    "authors": [
      "Francesco Giannini",
      " Stefano Fioravanti",
      " Oguzhan Keskin",
      " Alisia Maria Lupidi",
      " Lucie Charlotte Magister",
      " Pietro Lio",
      " Pietro Barbiero"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10548",
    "paper_id": "2305.10548",
    "abstract": "\n        The discovery of individual objectives in collective behavior of complex dynamical systems such as fish schools and bacteria colonies is a long-standing challenge. Inverse reinforcement learning is a potent approach for addressing this challenge but its applicability to dynamical systems, involving continuous state-action spaces and multiple interacting agents, has been limited. In this study, we tackle this challenge by introducing an off-policy inverse multi-agent reinforcement learning algorithm (IMARL). Our approach combines the ReF-ER techniques with guided cost learning. By leveraging demonstrations, our algorithm automatically uncovers the reward function and learns an effective policy for the agents. Through extensive experimentation, we demonstrate that the proposed policy captures the behavior observed in the provided data, and achieves promising results across problem domains including single agent models in the OpenAI gym and multi-agent models of schooling behavior. The present study shows that the proposed IMARL algorithm is a significant step towards understanding collective dynamics from the perspective of its constituents, and showcases its value as a tool for studying complex physical systems exhibiting collective behaviour.\n        \u25b3 Less\n      ",
    "title": "Discovering Individual Rewards in Collective Behavior through Inverse Multi-Agent Reinforcement Learning",
    "date": "17 May, 2023",
    "authors": [
      "Daniel Waelchli",
      " Pascal Weber",
      " Petros Koumoutsakos"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.11879",
    "paper_id": "2301.11879",
    "abstract": "\n        The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewer retrieved cases, and that the size of the case database has a negligible effect on the performance. Finally, we dive deeper into the relationship between the properties of the retrieved cases and the model performance.\n        \u25b3 Less\n      ",
    "title": "Case-Based Reasoning with Language Models for Classification of Logical Fallacies",
    "date": "17 May, 2023",
    "authors": [
      "Zhivar Sourati",
      " Filip Ilievski",
      " H\u00f4ng-\u00c2n Sandlin",
      " Alain Mermoud"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10556",
    "paper_id": "2305.10556",
    "abstract": "\n        Urban air mobility (UAM) has the potential to revolutionize our daily transportation, offering rapid and efficient deliveries of passengers and cargo between dedicated locations within and around the urban environment. Before the commercialization and adoption of this emerging transportation mode, however, aviation safety must be guaranteed, i.e., all the aircraft have to be safely separated by strategic and tactical deconfliction. Reinforcement learning has demonstrated effectiveness in the tactical deconfliction of en route commercial air traffic in simulation. However, its performance is found to be dependent on the traffic density. In this project, we propose a novel framework that combines demand capacity balancing (DCB) for strategic conflict management and reinforcement learning for tactical separation. By using DCB to precondition traffic to proper density levels, we show that reinforcement learning can achieve much better performance for tactical safety separation. Our results also indicate that this DCB preconditioning can allow target levels of safety to be met that are otherwise impossible. In addition, combining strategic DCB with reinforcement learning for tactical separation can meet these safety levels while achieving greater operational efficiency than alternative solutions.\n        \u25b3 Less\n      ",
    "title": "Integrated Conflict Management for UAM with Strategic Demand Capacity Balancing and Learning-based Tactical Deconfliction",
    "date": "17 May, 2023",
    "authors": [
      "Shulu Chen",
      " Antony Evans",
      " Marc Brittain",
      " Peng Wei"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10566",
    "paper_id": "2305.10566",
    "abstract": "\n        Generative AI models like DALL-E 2 can interpret textual prompts and generate high-quality images exhibiting human creativity. Though public enthusiasm is booming, systematic auditing of potential gender biases in AI-generated images remains scarce. We addressed this gap by examining the prevalence of two occupational gender biases (representational and presentational biases) in 15,300 DALL-E 2 images spanning 153 occupations, and assessed potential bias amplification by benchmarking against 2021 census labor statistics and Google Images. Our findings reveal that DALL-E 2 underrepresents women in male-dominated fields while overrepresenting them in female-dominated occupations. Additionally, DALL-E 2 images tend to depict more women than men with smiling faces and downward-pitching heads, particularly in female-dominated (vs. male-dominated) occupations. Our computational algorithm auditing study demonstrates more pronounced representational and presentational biases in DALL-E 2 compared to Google Images and calls for feminist interventions to prevent such bias-laden AI-generated images to feedback into the media ecology.\n        \u25b3 Less\n      ",
    "title": "Smiling Women Pitching Down: Auditing Representational and Presentational Gender Biases in Image Generative AI",
    "date": "17 May, 2023",
    "authors": [
      "Luhang Sun",
      " Mian Wei",
      " Yibing Sun",
      " Yoo Ji Suh",
      " Liwei Shen",
      " Sijia Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2107.08924",
    "paper_id": "2107.08924",
    "abstract": "\n        Intelligence relies on an agent's knowledge of what it does not know. This capability can be assessed based on the quality of joint predictions of labels across multiple inputs. In principle, ensemble-based approaches produce effective joint predictions, but the computational costs of training large ensembles can become prohibitive. We introduce the epinet: an architecture that can supplement any conventional neural network, including large pretrained models, and can be trained with modest incremental computation to estimate uncertainty. With an epinet, conventional neural networks outperform very large ensembles, consisting of hundreds or more particles, with orders of magnitude less computation. The epinet does not fit the traditional framework of Bayesian neural networks. To accommodate development of approaches beyond BNNs, such as the epinet, we introduce the epistemic neural network (ENN) as an interface for models that produce joint predictions.\n        \u25b3 Less\n      ",
    "title": "Epistemic Neural Networks",
    "date": "17 May, 2023",
    "authors": [
      "Ian Osband",
      " Zheng Wen",
      " Seyed Mohammad Asghari",
      " Vikranth Dwaracherla",
      " Morteza Ibrahimi",
      " Xiuyuan Lu",
      " Benjamin Van Roy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10580",
    "paper_id": "2305.10580",
    "abstract": "\n        In this paper, we present Sim-MEES: a large-scale synthetic dataset that contains 1,550 objects with varying difficulty levels and physics properties, as well as 11 million grasp labels for mobile manipulators to plan grasps using different gripper modalities in cluttered environments. Our dataset generation process combines analytic models and dynamic simulations of the entire cluttered environment to provide accurate grasp labels. We provide a detailed study of our proposed labeling process for both parallel jaw grippers and suction cup grippers, comparing them with state-of-the-art methods to demonstrate how Sim-MEES can provide precise grasp labels in cluttered environments.\n        \u25b3 Less\n      ",
    "title": "Sim-MEES: Modular End-Effector System Grasping Dataset for Mobile Manipulators in Cluttered Environments",
    "date": "17 May, 2023",
    "authors": [
      "Juncheng Li",
      " David J. Cappelleri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.03212",
    "paper_id": "2305.03212",
    "abstract": "\n        Trained on a vast amount of data, Large Language models (LLMs) have achieved unprecedented success and generalization in modeling fairly complex textual inputs in the abstract space, making them powerful tools for zero-shot learning. Such capability is extended to other modalities such as the visual domain using cross-modal foundation models such as CLIP, and as a result, semantically meaningful representation are extractable from visual inputs.\n  In this work, we leverage this capability and propose an approach that can provide semantic insights into a model's patterns of failures and biases. Given a black box model, its training data, and task definition, we first calculate its task-related loss for each data point. We then extract a semantically meaningful representation for each training data point (such as CLIP embeddings from its visual encoder) and train a lightweight diagnosis model which maps this semantically meaningful representation of a data point to its task loss. We show that an ensemble of such lightweight models can be used to generate insights on the performance of the black-box model, in terms of identifying its patterns of failures and biases.\n        \u25b3 Less\n      ",
    "title": "LLM2Loss: Leveraging Language Models for Explainable Model Diagnostics",
    "date": "17 May, 2023",
    "authors": [
      "Shervin Ardeshir"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.01890",
    "paper_id": "2304.01890",
    "abstract": "\n        We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot performance.\n        \u25b3 Less\n      ",
    "title": "Sociocultural knowledge is needed for selection of shots in hate speech detection tasks",
    "date": "17 May, 2023",
    "authors": [
      "Antonis Maronikolakis",
      " Abdullatif K\u00f6ksal",
      " Hinrich Sch\u00fctze"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06137",
    "paper_id": "2305.06137",
    "abstract": "\n        We show the convergence of Wasserstein inverse reinforcement learning for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the multi-objective optimization problem. In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guided cost learning) with gradient descent and the projective subgradient method.\n        \u25b3 Less\n      ",
    "title": "A proof of convergence of inverse reinforcement learning for multi-objective optimization",
    "date": "17 May, 2023",
    "authors": [
      "Akira Kitaoka",
      " Riki Eto"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10089",
    "paper_id": "2305.10089",
    "abstract": "\n        We prove Wasserstein inverse reinforcement learning enables the learner's reward values to imitate the expert's reward values in a finite iteration for multi-objective optimizations. Moreover, we prove Wasserstein inverse reinforcement learning enables the learner's optimal solutions to imitate the expert's optimal solutions for multi-objective optimizations with lexicographic order.\n        \u25b3 Less\n      ",
    "title": "A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization",
    "date": "17 May, 2023",
    "authors": [
      "Akira Kitaoka",
      " Riki Eto"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2101.10229",
    "paper_id": "2101.10229",
    "abstract": "\n        We prove a universal approximation property (UAP) for a class of ODENet and a class of ResNet, which are simplified mathematical models for deep learning systems with skip connections. The UAP can be stated as follows. Let nn and mm be the dimension of input and output data, and assume m\u2264nm\\leq n. Then we show that ODENet of width n+mn+m with any non-polynomial continuous activation function can approximate any continuous function on a compact subset on Rn\\mathbb{R}^n. We also show that ResNet has the same property as the depth tends to infinity. Furthermore, we derive the gradient of a loss function explicitly with respect to a certain tuning variable. We use this to construct a learning algorithm for ODENet. To demonstrate the usefulness of this algorithm, we apply it to a regression problem, a binary classification, and a multinomial classification in MNIST.\n        \u25b3 Less\n      ",
    "title": "Universal Approximation Properties for an ODENet and a ResNet: Mathematical Analysis and Numerical Experiments",
    "date": "17 May, 2023",
    "authors": [
      "Yuto Aizawa",
      " Masato Kimura",
      " Kazunori Matsui"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.05668",
    "paper_id": "2303.05668",
    "abstract": "\n        In this paper, we introduce UnFuSeD, a novel approach to leverage self-supervised learning and reduce the need for large amounts of labeled data for audio classification. Unlike prior works, which directly fine-tune a self-supervised pre-trained encoder on a target dataset, we use the encoder to generate pseudo-labels for unsupervised fine-tuning before the actual fine-tuning step. We first train an encoder using a novel self-supervised learning algorithm (SSL) on an unlabeled audio dataset. Then, we use that encoder to generate pseudo-labels on our target task dataset via clustering the extracted representations. These pseudo-labels are then used to guide self-distillation on a randomly initialized model, which we call unsupervised fine-tuning. Finally, the resultant encoder is then fine-tuned on our target task dataset. Through UnFuSeD, we propose the first system that moves away from generic SSL paradigms in literature, which pre-train and fine-tune the same encoder, and present a novel self-distillation-based system to leverage SSL pre-training for low-resource audio classification. In practice, UnFuSeD achieves state-of-the-art results on the LAPE Benchmark, significantly outperforming all our baselines. Additionally, UnFuSeD allows us to achieve this at a 40% reduction in the number of parameters over the previous state-of-the-art system. We make all our codes publicly available.\n        \u25b3 Less\n      ",
    "title": "UNFUSED: UNsupervised Finetuning Using SElf supervised Distillation",
    "date": "17 May, 2023",
    "authors": [
      "Ashish Seth",
      " Sreyan Ghosh",
      " S. Umesh",
      " Dinesh Manocha"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.01519",
    "paper_id": "2211.01519",
    "abstract": "\n        We present a new Self-Supervised Learning (SSL) approach to pre-train encoders on unlabeled audio data that reduces the need for large amounts of labeled data for audio and speech classification. Our primary aim is to learn audio representations that can generalize across a large variety of speech and non-speech tasks in a low-resource un-labeled audio pre-training setting. Inspired by the recent success of clustering and contrasting learning paradigms for SSL-based speech representation learning, we propose SLICER (Symmetrical Learning of Instance and Cluster-level Efficient Representations), which brings together the best of both clustering and contrasting learning paradigms. We use a symmetric loss between latent representations from student and teacher encoders and simultaneously solve instance and cluster-level contrastive learning tasks. We obtain cluster representations online by just projecting the input spectrogram into an output subspace with dimensions equal to the number of clusters. In addition, we propose a novel mel-spectrogram augmentation procedure, k-mix, based on mixup, which does not require labels and aids unsupervised representation learning for audio. Overall, SLICER achieves state-of-the-art results on the LAPE Benchmark \\cite{9868132}, significantly outperforming DeLoRes-M and other prior approaches, which are pre-trained on 10\u00d710\\times larger of unsupervised data. We will make all our codes available on GitHub.\n        \u25b3 Less\n      ",
    "title": "SLICER: Learning universal audio representations using low-resource self-supervised pre-training",
    "date": "17 May, 2023",
    "authors": [
      "Ashish Seth",
      " Sreyan Ghosh",
      " S. Umesh",
      " Dinesh Manocha"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.01515",
    "paper_id": "2211.01515",
    "abstract": "\n        We present Multiscale Audio Spectrogram Transformer (MAST) for audio classification, which brings the concept of multiscale feature hierarchies to the Audio Spectrogram Transformer (AST). Given an input audio spectrogram, we first patchify and project it into an initial temporal resolution and embedding dimension, post which the multiple stages in MAST progressively expand the embedding dimension while reducing the temporal resolution of the input. We use a pyramid structure that allows early layers of MAST operating at a high temporal resolution but low embedding space to model simple low-level acoustic information and deeper temporally coarse layers to model high-level acoustic information with high-dimensional embeddings. We also extend our approach to present a new Self-Supervised Learning (SSL) method called SS-MAST, which calculates a symmetric contrastive loss between latent representations from a student and a teacher encoder, leveraging patch-drop, a novel audio augmentation approach that we introduce. In practice, MAST significantly outperforms AST by an average accuracy of 3.4% across 8 speech and non-speech tasks from the LAPE Benchmark, achieving state-of-the-art results on keyword spotting in Speech Commands. Additionally, our proposed SS-MAST achieves an absolute average improvement of 2.6% over the previously proposed SSAST.\n        \u25b3 Less\n      ",
    "title": "MAST: Multiscale Audio Spectrogram Transformers",
    "date": "17 May, 2023",
    "authors": [
      "Sreyan Ghosh",
      " Ashish Seth",
      " S. Umesh",
      " Dinesh Manocha"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10646",
    "paper_id": "2305.10646",
    "abstract": "\n        Large language models, e.g. ChatGPT are currently contributing enormously to make artificial intelligence even more popular, especially among the general population. However, such chatbot models were developed as tools to support natural language communication between humans. Problematically, it is very much a ``statistical correlation machine\" (correlation instead of causality) and there are indeed ethical concerns associated with the use of AI language models such as ChatGPT, such as Bias, Privacy, and Abuse. This paper highlights specific ethical concerns on ChatGPT and articulates key challenges when ChatGPT is used in various applications. Practical commandments for different stakeholders of ChatGPT are also proposed that can serve as checklist guidelines for those applying ChatGPT in their applications. These commandment examples are expected to motivate the ethical use of ChatGPT.\n        \u25b3 Less\n      ",
    "title": "Ethical ChatGPT: Concerns, Challenges, and Commandments",
    "date": "17 May, 2023",
    "authors": [
      "Jianlong Zhou",
      " Heimo M\u00fcller",
      " Andreas Holzinger",
      " Fang Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10647",
    "paper_id": "2305.10647",
    "abstract": "\n        Biomedical Named Entity Recognition (BioNER) is the fundamental task of identifying named entities from biomedical text. However, BioNER suffers from severe data scarcity and lacks high-quality labeled data due to the highly specialized and expert knowledge required for annotation. Though data augmentation has shown to be highly effective for low-resource NER in general, existing data augmentation techniques fail to produce factual and diverse augmentations for BioNER. In this paper, we present BioAug, a novel data augmentation framework for low-resource BioNER. BioAug, built on BART, is trained to solve a novel text reconstruction task based on selective masking and knowledge augmentation. Post training, we perform conditional generation and generate diverse augmentations conditioning BioAug on selectively corrupted text similar to the training stage. We demonstrate the effectiveness of BioAug on 5 benchmark BioNER datasets and show that BioAug outperforms all our baselines by a significant margin (1.5%-21.5% absolute improvement) and is able to generate augmentations that are both more factual and diverse. Code: https://github.com/Sreyan88/BioAug.\n        \u25b3 Less\n      ",
    "title": "BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER",
    "date": "17 May, 2023",
    "authors": [
      "Sreyan Ghosh",
      " Utkarsh Tyagi",
      " Sonal Kumar",
      " Dinesh Manocha"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10648",
    "paper_id": "2305.10648",
    "abstract": "\n        It is not uncommon that real-world data are distributed with a long tail. For such data, the learning of deep neural networks becomes challenging because it is hard to classify tail classes correctly. In the literature, several existing methods have addressed this problem by reducing classifier bias provided that the features obtained with long-tailed data are representative enough. However, we find that training directly on long-tailed data leads to uneven embedding space. That is, the embedding space of head classes severely compresses that of tail classes, which is not conducive to subsequent classifier learning. %further improving model performance. This paper therefore studies the problem of long-tailed visual recognition from the perspective of feature level. We introduce feature augmentation to balance the embedding distribution. The features of different classes are perturbed with varying amplitudes in Gaussian form. Based on these perturbed features, two novel logit adjustment methods are proposed to improve model performance at a modest computational overhead. Subsequently, the distorted embedding spaces of all classes can be calibrated. In such balanced-distributed embedding spaces, the biased classifier can be eliminated by simply retraining the classifier with class-balanced sampling data. Extensive experiments conducted on benchmark datasets demonstrate the superior performance of the proposed method over the state-of-the-art ones.\n        \u25b3 Less\n      ",
    "title": "Adjusting Logit in Gaussian Form for Long-Tailed Visual Recognition",
    "date": "17 May, 2023",
    "authors": [
      "Mengke Li",
      " Yiu-ming Cheung",
      " Yang Lu",
      " Zhikai Hu",
      " Weichao Lan",
      " Hui Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.02061",
    "paper_id": "2302.02061",
    "abstract": "\n        We introduce Dynamic Contextual Markov Decision Processes (DCMDPs), a novel reinforcement learning framework for history-dependent environments that generalizes the contextual MDP framework to handle non-Markov environments, where contexts change over time. We consider special cases of the model, with a focus on logistic DCMDPs, which break the exponential dependence on history length by leveraging aggregation functions to determine context transitions. This special structure allows us to derive an upper-confidence-bound style algorithm for which we establish regret bounds. Motivated by our theoretical results, we introduce a practical model-based algorithm for logistic DCMDPs that plans in a latent space and uses optimism over history-dependent features. We demonstrate the efficacy of our approach on a recommendation task (using MovieLens data) where user behavior dynamics evolve in response to recommendations.\n        \u25b3 Less\n      ",
    "title": "Reinforcement Learning with History-Dependent Dynamic Contexts",
    "date": "17 May, 2023",
    "authors": [
      "Guy Tennenholtz",
      " Nadav Merlis",
      " Lior Shani",
      " Martin Mladenov",
      " Craig Boutilier"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09860",
    "paper_id": "2305.09860",
    "abstract": "\n        Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested sampling methods across four language pairs.\n        \u25b3 Less\n      ",
    "title": "Epsilon Sampling Rocks: Investigating Sampling Strategies for Minimum Bayes Risk Decoding for Machine Translation",
    "date": "17 May, 2023",
    "authors": [
      "Markus Freitag",
      " Behrooz Ghorbani",
      " Patrick Fernandes"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10654",
    "paper_id": "2305.10654",
    "abstract": "\n        There have been increasing challenges to dual-system descriptions of System-1 and System-2, critiquing them as imprecise and fostering misconceptions. We address these issues here by way of Dennett's appeal to use computational thinking as an analytical tool, specifically we employ the Common Model of Cognition. Results show that the characteristics thought to be distinctive of System-1 and System-2 instead form a spectrum of cognitive properties. By grounding System-1 and System-2 in the Common Model we aim to clarify their underlying mechanisms, persisting misconceptions, and implications for metacognition.\n        \u25b3 Less\n      ",
    "title": "Clarifying System 1 & 2 through the Common Model of Cognition",
    "date": "17 May, 2023",
    "authors": [
      "Brendan Conway-Smith",
      " Robert L. West"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11189",
    "paper_id": "2305.11189",
    "abstract": "\n        Artificial Intelligence (AI) is playing a vital role in all aspects of technology including cyber security. Application of Conversational AI like the chatbots are also becoming very popular in the medical field to provide timely and immediate medical assistance to patients in need. As medical chatbots deal with a lot of sensitive information, the security of these chatbots is crucial. To secure the confidentiality, integrity, and availability of cloud-hosted assets like these, medical chatbots can be monitored using AISecOps (Artificial Intelligence for Secure IT Operations). AISecOPs is an emerging field that integrates three different but interrelated domains like the IT operation, AI, and security as one domain, where the expertise from all these three domains are used cohesively to secure the cyber assets. It considers cloud operations and security in a holistic framework to collect the metrics required to assess the security threats and train the AI models to take immediate actions. This work is focused on applying the STRIDE threat modeling framework to model the possible threats involved in each component of the chatbot to enable the automatic threat detection using the AISecOps techniques. This threat modeling framework is tailored to the medical chatbots that involves sensitive data sharing but could also be applied for chatbots used in other sectors like the financial services, public sector, and government sectors that are concerned with security and compliance.\n        \u25b3 Less\n      ",
    "title": "Taxonomy of AISecOps Threat Modeling for Cloud Based Medical Chatbots",
    "date": "17 May, 2023",
    "authors": [
      "Ruby Annette J",
      " Aisha Banu",
      " Sharon Priya S",
      " Subash Chandran"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10659",
    "paper_id": "2305.10659",
    "abstract": "\n        A key challenge in dysarthric speech recognition is the speaker-level diversity attributed to both speaker-identity associated factors such as gender, and speech impairment severity. Most prior researches on addressing this issue focused on using speaker-identity only. To this end, this paper proposes a novel set of techniques to use both severity and speaker-identity in dysarthric speech recognition: a) multitask training incorporating severity prediction error; b) speaker-severity aware auxiliary feature adaptation; and c) structured LHUC transforms separately conditioned on speaker-identity and severity. Experiments conducted on UASpeech suggest incorporating additional speech impairment severity into state-of-the-art hybrid DNN, E2E Conformer and pre-trained Wav2vec 2.0 ASR systems produced statistically significant WER reductions up to 4.78% (14.03% relative). Using the best system the lowest published WER of 17.82% (51.25% on very low intelligibility) was obtained on UASpeech.\n        \u25b3 Less\n      ",
    "title": "Use of Speech Impairment Severity for Dysarthric Speech Recognition",
    "date": "17 May, 2023",
    "authors": [
      "Mengzhe Geng",
      " Zengrui Jin",
      " Tianzi Wang",
      " Shujie Hu",
      " Jiajun Deng",
      " Mingyu Cui",
      " Guinan Li",
      " Jianwei Yu",
      " Xurong Xie",
      " Xunying Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.09833",
    "paper_id": "2301.09833",
    "abstract": "\n        Determining the satisfiability of Boolean constraint-satisfaction problems with different types of constraints, that is hybrid constraints, is a well-studied problem with important applications. We study here a new application of hybrid Boolean constraints, which arises in quantum computing. The problem relates to constrained perfect matching in edge-colored graphs. While general-purpose hybrid constraint solvers can be powerful, we show that direct encodings of the constrained-matching problem as hybrid constraints scale poorly and special techniques are still needed. We propose a novel encoding based on Tutte's Theorem in graph theory as well as optimization techniques. Empirical results demonstrate that our encoding, in suitable languages with advanced SAT solvers, scales significantly better than a number of competing approaches on constrained-matching benchmarks. Our study identifies the necessity of designing problem-specific encodings when applying powerful general-purpose constraint solvers.\n        \u25b3 Less\n      ",
    "title": "Solving Quantum-Inspired Perfect Matching Problems via Tutte's Theorem-Based Hybrid Boolean Constraints",
    "date": "17 May, 2023",
    "authors": [
      "Moshe Y. Vardi",
      " Zhiwei Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10661",
    "paper_id": "2305.10661",
    "abstract": "\n        Weakly supervised learning based on scribble annotations in target extraction of remote sensing images has drawn much interest due to scribbles' flexibility in denoting winding objects and low cost of manually labeling. However, scribbles are too sparse to identify object structure and detailed information, bringing great challenges in target localization and boundary description. To alleviate these problems, in this paper, we construct two inner structure-constraints, a deformation consistency loss and a trainable active contour loss, together with a scribble-constraint to supervise the optimization of the encoder-decoder network without introducing any auxiliary module or extra operation based on prior cues. Comprehensive experiments demonstrate our method's superiority over five state-of-the-art algorithms in this field. Source code is available at https://github.com/yitongli123/ISC-TE.\n        \u25b3 Less\n      ",
    "title": "Scribble-Supervised Target Extraction Method Based on Inner Structure-Constraint for Remote Sensing Images",
    "date": "17 May, 2023",
    "authors": [
      "Yitong Li",
      " Chang Liu",
      " Jie Ma"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10668",
    "paper_id": "2305.10668",
    "abstract": "\n        Graph anomaly detection has long been an important problem in various domains pertaining to information security such as financial fraud, social spam, network intrusion, etc. The majority of existing methods are performed in an unsupervised manner, as labeled anomalies in a large scale are often too expensive to acquire. However, the identified anomalies may turn out to be data noises or uninteresting data instances due to the lack of prior knowledge on the anomalies. In realistic scenarios, it is often feasible to obtain limited labeled anomalies, which have great potential to advance graph anomaly detection. However, the work exploring limited labeled anomalies and a large amount of unlabeled nodes in graphs to detect anomalies is rather limited. Therefore, in this paper, we study a novel problem of few-shot graph anomaly detection. We propose a new framework MetaGAD to learn to meta-transfer the knowledge between unlabeled and labeled nodes for graph anomaly detection. Experimental results on six real-world datasets with synthetic anomalies and \"organic\" anomalies (available in the dataset) demonstrate the effectiveness of the proposed approach in detecting anomalies with limited labeled anomalies.\n        \u25b3 Less\n      ",
    "title": "MetaGAD: Learning to Meta Transfer for Few-shot Graph Anomaly Detection",
    "date": "17 May, 2023",
    "authors": [
      "Xiongxiao Xu",
      " Kaize Ding",
      " Canyu Chen",
      " Kai Shu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10673",
    "paper_id": "2305.10673",
    "abstract": "\n        The prevalence of large-scale graphs poses great challenges in time and storage for training and deploying graph neural networks (GNNs). Several recent works have explored solutions for pruning the large original graph into a small and highly-informative one, such that training and inference on the pruned and large graphs have comparable performance. Although empirically effective, current researches focus on static or non-temporal graphs, which are not directly applicable to dynamic scenarios. In addition, they require labels as ground truth to learn the informative structure, limiting their applicability to new problem domains where labels are hard to obtain. To solve the dilemma, we propose and study the problem of unsupervised graph pruning on dynamic graphs. We approach the problem by our proposed STEP, a self-supervised temporal pruning framework that learns to remove potentially redundant edges from input dynamic graphs. From a technical and industrial viewpoint, our method overcomes the trade-offs between the performance and the time & memory overheads. Our results on three real-world datasets demonstrate the advantages on improving the efficacy, robustness, and efficiency of GNNs on dynamic node classification tasks. Most notably, STEP is able to prune more than 50% of edges on a million-scale industrial graph Alipay (7M nodes, 21M edges) while approximating up to 98% of the original performance. Code is available at https://github.com/EdisonLeeeee/STEP.\n        \u25b3 Less\n      ",
    "title": "Less Can Be More: Unsupervised Graph Pruning for Large-scale Dynamic Graphs",
    "date": "17 May, 2023",
    "authors": [
      "Jintang Li",
      " Sheng Tian",
      " Ruofan Wu",
      " Liang Zhu",
      " Welong Zhao",
      " Changhua Meng",
      " Liang Chen",
      " Zibin Zheng",
      " Hongzhi Yin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10679",
    "paper_id": "2305.10679",
    "abstract": "\n        Code generation aims to automatically generate source code from high-level task specifications, which can significantly increase productivity of software engineering. Recently, approaches based on large language models (LLMs) have shown remarkable code generation abilities on simple tasks. However, generate code for more complex tasks, such as competition-level problems, remains challenging. In this paper, we introduce Brainstorm framework for code generation. It leverages a brainstorming step that generates and selects diverse thoughts on the problem to facilitate algorithmic reasoning, where the thoughts are possible blueprint of solving the problem. We demonstrate that Brainstorm significantly enhances the ability of LLMs to solve competition-level programming problems, resulting in a more than 50% increase in the pass@kk metrics for ChatGPT on the CodeContests benchmark, achieving state-of-the-art performance. Furthermore, our experiments conducted on LeetCode contests show that our framework boosts the ability of ChatGPT to a level comparable to that of human programmers.\n        \u25b3 Less\n      ",
    "title": "Think Outside the Code: Brainstorming Boosts Large Language Models in Code Generation",
    "date": "17 May, 2023",
    "authors": [
      "Xin-Ye Li",
      " Jiang-Tian Xue",
      " Zheng Xie",
      " Ming Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11191",
    "paper_id": "2305.11191",
    "abstract": "\n        Artificial Intelligence (AI) is making a profound impact in almost every domain. One of the crucial factors contributing to this success has been the access to an abundance of high-quality data for constructing machine learning models. Lately, as the role of data in artificial intelligence has been significantly magnified, concerns have arisen regarding the secure utilization of data, particularly in the context of unauthorized data usage. To mitigate data exploitation, data unlearning have been introduced to render data unexploitable. However, current unlearnable examples lack the generalization required for wide applicability. In this paper, we present a novel, generalizable data protection method by generating transferable unlearnable examples. To the best of our knowledge, this is the first solution that examines data privacy from the perspective of data distribution. Through extensive experimentation, we substantiate the enhanced generalizable protection capabilities of our proposed method.\n        \u25b3 Less\n      ",
    "title": "Towards Generalizable Data Protection With Transferable Unlearnable Examples",
    "date": "17 May, 2023",
    "authors": [
      "Bin Fang",
      " Bo Li",
      " Shuang Wu",
      " Tianyi Zheng",
      " Shouhong Ding",
      " Ran Yi",
      " Lizhuang Ma"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.12550",
    "paper_id": "2304.12550",
    "abstract": "\n        Adversarial training is an effective learning technique to improve the robustness of deep neural networks. In this study, the influence of adversarial training on deep learning models in terms of fairness, robustness, and generalization is theoretically investigated under more general perturbation scope that different samples can have different perturbation directions (the adversarial and anti-adversarial directions) and varied perturbation bounds. Our theoretical explorations suggest that the combination of adversaries and anti-adversaries (samples with anti-adversarial perturbations) in training can be more effective in achieving better fairness between classes and a better tradeoff between robustness and generalization in some typical learning scenarios (e.g., noisy label learning and imbalance learning) compared with standard adversarial training. On the basis of our theoretical findings, a more general learning objective that combines adversaries and anti-adversaries with varied bounds on each training sample is presented. Meta learning is utilized to optimize the combination weights. Experiments on benchmark datasets under different learning scenarios verify our theoretical findings and the effectiveness of the proposed methodology.\n        \u25b3 Less\n      ",
    "title": "Combining Adversaries with Anti-adversaries in Training",
    "date": "17 May, 2023",
    "authors": [
      "Xiaoling Zhou",
      " Nan Yang",
      " Ou Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.05881",
    "paper_id": "2302.05881",
    "abstract": "\n        Tensor completion is important to many areas such as computer vision, data analysis, and signal processing. Enforcing low-rank structures on completed tensors, a category of methods known as low-rank tensor completion, has recently been studied extensively. Whilst such methods attained great success, none considered exploiting numerical priors of tensor elements. Ignoring numerical priors causes loss of important information regarding the data, and therefore prevents the algorithms from reaching optimal accuracy. This work attempts to construct a new methodological framework called GCDTC (Generalized CP Decomposition Tensor Completion) for leveraging numerical priors and achieving higher accuracy in tensor completion. In this newly introduced framework, a generalized form of CP Decomposition is applied to low-rank tensor completion. This paper also proposes an algorithm known as SPTC (Smooth Poisson Tensor Completion) for nonnegative integer tensor completion as an instantiation of the GCDTC framework. A series of experiments on real-world data indicate that SPTC could produce results superior in completion accuracy to current state-of-the-art methods. Related code is available in the supplemental materials.\n        \u25b3 Less\n      ",
    "title": "Exploring Numerical Priors for Low-Rank Tensor Completion with Generalized CP Decomposition",
    "date": "17 May, 2023",
    "authors": [
      "Shiran Yuan",
      " Kaizhu Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10708",
    "paper_id": "2305.10708",
    "abstract": "\n        The uptake of health insurance has been poor in Nigeria, a significant step to improving this includes improved awareness, access to information and tools to support decision making. Artificial intelligence (AI) based recommender systems have gained popularity in helping individuals find movies, books, music, and different types of products on the internet including diverse applications in healthcare. The content-based methodology (item-based approach) was employed in the recommender system. We applied both the K-Nearest Neighbor (KNN) and Cosine similarity algorithm. We chose the Cosine similarity as our chosen algorithm after several evaluations based of their outcomes in comparison with domain knowledge. The recommender system takes into consideration the choices entered by the user, filters the health management organization (HMO) data by location and chosen prices. It then recommends the top 3 HMOs with closest similarity in services offered. A recommendation tool to help people find and select the best health insurance plan for them is useful in reducing the barrier of accessing health insurance. Users are empowered to easily find appropriate information on available plans, reduce cognitive overload in dealing with over 100 options available in the market and easily see what matches their financial capacity.\n        \u25b3 Less\n      ",
    "title": "Machine Learning Recommendation System For Health Insurance Decision Making In Nigeria",
    "date": "17 May, 2023",
    "authors": [
      "Ayomide Owoyemi",
      " Emmanuel Nnaemeka",
      " Temitope O. Benson",
      " Ronald Ikpe",
      " Blessing Nwachukwu",
      " Temitope Isedowo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10709",
    "paper_id": "2305.10709",
    "abstract": "\n        Large-scale datasets in the real world inevitably involve label noise. Deep models can gradually overfit noisy labels and thus degrade model generalization. To mitigate the effects of label noise, learning with noisy labels (LNL) methods are designed to achieve better generalization performance. Due to the lack of suitable datasets, previous studies have frequently employed synthetic label noise to mimic real-world label noise. However, synthetic noise is not instance-dependent, making this approximation not always effective in practice. Recent research has proposed benchmarks for learning with real-world noisy labels. However, the noise sources within may be single or fuzzy, making benchmarks different from data with heterogeneous label noises in the real world. To tackle these issues, we contribute NoisywikiHow, the largest NLP benchmark built with minimal supervision. Specifically, inspired by human cognition, we explicitly construct multiple sources of label noise to imitate human errors throughout the annotation, replicating real-world noise, whose corruption is affected by both ground-truth labels and instances. Moreover, we provide a variety of noise levels to support controlled experiments on noisy data, enabling us to evaluate LNL methods systematically and comprehensively. After that, we conduct extensive multi-dimensional experiments on a broad range of LNL methods, obtaining new and intriguing findings.\n        \u25b3 Less\n      ",
    "title": "NoisywikiHow: A Benchmark for Learning with Real-world Noisy Labels in Natural Language Processing",
    "date": "17 May, 2023",
    "authors": [
      "Tingting Wu",
      " Xiao Ding",
      " Minji Tang",
      " Hao Zhang",
      " Bing Qin",
      " Ting Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10716",
    "paper_id": "2305.10716",
    "abstract": "\n        Time-Series Mining (TSM) is an important research area since it shows great potential in practical applications. Deep learning models that rely on massive labeled data have been utilized for TSM successfully. However, constructing a large-scale well-labeled dataset is difficult due to data annotation costs. Recently, Pre-Trained Models have gradually attracted attention in the time series domain due to their remarkable performance in computer vision and natural language processing. In this survey, we provide a comprehensive review of Time-Series Pre-Trained Models (TS-PTMs), aiming to guide the understanding, applying, and studying TS-PTMs. Specifically, we first briefly introduce the typical deep learning models employed in TSM. Then, we give an overview of TS-PTMs according to the pre-training techniques. The main categories we explore include supervised, unsupervised, and self-supervised TS-PTMs. Further, extensive experiments are conducted to analyze the advantages and disadvantages of transfer learning strategies, Transformer-based models, and representative TS-PTMs. Finally, we point out some potential directions of TS-PTMs for future work.\n        \u25b3 Less\n      ",
    "title": "A Survey on Time-Series Pre-Trained Models",
    "date": "17 May, 2023",
    "authors": [
      "Qianli Ma",
      " Zhen Liu",
      " Zhenjing Zheng",
      " Ziyang Huang",
      " Siying Zhu",
      " Zhongzhong Yu",
      " James T. Kwok"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07031",
    "paper_id": "2305.07031",
    "abstract": "\n        Hawkes processes are a popular framework to model the occurrence of sequential events, i.e., occurrence dynamics, in several fields such as social diffusion. In real-world scenarios, the inter-arrival time among events is irregular. However, existing neural network-based Hawkes process models not only i) fail to capture such complicated irregular dynamics, but also ii) resort to heuristics to calculate the log-likelihood of events since they are mostly based on neural networks designed for regular discrete inputs. To this end, we present the concept of Hawkes process based on controlled differential equations (HP-CDE), by adopting the neural controlled differential equation (neural CDE) technology which is an analogue to continuous RNNs. Since HP-CDE continuously reads data, i) irregular time-series datasets can be properly treated preserving their uneven temporal spaces, and ii) the log-likelihood can be exactly computed. Moreover, as both Hawkes processes and neural CDEs are first developed to model complicated human behavioral dynamics, neural CDE-based Hawkes processes are successful in modeling such occurrence dynamics. In our experiments with 4 real-world datasets, our method outperforms existing methods by non-trivial margins.\n        \u25b3 Less\n      ",
    "title": "Hawkes Process Based on Controlled Differential Equations",
    "date": "17 May, 2023",
    "authors": [
      "Minju Jo",
      " Seungji Kook",
      " Noseong Park"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10721",
    "paper_id": "2305.10721",
    "abstract": "\n        Long-term time series forecasting has gained significant attention in recent years. While there are various specialized designs for capturing temporal dependency, previous studies have demonstrated that a single linear layer can achieve competitive forecasting performance compared to other complex architectures. In this paper, we thoroughly investigate the intrinsic effectiveness of recent approaches and make three key observations: 1) linear mapping is critical to prior long-term time series forecasting efforts; 2) RevIN (reversible normalization) and CI (Channel Independent) play a vital role in improving overall forecasting performance; and 3) linear mapping can effectively capture periodic features in time series and has robustness for different periods across channels when increasing the input horizon. We provide theoretical and experimental explanations to support our findings and also discuss the limitations and future works. Our framework's code is available at \\url{https://github.com/plumprc/RTSF}.\n        \u25b3 Less\n      ",
    "title": "Revisiting Long-term Time Series Forecasting: An Investigation on Linear Mapping",
    "date": "17 May, 2023",
    "authors": [
      "Zhe Li",
      " Shiyi Qi",
      " Yiduo Li",
      " Zenglin Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.00148",
    "paper_id": "2207.00148",
    "abstract": "\n        Graph contrastive learning has emerged as a powerful tool for unsupervised graph representation learning. The key to the success of graph contrastive learning is to acquire high-quality positive and negative samples as contrasting pairs for the purpose of learning underlying structural semantics of the input graph. Recent works usually sample negative samples from the same training batch with the positive samples, or from an external irrelevant graph. However, a significant limitation lies in such strategies, which is the unavoidable problem of sampling false negative samples. In this paper, we propose a novel method to utilize \\textbf{C}ounterfactual mechanism to generate artificial hard negative samples for \\textbf{G}raph \\textbf{C}ontrastive learning, namely \\textbf{CGC}, which has a different perspective compared to those sampling-based strategies. We utilize counterfactual mechanism to produce hard negative samples, which ensures that the generated samples are similar to, but have labels that different from the positive sample. The proposed method achieves satisfying results on several datasets compared to some traditional unsupervised graph learning methods and some SOTA graph contrastive learning methods. We also conduct some supplementary experiments to give an extensive illustration of the proposed method, including the performances of CGC with different hard negative samples and evaluations for hard negative samples generated with different similarity measurements.\n        \u25b3 Less\n      ",
    "title": "Generating Counterfactual Hard Negative Samples for Graph Contrastive Learning",
    "date": "17 May, 2023",
    "authors": [
      "Haoran Yang",
      " Hongxu Chen",
      " Sixiao Zhang",
      " Xiangguo Sun",
      " Qian Li",
      " Xiangyu Zhao",
      " Guandong Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10724",
    "paper_id": "2305.10724",
    "abstract": "\n        We present a novel framework, i.e., Segment Any Anomaly + (SAA+), for zero-shot anomaly segmentation with hybrid prompt regularization to improve the adaptability of modern foundation models. Existing anomaly segmentation models typically rely on domain-specific fine-tuning, limiting their generalization across countless anomaly patterns. In this work, inspired by the great zero-shot generalization ability of foundation models like Segment Anything, we first explore their assembly to leverage diverse multi-modal prior knowledge for anomaly localization. For non-parameter foundation model adaptation to anomaly segmentation, we further introduce hybrid prompts derived from domain expert knowledge and target image context as regularization. Our proposed SAA+ model achieves state-of-the-art performance on several anomaly segmentation benchmarks, including VisA, MVTec-AD, MTD, and KSDD2, in the zero-shot setting. We will release the code at \\href{https://github.com/caoyunkang/Segment-Any-Anomaly}{https://github.com/caoyunkang/Segment-Any-Anomaly}.\n        \u25b3 Less\n      ",
    "title": "Segment Any Anomaly without Training via Hybrid Prompt Regularization",
    "date": "17 May, 2023",
    "authors": [
      "Yunkang Cao",
      " Xiaohao Xu",
      " Chen Sun",
      " Yuqi Cheng",
      " Zongwei Du",
      " Liang Gao",
      " Weiming Shen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10726",
    "paper_id": "2305.10726",
    "abstract": "\n        Today, we have a mixture of young and older individuals, people with special needs, and people who can care for themselves. Over 1 billion people are estimated to be disabled; this figure corresponds to about 15% of the world's population, with 3.8% (approximately 190 million people) accounting for people aged 15 and up (Organization, 2011). The number of people with disabilities is upward due to the increase in chronic health conditions and many other things. These and other factors have made the need for proper care facilities urgent in today's society. Several care facilities are built to help people with disabilities live their everyday lives and not be left out of the community.\n        \u25b3 Less\n      ",
    "title": "Ambient Technology & Intelligence",
    "date": "17 May, 2023",
    "authors": [
      "Amos Okomayin",
      " Tosin Ige"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07854",
    "paper_id": "2305.07854",
    "abstract": "\n        Data-driven industrial health prognostics require rich training data to develop accurate and reliable predictive models. However, stringent data privacy laws and the abundance of edge industrial data necessitate decentralized data utilization. Thus, the industrial health prognostics field is well suited to significantly benefit from federated learning (FL), a decentralized and privacy-preserving learning technique. However, FL-based health prognostics tasks have hardly been investigated due to the complexities of meaningfully aggregating model parameters trained from heterogeneous data to form a high performing federated model. Specifically, data heterogeneity among edge devices, stemming from dissimilar degradation mechanisms and unequal dataset sizes, poses a critical statistical challenge for developing accurate federated models. We propose a pioneering FL-based health prognostic model with a feature similarity-matched parameter aggregation algorithm to discriminatingly learn from heterogeneous edge data. The algorithm searches across the heterogeneous locally trained models and matches neurons with probabilistically similar feature extraction functions first, before selectively averaging them to form the federated model parameters. As the algorithm only averages similar neurons, as opposed to conventional naive averaging of coordinate-wise neurons, the distinct feature extractors of local models are carried over with less dilution to the resultant federated model. Using both cyclic degradation data of Li-ion batteries and non-cyclic data of turbofan engines, we demonstrate that the proposed method yields accuracy improvements as high as 44.5\\% and 39.3\\% for state-of-health estimation and remaining useful life estimation, respectively.\n        \u25b3 Less\n      ",
    "title": "A Federated Learning-based Industrial Health Prognostics for Heterogeneous Edge Devices using Matched Feature Extraction",
    "date": "17 May, 2023",
    "authors": [
      "Anushiya Arunan",
      " Yan Qin",
      " Xiaoli Li",
      " Chau Yuen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10736",
    "paper_id": "2305.10736",
    "abstract": "\n        Despite substantial progress in abstractive text summarization to generate fluent and informative texts, the factual inconsistency in the generated summaries remains an important yet challenging problem to be solved. In this paper, we construct causal graphs for abstractive text summarization and identify the intrinsic causes of the factual inconsistency, i.e., the language bias and irrelevancy bias, and further propose a debiasing framework, named CoFactSum, to alleviate the causal effects of these biases by counterfactual estimation. Specifically, the proposed CoFactSum provides two counterfactual estimation strategies, i.e., Explicit Counterfactual Masking with an explicit dynamic masking strategy, and Implicit Counterfactual Training with an implicit discriminative cross-attention mechanism. Meanwhile, we design a Debiasing Degree Adjustment mechanism to dynamically adapt the debiasing degree at each decoding step. Extensive experiments on two widely-used summarization datasets demonstrate the effectiveness of CoFactSum in enhancing the factual consistency of generated summaries compared with several baselines.\n        \u25b3 Less\n      ",
    "title": "Counterfactual Debiasing for Generating Factually Consistent Text Summaries",
    "date": "17 May, 2023",
    "authors": [
      "Chenhe Dong",
      " Yuexiang Xie",
      " Yaliang Li",
      " Ying Shen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10766",
    "paper_id": "2305.10766",
    "abstract": "\n        Adversarial attack is commonly regarded as a huge threat to neural networks because of misleading behavior. This paper presents an opposite perspective: adversarial attacks can be harnessed to improve neural models if amended correctly. Unlike traditional adversarial defense or adversarial training schemes that aim to improve the adversarial robustness, the proposed adversarial amendment (AdvAmd) method aims to improve the original accuracy level of neural models on benign samples. We thoroughly analyze the distribution mismatch between the benign and adversarial samples. This distribution mismatch and the mutual learning mechanism with the same learning ratio applied in prior art defense strategies is the main cause leading the accuracy degradation for benign samples. The proposed AdvAmd is demonstrated to steadily heal the accuracy degradation and even leads to a certain accuracy boost of common neural models on benign classification, object detection, and segmentation tasks. The efficacy of the AdvAmd is contributed by three key components: mediate samples (to reduce the influence of distribution mismatch with a fine-grained amendment), auxiliary batch norm (to solve the mutual learning mechanism and the smoother judgment surface), and AdvAmd loss (to adjust the learning ratios according to different attack vulnerabilities) through quantitative and ablation experiments.\n        \u25b3 Less\n      ",
    "title": "Adversarial Amendment is the Only Force Capable of Transforming an Enemy into a Friend",
    "date": "17 May, 2023",
    "authors": [
      "Chong Yu",
      " Tao Chen",
      " Zhongxue Gan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10773",
    "paper_id": "2305.10773",
    "abstract": "\n        Recently, the ever-increasing demand for bandwidth in multi-modal communication systems requires a paradigm shift. Powered by deep learning, semantic communications are applied to multi-modal scenarios to boost communication efficiency and save communication resources. However, the existing end-to-end neural network (NN) based framework without the channel encoder/decoder is incompatible with modern digital communication systems. Moreover, most end-to-end designs are task-specific and require re-design and re-training for new tasks, which limits their applications. In this paper, we propose a distributed multi-modal semantic communication framework incorporating the conventional channel encoder/decoder. We adopt NN-based semantic encoder and decoder to extract correlated semantic information contained in different modalities, including speech, text, and image. Based on the proposed framework, we further establish a general rate-adaptive coding mechanism for various types of multi-modal semantic tasks. In particular, we utilize unequal error protection based on semantic importance, which is derived by evaluating the distortion bound of each modality. We further formulate and solve an optimization problem that aims at minimizing inference delay while maintaining inference accuracy for semantic tasks. Numerical results show that the proposed mechanism fares better than both conventional communication and existing semantic communication systems in terms of task performance, inference delay, and deployment complexity.\n        \u25b3 Less\n      ",
    "title": "Rate-Adaptive Coding Mechanism for Semantic Communications With Multi-Modal Data",
    "date": "17 May, 2023",
    "authors": [
      "Yangshuo He",
      " Guanding Yu",
      " Yunlong Cai"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10783",
    "paper_id": "2305.10783",
    "abstract": "\n        Human intelligence's adaptability is remarkable, allowing us to adjust to new tasks and multi-modal environments swiftly. This skill is evident from a young age as we acquire new abilities and solve problems by imitating others or following natural language instructions. The research community is actively pursuing the development of interactive \"embodied agents\" that can engage in natural conversations with humans and assist them with real-world tasks. These agents must possess the ability to promptly request feedback in case communication breaks down or instructions are unclear. Additionally, they must demonstrate proficiency in learning new vocabulary specific to a given domain.\n  In this paper, we made the following contributions: (1) a crowd-sourcing tool for collecting grounded language instructions; (2) the largest dataset of grounded language instructions; and (3) several state-of-the-art baselines. These contributions are suitable as a foundation for further research.\n        \u25b3 Less\n      ",
    "title": "Transforming Human-Centered AI Collaboration: Redefining Embodied Agents Capabilities through Interactive Grounded Language Instructions",
    "date": "17 May, 2023",
    "authors": [
      "Shrestha Mohanty",
      " Negar Arabzadeh",
      " Julia Kiseleva",
      " Artem Zholus",
      " Milagro Teruel",
      " Ahmed Awadallah",
      " Yuxuan Sun",
      " Kavya Srinet",
      " Arthur Szlam"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04757",
    "paper_id": "2305.04757",
    "abstract": "\n        Large Language Models (LLMs) have significantly advanced natural language processing (NLP) with their impressive language understanding and generation capabilities. However, their performance may be suboptimal for domain-specific tasks that require specialized knowledge due to limited exposure to the related data. Additionally, the lack of transparency of most state-of-the-art (SOTA) LLMs, which can only be accessed via APIs, impedes further fine-tuning with domain custom data. Moreover, providing private data to the LLMs' owner leads to data privacy problems. To address these challenges, we propose the novel Parametric Knowledge Guiding (PKG) framework, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters. Our PKG is based on open-source \"white-box\" language models, allowing offline memory of any knowledge that LLMs require. We demonstrate that our PKG framework can enhance the performance of \"black-box\" LLMs on a range of domain knowledge-intensive tasks that require factual (+7.9%), tabular (+11.9%), medical (+3.0%), and multimodal (+8.1%) knowledge.\n        \u25b3 Less\n      ",
    "title": "Augmented Large Language Models with Parametric Knowledge Guiding",
    "date": "17 May, 2023",
    "authors": [
      "Ziyang Luo",
      " Can Xu",
      " Pu Zhao",
      " Xiubo Geng",
      " Chongyang Tao",
      " Jing Ma",
      " Qingwei Lin",
      " Daxin Jiang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10822",
    "paper_id": "2305.10822",
    "abstract": "\n        Modern online service providers such as online shopping platforms often provide both search and recommendation (S&R) services to meet different user needs. Rarely has there been any effective means of incorporating user behavior data from both S&R services. Most existing approaches either simply treat S&R behaviors separately, or jointly optimize them by aggregating data from both services, ignoring the fact that user intents in S&R can be distinctively different. In our paper, we propose a Search-Enhanced framework for the Sequential Recommendation (SESRec) that leverages users' search interests for recommendation, by disentangling similar and dissimilar representations within S&R behaviors. Specifically, SESRec first aligns query and item embeddings based on users' query-item interactions for the computations of their similarities. Two transformer encoders are used to learn the contextual representations of S&R behaviors independently. Then a contrastive learning task is designed to supervise the disentanglement of similar and dissimilar representations from behavior sequences of S&R. Finally, we extract user interests by the attention mechanism from three perspectives, i.e., the contextual representations, the two separated behaviors containing similar and dissimilar interests. Extensive experiments on both industrial and public datasets demonstrate that SESRec consistently outperforms state-of-the-art models. Empirical studies further validate that SESRec successfully disentangle similar and dissimilar user interests from their S&R behaviors.\n        \u25b3 Less\n      ",
    "title": "When Search Meets Recommendation: Learning Disentangled Search Representation for Recommendation",
    "date": "18 May, 2023",
    "authors": [
      "Zihua Si",
      " Zhongxiang Sun",
      " Xiao Zhang",
      " Jun Xu",
      " Xiaoxue Zang",
      " Yang Song",
      " Kun Gai",
      " Ji-Rong Wen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10827",
    "paper_id": "2305.10827",
    "abstract": "\n        In recent decades, the field of affective computing has made substantial progress in advancing the ability of AI systems to recognize and express affective phenomena, such as affect and emotions, during human-human and human-machine interactions. This paper describes our examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas. We examined over 16,000 papers from selected conferences in multimodal interaction, affective computing, and natural language processing: ACM International Conference on Multimodal Interaction, AAAC International Conference on Affective Computing and Intelligent Interaction, Annual Meeting of the Association for Computational Linguistics, and Conference on Empirical Methods in Natural Language Processing. We identified 910 affect-related papers and present our analysis of the role of affective phenomena in these papers. We find that this body of research has primarily focused on enabling machines to recognize and express affect and emotion. However, we find limited research on how affect and emotion predictions might be used by AI systems to enhance machine understanding of human social behaviors and cognitive states. Based on our analysis, we discuss directions to expand the role of affective phenomena in multimodal interaction research.\n        \u25b3 Less\n      ",
    "title": "Expanding the Role of Affective Phenomena in Multimodal Interaction Research",
    "date": "18 May, 2023",
    "authors": [
      "Leena Mathur",
      " Maja J Matari\u0107",
      " Louis-Philippe Morency"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10830",
    "paper_id": "2305.10830",
    "abstract": "\n        Shear wall structures are widely used in high-rise residential buildings, and the layout of shear walls requires many years of design experience and iterative trial and error. Currently, there are methods based on heuristic algorithms, but they generate results too slowly. Those based on Generative Adversarial Networks (GANs) or Graph Neural Networks (GNNs) can only generate single arrangements and require large amounts of training data. At present, Stable Diffusion is being widely used, and by using the Low-Rank Adaptation (LoRA) method to fine-tune large models with small amounts of data, good generative results can be achieved. Therefore, this paper proposes a personalized AI assistant for shear wall layout based on Stable Diffusion, which has been proven to produce good generative results through testing.\n        \u25b3 Less\n      ",
    "title": "Constructing a personalized AI assistant for shear wall layout using Stable Diffusion",
    "date": "18 May, 2023",
    "authors": [
      "Lufeng Wang",
      " Jiepeng Liu",
      " Guozhong Cheng",
      " En Liu",
      " Wei Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10834",
    "paper_id": "2305.10834",
    "abstract": "\n        During 2022, both transformer-based AI text generation sys-tems such as GPT-3 and AI text-to-image generation systems such as DALL-E 2 and Stable Diffusion made exponential leaps forward and are unquestionably altering the fields of digital art and electronic literature. In this panel a group of electronic literature authors and theorists consider new oppor-tunities for human creativity presented by these systems and present new works have produced during the past year that specifically address these systems as environments for literary expressions that are translated through iterative interlocutive processes into visual representations. The premise that binds these presentations is that these systems and the works gener-ated must be considered from a literary perspective, as they originate in human writing. In works ranging from a visual memoir of the personal experience of a health crisis, to interac-tive web comics, to architectures based on abstract poetic language, to political satire, four artists explore the capabili-ties of these writing environments for new genres of literary artist practice, while a digital culture theorist considers the origins and effects of the particular training datasets of human language and images on which these new hybrid forms are based.\n        \u25b3 Less\n      ",
    "title": "AIwriting: Relations Between Image Generation and Digital Writing",
    "date": "18 May, 2023",
    "authors": [
      "Scott Rettberg",
      " Talan Memmott",
      " Jill Walker Rettberg",
      " Jason Nelson",
      " Patrick Lichty"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10840",
    "paper_id": "2305.10840",
    "abstract": "\n        Uncertainty-quantification methods are applied to estimate the confidence of deep-neural-networks classifiers over their predictions. However, most widely used methods are known to be overconfident. We address this problem by developing an algorithm that exploits the latent-space representation of data points fed into the network, to assess the accuracy of their prediction. Using the latent-space representation generated by the fraction of training set that the network classifies correctly, we build a statistical model that is able to capture the likelihood of a given prediction. We show on a synthetic dataset that commonly used methods are mostly overconfident. Overconfidence occurs also for predictions made on data points that are outside the distribution that generated the training data. In contrast, our method can detect such out-of-distribution data points as inaccurately predicted, thus aiding in the automatic detection of outliers.\n        \u25b3 Less\n      ",
    "title": "Uncertainty Quantification in Deep Neural Networks through Statistical Inference on Latent Space",
    "date": "18 May, 2023",
    "authors": [
      "Luigi Sbail\u00f2",
      " Luca M. Ghiringhelli"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10846",
    "paper_id": "2305.10846",
    "abstract": "\n        Approximation fixpoint theory (AFT) is an abstract and general algebraic framework for studying the semantics of non-monotonic logics. In recent work, AFT was generalized to non-deterministic operators, i.e.\\ operators whose range are sets of elements rather than single elements. In this paper, we make three further contributions to non-deterministic AFT: (1) we define and study ultimate approximations of non-deterministic operators, (2) we give an algebraic formulation of the semi-equilibrium semantics by Amendola, et al., and (3) we generalize the characterisations of disjunctive logic programs to disjunctive logic programs with aggregates.\n        \u25b3 Less\n      ",
    "title": "Non-deterministic approximation operators: ultimate operators, semi-equilibrium semantics and aggregates (full version)",
    "date": "18 May, 2023",
    "authors": [
      "Jesse Heyninck",
      " Bart Bogaerts"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10863",
    "paper_id": "2305.10863",
    "abstract": "\n        Systems for serving inference requests on graph neural networks (GNN) must combine low latency with high throughout, but they face irregular computation due to skew in the number of sampled graph nodes and aggregated GNN features. This makes it challenging to exploit GPUs effectively: using GPUs to sample only a few graph nodes yields lower performance than CPU-based sampling; and aggregating many features exhibits high data movement costs between GPUs and CPUs. Therefore, current GNN serving systems use CPUs for graph sampling and feature aggregation, limiting throughput.\n  We describe Quiver, a distributed GPU-based GNN serving system with low-latency and high-throughput. Quiver's key idea is to exploit workload metrics for predicting the irregular computation of GNN requests, and governing the use of GPUs for graph sampling and feature aggregation: (1) for graph sampling, Quiver calculates the probabilistic sampled graph size, a metric that predicts the degree of parallelism in graph sampling. Quiver uses this metric to assign sampling tasks to GPUs only when the performance gains surpass CPU-based sampling; and (2) for feature aggregation, Quiver relies on the feature access probability to decide which features to partition and replicate across a distributed GPU NUMA topology. We show that Quiver achieves up to 35 times lower latency with an 8 times higher throughput compared to state-of-the-art GNN approaches (DGL and PyG).\n        \u25b3 Less\n      ",
    "title": "Quiver: Supporting GPUs for Low-Latency, High-Throughput GNN Serving with Workload Awareness",
    "date": "18 May, 2023",
    "authors": [
      "Zeyuan Tan",
      " Xiulong Yuan",
      " Congjie He",
      " Man-Kit Sit",
      " Guo Li",
      " Xiaoze Liu",
      " Baole Ai",
      " Kai Zeng",
      " Peter Pietzuch",
      " Luo Mai"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08208",
    "paper_id": "2305.08208",
    "abstract": "\n        There have been growing concerns regarding the out-of-domain generalization ability of natural language processing (NLP) models, particularly in question-answering (QA) tasks. Current synthesized data augmentation methods for QA are hampered by increased training costs. To address this issue, we propose a novel approach that combines prompting methods and linear probing then fine-tuning strategy, which does not entail additional cost. Our method has been theoretically and empirically shown to be effective in enhancing the generalization ability of both generative and discriminative models. Our approach outperforms state-of-the-art baselines, with an average increase in F1 score of 4.5%-7.9%. Furthermore, our method can be easily integrated into any pre-trained models and offers a promising solution to the under-explored cross-domain QA task. We release our source code at GitHub*.\n        \u25b3 Less\n      ",
    "title": "Learning to Generalize for Cross-domain QA",
    "date": "18 May, 2023",
    "authors": [
      "Yingjie Niu",
      " Linyi Yang",
      " Ruihai Dong",
      " Yue Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18234",
    "paper_id": "2305.18234",
    "abstract": "\n        Emotion recognition plays a crucial role in human-computer interaction, and electroencephalography (EEG) is advantageous for reflecting human emotional states. In this study, we propose MACTN, a hierarchical hybrid model for jointly modeling local and global temporal information. The model is inspired by neuroscience research on the temporal dynamics of emotions. MACTN extracts local emotional features through a convolutional neural network (CNN) and integrates sparse global emotional features through a transformer. Moreover, we employ channel attention mechanisms to identify the most task-relevant channels. Through extensive experimentation on two publicly available datasets, namely THU-EP and DEAP, our proposed method, MACTN, consistently achieves superior classification accuracy and F1 scores compared to other existing methods in most experimental settings. Furthermore, ablation studies have shown that the integration of both self-attention mechanisms and channel attention mechanisms leads to improved classification performance. Finally, an earlier version of this method, which shares the same ideas, won the Emotional BCI Competition's final championship in the 2022 World Robot Contest.\n        \u25b3 Less\n      ",
    "title": "Temporal Aware Mixed Attention-based Convolution and Transformer Network (MACTN) for EEG Emotion Recognition",
    "date": "18 May, 2023",
    "authors": [
      "Xiaopeng Si",
      " Dong Huang",
      " Yulin Sun",
      " Dong Ming"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02777",
    "paper_id": "2305.02777",
    "abstract": "\n        Existing neural machine translation (NMT) studies mainly focus on developing dataset-specific models based on data from different tasks (e.g., document translation and chat translation). Although the dataset-specific models have achieved impressive performance, it is cumbersome as each dataset demands a model to be designed, trained, and stored. In this work, we aim to unify these translation tasks into a more general setting. Specifically, we propose a ``versatile'' model, i.e., the Unified Model Learning for NMT (UMLNMT) that works with data from different tasks, and can translate well in multiple settings simultaneously, and theoretically it can be as many as possible. Through unified learning, UMLNMT is able to jointly train across multiple tasks, implementing intelligent on-demand translation. On seven widely-used translation tasks, including sentence translation, document translation, and chat translation, our UMLNMT results in substantial improvements over dataset-specific models with significantly reduced model deployment costs. Furthermore, UMLNMT can achieve competitive or better performance than state-of-the-art dataset-specific methods. Human evaluation and in-depth analysis also demonstrate the superiority of our approach on generating diverse and high-quality translations. Additionally, we provide a new genre translation dataset about famous aphorisms with 186k Chinese->English sentence pairs.\n        \u25b3 Less\n      ",
    "title": "Unified Model Learning for Various Neural Machine Translation",
    "date": "18 May, 2023",
    "authors": [
      "Yunlong Liang",
      " Fandong Meng",
      " Jinan Xu",
      " Jiaan Wang",
      " Yufeng Chen",
      " Jie Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2208.10364",
    "paper_id": "2208.10364",
    "abstract": "\n        Recent years have seen a surge in research on dynamic graph representation learning, which aims to model temporal graphs that are dynamic and evolving constantly over time. However, current work typically models graph dynamics with recurrent neural networks (RNNs), making them suffer seriously from computation and memory overheads on large temporal graphs. So far, scalability of dynamic graph representation learning on large temporal graphs remains one of the major challenges. In this paper, we present a scalable framework, namely SpikeNet, to efficiently capture the temporal and structural patterns of temporal graphs. We explore a new direction in that we can capture the evolving dynamics of temporal graphs with spiking neural networks (SNNs) instead of RNNs. As a low-power alternative to RNNs, SNNs explicitly model graph dynamics as spike trains of neuron populations and enable spike-based propagation in an efficient way. Experiments on three large real-world temporal graph datasets demonstrate that SpikeNet outperforms strong baselines on the temporal node classification task with lower computational costs. Particularly, SpikeNet generalizes to a large temporal graph (2.7M nodes and 13.9M edges) with significantly fewer parameters and computation overheads.Our code is publicly available at \\url{https://github.com/EdisonLeeeee/SpikeNet}.\n        \u25b3 Less\n      ",
    "title": "Scaling Up Dynamic Graph Representation Learning via Spiking Neural Networks",
    "date": "18 May, 2023",
    "authors": [
      "Jintang Li",
      " Zhouxin Yu",
      " Zulun Zhu",
      " Liang Chen",
      " Qi Yu",
      " Zibin Zheng",
      " Sheng Tian",
      " Ruofan Wu",
      " Changhua Meng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10920",
    "paper_id": "2305.10920",
    "abstract": "\n        To develop computational agents that better communicate using their own emergent language, we endow the agents with an ability to focus their attention on particular concepts in the environment. Humans often understand an object or scene as a composite of concepts and those concepts are further mapped onto words. We implement this intuition as cross-modal attention mechanisms in Speaker and Listener agents in a referential game and show attention leads to more compositional and interpretable emergent language. We also demonstrate how attention aids in understanding the learned communication protocol by investigating the attention weights associated with each message symbol and the alignment of attention weights between Speaker and Listener agents. Overall, our results suggest that attention is a promising mechanism for developing more human-like emergent language.\n        \u25b3 Less\n      ",
    "title": "Emergent Communication with Attention",
    "date": "18 May, 2023",
    "authors": [
      "Ryokan Ri",
      " Ryo Ueda",
      " Jason Naradowsky"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08018",
    "paper_id": "2305.08018",
    "abstract": "\n        Message passing neural networks (MPNNs) have been shown to suffer from the phenomenon of over-squashing that causes poor performance for tasks relying on long-range interactions. This can be largely attributed to message passing only occurring locally, over a node's immediate neighbours. Rewiring approaches attempting to make graphs 'more connected', and supposedly better suited to long-range tasks, often lose the inductive bias provided by distance on the graph since they make distant nodes communicate instantly at every layer. In this paper we propose a framework, applicable to any MPNN architecture, that performs a layer-dependent rewiring to ensure gradual densification of the graph. We also propose a delay mechanism that permits skip connections between nodes depending on the layer and their mutual distance. We validate our approach on several long-range tasks and show that it outperforms graph Transformers and multi-hop MPNNs.\n        \u25b3 Less\n      ",
    "title": "DRew: Dynamically Rewired Message Passing with Delay",
    "date": "18 May, 2023",
    "authors": [
      "Benjamin Gutteridge",
      " Xiaowen Dong",
      " Michael Bronstein",
      " Francesco Di Giovanni"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11914",
    "paper_id": "2305.11914",
    "abstract": "\n        The adoption of modern technologies for use in healthcare has become an inevitable change. The emergence of artificial intelligence drives this digital disruption. Artificial intelligence has augmented machine capabilities to act like and interact with human beings. As the healthcare industry adopts technology in most areas, an area in healthcare that is touched by this change is clinical practice. New technologies are being designed to improve healthcare services. One aspect of these technologies is voice user interfaces. This paper reviews applications of voice user interfaces in clinical settings. Several information sources were consulted, and based on eligibility criteria, a search was conducted, and ten papers selected. This study presents findings from the last ten years (2009-2019). The results are categorized based on findings, also they contribute to the discussion and the research gaps identified for future study as regards context-aware voice user interfaces and the appearance of conversational agents from a given set of options.\n        \u25b3 Less\n      ",
    "title": "Applications of Voice User Interfaces in Clinical Settings",
    "date": "18 May, 2023",
    "authors": [
      "Akiri Surely"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10938",
    "paper_id": "2305.10938",
    "abstract": "\n        Despite its successes, to date Artificial Intelligence (AI) is still characterized by a number of shortcomings with regards to different application domains and goals. These limitations are arguably both conceptual (e.g., related to underlying theoretical models, such as symbolic vs. connectionist), and operational (e.g., related to robustness and ability to generalize). Biologically inspired AI, and more specifically brain-inspired AI, promises to provide further biological aspects beyond those that are already traditionally included in AI, making it possible to assess and possibly overcome some of its present shortcomings. This article examines some conceptual, technical, and ethical issues raised by the development and use of brain-inspired AI. Against this background, the paper asks whether there is anything ethically unique about brain-inspired AI. The aim of the paper is to introduce a method that has a heuristic nature and that can be applied to identify and address the ethical issues arising from brain-inspired AI. The conclusion resulting from the application of this method is that, compared to traditional AI, brain-inspired AI raises new foundational ethical issues and some new practical ethical issues, and exacerbates some of the issues raised by traditional AI.\n        \u25b3 Less\n      ",
    "title": "A method for the ethical analysis of brain-inspired AI",
    "date": "18 May, 2023",
    "authors": [
      "Michele Farisco",
      " Gianluca Baldassarre",
      " Emilio Cartoni",
      " Antonia Leach",
      " Mihai A. Petrovici",
      " Achim Rosemann",
      " Arleen Salles",
      " Bernd Stahl",
      " Sacha J. van Albada"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10960",
    "paper_id": "2305.10960",
    "abstract": "\n        We address the problem of teleoperating an industrial robot manipulator via a commercially available Virtual Reality (VR) interface. Previous works on VR teleoperation for robot manipulators focus primarily on collaborative or research robot platforms (whose dynamics and constraints differ from industrial robot arms), or only address tasks where the robot's dynamics are not as important (e.g: pick and place tasks). We investigate the usage of commercially available VR interfaces for effectively teleoeprating industrial robot manipulators in a variety of contact-rich manipulation tasks. We find that applying standard practices for VR control of robot arms is challenging for industrial platforms because torque and velocity control is not exposed, and position control is mediated through a black-box controller. To mitigate these problems, we propose a simplified filtering approach to process command signals to enable operators to effectively teleoperate industrial robot arms with VR interfaces in dexterous manipulation tasks. We hope our findings will help robot practitioners implement and setup effective VR teleoperation interfaces for robot manipulators. The proposed method is demonstrated on a variety of contact-rich manipulation tasks which can also involve very precise movement of the robot during execution (videos can be found at https://www.youtube.com/watch?v=OhkCB9mOaBc)\n        \u25b3 Less\n      ",
    "title": "A Virtual Reality Teleoperation Interface for Industrial Robot Manipulators",
    "date": "18 May, 2023",
    "authors": [
      "Eric Rosen",
      " Devesh K. Jha"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10961",
    "paper_id": "2305.10961",
    "abstract": "\n        Prevention is better than cure. This old truth applies not only to the prevention of diseases but also to the prevention of issues with AI models used in medicine. The source of malfunctioning of predictive models often lies not in the training process but reaches the data acquisition phase or design of the experiment phase.\n  In this paper, we analyze in detail a single use case - a Kaggle competition related to the detection of abnormalities in X-ray lung images. We demonstrate how a series of simple tests for data imbalance exposes faults in the data acquisition and annotation process. Complex models are able to learn such artifacts and it is difficult to remove this bias during or after the training. Errors made at the data collection stage make it difficult to validate the model correctly.\n  Based on this use case, we show how to monitor data and model balance (fairness) throughout the life cycle of a predictive model, from data acquisition to parity analysis of model scores.\n        \u25b3 Less\n      ",
    "title": "Prevention is better than cure: a case study of the abnormalities detection in the chest",
    "date": "18 May, 2023",
    "authors": [
      "Weronika Hryniewska",
      " Piotr Czarnecki",
      " Jakub Wi\u015bniewski",
      " Przemys\u0142aw Bombi\u0144ski",
      " Przemys\u0142aw Biecek"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10972",
    "paper_id": "2305.10972",
    "abstract": "\n        In an indivisible participatory budgeting (PB) framework, we have a limited budget that is to be distributed among a set of projects, by aggregating the preferences of voters for the projects. All the prior work on indivisible PB assumes that each project has only one possible cost. In this work, we let each project have a set of permissible costs, each reflecting a possible degree of sophistication of the project. Each voter approves a range of costs for each project, by giving an upper and lower bound on the cost that she thinks the project deserves. The outcome of a PB rule selects a subset of projects and also specifies their corresponding costs. We study different utility notions and prove that the existing positive results when every project has exactly one permissible cost can also be extended to our framework where a project has several permissible costs. We also analyze the fixed parameter tractability of the problem. Finally, we propose some important and intuitive axioms and analyze their satisfiability by different PB rules. We conclude by making some crucial remarks.\n        \u25b3 Less\n      ",
    "title": "Participatory Budgeting With Multiple Degrees of Projects And Ranged Approval Votes",
    "date": "18 May, 2023",
    "authors": [
      "Gogulapati Sreedurga"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.13494",
    "paper_id": "2303.13494",
    "abstract": "\n        Attention is the crucial cognitive ability that limits and selects what information we observe. Previous work by Bolander et al. (2016) proposes a model of attention based on dynamic epistemic logic (DEL) where agents are either fully attentive or not attentive at all. While introducing the realistic feature that inattentive agents believe nothing happens, the model does not represent the most essential aspect of attention: its selectivity. Here, we propose a generalization that allows for paying attention to subsets of atomic formulas. We introduce the corresponding logic for propositional attention, and show its axiomatization to be sound and complete. We then extend the framework to account for inattentive agents that, instead of assuming nothing happens, may default to a specific truth-value of what they failed to attend to (a sort of prior concerning the unattended atoms). This feature allows for a more cognitively plausible representation of the inattentional blindness phenomenon, where agents end up with false beliefs due to their failure to attend to conspicuous but unexpected events. Both versions of the model define attention-based learning through appropriate DEL event models based on a few and clear edge principles. While the size of such event models grow exponentially both with the number of agents and the number of atoms, we introduce a new logical language for describing event models syntactically and show that using this language our event models can be represented linearly in the number of agents and atoms. Furthermore, representing our event models using this language is achieved by a straightforward formalisation of the aforementioned edge principles.\n        \u25b3 Less\n      ",
    "title": "Attention! Dynamic Epistemic Logic Models of (In)attentive Agents",
    "date": "18 May, 2023",
    "authors": [
      "Gaia Belardinelli",
      " Thomas Bolander"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10975",
    "paper_id": "2305.10975",
    "abstract": "\n        Ocular Toxoplasmosis (OT), is a common eye infection caused by T. gondii that can cause vision problems. Diagnosis is typically done through a clinical examination and imaging, but these methods can be complicated and costly, requiring trained personnel. To address this issue, we have created a benchmark study that evaluates the effectiveness of existing pre-trained networks using transfer learning techniques to detect OT from fundus images. Furthermore, we have also analysed the performance of transfer-learning based segmentation networks to segment lesions in the images. This research seeks to provide a guide for future researchers looking to utilise DL techniques and develop a cheap, automated, easy-to-use, and accurate diagnostic method. We have performed in-depth analysis of different feature extraction techniques in order to find the most optimal one for OT classification and segmentation of lesions. For classification tasks, we have evaluated pre-trained models such as VGG16, MobileNetV2, InceptionV3, ResNet50, and DenseNet121 models. Among them, MobileNetV2 outperformed all other models in terms of Accuracy (Acc), Recall, and F1 Score outperforming the second-best model, InceptionV3 by 0.7% higher Acc. However, DenseNet121 achieved the best result in terms of Precision, which was 0.1% higher than MobileNetv2. For the segmentation task, this work has exploited U-Net architecture. In order to utilize transfer learning the encoder block of the traditional U-Net was replaced by MobileNetV2, InceptionV3, ResNet34, and VGG16 to evaluate different architectures moreover two different two different loss functions (Dice loss and Jaccard loss) were exploited in order to find the most optimal one. The MobileNetV2/U-Net outperformed ResNet34 by 0.5% and 2.1% in terms of Acc and Dice Score, respectively when Jaccard loss function is employed during the training.\n        \u25b3 Less\n      ",
    "title": "Benchmarking Deep Learning Frameworks for Automated Diagnosis of Ocular Toxoplasmosis: A Comprehensive Approach to Classification and Segmentation",
    "date": "18 May, 2023",
    "authors": [
      "Syed Samiul Alam",
      " Samiul Based Shuvo",
      " Shams Nafisa Ali",
      " Fardeen Ahmed",
      " Arbil Chakma",
      " Yeong Min Jang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11197",
    "paper_id": "2305.11197",
    "abstract": "\n        Data with missing values is ubiquitous in many applications. Recent years have witnessed increasing attention on prediction with only incomplete data consisting of observed features and a mask that indicates the missing pattern. Existing methods assume that the training and testing distributions are the same, which may be violated in real-world scenarios. In this paper, we consider prediction with incomplete data in the presence of distribution shift. We focus on the case where the underlying joint distribution of complete features and label is invariant, but the missing pattern, i.e., mask distribution may shift agnostically between training and testing. To achieve generalization, we leverage the observation that for each mask, there is an invariant optimal predictor. To avoid the exponential explosion when learning them separately, we approximate the optimal predictors jointly using a double parameterization technique. This has the undesirable side effect of allowing the learned predictors to rely on the intra-mask correlation and that between features and mask. We perform decorrelation to minimize this effect. Combining the techniques above, we propose a novel prediction method called StableMiss. Extensive experiments on both synthetic and real-world datasets show that StableMiss is robust and outperforms state-of-the-art methods under agnostic mask distribution shift.\n        \u25b3 Less\n      ",
    "title": "Prediction with Incomplete Data under Agnostic Mask Distribution Shift",
    "date": "18 May, 2023",
    "authors": [
      "Yichen Zhu",
      " Jian Yuan",
      " Bo Jiang",
      " Tao Lin",
      " Haiming Jin",
      " Xinbing Wang",
      " Chenghu Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10991",
    "paper_id": "2305.10991",
    "abstract": "\n        The softmax attention mechanism has emerged as a noteworthy development in the field of Artificial Intelligence research, building on the successes of Transformer-based architectures. However, their ever increasing sizes necessitate ever increasing computational memory, that limits their usage. We propose KgV, a sigmoid gating mechanism that, in conjunction with softmax attention, significantly boosts performance without increasing architecture size. To amend the size requirements, we leverage Tensor Chains to identify and prune the excess parameters. We find that such excess resides primarily within the embedding layer, and not in the output linear layer. To further improve embedding and significantly reduce parameters, we introduce H-SoftPOS, a hierarchical embedding layer which simultaneously enhances performance. Remarkably, on the WMT14 English-German validation set, our approach yields a threefold reduction in perplexity, surpassing the current state-of-the-art, while reducing parameter counts also by a factor of 3. When we further reduce the number of parameters up to sevenfold, we can still achieve a 21\\% decrease in perplexity with respect to the baseline Transformer. To understand generalization capabilities, we conduct experiments on the 7 language pairs of the WMT17 dataset. Our method outperforms existing techniques in terms of test loss while simultaneously halving the number of parameters. Moreover, we observe a 70 times reduction in variance with respect to the prior state-of-the-art. In conclusion, our proposed method yields significant improvements in performance and much lower memory cost. We call the resulting architecture Anthe.\n        \u25b3 Less\n      ",
    "title": "Less is More! A slim architecture for optimal language translation",
    "date": "18 May, 2023",
    "authors": [
      "Luca Herranz-Celotti",
      " Ermal Rrapaj"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10992",
    "paper_id": "2305.10992",
    "abstract": "\n        Masked language modeling (MLM) is a widely used self-supervised pretraining objective, where a model needs to predict an original token that is replaced with a mask given contexts. Although simpler and computationally efficient pretraining objectives, e.g., predicting the first character of a masked token, have recently shown comparable results to MLM, no objectives with a masking scheme actually outperform it in downstream tasks. Motivated by the assumption that their lack of complexity plays a vital role in the degradation, we validate whether more complex masked objectives can achieve better results and investigate how much complexity they should have to perform comparably to MLM. Our results using GLUE, SQuAD, and Universal Dependencies benchmarks demonstrate that more complicated objectives tend to show better downstream results with at least half of the MLM complexity needed to perform comparably to MLM. Finally, we discuss how we should pretrain a model using a masked objective from the task complexity perspective.\n        \u25b3 Less\n      ",
    "title": "How does the task complexity of masked pretraining objectives affect downstream performance?",
    "date": "18 May, 2023",
    "authors": [
      "Atsuki Yamaguchi",
      " Hiroaki Ozaki",
      " Terufumi Morishita",
      " Gaku Morio",
      " Yasuhiro Sogawa"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10997",
    "paper_id": "2305.10997",
    "abstract": "\n        Lifelong learning agents aim to learn multiple tasks sequentially over a lifetime. This involves the ability to exploit previous knowledge when learning new tasks and to avoid forgetting. Modulating masks, a specific type of parameter isolation approach, have recently shown promise in both supervised and reinforcement learning. While lifelong learning algorithms have been investigated mainly within a single-agent approach, a question remains on how multiple agents can share lifelong learning knowledge with each other. We show that the parameter isolation mechanism used by modulating masks is particularly suitable for exchanging knowledge among agents in a distributed and decentralized system of lifelong learners. The key idea is that the isolation of specific task knowledge to specific masks allows agents to transfer only specific knowledge on-demand, resulting in robust and effective distributed lifelong learning. We assume fully distributed and asynchronous scenarios with dynamic agent numbers and connectivity. An on-demand communication protocol ensures agents query their peers for specific masks to be transferred and integrated into their policies when facing each task. Experiments indicate that on-demand mask communication is an effective way to implement distributed lifelong reinforcement learning and provides a lifelong learning benefit with respect to distributed RL baselines such as DD-PPO, IMPALA, and PPO+EWC. The system is particularly robust to connection drops and demonstrates rapid learning due to knowledge exchange.\n        \u25b3 Less\n      ",
    "title": "Sharing Lifelong Reinforcement Learning Knowledge via Modulating Masks",
    "date": "18 May, 2023",
    "authors": [
      "Saptarshi Nath",
      " Christos Peridis",
      " Eseoghene Ben-Iwhiwhu",
      " Xinran Liu",
      " Shirin Dora",
      " Cong Liu",
      " Soheil Kolouri",
      " Andrea Soltoggio"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.02251",
    "paper_id": "2303.02251",
    "abstract": "\n        Recent work have demonstrated that robustness (to \"corruption\") can be at odds with generalization. Adversarial training, for instance, aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar \"robust overfitting\" phenomenon. Subsequently, we advance a novel distributionally robust loss function bridging robustness and generalization. We demonstrate both theoretically as well as empirically the loss to enjoy a certified level of robustness against two common types of corruption--data evasion and poisoning attacks--while ensuring guaranteed generalization. We show through careful numerical experiments that our resulting holistic robust (HR) training procedure yields SOTA performance. Finally, we indicate that HR training can be interpreted as a direct extension of adversarial training and comes with a negligible additional computational burden. A ready-to-use python library implementing our algorithm is available at https://github.com/RyanLucas3/HR_Neural_Networks.\n        \u25b3 Less\n      ",
    "title": "Certified Robust Neural Networks: Generalization and Corruption Resistance",
    "date": "18 May, 2023",
    "authors": [
      "Amine Bennouna",
      " Ryan Lucas",
      " Bart Van Parys"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11039",
    "paper_id": "2305.11039",
    "abstract": "\n        Recent advancements in artificial intelligence (AI) and machine learning (ML) algorithms, coupled with the availability of faster computing infrastructure, have enhanced the security posture of cybersecurity operations centers (defenders) through the development of ML-aided network intrusion detection systems (NIDS). Concurrently, the abilities of adversaries to evade security have also increased with the support of AI/ML models. Therefore, defenders need to proactively prepare for evasion attacks that exploit the detection mechanisms of NIDS. Recent studies have found that the perturbation of flow-based and packet-based features can deceive ML models, but these approaches have limitations. Perturbations made to the flow-based features are difficult to reverse-engineer, while samples generated with perturbations to the packet-based features are not playable.\n  Our methodological framework, Deep PackGen, employs deep reinforcement learning to generate adversarial packets and aims to overcome the limitations of approaches in the literature. By taking raw malicious network packets as inputs and systematically making perturbations on them, Deep PackGen camouflages them as benign packets while still maintaining their functionality. In our experiments, using publicly available data, Deep PackGen achieved an average adversarial success rate of 66.4\\% against various ML models and across different attack types. Our investigation also revealed that more than 45\\% of the successful adversarial samples were out-of-distribution packets that evaded the decision boundaries of the classifiers. The knowledge gained from our study on the adversary's ability to make specific evasive perturbations to different types of malicious packets can help defenders enhance the robustness of their NIDS against evolving adversarial attacks.\n        \u25b3 Less\n      ",
    "title": "Deep PackGen: A Deep Reinforcement Learning Framework for Adversarial Network Packet Generation",
    "date": "18 May, 2023",
    "authors": [
      "Soumyadeep Hore",
      " Jalal Ghadermazi",
      " Diwas Paudel",
      " Ankit Shah",
      " Tapas K. Das",
      " Nathaniel D. Bastian"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11040",
    "paper_id": "2305.11040",
    "abstract": "\n        The quantum perceptron, the variational circuit, and the Grover algorithm have been proposed as promising components for quantum machine learning. This paper presents a new quantum perceptron that combines the quantum variational circuit and the Grover algorithm. However, this does not guarantee that this quantum variational perceptron with Grover's algorithm (QVPG) will have any advantage over its quantum variational (QVP) and classical counterparts. Here, we examine the performance of QVP and QVP-G by computing their loss function and analyzing their accuracy on the classification task, then comparing these two quantum models to the classical perceptron (CP). The results show that our two quantum models are more efficient than CP, and our novel suggested model QVP-G outperforms the QVP, demonstrating that the Grover can be applied to the classification task and even makes the model more accurate, besides the unstructured search problems.\n        \u25b3 Less\n      ",
    "title": "Simulation of a Variational Quantum Perceptron using Grover's Algorithm",
    "date": "18 May, 2023",
    "authors": [
      "Nouhaila Innan",
      " Mohamed Bennai"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.10770",
    "paper_id": "2304.10770",
    "abstract": "\n        Exploration is a fundamental aspect of reinforcement learning (RL), and its effectiveness is a deciding factor in the performance of RL algorithms, especially when facing sparse extrinsic rewards. Recent studies have shown the effectiveness of encouraging exploration with intrinsic rewards estimated from novelties in observations. However, there is a gap between the novelty of an observation and an exploration, as both the stochasticity in the environment and the agent's behavior may affect the observation. To evaluate exploratory behaviors accurately, we propose DEIR, a novel method in which we theoretically derive an intrinsic reward with a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and then implement the reward with a discriminative forward model. Extensive experiments on both standard and advanced exploration tasks in MiniGrid show that DEIR quickly learns a better policy than the baselines. Our evaluations on ProcGen demonstrate both the generalization capability and the general applicability of our intrinsic reward. Our source code is available at https://github.com/swan-utokyo/deir.\n        \u25b3 Less\n      ",
    "title": "DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards",
    "date": "18 May, 2023",
    "authors": [
      "Shanchuan Wan",
      " Yujin Tang",
      " Yingtao Tian",
      " Tomoyuki Kaneko"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11051",
    "paper_id": "2305.11051",
    "abstract": "\n        Recently, an increasing interest in the management of water and health resources has been recorded. This interest is fed by the global sustainability challenges posed to the humanity that have water scarcity and quality at their core. Thus, the availability of effective, meaningful and open data is crucial to address those issues in the broader context of the Sustainable Development Goals of clean water and sanitation as targeted by the United Nations. In this paper, we present the Water Health Open Knowledge Graph (WHOW-KG) along with its design methodology and analysis on impact. WHOW-KG is a semantic knowledge graph that models data on water consumption, pollution, infectious disease rates and drug distribution. The WHOW-KG is developed in the context of the EU-funded WHOW (Water Health Open Knowledge) project and aims at supporting a wide range of applications: from knowledge discovery to decision-making, making it a valuable resource for researchers, policymakers, and practitioners in the water and health domains. The WHOW-KG consists of a network of five ontologies and related linked open data, modelled according to those ontologies.\n        \u25b3 Less\n      ",
    "title": "The Water Health Open Knowledge Graph",
    "date": "18 May, 2023",
    "authors": [
      "Gianluca Carletti",
      " Elio Giulianelli",
      " Anna Sofia Lippolis",
      " Giorgia Lodi",
      " Andrea Giovanni Nuzzolese",
      " Marco Picone",
      " Giulio Settanta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.17995",
    "paper_id": "2303.17995",
    "abstract": "\n        Entropy measures are effective features for time series classification problems. Traditional entropy measures, such as Shannon entropy, use probability distribution function. However, for the effective separation of time series, new entropy estimation methods are required to characterize the chaotic dynamic of the system. Our concept of Neural Network Entropy (NNetEn) is based on the classification of special datasets in relation to the entropy of the time series recorded in the reservoir of the neural network. NNetEn estimates the chaotic dynamics of time series in an original way and does not take into account probability distribution functions. We propose two new classification metrics: R2 Efficiency and Pearson Efficiency. The efficiency of NNetEn is verified on separation of two chaotic time series of sine mapping using dispersion analysis. For two close dynamic time series (r = 1.1918 and r = 1.2243), the F-ratio has reached the value of 124 and reflects high efficiency of the introduced method in classification problems. The electroenceph-alography signal classification for healthy persons and patients with Alzheimer disease illustrates the practical application of the NNetEn features. Our computations demonstrate the synergistic effect of increasing classification accuracy when applying traditional entropy measures and the NNetEn concept conjointly. An implementation of the algorithms in Python is presented.\n        \u25b3 Less\n      ",
    "title": "Neural Network Entropy (NNetEn): Entropy-Based EEG Signal and Chaotic Time Series Classification, Python Package for NNetEn Calculation",
    "date": "18 May, 2023",
    "authors": [
      "Andrei Velichko",
      " Maksim Belyaev",
      " Yuriy Izotov",
      " Murugappan Murugappan",
      " Hanif Heidari"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.10505",
    "paper_id": "2205.10505",
    "abstract": "\n        Transformer-based models have delivered impressive results on many tasks, particularly vision and language tasks. In many model training situations, conventional configurations are typically adopted. For example, we often set the base model with hidden dimensions (i.e. model width) to be 768 and the number of transformer layers (i.e. model depth) to be 12. In this paper, we revisit these conventional configurations. Through theoretical analysis and experimental evaluation, we show that the masked autoencoder is effective in alleviating the over-smoothing issue in deep transformer training. Based on this finding, we propose Bamboo, an idea of using deeper and narrower transformer configurations, for masked autoencoder training. On ImageNet, with such a simple change in configuration, re-designed model achieves 87.1% top-1 accuracy and outperforms SoTA models like MAE and BEiT. On language tasks, re-designed model outperforms BERT with default setting by 1.1 points on average, on GLUE datasets.\n        \u25b3 Less\n      ",
    "title": "A Study on Transformer Configuration and Training Objective",
    "date": "18 May, 2023",
    "authors": [
      "Fuzhao Xue",
      " Jianghai Chen",
      " Aixin Sun",
      " Xiaozhe Ren",
      " Zangwei Zheng",
      " Xiaoxin He",
      " Yongming Chen",
      " Xin Jiang",
      " Yang You"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11098",
    "paper_id": "2305.11098",
    "abstract": "\n        Statistical learning and logical reasoning are two major fields of AI expected to be unified for human-like machine intelligence. Most existing work considers how to combine existing logical and statistical systems. However, there is no theory of inference so far explaining how basic approaches to statistical learning and logical reasoning stem from a common principle. Inspired by the fact that much empirical work in neuroscience suggests Bayesian (or probabilistic generative) approaches to brain function including learning and reasoning, we here propose a simple Bayesian model of logical reasoning and statistical learning. The theory is statistically correct as it satisfies Kolmogorov's axioms, is consistent with both Fenstad's representation theorem and maximum likelihood estimation and performs exact Bayesian inference with a linear-time complexity. The theory is logically correct as it is a data-driven generalisation of uncertain reasoning from consistency, possibility, inconsistency and impossibility. The theory is correct in terms of machine learning as its solution to generation and prediction tasks on the MNIST dataset is not only empirically reasonable but also theoretically correct against the K nearest neighbour method. We simply model how data causes symbolic knowledge in terms of its satisfiability in formal logic. Symbolic reasoning emerges as a result of the process of going the causality forwards and backwards. The forward and backward processes correspond to an interpretation and inverse interpretation in formal logic, respectively. The inverse interpretation differentiates our work from the mainstream often referred to as inverse entailment, inverse deduction or inverse resolution. The perspective gives new insights into learning and reasoning towards human-like machine intelligence.\n        \u25b3 Less\n      ",
    "title": "A Simple Generative Model of Logical Reasoning and Statistical Learning",
    "date": "18 May, 2023",
    "authors": [
      "Hiroyuki Kido"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11107",
    "paper_id": "2305.11107",
    "abstract": "\n        In motor neuroscience, artificial recurrent neural networks models often complement animal studies. However, most modeling efforts are limited to data-fitting, and the few that examine virtual embodied agents in a reinforcement learning context, do not draw direct comparisons to their biological counterparts. Our study addressing this gap, by uncovering structured neural activity of a virtual robot performing legged locomotion that directly support experimental findings of primate walking and cycling. We find that embodied agents trained to walk exhibit smooth dynamics that avoid tangling -- or opposing neural trajectories in neighboring neural space -- a core principle in computational neuroscience. Specifically, across a wide suite of gaits, the agent displays neural trajectories in the recurrent layers are less tangled than those in the input-driven actuation layers. To better interpret the neural separation of these elliptical-shaped trajectories, we identify speed axes that maximizes variance of mean activity across different forward, lateral, and rotational speed conditions.\n        \u25b3 Less\n      ",
    "title": "From Data-Fitting to Discovery: Interpreting the Neural Dynamics of Motor Control through Reinforcement Learning",
    "date": "18 May, 2023",
    "authors": [
      "Eugene R. Rush",
      " Kaushik Jayaram",
      " J. Sean Humbert"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09557",
    "paper_id": "2305.09557",
    "abstract": "\n        Protecting user privacy is a major concern for many machine learning systems that are deployed at scale and collect from a diverse set of population. One way to address this concern is by collecting and releasing data labels in an aggregated manner so that the information about a single user is potentially combined with others. In this paper, we explore the possibility of training machine learning models with aggregated data labels, rather than individual labels. Specifically, we consider two natural aggregation procedures suggested by practitioners: curated bags where the data points are grouped based on common features and random bags where the data points are grouped randomly in bag of similar sizes. For the curated bag setting and for a broad range of loss functions, we show that we can perform gradient-based learning without any degradation in performance that may result from aggregating data. Our method is based on the observation that the sum of the gradients of the loss function on individual data examples in a curated bag can be computed from the aggregate label without the need for individual labels. For the random bag setting, we provide a generalization risk bound based on the Rademacher complexity of the hypothesis class and show how empirical risk minimization can be regularized to achieve the smallest risk bound. In fact, in the random bag setting, there is a trade-off between size of the bag and the achievable error rate as our bound indicates. Finally, we conduct a careful empirical study to confirm our theoretical findings. In particular, our results suggest that aggregate learning can be an effective method for preserving user privacy while maintaining model accuracy.\n        \u25b3 Less\n      ",
    "title": "Learning from Aggregated Data: Curated Bags versus Random Bags",
    "date": "18 May, 2023",
    "authors": [
      "Lin Chen",
      " Gang Fu",
      " Amin Karbasi",
      " Vahab Mirrokni"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11125",
    "paper_id": "2305.11125",
    "abstract": "\n        Cancerous skin lesions are one of the most common malignancies detected in humans, and if not detected at an early stage, they can lead to death. Therefore, it is crucial to have access to accurate results early on to optimize the chances of survival. Unfortunately, accurate results are typically obtained by highly trained dermatologists, who may not be accessible to many people, particularly in low-income and middle-income countries. Artificial Intelligence (AI) appears to be a potential solution to this problem, as it has proven to provide equal or even better diagnoses than healthcare professionals. This project aims to address the issue by collecting state-of-the-art techniques for image classification from various fields and implementing them. Some of these techniques include mixup, presizing, and test-time augmentation, among others. Three architectures were used for the implementation: DenseNet121, VGG16 with batch normalization, and ResNet50. The models were designed with two main purposes. First, to classify images into seven categories, including melanocytic nevus, melanoma, benign keratosis-like lesions, basal cell carcinoma, actinic keratoses and intraepithelial carcinoma, vascular lesions, and dermatofibroma. Second, to classify images into benign or malignant. The models were trained using a dataset of 8012 images, and their performance was evaluated using 2003 images. It's worth noting that this model is trained end-to-end, directly from the image to the labels, without the need for handcrafted feature extraction.\n        \u25b3 Less\n      ",
    "title": "Skin Lesion Diagnosis Using Convolutional Neural Networks",
    "date": "18 May, 2023",
    "authors": [
      "Daniel Alonso Villanueva Nunez",
      " Yongmin Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11137",
    "paper_id": "2305.11137",
    "abstract": "\n        What are the computational foundations of social grouping? Traditional approaches to this question have focused on verbal reasoning or simple (low-dimensional) quantitative models. In the real world, however, social preferences emerge when high-dimensional learning systems (brains and bodies) interact with high-dimensional sensory inputs during an animal's embodied interactions with the world. A deep understanding of social grouping will therefore require embodied models that learn directly from sensory inputs using high-dimensional learning mechanisms. To this end, we built artificial neural networks (ANNs), embodied those ANNs in virtual fish bodies, and raised the artificial fish in virtual fish tanks that mimicked the rearing conditions of real fish. We then compared the social preferences that emerged in real fish versus artificial fish. We found that when artificial fish had two core learning mechanisms (reinforcement learning and curiosity-driven learning), artificial fish developed fish-like social preferences. Like real fish, the artificial fish spontaneously learned to prefer members of their own group over members of other groups. The artificial fish also spontaneously learned to self-segregate with their in-group, akin to self-segregation behavior seen in nature. Our results suggest that social grouping can emerge from three ingredients: (1) reinforcement learning, (2) intrinsic motivation, and (3) early social experiences with in-group members. This approach lays a foundation for reverse engineering animal-like social behavior with image-computable models, bridging the divide between high-dimensional sensory inputs and social preferences.\n        \u25b3 Less\n      ",
    "title": "Parallel development of social preferences in fish and machines",
    "date": "18 May, 2023",
    "authors": [
      "Joshua McGraw",
      " Donsuk Lee",
      " Justin Wood"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11206",
    "paper_id": "2305.11206",
    "abstract": "\n        Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.\n        \u25b3 Less\n      ",
    "title": "LIMA: Less Is More for Alignment",
    "date": "18 May, 2023",
    "authors": [
      "Chunting Zhou",
      " Pengfei Liu",
      " Puxin Xu",
      " Srini Iyer",
      " Jiao Sun",
      " Yuning Mao",
      " Xuezhe Ma",
      " Avia Efrat",
      " Ping Yu",
      " Lili Yu",
      " Susan Zhang",
      " Gargi Ghosh",
      " Mike Lewis",
      " Luke Zettlemoyer",
      " Omer Levy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11170",
    "paper_id": "2305.11170",
    "abstract": "\n        The primary way of building AI applications is shifting from training specialist models to prompting generalist models. A common practice for prompting generalist models, often referred to as in-context learning, is to append a few examples (demonstrations) to the prompt to help the model better understand the task. While effective, in-context learning can be inefficient because it makes the input prompt much longer, consuming valuable space in the context window and leading to larger computational costs. In this paper, we propose DynaICL, a recipe for efficient prompting with black-box generalist models that dynamically allocate in-context examples according to the input complexity and the computational budget. To achieve this, we train a meta controller that predicts the number of in-context examples suitable for the generalist model to make a good prediction based on the performance-efficiency trade-off for a specific input. We then dynamically allocate the number of demonstrations for an input according to predictions from the meta controller and the given computation budget. Experimental results show that dynamic example allocation helps achieve a better performance-efficiency trade-off in two practical settings where computational resources or the required performance is constrained. Specifically, DynaICL saves up to 46% token budget compared to the common practice that allocates the same number of in-context examples to each input. We also find that a meta controller trained on a certain backbone model and tasks can successfully generalize to unseen models and tasks.\n        \u25b3 Less\n      ",
    "title": "Efficient Prompting via Dynamic In-Context Learning",
    "date": "18 May, 2023",
    "authors": [
      "Wangchunshu Zhou",
      " Yuchen Eleanor Jiang",
      " Ryan Cotterell",
      " Mrinmaya Sachan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.07690",
    "paper_id": "2301.07690",
    "abstract": "\n        Robotic systems have subsystems with a combinatorially large configuration space and hundreds or thousands of possible software and hardware configuration options interacting non-trivially. The configurable parameters are set to target specific objectives, but they can cause functional faults when incorrectly configured. Finding the root cause of such faults is challenging due to the exponentially large configuration space and the dependencies between the robot's configuration settings and performance. This paper proposes CaRE -- a method for diagnosing the root cause of functional faults through the lens of causality. CaRE abstracts the causal relationships between various configuration options and the robot's performance objectives by learning a causal structure and estimating the causal effects of options on robot performance indicators. We demonstrate CaRE's efficacy by finding the root cause of the observed functional faults and validating the diagnosed root cause by conducting experiments in both physical robots (Husky and Turtlebot 3) and in simulation (Gazebo). Furthermore, we demonstrate that the causal models learned from robots in simulation (e.g., Husky in Gazebo) are transferable to physical robots across different platforms (e.g., Husky and Turtlebot 3).\n        \u25b3 Less\n      ",
    "title": "CaRE: Finding Root Causes of Configuration Issues in Highly-Configurable Robots",
    "date": "18 May, 2023",
    "authors": [
      "Md Abir Hossen",
      " Sonam Kharade",
      " Bradley Schmerl",
      " Javier C\u00e1mara",
      " Jason M. O'Kane",
      " Ellen C. Czaplinski",
      " Katherine A. Dzurilla",
      " David Garlan",
      " Pooyan Jamshidi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11236",
    "paper_id": "2305.11236",
    "abstract": "\n        The majority of work in privacy-preserving federated learning (FL) has been focusing on horizontally partitioned datasets where clients share the same sets of features and can train complete models independently. However, in many interesting problems, such as financial fraud detection and disease detection, individual data points are scattered across different clients/organizations in vertical federated learning. Solutions for this type of FL require the exchange of gradients between participants and rarely consider privacy and security concerns, posing a potential risk of privacy leakage. In this work, we present a novel design for training vertical FL securely and efficiently using state-of-the-art security modules for secure aggregation. We demonstrate empirically that our method does not impact training performance whilst obtaining 9.1e2 ~3.8e4 speedup compared to homomorphic encryption (HE).\n        \u25b3 Less\n      ",
    "title": "Efficient Vertical Federated Learning with Secure Aggregation",
    "date": "18 May, 2023",
    "authors": [
      "Xinchi Qiu",
      " Heng Pan",
      " Wanru Zhao",
      " Chenyang Ma",
      " Pedro Porto Buarque de Gusm\u00e3o",
      " Nicholas D. Lane"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18312",
    "paper_id": "2305.18312",
    "abstract": "\n        Computerized adaptive testing (CAT) is a form of personalized testing that accurately measures students' knowledge levels while reducing test length. Bilevel optimization-based CAT (BOBCAT) is a recent framework that learns a data-driven question selection algorithm to effectively reduce test length and improve test accuracy. However, it suffers from high question exposure and test overlap rates, which potentially affects test security. This paper introduces a constrained version of BOBCAT to address these problems by changing its optimization setup and enabling us to trade off test accuracy for question exposure and test overlap rates. We show that C-BOBCAT is effective through extensive experiments on two real-world adult testing datasets.\n        \u25b3 Less\n      ",
    "title": "Balancing Test Accuracy and Security in Computerized Adaptive Testing",
    "date": "18 May, 2023",
    "authors": [
      "Wanyong Feng",
      " Aritra Ghosh",
      " Stephen Sireci",
      " Andrew S. Lan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11252",
    "paper_id": "2305.11252",
    "abstract": "\n        Artificial neural networks (ANNs) have emerged as an essential tool in machine learning, achieving remarkable success across diverse domains, including image and speech generation, game playing, and robotics. However, there exist fundamental differences between ANNs' operating mechanisms and those of the biological brain, particularly concerning learning processes. This paper presents a comprehensive review of current brain-inspired learning representations in artificial neural networks. We investigate the integration of more biologically plausible mechanisms, such as synaptic plasticity, to enhance these networks' capabilities. Moreover, we delve into the potential advantages and challenges accompanying this approach. Ultimately, we pinpoint promising avenues for future research in this rapidly advancing field, which could bring us closer to understanding the essence of intelligence.\n        \u25b3 Less\n      ",
    "title": "Brain-inspired learning in artificial neural networks: a review",
    "date": "18 May, 2023",
    "authors": [
      "Samuel Schmidgall",
      " Jascha Achterberg",
      " Thomas Miconi",
      " Louis Kirsch",
      " Rojin Ziaei",
      " S. Pardis Hajiseyedrazi",
      " Jason Eshraghian"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11254",
    "paper_id": "2305.11254",
    "abstract": "\n        A generalized strategy for the design of intelligent robust control systems based on quantum / soft computing technologies is described. The reliability of hybrid intelligent controllers increase by providing the ability to self-organize of imperfect knowledge bases. The main attention is paid to increasing the level of robustness of intelligent control systems in unpredictable control situations with the demonstration by illustrative examples. A SW & HW platform and support tools for a supercomputer accelerator for modeling quantum algorithms on a classical computer are described.\n        \u25b3 Less\n      ",
    "title": "Robust Quantum Controllers: Quantum Information -- Thermodynamic Hidden Force Control in Intelligent Robotics based on Quantum Soft Computing",
    "date": "18 May, 2023",
    "authors": [
      "Sergey V. Ulyanov",
      " Viktor S. Ulyanov",
      " Takakhide Hagiwara"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.03000",
    "paper_id": "2212.03000",
    "abstract": "\n        Objective: We aim to develop an open-source natural language processing (NLP) package, SODA (i.e., SOcial DeterminAnts), with pre-trained transformer models to extract social determinants of health (SDoH) for cancer patients, examine the generalizability of SODA to a new disease domain (i.e., opioid use), and evaluate the extraction rate of SDoH using cancer populations.\n  Methods: We identified SDoH categories and attributes and developed an SDoH corpus using clinical notes from a general cancer cohort. We compared four transformer-based NLP models to extract SDoH, examined the generalizability of NLP models to a cohort of patients prescribed with opioids, and explored customization strategies to improve performance. We applied the best NLP model to extract 19 categories of SDoH from the breast (n=7,971), lung (n=11,804), and colorectal cancer (n=6,240) cohorts.\n  Results and Conclusion: We developed a corpus of 629 cancer patients notes with annotations of 13,193 SDoH concepts/attributes from 19 categories of SDoH. The Bidirectional Encoder Representations from Transformers (BERT) model achieved the best strict/lenient F1 scores of 0.9216 and 0.9441 for SDoH concept extraction, 0.9617 and 0.9626 for linking attributes to SDoH concepts. Fine-tuning the NLP models using new annotations from opioid use patients improved the strict/lenient F1 scores from 0.8172/0.8502 to 0.8312/0.8679. The extraction rates among 19 categories of SDoH varied greatly, where 10 SDoH could be extracted from >70% of cancer patients, but 9 SDoH had a low extraction rate (<70% of cancer patients). The SODA package with pre-trained transformer models is publicly available at https://github.com/uf-hobiinformatics-lab/SDoH_SODA.\n        \u25b3 Less\n      ",
    "title": "SODA: A Natural Language Processing Package to Extract Social Determinants of Health for Cancer Studies",
    "date": "18 May, 2023",
    "authors": [
      "Zehao Yu",
      " Xi Yang",
      " Chong Dang",
      " Prakash Adekkanattu",
      " Braja Gopal Patra",
      " Yifan Peng",
      " Jyotishman Pathak",
      " Debbie L. Wilson",
      " Ching-Yuan Chang",
      " Wei-Hsuan Lo-Ciganic",
      " Thomas J. George",
      " William R. Hogan",
      " Yi Guo",
      " Jiang Bian",
      " Yonghui Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.17707",
    "paper_id": "2303.17707",
    "abstract": "\n        Setting proper evaluation objectives for explainable artificial intelligence (XAI) is vital for making XAI algorithms follow human communication norms, support human reasoning processes, and fulfill human needs for AI explanations. In this position paper, we examine the most pervasive human-grounded concept in XAI evaluation, explanation plausibility. Plausibility measures how reasonable the machine explanation is compared to the human explanation. Plausibility has been conventionally formulated as an important evaluation objective for AI explainability tasks. We argue against this idea, and show how optimizing and evaluating XAI for plausibility is sometimes harmful, and always ineffective in achieving model understandability, transparency, and trustworthiness. Specifically, evaluating XAI algorithms for plausibility regularizes the machine explanation to express exactly the same content as human explanation, which deviates from the fundamental motivation for humans to explain: expressing similar or alternative reasoning trajectories while conforming to understandable forms or language. Optimizing XAI for plausibility regardless of the model decision correctness also jeopardizes model trustworthiness, because doing so breaks an important assumption in human-human explanation that plausible explanations typically imply correct decisions, and vice versa; and violating this assumption eventually leads to either undertrust or overtrust of AI models. Instead of being the end goal in XAI evaluation, plausibility can serve as an intermediate computational proxy for the human process of interpreting explanations to optimize the utility of XAI. We further highlight the importance of explainability-specific evaluation objectives by differentiating the AI explanation task from the object localization task.\n        \u25b3 Less\n      ",
    "title": "The XAI Alignment Problem: Rethinking How Should We Evaluate Human-Centered AI Explainability Techniques",
    "date": "18 May, 2023",
    "authors": [
      "Weina Jin",
      " Xiaoxiao Li",
      " Ghassan Hamarneh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.01927",
    "paper_id": "2205.01927",
    "abstract": "\n        Learning multi-agent dynamics is a core AI problem with broad applications in robotics and autonomous driving. While most existing works focus on deterministic prediction, producing probabilistic forecasts to quantify uncertainty and assess risks is critical for downstream decision-making tasks such as motion planning and collision avoidance. Multi-agent dynamics often contains internal symmetry. By leveraging symmetry, specifically rotation equivariance, we can improve not only the prediction accuracy but also uncertainty calibration. We introduce Energy Score, a proper scoring rule, to evaluate probabilistic predictions. We propose a novel deep dynamics model, Probabilistic Equivariant Continuous COnvolution (PECCO) for probabilistic prediction of multi-agent trajectories. PECCO extends equivariant continuous convolution to model the joint velocity distribution of multiple agents. It uses dynamics integration to propagate the uncertainty from velocity to position. On both synthetic and real-world datasets, PECCO shows significant improvements in accuracy and calibration compared to non-equivariant baselines.\n        \u25b3 Less\n      ",
    "title": "Probabilistic Symmetry for Multi-Agent Dynamics",
    "date": "18 May, 2023",
    "authors": [
      "Sophia Sun",
      " Robin Walters",
      " Jinxi Li",
      " Rose Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11271",
    "paper_id": "2305.11271",
    "abstract": "\n        Collaborative tasks often begin with partial task knowledge and incomplete initial plans from each partner. To complete these tasks, agents need to engage in situated communication with their partners and coordinate their partial plans towards a complete plan to achieve a joint task goal. While such collaboration seems effortless in a human-human team, it is highly challenging for human-AI collaboration. To address this limitation, this paper takes a step towards collaborative plan acquisition, where humans and agents strive to learn and communicate with each other to acquire a complete plan for joint tasks. Specifically, we formulate a novel problem for agents to predict the missing task knowledge for themselves and for their partners based on rich perceptual and dialogue history. We extend a situated dialogue benchmark for symmetric collaborative tasks in a 3D blocks world and investigate computational strategies for plan acquisition. Our empirical results suggest that predicting the partner's missing knowledge is a more viable approach than predicting one's own. We show that explicit modeling of the partner's dialogue moves and mental states produces improved and more stable results than without. These results provide insight for future AI agents that can predict what knowledge their partner is missing and, therefore, can proactively communicate such information to help their partner acquire such missing knowledge toward a common understanding of joint tasks.\n        \u25b3 Less\n      ",
    "title": "Towards Collaborative Plan Acquisition through Theory of Mind Modeling in Situated Dialogue",
    "date": "18 May, 2023",
    "authors": [
      "Cristian-Paul Bara",
      " Ziqiao Ma",
      " Yingzhuo Yu",
      " Julie Shah",
      " Joyce Chai"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11294",
    "paper_id": "2305.11294",
    "abstract": "\n        The proposed approach is to formalise the probabilistic puzzle in equational FOL. Two formalisations are needed: one theory for all models of the given puzzle, and a second theory for the favorable models. Then Mace4 - that computes all the interpretation models of a FOL theory - is called twice. First, it is asked to compute all the possible models M p .Second, the additional constraint is added, and Mace4 computes only favourabile models M f. Finally, the definition of probability is applied: the number of favorable models is divided by the number of possible models. The proposed approach equips students from the logic tribe to find the correct solution for puzzles from the probabilitistic tribe, by using their favourite instruments: modelling and formalisation. I have exemplified here five probabilistic puzzles and how they can be solved by translating the min FOL and then find the corresponding interpretation models. Mace4 was the tool of choice here. Ongoing work is investigating the limits of this method on various collections of probabilistic puzzles\n        \u25b3 Less\n      ",
    "title": "Solving probability puzzles with logic toolkit",
    "date": "18 May, 2023",
    "authors": [
      "Adrian Groza"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09062",
    "paper_id": "2305.09062",
    "abstract": "\n        Few-shot learning is a challenging area of research that aims to learn new concepts with only a few labeled samples of data. Recent works based on metric-learning approaches leverage the meta-learning approach, which is encompassed by episodic tasks that make use a support (training) and query set (test) with the objective of learning a similarity comparison metric between those sets. Due to the lack of data, the learning process of the embedding network becomes an important part of the few-shot task. Previous works have addressed this problem using metric learning approaches, but the properties of the underlying latent space and the separability of the difference classes on it was not entirely enforced. In this work, we propose two different loss functions which consider the importance of the embedding vectors by looking at the intra-class and inter-class distance between the few data. The first loss function is the Proto-Triplet Loss, which is based on the original triplet loss with the modifications needed to better work on few-shot scenarios. The second loss function, which we dub ICNN loss is based on an inter and intra class nearest neighbors score, which help us to assess the quality of embeddings obtained from the trained network. Our results, obtained from a extensive experimental setup show a significant improvement in accuracy in the miniImagenNet benchmark compared to other metric-based few-shot learning methods by a margin of 2%, demonstrating the capability of these loss functions to allow the network to generalize better to previously unseen classes. In our experiments, we demonstrate competitive generalization capabilities to other domains, such as the Caltech CUB, Dogs and Cars datasets compared with the state of the art.\n        \u25b3 Less\n      ",
    "title": "SuSana Distancia is all you need: Enforcing class separability in metric learning via two novel distance-based loss functions for few-shot image classification",
    "date": "18 May, 2023",
    "authors": [
      "Mauricio Mendez-Ruiz",
      " Jorge Gonzalez-Zapata",
      " Ivan Reyes-Amezcua",
      " Daniel Flores-Araiza",
      " Francisco Lopez-Tiro",
      " Andres Mendez-Vazquez",
      " Gilberto Ochoa-Ruiz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11308",
    "paper_id": "2305.11308",
    "abstract": "\n        We introduce Multi-Objective Counterfactuals for Design (MCD), a novel method for counterfactual optimization in design problems. Counterfactuals are hypothetical situations that can lead to a different decision or choice. In this paper, the authors frame the counterfactual search problem as a design recommendation tool that can help identify modifications to a design, leading to better functional performance. MCD improves upon existing counterfactual search methods by supporting multi-objective queries, which are crucial in design problems, and by decoupling the counterfactual search and sampling processes, thus enhancing efficiency and facilitating objective tradeoff visualization. The paper demonstrates MCD's core functionality using a two-dimensional test case, followed by three case studies of bicycle design that showcase MCD's effectiveness in real-world design problems. In the first case study, MCD excels at recommending modifications to query designs that can significantly enhance functional performance, such as weight savings and improvements to the structural safety factor. The second case study demonstrates that MCD can work with a pre-trained language model to suggest design changes based on a subjective text prompt effectively. Lastly, the authors task MCD with increasing a query design's similarity to a target image and text prompt while simultaneously reducing weight and improving structural performance, demonstrating MCD's performance on a complex multimodal query. Overall, MCD has the potential to provide valuable recommendations for practitioners and design automation researchers looking for answers to their ``What if'' questions by exploring hypothetical design modifications and their impact on multiple design objectives. The code, test problems, and datasets used in the paper are available to the public at decode.mit.edu/projects/counterfactuals/.\n        \u25b3 Less\n      ",
    "title": "Counterfactuals for Design: A Model-Agnostic Method For Design Recommendations",
    "date": "18 May, 2023",
    "authors": [
      "Lyle Regenwetter",
      " Yazan Abu Obaideh",
      " Faez Ahmed"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11311",
    "paper_id": "2305.11311",
    "abstract": "\n        In recent years, understanding the decision-making process of black-box models has become not only a legal requirement but also an additional way to assess their performance. However, the state of the art post-hoc interpretation approaches rely on synthetic data generation. This introduces uncertainty and can hurt the reliability of the interpretations. Furthermore, they tend to produce explanations that apply to only very few data points. This makes the explanations brittle and limited in scope. Finally, they provide scores that have no direct verifiable meaning. In this paper, we present BELLA, a deterministic model-agnostic post-hoc approach for explaining the individual predictions of regression black-box models. BELLA provides explanations in the form of a linear model trained in the feature space. Thus, its coefficients can be used directly to compute the predicted value from the feature values. Furthermore, BELLA maximizes the size of the neighborhood to which the linear model applies, so that the explanations are accurate, simple, general, and robust. BELLA can produce both factual and counterfactual explanations. Our user study confirms the importance of the desiderata we optimize, and our experiments show that BELLA outperforms the state-of-the-art approaches on these desiderata.\n        \u25b3 Less\n      ",
    "title": "BELLA: Black box model Explanations by Local Linear Approximations",
    "date": "18 May, 2023",
    "authors": [
      "Nedeljko Radulovic",
      " Albert Bifet",
      " Fabian Suchanek"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11320",
    "paper_id": "2305.11320",
    "abstract": "\n        This paper presents a parameter-efficient learning (PEL) to develop a low-resource accent adaptation for text-to-speech (TTS). A resource-efficient adaptation from a frozen pre-trained TTS model is developed by using only 1.2\\% to 0.8\\% of original trainable parameters to achieve competitive performance in voice synthesis. Motivated by a theoretical foundation of optimal transport (OT), this study carries out PEL for TTS where an auxiliary unsupervised loss based on OT is introduced to maximize a difference between the pre-trained source domain and the (unseen) target domain, in addition to its supervised training loss. Further, we leverage upon this unsupervised loss refinement to boost system performance via either sliced Wasserstein distance or maximum mean discrepancy. The merit of this work is demonstrated by fulfilling PEL solutions based on residual adapter learning, and model reprogramming when evaluating the Mandarin accent adaptation. Experiment results show that the proposed methods can achieve competitive naturalness with parameter-efficient decoder fine-tuning, and the auxiliary unsupervised loss improves model performance empirically.\n        \u25b3 Less\n      ",
    "title": "Parameter-Efficient Learning for Text-to-Speech Accent Adaptation",
    "date": "18 May, 2023",
    "authors": [
      "Li-Jen Yang",
      " Chao-Han Huck Yang",
      " Jen-Tzung Chien"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11334",
    "paper_id": "2305.11334",
    "abstract": "\n        We introduce two novel methods, Tree-Search and Self-contextualizing QA, designed to enhance the performance of large language models (LLMs) in question-answering tasks. Tree-Search is a sampling technique specifically created to extract diverse information from an LLM for a given prompt. Self-contextualizing QA leverages Tree-Search to enable the model to create its own context using a wide range of information relevant to the prompt, evaluate it explicitly and return a open book answer to the initial prompt . We demonstrate that the quality of generated answers improves according to various metrics, including accuracy, informativeness, coherence, and consistency, as evaluated by GPT3.5(text-davinci-003). Furthermore, we show that our methods result in increased robustness and that performance is positively correlated with tree size, benefiting both answer quality and robustness. Finally, we discuss other promising applications of Tree-Search, highlighting its potential to enhance a broad range of tasks beyond question-answering.\n  \\noindent We also discuss several areas for future work, including refining the Tree-Search and Self-Contextualizing QA methods, improving the coherence of the generated context, and investigating the impact of bootstrapping on model robustness\n        \u25b3 Less\n      ",
    "title": "Writing your own book: A method for going from closed to open book QA to improve robustness and performance of smaller LLMs",
    "date": "18 May, 2023",
    "authors": [
      "Giorgi Kokaia",
      " Pratyush Sinha",
      " Yutong Jiang",
      " Nozha Boujemaa"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11347",
    "paper_id": "2305.11347",
    "abstract": "\n        In overhead image segmentation tasks, including additional spectral bands beyond the traditional RGB channels can improve model performance. However, it is still unclear how incorporating this additional data impacts model robustness to adversarial attacks and natural perturbations. For adversarial robustness, the additional information could improve the model's ability to distinguish malicious inputs, or simply provide new attack avenues and vulnerabilities. For natural perturbations, the additional information could better inform model decisions and weaken perturbation effects or have no significant influence at all. In this work, we seek to characterize the performance and robustness of a multispectral (RGB and near infrared) image segmentation model subjected to adversarial attacks and natural perturbations. While existing adversarial and natural robustness research has focused primarily on digital perturbations, we prioritize on creating realistic perturbations designed with physical world conditions in mind. For adversarial robustness, we focus on data poisoning attacks whereas for natural robustness, we focus on extending ImageNet-C common corruptions for fog and snow that coherently and self-consistently perturbs the input data. Overall, we find both RGB and multispectral models are vulnerable to data poisoning attacks regardless of input or fusion architectures and that while physically realizable natural perturbations still degrade model performance, the impact differs based on fusion architecture and input data.\n        \u25b3 Less\n      ",
    "title": "Quantifying the robustness of deep multispectral segmentation models against natural perturbations and data poisoning",
    "date": "18 May, 2023",
    "authors": [
      "Elise Bishoff",
      " Charles Godfrey",
      " Myles McKay",
      " Eleanor Byler"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.02920",
    "paper_id": "2303.02920",
    "abstract": "\n        [Context] Artificial intelligence (AI) components used in building software solutions have substantially increased in recent years. However, many of these solutions focus on technical aspects and ignore critical human-centered aspects. [Objective] Including human-centered aspects during requirements engineering (RE) when building AI-based software can help achieve more responsible, unbiased, and inclusive AI-based software solutions. [Method] In this paper, we present a new framework developed based on human-centered AI guidelines and a user survey to aid in collecting requirements for human-centered AI-based software. We provide a catalog to elicit these requirements and a conceptual model to present them visually. [Results] The framework is applied to a case study to elicit and model requirements for enhancing the quality of 360 degree~videos intended for virtual reality (VR) users. [Conclusion] We found that our proposed approach helped the project team fully understand the human-centered needs of the project to deliver. Furthermore, the framework helped to understand what requirements need to be captured at the initial stages against later stages in the engineering process of AI-based software.\n        \u25b3 Less\n      ",
    "title": "Requirements Engineering Framework for Human-centered Artificial Intelligence Software Systems",
    "date": "18 May, 2023",
    "authors": [
      "Khlood Ahmad",
      " Mohamed Abdelrazek",
      " Chetan Arora",
      " Arbind Agrahari Baniya",
      " Muneera Bano",
      " John Grundy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11353",
    "paper_id": "2305.11353",
    "abstract": "\n        This article proposes a meta-learning method for estimating the conditional average treatment effect (CATE) from a few observational data. The proposed method learns how to estimate CATEs from multiple tasks and uses the knowledge for unseen tasks. In the proposed method, based on the meta-learner framework, we decompose the CATE estimation problem into sub-problems. For each sub-problem, we formulate our estimation models using neural networks with task-shared and task-specific parameters. With our formulation, we can obtain optimal task-specific parameters in a closed form that are differentiable with respect to task-shared parameters, making it possible to perform effective meta-learning. The task-shared parameters are trained such that the expected CATE estimation performance in few-shot settings is improved by minimizing the difference between a CATE estimated with a large amount of data and one estimated with just a few data. Our experimental results demonstrate that our method outperforms the existing meta-learning approaches and CATE estimation methods.\n        \u25b3 Less\n      ",
    "title": "Meta-learning for heterogeneous treatment effect estimation with closed-form solvers",
    "date": "18 May, 2023",
    "authors": [
      "Tomoharu Iwata",
      " Yoichi Chikahara"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.00899",
    "paper_id": "2301.00899",
    "abstract": "\n        Deep reinforcement learning has considerable potential to improve irrigation scheduling in many cropping systems by applying adaptive amounts of water based on various measurements over time. The goal is to discover an intelligent decision rule that processes information available to growers and prescribes sensible irrigation amounts for the time steps considered. Due to the technical novelty, however, the research on the technique remains sparse and impractical. To accelerate the progress, the paper proposes a principled framework and actionable procedure that allow researchers to formulate their own optimisation problems and implement solution algorithms based on deep reinforcement learning. The effectiveness of the framework was demonstrated using a case study of irrigated wheat grown in a productive region of Australia where profits were maximised. Specifically, the decision rule takes nine state variable inputs: crop phenological stage, leaf area index, extractable soil water for each of the five top layers, cumulative rainfall and cumulative irrigation. It returns a probabilistic prescription over five candidate irrigation amounts (0, 10, 20, 30 and 40 mm) every day. The production system was simulated at Goondiwindi using the APSIM-Wheat crop model. After training in the learning environment using 1981-2010 weather data, the learned decision rule was tested individually for each year of 2011-2020. The results were compared against the benchmark profits obtained by a conventional rule common in the region. The discovered decision rule prescribed daily irrigation amounts that uniformly improved on the conventional rule for all the testing years, and the largest improvement reached 17% in 2018. The framework is general and applicable to a wide range of cropping systems with realistic optimisation problems.\n        \u25b3 Less\n      ",
    "title": "Deep reinforcement learning for irrigation scheduling using high-dimensional sensor feedback",
    "date": "18 May, 2023",
    "authors": [
      "Yuji Saikai",
      " Allan Peake",
      " Karine Chenu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08285",
    "paper_id": "2305.08285",
    "abstract": "\n        The increasing size of language models raises great research interests in parameter-efficient fine-tuning such as LoRA that freezes the pre-trained model, and injects small-scale trainable parameters for multiple downstream tasks (e.g., summarization, question answering and translation). To further enhance the efficiency of fine-tuning, we propose a framework that integrates LoRA and structured layer pruning. The integrated framework is validated on two created deidentified medical report summarization datasets based on MIMIC-IV-Note and two public medical dialogue datasets. By tuning 0.6% parameters of the original model and pruning over 30% Transformer-layers, our framework can reduce 50% of GPU memory usage and speed up 100% of the training phase, while preserving over 92% generation qualities on free-text sequence-to-sequence tasks.\n        \u25b3 Less\n      ",
    "title": "Parameter-Efficient Fine-Tuning with Layer Pruning on Free-Text Sequence-to-Sequence Modeling",
    "date": "18 May, 2023",
    "authors": [
      "Yunqi Zhu",
      " Xuebing Yang",
      " Yuanyuan Wu",
      " Wensheng Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.19274",
    "paper_id": "2305.19274",
    "abstract": "\n        There are two approaches for simulating memory as well as learning in artificial intelligence; the functionalistic approach and the cognitive approach. The necessary condition to put the second approach into account is to provide a model of brain activity that contains a quite good congruence with observational facts such as mistakes and forgotten experiences. Given that human memory has a solid core that includes the components of our identity, our family and our hometown, the major and determinative events of our lives, and the countless repeated and accepted facts of our culture, the more we go to the peripheral spots the data becomes flimsier and more easily exposed to oblivion. It was essential to propose a model in which the topographical differences are quite distinguishable. In our proposed model, we have translated this topographical situation into quantities, which are attributed to the nodes. The result is an edge-weighted graph with mass-based values on the nodes which demonstrates the importance of each atomic proposition, as a truth, for an intelligent being. Furthermore, it dynamically develops and modifies, and in successive phases, it changes the mass of the nodes and weight of the edges depending on gathered inputs from the environment.\n        \u25b3 Less\n      ",
    "title": "Memory as a Mass-based Graph: Towards a Conceptual Framework for the Simulation Model of Human Memory in AI",
    "date": "18 May, 2023",
    "authors": [
      "Mahdi Mollakazemiha",
      " Hassan Fatzade"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11067",
    "paper_id": "2305.11067",
    "abstract": "\n        Past work demonstrated that using neural networks, we can extend unfinished music pieces while maintaining the music style of the musician. With recent advancements in large language models and diffusion models, we are now capable of generating comics with an interesting storyline while maintaining the art style of the artist. In this paper, we used ChatGPT to generate storylines and dialogue and then generated the comic using stable diffusion. We introduced a novel way to evaluate AI-generated stories, and we achieved SOTA performance on character fidelity and art style by fine-tuning stable diffusion using LoRA, ControlNet, etc.\n        \u25b3 Less\n      ",
    "title": "Generating coherent comic with rich story using ChatGPT and Stable Diffusion",
    "date": "18 May, 2023",
    "authors": [
      "Ze Jin",
      " Zorina Song"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11387",
    "paper_id": "2305.11387",
    "abstract": "\n        This study comes as a timely response to mounting criticism of the information bottleneck (IB) theory, injecting fresh perspectives to rectify misconceptions and reaffirm its validity. Firstly, we introduce an auxiliary function to reinterpret the maximal coding rate reduction method as a special yet local optimal case of IB theory. Through this auxiliary function, we clarify the paradox of decreasing mutual information during the application of ReLU activation in deep learning (DL) networks. Secondly, we challenge the doubts about IB theory's applicability by demonstrating its capacity to explain the absence of a compression phase with linear activation functions in hidden layers, when viewed through the lens of the auxiliary function. Lastly, by taking a novel theoretical stance, we provide a new way to interpret the inner organizations of DL networks by using IB theory, aligning them with recent experimental evidence. Thus, this paper serves as an act of justice for IB theory, potentially reinvigorating its standing and application in DL and other fields such as communications and biomedical research.\n        \u25b3 Less\n      ",
    "title": "Justices for Information Bottleneck Theory",
    "date": "18 May, 2023",
    "authors": [
      "Faxian Cao",
      " Yongqiang Cheng",
      " Adil Mehmood Khan",
      " Zhijing Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11918",
    "paper_id": "2305.11918",
    "abstract": "\n        Vision-and-language navigation (VLN) is a crucial but challenging cross-modal navigation task. One powerful technique to enhance the generalization performance in VLN is the use of an independent speaker model to provide pseudo instructions for data augmentation. However, current speaker models based on Long-Short Term Memory (LSTM) lack the ability to attend to features relevant at different locations and time steps. To address this, we propose a novel progress-aware spatio-temporal transformer speaker (PASTS) model that uses the transformer as the core of the network. PASTS uses a spatio-temporal encoder to fuse panoramic representations and encode intermediate connections through steps. Besides, to avoid the misalignment problem that could result in incorrect supervision, a speaker progress monitor (SPM) is proposed to enable the model to estimate the progress of instruction generation and facilitate more fine-grained caption results. Additionally, a multifeature dropout (MFD) strategy is introduced to alleviate overfitting. The proposed PASTS is flexible to be combined with existing VLN models. The experimental results demonstrate that PASTS outperforms all existing speaker models and successfully improves the performance of previous VLN models, achieving state-of-the-art performance on the standard Room-to-Room (R2R) dataset.\n        \u25b3 Less\n      ",
    "title": "PASTS: Progress-Aware Spatio-Temporal Transformer Speaker For Vision-and-Language Navigation",
    "date": "18 May, 2023",
    "authors": [
      "Liuyi Wang",
      " Chengju Liu",
      " Zongtao He",
      " Shu Li",
      " Qingqing Yan",
      " Huiyi Chen",
      " Qijun Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11390",
    "paper_id": "2305.11390",
    "abstract": "\n        In this paper, we consider the problem of long tail scenario modeling with budget limitation, i.e., insufficient human resources for model training stage and limited time and computing resources for model inference stage. This problem is widely encountered in various applications, yet has received deficient attention so far. We present an automatic system named ALT to deal with this problem. Several efforts are taken to improve the algorithms used in our system, such as employing various automatic machine learning related techniques, adopting the meta learning philosophy, and proposing an essential budget-limited neural architecture search method, etc. Moreover, to build the system, many optimizations are performed from a systematic perspective, and essential modules are armed, making the system more feasible and efficient. We perform abundant experiments to validate the effectiveness of our system and demonstrate the usefulness of the critical modules in our system. Moreover, online results are provided, which fully verified the efficacy of our system.\n        \u25b3 Less\n      ",
    "title": "ALT: An Automatic System for Long Tail Scenario Modeling",
    "date": "18 May, 2023",
    "authors": [
      "Ya-Lin Zhang",
      " Jun Zhou",
      " Yankun Ren",
      " Yue Zhang",
      " Xinxing Yang",
      " Meng Li",
      " Qitao Shi",
      " Longfei Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11407",
    "paper_id": "2305.11407",
    "abstract": "\n        Electronic health record (EHR) data are increasingly used to support real-world evidence (RWE) studies. Yet its ability to generate reliable RWE is limited by the lack of readily available precise information on the timing of clinical events such as the onset time of heart failure. We propose a LAbel-efficienT incidenT phEnotyping (LATTE) algorithm to accurately annotate the timing of clinical events from longitudinal EHR data. By leveraging the pre-trained semantic embedding vectors from large-scale EHR data as prior knowledge, LATTE selects predictive EHR features in a concept re-weighting module by mining their relationship to the target event and compresses their information into longitudinal visit embeddings through a visit attention learning network. LATTE employs a recurrent neural network to capture the sequential dependency between the target event and visit embeddings before/after it. To improve label efficiency, LATTE constructs highly informative longitudinal silver-standard labels from large-scale unlabeled patients to perform unsupervised pre-training and semi-supervised joint training. Finally, LATTE enhances cross-site portability via contrastive representation learning. LATTE is evaluated on three analyses: the onset of type-2 diabetes, heart failure, and the onset and relapses of multiple sclerosis. We use various evaluation metrics present in the literature including the ABCgainABC_{gain}, the proportion of reduction in the area between the observed event indicator and the predicted cumulative incidences in reference to the prediction per incident prevalence. LATTE consistently achieves substantial improvement over benchmark methods such as SAMGEP and RETAIN in all settings.\n        \u25b3 Less\n      ",
    "title": "LATTE: Label-efficient Incident Phenotyping from Longitudinal Electronic Health Records",
    "date": "18 May, 2023",
    "authors": [
      "Jun Wen",
      " Jue Hou",
      " Clara-Lea Bonzel",
      " Yihan Zhao",
      " Victor M. Castro",
      " Vivian S. Gainer",
      " Dana Weisenfeld",
      " Tianrun Cai",
      " Yuk-Lam Ho",
      " Vidul A. Panickan",
      " Lauren Costa",
      " Chuan Hong",
      " J. Michael Gaziano",
      " Katherine P. Liao",
      " Junwei Lu",
      " Kelly Cho",
      " Tianxi Cai"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06590",
    "paper_id": "2305.06590",
    "abstract": "\n        In real world applications, knowledge graphs (KG) are widely used in various domains (e.g. medical applications and dialogue agents). However, for fact verification, KGs have not been adequately utilized as a knowledge source. KGs can be a valuable knowledge source in fact verification due to their reliability and broad applicability. A KG consists of nodes and edges which makes it clear how concepts are linked together, allowing machines to reason over chains of topics. However, there are many challenges in understanding how these machine-readable concepts map to information in text. To enable the community to better use KGs, we introduce a new dataset, FactKG: Fact Verification via Reasoning on Knowledge Graphs. It consists of 108k natural language claims with five types of reasoning: One-hop, Conjunction, Existence, Multi-hop, and Negation. Furthermore, FactKG contains various linguistic patterns, including colloquial style claims as well as written style claims to increase practicality. Lastly, we develop a baseline approach and analyze FactKG over these reasoning types. We believe FactKG can advance both reliability and practicality in KG-based fact verification.\n        \u25b3 Less\n      ",
    "title": "FactKG: Fact Verification via Reasoning on Knowledge Graphs",
    "date": "18 May, 2023",
    "authors": [
      "Jiho Kim",
      " Sungjin Park",
      " Yeonsu Kwon",
      " Yohan Jo",
      " James Thorne",
      " Edward Choi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11419",
    "paper_id": "2305.11419",
    "abstract": "\n        Real-time semantic segmentation is a challenging task that requires high-accuracy models with low-inference times. Implementing these models on embedded systems is limited by hardware capability and memory usage, which produces bottlenecks. We propose an efficient model for real-time semantic segmentation called JetSeg, consisting of an encoder called JetNet, and an improved RegSeg decoder. The JetNet is designed for GPU-Embedded Systems and includes two main components: a new light-weight efficient block called JetBlock, that reduces the number of parameters minimizing memory usage and inference time without sacrificing accuracy; a new strategy that involves the combination of asymmetric and non-asymmetric convolutions with depthwise-dilated convolutions called JetConv, a channel shuffle operation, light-weight activation functions, and a convenient number of group convolutions for embedded systems, and an innovative loss function named JetLoss, which integrates the Precision, Recall, and IoUB losses to improve semantic segmentation and reduce computational complexity. Experiments demonstrate that JetSeg is much faster on workstation devices and more suitable for Low-Power GPU-Embedded Systems than existing state-of-the-art models for real-time semantic segmentation. Our approach outperforms state-of-the-art real-time encoder-decoder models by reducing 46.70M parameters and 5.14% GFLOPs, which makes JetSeg up to 2x faster on the NVIDIA Titan RTX GPU and the Jetson Xavier than other models. The JetSeg code is available at https://github.com/mmontielpz/jetseg.\n        \u25b3 Less\n      ",
    "title": "JetSeg: Efficient Real-Time Semantic Segmentation Model for Low-Power GPU-Embedded Systems",
    "date": "18 May, 2023",
    "authors": [
      "Miguel Lopez-Montiel",
      " Daniel Alejandro Lopez",
      " Oscar Montiel"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.02231",
    "paper_id": "2302.02231",
    "abstract": "\n        Research publications are the primary vehicle for sharing scientific progress in the form of new discoveries, methods, techniques, and insights. Unfortunately, the lack of a large-scale, comprehensive, and easy-to-use resource capturing the myriad relationships between publications, their authors, and venues presents a barrier to applications for gaining a deeper understanding of science. In this paper, we present PubGraph, a new resource for studying scientific progress that takes the form of a large-scale knowledge graph (KG) with more than 385M entities, 13B main edges, and 1.5B qualifier edges. PubGraph is comprehensive and unifies data from various sources, including Wikidata, OpenAlex, and Semantic Scholar, using the Wikidata ontology. Beyond the metadata available from these sources, PubGraph includes outputs from auxiliary community detection algorithms and large language models. To further support studies on reasoning over scientific networks, we create several large-scale benchmarks extracted from PubGraph for the core task of knowledge graph completion (KGC). These benchmarks present many challenges for knowledge graph embedding models, including an adversarial community-based KGC evaluation setting, zero-shot inductive learning, and large-scale learning. All of the aforementioned resources are accessible at https://pubgraph.isi.edu/ and released under the CC-BY-SA license. We plan to update PubGraph quarterly to accommodate the release of new publications.\n        \u25b3 Less\n      ",
    "title": "PubGraph: A Large-Scale Scientific Knowledge Graph",
    "date": "18 May, 2023",
    "authors": [
      "Kian Ahrabian",
      " Xinwei Du",
      " Richard Delwin Myloth",
      " Arun Baalaaji Sankar Ananthan",
      " Jay Pujara"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07889",
    "paper_id": "2305.07889",
    "abstract": "\n        Infusing deep learning with structural engineering has received widespread attention for both forward problems (structural simulation) and inverse problems (structural health monitoring). Based on Fourier Neural Operator, this study proposes VINO (Vehicle-bridge Interaction Neural Operator) to serve as the digital twin of bridge structures. VINO learns mappings between structural response fields and damage fields. In this study, VBI-FE dataset was established by running parametric finite element (FE) simulations considering a random distribution of structural initial damage field. Subsequently, VBI-EXP dataset was produced by conducting an experimental study under four damage scenarios. After VINO was pre-trained by VBI-FE and fine-tuned by VBI-EXP from the bridge at the healthy state, the model achieved the following two improvements. First, forward VINO can predict structural responses from damage field inputs more accurately than the FE model. Second, inverse VINO can determine, localize, and quantify damages in all scenarios, suggesting the practicality of data-driven approaches.\n        \u25b3 Less\n      ",
    "title": "Neural operator for structural simulation and bridge health monitoring",
    "date": "18 May, 2023",
    "authors": [
      "Chawit Kaewnuratchadasorn",
      " Jiaji Wang",
      " Chul-Woo Kim"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.13447",
    "paper_id": "2211.13447",
    "abstract": "\n        We study the computational complexity of counterfactual reasoning in relation to the complexity of associational and interventional reasoning on structural causal models (SCMs). We show that counterfactual reasoning is no harder than associational or interventional reasoning on fully specified SCMs in the context of two computational frameworks. The first framework is based on the notion of treewidth and includes the classical variable elimination and jointree algorithms. The second framework is based on the more recent and refined notion of causal treewidth which is directed towards models with functional dependencies such as SCMs. Our results are constructive and based on bounding the (causal) treewidth of twin networks -- used in standard counterfactual reasoning that contemplates two worlds, real and imaginary -- to the (causal) treewidth of the underlying SCM structure. In particular, we show that the latter (causal) treewidth is no more than twice the former plus one. Hence, if associational or interventional reasoning is tractable on a fully specified SCM then counterfactual reasoning is tractable too. We extend our results to general counterfactual reasoning that requires contemplating more than two worlds and discuss applications of our results to counterfactual reasoning with a partially specified SCM that is coupled with data. We finally present empirical results that measure the gap between the complexities of counterfactual reasoning and associational/interventional reasoning on random SCMs.\n        \u25b3 Less\n      ",
    "title": "On the Complexity of Counterfactual Reasoning",
    "date": "18 May, 2023",
    "authors": [
      "Yunqiu Han",
      " Yizuo Chen",
      " Adnan Darwiche"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.09667",
    "paper_id": "2212.09667",
    "abstract": "\n        Users' physical safety is an increasing concern as the market for intelligent systems continues to grow, where unconstrained systems may recommend users dangerous actions that can lead to serious injury. Covertly unsafe text is an area of particular interest, as such text may arise from everyday scenarios and are challenging to detect as harmful. We propose FARM, a novel framework leveraging external knowledge for trustworthy rationale generation in the context of safety. In particular, FARM foveates on missing knowledge to qualify the information required to reason in specific scenarios and retrieves this information with attribution to trustworthy sources. This knowledge is used to both classify the safety of the original text and generate human-interpretable rationales, shedding light on the risk of systems to specific user groups and helping both stakeholders manage the risks of their systems and policymakers to provide concrete safeguards for consumer safety. Our experiments show that FARM obtains state-of-the-art results on the SafeText dataset, showing absolute improvement in safety classification accuracy by 5.9%.\n        \u25b3 Less\n      ",
    "title": "Foveate, Attribute, and Rationalize: Towards Physically Safe and Trustworthy AI",
    "date": "18 May, 2023",
    "authors": [
      "Alex Mei",
      " Sharon Levy",
      " William Yang Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11437",
    "paper_id": "2305.11437",
    "abstract": "\n        Federated Learning (FL) has emerged as an effective learning paradigm for distributed computation owing to its strong potential in capturing underlying data statistics while preserving data privacy. However, in cases of practical data heterogeneity among FL clients, existing FL frameworks still exhibit deficiency in capturing the overall feature properties of local client data that exhibit disparate distributions. In response, generative adversarial networks (GANs) have recently been exploited in FL to address data heterogeneity since GANs can be integrated for data regeneration without exposing original raw data. Despite some successes, existing GAN-related FL frameworks often incur heavy communication cost and also elicit other privacy concerns, which limit their applications in real scenarios. To this end, this work proposes a novel FL framework that requires only partial GAN model sharing. Named as PS-FedGAN, this new framework enhances the GAN releasing and training mechanism to address heterogeneous data distributions across clients and to strengthen privacy preservation at reduced communication cost, especially over wireless networks. Our analysis demonstrates the convergence and privacy benefits of the proposed PS-FEdGAN framework. Through experimental results based on several well-known benchmark datasets, our proposed PS-FedGAN shows great promise to tackle FL under non-IID client data distributions, while securing data privacy and lowering communication overhead.\n        \u25b3 Less\n      ",
    "title": "PS-FedGAN: An Efficient Federated Learning Framework Based on Partially Shared Generative Adversarial Networks For Data Privacy",
    "date": "18 May, 2023",
    "authors": [
      "Achintha Wijesinghe",
      " Songyang Zhang",
      " Zhi Ding"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11444",
    "paper_id": "2305.11444",
    "abstract": "\n        We have constructed Arukikata Travelogue Dataset and released it free of charge for academic research. This dataset is a Japanese text dataset with a total of over 31 million words, comprising 4,672 Japanese domestic travelogues and 9,607 overseas travelogues. Before providing our dataset, there was a scarcity of widely available travelogue data for research purposes, and each researcher had to prepare their own data. This hinders the replication of existing studies and fair comparative analysis of experimental results. Our dataset enables any researchers to conduct investigation on the same data and to ensure transparency and reproducibility in research. In this paper, we describe the academic significance, characteristics, and prospects of our dataset.\n        \u25b3 Less\n      ",
    "title": "Arukikata Travelogue Dataset",
    "date": "18 May, 2023",
    "authors": [
      "Hiroki Ouchi",
      " Hiroyuki Shindo",
      " Shoko Wakamiya",
      " Yuki Matsuda",
      " Naoya Inoue",
      " Shohei Higashiyama",
      " Satoshi Nakamura",
      " Taro Watanabe"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11455",
    "paper_id": "2305.11455",
    "abstract": "\n        A centerpiece of the ever-popular reinforcement learning from human feedback (RLHF) approach to fine-tuning autoregressive language models is the explicit training of a reward model to emulate human feedback, distinct from the language model itself. This reward model is then coupled with policy-gradient methods to dramatically improve the alignment between language model outputs and desired responses. In this work, we adopt a novel perspective wherein a pre-trained language model is itself simultaneously a policy, reward function, and transition function. An immediate consequence of this is that reward learning and language model fine-tuning can be performed jointly and directly, without requiring any further downstream policy optimization. While this perspective does indeed break the traditional agent-environment interface, we nevertheless maintain that there can be enormous statistical benefits afforded by bringing to bear traditional algorithmic concepts from reinforcement learning. Our experiments demonstrate one concrete instance of this through efficient exploration based on the representation and resolution of epistemic uncertainty. In order to illustrate these ideas in a transparent manner, we restrict attention to a simple didactic data generating process and leave for future work extension to systems of practical scale.\n        \u25b3 Less\n      ",
    "title": "Shattering the Agent-Environment Interface for Fine-Tuning Inclusive Language Models",
    "date": "18 May, 2023",
    "authors": [
      "Wanqiao Xu",
      " Shi Dong",
      " Dilip Arumugam",
      " Benjamin Van Roy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11460",
    "paper_id": "2305.11460",
    "abstract": "\n        Finding an agreement among diverse opinions is a challenging topic in multiagent systems. Recently, large language models (LLMs) have shown great potential in addressing this challenge due to their remarkable capabilities in comprehending human opinions and generating human-like text. However, they typically rely on extensive human-annotated data. In this paper, we propose Self-Agreement, a novel framework for fine-tuning LLMs to autonomously find agreement using data generated by LLM itself. Specifically, our approach employs the generative pre-trained transformer-3 (GPT-3) to generate multiple opinions for each question in a question dataset and create several agreement candidates among these opinions. Then, a bidirectional encoder representations from transformers (BERT)-based model evaluates the agreement score of each agreement candidate and selects the one with the highest agreement score. This process yields a dataset of question-opinion-agreements, which we use to fine-tune a pre-trained LLM for discovering agreements among diverse opinions. Remarkably, a pre-trained LLM fine-tuned by our Self-Agreement framework achieves comparable performance to GPT-3 with only 1/25 of its parameters, showcasing its ability to identify agreement among various opinions without the need for human-annotated data.\n        \u25b3 Less\n      ",
    "title": "Self-Agreement: A Framework for Fine-tuning Language Models to Find Agreement among Diverse Opinions",
    "date": "18 May, 2023",
    "authors": [
      "Shiyao Ding",
      " Takayuki Ito"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11480",
    "paper_id": "2305.11480",
    "abstract": "\n        We propose and study Complementary Concept Generation (CCGen): given a concept of interest, e.g., \"Digital Cameras\", generating a list of complementary concepts, e.g., 1) Camera Lenses 2) Batteries 3) Camera Cases 4) Memory Cards 5) Battery Chargers. CCGen is beneficial for various applications like query suggestion and item recommendation, especially in the e-commerce domain. To solve CCGen, we propose to train language models to generate ranked lists of concepts with a two-step training strategy. We also teach the models to generate explanations by incorporating explanations distilled from large teacher models. Extensive experiments and analysis demonstrate that our model can generate high-quality concepts complementary to the input concept while producing explanations to justify the predictions.\n        \u25b3 Less\n      ",
    "title": "CCGen: Explainable Complementary Concept Generation in E-Commerce",
    "date": "18 May, 2023",
    "authors": [
      "Jie Huang",
      " Yifan Gao",
      " Zheng Li",
      " Jingfeng Yang",
      " Yangqiu Song",
      " Chao Zhang",
      " Zining Zhu",
      " Haoming Jiang",
      " Kevin Chen-Chuan Chang",
      " Bing Yin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11104",
    "paper_id": "2305.11104",
    "abstract": "\n        Speech super-resolution (SSR) aims to recover a high resolution (HR) speech from its corresponding low resolution (LR) counterpart. Recent SSR methods focus more on the reconstruction of the magnitude spectrogram, ignoring the importance of phase reconstruction, thereby limiting the recovery quality. To address this issue, we propose mdctGAN, a novel SSR framework based on modified discrete cosine transform (MDCT). By adversarial learning in the MDCT domain, our method reconstructs HR speeches in a phase-aware manner without vocoders or additional post-processing. Furthermore, by learning frequency consistent features with self-attentive mechanism, mdctGAN guarantees a high quality speech reconstruction. For VCTK corpus dataset, the experiment results show that our model produces natural auditory quality with high MOS and PESQ scores. It also achieves the state-of-the-art log-spectral-distance (LSD) performance on 48 kHz target resolution from various input rates. Code is available from https://github.com/neoncloud/mdctGAN\n        \u25b3 Less\n      ",
    "title": "mdctGAN: Taming transformer-based GAN for speech super-resolution with Modified DCT spectra",
    "date": "19 May, 2023",
    "authors": [
      "Chenhao Shuai",
      " Chaohua Shi",
      " Lu Gan",
      " Hongqing Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.11954",
    "paper_id": "2304.11954",
    "abstract": "\n        Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks, due to their event-driven spiking computation. However, state-of-the-art deep SNNs (including Spikformer and SEW ResNet) suffer from non-spike computations (integer-float multiplications) caused by the structure of their residual connection. These non-spike computations increase SNNs' power consumption and make them unsuitable for deployment on mainstream neuromorphic hardware, which only supports spike operations. In this paper, we propose a hardware-friendly spike-driven residual learning architecture for SNNs to avoid non-spike computations. Based on this residual design, we develop Spikingformer, a pure transformer-based spiking neural network. We evaluate Spikingformer on ImageNet, CIFAR10, CIFAR100, CIFAR10-DVS and DVS128 Gesture datasets, and demonstrate that Spikingformer outperforms the state-of-the-art in directly trained pure SNNs as a novel advanced backbone (75.85%\\% top-1 accuracy on ImageNet, + 1.04%\\% compared with Spikformer). Furthermore, our experiments verify that Spikingformer effectively avoids non-spike computations and significantly reduces energy consumption by 57.34%\\% compared with Spikformer on ImageNet. To our best knowledge, this is the first time that a pure event-driven transformer-based SNN has been developed.\n        \u25b3 Less\n      ",
    "title": "Spikingformer: Spike-driven Residual Learning for Transformer-based Spiking Neural Network",
    "date": "19 May, 2023",
    "authors": [
      "Chenlin Zhou",
      " Liutao Yu",
      " Zhaokun Zhou",
      " Zhengyu Ma",
      " Han Zhang",
      " Huihui Zhou",
      " Yonghong Tian"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11489",
    "paper_id": "2305.11489",
    "abstract": "\n        Incomplete multi-view clustering is a challenging and non-trivial task to provide effective data analysis for large amounts of unlabeled data in the real world. All incomplete multi-view clustering methods need to address the problem of how to reduce the impact of missing views. To address this issue, we propose diffusion completion to recover the missing views integrated into an incomplete multi-view clustering framework. Based on the observable views information, the diffusion model is used to recover the missing views, and then the consistency information of the multi-view data is learned by contrastive learning to improve the performance of multi-view clustering. To the best of our knowledge, this may be the first work to incorporate diffusion models into an incomplete multi-view clustering framework. Experimental results show that the proposed method performs well in recovering the missing views while achieving superior clustering performance compared to state-of-the-art methods.\n        \u25b3 Less\n      ",
    "title": "Incomplete Multi-view Clustering via Diffusion Completion",
    "date": "19 May, 2023",
    "authors": [
      "Sifan Fang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.02021",
    "paper_id": "2212.02021",
    "abstract": "\n        The focus of this work is to investigate unsupervised approaches to overcome quintessential challenges in designing task-oriented dialog schema: assigning intent labels to each dialog turn (intent clustering) and generating a set of intents based on the intent clustering methods (intent induction). We postulate there are two salient factors for automatic induction of intents: (1) clustering algorithm for intent labeling and (2) user utterance embedding space. We compare existing off-the-shelf clustering models and embeddings based on DSTC11 evaluation. Our extensive experiments demonstrate that the combined selection of utterance embedding and clustering method in the intent induction task should be carefully considered. We also present that pretrained MiniLM with Agglomerative clustering shows significant improvement in NMI, ARI, F1, accuracy and example coverage in intent induction tasks. The source codes are available at https://github.com/Jeiyoon/dstc11-track2.\n        \u25b3 Less\n      ",
    "title": "Analysis of Utterance Embeddings and Clustering Methods Related to Intent Induction for Task-Oriented Dialogue",
    "date": "19 May, 2023",
    "authors": [
      "Jeiyoon Park",
      " Yoonna Jang",
      " Chanhee Lee",
      " Heuiseok Lim"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11497",
    "paper_id": "2305.11497",
    "abstract": "\n        Prompt tuning has achieved great success in transferring the knowledge from large pretrained vision-language models into downstream tasks, and has dominated the performance on visual grounding (VG). However, almost all existing prompt tuning paradigms suffer from poor interpretability. In this paper, we argue that their poor interpretability is attributed to the holistic prompt generation and inference process. By \"holistic\", we mean that they usually directly learn a set of vectors as the prompt (i.e., prompt generation), and use the learned global prompt to augment the textual input for the VG model (i.e., prompt inference). To this end, we propose a new prompt construction paradigm with explicit explainable ability, named TreePrompt. Specifically, we first deconstruct a complex sentence into a tree, that is consistent with human reasoning. Then, following the syntax tree, we compose a structured prompt in a bottom-up manner. Thanks to this step-by-step prompt construction process, each intermediate prompt (i.e., tree node) permits us to understand the reasoning process. Extensive ablations on various backbones and benchmarks consistently demonstrate the effectiveness and interpretability of our TreePrompt.\n        \u25b3 Less\n      ",
    "title": "TreePrompt: Learning to Compose Tree Prompts for Explainable Visual Grounding",
    "date": "19 May, 2023",
    "authors": [
      "Chenchi Zhang",
      " Jun Xiao",
      " Lei Chen",
      " Jian Shao",
      " Long Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11498",
    "paper_id": "2305.11498",
    "abstract": "\n        Event Extraction (EE), aiming to identify and classify event triggers and arguments from event mentions, has benefited from pre-trained language models (PLMs). However, existing PLM-based methods ignore the information of trigger/argument fields, which is crucial for understanding event schemas. To this end, we propose a Probabilistic reCoupling model enhanced Event extraction framework (ProCE). Specifically, we first model the syntactic-related event fields as probabilistic biases, to clarify the event fields from ambiguous entanglement. Furthermore, considering multiple occurrences of the same triggers/arguments in EE, we explore probabilistic interaction strategies among multiple fields of the same triggers/arguments, to recouple the corresponding clarified distributions and capture more latent information fields. Experiments on EE datasets demonstrate the effectiveness and generalization of our proposed approach.\n        \u25b3 Less\n      ",
    "title": "Recouple Event Field via Probabilistic Bias for Event Extraction",
    "date": "19 May, 2023",
    "authors": [
      "Xingyu Bai",
      " Taiqiang Wu",
      " Han Guo",
      " Zhe Zhao",
      " Xuefeng Yang",
      " Jiayi Li",
      " Weijie Liu",
      " Qi Ju",
      " Weigang Guo",
      " Yujiu Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11510",
    "paper_id": "2305.11510",
    "abstract": "\n        In automated warehouses, teams of mobile robots fulfill the packaging process by transferring inventory pods to designated workstations while navigating narrow aisles formed by tightly packed pods. This problem is typically modeled as a Multi-Agent Pickup and Delivery (MAPD) problem, which is then solved by repeatedly planning collision-free paths for agents on a fixed graph, as in the Rolling-Horizon Collision Resolution (RHCR) algorithm. However, existing approaches make the limiting assumption that agents are only allowed to move pods that correspond to their current task, while considering the other pods as stationary obstacles (even though all pods are movable). This behavior can result in unnecessarily long paths which could otherwise be avoided by opening additional corridors via pod manipulation. To this end, we explore the implications of allowing agents the flexibility of dynamically relocating pods. We call this new problem Terraforming MAPD (tMAPD) and develop an RHCR-based approach to tackle it. As the extra flexibility of terraforming comes at a significant computational cost, we utilize this capability judiciously by identifying situations where it could make a significant impact on the solution quality. In particular, we invoke terraforming in response to disruptions that often occur in automated warehouses, e.g., when an item is dropped from a pod or when agents malfunction. Empirically, using our approach for tMAPD, where disruptions are modeled via a stochastic process, we improve throughput by over 10%, reduce the maximum service time (the difference between the drop-off time and the pickup time of a pod) by more than 50%, without drastically increasing the runtime, compared to the MAPD setting.\n        \u25b3 Less\n      ",
    "title": "Terraforming -- Environment Manipulation during Disruptions for Multi-Agent Pickup and Delivery",
    "date": "19 May, 2023",
    "authors": [
      "David Vainshtein",
      " Yaakov Sherma",
      " Kiril Solovey",
      " Oren Salzman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14370",
    "paper_id": "2305.14370",
    "abstract": "\n        Machine learning is employed in healthcare to draw approximate conclusions regarding human diseases and mental health problems. Compared to older traditional methods, it can help to analyze data more efficiently and produce better and more dependable results. Millions of people are affected by schizophrenia, which is a chronic mental disorder that can significantly impact their lives. Many machine learning algorithms have been developed to predict and prevent this disease, and they can potentially be implemented in the diagnosis of individuals who have it. This survey aims to review papers that have focused on the use of deep learning to detect and predict schizophrenia using EEG signals, functional magnetic resonance imaging (fMRI), and diffusion magnetic resonance imaging (dMRI). With our chosen search strategy, we assessed ten publications from 2019 to 2022. All studies achieved successful predictions of more than 80%. This review provides summaries of the studies and compares their notable aspects. In the field of artificial intelligence (AI) and machine learning (ML) for schizophrenia, significant advances have been made due to the availability of ML tools, and we are optimistic that this field will continue to grow.\n        \u25b3 Less\n      ",
    "title": "A Survey on the Role of Artificial Intelligence in the Prediction and Diagnosis of Schizophrenia",
    "date": "19 May, 2023",
    "authors": [
      "Narges Ramesh",
      " Yasmin Ghodsi",
      " Hamidreza Bolhasani"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11517",
    "paper_id": "2305.11517",
    "abstract": "\n        Diffusion models have emerged as the new state-of-the-art family of deep generative models, and their promising potentials for text generation have recently attracted increasing attention. Existing studies mostly adopt a single encoder architecture with partially noising processes for conditional text generation, but its degree of flexibility for conditional modeling is limited. In fact, the encoder-decoder architecture is naturally more flexible for its detachable encoder and decoder modules, which is extensible to multilingual and multimodal generation tasks for conditions and target texts. However, the encoding process of conditional texts lacks the understanding of target texts. To this end, a spiral interaction architecture for encoder-decoder text diffusion (DiffuSIA) is proposed. Concretely, the conditional information from encoder is designed to be captured by the diffusion decoder, while the target information from decoder is designed to be captured by the conditional encoder. These two types of information flow run through multilayer interaction spirally for deep fusion and understanding. DiffuSIA is evaluated on four text generation tasks, including paraphrase, text simplification, question generation, and open-domain dialogue generation. Experimental results show that DiffuSIA achieves competitive performance among previous methods on all four tasks, demonstrating the effectiveness and generalization ability of the proposed method.\n        \u25b3 Less\n      ",
    "title": "DiffuSIA: A Spiral Interaction Architecture for Encoder-Decoder Text Diffusion",
    "date": "19 May, 2023",
    "authors": [
      "Chao-Hong Tan",
      " Jia-Chen Gu",
      " Zhen-Hua Ling"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11519",
    "paper_id": "2305.11519",
    "abstract": "\n        Adam Smith developed a version of moral philosophy where better decisions are made by interrogating an impartial spectator within us. We discuss the possibility of using an external non-human-based substitute tool that would augment our internal mental processes and play the role of the impartial spectator. Such tool would have more knowledge about the world, be more impartial, and would provide a more encompassing perspective on moral assessment.\n        \u25b3 Less\n      ",
    "title": "Artificial intelligence moral agent as Adam Smith's impartial spectator",
    "date": "19 May, 2023",
    "authors": [
      "Nikodem Tomczak"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.05134",
    "paper_id": "2302.05134",
    "abstract": "\n        Recent work on deep clustering has found new promising methods also for constrained clustering problems. Their typically pairwise constraints often can be used to guide the partitioning of the data. Many problems however, feature cluster-level constraints, e.g. the Capacitated Clustering Problem (CCP), where each point has a weight and the total weight sum of all points in each cluster is bounded by a prescribed capacity. In this paper we propose a new method for the CCP, Neural Capacited Clustering, that learns a neural network to predict the assignment probabilities of points to cluster centers from a data set of optimal or near optimal past solutions of other problem instances. During inference, the resulting scores are then used in an iterative k-means like procedure to refine the assignment under capacity constraints. In our experiments on artificial data and two real world datasets our approach outperforms several state-of-the-art mathematical and heuristic solvers from the literature. Moreover, we apply our method in the context of a cluster-first-route-second approach to the Capacitated Vehicle Routing Problem (CVRP) and show competitive results on the well-known Uchoa benchmark.\n        \u25b3 Less\n      ",
    "title": "Neural Capacitated Clustering",
    "date": "19 May, 2023",
    "authors": [
      "Jonas K. Falkner",
      " Lars Schmidt-Thieme"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11921",
    "paper_id": "2305.11921",
    "abstract": "\n        The measurement of progress using benchmarks evaluations is ubiquitous in computer science and machine learning. However, common approaches to analyzing and presenting the results of benchmark comparisons of multiple algorithms over multiple datasets, such as the critical difference diagram introduced by Dem\u0161ar (2006), have important shortcomings and, we show, are open to both inadvertent and intentional manipulation. To address these issues, we propose a new approach to presenting the results of benchmark comparisons, the Multiple Comparison Matrix (MCM), that prioritizes pairwise comparisons and precludes the means of manipulating experimental results in existing approaches. MCM can be used to show the results of an all-pairs comparison, or to show the results of a comparison between one or more selected algorithms and the state of the art. MCM is implemented in Python and is publicly available.\n        \u25b3 Less\n      ",
    "title": "An Approach to Multiple Comparison Benchmark Evaluations that is Stable Under Manipulation of the Comparate Set",
    "date": "19 May, 2023",
    "authors": [
      "Ali Ismail-Fawaz",
      " Angus Dempster",
      " Chang Wei Tan",
      " Matthieu Herrmann",
      " Lynn Miller",
      " Daniel F. Schmidt",
      " Stefano Berretti",
      " Jonathan Weber",
      " Maxime Devanne",
      " Germain Forestier",
      " Geoffrey I. Webb"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11537",
    "paper_id": "2305.11537",
    "abstract": "\n        Federated Learning (FL) has emerged as a significant advancement in the field of Artificial Intelligence (AI), enabling collaborative model training across distributed devices while maintaining data privacy. As the importance of FL increases, addressing trustworthiness issues in its various aspects becomes crucial. In this survey, we provide an extensive overview of the current state of Trustworthy FL, exploring existing solutions and well-defined pillars relevant to Trustworthy . Despite the growth in literature on trustworthy centralized Machine Learning (ML)/Deep Learning (DL), further efforts are necessary to identify trustworthiness pillars and evaluation metrics specific to FL models, as well as to develop solutions for computing trustworthiness levels. We propose a taxonomy that encompasses three main pillars: Interpretability, Fairness, and Security & Privacy. Each pillar represents a dimension of trust, further broken down into different notions. Our survey covers trustworthiness challenges at every level in FL settings. We present a comprehensive architecture of Trustworthy FL, addressing the fundamental principles underlying the concept, and offer an in-depth analysis of trust assessment mechanisms. In conclusion, we identify key research challenges related to every aspect of Trustworthy FL and suggest future research directions. This comprehensive survey serves as a valuable resource for researchers and practitioners working on the development and implementation of Trustworthy FL systems, contributing to a more secure and reliable AI landscape.\n        \u25b3 Less\n      ",
    "title": "Trustworthy Federated Learning: A Survey",
    "date": "19 May, 2023",
    "authors": [
      "Asadullah Tariq",
      " Mohamed Adel Serhani",
      " Farag Sallabi",
      " Tariq Qayyum",
      " Ezedin S. Barka",
      " Khaled A. Shuaib"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11560",
    "paper_id": "2305.11560",
    "abstract": "\n        Every day, the human brain processes an immense volume of visual information, relying on intricate neural mechanisms to perceive and interpret these stimuli. Recent breakthroughs in functional magnetic resonance imaging (fMRI) have enabled scientists to extract visual information from human brain activity patterns. In this study, we present an innovative method for decoding brain activity into meaningful images and captions, with a specific focus on brain captioning due to its enhanced flexibility as compared to brain decoding into images. Our approach takes advantage of cutting-edge image captioning models and incorporates a unique image reconstruction pipeline that utilizes latent diffusion models and depth estimation. We utilized the Natural Scenes Dataset, a comprehensive fMRI dataset from eight subjects who viewed images from the COCO dataset. We employed the Generative Image-to-text Transformer (GIT) as our backbone for captioning and propose a new image reconstruction pipeline based on latent diffusion models. The method involves training regularized linear regression models between brain activity and extracted features. Additionally, we incorporated depth maps from the ControlNet model to further guide the reconstruction process. We evaluate our methods using quantitative metrics for both generated captions and images. Our brain captioning approach outperforms existing methods, while our image reconstruction pipeline generates plausible images with improved spatial relationships. In conclusion, we demonstrate significant progress in brain decoding, showcasing the enormous potential of integrating vision and language to better understand human cognition. Our approach provides a flexible platform for future research, with potential applications in various fields, including neural art, style transfer, and portable devices.\n        \u25b3 Less\n      ",
    "title": "Brain Captioning: Decoding human brain activity into images and text",
    "date": "19 May, 2023",
    "authors": [
      "Matteo Ferrante",
      " Furkan Ozcelik",
      " Tommaso Boccato",
      " Rufin VanRullen",
      " Nicola Toschi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.11275",
    "paper_id": "2211.11275",
    "abstract": "\n        Although speech is a simple and effective way for humans to communicate with the outside world, a more realistic speech interaction contains multimodal information, e.g., vision, text. How to design a unified framework to integrate different modal information and leverage different resources (e.g., visual-audio pairs, audio-text pairs, unlabeled speech, and unlabeled text) to facilitate speech representation learning was not well explored. In this paper, we propose a unified cross-modal representation learning framework VATLM (Visual-Audio-Text Language Model). The proposed VATLM employs a unified backbone network to model the modality-independent information and utilizes three simple modality-dependent modules to preprocess visual, speech, and text inputs. In order to integrate these three modalities into one shared semantic space, VATLM is optimized with a masked prediction task of unified tokens, given by our proposed unified tokenizer. We evaluate the pre-trained VATLM on audio-visual related downstream tasks, including audio-visual speech recognition (AVSR), visual speech recognition (VSR) tasks. Results show that the proposed VATLM outperforms previous the state-of-the-art models, such as audio-visual pre-trained AV-HuBERT model, and analysis also demonstrates that VATLM is capable of aligning different modalities into the same space. To facilitate future research, we release the code and pre-trained models at https://aka.ms/vatlm.\n        \u25b3 Less\n      ",
    "title": "VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning",
    "date": "19 May, 2023",
    "authors": [
      "Qiushi Zhu",
      " Long Zhou",
      " Ziqiang Zhang",
      " Shujie Liu",
      " Binxing Jiao",
      " Jie Zhang",
      " Lirong Dai",
      " Daxin Jiang",
      " Jinyu Li",
      " Furu Wei"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11581",
    "paper_id": "2305.11581",
    "abstract": "\n        While the increased use of AI in the manufacturing sector has been widely noted, there is little understanding on the risks that it may raise in a manufacturing organisation. Although various high level frameworks and definitions have been proposed to consolidate potential risks, practitioners struggle with understanding and implementing them.\n  This lack of understanding exposes manufacturing to a multitude of risks, including the organisation, its workers, as well as suppliers and clients. In this paper, we explore and interpret the applicability of responsible, ethical, and trustworthy AI within the context of manufacturing. We then use a broadened adaptation of a machine learning lifecycle to discuss, through the use of illustrative examples, how each step may result in a given AI trustworthiness concern. We additionally propose a number of research questions to the manufacturing research community, in order to help guide future research so that the economic and societal benefits envisaged by AI in manufacturing are delivered safely and responsibly.\n        \u25b3 Less\n      ",
    "title": "Trustworthy, responsible, ethical AI in manufacturing and supply chains: synthesis and emerging research questions",
    "date": "19 May, 2023",
    "authors": [
      "Alexandra Brintrup",
      " George Baryannis",
      " Ashutosh Tiwari",
      " Svetan Ratchev",
      " Giovanna Martinez-Arellano",
      " Jatinder Singh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2202.03932",
    "paper_id": "2202.03932",
    "abstract": "\n        As an emerging type of Neural Networks (NNs), Transformers are used in many domains ranging from Natural Language Processing to Autonomous Driving. In this paper, we study the robustness problem of Transformers, a key characteristic as low robustness may cause safety concerns. Specifically, we focus on Sparsemax-based Transformers and reduce the finding of their maximum robustness to a Mixed Integer Quadratically Constrained Programming (MIQCP) problem. We also design two pre-processing heuristics that can be embedded in the MIQCP encoding and substantially accelerate its solving. We then conduct experiments using the application of Land Departure Warning to compare the robustness of Sparsemax-based Transformers against that of the more conventional Multi-Layer-Perceptron (MLP) NNs. To our surprise, Transformers are not necessarily more robust, leading to profound considerations in selecting appropriate NN architectures for safety-critical domain applications.\n        \u25b3 Less\n      ",
    "title": "Are Transformers More Robust? Towards Exact Robustness Verification for Transformers",
    "date": "19 May, 2023",
    "authors": [
      "Brian Hsuan-Cheng Liao",
      " Chih-Hong Cheng",
      " Hasan Esen",
      " Alois Knoll"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11597",
    "paper_id": "2305.11597",
    "abstract": "\n        Trustworthiness of artificially intelligent agents is vital for the acceptance of human-machine teaming in industrial manufacturing environments. Predictable behaviours and explainable (and understandable) rationale allow humans collaborating with (and building) these agents to understand their motivations and therefore validate decisions that are made. To that aim, we make use of G\u00e4rdenfors's cognitively inspired Conceptual Space framework to represent the agent's knowledge using concepts as convex regions in a space spanned by inherently comprehensible quality dimensions. A simple typicality quantification model is built on top of it to determine fuzzy category membership and classify instances interpretably. We apply it on a use case from the manufacturing domain, using objects' physical properties obtained from cobots' onboard sensors and utilisation properties from crowdsourced commonsense knowledge available at public knowledge bases. Such flexible knowledge representation based on property decomposition allows for data-efficient representation learning of typically highly specialist or specific manufacturing artefacts. In such a setting, traditional data-driven (e.g., computer vision-based) classification approaches would struggle due to training data scarcity. This allows for comprehensibility of an AI agent's acquired knowledge by the human collaborator thus contributing to trustworthiness. We situate our approach within an existing explainability framework specifying explanation desiderata. We provide arguments for our system's applicability and appropriateness for different roles of human agents collaborating with the AI system throughout its design, validation, and operation.\n        \u25b3 Less\n      ",
    "title": "Flexible and Inherently Comprehensible Knowledge Representation for Data-Efficient Learning and Trustworthy Human-Machine Teaming in Manufacturing Environments",
    "date": "19 May, 2023",
    "authors": [
      "Vedran Galeti\u0107",
      " Alistair Nottle"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11598",
    "paper_id": "2305.11598",
    "abstract": "\n        The emergence of large language models (LLMs) has substantially influenced natural language processing, demonstrating exceptional results across various tasks. In this study, we employ ``Introspective Tips\" to facilitate LLMs in self-optimizing their decision-making. By introspectively examining trajectories, LLM refines its policy by generating succinct and valuable tips. Our method enhances the agent's performance in both few-shot and zero-shot learning situations by considering three essential scenarios: learning from the agent's past experiences, integrating expert demonstrations, and generalizing across diverse games. Importantly, we accomplish these improvements without fine-tuning the LLM parameters; rather, we adjust the prompt to generalize insights from the three aforementioned situations. Our framework not only supports but also emphasizes the advantage of employing LLM in in-contxt decision-making. Experiments involving over 100 games in TextWorld illustrate the superior performance of our approach.\n        \u25b3 Less\n      ",
    "title": "Introspective Tips: Large Language Model for In-Context Decision Making",
    "date": "19 May, 2023",
    "authors": [
      "Liting Chen",
      " Lu Wang",
      " Hang Dong",
      " Yali Du",
      " Jie Yan",
      " Fangkai Yang",
      " Shuang Li",
      " Pu Zhao",
      " Si Qin",
      " Saravan Rajmohan",
      " Qingwei Lin",
      " Dongmei Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11605",
    "paper_id": "2305.11605",
    "abstract": "\n        We describe a proof-of-principle implementation of a system for drawing melodies that abstracts away from a note-level input representation via melodic contours. The aim is to allow users to express their musical intentions without requiring prior knowledge of how notes fit together melodiously. Current approaches to controllable melody generation often require users to choose parameters that are static across a whole sequence, via buttons or sliders. In contrast, our method allows users to quickly specify how parameters should change over time by drawing a contour.\n        \u25b3 Less\n      ",
    "title": "MIDI-Draw: Sketching to Control Melody Generation",
    "date": "19 May, 2023",
    "authors": [
      "Tashi Namgyal",
      " Peter Flach",
      " Raul Santos-Rodriguez"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11619",
    "paper_id": "2305.11619",
    "abstract": "\n        Automatic code generation has recently attracted large attention and is becoming more significant to the software development process. Solutions based on Machine Learning and Artificial Intelligence are being used to increase human and software efficiency in potent and innovative ways. In this paper, we aim to leverage these developments and introduce a novel approach to generating frontend component code for the popular Angular framework. We propose to do this using behavior-driven development test specifications as input to a transformer-based machine learning model. Our approach aims to drastically reduce the development time needed for web applications while potentially increasing software quality and introducing new research ideas toward automatic code generation.\n        \u25b3 Less\n      ",
    "title": "Towards Code Generation from BDD Test Case Specifications: A Vision",
    "date": "19 May, 2023",
    "authors": [
      "Leon Chemnitz",
      " David Reichenbach",
      " Hani Aldebes",
      " Mariam Naveed",
      " Krishna Narasimhan",
      " Mira Mezini"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.07530",
    "paper_id": "2212.07530",
    "abstract": "\n        Multilingual machine translation models can benefit from synergy between different language pairs, but also suffer from interference. While there is a growing number of sophisticated methods that aim to eliminate interference, our understanding of interference as a phenomenon is still limited. This work identifies the main factors that contribute to interference in multilingual machine translation. Through systematic experimentation, we find that interference (or synergy) are primarily determined by model size, data size, and the proportion of each language pair within the total dataset. We observe that substantial interference occurs mainly when the model is very small with respect to the available training data, and that using standard transformer configurations with less than one billion parameters largely alleviates interference and promotes synergy. Moreover, we show that tuning the sampling temperature to control the proportion of each language pair in the data is key to balancing the amount of interference between low and high resource language pairs effectively, and can lead to superior performance overall.\n        \u25b3 Less\n      ",
    "title": "Causes and Cures for Interference in Multilingual Translation",
    "date": "19 May, 2023",
    "authors": [
      "Uri Shaham",
      " Maha Elbayad",
      " Vedanuj Goswami",
      " Omer Levy",
      " Shruti Bhosale"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2208.11311",
    "paper_id": "2208.11311",
    "abstract": "\n        In federated learning, all networked clients contribute to the model training cooperatively. However, with model sizes increasing, even sharing the trained partial models often leads to severe communication bottlenecks in underlying networks, especially when communicated iteratively. In this paper, we introduce a federated learning framework FedD3 requiring only one-shot communication by integrating dataset distillation instances. Instead of sharing model updates in other federated learning approaches, FedD3 allows the connected clients to distill the local datasets independently, and then aggregates those decentralized distilled datasets (e.g. a few unrecognizable images) from networks for model training. Our experimental results show that FedD3 significantly outperforms other federated learning frameworks in terms of needed communication volumes, while it provides the additional benefit to be able to balance the trade-off between accuracy and communication cost, depending on usage scenario or target dataset. For instance, for training an AlexNet model on CIFAR-10 with 10 clients under non-independent and identically distributed (Non-IID) setting, FedD3 can either increase the accuracy by over 71% with a similar communication volume, or save 98% of communication volume, while reaching the same accuracy, compared to other one-shot federated learning approaches.\n        \u25b3 Less\n      ",
    "title": "Federated Learning via Decentralized Dataset Distillation in Resource-Constrained Edge Environments",
    "date": "19 May, 2023",
    "authors": [
      "Rui Song",
      " Dai Liu",
      " Dave Zhenyu Chen",
      " Andreas Festag",
      " Carsten Trinitis",
      " Martin Schulz",
      " Alois Knoll"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11648",
    "paper_id": "2305.11648",
    "abstract": "\n        Multi-objective optimisation problems involve finding solutions with varying trade-offs between multiple and often conflicting objectives. Ising machines are physical devices that aim to find the absolute or approximate ground states of an Ising model. To apply Ising machines to multi-objective problems, a weighted sum objective function is used to convert multi-objective into single-objective problems. However, deriving scalarisation weights that archives evenly distributed solutions across the Pareto front is not trivial. Previous work has shown that adaptive weights based on dichotomic search, and one based on averages of previously explored weights can explore the Pareto front quicker than uniformly generated weights. However, these adaptive methods have only been applied to bi-objective problems in the past. In this work, we extend the adaptive method based on averages in two ways: (i)~we extend the adaptive method of deriving scalarisation weights for problems with two or more objectives, and (ii)~we use an alternative measure of distance to improve performance.\n  We compare the proposed method with existing ones and show that it leads to the best performance on multi-objective Unconstrained Binary Quadratic Programming (mUBQP) instances with 3 and 4 objectives and that it is competitive with the best one for instances with 2 objectives.\n        \u25b3 Less\n      ",
    "title": "Applying Ising Machines to Multi-objective QUBOs",
    "date": "19 May, 2023",
    "authors": [
      "Mayowa Ayodele",
      " Richard Allmendinger",
      " Manuel L\u00f3pez-Ib\u00e1\u00f1ez",
      " Arnaud Liefooghe",
      " Matthieu Parizy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11663",
    "paper_id": "2305.11663",
    "abstract": "\n        This commentary tests a methodology proposed by Munk et al. (2022) for using failed predictions in machine learning as a method to identify ambiguous and rich cases for qualitative analysis. Using a dataset describing actions performed by fictional characters interacting with machine vision technologies in 500 artworks, movies, novels and videogames, I trained a simple machine learning algorithm (using the kNN algorithm in R) to predict whether or not an action was active or passive using only information about the fictional characters. Predictable actions were generally unemotional and unambiguous activities where machine vision technologies were treated as simple tools. Unpredictable actions, that is, actions that the algorithm could not correctly predict, were more ambivalent and emotionally loaded, with more complex power relationships between characters and technologies. The results thus support Munk et al.'s theory that failed predictions can be productively used to identify rich cases for qualitative analysis. This test goes beyond simply replicating Munk et al.'s results by demonstrating that the method can be applied to a broader humanities domain, and that it does not require complex neural networks but can also work with a simpler machine learning algorithm. Further research is needed to develop an understanding of what kinds of data the method is useful for and which kinds of machine learning are most generative. To support this, the R code required to produce the results is included so the test can be replicated. The code can also be reused or adapted to test the method on other datasets.\n        \u25b3 Less\n      ",
    "title": "Algorithmic failure as a humanities methodology: machine learning's mispredictions identify rich cases for qualitative analysis",
    "date": "19 May, 2023",
    "authors": [
      "Jill Walker Rettberg"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.04641",
    "paper_id": "2304.04641",
    "abstract": "\n        Federated learning (FL) is a new distributed learning paradigm, with privacy, utility, and efficiency as its primary pillars. Existing research indicates that it is unlikely to simultaneously attain infinitesimal privacy leakage, utility loss, and efficiency. Therefore, how to find an optimal trade-off solution is the key consideration when designing the FL algorithm. One common way is to cast the trade-off problem as a multi-objective optimization problem, i.e., the goal is to minimize the utility loss and efficiency reduction while constraining the privacy leakage not exceeding a predefined value. However, existing multi-objective optimization frameworks are very time-consuming, and do not guarantee the existence of the Pareto frontier, this motivates us to seek a solution to transform the multi-objective problem into a single-objective problem because it is more efficient and easier to be solved. To this end, we propose FedPAC, a unified framework that leverages PAC learning to quantify multiple objectives in terms of sample complexity, such quantification allows us to constrain the solution space of multiple objectives to a shared dimension, so that it can be solved with the help of a single-objective optimization algorithm. Specifically, we provide the results and detailed analyses of how to quantify the utility loss, privacy leakage, privacy-utility-efficiency trade-off, as well as the cost of the attacker from the PAC learning perspective.\n        \u25b3 Less\n      ",
    "title": "Probably Approximately Correct Federated Learning",
    "date": "19 May, 2023",
    "authors": [
      "Xiaojin Zhang",
      " Anbu Huang",
      " Lixin Fan",
      " Kai Chen",
      " Qiang Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.03075",
    "paper_id": "2207.03075",
    "abstract": "\n        Federated learning (FL) is an active area of research. One of the most suitable areas for adopting FL is the medical domain, where patient privacy must be respected. Previous research, however, does not provide a practical guide to applying FL in the medical domain. We propose empirical benchmarks and experimental settings for three representative medical datasets with different modalities: longitudinal electronic health records, skin cancer images, and electrocardiogram signals. The likely users of FL such as medical institutions and IT companies can take these benchmarks as guides for adopting FL and minimize their trial and error. For each dataset, each client data is from a different source to preserve real-world heterogeneity. We evaluate six FL algorithms designed for addressing data heterogeneity among clients, and a hybrid algorithm combining the strengths of two representative FL algorithms. Based on experiment results from three modalities, we discover that simple FL algorithms tend to outperform more sophisticated ones, while the hybrid algorithm consistently shows good, if not the best performance. We also find that a frequent global model update leads to better performance under a fixed training iteration budget. As the number of participating clients increases, higher cost is incurred due to increased IT administrators and GPUs, but the performance consistently increases. We expect future users will refer to these empirical benchmarks to design the FL experiments in the medical domain considering their clinical tasks and obtain stronger performance with lower costs.\n        \u25b3 Less\n      ",
    "title": "Towards the Practical Utility of Federated Learning in the Medical Domain",
    "date": "19 May, 2023",
    "authors": [
      "Seongjun Yang",
      " Hyeonji Hwang",
      " Daeyoung Kim",
      " Radhika Dua",
      " Jong-Yeup Kim",
      " Eunho Yang",
      " Edward Choi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.07242",
    "paper_id": "2303.07242",
    "abstract": "\n        Sensing technologies deployed in the workplace can unobtrusively collect detailed data about individual activities and group interactions that are otherwise difficult to capture. A hopeful application of these technologies is that they can help businesses and workers optimize productivity and wellbeing. However, given the workplace's inherent and structural power dynamics, the prevalent approach of accepting tacit compliance to monitor work activities rather than seeking workers' meaningful consent raises privacy and ethical concerns. This paper unpacks the challenges workers face when consenting to workplace wellbeing technologies. Using a hypothetical case to prompt reflection among six multi-stakeholder focus groups involving 15 participants, we explored participants' expectations and capacity to consent to these technologies. We sketched possible interventions that could better support meaningful consent to workplace wellbeing technologies by drawing on critical computing and feminist scholarship -- which reframes consent from a purely individual choice to a structural condition experienced at the individual level that needs to be freely given, reversible, informed, enthusiastic, and specific (FRIES). The focus groups revealed how workers are vulnerable to \"meaningless\" consent -- as they may be subject to power dynamics that minimize their ability to withhold consent and may thus experience an erosion of autonomy, also undermining the value of data gathered in the name of \"wellbeing.\" To meaningfully consent, participants wanted changes to the technology and to the policies and practices surrounding the technology. Our mapping of what prevents workers from meaningfully consenting to workplace wellbeing technologies (challenges) and what they require to do so (interventions) illustrates how the lack of meaningful consent is a structural problem requiring socio-technical solutions.\n        \u25b3 Less\n      ",
    "title": "Can Workers Meaningfully Consent to Workplace Wellbeing Technologies?",
    "date": "19 May, 2023",
    "authors": [
      "Shreya Chowdhary",
      " Anna Kawakami",
      " Mary L. Gray",
      " Jina Suh",
      " Alexandra Olteanu",
      " Koustuv Saha"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11692",
    "paper_id": "2305.11692",
    "abstract": "\n        Despite the availability of computer-aided simulators and recorded videos of surgical procedures, junior residents still heavily rely on experts to answer their queries. However, expert surgeons are often overloaded with clinical and academic workloads and limit their time in answering. For this purpose, we develop a surgical question-answering system to facilitate robot-assisted surgical scene and activity understanding from recorded videos. Most of the existing VQA methods require an object detector and regions based feature extractor to extract visual features and fuse them with the embedded text of the question for answer generation. However, (1) surgical object detection model is scarce due to smaller datasets and lack of bounding box annotation; (2) current fusion strategy of heterogeneous modalities like text and image is naive; (3) the localized answering is missing, which is crucial in complex surgical scenarios. In this paper, we propose Visual Question Localized-Answering in Robotic Surgery (Surgical-VQLA) to localize the specific surgical area during the answer prediction. To deal with the fusion of the heterogeneous modalities, we design gated vision-language embedding (GVLE) to build input patches for the Language Vision Transformer (LViT) to predict the answer. To get localization, we add the detection head in parallel with the prediction head of the LViT. We also integrate GIoU loss to boost localization performance by preserving the accuracy of the question-answering model. We annotate two datasets of VQLA by utilizing publicly available surgical videos from MICCAI challenges EndoVis-17 and 18. Our validation results suggest that Surgical-VQLA can better understand the surgical scene and localize the specific area related to the question-answering. GVLE presents an efficient language-vision embedding technique by showing superior performance over the existing benchmarks.\n        \u25b3 Less\n      ",
    "title": "Surgical-VQLA: Transformer with Gated Vision-Language Embedding for Visual Question Localized-Answering in Robotic Surgery",
    "date": "19 May, 2023",
    "authors": [
      "Long Bai",
      " Mobarakol Islam",
      " Lalithkumar Seenivasan",
      " Hongliang Ren"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.01762",
    "paper_id": "2304.01762",
    "abstract": "\n        Conventional Bayesian Neural Networks (BNNs) cannot leverage unlabelled data to improve their predictions. To overcome this limitation, we introduce Self-Supervised Bayesian Neural Networks, which use unlabelled data to learn improved prior predictive distributions by maximising an evidence lower bound during an unsupervised pre-training step. With a novel methodology developed to better understand prior predictive distributions, we then show that self-supervised prior predictives capture image semantics better than conventional BNN priors. In our empirical evaluations, we see that self-supervised BNNs offer the label efficiency of self-supervised methods and the uncertainty estimates of Bayesian methods, particularly outperforming conventional BNNs in low-to-medium data regimes.\n        \u25b3 Less\n      ",
    "title": "Incorporating Unlabelled Data into Bayesian Neural Networks",
    "date": "19 May, 2023",
    "authors": [
      "Mrinank Sharma",
      " Tom Rainforth",
      " Yee Whye Teh",
      " Vincent Fortuin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.10481",
    "paper_id": "2212.10481",
    "abstract": "\n        To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among top-performing code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling -- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.\n        \u25b3 Less\n      ",
    "title": "Execution-Based Evaluation for Open-Domain Code Generation",
    "date": "19 May, 2023",
    "authors": [
      "Zhiruo Wang",
      " Shuyan Zhou",
      " Daniel Fried",
      " Graham Neubig"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11000",
    "paper_id": "2305.11000",
    "abstract": "\n        Multi-modal large language models are regarded as a crucial step towards Artificial General Intelligence (AGI) and have garnered significant interest with the emergence of ChatGPT. However, current speech-language models typically adopt the cascade paradigm, preventing inter-modal knowledge transfer. In this paper, we propose SpeechGPT, a large language model with intrinsic cross-modal conversational abilities, capable of perceiving and generating multi-model content. With discrete speech representations, we first construct SpeechInstruct, a large-scale cross-modal speech instruction dataset. Additionally, we employ a three-stage training strategy that includes modality-adaptation pre-training, cross-modal instruction fine-tuning, and chain-of-modality instruction fine-tuning. The experimental results demonstrate that SpeechGPT has an impressive capacity to follow multi-modal human instructions and highlight the potential of handling multiple modalities with one model. Demos are shown in https://0nutation.github.io/SpeechGPT.github.io/.\n        \u25b3 Less\n      ",
    "title": "SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal Conversational Abilities",
    "date": "19 May, 2023",
    "authors": [
      "Dong Zhang",
      " Shimin Li",
      " Xin Zhang",
      " Jun Zhan",
      " Pengyu Wang",
      " Yaqian Zhou",
      " Xipeng Qiu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11928",
    "paper_id": "2305.11928",
    "abstract": "\n        Energy efficiency is a crucial requirement for enabling powerful artificial intelligence applications at the microedge. Hardware acceleration with frugal architectural allocation is an effective method for reducing energy. Many emerging applications also require the systems design to incorporate interpretable decision models to establish responsibility and transparency. The design needs to provision for additional resources to provide reachable states in real-world data scenarios, defining conflicting design tradeoffs between energy efficiency. is challenging.\n  Recently a new machine learning algorithm, called the Tsetlin machine, has been proposed. The algorithm is fundamentally based on the principles of finite-state automata and benefits from natural logic underpinning rather than arithmetic. In this paper, we investigate methods of energy-frugal artificial intelligence hardware design by suitably tuning the hyperparameters, while maintaining high learning efficacy. To demonstrate interpretability, we use reachability and game-theoretic analysis in two simulation environments: a SystemC model to study the bounded state transitions in the presence of hardware faults and Nash equilibrium between states to analyze the learning convergence. Our analyses provides the first insights into conflicting design tradeoffs involved in energy-efficient and interpretable decision models for this new artificial intelligence hardware architecture. We show that frugal resource allocation coupled with systematic prodigality between randomized reinforcements can provide decisive energy reduction while also achieving robust and interpretable learning.\n        \u25b3 Less\n      ",
    "title": "Energy-frugal and Interpretable AI Hardware Design using Learning Automata",
    "date": "19 May, 2023",
    "authors": [
      "Rishad Shafik",
      " Tousif Rahman",
      " Adrian Wheeldon",
      " Ole-Christoffer Granmo",
      " Alex Yakovlev"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11759",
    "paper_id": "2305.11759",
    "abstract": "\n        Large Language Models (LLMs) are known to memorize significant portions of their training data. Parts of this memorized content have been shown to be extractable by simply querying the model, which poses a privacy risk. We present a novel approach which uses prompt-tuning to control the extraction rates of memorized content in LLMs. We present two prompt training strategies to increase and decrease extraction rates, which correspond to an attack and a defense, respectively. We demonstrate the effectiveness of our techniques by using models from the GPT-Neo family on a public benchmark. For the 1.3B parameter GPT-Neo model, our attack yields a 9.3 percentage point increase in extraction rate compared to our baseline. Our defense can be tuned to achieve different privacy-utility trade-offs by a user-specified hyperparameter. We achieve an extraction rate reduction of up to 97.7% relative to our baseline, with a perplexity increase of 16.9%.\n        \u25b3 Less\n      ",
    "title": "Controlling the Extraction of Memorized Data from Large Language Models via Prompt-Tuning",
    "date": "19 May, 2023",
    "authors": [
      "Mustafa Safa Ozdayi",
      " Charith Peris",
      " Jack FitzGerald",
      " Christophe Dupuy",
      " Jimit Majmudar",
      " Haidar Khan",
      " Rahil Parikh",
      " Rahul Gupta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11779",
    "paper_id": "2305.11779",
    "abstract": "\n        The recognition of dataset names is a critical task for automatic information extraction in scientific literature, enabling researchers to understand and identify research opportunities. However, existing corpora for dataset mention detection are limited in size and naming diversity. In this paper, we introduce the Dataset Mentions Detection Dataset (DMDD), the largest publicly available corpus for this task. DMDD consists of the DMDD main corpus, comprising 31,219 scientific articles with over 449,000 dataset mentions weakly annotated in the format of in-text spans, and an evaluation set, which comprises of 450 scientific articles manually annotated for evaluation purposes. We use DMDD to establish baseline performance for dataset mention detection and linking. By analyzing the performance of various models on DMDD, we are able to identify open problems in dataset mention detection. We invite the community to use our dataset as a challenge to develop novel dataset mention detection models.\n        \u25b3 Less\n      ",
    "title": "DMDD: A Large-Scale Dataset for Dataset Mentions Detection",
    "date": "19 May, 2023",
    "authors": [
      "Huitong Pan",
      " Qi Zhang",
      " Eduard Dragut",
      " Cornelia Caragea",
      " Longin Jan Latecki"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08196",
    "paper_id": "2305.08196",
    "abstract": "\n        Artificial intelligence (AI) is evolving towards artificial general intelligence, which refers to the ability of an AI system to perform a wide range of tasks and exhibit a level of intelligence similar to that of a human being. This is in contrast to narrow or specialized AI, which is designed to perform specific tasks with a high degree of efficiency. Therefore, it is urgent to design a general class of models, which we term foundation models, trained on broad data that can be adapted to various downstream tasks. The recently proposed segment anything model (SAM) has made significant progress in breaking the boundaries of segmentation, greatly promoting the development of foundation models for computer vision. To fully comprehend SAM, we conduct a survey study. As the first to comprehensively review the progress of segmenting anything task for vision and beyond based on the foundation model of SAM, this work focuses on its applications to various tasks and data types by discussing its historical development, recent progress, and profound impact on broad applications. We first introduce the background and terminology for foundation models including SAM, as well as state-of-the-art methods contemporaneous with SAM that are significant for segmenting anything task. Then, we analyze and summarize the advantages and limitations of SAM across various image processing applications, including software scenes, real-world scenes, and complex scenes. Importantly, many insights are drawn to guide future research to develop more versatile foundation models and improve the architecture of SAM. We also summarize massive other amazing applications of SAM in vision and beyond. Finally, we maintain a continuously updated paper list and an open-source project summary for foundation model SAM at \\href{https://github.com/liliu-avril/Awesome-Segment-Anything}{\\color{magenta}{here}}.\n        \u25b3 Less\n      ",
    "title": "A Comprehensive Survey on Segment Anything Model for Vision and Beyond",
    "date": "19 May, 2023",
    "authors": [
      "Chunhui Zhang",
      " Li Liu",
      " Yawen Cui",
      " Guanjie Huang",
      " Weilin Lin",
      " Yiqian Yang",
      " Yuehong Hu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11807",
    "paper_id": "2305.11807",
    "abstract": "\n        The Private Aggregation of Teacher Ensembles (PATE) is a machine learning framework that enables the creation of private models through the combination of multiple \"teacher\" models and a \"student\" model. The student model learns to predict an output based on the voting of the teachers, and the resulting model satisfies differential privacy. PATE has been shown to be effective in creating private models in semi-supervised settings or when protecting data labels is a priority. This paper explores whether the use of PATE can result in unfairness, and demonstrates that it can lead to accuracy disparities among groups of individuals. The paper also analyzes the algorithmic and data properties that contribute to these disproportionate impacts, why these aspects are affecting different groups disproportionately, and offers recommendations for mitigating these effects\n        \u25b3 Less\n      ",
    "title": "On the Fairness Impacts of Private Ensembles Models",
    "date": "19 May, 2023",
    "authors": [
      "Cuong Tran",
      " Ferdinando Fioretto"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11811",
    "paper_id": "2305.11811",
    "abstract": "\n        Decentralized partially observable Markov decision processes (Dec-POMDPs) formalize the problem of designing individual controllers for a group of collaborative agents under stochastic dynamics and partial observability. Seeking a global optimum is difficult (NEXP complete), but seeking a Nash equilibrium -- each agent policy being a best response to the other agents -- is more accessible, and allowed addressing infinite-horizon problems with solutions in the form of finite state controllers. In this paper, we show that this approach can be adapted to cases where only a generative model (a simulator) of the Dec-POMDP is available. This requires relying on a simulation-based POMDP solver to construct an agent's FSC node by node. A related process is used to heuristically derive initial FSCs. Experiment with benchmarks shows that MC-JESP is competitive with exisiting Dec-POMDP solvers, even better than many offline methods using explicit models.\n        \u25b3 Less\n      ",
    "title": "Monte-Carlo Search for an Equilibrium in Dec-POMDPs",
    "date": "19 May, 2023",
    "authors": [
      "Yang You",
      " Vincent Thomas",
      " Francis Colas",
      " Olivier Buffet"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.00251",
    "paper_id": "2211.00251",
    "abstract": "\n        Model selection is a strategy aimed at creating accurate and robust models. A key challenge in designing these algorithms is identifying the optimal model for classifying any particular input sample. This paper addresses this challenge and proposes a novel framework for differentiable model selection integrating machine learning and combinatorial optimization. The framework is tailored for ensemble learning, a strategy that combines the outputs of individually pre-trained models, and learns to select appropriate ensemble members for a particular input sample by transforming the ensemble learning task into a differentiable selection program trained end-to-end within the ensemble learning model. Tested on various tasks, the proposed framework demonstrates its versatility and effectiveness, outperforming conventional and advanced consensus rules across a variety of settings and learning tasks.\n        \u25b3 Less\n      ",
    "title": "Differentiable Model Selection for Ensemble Learning",
    "date": "19 May, 2023",
    "authors": [
      "James Kotary",
      " Vincenzo Di Vito",
      " Ferdinando Fioretto"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.06012",
    "paper_id": "2210.06012",
    "abstract": "\n        Agent based modelling (ABM) is a computational approach to modelling complex systems by specifying the behaviour of autonomous decision-making components or agents in the system and allowing the system dynamics to emerge from their interactions. Recent advances in the field of Multi-agent reinforcement learning (MARL) have made it feasible to study the equilibrium of complex environments where multiple agents learn simultaneously. However, most ABM frameworks are not RL-native, in that they do not offer concepts and interfaces that are compatible with the use of MARL to learn agent behaviours. In this paper, we introduce a new open-source framework, Phantom, to bridge the gap between ABM and MARL. Phantom is an RL-driven framework for agent-based modelling of complex multi-agent systems including, but not limited to economic systems and markets. The framework aims to provide the tools to simplify the ABM specification in a MARL-compatible way - including features to encode dynamic partial observability, agent utility functions, heterogeneity in agent preferences or types, and constraints on the order in which agents can act (e.g. Stackelberg games, or more complex turn-taking environments). In this paper, we present these features, their design rationale and present two new environments leveraging the framework.\n        \u25b3 Less\n      ",
    "title": "Phantom -- A RL-driven multi-agent framework to model complex systems",
    "date": "19 May, 2023",
    "authors": [
      "Leo Ardon",
      " Jared Vann",
      " Deepeka Garg",
      " Tom Spooner",
      " Sumitra Ganesh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11833",
    "paper_id": "2305.11833",
    "abstract": "\n        We study the complexity of the problem of training neural networks defined via various activation functions. The training problem is known to be existsR-complete with respect to linear activation functions and the ReLU activation function. We consider the complexity of the problem with respect to the sigmoid activation function and other effectively continuous functions. We show that these training problems are polynomial-time many-one bireducible to the existential theory of the reals extended with the corresponding activation functions. In particular, we establish that the sigmoid activation function leads to the existential theory of the reals with the exponential function. It is thus open, and equivalent with the decidability of the existential theory of the reals with the exponential function, whether training neural networks using the sigmoid activation function is algorithmically solvable. In contrast, we obtain that the training problem is undecidable if sinusoidal activation functions are considered. Finally, we obtain general upper bounds for the complexity of the training problem in the form of low levels of the arithmetical hierarchy.\n        \u25b3 Less\n      ",
    "title": "Complexity of Neural Network Training and ETR: Extensions with Effectively Continuous Functions",
    "date": "19 May, 2023",
    "authors": [
      "Teemu Hankala",
      " Miika Hannula",
      " Juha Kontinen",
      " Jonni Virtema"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11844",
    "paper_id": "2305.11844",
    "abstract": "\n          This paper presents a community-centered study of cultural limitations of text-to-image (T2I) models in the South Asian context. We theorize these failures using scholarship on dominant media regimes of representations and locate them within participants' reporting of their existing social marginalizations. We thus show how generative AI can reproduce an outsiders gaze for viewing South Asian cultures, shaped by global and regional power inequities. By centering communities as experts and soliciting their perspectives on T2I limitations, our study adds rich nuance into existing evaluative frameworks and deepens our understanding of the culturally-specific ways AI technologies can fail in non-Western and Global South settings. We distill lessons for responsible development of T2I models, recommending concrete pathways forward that can allow for recognition of structural inequalities.\n        \u25b3 Less\n      ",
    "title": "AI's Regimes of Representation: A Community-centered Study of Text-to-Image Models in South Asia",
    "date": "19 May, 2023",
    "authors": [
      "Rida Qadri",
      " Renee Shelby",
      " Cynthia L. Bennett",
      " Remi Denton"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11845",
    "paper_id": "2305.11845",
    "abstract": "\n        Reaction diagram parsing is the task of extracting reaction schemes from a diagram in the chemistry literature. The reaction diagrams can be arbitrarily complex, thus robustly parsing them into structured data is an open challenge. In this paper, we present RxnScribe, a machine learning model for parsing reaction diagrams of varying styles. We formulate this structured prediction task with a sequence generation approach, which condenses the traditional pipeline into an end-to-end model. We train RxnScribe on a dataset of 1,378 diagrams and evaluate it with cross validation, achieving an 80.0% soft match F1 score, with significant improvements over previous models. Our code and data are publicly available at https://github.com/thomas0809/RxnScribe.\n        \u25b3 Less\n      ",
    "title": "RxnScribe: A Sequence Generation Model for Reaction Diagram Parsing",
    "date": "19 May, 2023",
    "authors": [
      "Yujie Qian",
      " Jiang Guo",
      " Zhengkai Tu",
      " Connor W. Coley",
      " Regina Barzilay"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.03945",
    "paper_id": "2210.03945",
    "abstract": "\n        Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -- i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl.\n        \u25b3 Less\n      ",
    "title": "Understanding HTML with Large Language Models",
    "date": "19 May, 2023",
    "authors": [
      "Izzeddin Gur",
      " Ofir Nachum",
      " Yingjie Miao",
      " Mustafa Safdari",
      " Austin Huang",
      " Aakanksha Chowdhery",
      " Sharan Narang",
      " Noah Fiedel",
      " Aleksandra Faust"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.08141",
    "paper_id": "2209.08141",
    "abstract": "\n        Probabilistic models of language understanding are valuable tools for investigating human language use. However, they need to be hand-designed for a particular domain. In contrast, large language models (LLMs) are trained on text that spans a wide array of domains, but they lack the structure and interpretability of probabilistic models. In this paper, we use chain-of-thought prompts to introduce structures from probabilistic models into LLMs. We explore this approach in the case of metaphor understanding. Our chain-of-thought prompts lead language models to infer latent variables and reason about their relationships in order to choose appropriate paraphrases for metaphors. The latent variables and relationships chosen are informed by theories of metaphor understanding from cognitive psychology. We apply these prompts to the two largest versions of GPT-3 and show that they can improve performance in a paraphrase selection task.\n        \u25b3 Less\n      ",
    "title": "Psychologically-informed chain-of-thought prompts for metaphor understanding in large language models",
    "date": "19 May, 2023",
    "authors": [
      "Ben Prystawski",
      " Paul Thibodeau",
      " Christopher Potts",
      " Noah D. Goodman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.10573",
    "paper_id": "2304.10573",
    "abstract": "\n        Effective offline RL methods require properly handling out-of-distribution actions. Implicit Q-learning (IQL) addresses this by training a Q-function using only dataset actions through a modified Bellman backup. However, it is unclear which policy actually attains the values represented by this implicitly trained Q-function. In this paper, we reinterpret IQL as an actor-critic method by generalizing the critic objective and connecting it to a behavior-regularized implicit actor. This generalization shows how the induced actor balances reward maximization and divergence from the behavior policy, with the specific loss choice determining the nature of this tradeoff. Notably, this actor can exhibit complex and multimodal characteristics, suggesting issues with the conditional Gaussian actor fit with advantage weighted regression (AWR) used in prior methods. Instead, we propose using samples from a diffusion parameterized behavior policy and weights computed from the critic to then importance sampled our intended policy. We introduce Implicit Diffusion Q-learning (IDQL), combining our general IQL critic with the policy extraction method. IDQL maintains the ease of implementation of IQL while outperforming prior offline RL methods and demonstrating robustness to hyperparameters. Code is available at https://github.com/philippe-eecs/IDQL.\n        \u25b3 Less\n      ",
    "title": "IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies",
    "date": "19 May, 2023",
    "authors": [
      "Philippe Hansen-Estruch",
      " Ilya Kostrikov",
      " Michael Janner",
      " Jakub Grudzien Kuba",
      " Sergey Levine"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11965",
    "paper_id": "2305.11965",
    "abstract": "\n        In this paper, we aim to optimize a contrastive loss with individualized temperatures in a principled and systematic manner for self-supervised learning. The common practice of using a global temperature parameter \u03c4\u03c4 ignores the fact that ``not all semantics are created equal\", meaning that different anchor data may have different numbers of samples with similar semantics, especially when data exhibits long-tails. First, we propose a new robust contrastive loss inspired by distributionally robust optimization (DRO), providing us an intuition about the effect of \u03c4\u03c4 and a mechanism for automatic temperature individualization. Then, we propose an efficient stochastic algorithm for optimizing the robust contrastive loss with a provable convergence guarantee without using large mini-batch sizes. Theoretical and experimental results show that our algorithm automatically learns a suitable \u03c4\u03c4 for each sample. Specifically, samples with frequent semantics use large temperatures to keep local semantic structures, while samples with rare semantics use small temperatures to induce more separable features. Our method not only outperforms prior strong baselines (e.g., SimCLR, CLIP) on unimodal and bimodal datasets with larger improvements on imbalanced data but also is less sensitive to hyper-parameters. To our best knowledge, this is the first methodical approach to optimizing a contrastive loss with individualized temperatures.\n        \u25b3 Less\n      ",
    "title": "Not All Semantics are Created Equal: Contrastive Self-supervised Learning with Automatic Temperature Individualization",
    "date": "19 May, 2023",
    "authors": [
      "Zi-Hao Qiu",
      " Quanqi Hu",
      " Zhuoning Yuan",
      " Denny Zhou",
      " Lijun Zhang",
      " Tianbao Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11969",
    "paper_id": "2305.11969",
    "abstract": "\n        In Answer Set Programming (ASP), the user can define declaratively a problem and solve it with efficient solvers; practical applications of ASP are countless and several constraint problems have been successfully solved with ASP. On the other hand, solution time usually grows in a superlinear way (often, exponential) with respect to the size of the instance, which is impractical for large instances. A widely used approach is to split the optimization problem into sub-problems that are solved in sequence, some committing to the values assigned by others, and reconstructing a valid assignment for the whole problem by juxtaposing the solutions of the single sub-problems. On the one hand this approach is much faster, due to the superlinear behavior; on the other hand, it does not provide any guarantee of optimality: committing to the assignment of one sub-problem can rule out the optimal solution from the search space. In other research areas, Logic-Based Benders Decomposition (LBBD) proved effective; in LBBD, the problem is decomposed into a Master Problem (MP) and one or several Sub-Problems (SP). The solution of the MP is passed to the SPs, that can possibly fail. In case of failure, a no-good is returned to the MP, that is solved again with the addition of the new constraint. The solution process is iterated until a valid solution is obtained for all the sub-problems or the MP is proven infeasible. The obtained solution is provably optimal under very mild conditions. In this paper, we apply for the first time LBBD to ASP, exploiting an application in health care as case study. Experimental results show the effectiveness of the approach. We believe that the availability of LBBD can further increase the practical applicability of ASP technologies.\n        \u25b3 Less\n      ",
    "title": "Logic-Based Benders Decomposition in Answer Set Programming for Chronic Outpatients Scheduling",
    "date": "19 May, 2023",
    "authors": [
      "Paola Cappanera",
      " Marco Gavanelli",
      " Maddalena Nonato",
      " Marco Roma"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11981",
    "paper_id": "2305.11981",
    "abstract": "\n        The complexity and increasingly tight coupling of supply chains poses a major logistical challenge for leading companies. Another challenge is that leading companies -- under pressure from consumers, a critical public and legislative measures such as supply chain laws -- have to take more responsibility than before for their suppliers' labour standards. In this paper, we discuss a new approach that leading companies are using to try to address these challenges: algorithmic prediction of business risks, but also environmental and social risks. We describe the technical and cultural conditions for algorithmic prediction and explain how -- from the perspective of leading companies -- it helps to address both challenges. We then develop scenarios on how and with what kind of social consequences algorithmic prediction can be used by leading companies. From the scenarios, we derive policy options for different stakeholder groups to help develop algorithmic prediction towards improving labour standards and worker voice.\n  --\n  Die Komplexit\u00e4t und zunehmend enge Kopplung vieler Lieferketten stellt eine gro\u00dfe logistische Herausforderung f\u00fcr Leitunternehmen dar. Eine weitere Herausforderung besteht darin, dass Leitunternehmen -- gedr\u00e4ngt durch Konsument:innen, eine kritische \u00d6ffentlichkeit und gesetzgeberische Ma\u00dfnahmen wie die Lieferkettengesetze -- st\u00e4rker als bisher Verantwortung f\u00fcr Arbeitsstandards in ihren Zulieferbetrieben \u00fcbernehmen m\u00fcssen. In diesem Beitrag diskutieren wir einen neuen Ansatz, mit dem Leitunternehmen versuchen, diese Herausforderungen zu bearbeiten: die algorithmische Vorhersage von betriebswirtschaftlichen, aber auch \u00f6kologischen und sozialen Risiken. Wir beschreiben die technischen und kulturellen Bedingungen f\u00fcr algorithmische Vorhersage und erkl\u00e4ren, wie diese -- aus Perspektive von Leitunternehmen -- bei der Bearbeitung beider Herausforderungen hilft. Anschlie\u00dfend entwickeln wir Szenarien, wie und mit welchen sozialen Konsequenzen algorithmische Vorhersage durch Leitunternehmen eingesetzt werden kann. Aus den Szenarien leiten wir Handlungsoptionen f\u00fcr verschiedene Stakeholder-Gruppen ab, die dabei helfen sollen, algorithmische Vorhersage im Sinne einer Verbesserung von Arbeitsstandards und Workers' Voice weiterzuentwickeln.\n        \u25b3 Less\n      ",
    "title": "\"Sch\u00f6ne neue Lieferkettenwelt\": Workers' Voice und Arbeitsstandards in Zeiten algorithmischer Vorhersage",
    "date": "19 May, 2023",
    "authors": [
      "Lukas Daniel Klausner",
      " Maximilian Heimst\u00e4dt",
      " Leonhard Dobusch"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14373",
    "paper_id": "2305.14373",
    "abstract": "\n        Most semi-supervised learning (SSL) models entail complex structures and iterative training processes as well as face difficulties in interpreting their predictions to users. To address these issues, this paper proposes a new interpretable SSL model using the supervised and unsupervised Adaptive Resonance Theory (ART) family of networks, which is denoted as SSL-ART. Firstly, SSL-ART adopts an unsupervised fuzzy ART network to create a number of prototype nodes using unlabeled samples. Then, it leverages a supervised fuzzy ARTMAP structure to map the established prototype nodes to the target classes using labeled samples. Specifically, a one-to-many (OtM) mapping scheme is devised to associate a prototype node with more than one class label. The main advantages of SSL-ART include the capability of: (i) performing online learning, (ii) reducing the number of redundant prototype nodes through the OtM mapping scheme and minimizing the effects of noisy samples, and (iii) providing an explanation facility for users to interpret the predicted outcomes. In addition, a weighted voting strategy is introduced to form an ensemble SSL-ART model, which is denoted as WESSL-ART. Every ensemble member, i.e., SSL-ART, assigns {\\color{black}a different weight} to each class based on its performance pertaining to the corresponding class. The aim is to mitigate the effects of training data sequences on all SSL-ART members and improve the overall performance of WESSL-ART. The experimental results on eighteen benchmark data sets, three artificially generated data sets, and a real-world case study indicate the benefits of the proposed SSL-ART and WESSL-ART models for tackling pattern classification problems.\n        \u25b3 Less\n      ",
    "title": "An Ensemble Semi-Supervised Adaptive Resonance Theory Model with Explanation Capability for Pattern Classification",
    "date": "19 May, 2023",
    "authors": [
      "Farhad Pourpanah",
      " Chee Peng Lim",
      " Ali Etemad",
      " Q. M. Jonathan Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12013",
    "paper_id": "2305.12013",
    "abstract": "\n        Generative AI tools introduce new and accessible forms of media creation for youth. They also raise ethical concerns about the generation of fake media, data protection, privacy and ownership of AI-generated art. Since generative AI is already being used in products used by youth, it is critical that they understand how these tools work and how they can be used or misused. In this work, we facilitated students' generative AI learning through expression of their imagined future identities. We designed a learning workshop - Dreaming with AI - where students learned about the inner workings of generative AI tools, used text-to-image generation algorithms to create their imaged future dreams, reflected on the potential benefits and harms of generative AI tools and voiced their opinions about policies for the use of these tools in classrooms. In this paper, we present the learning activities and experiences of 34 high school students who engaged in our workshops. Students reached creative learning objectives by using prompt engineering to create their future dreams, gained technical knowledge by learning the abilities, limitations, text-visual mappings and applications of generative AI, and identified most potential societal benefits and harms of generative AI.\n        \u25b3 Less\n      ",
    "title": "Constructing Dreams using Generative AI",
    "date": "19 May, 2023",
    "authors": [
      "Safinah Ali",
      " Daniella DiPaola",
      " Randi Williams",
      " Prerna Ravi",
      " Cynthia Breazeal"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12030",
    "paper_id": "2305.12030",
    "abstract": "\n        Continual learning~(CL) is a field concerned with learning a series of inter-related task with the tasks typically defined in the sense of either regression or classification. In recent years, CL has been studied extensively when these tasks are defined using Euclidean data -- data, such as images, that can be described by a set of vectors in an n-dimensional real space. However, the literature is quite sparse, when the data corresponding to a CL task is nonEuclidean -- data , such as graphs, point clouds or manifold, where the notion of similarity in the sense of Euclidean metric does not hold. For instance, a graph is described by a tuple of vertices and edges and similarities between two graphs is not well defined through a Euclidean metric. Due to this fundamental nature of the data, developing CL for nonEuclidean data presents several theoretical and methodological challenges. In particular, CL for graphs requires explicit modelling of nonstationary behavior of vertices and edges and their effects on the learning problem. Therefore, in this work, we develop a adaptive dynamic programming viewpoint for CL with graphs. In this work, we formulate a two-player sequential game between the act of learning new tasks~(generalization) and remembering previously learned tasks~(forgetting). We prove mathematically the existence of a solution to the game and demonstrate convergence to the solution of the game. Finally, we demonstrate the efficacy of our method on a number of graph benchmarks with a comprehensive ablation study while establishing state-of-the-art performance.\n        \u25b3 Less\n      ",
    "title": "Learning Continually on a Sequence of Graphs -- The Dynamical System Way",
    "date": "19 May, 2023",
    "authors": [
      "Krishnan Raghavan",
      " Prasanna Balaprakash"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.05732",
    "paper_id": "2211.05732",
    "abstract": "\n        We study the hidden-action principal-agent problem in an online setting. In each round, the principal posts a contract that specifies the payment to the agent based on each outcome. The agent then makes a strategic choice of action that maximizes her own utility, but the action is not directly observable by the principal. The principal observes the outcome and receives utility from the agent's choice of action. Based on past observations, the principal dynamically adjusts the contracts with the goal of maximizing her utility.\n  We introduce an online learning algorithm and provide an upper bound on its Stackelberg regret. We show that when the contract space is [0,1]^m[0,1]^m, the Stackelberg regret is upper bounded by \\widetilde O(\\sqrt{m} \\cdot T^{1-1/(2m+1)})\\widetilde O(\\sqrt{m} \\cdot T^{1-1/(2m+1)}), and lower bounded by \u03a9(T^{1-1/(m+2)})\u03a9(T^{1-1/(m+2)}), where \\widetilde O\\widetilde O omits logarithmic factors. This result shows that exponential-in-mm samples are sufficient and necessary to learn a near-optimal contract, resolving an open problem on the hardness of online contract design. Moreover, when contracts are restricted to some subset \\mathcal{F} \\subset [0,1]^m\\mathcal{F} \\subset [0,1]^m, we define an intrinsic dimension of \\mathcal{F}\\mathcal{F} that depends on the covering number of the spherical code in the space and bound the regret in terms of this intrinsic dimension. When \\mathcal{F}\\mathcal{F} is the family of linear contracts, we show that the Stackelberg regret grows exactly as \u0398(T^{2/3})\u0398(T^{2/3}).\n  The contract design problem is challenging because the utility function is discontinuous. Bounding the discretization error in this setting has been an open problem. In this paper, we identify a limited set of directions in which the utility function is continuous, allowing us to design a new discretization method and bound its error. This approach enables the first upper bound with no restrictions on the contract and action space.\n        \u25b3 Less\n      ",
    "title": "The Sample Complexity of Online Contract Design",
    "date": "19 May, 2023",
    "authors": [
      "Banghua Zhu",
      " Stephen Bates",
      " Zhuoran Yang",
      " Yixin Wang",
      " Jiantao Jiao",
      " Michael I. Jordan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18315",
    "paper_id": "2305.18315",
    "abstract": "\n        A basic task for most Legal Artificial Intelligence (Legal AI) applications is Named Entity Recognition (NER). However, texts produced in the context of legal practice make references to entities that are not trivially recognized by the currently available NERs. There is a lack of categorization of legislation, jurisprudence, evidence, penalties, the roles of people in a legal process (judge, lawyer, victim, defendant, witness), types of locations (crime location, defendant's address), etc. In this sense, there is still a need for a robust golden collection, annotated with fine-grained entities of the legal domain, and which covers various documents of a legal process, such as petitions, inquiries, complaints, decisions and sentences. In this article, we describe the development of the Golden Collection of the Brazilian Judiciary (CDJUR-BR) contemplating a set of fine-grained named entities that have been annotated by experts in legal documents. The creation of CDJUR-BR followed its own methodology that aimed to attribute a character of comprehensiveness and robustness. Together with the CDJUR-BR repository we provided a NER based on the BERT model and trained with the CDJUR-BR, whose results indicated the prevalence of the CDJUR-BR.\n        \u25b3 Less\n      ",
    "title": "CDJUR-BR -- A Golden Collection of Legal Document from Brazilian Justice with Fine-Grained Named Entities",
    "date": "19 May, 2023",
    "authors": [
      "Antonio Mauricio",
      " Vladia Pinheiro",
      " Vasco Furtado",
      " Jo\u00e3o Ara\u00fajo Monteiro Neto",
      " Francisco das Chagas Juc\u00e1 Bomfim",
      " Andr\u00e9 C\u00e2mara Ferreira da Costa",
      " Raquel Silveira",
      " Nilsiton Arag\u00e3o"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2106.07824",
    "paper_id": "2106.07824",
    "abstract": "\n        The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of \\emph{language}: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the \\textit{Language-complete ARC}: a collection of natural language descriptions by a group of human participants who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88\\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers.\n        \u25b3 Less\n      ",
    "title": "Communicating Natural Programs to Humans and Machines",
    "date": "19 May, 2023",
    "authors": [
      "Samuel Acquaviva",
      " Yewen Pu",
      " Marta Kryven",
      " Theodoros Sechopoulos",
      " Catherine Wong",
      " Gabrielle E Ecanow",
      " Maxwell Nye",
      " Michael Henry Tessler",
      " Joshua B. Tenenbaum"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12058",
    "paper_id": "2305.12058",
    "abstract": "\n        Click-Through Rate (CTR) prediction is one of the main tasks of the recommendation system, which is conducted by a user for different items to give the recommendation results. Cross-domain CTR prediction models have been proposed to overcome problems of data sparsity, long tail distribution of user-item interactions, and cold start of items or users. In order to make knowledge transfer from source domain to target domain more smoothly, an innovative deep learning cross-domain CTR prediction model, Domain Adversarial Deep Interest Network (DADIN) is proposed to convert the cross-domain recommendation task into a domain adaptation problem. The joint distribution alignment of two domains is innovatively realized by introducing domain agnostic layers and specially designed loss, and optimized together with CTR prediction loss in a way of adversarial training. It is found that the Area Under Curve (AUC) of DADIN is 0.08% higher than the most competitive baseline on Huawei dataset and is 0.71% higher than its competitors on Amazon dataset, achieving the state-of-the-art results on the basis of the evaluation of this model performance on two real datasets. The ablation study shows that by introducing adversarial method, this model has respectively led to the AUC improvements of 2.34% on Huawei dataset and 16.67% on Amazon dataset.\n        \u25b3 Less\n      ",
    "title": "DADIN: Domain Adversarial Deep Interest Network for Cross Domain Recommender Systems",
    "date": "19 May, 2023",
    "authors": [
      "Menglin Kong",
      " Muzhou Hou",
      " Shaojie Zhao",
      " Feng Liu",
      " Ri Su",
      " Yinghao Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12060",
    "paper_id": "2305.12060",
    "abstract": "\n        Magnesium alloys are attractive options for temporary bio-implants because of their biocompatibility, controlled corrosion rate, and similarity to natural bone in terms of stiffness and density. Nevertheless, their low mechanical strength hinders their use as cardiovascular stents and bone substitutes. While it is possible to engineer alloys with the desired mechanical strength, optimizing the mechanical properties of biocompatible magnesium alloys using conventional experimental methods is time-consuming and expensive. Therefore, Artificial Intelligence (AI) can be leveraged to streamline the alloy design process and reduce the required time. In this study, a machine learning model was developed to predict the yield strength (YS) of biocompatible magnesium alloys with an R2R^2 accuracy of 91\\%. The predictive model was then validated using the CALPHAD technique and thermodynamics calculations. Next, the predictive model was employed as the fitness function of a genetic algorithm to optimize the alloy composition for high-strength biocompatible magnesium implants. As a result, two alloys were proposed and synthesized, exhibiting YS values of 108 and 113 MPa, respectively. These values were substantially higher than those of conventional magnesium biocompatible alloys and closer to the YS and compressive strength of natural bone. Finally, the synthesized alloys were subjected to microstructure analysis and mechanical property testing to validate and evaluate the performance of the proposed AI-based alloy design approach for creating alloys with specific properties suitable for diverse applications.\n        \u25b3 Less\n      ",
    "title": "Mechanical Property Design of Bio-compatible Mg alloys using Machine-Learning Algorithms",
    "date": "19 May, 2023",
    "authors": [
      "Parham Valipoorsalimi",
      " Yuksel Asli Sari",
      " Mihriban Pekguleryuz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12068",
    "paper_id": "2305.12068",
    "abstract": "\n        The ADMANI datasets (annotated digital mammograms and associated non-image datasets) from the Transforming Breast Cancer Screening with AI programme (BRAIx) run by BreastScreen Victoria in Australia are multi-centre, large scale, clinically curated, real-world databases. The datasets are expected to aid in the development of clinically relevant Artificial Intelligence (AI) algorithms for breast cancer detection, early diagnosis, and other applications. To ensure high data quality, technical outliers must be removed before any downstream algorithm development. As a first step, we randomly select 30,000 individual mammograms and use Convolutional Variational Autoencoder (CVAE), a deep generative neural network, to detect outliers. CVAE is expected to detect all sorts of outliers, although its detection performance differs among different types of outliers. Traditional image processing techniques such as erosion and pectoral muscle analysis can compensate for the poor performance of CVAE in certain outlier types. We identify seven types of technical outliers: implant, pacemaker, cardiac loop recorder, improper radiography, atypical lesion/calcification, incorrect exposure parameter and improper placement. The outlier recall rate for the test set is 61% if CVAE, erosion and pectoral muscle analysis each select the top 1% images ranked in ascending or descending order according to image outlier score under each detection method, and 83% if each selects the top 5% images. This study offers an overview of technical outliers in the ADMANI dataset and suggests future directions to improve outlier detection effectiveness.\n        \u25b3 Less\n      ",
    "title": "Technical outlier detection via convolutional variational autoencoder for the ADMANI breast mammogram dataset",
    "date": "19 May, 2023",
    "authors": [
      "Hui Li",
      " Carlos A. Pena Solorzano",
      " Susan Wei",
      " Davis J. McCarthy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12088",
    "paper_id": "2305.12088",
    "abstract": "\n        In this paper, we navigate the intricate domain of reviewer rewards in open-access academic publishing, leveraging the precision of mathematics and the strategic acumen of game theory. We conceptualize the prevailing voucher-based reviewer reward system as a two-player game, subsequently identifying potential shortcomings that may incline reviewers towards binary decisions. To address this issue, we propose and mathematically formalize an alternative reward system with the objective of mitigating this bias and promoting more comprehensive reviews. We engage in a detailed investigation of the properties and outcomes of both systems, employing rigorous game-theoretical analysis and deep reinforcement learning simulations. Our results underscore a noteworthy divergence between the two systems, with our proposed system demonstrating a more balanced decision distribution and enhanced stability. This research not only augments the mathematical understanding of reviewer reward systems, but it also provides valuable insights for the formulation of policies within journal review system. Our contribution to the mathematical community lies in providing a game-theoretical perspective to a real-world problem and in the application of deep reinforcement learning to simulate and understand this complex system.\n        \u25b3 Less\n      ",
    "title": "Game-Theoretical Analysis of Reviewer Rewards in Peer-Review Journal Systems: Analysis and Experimental Evaluation using Deep Reinforcement Learning",
    "date": "19 May, 2023",
    "authors": [
      "Minhyeok Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12090",
    "paper_id": "2305.12090",
    "abstract": "\n        Recent advancements in foundation models such as large language models (LLM) have propelled them to the forefront of recommender systems (RS). Moreover, fairness in RS is critical since many users apply it for decision-making and demand fulfillment. However, at present, there is a lack of understanding regarding the level of fairness exhibited by recommendation foundation models and the appropriate methods for equitably treating different groups of users in foundation models. In this paper, we focus on user-side unfairness problem and show through a thorough examination that there is unfairness involved in LLMs that lead to unfair recommendation results. To eliminate bias from LLM for fairness-aware recommendation, we introduce a novel Unbiased P5 (UP5) foundation model based on Counterfactually-Fair-Prompting (CFP) techniques. CFP includes two sub-modules: a personalized prefix prompt that enhances fairness with respect to individual sensitive attributes, and a Prompt Mixture that integrates multiple counterfactually-fair prompts for a set of sensitive attributes. Experiments are conducted on two real-world datasets, MovieLens-1M and Insurance, and results are compared with both matching-based and sequential-based fairness-aware recommendation models. The results show that UP5 achieves better recommendation performance and meanwhile exhibits a high level of fairness.\n        \u25b3 Less\n      ",
    "title": "UP5: Unbiased Foundation Model for Fairness-aware Recommendation",
    "date": "19 May, 2023",
    "authors": [
      "Wenyue Hua",
      " Yingqiang Ge",
      " Shuyuan Xu",
      " Jianchao Ji",
      " Yongfeng Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14222",
    "paper_id": "2305.14222",
    "abstract": "\n        We develop a general framework for abstracting the behavior of an agent that operates in a nondeterministic domain, i.e., where the agent does not control the outcome of the nondeterministic actions, based on the nondeterministic situation calculus and the ConGolog programming language. We assume that we have both an abstract and a concrete nondeterministic basic action theory, and a refinement mapping which specifies how abstract actions, decomposed into agent actions and environment reactions, are implemented by concrete ConGolog programs. This new setting supports strategic reasoning and strategy synthesis, by allowing us to quantify separately on agent actions and environment reactions. We show that if the agent has a (strong FOND) plan/strategy to achieve a goal/complete a task at the abstract level, and it can always execute the nondeterministic abstract actions to completion at the concrete level, then there exists a refinement of it that is a (strong FOND) plan/strategy to achieve the refinement of the goal/task at the concrete level.\n        \u25b3 Less\n      ",
    "title": "Abstraction of Nondeterministic Situation Calculus Action Theories -- Extended Version",
    "date": "19 May, 2023",
    "authors": [
      "Bita Banihashemi",
      " Giuseppe De Giacomo",
      " Yves Lesp\u00e9rance"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12106",
    "paper_id": "2305.12106",
    "abstract": "\n        Convolutional neural networks (ConvNets) have been successfully applied to satellite image scene classification. Human-labeled training datasets are essential for ConvNets to perform accurate classification. Errors in human-labeled training datasets are unavoidable due to the complexity of satellite images. However, the distribution of human labeling errors on satellite images and their impact on ConvNets have not been investigated. To fill this research gap, this study, for the first time, collected real-world labels from 32 participants and explored how their errors affect three ConvNets (VGG16, GoogleNet and ResNet-50) for high-resolution satellite image scene classification. We found that: (1) human labeling errors have significant class and instance dependence, which is fundamentally different from the simulation noise in previous studies; (2) regarding the overall accuracy of all classes, when human labeling errors in training data increase by one unit, the overall accuracy of ConvNets classification decreases by approximately half a unit; (3) regarding the accuracy of each class, the impact of human labeling errors on ConvNets shows large heterogeneity across classes. To uncover the mechanism underlying the impact of human labeling errors on ConvNets, we further compared it with two types of simulated labeling noise: uniform noise (errors independent of both classes and instances) and class-dependent noise (errors independent of instances but not classes). Our results show that the impact of human labeling errors on ConvNets is similar to that of the simulated class-dependent noise but not to that of the simulated uniform noise, suggesting that the impact of human labeling errors on ConvNets is mainly due to class-dependent errors rather than instance-dependent errors.\n        \u25b3 Less\n      ",
    "title": "Human labeling errors and their impact on ConvNets for satellite image scene classification",
    "date": "19 May, 2023",
    "authors": [
      "Longkang Peng",
      " Tao Wei",
      " Xuehong Chen",
      " Xiaobei Chen",
      " Rui Sun",
      " Luoma Wan",
      " Xiaolin Zhu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12114",
    "paper_id": "2305.12114",
    "abstract": "\n        Currently, density-based clustering algorithms are widely applied because they can detect clusters with arbitrary shapes. However, they perform poorly in measuring global density, determining reasonable cluster centers or structures, assigning samples accurately and handling data with large density differences among clusters. To overcome their drawbacks, this paper proposes a granule fusion density-based clustering with evidential reasoning (GFDC). Both local and global densities of samples are measured by a sparse degree metric first. Then information granules are generated in high-density and low-density regions, assisting in processing clusters with significant density differences. Further, three novel granule fusion strategies are utilized to combine granules into stable cluster structures, helping to detect clusters with arbitrary shapes. Finally, by an assignment method developed from Dempster-Shafer theory, unstable samples are assigned. After using GFDC, a reasonable clustering result and some identified outliers can be obtained. The experimental results on extensive datasets demonstrate the effectiveness of GFDC.\n        \u25b3 Less\n      ",
    "title": "GFDC: A Granule Fusion Density-Based Clustering with Evidential Reasoning",
    "date": "19 May, 2023",
    "authors": [
      "Mingjie Cai",
      " Zhishan Wu",
      " Qingguo Li",
      " Feng Xu",
      " Jie Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11130",
    "paper_id": "2305.11130",
    "abstract": "\n        Language models trained on large-scale corpora can generate remarkably fluent results in open-domain dialogue. However, for the persona-based dialogue generation task, consistency and coherence are also key factors, which are great challenges for language models. Existing works mainly focus on valuable data filtering, model structure modifying, or objective function designing, while their improvements are limited and hard to generalize to all types of pre-trained language models. However, we find that language models can produce consistent and coherent responses if we consider enough generations. Thus, the problems lay in large-scale response generation and target response selection. In this work, a simple but effective two-stage SimOAP strategy is proposed, i.e., over-sampling and post-evaluation. The over-sampling stage takes large-scale responses from existing trained models efficiently via off-the-shelf distilling and compressing methods, and the post-evaluation stage selects a good response based on multiple well-designed evaluation metrics from large-scale candidates. Experimental results show that the proposed plug-in SimOAP strategy improves the backbone models and outperforms the baseline strategies in both automatic and human evaluations.\n        \u25b3 Less\n      ",
    "title": "SimOAP: Improve Coherence and Consistency in Persona-based Dialogue Generation via Over-sampling and Post-evaluation",
    "date": "19 May, 2023",
    "authors": [
      "Junkai Zhou",
      " Liang Pang",
      " Huawei Shen",
      " Xueqi Cheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12118",
    "paper_id": "2305.12118",
    "abstract": "\n        In standard adversarial training, models are optimized to fit one-hot labels within allowable adversarial perturbation budgets. However, the ignorance of underlying distribution shifts brought by perturbations causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that accurately reflects the distribution shift under attack during adversarial training. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by replacing the hard labels in their objectives. We demonstrate the efficacy of ADR through extensive experiments and strong performances across datasets.\n        \u25b3 Less\n      ",
    "title": "Annealing Self-Distillation Rectification Improves Adversarial Training",
    "date": "19 May, 2023",
    "authors": [
      "Yu-Yu Wu",
      " Hung-Jui Wang",
      " Shang-Tse Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.06027",
    "paper_id": "2212.06027",
    "abstract": "\n        In many real-world settings agents engage in strategic interactions with multiple opposing agents who can employ a wide variety of strategies. The standard approach for designing agents for such settings is to compute or approximate a relevant game-theoretic solution concept such as Nash equilibrium and then follow the prescribed strategy. However, such a strategy ignores any observations of opponents' play, which may indicate shortcomings that can be exploited. We present an approach for opponent modeling in multiplayer imperfect-information games where we collect observations of opponents' play through repeated interactions. We run experiments against a wide variety of real opponents and exact Nash equilibrium strategies in three-player Kuhn poker and show that our algorithm significantly outperforms all of the agents, including the exact Nash equilibrium strategies.\n        \u25b3 Less\n      ",
    "title": "Bayesian Opponent Modeling in Multiplayer Imperfect-Information Games",
    "date": "19 May, 2023",
    "authors": [
      "Sam Ganzfried",
      " Kevin A. Wang",
      " Max Chiswick"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12125",
    "paper_id": "2305.12125",
    "abstract": "\n        We present a novel algorithm for training deep neural networks in supervised (classification and regression) and unsupervised (reinforcement learning) scenarios. This algorithm combines the standard stochastic gradient descent and the gradient clipping method. The output layer is updated using clipped gradients, the rest of the neural network is updated using standard gradients. Updating the output layer using clipped gradient stabilizes it. We show that the remaining layers are automatically stabilized provided the neural network is only composed of squashing (compact range) activations. We also present a novel squashing activation function - it is obtained by modifying a Gaussian Error Linear Unit (GELU) to have compact range - we call it Truncated GELU (tGELU). Unlike other squashing activations, such as sigmoid, the range of tGELU can be explicitly specified. As a consequence, the problem of vanishing gradients that arise due to a small range, e.g., in the case of a sigmoid activation, is eliminated. We prove that a NN composed of squashing activations (tGELU, sigmoid, etc.), when updated using the algorithm presented herein, is numerically stable and has consistent performance (low variance). The theory is supported by extensive experiments. Within reinforcement learning, as a consequence of our study, we show that target networks in Deep Q-Learning can be omitted, greatly speeding up learning and alleviating memory requirements. Cross-entropy based classification algorithms that suffer from high variance issues are more consistent when trained using our framework. One symptom of numerical instability in training is the high variance of the neural network update values. We show, in theory and through experiments, that our algorithm updates have low variance, and the training loss reduces in a smooth manner.\n        \u25b3 Less\n      ",
    "title": "A Framework for Provably Stable and Consistent Training of Deep Feedforward Networks",
    "date": "19 May, 2023",
    "authors": [
      "Arunselvan Ramaswamy",
      " Shalabh Bhatnagar",
      " Naman Saxena"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12127",
    "paper_id": "2305.12127",
    "abstract": "\n        In this work, we propose algorithms and methods that enable learning dexterous object manipulation using simulated one- or two-armed robots equipped with multi-fingered hand end-effectors. Using a parallel GPU-accelerated physics simulator (Isaac Gym), we implement challenging tasks for these robots, including regrasping, grasp-and-throw, and object reorientation. To solve these problems we introduce a decentralized Population-Based Training (PBT) algorithm that allows us to massively amplify the exploration capabilities of deep reinforcement learning. We find that this method significantly outperforms regular end-to-end learning and is able to discover robust control policies in challenging tasks. Video demonstrations of learned behaviors and the code can be found at https://sites.google.com/view/dexpbt\n        \u25b3 Less\n      ",
    "title": "DexPBT: Scaling up Dexterous Manipulation for Hand-Arm Systems with Population Based Training",
    "date": "19 May, 2023",
    "authors": [
      "Aleksei Petrenko",
      " Arthur Allshire",
      " Gavriel State",
      " Ankur Handa",
      " Viktor Makoviychuk"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12130",
    "paper_id": "2305.12130",
    "abstract": "\n        With the rapid development of artificial general intelligence (AGI), various multimedia services based on pretrained foundation models (PFMs) need to be effectively deployed. With edge servers that have cloud-level computing power, edge intelligence can extend the capabilities of AGI to mobile edge networks. However, compared with cloud data centers, resource-limited edge servers can only cache and execute a small number of PFMs, which typically consist of billions of parameters and require intensive computing power and GPU memory during inference. To address this challenge, in this paper, we propose a joint foundation model caching and inference framework that aims to balance the tradeoff among inference latency, accuracy, and resource consumption by managing cached PFMs and user requests efficiently during the provisioning of generative AI services. Specifically, considering the in-context learning ability of PFMs, a new metric named the Age of Context (AoC), is proposed to model the freshness and relevance between examples in past demonstrations and current service requests. Based on the AoC, we propose a least context caching algorithm to manage cached PFMs at edge servers with historical prompts and inference results. The numerical results demonstrate that the proposed algorithm can reduce system costs compared with existing baselines by effectively utilizing contextual information.\n        \u25b3 Less\n      ",
    "title": "Joint Foundation Model Caching and Inference of Generative AI Services for Edge Intelligence",
    "date": "19 May, 2023",
    "authors": [
      "Minrui Xu",
      " Dusit Niyato",
      " Hongliang Zhang",
      " Jiawen Kang",
      " Zehui Xiong",
      " Shiwen Mao",
      " Zhu Han"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.12029",
    "paper_id": "2209.12029",
    "abstract": "\n        While Reinforcement Learning can achieve impressive results for complex tasks, the learned policies are generally prone to fail in downstream tasks with even minor model mismatch or unexpected perturbations. Recent works have demonstrated that a policy population with diverse behavior characteristics can generalize to downstream environments with various discrepancies. However, such policies might result in catastrophic damage during the deployment in practical scenarios like real-world systems due to the unrestricted behaviors of trained policies. Furthermore, training diverse policies without regulation of the behavior can result in inadequate feasible policies for extrapolating to a wide range of test conditions with dynamics shifts. In this work, we aim to train diverse policies under the regularization of the behavior patterns. We motivate our paradigm by observing the inverse dynamics in the environment with partial state information and propose Diversity in Regulation (DiR) training diverse policies with regulated behaviors to discover desired patterns that benefit the generalization. Considerable empirical results on various variations of different environments indicate that our method attains improvements over other diversity-driven counterparts.\n        \u25b3 Less\n      ",
    "title": "Open-Ended Diverse Solution Discovery with Regulated Behavior Patterns for Cross-Domain Adaptation",
    "date": "19 May, 2023",
    "authors": [
      "Kang Xu",
      " Yan Ma",
      " Bingsheng Wei",
      " Wei Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.11534",
    "paper_id": "2211.11534",
    "abstract": "\n        The robustness of recommender systems under node injection attacks has garnered significant attention. Recently, GraphRfi, a GNN-based recommender system, was proposed and shown to effectively mitigate the impact of injected fake users. However, we demonstrate that GraphRfi remains vulnerable to attacks due to the supervised nature of its fraudster detection component, where obtaining clean labels is challenging in practice. In particular, we propose a powerful poisoning attack, MetaC, against both GNN-based and MF-based recommender systems. Furthermore, we analyze why GraphRfi fails under such an attack. Then, based on our insights obtained from vulnerability analysis, we design an adaptive fraudster detection module that explicitly considers label uncertainty. This module can serve as a plug-in for different recommender systems, resulting in a robust framework named PDR. Comprehensive experiments show that our defense approach outperforms other benchmark methods under attacks. Overall, our research presents an effective framework for integrating fraudster detection into recommendation systems to achieve adversarial robustness.\n        \u25b3 Less\n      ",
    "title": "Towards Adversarially Robust Recommendation from Adaptive Fraudster Detection",
    "date": "19 May, 2023",
    "authors": [
      "Yuni Lai",
      " Yulin Zhu",
      " Wenqi Fan",
      " Xiaoge Zhang",
      " Kai Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12144",
    "paper_id": "2305.12144",
    "abstract": "\n        Current image captioning works usually focus on generating descriptions in an autoregressive manner. However, there are limited works that focus on generating descriptions non-autoregressively, which brings more decoding diversity. Inspired by the success of diffusion models on generating natural-looking images, we propose a novel method DiffCap to apply continuous diffusions on image captioning. Unlike image generation where the output is fixed-size and continuous, image description length varies with discrete tokens. Our method transforms discrete tokens in a natural way and applies continuous diffusion on them to successfully fuse extracted image features for diffusion caption generation. Our experiments on COCO dataset demonstrate that our method uses a much simpler structure to achieve comparable results to the previous non-autoregressive works. Apart from quality, an intriguing property of DiffCap is its high diversity during generation, which is missing from many autoregressive models. We believe our method on fusing multimodal features in diffusion language generation will inspire more researches on multimodal language generation tasks for its simplicity and decoding flexibility.\n        \u25b3 Less\n      ",
    "title": "DiffCap: Exploring Continuous Diffusion on Image Captioning",
    "date": "19 May, 2023",
    "authors": [
      "Yufeng He",
      " Zefan Cai",
      " Xu Gan",
      " Baobao Chang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15153",
    "paper_id": "2305.15153",
    "abstract": "\n        Is there a unified framework for graph-based retrosynthesis prediction? Through analysis of full-, semi-, and non-template retrosynthesis methods, we discovered that they strive to strike an optimal balance between combinability and consistency: \\textit{Should atoms be combined as motifs to simplify the molecular editing process, or should motifs be broken down into atoms to reduce the vocabulary and improve predictive consistency?}\n  Recent works have studied several specific cases, while none of them explores different combinability-consistency trade-offs. Therefore, we propose MotifRetro, a dynamic motif editing framework for retrosynthesis prediction that can explore the entire trade-off space and unify graph-based models. MotifRetro comprises two components: RetroBPE, which controls the combinability-consistency trade-off, and a motif editing model, where we introduce a novel LG-EGAT module to dynamiclly add motifs to the molecule. We conduct extensive experiments on USPTO-50K to explore how the trade-off affects the model performance and finally achieve state-of-the-art performance.\n        \u25b3 Less\n      ",
    "title": "MotifRetro: Exploring the Combinability-Consistency Trade-offs in retrosynthesis via Dynamic Motif Editing",
    "date": "19 May, 2023",
    "authors": [
      "Zhangyang Gao",
      " Xingran Chen",
      " Cheng Tan",
      " Stan Z. Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.01918",
    "paper_id": "2305.01918",
    "abstract": "\n        Contrastive learning has become a popular approach in natural language processing, particularly for the learning of sentence embeddings. However, the discrete nature of natural language makes it difficult to ensure the quality of positive and negative sample pairs generated through data augmentation methods. Although supervised contrastive learning can produce more accurate sample pairs with human feedback labels, it still lacks fine-grained training signals. In this paper, we propose to improve \\textbf{C}ontrastive \\textbf{L}earning of sentence embeddings from \\textbf{AI} \\textbf{F}eedback \\textbf{(CLAIF)}. Our method utilizes AI feedback from large pre-trained language models (LLMs) to construct sample pairs with fine-grained sample similarity scores to improve contrastive learning. Besides, we combine human feedback and AI feedback to provide better supervision signals for supervised contrastive learning of sentence embeddings. Experimental results show that our method achieves state-of-the-art performance on several semantic textual similarity (STS) and transfer learning tasks compared to other unsupervised and supervised contrastive learning methods.\n        \u25b3 Less\n      ",
    "title": "Improving Contrastive Learning of Sentence Embeddings from AI Feedback",
    "date": "19 May, 2023",
    "authors": [
      "Qinyuan Cheng",
      " Xiaogui Yang",
      " Tianxiang Sun",
      " Linyang Li",
      " Xipeng Qiu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12167",
    "paper_id": "2305.12167",
    "abstract": "\n        As artificial intelligence (AI) becomes more prevalent there is a growing demand from regulators to accompany decisions made by such systems with explanations. However, a persistent gap exists between the need to execute a meaningful right to explanation vs. the ability of Machine Learning systems to deliver on such a legal requirement. The regulatory appeal towards \"a right to explanation\" of AI systems can be attributed to the significant role of explanations, part of the notion called reason-giving, in law. Therefore, in this work we examine reason-giving's purposes in law to analyze whether reasons provided by end-user Explainability can adequately fulfill them.\n  We find that reason-giving's legal purposes include: (a) making a better and more just decision, (b) facilitating due-process, (c) authenticating human agency, and (d) enhancing the decision makers' authority. Using this methodology, we demonstrate end-user Explainabilty's inadequacy to fulfil reason-giving's role in law, given reason-giving's functions rely on its impact over a human decision maker. Thus, end-user Explainability fails, or is unsuitable, to fulfil the first, second and third legal function. In contrast we find that end-user Explainability excels in the fourth function, a quality which raises serious risks considering recent end-user Explainability research trends, Large Language Models' capabilities, and the ability to manipulate end-users by both humans and machines. Hence, we suggest that in some cases the right to explanation of AI systems could bring more harm than good to end users. Accordingly, this study carries some important policy ramifications, as it calls upon regulators and Machine Learning practitioners to reconsider the widespread pursuit of end-user Explainability and a right to explanation of AI systems.\n        \u25b3 Less\n      ",
    "title": "The Case Against Explainability",
    "date": "19 May, 2023",
    "authors": [
      "Hofit Wasserman Rozen",
      " Niva Elkin-Koren",
      " Ran Gilad-Bachrach"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.17140",
    "paper_id": "2305.17140",
    "abstract": "\n        Many practical problems can be understood as the search for a state of affairs that extends a fixed partial state of affairs, the \\emph{environment}, while satisfying certain conditions that are formally specified. Such problems are found in, e.g., engineering, law or economics.\n  We study this class of problems in a context where some of the relevant information about the environment is not known by the user at the start of the search. During the search, the user may consider tentative solutions that make implicit hypotheses about these unknowns. To ensure that the solution is appropriate, these hypotheses must be verified by observing the environment. Furthermore, we assume that, in addition to knowledge of what constitutes a solution, knowledge of general laws of the environment is also present. We formally define partial solutions with enough verified facts to guarantee the existence of complete and appropriate solutions.\n  Additionally, we propose an interactive system to assist the user in their search by determining 1) which hypotheses implicit in a tentative solution must be verified in the environment, and 2) which observations can bring useful information for the search. We present an efficient method to over-approximate the set of relevant information, and evaluate our implementation.\n        \u25b3 Less\n      ",
    "title": "Interactive Model Expansion in an Observable Environment",
    "date": "20 May, 2023",
    "authors": [
      "Pierre Carbonnelle",
      " Joost Vennekens",
      " Bart Bogaerts",
      " Marc Denecker"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.01750",
    "paper_id": "2306.01750",
    "abstract": "\n        In this survey paper, we deep dive into the field of Explainable Artificial Intelligence (XAI). After introducing the scope of this paper, we start by discussing what an \"explanation\" really is. We then move on to discuss some of the existing approaches to XAI and build a taxonomy of the most popular methods. Next, we also look at a few applications of these and other XAI techniques in four primary domains: finance, autonomous driving, healthcare and manufacturing. We end by introducing a promising discipline, \"Explanation Engineering,\" which includes a systematic approach for designing explainability into AI systems.\n        \u25b3 Less\n      ",
    "title": "A Survey of Explainable AI and Proposal for a Discipline of Explanation Engineering",
    "date": "20 May, 2023",
    "authors": [
      "Clive Gomes",
      " Lalitha Natraj",
      " Shijun Liu",
      " Anushka Datta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12185",
    "paper_id": "2305.12185",
    "abstract": "\n        As deep learning gains popularity in modelling dynamical systems, we expose an underappreciated misunderstanding relevant to modelling dynamics on networks. Strongly influenced by graph neural networks, latent vertex embeddings are naturally adopted in many neural dynamical network models. However, we show that embeddings tend to induce a model that fits observations well but simultaneously has incorrect dynamical behaviours. Recognising that previous studies narrowly focus on short-term predictions during the transient phase of a flow, we propose three tests for correct long-term behaviour, and illustrate how an embedding-based dynamical model fails these tests, and analyse the causes, particularly through the lens of topological conjugacy. In doing so, we show that the difficulties can be avoided by not using embedding. We propose a simple embedding-free alternative based on parametrising two additive vector-field components. Through extensive experiments, we verify that the proposed model can reliably recover a broad class of dynamics on different network topologies from time series data.\n        \u25b3 Less\n      ",
    "title": "Do We Need an Encoder-Decoder to Model Dynamical Systems on Networks?",
    "date": "20 May, 2023",
    "authors": [
      "Bing Liu",
      " Wei Luo",
      " Gang Li",
      " Jing Huang",
      " Bo Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12200",
    "paper_id": "2305.12200",
    "abstract": "\n        Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian. The audio samples are available at https://xh621.github.io/stand-up-comedy-demo/\n        \u25b3 Less\n      ",
    "title": "ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios",
    "date": "20 May, 2023",
    "authors": [
      "Yuyue Wang",
      " Huan Xiao",
      " Yihan Wu",
      " Ruihua Song"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.09124",
    "paper_id": "2303.09124",
    "abstract": "\n        Neuroimaging measures of the brain's white matter connections can enable the prediction of non-imaging phenotypes, such as demographic and cognitive measures. Existing works have investigated traditional microstructure and connectivity measures from diffusion MRI tractography, without considering the shape of the connections reconstructed by tractography. In this paper, we investigate the potential of fiber tract shape features for predicting non-imaging phenotypes, both individually and in combination with traditional features. We focus on three basic shape features: length, diameter, and elongation. Two different prediction methods are used, including a traditional regression method and a deep-learning-based prediction method. Experiments use an efficient two-stage fusion strategy for prediction using microstructure, connectivity, and shape measures. To reduce predictive bias due to brain size, normalized shape features are also investigated. Experimental results on the Human Connectome Project (HCP) young adult dataset (n=1065) demonstrate that individual shape features are predictive of non-imaging phenotypes. When combined with microstructure and connectivity features, shape features significantly improve performance for predicting the cognitive score TPVT (NIH Toolbox picture vocabulary test). Overall, this study demonstrates that the shape of fiber tracts contains useful information for the description and study of the living human brain using machine learning.\n        \u25b3 Less\n      ",
    "title": "Fiber Tract Shape Measures Inform Prediction of Non-Imaging Phenotypes",
    "date": "20 May, 2023",
    "authors": [
      "Wan Liu",
      " Yuqian Chen",
      " Chuyang Ye",
      " Nikos Makris",
      " Yogesh Rathi",
      " Weidong Cai",
      " Fan Zhang",
      " Lauren J. O'Donnell"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12216",
    "paper_id": "2305.12216",
    "abstract": "\n        Meta-Reinforcement Learning (MRL) is a promising framework for training agents that can quickly adapt to new environments and tasks. In this work, we study the MRL problem under the policy gradient formulation, where we propose a novel algorithm that uses Moreau envelope surrogate regularizers to jointly learn a meta-policy that is adjustable to the environment of each individual task. Our algorithm, called Moreau Envelope Meta-Reinforcement Learning (MEMRL), learns a meta-policy that can adapt to a distribution of tasks by efficiently updating the policy parameters using a combination of gradient-based optimization and Moreau Envelope regularization. Moreau Envelopes provide a smooth approximation of the policy optimization problem, which enables us to apply standard optimization techniques and converge to an appropriate stationary point. We provide a detailed analysis of the MEMRL algorithm, where we show a sublinear convergence rate to a first-order stationary point for non-convex policy gradient optimization. We finally show the effectiveness of MEMRL on a multi-task 2D-navigation problem.\n        \u25b3 Less\n      ",
    "title": "On First-Order Meta-Reinforcement Learning with Moreau Envelopes",
    "date": "20 May, 2023",
    "authors": [
      "Mohammad Taha Toghani",
      " Sebastian Perez-Salazar",
      " C\u00e9sar A. Uribe"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12218",
    "paper_id": "2305.12218",
    "abstract": "\n        Text-video retrieval is a challenging cross-modal task, which aims to align visual entities with natural language descriptions. Current methods either fail to leverage the local details or are computationally expensive. What's worse, they fail to leverage the heterogeneous concepts in data. In this paper, we propose the Disentangled Conceptualization and Set-to-set Alignment (DiCoSA) to simulate the conceptualizing and reasoning process of human beings. For disentangled conceptualization, we divide the coarse feature into multiple latent factors related to semantic concepts. For set-to-set alignment, where a set of visual concepts correspond to a set of textual concepts, we propose an adaptive pooling method to aggregate semantic concepts to address the partial matching. In particular, since we encode concepts independently in only a few dimensions, DiCoSA is superior at efficiency and granularity, ensuring fine-grained interactions using a similar computational complexity as coarse-grained alignment. Extensive experiments on five datasets, including MSR-VTT, LSMDC, MSVD, ActivityNet, and DiDeMo, demonstrate that our method outperforms the existing state-of-the-art methods.\n        \u25b3 Less\n      ",
    "title": "Text-Video Retrieval with Disentangled Conceptualization and Set-to-Set Alignment",
    "date": "20 May, 2023",
    "authors": [
      "Peng Jin",
      " Hao Li",
      " Zesen Cheng",
      " Jinfa Huang",
      " Zhennan Wang",
      " Li Yuan",
      " Chang Liu",
      " Jie Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18316",
    "paper_id": "2305.18316",
    "abstract": "\n        The public sector faces several challenges, such as a number of external and internal demands for change, citizens' dissatisfaction and frustration with public sector organizations, that need to be addressed. An alternative to the traditional top-down development of public services is co-creation of public services. Co-creation promotes collaboration between stakeholders with the aim to create better public services and achieve public values. At the same time, data analytics has been fuelled by the availability of immense amounts of textual data. Whilst both co-creation and TA have been used in the private sector, we study existing works on the application of Text Analytics (TA) techniques on text data to support public service co-creation. We systematically review 75 of the 979 papers that focus directly or indirectly on the application of TA in the context of public service development. In our review, we analyze the TA techniques, the public service they support, public value outcomes, and the co-creation phase they are used in. Our findings indicate that the TA implementation for co-creation is still in its early stages and thus still limited. Our research framework promotes the concept and stimulates the strengthening of the role of Text Analytics techniques to support public sector organisations and their use of co-creation process. From policy-makers' and public administration managers' standpoints, our findings and the proposed research framework can be used as a guideline in developing a strategy for the designing co-created and user-centred public services.\n        \u25b3 Less\n      ",
    "title": "Application of Text Analytics in Public Service Co-Creation: Literature Review and Research Framework",
    "date": "20 May, 2023",
    "authors": [
      "Nina Rizun",
      " Aleksandra Revina",
      " Noella Edelmann"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.06046",
    "paper_id": "2207.06046",
    "abstract": "\n        Deep learning has been actively applied to time series forecasting, leading to a deluge of new methods, belonging to the class of historical-value models. Yet, despite the attractive properties of time-index models, such as being able to model the continuous nature of underlying time series dynamics, little attention has been given to them. Indeed, while naive deep time-index models are far more expressive than the manually predefined function representations of classical time-index models, they are inadequate for forecasting, being unable to generalize to unseen time steps due to the lack of inductive bias. In this paper, we propose DeepTime, a meta-optimization framework to learn deep time-index models which overcome these limitations, yielding an efficient and accurate forecasting model. Extensive experiments on real world datasets in the long sequence time-series forecasting setting demonstrate that our approach achieves competitive results with state-of-the-art methods, and is highly efficient. Code is available at https://github.com/salesforce/DeepTime.\n        \u25b3 Less\n      ",
    "title": "Learning Deep Time-index Models for Time Series Forecasting",
    "date": "20 May, 2023",
    "authors": [
      "Gerald Woo",
      " Chenghao Liu",
      " Doyen Sahoo",
      " Akshat Kumar",
      " Steven Hoi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12257",
    "paper_id": "2305.12257",
    "abstract": "\n        Fine-grained financial sentiment analysis on news headlines is a challenging task requiring human-annotated datasets to achieve high performance. Limited studies have tried to address the sentiment extraction task in a setting where multiple entities are present in a news headline. In an effort to further research in this area, we make publicly available SEntFiN 1.0, a human-annotated dataset of 10,753 news headlines with entity-sentiment annotations, of which 2,847 headlines contain multiple entities, often with conflicting sentiments. We augment our dataset with a database of over 1,000 financial entities and their various representations in news media amounting to over 5,000 phrases. We propose a framework that enables the extraction of entity-relevant sentiments using a feature-based approach rather than an expression-based approach. For sentiment extraction, we utilize 12 different learning schemes utilizing lexicon-based and pre-trained sentence representations and five classification approaches. Our experiments indicate that lexicon-based n-gram ensembles are above par with pre-trained word embedding schemes such as GloVe. Overall, RoBERTa and finBERT (domain-specific BERT) achieve the highest average accuracy of 94.29% and F1-score of 93.27%. Further, using over 210,000 entity-sentiment predictions, we validate the economic effect of sentiments on aggregate market movements over a long duration.\n        \u25b3 Less\n      ",
    "title": "SEntFiN 1.0: Entity-Aware Sentiment Analysis for Financial News",
    "date": "20 May, 2023",
    "authors": [
      "Ankur Sinha",
      " Satishwar Kedas",
      " Rishu Kumar",
      " Pekka Malo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12268",
    "paper_id": "2305.12268",
    "abstract": "\n        A real-world text corpus sometimes comprises not only text documents but also semantic links between them (e.g., academic papers in a bibliographic network are linked by citations and co-authorships). Text documents and semantic connections form a text-rich network, which empowers a wide range of downstream tasks such as classification and retrieval. However, pretraining methods for such structures are still lacking, making it difficult to build one generic model that can be adapted to various tasks on text-rich networks. Current pretraining objectives, such as masked language modeling, purely model texts and do not take inter-document structure information into consideration. To this end, we propose our PretrAining on TexT-Rich NetwOrk framework Patton. Patton includes two pretraining strategies: network-contextualized masked language modeling and masked node prediction, to capture the inherent dependency between textual attributes and network structure. We conduct experiments on four downstream tasks in five datasets from both academic and e-commerce domains, where Patton outperforms baselines significantly and consistently.\n        \u25b3 Less\n      ",
    "title": "Patton: Language Model Pretraining on Text-Rich Networks",
    "date": "20 May, 2023",
    "authors": [
      "Bowen Jin",
      " Wentao Zhang",
      " Yu Zhang",
      " Yu Meng",
      " Xinyang Zhang",
      " Qi Zhu",
      " Jiawei Han"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12272",
    "paper_id": "2305.12272",
    "abstract": "\n        To predict the next token, autoregressive models ordinarily examine the past. Could they also benefit from also examining hypothetical futures? We consider a novel Transformer-based autoregressive architecture that estimates the next-token distribution by extrapolating multiple continuations of the past, according to some proposal distribution, and attending to these extended strings. This architecture draws insights from classical AI systems such as board game players: when making a local decision, a policy may benefit from exploring possible future trajectories and analyzing them. On multiple tasks including morphological inflection and Boolean satisfiability, our lookahead model is able to outperform the ordinary Transformer model of comparable size. However, on some tasks, it appears to be benefiting from the extra computation without actually using the lookahead information. We discuss possible variant architectures as well as future speedups.\n        \u25b3 Less\n      ",
    "title": "Autoregressive Modeling with Lookahead Attention",
    "date": "20 May, 2023",
    "authors": [
      "Li Du",
      " Hongyuan Mei",
      " Jason Eisner"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18225",
    "paper_id": "2305.18225",
    "abstract": "\n        We present Locksynth, a tool that automatically derives synchronization needed for destructive updates to concurrent data structures that involve a constant number of shared heap memory write operations. Locksynth serves as the implementation of our prior work on deriving abstract synchronization code. Designing concurrent data structures involves inferring correct synchronization code starting with a prior understanding of the sequential data structure's operations. Further, an understanding of shared memory model and the synchronization primitives is also required. The reasoning involved transforming a sequential data structure into its concurrent version can be performed using Answer Set Programming and we mechanized our approach in previous work. The reasoning involves deduction and abduction that can be succinctly modeled in ASP. We assume that the abstract sequential code of the data structure's operations is provided, alongside axioms that describe concurrent behavior. This information is used to automatically derive concurrent code for that data structure, such as dictionary operations for linked lists and binary search trees that involve a constant number of destructive update operations. We also are able to infer the correct set of locks (but not code synthesis) for external height-balanced binary search trees that involve left/right tree rotations. Locksynth performs the analyses required to infer correct sets of locks and as a final step, also derives the C++ synchronization code for the synthesized data structures. We also provide a performance analysis of the C++ code synthesized by Locksynth with the hand-crafted versions available from the Synchrobench microbenchmark suite. To the best of our knowledge, our tool is the first to employ ASP as a backend reasoner to perform concurrent data structure synthesis.\n        \u25b3 Less\n      ",
    "title": "Locksynth: Deriving Synchronization Code for Concurrent Data Structures with ASP",
    "date": "20 May, 2023",
    "authors": [
      "Sarat Chandra Varanasi",
      " Neeraj Mittal",
      " Gopal Gupta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2101.07140",
    "paper_id": "2101.07140",
    "abstract": "\n        Human-AI policy specification is a novel procedure we define in which humans can collaboratively warm-start a robot's reinforcement learning policy. This procedure is comprised of two steps; (1) Policy Specification, i.e. humans specifying the behavior they would like their companion robot to accomplish, and (2) Policy Optimization, i.e. the robot applying reinforcement learning to improve the initial policy. Existing approaches to enabling collaborative policy specification are often unintelligible black-box methods, and are not catered towards making the autonomous system accessible to a novice end-user. In this paper, we develop a novel collaborative framework to allow humans to initialize and interpret an autonomous agent's behavior. Through our framework, we enable humans to specify an initial behavior model via unstructured, natural language (NL), which we convert to lexical decision trees. Next, we leverage these translated specifications, to warm-start reinforcement learning and allow the agent to further optimize these potentially suboptimal policies. Our approach warm-starts an RL agent by utilizing non-expert natural language specifications without incurring the additional domain exploration costs. We validate our approach by showing that our model is able to produce >80% translation accuracy, and that policies initialized by a human can match the performance of relevant RL baselines in two domains.\n        \u25b3 Less\n      ",
    "title": "Natural Language Specification of Reinforcement Learning Policies through Differentiable Decision Trees",
    "date": "20 May, 2023",
    "authors": [
      "Pradyumna Tambwekar",
      " Andrew Silva",
      " Nakul Gopalan",
      " Matthew Gombolay"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.17144",
    "paper_id": "2303.17144",
    "abstract": "\n        Real-time perception, or streaming perception, is a crucial aspect of autonomous driving that has yet to be thoroughly explored in existing research. To address this gap, we present DAMO-StreamNet, an optimized framework that combines recent advances from the YOLO series with a comprehensive analysis of spatial and temporal perception mechanisms, delivering a cutting-edge solution. The key innovations of DAMO-StreamNet are (1) A robust neck structure incorporating deformable convolution, enhancing the receptive field and feature alignment capabilities (2) A dual-branch structure that integrates short-path semantic features and long-path temporal features, improving motion state prediction accuracy. (3) Logits-level distillation for efficient optimization, aligning the logits of teacher and student networks in semantic space. (4) A real-time forecasting mechanism that updates support frame features with the current frame, ensuring seamless streaming perception during inference. Our experiments demonstrate that DAMO-StreamNet surpasses existing state-of-the-art methods, achieving 37.8% (normal size (600, 960)) and 43.3% (large size (1200, 1920)) sAP without using extra data. This work not only sets a new benchmark for real-time perception but also provides valuable insights for future research. Additionally, DAMO-StreamNet can be applied to various autonomous systems, such as drones and robots, paving the way for real-time perception. The code is at https://github.com/zhiqic/DAMO-StreamNet.\n        \u25b3 Less\n      ",
    "title": "DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving",
    "date": "20 May, 2023",
    "authors": [
      "Jun-Yan He",
      " Zhi-Qi Cheng",
      " Chenyang Li",
      " Wangmeng Xiang",
      " Binghui Chen",
      " Bin Luo",
      " Yifeng Geng",
      " Xuansong Xie"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.02783",
    "paper_id": "2303.02783",
    "abstract": "\n        We consider the problem of learning a control policy that is robust against the parameter mismatches between the training environment and testing environment. We formulate this as a distributionally robust reinforcement learning (DR-RL) problem where the objective is to learn the policy which maximizes the value function against the worst possible stochastic model of the environment in an uncertainty set. We focus on the tabular episodic learning setting where the algorithm has access to a generative model of the nominal (training) environment around which the uncertainty set is defined. We propose the Robust Phased Value Learning (RPVL) algorithm to solve this problem for the uncertainty sets specified by four different divergences: total variation, chi-square, Kullback-Leibler, and Wasserstein. We show that our algorithm achieves O~(|S||A|H5)\\tilde{\\mathcal{O}}(|\\mathcal{S}||\\mathcal{A}| H^{5}) sample complexity, which is uniformly better than the existing results by a factor of |S||\\mathcal{S}|, where |S||\\mathcal{S}| is number of states, |A||\\mathcal{A}| is the number of actions, and HH is the horizon length. We also provide the first-ever sample complexity result for the Wasserstein uncertainty set. Finally, we demonstrate the performance of our algorithm using simulation experiments.\n        \u25b3 Less\n      ",
    "title": "Improved Sample Complexity Bounds for Distributionally Robust Reinforcement Learning",
    "date": "20 May, 2023",
    "authors": [
      "Zaiyan Xu",
      " Kishan Panaganti",
      " Dileep Kalathil"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12301",
    "paper_id": "2305.12301",
    "abstract": "\n        The pre-trained speech encoder wav2vec 2.0 performs very well on various spoken language understanding (SLU) tasks. However, on many tasks, it trails behind text encoders with textual input. To improve the understanding capability of SLU encoders, various studies have used knowledge distillation to transfer knowledge from natural language understanding (NLU) encoders. We use a very simple method of distilling from a textual sentence embedder directly into wav2vec 2.0 as pre-training, utilizing paired audio-text datasets. We observed that this method is indeed capable of improving SLU task performance in fine-tuned settings, as well as full-data and few-shot transfer on a frozen encoder. However, the model performs worse on certain tasks highlighting the strengths and weaknesses of our approach.\n        \u25b3 Less\n      ",
    "title": "Sentence Embedder Guided Utterance Encoder (SEGUE) for Spoken Language Understanding",
    "date": "20 May, 2023",
    "authors": [
      "Yi Xuan Tan",
      " Navonil Majumder",
      " Soujanya Poria"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12311",
    "paper_id": "2305.12311",
    "abstract": "\n        The convergence of text, visual, and audio data is a key step towards human-like artificial intelligence, however the current Vision-Language-Speech landscape is dominated by encoder-only models which lack generative abilities. We propose closing this gap with i-Code V2, the first model capable of generating natural language from any combination of Vision, Language, and Speech data. i-Code V2 is an integrative system that leverages state-of-the-art single-modality encoders, combining their outputs with a new modality-fusing encoder in order to flexibly project combinations of modalities into a shared representational space. Next, language tokens are generated from these representations via an autoregressive decoder. The whole framework is pretrained end-to-end on a large collection of dual- and single-modality datasets using a novel text completion objective that can be generalized across arbitrary combinations of modalities. i-Code V2 matches or outperforms state-of-the-art single- and dual-modality baselines on 7 multimodal tasks, demonstrating the power of generative multimodal pretraining across a diversity of tasks and signals.\n        \u25b3 Less\n      ",
    "title": "i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data",
    "date": "20 May, 2023",
    "authors": [
      "Ziyi Yang",
      " Mahmoud Khademi",
      " Yichong Xu",
      " Reid Pryzant",
      " Yuwei Fang",
      " Chenguang Zhu",
      " Dongdong Chen",
      " Yao Qian",
      " Mei Gao",
      " Yi-Ling Chen",
      " Robert Gmyr",
      " Naoyuki Kanda",
      " Noel Codella",
      " Bin Xiao",
      " Yu Shi",
      " Lu Yuan",
      " Takuya Yoshioka",
      " Michael Zeng",
      " Xuedong Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12328",
    "paper_id": "2305.12328",
    "abstract": "\n        We present an end-to-end diffusion-based method for editing videos with human language instructions, namely InstructVid2Vid\\textbf{InstructVid2Vid}. Our approach enables the editing of input videos based on natural language instructions without any per-example fine-tuning or inversion. The proposed InstructVid2Vid model combines a pretrained image generation model, Stable Diffusion, with a conditional 3D U-Net architecture to generate time-dependent sequence of video frames. To obtain the training data, we incorporate the knowledge and expertise of different models, including ChatGPT, BLIP, and Tune-a-Video, to synthesize video-instruction triplets, which is a more cost-efficient alternative to collecting data in real-world scenarios. To improve the consistency between adjacent frames of generated videos, we propose the Frame Difference Loss, which is incorporated during the training process. During inference, we extend the classifier-free guidance to text-video input to guide the generated results, making them more related to both the input video and instruction. Experiments demonstrate that InstructVid2Vid is able to generate high-quality, temporally coherent videos and perform diverse edits, including attribute editing, change of background, and style transfer. These results highlight the versatility and effectiveness of our proposed method. Code is released in InstructVid2Vid\\href{https://github.com/BrightQin/InstructVid2Vid}{InstructVid2Vid}.\n        \u25b3 Less\n      ",
    "title": "InstructVid2Vid: Controllable Video Editing with Natural Language Instructions",
    "date": "20 May, 2023",
    "authors": [
      "Bosheng Qin",
      " Juncheng Li",
      " Siliang Tang",
      " Tat-Seng Chua",
      " Yueting Zhuang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12356",
    "paper_id": "2305.12356",
    "abstract": "\n        Efficient deployment of large language models (LLMs) necessitates low-bit quantization to minimize model size and inference cost. While low-bit integer formats (e.g., INT8/INT4) have been the conventional choice, emerging low-bit floating-point formats (e.g., FP8/FP4) offer a compelling alternative and are gaining support from cutting-edge hardware, such as NVIDIA's H100 GPU. However, the superiority of low-bit INT versus FP formats for quantization on LLMs remains unclear. In this study, we conduct a comparative analysis of INT and FP quantization with the same bit-width, revealing that the optimal quantization format varies across different layers due to the complexity and diversity of tensor distribution. Consequently, we advocate the Mixture of Formats Quantization (MoFQ), which selects the optimal format on a layer-wise basis. This simple yet effective approach achieves state-of-the-art results in both weight-only (W-only) and weight-activation (WA) post-training quantization scenarios when tested on LLaMA across various tasks. In 4-bit W-only quantization, MoFQ surpasses GPTQ without complex hyperparameter tuning and with an order of magnitude faster quantization speed. While in 8-bit WA quantization, MoFQ significantly outperforms INT/FP-only methods, achieving performance close to the full precision model. Notably, MoFQ incurs no hardware overhead compared to INT/FP-only quantization, as the bit-width remains unchanged.\n        \u25b3 Less\n      ",
    "title": "Integer or Floating Point? New Outlooks for Low-Bit Quantization on Large Language Models",
    "date": "20 May, 2023",
    "authors": [
      "Yijia Zhang",
      " Lingran Zhao",
      " Shijie Cao",
      " Wenqiang Wang",
      " Ting Cao",
      " Fan Yang",
      " Mao Yang",
      " Shanghang Zhang",
      " Ningyi Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10250",
    "paper_id": "2305.10250",
    "abstract": "\n        Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a human-like memory mechanism. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models like ChatGLM. We exemplify application of MemoryBank through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialogs, SiliconFriend displays heightened empathy in its interactions. Experiment involves both qualitative analysis with real-world user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.\n        \u25b3 Less\n      ",
    "title": "MemoryBank: Enhancing Large Language Models with Long-Term Memory",
    "date": "20 May, 2023",
    "authors": [
      "Wanjun Zhong",
      " Lianghong Guo",
      " Qiqi Gao",
      " He Ye",
      " Yanlin Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12365",
    "paper_id": "2305.12365",
    "abstract": "\n        In recent years, the development of Artificial Intelligence (AI) has shown tremendous potential in diverse areas. Among them, reinforcement learning (RL) has proven to be an effective solution for learning intelligent control strategies. As an inevitable trend for mitigating climate change, hybrid electric vehicles (HEVs) rely on efficient energy management strategies (EMS) to minimize energy consumption. Many researchers have employed RL to learn optimal EMS for specific vehicle models. However, most of these models tend to be complex and proprietary, making them unsuitable for broad applicability. This paper presents a novel framework, in which we implement and integrate RL-based EMS with the open-source vehicle simulation tool called FASTSim. The learned RL-based EMSs are evaluated on various vehicle models using different test drive cycles and prove to be effective in improving energy efficiency.\n        \u25b3 Less\n      ",
    "title": "Towards Optimal Energy Management Strategy for Hybrid Electric Vehicle with Reinforcement Learning",
    "date": "20 May, 2023",
    "authors": [
      "Xinyang Wu",
      " Elisabeth Wedernikow",
      " Christof Nitsche",
      " Marco F. Huber"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12369",
    "paper_id": "2305.12369",
    "abstract": "\n        Accurately modeling affect dynamics, which refers to the changes and fluctuations in emotions and affective displays during human conversations, is crucial for understanding human interactions. By analyzing affect dynamics, we can gain insights into how people communicate, respond to different situations, and form relationships. However, modeling affect dynamics is challenging due to contextual factors, such as the complex and nuanced nature of interpersonal relationships, the situation, and other factors that influence affective displays. To address this challenge, we propose a Cross-person Memory Transformer (CPM-T) framework which is able to explicitly model affective dynamics (intrapersonal and interpersonal influences) by identifying verbal and non-verbal cues, and with a large language model to utilize the pre-trained knowledge and perform verbal reasoning. The CPM-T framework maintains memory modules to store and update the contexts within the conversation window, enabling the model to capture dependencies between earlier and later parts of a conversation. Additionally, our framework employs cross-modal attention to effectively align information from multi-modalities and leverage cross-person attention to align behaviors in multi-party interactions. We evaluate the effectiveness and generalizability of our approach on three publicly available datasets for joint engagement, rapport, and human beliefs prediction tasks. Remarkably, the CPM-T framework outperforms baseline models in average F1-scores by up to 7.3%, 9.3%, and 2.0% respectively. Finally, we demonstrate the importance of each component in the framework via ablation studies with respect to multimodal temporal behavior.\n        \u25b3 Less\n      ",
    "title": "HIINT: Historical, Intra- and Inter- personal Dynamics Modeling with Cross-person Memory Transformer",
    "date": "20 May, 2023",
    "authors": [
      "Yubin Kim",
      " Dong Won Lee",
      " Paul Pu Liang",
      " Sharifa Algohwinem",
      " Cynthia Breazeal",
      " Hae Won Park"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2203.02431",
    "paper_id": "2203.02431",
    "abstract": "\n        This paper presents a novel parametric curve-based method for lane detection in RGB images. Unlike state-of-the-art segmentation-based and point detection-based methods that typically require heuristics to either decode predictions or formulate a large sum of anchors, the curve-based methods can learn holistic lane representations naturally. To handle the optimization difficulties of existing polynomial curve methods, we propose to exploit the parametric B\u00e9zier curve due to its ease of computation, stability, and high freedom degrees of transformations. In addition, we propose the deformable convolution-based feature flip fusion, for exploiting the symmetry properties of lanes in driving scenes. The proposed method achieves a new state-of-the-art performance on the popular LLAMAS benchmark. It also achieves favorable accuracy on the TuSimple and CULane datasets, while retaining both low latency (> 150 FPS) and small model size (< 10M). Our method can serve as a new baseline, to shed the light on the parametric curves modeling for lane detection. Codes of our model and PytorchAutoDrive: a unified framework for self-driving perception, are available at: https://github.com/voldemortX/pytorch-auto-drive .\n        \u25b3 Less\n      ",
    "title": "Rethinking Efficient Lane Detection via Curve Modeling",
    "date": "20 May, 2023",
    "authors": [
      "Zhengyang Feng",
      " Shaohua Guo",
      " Xin Tan",
      " Ke Xu",
      " Min Wang",
      " Lizhuang Ma"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12402",
    "paper_id": "2305.12402",
    "abstract": "\n        We investigate the online bandit learning of the monotone multi-linear DR-submodular functions, designing the algorithm BanditMLSM\\mathtt{BanditMLSM} that attains O(T2/3logT)O(T^{2/3}\\log T) of (1\u22121/e)(1-1/e)-regret. Then we reduce submodular bandit with partition matroid constraint and bandit sequential monotone maximization to the online bandit learning of the monotone multi-linear DR-submodular functions, attaining O(T2/3logT)O(T^{2/3}\\log T) of (1\u22121/e)(1-1/e)-regret in both problems, which improve the existing results. To the best of our knowledge, we are the first to give a sublinear regret algorithm for the submodular bandit with partition matroid constraint. A special case of this problem is studied by Streeter et al.(2009). They prove a O(T4/5)O(T^{4/5}) (1\u22121/e)(1-1/e)-regret upper bound. For the bandit sequential submodular maximization, the existing work proves an O(T2/3)O(T^{2/3}) regret with a suboptimal 1/21/2 approximation ratio (Niazadeh et al. 2021).\n        \u25b3 Less\n      ",
    "title": "Bandit Multi-linear DR-Submodular Maximization and Its Applications on Adversarial Submodular Bandits",
    "date": "20 May, 2023",
    "authors": [
      "Zongqi Wan",
      " Jialin Zhang",
      " Wei Chen",
      " Xiaoming Sun",
      " Zhijie Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12414",
    "paper_id": "2305.12414",
    "abstract": "\n        We present a unified pipeline architecture for a real-time detection system on an embedded system for UAVs. Neural architectures have been the industry standard for computer vision. However, most existing works focus solely on concatenating deeper layers to achieve higher accuracy with run-time performance as the trade-off. This pipeline of networks can exploit the domain-specific knowledge on aerial pedestrian detection and activity recognition for the emerging UAV applications of autonomous surveying and activity reporting. In particular, our pipeline architectures operate in a time-sensitive manner, have high accuracy in detecting pedestrians from various aerial orientations, use a novel attention map for multi-activities recognition, and jointly refine its detection with temporal information. Numerically, we demonstrate our model's accuracy and fast inference speed on embedded systems. We empirically deployed our prototype hardware with full live feeds in a real-world open-field environment.\n        \u25b3 Less\n      ",
    "title": "Real-time Aerial Detection and Reasoning on Embedded-UAVs",
    "date": "21 May, 2023",
    "authors": [
      "Tin Lai"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12417",
    "paper_id": "2305.12417",
    "abstract": "\n        Novel high-resolution pressure-sensor arrays allow treating pressure readings as standard images. Computer vision algorithms and methods such as Convolutional Neural Networks (CNN) can be used to identify contact objects. In this paper, a high-resolution tactile sensor has been attached to a robotic end-effector to identify contacted objects. Two CNN-based approaches have been employed to classify pressure images. These methods include a transfer learning approach using a pre-trained CNN on an RGB-images dataset and a custom-made CNN (TactNet) trained from scratch with tactile information. The transfer learning approach can be carried out by retraining the classification layers of the network or replacing these layers with an SVM. Overall, 11 configurations based on these methods have been tested: 8 transfer learning-based, and 3 TactNet-based. Moreover, a study of the performance of the methods and a comparative discussion with the current state-of-the-art on tactile object recognition is presented.\n        \u25b3 Less\n      ",
    "title": "CNN-based Methods for Object Recognition with High-Resolution Tactile Sensors",
    "date": "21 May, 2023",
    "authors": [
      "Juan M. Gandarias",
      " Alfonso J. Garc\u00eda-Cerezo",
      " Jes\u00fas M. G\u00f3mez-de-Gabriel"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10435",
    "paper_id": "2305.10435",
    "abstract": "\n        The Generative Pre-trained Transformer (GPT) represents a notable breakthrough in the domain of natural language processing, which is propelling us toward the development of machines that can understand and communicate using language in a manner that closely resembles that of humans. GPT is based on the transformer architecture, a deep neural network designed for natural language processing tasks. Due to their impressive performance on natural language processing tasks and ability to effectively converse, GPT have gained significant popularity among researchers and industrial communities, making them one of the most widely used and effective models in natural language processing and related fields, which motivated to conduct this review. This review provides a detailed overview of the GPT, including its architecture, working process, training procedures, enabling technologies, and its impact on various applications. In this review, we also explored the potential challenges and limitations of a GPT. Furthermore, we discuss potential solutions and future directions. Overall, this paper aims to provide a comprehensive understanding of GPT, enabling technologies, their impact on various applications, emerging challenges, and potential solutions.\n        \u25b3 Less\n      ",
    "title": "Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions",
    "date": "21 May, 2023",
    "authors": [
      "Gokul Yenduri",
      " Ramalingam M",
      " Chemmalar Selvi G",
      " Supriya Y",
      " Gautam Srivastava",
      " Praveen Kumar Reddy Maddikunta",
      " Deepti Raj G",
      " Rutvij H Jhaveri",
      " Prabadevi B",
      " Weizheng Wang",
      " Athanasios V. Vasilakos",
      " Thippa Reddy Gadekallu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12424",
    "paper_id": "2305.12424",
    "abstract": "\n        While visual and auditory information conveyed by wavelength of light and frequency of sound have been decoded, predicting olfactory information encoded by the combination of odorants remains challenging due to the unknown and potentially discontinuous perceptual space of smells and odorants. Herein, we develop a deep learning model called Mol-PECO (Molecular Representation by Positional Encoding of Coulomb Matrix) to predict olfactory perception from molecular structures. Mol-PECO updates the learned atom embedding by directional graph convolutional networks (GCN), which model the Laplacian eigenfunctions as positional encoding, and Coulomb matrix, which encodes atomic coordinates and charges. With a comprehensive dataset of 8,503 molecules, Mol-PECO directly achieves an area-under-the-receiver-operating-characteristic (AUROC) of 0.813 in 118 odor descriptors, superior to the machine learning of molecular fingerprints (AUROC of 0.761) and GCN of adjacency matrix (AUROC of 0.678). The learned embeddings by Mol-PECO also capture a meaningful odor space with global clustering of descriptors and local retrieval of similar odorants. Our work may promote the understanding and decoding of the olfactory sense and mechanisms.\n        \u25b3 Less\n      ",
    "title": "Mol-PECO: a deep learning model to predict human olfactory perception from molecular structures",
    "date": "21 May, 2023",
    "authors": [
      "Mengji Zhang",
      " Yusuke Hiki",
      " Akira Funahashi",
      " Tetsuya J. Kobayashi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12434",
    "paper_id": "2305.12434",
    "abstract": "\n        Powered by advanced Artificial Intelligence (AI) techniques, conversational AI systems, such as ChatGPT and digital assistants like Siri, have been widely deployed in daily life. However, such systems may still produce content containing biases and stereotypes, causing potential social problems. Due to the data-driven, black-box nature of modern AI techniques, comprehensively identifying and measuring biases in conversational systems remains a challenging task. Particularly, it is hard to generate inputs that can comprehensively trigger potential bias due to the lack of data containing both social groups as well as biased properties. In addition, modern conversational systems can produce diverse responses (e.g., chatting and explanation), which makes existing bias detection methods simply based on the sentiment and the toxicity hardly being adopted. In this paper, we propose BiasAsker, an automated framework to identify and measure social bias in conversational AI systems. To obtain social groups and biased properties, we construct a comprehensive social bias dataset, containing a total of 841 groups and 8,110 biased properties. Given the dataset, BiasAsker automatically generates questions and adopts a novel method based on existence measurement to identify two types of biases (i.e., absolute bias and related bias) in conversational systems. Extensive experiments on 8 commercial systems and 2 famous research models, such as ChatGPT and GPT-3, show that 32.83% of the questions generated by BiasAsker can trigger biased behaviors in these widely deployed conversational systems. All the code, data, and experimental results have been released to facilitate future research.\n        \u25b3 Less\n      ",
    "title": "BiasAsker: Measuring the Bias in Conversational AI System",
    "date": "21 May, 2023",
    "authors": [
      "Yuxuan Wan",
      " Wenxuan Wang",
      " Pinjia He",
      " Jiazhen Gu",
      " Haonan Bai",
      " Michael Lyu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12449",
    "paper_id": "2305.12449",
    "abstract": "\n        Federated Multilingual Neural Machine Translation (Fed-MNMT) has emerged as a promising paradigm for institutions with limited language resources. This approach allows multiple institutions to act as clients and train a unified model through model synchronization, rather than collecting sensitive data for centralized training. This significantly reduces the cost of corpus collection and preserves data privacy. However, as pre-trained language models (PLMs) continue to increase in size, the communication cost for transmitting parameters during synchronization has become a training speed bottleneck. In this paper, we propose a communication-efficient Fed-MNMT framework that addresses this issue by keeping PLMs frozen and only transferring lightweight adapter modules between clients. Since different language pairs exhibit substantial discrepancies in data distributions, adapter parameters of clients may conflict with each other. To tackle this, we explore various clustering strategies to group parameters for integration and mitigate the negative effects of conflicting parameters. Experimental results demonstrate that our framework reduces communication cost by over 98% while achieving similar or even better performance compared to competitive baselines. Further analysis reveals that clustering strategies effectively solve the problem of linguistic discrepancy and pruning adapter modules further improves communication efficiency.\n        \u25b3 Less\n      ",
    "title": "Communication Efficient Federated Learning for Multilingual Neural Machine Translation with Adapter",
    "date": "21 May, 2023",
    "authors": [
      "Yi Liu",
      " Xiaohan Bi",
      " Lei Li",
      " Sishuo Chen",
      " Wenkai Yang",
      " Xu Sun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.06037",
    "paper_id": "2302.06037",
    "abstract": "\n        This paper presents a novel end-to-end deep learning framework for real-time inertial attitude estimation using 6DoF IMU measurements. Inertial Measurement Units are widely used in various applications, including engineering and medical sciences. However, traditional filters used for attitude estimation suffer from poor generalization over different motion patterns and environmental disturbances. To address this problem, we propose two deep learning models that incorporate accelerometer and gyroscope readings as inputs. These models are designed to be generalized to different motion patterns, sampling rates, and environmental disturbances. Our models consist of convolutional neural network layers combined with Bi-Directional Long-Short Term Memory followed by a Fully Forward Neural Network to estimate the quaternion. We evaluate the proposed method on seven publicly available datasets, totaling more than 120 hours and 200 kilometers of IMU measurements. Our results show that the proposed method outperforms state-of-the-art methods in terms of accuracy and robustness. Additionally, our framework demonstrates superior generalization over various motion characteristics and sensor sampling rates. Overall, this paper provides a comprehensive and reliable solution for real-time inertial attitude estimation using 6DoF IMUs, which has significant implications for a wide range of applications.\n        \u25b3 Less\n      ",
    "title": "Generalizable End-to-End Deep Learning Frameworks for Real-Time Attitude Estimation Using 6DoF Inertial Measurement Units",
    "date": "21 May, 2023",
    "authors": [
      "Arman Asgharpoor Golroudbari",
      " Mohammad Hossein Sabour"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12463",
    "paper_id": "2305.12463",
    "abstract": "\n        Randomly masking text spans in ordinary texts in the pre-training stage hardly allows models to acquire the ability to generate simple texts. It can hurt the performance of pre-trained models on text simplification tasks. In this paper, we propose a new continued pre-training strategy to teach the pre-trained model to generate simple texts. We continue pre-training BART, a representative model, to obtain SimpleBART. It consistently and significantly improves the results on lexical simplification, sentence simplification, and document-level simplification tasks over BART. At the end, we compare SimpleBART with several representative large language models (LLMs).\n        \u25b3 Less\n      ",
    "title": "Teaching the Pre-trained Model to Generate Simple Texts for Text Simplification",
    "date": "21 May, 2023",
    "authors": [
      "Renliang Sun",
      " Wei Xu",
      " Xiaojun Wan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.04068",
    "paper_id": "2212.04068",
    "abstract": "\n        While pre-trained Chinese language models have demonstrated impressive performance on a wide range of NLP tasks, the Chinese Spell Checking (CSC) task remains a challenge. Previous research has explored using information such as glyphs and phonetics to improve the ability to distinguish misspelled characters, with good results. However, the generalization ability of these models is not well understood: it is unclear whether they incorporate glyph-phonetic information and, if so, whether this information is fully utilized. In this paper, we aim to better understand the role of glyph-phonetic information in the CSC task and suggest directions for improvement. Additionally, we propose a new, more challenging, and practical setting for testing the generalizability of CSC models. All code is made publicly available.\n        \u25b3 Less\n      ",
    "title": "Investigating Glyph Phonetic Information for Chinese Spell Checking: What Works and What's Next",
    "date": "21 May, 2023",
    "authors": [
      "Xiaotian Zhang",
      " Yanjun Zheng",
      " Hang Yan",
      " Xipeng Qiu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08135",
    "paper_id": "2305.08135",
    "abstract": "\n        Existing knowledge-enhanced methods have achieved remarkable results in certain QA tasks via obtaining diverse knowledge from different knowledge bases. However, limited by the properties of retrieved knowledge, they still have trouble benefiting from both the knowledge relevance and distinguishment simultaneously. To address the challenge, we propose CPACE, a Concept-centric Prompt-bAsed Contrastive Explanation Generation model, which aims to convert obtained symbolic knowledge into a contrastive explanation for better distinguishing the differences among given candidates. Firstly, following previous works, we retrieve different types of symbolic knowledge with a concept-centric knowledge extraction module. After that, we generate corresponding contrastive explanations using acquired symbolic knowledge and explanation prompts as guidance for better modeling the knowledge distinguishment and interpretability. Finally, we regard the generated contrastive explanation as external knowledge for downstream task enhancement. We conduct a series of experiments on three widely-used question-answering datasets: CSQA, QASC, and OBQA. Experimental results demonstrate that with the help of generated contrastive explanation, our CPACE model achieves new SOTA on CSQA (89.8% on the testing set, 0.9% higher than human performance), and gains impressive improvement on QASC and OBQA (4.2% and 3.5%, respectively).\n        \u25b3 Less\n      ",
    "title": "Distinguish Before Answer: Generating Contrastive Explanation as Knowledge for Commonsense Question Answering",
    "date": "21 May, 2023",
    "authors": [
      "Qianglong Chen",
      " Guohai Xu",
      " Ming Yan",
      " Ji Zhang",
      " Fei Huang",
      " Luo Si",
      " Yin Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12483",
    "paper_id": "2305.12483",
    "abstract": "\n        Ambiguous questions are a challenge for Question Answering models, as they require answers that cover multiple interpretations of the original query. To this end, these models are required to generate long-form answers that often combine conflicting pieces of information. Although recent advances in the field have shown strong capabilities in generating fluent responses, certain research questions remain unanswered. Does model/data scaling improve the answers' quality? Do automated metrics align with human judgment? To what extent do these models ground their answers in evidence? In this study, we aim to thoroughly investigate these aspects, and provide valuable insights into the limitations of the current approaches. To aid in reproducibility and further extension of our work, we open-source our code at https://github.com/din0s/ambig_lfqa.\n        \u25b3 Less\n      ",
    "title": "Model Analysis & Evaluation for Ambiguous Question Answering",
    "date": "21 May, 2023",
    "authors": [
      "Konstantinos Papakostas",
      " Irene Papadopoulou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12487",
    "paper_id": "2305.12487",
    "abstract": "\n        Humans learn to master open-ended repertoires of skills by imagining and practicing their own goals. This autotelic learning process, literally the pursuit of self-generated (auto) goals (telos), becomes more and more open-ended as the goals become more diverse, abstract and creative. The resulting exploration of the space of possible skills is supported by an inter-individual exploration: goal representations are culturally evolved and transmitted across individuals, in particular using language. Current artificial agents mostly rely on predefined goal representations corresponding to goal spaces that are either bounded (e.g. list of instructions), or unbounded (e.g. the space of possible visual inputs) but are rarely endowed with the ability to reshape their goal representations, to form new abstractions or to imagine creative goals. In this paper, we introduce a language model augmented autotelic agent (LMA3) that leverages a pretrained language model (LM) to support the representation, generation and learning of diverse, abstract, human-relevant goals. The LM is used as an imperfect model of human cultural transmission; an attempt to capture aspects of humans' common-sense, intuitive physics and overall interests. Specifically, it supports three key components of the autotelic architecture: 1)~a relabeler that describes the goals achieved in the agent's trajectories, 2)~a goal generator that suggests new high-level goals along with their decomposition into subgoals the agent already masters, and 3)~reward functions for each of these goals. Without relying on any hand-coded goal representations, reward functions or curriculum, we show that LMA3 agents learn to master a large diversity of skills in a task-agnostic text-based environment.\n        \u25b3 Less\n      ",
    "title": "Augmenting Autotelic Agents with Large Language Models",
    "date": "21 May, 2023",
    "authors": [
      "C\u00e9dric Colas",
      " Laetitia Teodorescu",
      " Pierre-Yves Oudeyer",
      " Xingdi Yuan",
      " Marc-Alexandre C\u00f4t\u00e9"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.12600",
    "paper_id": "2302.12600",
    "abstract": "\n        Evolutionary computation is an important component within various fields such as artificial intelligence research, reinforcement learning, robotics, industrial automation and/or optimization, engineering design, etc. Considering the increasing computational demands and the dimensionalities of modern optimization problems, the requirement for scalable, re-usable, and practical evolutionary algorithm implementations has been growing. To address this requirement, we present EvoTorch: an evolutionary computation library designed to work with high-dimensional optimization problems, with GPU support and with high parallelization capabilities. EvoTorch is based on and seamlessly works with the PyTorch library, and therefore, allows the users to define their optimization problems using a well-known API.\n        \u25b3 Less\n      ",
    "title": "EvoTorch: Scalable Evolutionary Computation in Python",
    "date": "21 May, 2023",
    "authors": [
      "Nihat Engin Toklu",
      " Timothy Atkinson",
      " Vojt\u011bch Micka",
      " Pawe\u0142 Liskowski",
      " Rupesh Kumar Srivastava"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12519",
    "paper_id": "2305.12519",
    "abstract": "\n        Large Language Models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating fake social media postings that can sway election results. Detecting whether a text is machine-generated has thus become increasingly important. While machine-learning-based detection strategies exhibit superior performance, they often lack generalizability, limiting their practicality. In this work, we introduce GPT Paternity Test (GPT-Pat), which reliably detects machine-generated text across varied datasets. Given a text under scrutiny, we leverage ChatGPT to generate a corresponding question and provide a re-answer to the question. By comparing the similarity between the original text and the generated re-answered text, it can be determined whether the text is machine-generated. GPT-Pat consists of a Siamese network to compute the similarity between the original text and the generated re-answered text and a binary classifier. Our method achieved an average accuracy of 94.57% on four generalization test sets, surpassing the state-of-the-art RoBERTa-based method by 12.34%. The accuracy drop of our method is only about half of that of the RoBERTa-based method when it is attacked by re-translation and polishing.\n        \u25b3 Less\n      ",
    "title": "GPT Paternity Test: GPT Generated Text Detection with GPT Genetic Inheritance",
    "date": "21 May, 2023",
    "authors": [
      "Xiao Yu",
      " Yuang Qi",
      " Kejiang Chen",
      " Guoqiang Chen",
      " Xi Yang",
      " Pengyuan Zhu",
      " Weiming Zhang",
      " Nenghai Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12535",
    "paper_id": "2305.12535",
    "abstract": "\n        Language Generation Models produce words based on the previous context. Although existing methods offer input attributions as explanations for a model's prediction, it is still unclear how prior words affect the model's decision throughout the layers. In this work, we leverage recent advances in explainability of the Transformer and present a procedure to analyze models for language generation. Using contrastive examples, we compare the alignment of our explanations with evidence of the linguistic phenomena, and show that our method consistently aligns better than gradient-based and perturbation-based baselines. Then, we investigate the role of MLPs inside the Transformer and show that they learn features that help the model predict words that are grammatically acceptable. Lastly, we apply our method to Neural Machine Translation models, and demonstrate that they generate human-like source-target alignments for building predictions.\n        \u25b3 Less\n      ",
    "title": "Explaining How Transformers Use Context to Build Predictions",
    "date": "21 May, 2023",
    "authors": [
      "Javier Ferrando",
      " Gerard I. G\u00e1llego",
      " Ioannis Tsiamas",
      " Marta R. Costa-juss\u00e0"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13341",
    "paper_id": "2305.13341",
    "abstract": "\n        Physics is a field of science that has traditionally used the scientific method to answer questions about why natural phenomena occur and to make testable models that explain the phenomena. Discovering equations, laws and principles that are invariant, robust and causal explanations of the world has been fundamental in physical sciences throughout the centuries. Discoveries emerge from observing the world and, when possible, performing interventional studies in the system under study. With the advent of big data and the use of data-driven methods, causal and equation discovery fields have grown and made progress in computer science, physics, statistics, philosophy, and many applied fields. All these domains are intertwined and can be used to discover causal relations, physical laws, and equations from observational data. This paper reviews the concepts, methods, and relevant works on causal and equation discovery in the broad field of Physics and outlines the most important challenges and promising future lines of research. We also provide a taxonomy for observational causal and equation discovery, point out connections, and showcase a complete set of case studies in Earth and climate sciences, fluid dynamics and mechanics, and the neurosciences. This review demonstrates that discovering fundamental laws and causal relations by observing natural phenomena is being revolutionised with the efficient exploitation of observational data, modern machine learning algorithms and the interaction with domain knowledge. Exciting times are ahead with many challenges and opportunities to improve our understanding of complex systems.\n        \u25b3 Less\n      ",
    "title": "Discovering Causal Relations and Equations from Data",
    "date": "21 May, 2023",
    "authors": [
      "Gustau Camps-Valls",
      " Andreas Gerhardus",
      " Urmi Ninad",
      " Gherardo Varando",
      " Georg Martius",
      " Emili Balaguer-Ballester",
      " Ricardo Vinuesa",
      " Emiliano Diaz",
      " Laure Zanna",
      " Jakob Runge"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12557",
    "paper_id": "2305.12557",
    "abstract": "\n        Federated Learning (FL) is a distributed learning scheme to train a shared model across clients. One common and fundamental challenge in FL is that the sets of data across clients could be non-identically distributed and have different sizes. Personalized Federated Learning (PFL) attempts to solve this challenge via locally adapted models. In this work, we present a novel framework for PFL based on hierarchical Bayesian modeling and variational inference. A global model is introduced as a latent variable to augment the joint distribution of clients' parameters and capture the common trends of different clients, optimization is derived based on the principle of maximizing the marginal likelihood and conducted using variational expectation maximization. Our algorithm gives rise to a closed-form estimation of a confidence value which comprises the uncertainty of clients' parameters and local model deviations from the global model. The confidence value is used to weigh clients' parameters in the aggregation stage and adjust the regularization effect of the global model. We evaluate our method through extensive empirical studies on multiple datasets. Experimental results show that our approach obtains competitive results under mild heterogeneous circumstances while significantly outperforming state-of-the-art PFL frameworks in highly heterogeneous settings. Our code is available at https://github.com/JunyiZhu-AI/confidence_aware_PFL.\n        \u25b3 Less\n      ",
    "title": "Confidence-aware Personalized Federated Learning via Variational Expectation Maximization",
    "date": "21 May, 2023",
    "authors": [
      "Junyi Zhu",
      " Xingchen Ma",
      " Matthew B. Blaschko"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12564",
    "paper_id": "2305.12564",
    "abstract": "\n        We investigate how people perceive ChatGPT, and, in particular, how they assign human-like attributes such as gender to the chatbot. Across five pre-registered studies (N = 1,552), we find that people are more likely to perceive ChatGPT to be male than female. Specifically, people perceive male gender identity (1) following demonstrations of ChatGPT's core abilities (e.g., providing information or summarizing text), (2) in the absence of such demonstrations, and (3) across different methods of eliciting perceived gender (using various scales and asking to name ChatGPT). Moreover, we find that this seemingly default perception of ChatGPT as male can reverse when ChatGPT's feminine-coded abilities are highlighted (e.g., providing emotional support for a user).\n        \u25b3 Less\n      ",
    "title": "ChatGPT Is More Likely to Be Perceived as Male Than Female",
    "date": "21 May, 2023",
    "authors": [
      "Jared Wong",
      " Jin Kim"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.09714",
    "paper_id": "2207.09714",
    "abstract": "\n        Mechanistic simulators are an indispensable tool for epidemiology to explore the behavior of complex, dynamic infections under varying conditions and navigate uncertain environments. Agent-based models (ABMs) are an increasingly popular simulation paradigm that can represent the heterogeneity of contact interactions with granular detail and agency of individual behavior. However, conventional ABM frameworks are not differentiable and present challenges in scalability; due to which it is non-trivial to connect them to auxiliary data sources. In this paper, we introduce GradABM: a scalable, differentiable design for agent-based modeling that is amenable to gradient-based learning with automatic differentiation. GradABM can quickly simulate million-size populations in few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous data sources. This provides an array of practical benefits for calibration, forecasting, and evaluating policy interventions. We demonstrate the efficacy of GradABM via extensive experiments with real COVID-19 and influenza datasets.\n        \u25b3 Less\n      ",
    "title": "Differentiable Agent-based Epidemiology",
    "date": "21 May, 2023",
    "authors": [
      "Ayush Chopra",
      " Alexander Rodr\u00edguez",
      " Jayakumar Subramanian",
      " Arnau Quera-Bofarull",
      " Balaji Krishnamurthy",
      " B. Aditya Prakash",
      " Ramesh Raskar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12571",
    "paper_id": "2305.12571",
    "abstract": "\n        Machine learning is facing a 'reproducibility crisis' where a significant number of works report failures when attempting to reproduce previously published results. We evaluate the sources of reproducibility failures using a meta-analysis of 142 replication studies from ReScience C and 204 code repositories. We find that missing experiment details such as hyperparameters are potential causes of unreproducibility. We experimentally show the bias of different hyperparameter selection strategies and conclude that consolidated artifacts with a unified framework can help support reproducibility.\n        \u25b3 Less\n      ",
    "title": "Reproducibility Requires Consolidated Artifacts",
    "date": "21 May, 2023",
    "authors": [
      "Iordanis Fostiropoulos",
      " Bowman Brown",
      " Laurent Itti"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07494",
    "paper_id": "2305.07494",
    "abstract": "\n        Most networks are not static objects, but instead they change over time. This observation has sparked rigorous research on temporal graphs within the last years. In temporal graphs, we have a fixed set of nodes and the connections between them are only available at certain time steps. This gives rise to a plethora of algorithmic problems on such graphs, most prominently the problem of finding temporal spanners, i.e., the computation of subgraphs that guarantee all pairs reachability via temporal paths. To the best of our knowledge, only centralized approaches for the solution of this problem are known. However, many real-world networks are not shaped by a central designer but instead they emerge and evolve by the interaction of many strategic agents. This observation is the driving force of the recent intensive research on game-theoretic network formation models.\n  In this work we bring together these two recent research directions: temporal graphs and game-theoretic network formation. As a first step into this new realm, we focus on a simplified setting where a complete temporal host graph is given and the agents, corresponding to its nodes, selfishly create incident edges to ensure that they can reach all other nodes via temporal paths in the created network. This yields temporal spanners as equilibria of our game. We prove results on the convergence to and the existence of equilibrium networks, on the complexity of finding best agent strategies, and on the quality of the equilibria. By taking these first important steps, we uncover challenging open problems that call for an in-depth exploration of the creation of temporal graphs by strategic agents.\n        \u25b3 Less\n      ",
    "title": "Temporal Network Creation Games",
    "date": "21 May, 2023",
    "authors": [
      "Davide Bil\u00f2",
      " Sarel Cohen",
      " Tobias Friedrich",
      " Hans Gawendowicz",
      " Nicolas Klodt",
      " Pascal Lenzner",
      " George Skretas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12600",
    "paper_id": "2305.12600",
    "abstract": "\n        In-context learning is the ability of a pretrained model to adapt to novel and diverse downstream tasks by conditioning on prompt examples, without optimizing any parameters. While large language models have demonstrated this ability, how in-context learning could be performed over graphs is unexplored. In this paper, we develop \\textbf{Pr}etraining \\textbf{O}ver \\textbf{D}iverse \\textbf{I}n-Context \\textbf{G}raph S\\textbf{y}stems (PRODIGY), the first pretraining framework that enables in-context learning over graphs. The key idea of our framework is to formulate in-context learning over graphs with a novel \\emph{prompt graph} representation, which connects prompt examples and queries. We then propose a graph neural network architecture over the prompt graph and a corresponding family of in-context pretraining objectives. With PRODIGY, the pretrained model can directly perform novel downstream classification tasks on unseen graphs via in-context learning. We provide empirical evidence of the effectiveness of our framework by showcasing its strong in-context learning performance on tasks involving citation networks and knowledge graphs. Our approach outperforms the in-context learning accuracy of contrastive pretraining baselines with hard-coded adaptation by 18\\% on average across all setups. Moreover, it also outperforms standard finetuning with limited data by 33\\% on average with in-context learning.\n        \u25b3 Less\n      ",
    "title": "PRODIGY: Enabling In-context Learning Over Graphs",
    "date": "21 May, 2023",
    "authors": [
      "Qian Huang",
      " Hongyu Ren",
      " Peng Chen",
      " Gregor Kr\u017emanc",
      " Daniel Zeng",
      " Percy Liang",
      " Jure Leskovec"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.09721",
    "paper_id": "2212.09721",
    "abstract": "\n        Language models (LMs) have yielded impressive results on many language reasoning tasks, but their unexpected errors raise doubts about their reasoning abilities. In light of this, there is growing interest in finetuning/prompting LMs with both task instances and their associated free-text rationales (FTRs), which explain the correct reasoning process for predicting the correct task output (i.e., how to be \"right for the right reasons\"). However, existing finetuning methods fail to improve LM performance, while prompting needs prohibitively large (i.e., >50B) LMs to work well. We propose KNIFE, which shows that reasoning knowledge can be effectively distilled from FTRs into a small (i.e., <1B) LM and improve the LM's performance. First, KNIFE finetunes a teacher LM (given task input and FTR) to predict the task output, transferring reasoning knowledge from the FTRs to the teacher's hidden states. Second, KNIFE finetunes a student LM (given task input only) such that its hidden states are aligned with the teacher's. Thus, the student is endowed with reasoning knowledge but can be used for inference without direct FTR input. On two question-answering datasets, KNIFE outperforms various finetuning and prompting baselines in fully-supervised and low-resource settings. Also, we observe that FTR quality is crucial to KNIFE's performance.\n        \u25b3 Less\n      ",
    "title": "KNIFE: Distilling Reasoning Knowledge From Free-Text Rationales",
    "date": "21 May, 2023",
    "authors": [
      "Aaron Chan",
      " Zhiyuan Zeng",
      " Wyatt Lake",
      " Brihi Joshi",
      " Hanjie Chen",
      " Xiang Ren"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12618",
    "paper_id": "2305.12618",
    "abstract": "\n        Molecular representation learning is a crucial task in predicting molecular properties. Molecules are often modeled as graphs where atoms and chemical bonds are represented as nodes and edges, respectively, and Graph Neural Networks (GNNs) have been commonly utilized to predict atom-related properties, such as reactivity and solubility. However, functional groups (subgraphs) are closely related to some chemical properties of molecules, such as efficacy, and metabolic properties, which cannot be solely determined by individual atoms. In this paper, we introduce a new model for molecular representation learning called the Atomic and Subgraph-aware Bilateral Aggregation (ASBA), which addresses the limitations of previous atom-wise and subgraph-wise models by incorporating both types of information. ASBA consists of two branches, one for atom-wise information and the other for subgraph-wise information. Considering existing atom-wise GNNs cannot properly extract invariant subgraph features, we propose a decomposition-polymerization GNN architecture for the subgraph-wise branch. Furthermore, we propose cooperative node-level and graph-level self-supervised learning strategies for ASBA to improve its generalization. Our method offers a more comprehensive way to learn representations for molecular property prediction and has broad potential in drug and material discovery applications. Extensive experiments have demonstrated the effectiveness of our method.\n        \u25b3 Less\n      ",
    "title": "Atomic and Subgraph-aware Bilateral Aggregation for Molecular Representation Learning",
    "date": "21 May, 2023",
    "authors": [
      "Jiahao Chen",
      " Yurou Liu",
      " Jiangmeng Li",
      " Bing Su",
      " Jirong Wen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12623",
    "paper_id": "2305.12623",
    "abstract": "\n        The ability to continuously learn and adapt to new situations is one where humans are far superior compared to AI agents. We propose an approach to knowledge transfer using behavioural strategies as a form of transferable knowledge influenced by the human cognitive ability to develop strategies. A strategy is defined as a partial sequence of events - where an event is both the result of an agent's action and changes in state - to reach some predefined event of interest. This information acts as guidance or a partial solution that an agent can generalise and use to make predictions about how to handle unknown observed phenomena. As a first step toward this goal, we develop a method for extracting strategies from an agent's existing knowledge that can be applied in multiple contexts. Our method combines observed event frequency information with local sequence alignment techniques to find patterns of significance that form a strategy. We show that our method can identify plausible strategies in three environments: Pacman, Bank Heist and a dungeon-crawling video game. Our evaluation serves as a promising first step toward extracting knowledge for generalisation and, ultimately, transfer learning.\n        \u25b3 Less\n      ",
    "title": "Strategy Extraction in Single-Agent Games",
    "date": "21 May, 2023",
    "authors": [
      "Archana Vadakattu",
      " Michelle Blom",
      " Adrian R. Pearce"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12627",
    "paper_id": "2305.12627",
    "abstract": "\n        Generative methods greatly promote aspect-based sentiment analysis via generating a sequence of sentiment elements in a specified format. However, existing studies usually predict sentiment elements in a fixed order, which ignores the effect of the interdependence of the elements in a sentiment tuple and the diversity of language expression on the results. In this work, we propose Multi-view Prompting (MvP) that aggregates sentiment elements generated in different orders, leveraging the intuition of human-like problem-solving processes from different views. Specifically, MvP introduces element order prompts to guide the language model to generate multiple sentiment tuples, each with a different element order, and then selects the most reasonable tuples by voting. MvP can naturally model multi-view and multi-task as permutations and combinations of elements, respectively, outperforming previous task-specific designed methods on multiple ABSA tasks with a single model. Extensive experiments show that MvP significantly advances the state-of-the-art performance on 10 datasets of 4 benchmark tasks, and performs quite effectively in low-resource settings. Detailed evaluation verified the effectiveness, flexibility, and cross-task transferability of MvP.\n        \u25b3 Less\n      ",
    "title": "MvP: Multi-view Prompting Improves Aspect Sentiment Tuple Prediction",
    "date": "21 May, 2023",
    "authors": [
      "Zhibin Gou",
      " Qingyan Guo",
      " Yujiu Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12640",
    "paper_id": "2305.12640",
    "abstract": "\n        The success of many healthcare programs depends on participants' adherence. We consider the problem of scheduling interventions in low resource settings (e.g., placing timely support calls from health workers) to increase adherence and/or engagement. Past works have successfully developed several classes of Restless Multi-armed Bandit (RMAB) based solutions for this problem. Nevertheless, all past RMAB approaches assume that the participants' behaviour follows the Markov property. We demonstrate significant deviations from the Markov assumption on real-world data on a maternal health awareness program from our partner NGO, ARMMAN. Moreover, we extend RMABs to continuous state spaces, a previously understudied area. To tackle the generalised non-Markovian RMAB setting we (i) model each participant's trajectory as a time-series, (ii) leverage the power of time-series forecasting models to learn complex patterns and dynamics to predict future states, and (iii) propose the Time-series Arm Ranking Index (TARI) policy, a novel algorithm that selects the RMAB arms that will benefit the most from an intervention, given our future state predictions. We evaluate our approach on both synthetic data, and a secondary analysis on real data from ARMMAN, and demonstrate significant increase in engagement compared to the SOTA, deployed Whittle index solution. This translates to 16.3 hours of additional content listened, 90.8% more engagement drops prevented, and reaching more than twice as many high dropout-risk beneficiaries.\n        \u25b3 Less\n      ",
    "title": "Limited Resource Allocation in a Non-Markovian World: The Case of Maternal and Child Healthcare",
    "date": "21 May, 2023",
    "authors": [
      "Panayiotis Danassis",
      " Shresth Verma",
      " Jackson A. Killian",
      " Aparna Taneja",
      " Milind Tambe"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2206.10606",
    "paper_id": "2206.10606",
    "abstract": "\n        In reality, it is often more efficient to ask for help than to search the entire space to find an object with an unknown location. We present a learning framework that enables an agent to actively ask for help in such embodied visual navigation tasks, where the feedback informs the agent of where the goal is in its view. To emulate the real-world scenario that a teacher may not always be present, we propose a training curriculum where feedback is not always available. We formulate an uncertainty measure of where the goal is and use empirical results to show that through this approach, the agent learns to ask for help effectively while remaining robust when feedback is not available.\n        \u25b3 Less\n      ",
    "title": "Good Time to Ask: A Learning Framework for Asking for Help in Embodied Visual Navigation",
    "date": "21 May, 2023",
    "authors": [
      "Jenny Zhang",
      " Samson Yu",
      " Jiafei Duan",
      " Cheston Tan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04160",
    "paper_id": "2305.04160",
    "abstract": "\n        Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimodal information into languages. (2) Aligning X2L representations with the LLM: single-modal encoders are aligned with the LLM through X2L interfaces independently. (3) Integrating multiple modalities: all single-modal encoders are aligned with the LLM through X2L interfaces to integrate multimodal capabilities into the LLM. Our experiments show that X-LLM demonstrates impressive multimodel chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 84.5\\% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. And we also conduct quantitative tests on using LLM for ASR and multimodal ASR, hoping to promote the era of LLM-based speech recognition.\n        \u25b3 Less\n      ",
    "title": "X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages",
    "date": "21 May, 2023",
    "authors": [
      "Feilong Chen",
      " Minglun Han",
      " Haozhi Zhao",
      " Qingyang Zhang",
      " Jing Shi",
      " Shuang Xu",
      " Bo Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12647",
    "paper_id": "2305.12647",
    "abstract": "\n        This paper presents Reflective Linguistic Programming (RLP), a unique approach to conversational AI that emphasizes self-awareness and strategic planning. RLP encourages models to introspect on their own predefined personality traits, emotional responses to incoming messages, and planned strategies, enabling contextually rich, coherent, and engaging interactions. A striking illustration of RLP's potential involves a toy example, an AI persona with an adversarial orientation, a demon named `Bogus' inspired by the children's fairy tale Hansel & Gretel. Bogus exhibits sophisticated behaviors, such as strategic deception and sensitivity to user discomfort, that spontaneously arise from the model's introspection and strategic planning. These behaviors are not pre-programmed or prompted, but emerge as a result of the model's advanced cognitive modeling. The potential applications of RLP in socially-aware AGI (Social AGI) are vast, from nuanced negotiations and mental health support systems to the creation of diverse and dynamic AI personas. Our exploration of deception serves as a stepping stone towards a new frontier in AGI, one filled with opportunities for advanced cognitive modeling and the creation of truly human `digital souls'.\n        \u25b3 Less\n      ",
    "title": "Reflective Linguistic Programming (RLP): A Stepping Stone in Socially-Aware AGI (SocialAGI)",
    "date": "21 May, 2023",
    "authors": [
      "Kevin A. Fischer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12663",
    "paper_id": "2305.12663",
    "abstract": "\n        Standard model-based reinforcement learning (MBRL) approaches fit a transition model of the environment to all past experience, but this wastes model capacity on data that is irrelevant for policy improvement. We instead propose a new \"transition occupancy matching\" (TOM) objective for MBRL model learning: a model is good to the extent that the current policy experiences the same distribution of transitions inside the model as in the real environment. We derive TOM directly from a novel lower bound on the standard reinforcement learning objective. To optimize TOM, we show how to reduce it to a form of importance weighted maximum-likelihood estimation, where the automatically computed importance weights identify policy-relevant past experiences from a replay buffer, enabling stable optimization. TOM thus offers a plug-and-play model learning sub-routine that is compatible with any backbone MBRL algorithm. On various Mujoco continuous robotic control tasks, we show that TOM successfully focuses model learning on policy-relevant experience and drives policies faster to higher task rewards than alternative model learning approaches.\n        \u25b3 Less\n      ",
    "title": "TOM: Learning Policy-Aware Models for Model-Based Reinforcement Learning via Transition Occupancy Matching",
    "date": "21 May, 2023",
    "authors": [
      "Yecheng Jason Ma",
      " Kausik Sivakumar",
      " Jason Yan",
      " Osbert Bastani",
      " Dinesh Jayaraman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12683",
    "paper_id": "2305.12683",
    "abstract": "\n        Diffusion Models (DMs) have empowered great success in artificial-intelligence-generated content, especially in artwork creation, yet raising new concerns in intellectual properties and copyright. For example, infringers can make profits by imitating non-authorized human-created paintings with DMs. Recent researches suggest that various adversarial examples for diffusion models can be effective tools against these copyright infringements. However, current adversarial examples show weakness in transferability over different painting-imitating methods and robustness under straightforward adversarial defense, for example, noise purification. We surprisingly find that the transferability of adversarial examples can be significantly enhanced by exploiting a fused and modified adversarial loss term under consistent parameters. In this work, we comprehensively evaluate the cross-method transferability of adversarial examples. The experimental observation shows that our method generates more transferable adversarial examples with even stronger robustness against the simple adversarial defense.\n        \u25b3 Less\n      ",
    "title": "Mist: Towards Improved Adversarial Examples for Diffusion Models",
    "date": "21 May, 2023",
    "authors": [
      "Chumeng Liang",
      " Xiaoyu Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12692",
    "paper_id": "2305.12692",
    "abstract": "\n        With emerging topics (e.g., COVID-19) on social media as a source for the spreading misinformation, overcoming the distributional shifts between the original training domain (i.e., source domain) and such target domains remains a non-trivial task for misinformation detection. This presents an elusive challenge for early-stage misinformation detection, where a good amount of data and annotations from the target domain is not available for training. To address the data scarcity issue, we propose MetaAdapt, a meta learning based approach for domain adaptive few-shot misinformation detection. MetaAdapt leverages limited target examples to provide feedback and guide the knowledge transfer from the source to the target domain (i.e., learn to adapt). In particular, we train the initial model with multiple source tasks and compute their similarity scores to the meta task. Based on the similarity scores, we rescale the meta gradients to adaptively learn from the source tasks. As such, MetaAdapt can learn how to adapt the misinformation detection model and exploit the source data for improved performance in the target domain. To demonstrate the efficiency and effectiveness of our method, we perform extensive experiments to compare MetaAdapt with state-of-the-art baselines and large language models (LLMs) such as LLaMA, where MetaAdapt achieves better performance in domain adaptive few-shot misinformation detection with substantially reduced parameters on real-world datasets.\n        \u25b3 Less\n      ",
    "title": "MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection via Meta Learning",
    "date": "21 May, 2023",
    "authors": [
      "Zhenrui Yue",
      " Huimin Zeng",
      " Yang Zhang",
      " Lanyu Shang",
      " Dong Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12694",
    "paper_id": "2305.12694",
    "abstract": "\n        This paper presents a spell checker and correction tool specifically designed for Wolof, an under-represented spoken language in Africa. The proposed spell checker leverages a combination of a trie data structure, dynamic programming, and the weighted Levenshtein distance to generate suggestions for misspelled words. We created novel linguistic resources for Wolof, such as a lexicon and a corpus of misspelled words, using a semi-automatic approach that combines manual and automatic annotation methods. Despite the limited data available for the Wolof language, the spell checker's performance showed a predictive accuracy of 98.31% and a suggestion accuracy of 93.33%. Our primary focus remains the revitalization and preservation of Wolof as an Indigenous and spoken language in Africa, providing our efforts to develop novel linguistic resources. This work represents a valuable contribution to the growth of computational tools and resources for the Wolof language and provides a strong foundation for future studies in the automatic spell checking and correction field.\n        \u25b3 Less\n      ",
    "title": "Automatic Spell Checker and Correction for Under-represented Spoken Languages: Case Study on Wolof",
    "date": "21 May, 2023",
    "authors": [
      "Thierno Ibrahima Ciss\u00e9",
      " Fatiha Sadat"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.06060",
    "paper_id": "2303.06060",
    "abstract": "\n        Deep artificial neural networks (ANNs) play a major role in modeling the visual pathways of primate and rodent. However, they highly simplify the computational properties of neurons compared to their biological counterparts. Instead, Spiking Neural Networks (SNNs) are more biologically plausible models since spiking neurons encode information with time sequences of spikes, just like biological neurons do. However, there is a lack of studies on visual pathways with deep SNNs models. In this study, we model the visual cortex with deep SNNs for the first time, and also with a wide range of state-of-the-art deep CNNs and ViTs for comparison. Using three similarity metrics, we conduct neural representation similarity experiments on three neural datasets collected from two species under three types of stimuli. Based on extensive similarity analyses, we further investigate the functional hierarchy and mechanisms across species. Almost all similarity scores of SNNs are higher than their counterparts of CNNs with an average of 6.6%. Depths of the layers with the highest similarity scores exhibit little differences across mouse cortical regions, but vary significantly across macaque regions, suggesting that the visual processing structure of mice is more regionally homogeneous than that of macaques. Besides, the multi-branch structures observed in some top mouse brain-like neural networks provide computational evidence of parallel processing streams in mice, and the different performance in fitting macaque neural representations under different stimuli exhibits the functional specialization of information processing in macaques. Taken together, our study demonstrates that SNNs could serve as promising candidates to better model and explain the functional hierarchy and mechanisms of the visual system.\n        \u25b3 Less\n      ",
    "title": "Deep Spiking Neural Networks with High Representation Similarity Model Visual Pathways of Macaque and Mouse",
    "date": "21 May, 2023",
    "authors": [
      "Liwei Huang",
      " Zhengyu Ma",
      " Liutao Yu",
      " Huihui Zhou",
      " Yonghong Tian"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.03370",
    "paper_id": "2210.03370",
    "abstract": "\n        Learning provides a powerful tool for vision-based navigation, but the capabilities of learning-based policies are constrained by limited training data. If we could combine data from all available sources, including multiple kinds of robots, we could train more powerful navigation models. In this paper, we study how a general goal-conditioned model for vision-based navigation can be trained on data obtained from many distinct but structurally similar robots, and enable broad generalization across environments and embodiments. We analyze the necessary design decisions for effective data sharing across robots, including the use of temporal context and standardized action spaces, and demonstrate that an omnipolicy trained from heterogeneous datasets outperforms policies trained on any single dataset. We curate 60 hours of navigation trajectories from 6 distinct robots, and deploy the trained GNM on a range of new robots, including an underactuated quadrotor. We find that training on diverse data leads to robustness against degradation in sensing and actuation. Using a pre-trained navigation model with broad generalization capabilities can bootstrap applications on novel robots going forward, and we hope that the GNM represents a step in that direction. For more information on the datasets, code, and videos, please check out our project page https://sites.google.com/view/drive-any-robot.\n        \u25b3 Less\n      ",
    "title": "GNM: A General Navigation Model to Drive Any Robot",
    "date": "21 May, 2023",
    "authors": [
      "Dhruv Shah",
      " Ajay Sridhar",
      " Arjun Bhorkar",
      " Noriaki Hirose",
      " Sergey Levine"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12712",
    "paper_id": "2305.12712",
    "abstract": "\n        Over the past few years, audio classification task on large-scale dataset such as AudioSet has been an important research area. Several deeper Convolution-based Neural networks have shown compelling performance notably Vggish, YAMNet, and Pretrained Audio Neural Network (PANN). These models are available as pretrained architecture for transfer learning as well as specific audio task adoption. In this paper, we propose a lightweight on-device deep learning-based model for audio classification, LEAN. LEAN consists of a raw waveform-based temporal feature extractor called as Wave Encoder and logmel-based Pretrained YAMNet. We show that using a combination of trainable wave encoder, Pretrained YAMNet along with cross attention-based temporal realignment, results in competitive performance on downstream audio classification tasks with lesser memory footprints and hence making it suitable for resource constraints devices such as mobile, edge devices, etc . Our proposed system achieves on-device mean average precision(mAP) of .445 with a memory footprint of a mere 4.5MB on the FSD50K dataset which is an improvement of 22% over baseline on-device mAP on same dataset.\n        \u25b3 Less\n      ",
    "title": "LEAN: Light and Efficient Audio Classification Network",
    "date": "21 May, 2023",
    "authors": [
      "Shwetank Choudhary",
      " CR Karthik",
      " Punuru Sri Lakshmi",
      " Sumit Kumar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12720",
    "paper_id": "2305.12720",
    "abstract": "\n        This study constructed a Japanese chat dataset for tuning large language models (LLMs), which consist of about 8.4 million records. Recently, LLMs have been developed and gaining popularity. However, high-performing LLMs are usually mainly for English. There are two ways to support languages other than English by those LLMs: constructing LLMs from scratch or tuning existing models. However, in both ways, datasets are necessary parts. In this study, we focused on supporting Japanese in those LLMs and making a dataset for training or tuning LLMs in Japanese. The dataset we constructed consisted of various tasks, such as translation and knowledge tasks. In our experiment, we tuned an existing LLM using our dataset and evaluated the performance qualitatively. The results suggest that our dataset is possibly beneficial for LLMs. However, we also revealed some difficulties in constructing LLMs in languages other than English.\n        \u25b3 Less\n      ",
    "title": "llm-japanese-dataset v0: Construction of Japanese Chat Dataset for Large Language Models and its Methodology",
    "date": "21 May, 2023",
    "authors": [
      "Masanori Hirano",
      " Masahiro Suzuki",
      " Hiroki Sakaji"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12723",
    "paper_id": "2305.12723",
    "abstract": "\n        Large language models (LLMs) demonstrate remarkable medical expertise, but data privacy concerns impede their direct use in healthcare environments. Although offering improved data privacy protection, domain-specific small language models (SLMs) often underperform LLMs, emphasizing the need for methods that reduce this performance gap while alleviating privacy concerns. In this paper, we present a simple yet effective method that harnesses LLMs' medical proficiency to boost SLM performance in medical tasks under privacy-restricted scenarios. Specifically, we mitigate patient privacy issues by extracting keywords from medical data and prompting the LLM to generate a medical knowledge-intensive context by simulating clinicians' thought processes. This context serves as additional input for SLMs, augmenting their decision-making capabilities. Our method significantly enhances performance in both few-shot and full training settings across three medical knowledge-intensive tasks, achieving up to a 22.57% increase in absolute accuracy compared to SLM fine-tuning without context, and sets new state-of-the-art results in two medical tasks within privacy-restricted scenarios. Further out-of-domain testing and experiments in two general domain datasets showcase its generalizability and broad applicability.\n        \u25b3 Less\n      ",
    "title": "Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting",
    "date": "21 May, 2023",
    "authors": [
      "Xinlu Zhang",
      " Shiyang Li",
      " Xianjun Yang",
      " Chenxin Tian",
      " Yao Qin",
      " Linda Ruth Petzold"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10306",
    "paper_id": "2305.10306",
    "abstract": "\n        We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on 1414 benchmarks IE datasets with the supervised setting. The state-of-the-art performance in low-resource scenarios also verifies the transferability and effectiveness of UniEX.\n        \u25b3 Less\n      ",
    "title": "UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective",
    "date": "21 May, 2023",
    "authors": [
      "Ping Yang",
      " Junyu Lu",
      " Ruyi Gan",
      " Junjie Wang",
      " Yuxiang Zhang",
      " Jiaxing Zhang",
      " Pingjian Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12728",
    "paper_id": "2305.12728",
    "abstract": "\n        To date, there has been little concrete practical advice about how to ensure that diversity and inclusion considerations should be embedded within both specific Artificial Intelligence (AI) systems and the larger global AI ecosystem. In this chapter, we present a clear definition of diversity and inclusion in AI, one which positions this concept within an evolving and holistic ecosystem. We use this definition and conceptual framing to present a set of practical guidelines primarily aimed at AI technologists, data scientists and project leaders.\n        \u25b3 Less\n      ",
    "title": "Diversity and Inclusion in Artificial Intelligence",
    "date": "21 May, 2023",
    "authors": [
      "Didar Zowghi",
      " Francesca da Rimini"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12734",
    "paper_id": "2305.12734",
    "abstract": "\n        Although remarkable progress has been made in recent years, current multi-exposure image fusion (MEF) research is still bounded by the lack of real ground truth, objective evaluation function, and robust fusion strategy. In this paper, we study the MEF problem from a new perspective. We don't utilize any synthesized ground truth, design any loss function, or develop any fusion strategy. Our proposed method EMEF takes advantage of the wisdom of multiple imperfect MEF contributors including both conventional and deep learning-based methods. Specifically, EMEF consists of two main stages: pre-train an imitator network and tune the imitator in the runtime. In the first stage, we make a unified network imitate different MEF targets in a style modulation way. In the second stage, we tune the imitator network by optimizing the style code, in order to find an optimal fusion result for each input pair. In the experiment, we construct EMEF from four state-of-the-art MEF methods and then make comparisons with the individuals and several other competitive methods on the latest released MEF benchmark dataset. The promising experimental results demonstrate that our ensemble framework can \"get the best of all worlds\". The code is available at https://github.com/medalwill/EMEF.\n        \u25b3 Less\n      ",
    "title": "EMEF: Ensemble Multi-Exposure Image Fusion",
    "date": "21 May, 2023",
    "authors": [
      "Renshuai Liu",
      " Chengyang Li",
      " Haitao Cao",
      " Yinglin Zheng",
      " Ming Zeng",
      " Xuan Cheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12737",
    "paper_id": "2305.12737",
    "abstract": "\n        Multilingual semantic parsing aims to leverage the knowledge from the high-resource languages to improve low-resource semantic parsing, yet commonly suffers from the data imbalance problem. Prior works propose to utilize the translations by either humans or machines to alleviate such issues. However, human translations are expensive, while machine translations are cheap but prone to error and bias. In this work, we propose an active learning approach that exploits the strengths of both human and machine translations by iteratively adding small batches of human translations into the machine-translated training set. Besides, we propose novel aggregated acquisition criteria that help our active learning method select utterances to be manually translated. Our experiments demonstrate that an ideal utterance selection can significantly reduce the error and bias in the translated data, resulting in higher parser accuracies than the parsers merely trained on the machine-translated data.\n        \u25b3 Less\n      ",
    "title": "The Best of Both Worlds: Combining Human and Machine Translations for Multilingual Semantic Parsing with Active Learning",
    "date": "21 May, 2023",
    "authors": [
      "Zhuang Li",
      " Lizhen Qu",
      " Philip R. Cohen",
      " Raj V. Tumuluri",
      " Gholamreza Haffari"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12738",
    "paper_id": "2305.12738",
    "abstract": "\n        Probabilistic logical rule learning has shown great strength in logical rule mining and knowledge graph completion. It learns logical rules to predict missing edges by reasoning on existing edges in the knowledge graph. However, previous efforts have largely been limited to only modeling chain-like Horn clauses such as R1(x,z)\u2227R2(z,y)\u21d2H(x,y)R_1(x,z)\\land R_2(z,y)\\Rightarrow H(x,y). This formulation overlooks additional contextual information from neighboring sub-graphs of entity variables xx, yy and zz. Intuitively, there is a large gap here, as local sub-graphs have been found to provide important information for knowledge graph completion. Inspired by these observations, we propose Logical Entity RePresentation (LERP) to encode contextual information of entities in the knowledge graph. A LERP is designed as a vector of probabilistic logical functions on the entity's neighboring sub-graph. It is an interpretable representation while allowing for differentiable optimization. We can then incorporate LERP into probabilistic logical rule learning to learn more expressive rules. Empirical results demonstrate that with LERP, our model outperforms other rule learning methods in knowledge graph completion and is comparable or even superior to state-of-the-art black-box methods. Moreover, we find that our model can discover a more expressive family of logical rules. LERP can also be further combined with embedding learning methods like TransE to make it more interpretable.\n        \u25b3 Less\n      ",
    "title": "Logical Entity Representation in Knowledge-Graphs for Differentiable Rule Learning",
    "date": "21 May, 2023",
    "authors": [
      "Chi Han",
      " Qizheng He",
      " Charles Yu",
      " Xinya Du",
      " Hanghang Tong",
      " Heng Ji"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12744",
    "paper_id": "2305.12744",
    "abstract": "\n        Fact-checking real-world claims often requires collecting multiple pieces of evidence and applying complex multi-step reasoning. In this paper, we present Program-Guided Fact-Checking (ProgramFC), a novel fact-checking model that decomposes complex claims into simpler sub-tasks that can be solved using a shared library of specialized functions. We first leverage the in-context learning ability of large language models to generate reasoning programs to guide the verification process. Afterward, we execute the program by delegating each sub-task to the corresponding sub-task handler. This process makes our model both explanatory and data-efficient, providing clear explanations of its reasoning process and requiring minimal training data. We evaluate ProgramFC on two challenging fact-checking datasets and show that it outperforms seven fact-checking baselines across different settings of evidence availability, with explicit output programs that benefit human debugging. Our codes and data are publicly available at https://github.com/mbzuai-nlp/ProgramFC.\n        \u25b3 Less\n      ",
    "title": "Fact-Checking Complex Claims with Program-Guided Reasoning",
    "date": "21 May, 2023",
    "authors": [
      "Liangming Pan",
      " Xiaobao Wu",
      " Xinyuan Lu",
      " Anh Tuan Luu",
      " William Yang Wang",
      " Min-Yen Kan",
      " Preslav Nakov"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12753",
    "paper_id": "2305.12753",
    "abstract": "\n        Query-focused meeting summarization(QFMS) aims to generate a specific summary for the given query according to the meeting transcripts. Due to the conflict between long meetings and limited input size, previous works mainly adopt extract-then-summarize methods, which use extractors to simulate binary labels or ROUGE scores to extract utterances related to the query and then generate a summary. However, the previous approach fails to fully use the comparison between utterances. To the extractor, comparison orders are more important than specific scores. In this paper, we propose a Ranker-Generator framework. It learns to rank the utterances by comparing them in pairs and learning from the global orders, then uses top utterances as the generator's input. We show that learning to rank utterances helps to select utterances related to the query effectively, and the summarizer can benefit from it. Experimental results on QMSum show that the proposed model outperforms all existing multi-stage models with fewer parameters.\n        \u25b3 Less\n      ",
    "title": "Learning to Rank Utterances for Query-Focused Meeting Summarization",
    "date": "21 May, 2023",
    "authors": [
      "Xingxian Liu",
      " Yajing Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12761",
    "paper_id": "2305.12761",
    "abstract": "\n        Cross-lingual natural language inference is a fundamental problem in cross-lingual language understanding. Many recent works have used prompt learning to address the lack of annotated parallel corpora in XNLI. However, these methods adopt discrete prompting by simply translating the templates to the target language and need external expert knowledge to design the templates. Besides, discrete prompts of human-designed template words are not trainable vectors and can not be migrated to target languages in the inference stage flexibly. In this paper, we propose a novel Soft prompt learning framework with the Multilingual Verbalizer (SoftMV) for XNLI. SoftMV first constructs cloze-style question with soft prompts for the input sample. Then we leverage bilingual dictionaries to generate an augmented multilingual question for the original question. SoftMV adopts a multilingual verbalizer to align the representations of original and augmented multilingual questions into the same semantic space with consistency regularization. Experimental results on XNLI demonstrate that SoftMV can achieve state-of-the-art performance and significantly outperform the previous methods under the few-shot and full-shot cross-lingual transfer settings.\n        \u25b3 Less\n      ",
    "title": "Enhancing Cross-lingual Natural Language Inference by Soft Prompting with Multilingual Verbalizer",
    "date": "21 May, 2023",
    "authors": [
      "Shuang Li",
      " Xuming Hu",
      " Aiwei Liu",
      " Yawen Yang",
      " Fukun Ma",
      " Philip S. Yu",
      " Lijie Wen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.01825",
    "paper_id": "2302.01825",
    "abstract": "\n        Human pose estimation is a challenging task due to its structured data sequence nature. Existing methods primarily focus on pair-wise interaction of body joints, which is insufficient for scenarios involving overlapping joints and rapidly changing poses. To overcome these issues, we introduce a novel approach, the High-order Directed Transformer (HDFormer), which leverages high-order bone and joint relationships for improved pose estimation. Specifically, HDFormer incorporates both self-attention and high-order attention to formulate a multi-order attention module. This module facilitates first-order \"joint\u2194\\leftrightarrowjoint\", second-order \"bone\u2194\\leftrightarrowjoint\", and high-order \"hyperbone\u2194\\leftrightarrowjoint\" interactions, effectively addressing issues in complex and occlusion-heavy situations. In addition, modern CNN techniques are integrated into the transformer-based architecture, balancing the trade-off between performance and efficiency. HDFormer significantly outperforms state-of-the-art (SOTA) models on Human3.6M and MPI-INF-3DHP datasets, requiring only 1/10 of the parameters and significantly lower computational costs. Moreover, HDFormer demonstrates broad real-world applicability, enabling real-time, accurate 3D pose estimation. The source code is in https://github.com/hyer/HDFormer\n        \u25b3 Less\n      ",
    "title": "HDFormer: High-order Directed Transformer for 3D Human Pose Estimation",
    "date": "21 May, 2023",
    "authors": [
      "Hanyuan Chen",
      " Jun-Yan He",
      " Wangmeng Xiang",
      " Zhi-Qi Cheng",
      " Wei Liu",
      " Hanbing Liu",
      " Bin Luo",
      " Yifeng Geng",
      " Xuansong Xie"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12768",
    "paper_id": "2305.12768",
    "abstract": "\n        Because implicit user feedback for the collaborative filtering (CF) models is biased toward popular items, CF models tend to yield recommendation lists with popularity bias. Previous studies have utilized inverse propensity weighting (IPW) or causal inference to mitigate this problem. However, they solely employ pointwise or pairwise loss functions and neglect to adopt a contrastive loss function for learning meaningful user and item representations. In this paper, we propose Unbiased ConTrastive Representation Learning (uCTRL), optimizing alignment and uniformity functions derived from the InfoNCE loss function for CF models. Specifically, we formulate an unbiased alignment function used in uCTRL. We also devise a novel IPW estimation method that removes the bias of both users and items. Despite its simplicity, uCTRL equipped with existing CF models consistently outperforms state-of-the-art unbiased recommender models, up to 12.22% for Recall@20 and 16.33% for NDCG@20 gains, on four benchmark datasets.\n        \u25b3 Less\n      ",
    "title": "uCTRL: Unbiased Contrastive Representation Learning via Alignment and Uniformity for Collaborative Filtering",
    "date": "21 May, 2023",
    "authors": [
      "Jae-woong Lee",
      " Seongmin Park",
      " Mincheol Yoon",
      " Jongwuk Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12770",
    "paper_id": "2305.12770",
    "abstract": "\n        Malware detection models based on deep learning have been widely used, but recent research shows that deep learning models are vulnerable to adversarial attacks. Adversarial attacks are to deceive the deep learning model by generating adversarial samples. When adversarial attacks are performed on the malware detection model, the attacker will generate adversarial malware with the same malicious functions as the malware, and make the detection model classify it as benign software. Studying adversarial malware generation can help model designers improve the robustness of malware detection models. At present, in the work on adversarial malware generation for byte-to-image malware detection models, there are mainly problems such as large amount of injection perturbation and low generation efficiency. Therefore, this paper proposes FGAM (Fast Generate Adversarial Malware), a method for fast generating adversarial malware, which iterates perturbed bytes according to the gradient sign to enhance adversarial capability of the perturbed bytes until the adversarial malware is successfully generated. It is experimentally verified that the success rate of the adversarial malware deception model generated by FGAM is increased by about 84\\% compared with existing methods.\n        \u25b3 Less\n      ",
    "title": "FGAM:Fast Adversarial Malware Generation Method Based on Gradient Sign",
    "date": "21 May, 2023",
    "authors": [
      "Kun Li",
      " Fan Zhang",
      " Wei Guo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12775",
    "paper_id": "2305.12775",
    "abstract": "\n        For autonomous driving, radar sensors provide superior reliability regardless of weather conditions as well as a significantly high detection range. State-of-the-art algorithms for environment perception based on radar scans build up on deep neural network architectures that can be costly in terms of memory and computation. By processing radar scans as point clouds, however, an increase in efficiency can be achieved in this respect. While Convolutional Neural Networks show superior performance on pattern recognition of regular data formats like images, the concept of convolutions is not yet fully established in the domain of radar detections represented as point clouds. The main challenge in convolving point clouds lies in their irregular and unordered data format and the associated permutation variance. Therefore, we apply a deep-learning based method introduced by PointCNN that weights and permutes grouped radar detections allowing the resulting permutation invariant cluster to be convolved. In addition, we further adapt this algorithm to radar-specific properties through distance-dependent clustering and pre-processing of input point clouds. Finally, we show that our network outperforms state-of-the-art approaches that are based on PointNet++ on the task of semantic segmentation of radar point clouds.\n        \u25b3 Less\n      ",
    "title": "Semantic Segmentation of Radar Detections using Convolutions on Point Clouds",
    "date": "22 May, 2023",
    "authors": [
      "Marco Braun",
      " Alessandro Cennamo",
      " Markus Schoeler",
      " Kevin Kollek",
      " Anton Kummert"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.14068",
    "paper_id": "2304.14068",
    "abstract": "\n        Deep learning methods are highly accurate, yet their opaque decision process prevents them from earning full human trust. Concept-based models aim to address this issue by learning tasks based on a set of human-understandable concepts. However, state-of-the-art concept-based models rely on high-dimensional concept embedding representations which lack a clear semantic meaning, thus questioning the interpretability of their decision process. To overcome this limitation, we propose the Deep Concept Reasoner (DCR), the first interpretable concept-based model that builds upon concept embeddings. In DCR, neural networks do not make task predictions directly, but they build syntactic rule structures using concept embeddings. DCR then executes these rules on meaningful concept truth degrees to provide a final interpretable and semantically-consistent prediction in a differentiable manner. Our experiments show that DCR: (i) improves up to +25% w.r.t. state-of-the-art interpretable concept-based models on challenging benchmarks (ii) discovers meaningful logic rules matching known ground truths even in the absence of concept supervision during training, and (iii), facilitates the generation of counterfactual examples providing the learnt rules as guidance.\n        \u25b3 Less\n      ",
    "title": "Interpretable Neural-Symbolic Concept Reasoning",
    "date": "22 May, 2023",
    "authors": [
      "Pietro Barbiero",
      " Gabriele Ciravegna",
      " Francesco Giannini",
      " Mateo Espinosa Zarlenga",
      " Lucie Charlotte Magister",
      " Alberto Tonda",
      " Pietro Lio'",
      " Frederic Precioso",
      " Mateja Jamnik",
      " Giuseppe Marra"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12782",
    "paper_id": "2305.12782",
    "abstract": "\n        Generating persona consistent dialogue response is important for developing an intelligent conversational agent. Recent works typically fine-tune large-scale pre-trained models on this task by concatenating persona texts and dialogue history as a single input sequence to generate the target response. While simple and effective, our analysis shows that this popular practice is seriously affected by order sensitivity where different input orders of persona sentences significantly impact the quality and consistency of generated response, resulting in severe performance fluctuations (i.e., 29.4% on GPT2 and 83.2% on BART). To mitigate the order sensitivity problem, we propose a model-agnostic framework, ORder Insensitive Generation (ORIG), which enables dialogue models to learn robust representation under different persona orders and improve the consistency of response generation. Experiments on the Persona-Chat dataset justify the effectiveness and superiority of our method with two dominant pre-trained models (GPT2 and BART).\n        \u25b3 Less\n      ",
    "title": "Towards Robust Personalized Dialogue Generation via Order-Insensitive Representation Regularization",
    "date": "22 May, 2023",
    "authors": [
      "Liang Chen",
      " Hongru Wang",
      " Yang Deng",
      " Wai-Chung Kwan",
      " Zezhong Wang",
      " Kam-Fai Wong"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.01073",
    "paper_id": "2302.01073",
    "abstract": "\n        Repeated games consider a situation where multiple agents are motivated by their independent rewards throughout learning. In general, the dynamics of their learning become complex. Especially when their rewards compete with each other like zero-sum games, the dynamics often do not converge to their optimum, i.e., the Nash equilibrium. To tackle such complexity, many studies have understood various learning algorithms as dynamical systems and discovered qualitative insights among the algorithms. However, such studies have yet to handle multi-memory games (where agents can memorize actions they played in the past and choose their actions based on their memories), even though memorization plays a pivotal role in artificial intelligence and interpersonal relationship. This study extends two major learning algorithms in games, i.e., replicator dynamics and gradient ascent, into multi-memory games. Then, we prove their dynamics are identical. Furthermore, theoretically and experimentally, we clarify that the learning dynamics diverge from the Nash equilibrium in multi-memory zero-sum games and reach heteroclinic cycles (sojourn longer around the boundary of the strategy space), providing a fundamental advance in learning in games.\n        \u25b3 Less\n      ",
    "title": "Learning in Multi-Memory Games Triggers Complex Dynamics Diverging from Nash Equilibrium",
    "date": "22 May, 2023",
    "authors": [
      "Yuma Fujimoto",
      " Kaito Ariu",
      " Kenshi Abe"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12792",
    "paper_id": "2305.12792",
    "abstract": "\n        Event Causality Identification (ECI) aims to identify causal relations between events in unstructured texts. This is a very challenging task, because causal relations are usually expressed by implicit associations between events. Existing methods usually capture such associations by directly modeling the texts with pre-trained language models, which underestimate two kinds of semantic structures vital to the ECI task, namely, event-centric structure and event-associated structure. The former includes important semantic elements related to the events to describe them more precisely, while the latter contains semantic paths between two events to provide possible supports for ECI. In this paper, we study the implicit associations between events by modeling the above explicit semantic structures, and propose a Semantic Structure Integration model (SemSIn). It utilizes a GNN-based event aggregator to integrate the event-centric structure information, and employs an LSTM-based path aggregator to capture the event-associated structure information between two events. Experimental results on three widely used datasets show that SemSIn achieves significant improvements over baseline methods.\n        \u25b3 Less\n      ",
    "title": "Semantic Structure Enhanced Event Causality Identification",
    "date": "22 May, 2023",
    "authors": [
      "Zhilei Hu",
      " Zixuan Li",
      " Xiaolong Jin",
      " Long Bai",
      " Saiping Guan",
      " Jiafeng Guo",
      " Xueqi Cheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12798",
    "paper_id": "2305.12798",
    "abstract": "\n        In recent years, large language models (LMs) have achieved remarkable progress across various natural language processing tasks. As pre-training and fine-tuning are costly and might negatively impact model performance, it is desired to efficiently adapt an existing model to different conditions such as styles, sentiments or narratives, when facing different audiences or scenarios. However, efficient adaptation of a language model to diverse conditions remains an open challenge. This work is inspired by the observation that text conditions are often associated with selection of certain words in a context. Therefore we introduce LM-Switch, a theoretically grounded, lightweight and simple method for generative language model conditioning. We begin by investigating the effect of conditions in Hidden Markov Models (HMMs), and establish a theoretical connection with language model. Our finding suggests that condition shifts in HMMs are associated with linear transformations in word embeddings. LM-Switch is then designed to deploy a learnable linear factor in the word embedding space for language model conditioning. We show that LM-Switch can model diverse tasks, and achieves comparable or better performance compared with state-of-the-art baselines in LM detoxification and generation control, despite requiring no more than 1% of parameters compared with baselines and little extra time overhead compared with base LMs. It is also able to learn from as few as a few sentences or one document. Moreover, a learned LM-Switch can be transferred to other LMs of different sizes, achieving a detoxification performance similar to the best baseline. We will make our code available to the research community following publication.\n        \u25b3 Less\n      ",
    "title": "LM-Switch: Lightweight Language Model Conditioning in Word Embedding Space",
    "date": "22 May, 2023",
    "authors": [
      "Chi Han",
      " Jialiang Xu",
      " Manling Li",
      " Yi Fung",
      " Chenkai Sun",
      " Nan Jiang",
      " Tarek Abdelzaher",
      " Heng Ji"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12802",
    "paper_id": "2305.12802",
    "abstract": "\n        Ultra-fine entity typing (UFET) is the task of inferring the semantic types, from a large set of fine-grained candidates, that apply to a given entity mention. This task is especially challenging because we only have a small number of training examples for many of the types, even with distant supervision strategies. State-of-the-art models, therefore, have to rely on prior knowledge about the type labels in some way. In this paper, we show that the performance of existing methods can be improved using a simple technique: we use pre-trained label embeddings to cluster the labels into semantic domains and then treat these domains as additional types. We show that this strategy consistently leads to improved results, as long as high-quality label embeddings are used. We furthermore use the label clusters as part of a simple post-processing technique, which results in further performance gains. Both strategies treat the UFET model as a black box and can thus straightforwardly be used to improve a wide range of existing models.\n        \u25b3 Less\n      ",
    "title": "Ultra-Fine Entity Typing with Prior Knowledge about Labels: A Simple Clustering Based Strategy",
    "date": "22 May, 2023",
    "authors": [
      "Na Li",
      " Zied Bouraoui",
      " Steven Schockaert"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12821",
    "paper_id": "2305.12821",
    "abstract": "\n        Reinforcement learning (RL), imitation learning (IL), and task and motion planning (TAMP) have demonstrated impressive performance across various robotic manipulation tasks. However, these approaches have been limited to learning simple behaviors in current real-world manipulation benchmarks, such as pushing or pick-and-place. To enable more complex, long-horizon behaviors of an autonomous robot, we propose to focus on real-world furniture assembly, a complex, long-horizon robot manipulation task that requires addressing many current robotic manipulation challenges to solve. We present FurnitureBench, a reproducible real-world furniture assembly benchmark aimed at providing a low barrier for entry and being easily reproducible, so that researchers across the world can reliably test their algorithms and compare them against prior work. For ease of use, we provide 200+ hours of pre-collected data (5000+ demonstrations), 3D printable furniture models, a robotic environment setup guide, and systematic task initialization. Furthermore, we provide FurnitureSim, a fast and realistic simulator of FurnitureBench. We benchmark the performance of offline RL and IL algorithms on our assembly tasks and demonstrate the need to improve such algorithms to be able to solve our tasks in the real world, providing ample opportunities for future research.\n        \u25b3 Less\n      ",
    "title": "FurnitureBench: Reproducible Real-World Benchmark for Long-Horizon Complex Manipulation",
    "date": "22 May, 2023",
    "authors": [
      "Minho Heo",
      " Youngwoon Lee",
      " Doohyun Lee",
      " Joseph J. Lim"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12835",
    "paper_id": "2305.12835",
    "abstract": "\n        Researchers have proposed various information extraction (IE) techniques to convert news articles into structured knowledge for news understanding. However, none of the existing methods have explicitly addressed the issue of framing bias that is inherent in news articles. We argue that studying and identifying framing bias is a crucial step towards trustworthy event understanding. We propose a novel task, neutral event graph induction, to address this problem. An event graph is a network of events and their temporal relations. Our task aims to induce such structural knowledge with minimal framing bias in an open domain. We propose a three-step framework to induce a neutral event graph from multiple input sources. The process starts by inducing an event graph from each input source, then merging them into one merged event graph, and lastly using a Graph Convolutional Network to remove event nodes with biased connotations. We demonstrate the effectiveness of our framework through the use of graph prediction metrics and bias-focused metrics.\n        \u25b3 Less\n      ",
    "title": "Open-Domain Event Graph Induction for Mitigating Framing Bias",
    "date": "22 May, 2023",
    "authors": [
      "Siyi Liu",
      " Hongming Zhang",
      " Hongwei Wang",
      " Kaiqiang Song",
      " Dan Roth",
      " Dong Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2110.02442",
    "paper_id": "2110.02442",
    "abstract": "\n        Transformer-based models have achieved great success in various NLP, vision, and speech tasks. However, the core of Transformer, the self-attention mechanism, has a quadratic time and memory complexity with respect to the sequence length, which hinders applications of Transformer-based models to long sequences. Many approaches have been proposed to mitigate this problem, such as sparse attention mechanisms, low-rank matrix approximations and scalable kernels, and token mixing alternatives to self-attention. We propose a novel Pooling Network (PoNet) for token mixing in long sequences with linear complexity. We design multi-granularity pooling and pooling fusion to capture different levels of contextual information and combine their interactions with tokens. On the Long Range Arena benchmark, PoNet significantly outperforms Transformer and achieves competitive accuracy, while being only slightly slower than the fastest model, FNet, across all sequence lengths measured on GPUs. We also conduct systematic studies on the transfer learning capability of PoNet and observe that PoNet achieves 95.7% of the accuracy of BERT on the GLUE benchmark, outperforming FNet by 4.5% relative. Comprehensive ablation analysis demonstrates effectiveness of the designed multi-granularity pooling and pooling fusion for token mixing in long sequences and efficacy of the designed pre-training tasks for PoNet to learn transferable contextualized language representations.\n        \u25b3 Less\n      ",
    "title": "PoNet: Pooling Network for Efficient Token Mixing in Long Sequences",
    "date": "22 May, 2023",
    "authors": [
      "Chao-Hong Tan",
      " Qian Chen",
      " Wen Wang",
      " Qinglin Zhang",
      " Siqi Zheng",
      " Zhen-Hua Ling"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12865",
    "paper_id": "2305.12865",
    "abstract": "\n        To support software developers in understanding and maintaining programs, various automatic code summarization techniques have been proposed to generate a concise natural language comment for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of natural language processing tasks. Among them, ChatGPT is the most popular one which has attracted wide attention from the software engineering community. However, it still remains unclear how ChatGPT performs in (automatic) code summarization. Therefore, in this paper, we focus on evaluating ChatGPT on a widely-used Python dataset called CSN-Python and comparing it with several state-of-the-art (SOTA) code summarization models. Specifically, we first explore an appropriate prompt to guide ChatGPT to generate in-distribution comments. Then, we use such a prompt to ask ChatGPT to generate comments for all code snippets in the CSN-Python test set. We adopt three widely-used metrics (including BLEU, METEOR, and ROUGE-L) to measure the quality of the comments generated by ChatGPT and SOTA models (including NCS, CodeBERT, and CodeT5). The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models. We also present some cases and discuss the advantages and disadvantages of ChatGPT in code summarization. Based on the findings, we outline several open challenges and opportunities in ChatGPT-based code summarization.\n        \u25b3 Less\n      ",
    "title": "Automatic Code Summarization via ChatGPT: How Far Are We?",
    "date": "22 May, 2023",
    "authors": [
      "Weisong Sun",
      " Chunrong Fang",
      " Yudu You",
      " Yun Miao",
      " Yi Liu",
      " Yuekang Li",
      " Gelei Deng",
      " Shenghan Huang",
      " Yuchen Chen",
      " Quanjun Zhang",
      " Hanwei Qian",
      " Yang Liu",
      " Zhenyu Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12868",
    "paper_id": "2305.12868",
    "abstract": "\n        Developing digital sound synthesizers is crucial to the music industry as it provides a low-cost way to produce high-quality sounds with rich timbres. Existing traditional synthesizers often require substantial expertise to determine the overall framework of a synthesizer and the parameters of submodules. Since expert knowledge is hard to acquire, it hinders the flexibility to quickly design and tune digital synthesizers for diverse sounds. In this paper, we propose ``NAS-FM'', which adopts neural architecture search (NAS) to build a differentiable frequency modulation (FM) synthesizer. Tunable synthesizers with interpretable controls can be developed automatically from sounds without any prior expert knowledge and manual operating costs. In detail, we train a supernet with a specifically designed search space, including predicting the envelopes of carriers and modulators with different frequency ratios. An evolutionary search algorithm with adaptive oscillator size is then developed to find the optimal relationship between oscillators and the frequency ratio of FM. Extensive experiments on recordings of different instrument sounds show that our algorithm can build a synthesizer fully automatically, achieving better results than handcrafted synthesizers. Audio samples are available at https://nas-fm.github.io/.\n        \u25b3 Less\n      ",
    "title": "NAS-FM: Neural Architecture Search for Tunable and Interpretable Sound Synthesis based on Frequency Modulation",
    "date": "22 May, 2023",
    "authors": [
      "Zhen Ye",
      " Wei Xue",
      " Xu Tan",
      " Qifeng Liu",
      " Yike Guo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12875",
    "paper_id": "2305.12875",
    "abstract": "\n        Memristor-based neural networks provide an exceptional energy-efficient platform for artificial intelligence (AI), presenting the possibility of self-powered operation when paired with energy harvesters. However, most memristor-based networks rely on analog in-memory computing, necessitating a stable and precise power supply, which is incompatible with the inherently unstable and unreliable energy harvesters. In this work, we fabricated a robust binarized neural network comprising 32,768 memristors, powered by a miniature wide-bandgap solar cell optimized for edge applications. Our circuit employs a resilient digital near-memory computing approach, featuring complementarily programmed memristors and logic-in-sense-amplifier. This design eliminates the need for compensation or calibration, operating effectively under diverse conditions. Under high illumination, the circuit achieves inference performance comparable to that of a lab bench power supply. In low illumination scenarios, it remains functional with slightly reduced accuracy, seamlessly transitioning to an approximate computing mode. Through image classification neural network simulations, we demonstrate that misclassified images under low illumination are primarily difficult-to-classify cases. Our approach lays the groundwork for self-powered AI and the creation of intelligent sensors for various applications in health, safety, and environment monitoring.\n        \u25b3 Less\n      ",
    "title": "Powering AI at the Edge: A Robust, Memristor-based Binarized Neural Network with Near-Memory Computing and Miniaturized Solar Cell",
    "date": "22 May, 2023",
    "authors": [
      "Fadi Jebali",
      " Atreya Majumdar",
      " Cl\u00e9ment Turck",
      " Kamel-Eddine Harabi",
      " Mathieu-Coumba Faye",
      " Eloi Muhr",
      " Jean-Pierre Walder",
      " Oleksandr Bilousov",
      " Amadeo Michaud",
      " Elisa Vianello",
      " Tifenn Hirtzlin",
      " Fran\u00e7ois Andrieu",
      " Marc Bocquet",
      " St\u00e9phane Collin",
      " Damien Querlioz",
      " Jean-Michel Portal"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12886",
    "paper_id": "2305.12886",
    "abstract": "\n        State-of-the-art sensorimotor learning algorithms offer policies that can often produce unstable behaviors, damaging the robot and/or the environment. Traditional robot learning, on the contrary, relies on dynamical system-based policies that can be analyzed for stability/safety. Such policies, however, are neither flexible nor generic and usually work only with proprioceptive sensor states. In this work, we bridge the gap between generic neural network policies and dynamical system-based policies, and we introduce Autonomous Neural Dynamic Policies (ANDPs) that: (a) are based on autonomous dynamical systems, (b) always produce asymptotically stable behaviors, and (c) are more flexible than traditional stable dynamical system-based policies. ANDPs are fully differentiable, flexible generic-policies that can be used in imitation learning setups while ensuring asymptotic stability. In this paper, we explore the flexibility and capacity of ANDPs in several imitation learning tasks including experiments with image observations. The results show that ANDPs combine the benefits of both neural network-based and dynamical system-based methods.\n        \u25b3 Less\n      ",
    "title": "End-to-End Stable Imitation Learning via Autonomous Neural Dynamic Policies",
    "date": "22 May, 2023",
    "authors": [
      "Dionis Totsila",
      " Konstantinos Chatzilygeroudis",
      " Denis Hadjivelichkov",
      " Valerio Modugno",
      " Ioannis Hatzilygeroudis",
      " Dimitrios Kanoulas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12887",
    "paper_id": "2305.12887",
    "abstract": "\n        In this study, we address the importance of modeling behavior style in virtual agents for personalized human-agent interaction. We propose a machine learning approach to synthesize gestures, driven by prosodic features and text, in the style of different speakers, even those unseen during training. Our model incorporates zero-shot multimodal style transfer using multimodal data from the PATS database, which contains videos of diverse speakers. We recognize style as a pervasive element during speech, influencing the expressivity of communicative behaviors, while content is conveyed through multimodal signals and text. By disentangling content and style, we directly infer the style embedding, even for speakers not included in the training phase, without the need for additional training or fine-tuning. Objective and subjective evaluations are conducted to validate our approach and compare it against two baseline methods.\n        \u25b3 Less\n      ",
    "title": "ZS-MSTM: Zero-Shot Style Transfer for Gesture Animation driven by Text and Speech using Adversarial Disentanglement of Multimodal Style Encoding",
    "date": "22 May, 2023",
    "authors": [
      "Mireille Fares",
      " Catherine Pelachaud",
      " Nicolas Obin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12907",
    "paper_id": "2305.12907",
    "abstract": "\n        Large language models have shown tremendous performance in a variety of tasks. In-context learning -- the ability to improve at a task after being provided with a number of demonstrations -- is seen as one of the main contributors to their success. In the present paper, we demonstrate that the in-context learning abilities of large language models can be recursively improved via in-context learning itself. We coin this phenomenon meta-in-context learning. Looking at two idealized domains, a one-dimensional regression task and a two-armed bandit task, we show that meta-in-context learning adaptively reshapes a large language model's priors over expected tasks. Furthermore, we find that meta-in-context learning modifies the in-context learning strategies of such models. Finally, we extend our approach to a benchmark of real-world regression problems where we observe competitive performance to traditional learning algorithms. Taken together, our work improves our understanding of in-context learning and paves the way toward adapting large language models to the environment they are applied purely through meta-in-context learning rather than traditional finetuning.\n        \u25b3 Less\n      ",
    "title": "Meta-in-context learning in large language models",
    "date": "22 May, 2023",
    "authors": [
      "Julian Coda-Forno",
      " Marcel Binz",
      " Zeynep Akata",
      " Matthew Botvinick",
      " Jane X. Wang",
      " Eric Schulz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12914",
    "paper_id": "2305.12914",
    "abstract": "\n        In-memory computing for Machine Learning (ML) applications remedies the von Neumann bottlenecks by organizing computation to exploit parallelism and locality. Non-volatile memory devices such as Resistive RAM (ReRAM) offer integrated switching and storage capabilities showing promising performance for ML applications. However, ReRAM devices have design challenges, such as non-linear digital-analog conversion and circuit overheads. This paper proposes an In-Memory Boolean-to-Current Inference Architecture (IMBUE) that uses ReRAM-transistor cells to eliminate the need for such conversions. IMBUE processes Boolean feature inputs expressed as digital voltages and generates parallel current paths based on resistive memory states. The proportional column current is then translated back to the Boolean domain for further digital processing. The IMBUE architecture is inspired by the Tsetlin Machine (TM), an emerging ML algorithm based on intrinsically Boolean logic. The IMBUE architecture demonstrates significant performance improvements over binarized convolutional neural networks and digital TM in-memory implementations, achieving up to a 12.99x and 5.28x increase, respectively.\n        \u25b3 Less\n      ",
    "title": "IMBUE: In-Memory Boolean-to-CUrrent Inference ArchitecturE for Tsetlin Machines",
    "date": "22 May, 2023",
    "authors": [
      "Omar Ghazal",
      " Simranjeet Singh",
      " Tousif Rahman",
      " Shengqi Yu",
      " Yujin Zheng",
      " Domenico Balsamo",
      " Sachin Patkar",
      " Farhad Merchant",
      " Fei Xia",
      " Alex Yakovlev",
      " Rishad Shafik"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.00506",
    "paper_id": "2212.00506",
    "abstract": "\n        In cooperative Multi-Agent Planning (MAP), a set of goals has to be achieved by a set of agents. Independently of whether they perform a pre-assignment of goals to agents or they directly search for a solution without any goal assignment, most previous works did not focus on a fair distribution/achievement of goals by agents. This paper adapts well-known fairness schemes to MAP, and introduces two novel approaches to generate cost-aware fair plans. The first one solves an optimization problem to pre-assign goals to agents, and then solves a centralized MAP task using that assignment. The second one consists of a planning-based compilation that allows solving the joint problem of goal assignment and planning while taking into account the given fairness scheme. Empirical results in several standard MAP benchmarks show that these approaches outperform different baselines. They also show that there is no need to sacrifice much plan cost to generate fair plans.\n        \u25b3 Less\n      ",
    "title": "Fairness in Multi-Agent Planning",
    "date": "22 May, 2023",
    "authors": [
      "Alberto Pozanco",
      " Daniel Borrajo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14382",
    "paper_id": "2305.14382",
    "abstract": "\n        Applications of deep learning in financial market prediction has attracted huge attention from investors and researchers. In particular, intra-day prediction at the minute scale, the dramatically fluctuating volume and stock prices within short time periods have posed a great challenge for the convergence of networks result. Informer is a more novel network, improved on Transformer with smaller computational complexity, longer prediction length and global time stamp features. We have designed three experiments to compare Informer with the commonly used networks LSTM, Transformer and BERT on 1-minute and 5-minute frequencies for four different stocks/ market indices. The prediction results are measured by three evaluation criteria: MAE, RMSE and MAPE. Informer has obtained best performance among all the networks on every dataset. Network without the global time stamp mechanism has significantly lower prediction effect compared to the complete Informer; it is evident that this mechanism grants the time series to the characteristics and substantially improves the prediction accuracy of the networks. Finally, transfer learning capability experiment is conducted, Informer also achieves a good performance. Informer has good robustness and improved performance in market prediction, which can be exactly adapted to real trading.\n        \u25b3 Less\n      ",
    "title": "Stock and market index prediction using Informer network",
    "date": "22 May, 2023",
    "authors": [
      "Yuze Lu",
      " Hailong Zhang",
      " Qiwen Guo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12918",
    "paper_id": "2305.12918",
    "abstract": "\n        We introduce Parallel Paraphrasing (Paraboth\\text{Para}_\\text{both}), an augmentation method for translation metrics making use of automatic paraphrasing of both the reference and hypothesis. This method counteracts the typically misleading results of speech translation metrics such as WER, CER, and BLEU if only a single reference is available. We introduce two new datasets explicitly created to measure the quality of metrics intended to be applied to Swiss German speech-to-text systems. Based on these datasets, we show that we are able to significantly improve the correlation with human quality perception if our method is applied to commonly used metrics.\n        \u25b3 Less\n      ",
    "title": "Improving Metrics for Speech Translation",
    "date": "22 May, 2023",
    "authors": [
      "Claudio Paonessa",
      " Dominik Frefel",
      " Manfred Vogel"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12926",
    "paper_id": "2305.12926",
    "abstract": "\n        We show that SCL(FOL) can simulate the derivation of non-redundant clauses by superposition for first-order logic without equality. Superposition-based reasoning is performed with respect to a fixed reduction ordering. The completeness proof of superposition relies on the grounding of the clause set. It builds a ground partial model according to the fixed ordering, where minimal false ground instances of clauses then trigger non-redundant superposition inferences. We define a respective strategy for the SCL calculus such that clauses learned by SCL and superposition inferences coincide. From this perspective the SCL calculus can be viewed as a generalization of the superposition calculus.\n        \u25b3 Less\n      ",
    "title": "SCL(FOL) Can Simulate Non-Redundant Superposition Clause Learning",
    "date": "22 May, 2023",
    "authors": [
      "Martin Bromberger",
      " Chaahat Jain",
      " Christoph Weidenbach"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07605",
    "paper_id": "2305.07605",
    "abstract": "\n        The launch of ChatGPT in November 2022 precipitated a panic among some educators while prompting qualified enthusiasm from others. Under the umbrella term Generative AI, ChatGPT is an example of a range of technologies for the delivery of computer-generated text, image, and other digitized media. This paper examines the implications for education of one generative AI technology, chatbots responding from large language models, or C-LLM. It reports on an application of a C-LLM to AI review and assessment of complex student work. In a concluding discussion, the paper explores the intrinsic limits of generative AI, bound as it is to language corpora and their textual representation through binary notation. Within these limits, we suggest the range of emerging and potential applications of Generative AI in education.\n        \u25b3 Less\n      ",
    "title": "Generative AI: Implications and Applications for Education",
    "date": "22 May, 2023",
    "authors": [
      "Anastasia Olga",
      " Tzirides",
      " Akash Saini",
      " Gabriela Zapata",
      " Duane Searsmith",
      " Bill Cope",
      " Mary Kalantzis",
      " Vania Castro",
      " Theodora Kourkoulou",
      " John Jones",
      " Rodrigo Abrantes da Silva",
      " Jen Whiting",
      " Nikoleta Polyxeni Kastania"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.08073",
    "paper_id": "2211.08073",
    "abstract": "\n        Pre-trained language models (PLMs) are known to improve the generalization performance of natural language understanding models by leveraging large amounts of data during the pre-training phase. However, the out-of-distribution (OOD) generalization problem remains a challenge in many NLP tasks, limiting the real-world deployment of these methods. This paper presents the first attempt at creating a unified benchmark named GLUE-X for evaluating OOD robustness in NLP models, highlighting the importance of OOD robustness and providing insights on how to measure the robustness of a model and how to improve it. The benchmark includes 13 publicly available datasets for OOD testing, and evaluations are conducted on 8 classic NLP tasks over 21 popularly used PLMs, including GPT-3 and GPT-3.5. Our findings confirm the need for improved OOD accuracy in NLP tasks, as significant performance degradation was observed in all settings compared to in-distribution (ID) accuracy.\n        \u25b3 Less\n      ",
    "title": "GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-distribution Generalization Perspective",
    "date": "22 May, 2023",
    "authors": [
      "Linyi Yang",
      " Shuibai Zhang",
      " Libo Qin",
      " Yafu Li",
      " Yidong Wang",
      " Hanmeng Liu",
      " Jindong Wang",
      " Xing Xie",
      " Yue Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12960",
    "paper_id": "2305.12960",
    "abstract": "\n        The backpropagation algorithm, despite its widespread use in neural network learning, may not accurately emulate the human cortex's learning process. Alternative strategies, such as the Forward-Forward Algorithm (FFA), offer a closer match to the human cortex's learning characteristics. However, the original FFA paper and related works on the Forward-Forward Algorithm only mentioned very limited types of neural network mechanisms and may limit its application and effectiveness. In response to these challenges, we propose an integrated method that combines the strengths of both FFA and shallow backpropagation, yielding a biologically plausible neural network training algorithm which can also be applied to various network structures. We applied this integrated approach to the classification of the Modified National Institute of Standards and Technology (MNIST) database, where it outperformed FFA and demonstrated superior resilience to noise compared to backpropagation. We show that training neural networks with the Integrated Forward-Forward Algorithm has the potential of generating neural networks with advantageous features like robustness.\n        \u25b3 Less\n      ",
    "title": "The Integrated Forward-Forward Algorithm: Integrating Forward-Forward and Shallow Backpropagation With Local Losses",
    "date": "22 May, 2023",
    "authors": [
      "Desmond Y. M. Tang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.11168",
    "paper_id": "2301.11168",
    "abstract": "\n        In reinforcement learning (RL) with experience replay, experiences stored in a replay buffer influence the RL agent's performance. Information about the influence is valuable for various purposes, including experience cleansing and analysis. One method for estimating the influence of individual experiences is agent comparison, but it is prohibitively expensive when there is a large number of experiences. In this paper, we present PI+ToD as a method for efficiently estimating the influence of experiences. PI+ToD is a policy iteration that efficiently estimates the influence of experiences by utilizing turn-over dropout. We demonstrate the efficiency of PI+ToD with experiments in MuJoCo environments.\n        \u25b3 Less\n      ",
    "title": "Which Experiences Are Influential for Your Agent? Policy Iteration with Turn-over Dropout",
    "date": "22 May, 2023",
    "authors": [
      "Takuya Hiraoka",
      " Takashi Onishi",
      " Yoshimasa Tsuruoka"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12995",
    "paper_id": "2305.12995",
    "abstract": "\n        Understanding the internal reasoning behind the predictions of machine learning systems is increasingly vital, given their rising adoption and acceptance. While previous approaches, such as LIME, generate algorithmic explanations by attributing importance to input features for individual examples, recent research indicates that practitioners prefer examining language explanations that explain sub-groups of examples. In this paper, we introduce MaNtLE, a model-agnostic natural language explainer that analyzes multiple classifier predictions and generates faithful natural language explanations of classifier rationale for structured classification tasks. MaNtLE uses multi-task training on thousands of synthetic classification tasks to generate faithful explanations. Simulated user studies indicate that, on average, MaNtLE-generated explanations are at least 11% more faithful compared to LIME and Anchors explanations across three tasks. Human evaluations demonstrate that users can better predict model behavior using explanations from MaNtLE compared to other techniques\n        \u25b3 Less\n      ",
    "title": "MaNtLE: Model-agnostic Natural Language Explainer",
    "date": "22 May, 2023",
    "authors": [
      "Rakesh R. Menon",
      " Kerem Zaman",
      " Shashank Srivastava"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.10308",
    "paper_id": "2305.10308",
    "abstract": "\n        Tabular data is the most widely used data format in machine learning (ML). While tree-based methods outperform DL-based methods in supervised learning, recent literature reports that self-supervised learning with Transformer-based models outperforms tree-based methods. In the existing literature on self-supervised learning for tabular data, contrastive learning is the predominant method. In contrastive learning, data augmentation is important to generate different views. However, data augmentation for tabular data has been difficult due to the unique structure and high complexity of tabular data. In addition, three main components are proposed together in existing methods: model structure, self-supervised learning methods, and data augmentation. Therefore, previous works have compared the performance without comprehensively considering these components, and it is not clear how each component affects the actual performance.\n  In this study, we focus on data augmentation to address these issues. We propose a novel data augmentation method, \\textbf{M}\\textbf{M}ask \\textbf{T}\\textbf{T}oken \\textbf{R}\\textbf{R}eplacement (\\texttt{MTR}\\texttt{MTR}), which replaces the mask token with a portion of each tokenized column; \\texttt{MTR}\\texttt{MTR} takes advantage of the properties of Transformer, which is becoming the predominant DL-based architecture for tabular data, to perform data augmentation for each column embedding. Through experiments with 13 diverse public datasets in both supervised and self-supervised learning scenarios, we show that \\texttt{MTR}\\texttt{MTR} achieves competitive performance against existing data augmentation methods and improves model performance. In addition, we discuss specific scenarios in which \\texttt{MTR}\\texttt{MTR} is most effective and identify the scope of its application. The code is available at https://github.com/somaonishi/MTR/.\n        \u25b3 Less\n      ",
    "title": "Rethinking Data Augmentation for Tabular Data in Deep Learning",
    "date": "22 May, 2023",
    "authors": [
      "Soma Onishi",
      " Shoya Meguro"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.06387",
    "paper_id": "2301.06387",
    "abstract": "\n        Zero-shot human-AI coordination holds the promise of collaborating with humans without human data. Prevailing methods try to train the ego agent with a population of partners via self-play. However, these methods suffer from two problems: 1) The diversity of a population with finite partners is limited, thereby limiting the capacity of the trained ego agent to collaborate with a novel human; 2) Current methods only provide a common best response for every partner in the population, which may result in poor zero-shot coordination performance with a novel partner or humans. To address these issues, we first propose the policy ensemble method to increase the diversity of partners in the population, and then develop a context-aware method enabling the ego agent to analyze and identify the partner's potential policy primitives so that it can take different actions accordingly. In this way, the ego agent is able to learn more universal cooperative behaviors for collaborating with diverse partners. We conduct experiments on the Overcooked environment, and evaluate the zero-shot human-AI coordination performance of our method with both behavior-cloned human proxies and real humans. The results demonstrate that our method significantly increases the diversity of partners and enables ego agents to learn more diverse behaviors than baselines, thus achieving state-of-the-art performance in all scenarios. We also open-source a human-AI coordination study framework on the Overcooked for the convenience of future studies.\n        \u25b3 Less\n      ",
    "title": "PECAN: Leveraging Policy Ensemble for Context-Aware Zero-Shot Human-AI Coordination",
    "date": "22 May, 2023",
    "authors": [
      "Xingzhou Lou",
      " Jiaxian Guo",
      " Junge Zhang",
      " Jun Wang",
      " Kaiqi Huang",
      " Yali Du"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13002",
    "paper_id": "2305.13002",
    "abstract": "\n        Semi-supervised learning (SSL) is a popular setting aiming to effectively utilize unlabelled data to improve model performance in downstream natural language processing (NLP) tasks. Currently, there are two popular approaches to make use of unlabelled data: Self-training (ST) and Task-adaptive pre-training (TAPT). ST uses a teacher model to assign pseudo-labels to the unlabelled data, while TAPT continues pre-training on the unlabelled data before fine-tuning. To the best of our knowledge, the effectiveness of TAPT in SSL tasks has not been systematically studied, and no previous work has directly compared TAPT and ST in terms of their ability to utilize the pool of unlabelled data. In this paper, we provide an extensive empirical study comparing five state-of-the-art ST approaches and TAPT across various NLP tasks and data sizes, including in- and out-of-domain settings. Surprisingly, we find that TAPT is a strong and more robust SSL learner, even when using just a few hundred unlabelled samples or in the presence of domain shifts, compared to more sophisticated ST approaches, and tends to bring greater improvements in SSL than in fully-supervised settings. Our further analysis demonstrates the risks of using ST approaches when the size of labelled or unlabelled data is small or when domain shifts exist. We offer a fresh perspective for future SSL research, suggesting the use of unsupervised pre-training objectives over dependency on pseudo labels.\n        \u25b3 Less\n      ",
    "title": "Rethinking Semi-supervised Learning with Language Models",
    "date": "22 May, 2023",
    "authors": [
      "Zhengxiang Shi",
      " Francesco Tonolini",
      " Nikolaos Aletras",
      " Emine Yilmaz",
      " Gabriella Kazai",
      " Yunlong Jiao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13019",
    "paper_id": "2305.13019",
    "abstract": "\n        This paper introduces ELUA, the Ecological Laboratory for Urban Agriculture, a collaboration among landscape architects, architects and computer scientists who specialize in artificial intelligence, robotics and computer vision. ELUA has two gantry robots, one indoors and the other outside on the rooftop of a 6-story campus building. Each robot can seed, water, weed, and prune in its garden. To support responsive landscape research, ELUA also includes sensor arrays, an AI-powered camera, and an extensive network infrastructure. This project demonstrates a way to integrate artificial intelligence into an evolving urban ecosystem, and encourages landscape architects to develop an adaptive design framework where design becomes a long-term engagement with the environment.\n        \u25b3 Less\n      ",
    "title": "Robots in the Garden: Artificial Intelligence and Adaptive Landscapes",
    "date": "22 May, 2023",
    "authors": [
      "Zihao Zhang",
      " Susan L. Epstein",
      " Casey Breen",
      " Sophia Xia",
      " Zhigang Zhu",
      " Christian Volkmann"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13046",
    "paper_id": "2305.13046",
    "abstract": "\n        Handling out-of-distribution samples is a long-lasting challenge for deep visual models. In particular, domain generalization (DG) is one of the most relevant tasks that aims to train a model with a generalization capability on novel domains. Most existing DG approaches share the same philosophy to minimize the discrepancy between domains by finding the domain-invariant representations. On the contrary, our proposed method called POEM acquires a strong DG capability by learning domain-invariant and domain-specific representations and polarizing them. Specifically, POEM cotrains category-classifying and domain-classifying embeddings while regularizing them to be orthogonal via minimizing the cosine-similarity between their features, i.e., the polarization of embeddings. The clear separation of embeddings suppresses domain-specific features in the domain-invariant embeddings. The concept of POEM shows a unique direction to enhance the domain robustness of representations that brings considerable and consistent performance gains when combined with existing DG methods. Extensive simulation results in popular DG benchmarks with the PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet datasets show that POEM indeed facilitates the category-classifying embedding to be more domain-invariant.\n        \u25b3 Less\n      ",
    "title": "POEM: Polarization of Embeddings for Domain-Invariant Representations",
    "date": "22 May, 2023",
    "authors": [
      "Sang-Yeong Jo",
      " Sung Whan Yoon"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.15368",
    "paper_id": "2210.15368",
    "abstract": "\n        The lack of clean speech is a practical challenge to the development of speech enhancement systems, which means that there is an inevitable mismatch between their training criterion and evaluation metric. In response to this unfavorable situation, we propose a training and inference strategy that additionally uses enhanced speech as a target by improving the previously proposed noisy-target training (NyTT). Because homogeneity between in-domain noise and extraneous noise is the key to the effectiveness of NyTT, we train various student models by remixing 1) the teacher model's estimated speech and noise for enhanced-target training or 2) raw noisy speech and the teacher model's estimated noise for noisy-target training. Experimental results show that our proposed method outperforms several baselines, especially with the teacher/student inference, where predicted clean speech is derived successively through the teacher and final student models.\n        \u25b3 Less\n      ",
    "title": "A Training and Inference Strategy Using Noisy and Enhanced Speech as Target for Speech Enhancement without Clean Speech",
    "date": "22 May, 2023",
    "authors": [
      "Li-Wei Chen",
      " Yao-Fei Cheng",
      " Hung-Shin Lee",
      " Yu Tsao",
      " Hsin-Min Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13052",
    "paper_id": "2305.13052",
    "abstract": "\n        Electronic Health Records (EHR) data contains medical records such as diagnoses, medications, procedures, and treatments of patients. This data is often considered sensitive medical information. Therefore, the EHR data from the medical centers often cannot be shared, making it difficult to create prediction models using multi-center EHR data, which is essential for such models' robustness and generalizability. Federated Learning (FL) is an algorithmic approach that allows learning a shared model using data in multiple locations without the need to store all data in a central place. An example of a prediction model's task is to predict future diseases. More specifically, the model needs to predict patient's next visit diagnoses, based on current and previous clinical data. Such a prediction model can support care providers in making clinical decisions and even provide preventive treatment. We propose a federated learning approach for learning medical concepts embedding. This pre-trained model can be used for fine-tuning for specific downstream tasks. Our approach is based on an embedding model like BEHRT, a deep neural sequence transduction model for EHR. We train using federated learning, both the Masked Language Modeling (MLM) and the next visit downstream model. We demonstrate our approach on the MIMIC-IV dataset. We compare the performance of a model trained with FL against a model trained on centralized data. We find that our federated learning approach reaches very close to the performance of a centralized model, and it outperforms local models in terms of average precision. We also show that pre-trained MLM improves the model's average precision performance in the next visit prediction task, compared to an MLM model without pre-training. Our code is available at https://github.com/nadavlab/FederatedBEHRT.\n        \u25b3 Less\n      ",
    "title": "Federated Learning of Medical Concepts Embedding using BEHRT",
    "date": "22 May, 2023",
    "authors": [
      "Ofir Ben Shoham",
      " Nadav Rappoport"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.02179",
    "paper_id": "2204.02179",
    "abstract": "\n        Myoelectric pattern recognition is one of the important aspects in the design of the control strategy for various applications including upper-limb prostheses and bio-robotic hand movement systems. The current work has proposed an approach to design an energy-efficient EMG-based controller by considering a kernelized SVM classifier for decoding the information of surface electromyography (sEMG) signals to infer the underlying muscle movements. In order to achieve the optimized performance of the EMG-based controller, our main strategy of classifier design is to reduce the false movements of the overall system (when the EMG-based controller is at the `Rest' position). To this end, we have formulated the training algorithm of the proposed supervised learning system as a general constrained multi-objective optimization problem. An elitist multi-objective evolutionary algorithm \u2212- the non-dominated sorting genetic algorithm II (NSGA-II) has been used to tune the hyperparameters of SVM. We have presented the experimental results by performing the experiments on a dataset consisting of the sEMG signals collected from eleven subjects at five different upper limb positions. Furthermore, the performance of the trained models based on the two-objective metrics, namely classification accuracy, and false-negative have been evaluated on two different test sets to examine the generalization capability of the proposed training approach while implementing limb-position invariant EMG classification. It is evident from the presented result that the proposed approach provides much more flexibility to the designer in selecting the parameters of the classifier to optimize the energy efficiency of the EMG-based controller.\n        \u25b3 Less\n      ",
    "title": "Towards Robust and Accurate Myoelectric Controller Design based on Multi-objective Optimization using Evolutionary Computation",
    "date": "22 May, 2023",
    "authors": [
      "Ahmed Aqeel Shaikh",
      " Anand Kumar Mukhopadhyay",
      " Soumyajit Poddar",
      " Suman Samui"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13080",
    "paper_id": "2305.13080",
    "abstract": "\n        We consider the problem of few-shot spoken word classification in a setting where a model is incrementally introduced to new word classes. This would occur in a user-defined keyword system where new words can be added as the system is used. In such a continual learning scenario, a model might start to misclassify earlier words as newer classes are added, i.e. catastrophic forgetting. To address this, we propose an extension to model-agnostic meta-learning (MAML): each inner learning loop, where a model \"learns how to learn'' new classes, ends with a single gradient update using stored templates from all the classes that the model has already seen (one template per class). We compare this method to OML (another extension of MAML) in few-shot isolated-word classification experiments on Google Commands and FACC. Our method consistently outperforms OML in experiments where the number of shots and the final number of classes are varied.\n        \u25b3 Less\n      ",
    "title": "Mitigating Catastrophic Forgetting for Few-Shot Spoken Word Classification Through Meta-Learning",
    "date": "22 May, 2023",
    "authors": [
      "Ruan van der Merwe",
      " Herman Kamper"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13088",
    "paper_id": "2305.13088",
    "abstract": "\n        The abundance of annotated data in natural language processing (NLP) poses both opportunities and challenges. While it enables the development of high-performing models for a variety of tasks, it also poses the risk of models learning harmful biases from the data, such as gender stereotypes. In this work, we investigate the role of attention, a widely-used technique in current state-of-the-art NLP models, in the propagation of social biases. Specifically, we study the relationship between the entropy of the attention distribution and the model's performance and fairness. We then propose a novel method for modulating attention weights to improve model fairness after training. Since our method is only applied post-training and pre-inference, it is an intra-processing method and is, therefore, less computationally expensive than existing in-processing and pre-processing approaches. Our results show an increase in fairness and minimal performance loss on different text classification and generation tasks using language models of varying sizes. WARNING: This work uses language that is offensive.\n        \u25b3 Less\n      ",
    "title": "Should We Attend More or Less? Modulating Attention for Fairness",
    "date": "22 May, 2023",
    "authors": [
      "Abdelrahman Zayed",
      " Goncalo Mordido",
      " Samira Shabanian",
      " Sarath Chandar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14384",
    "paper_id": "2305.14384",
    "abstract": "\n        The generative AI revolution in recent years has been spurred by an expansion in compute power and data quantity, which together enable extensive pre-training of powerful text-to-image (T2I) models. With their greater capabilities to generate realistic and creative content, these T2I models like DALL-E, MidJourney, Imagen or Stable Diffusion are reaching ever wider audiences. Any unsafe behaviors inherited from pretraining on uncurated internet-scraped datasets thus have the potential to cause wide-reaching harm, for example, through generated images which are violent, sexually explicit, or contain biased and derogatory stereotypes. Despite this risk of harm, we lack systematic and structured evaluation datasets to scrutinize model behavior, especially adversarial attacks that bypass existing safety filters. A typical bottleneck in safety evaluation is achieving a wide coverage of different types of challenging examples in the evaluation set, i.e., identifying 'unknown unknowns' or long-tail problems. To address this need, we introduce the Adversarial Nibbler challenge. The goal of this challenge is to crowdsource a diverse set of failure modes and reward challenge participants for successfully finding safety vulnerabilities in current state-of-the-art T2I models. Ultimately, we aim to provide greater awareness of these issues and assist developers in improving the future safety and reliability of generative AI models. Adversarial Nibbler is a data-centric challenge, part of the DataPerf challenge suite, organized and supported by Kaggle and MLCommons.\n        \u25b3 Less\n      ",
    "title": "Adversarial Nibbler: A Data-Centric Challenge for Improving the Safety of Text-to-Image Models",
    "date": "22 May, 2023",
    "authors": [
      "Alicia Parrish",
      " Hannah Rose Kirk",
      " Jessica Quaye",
      " Charvi Rastogi",
      " Max Bartolo",
      " Oana Inel",
      " Juan Ciro",
      " Rafael Mosquera",
      " Addison Howard",
      " Will Cukierski",
      " D. Sculley",
      " Vijay Janapa Reddi",
      " Lora Aroyo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13102",
    "paper_id": "2305.13102",
    "abstract": "\n        The landscape for building conversational interfaces (chatbots) has witnessed a paradigm shift with recent developments in generative Artificial Intelligence (AI) based Large Language Models (LLMs), such as ChatGPT by OpenAI (GPT3.5 and GPT4), Google's Bard, Large Language Model Meta AI (LLaMA), among others. In this paper, we analyze capabilities and limitations of incorporating such models in conversational interfaces for the telecommunication domain, specifically for enterprise wireless products and services. Using Cradlepoint's publicly available data for our experiments, we present a comparative analysis of the responses from such models for multiple use-cases including domain adaptation for terminology and product taxonomy, context continuity, robustness to input perturbations and errors. We believe this evaluation would provide useful insights to data scientists engaged in building customized conversational interfaces for domain-specific requirements.\n        \u25b3 Less\n      ",
    "title": "Observations on LLMs for Telecom Domain: Capabilities and Limitations",
    "date": "22 May, 2023",
    "authors": [
      "Sumit Soman",
      " Ranjani H G"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18319",
    "paper_id": "2305.18319",
    "abstract": "\n        Timely feedback is an important part of teaching and learning. Here we describe how a readily available neural network transformer (machine-learning) model (BERT) can be used to give feedback on the structure of the response to an abstracting exercise where students are asked to summarise the contents of a published article after finding it from a publication database. The dataset contained 207 submissions from two consecutive years of the course, summarising a total of 21 different papers from the primary literature. The model was pre-trained using an available dataset (approx. 15,000 samples) and then fine-tuned on 80% of the submitted dataset. This fine tuning was seen to be important. The sentences in the student submissions are characterised into three classes - background, technique and observation - which allows a comparison of how each submission is structured. Comparing the structure of the students' abstract a large collection of those from the PubMed database shows that students in this exercise concentrate more on the background to the paper and less on the techniques and results than the abstracts to papers themselves. The results allowed feedback for each submitted assignment to be automatically generated.\n        \u25b3 Less\n      ",
    "title": "Automated Feedback Generation for a Chemistry Database and Abstracting Exercise",
    "date": "22 May, 2023",
    "authors": [
      "Oscar Morris",
      " Russell Morris"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18320",
    "paper_id": "2305.18320",
    "abstract": "\n        Large language models are becoming increasingly integrated into our lives. Hence, it is important to understand the biases present in their outputs in order to avoid perpetuating harmful stereotypes, which originate in our own flawed ways of thinking. This challenge requires developing new benchmarks and methods for quantifying affective and semantic bias, keeping in mind that LLMs act as psycho-social mirrors that reflect the views and tendencies that are prevalent in society. One such tendency that has harmful negative effects is the global phenomenon of anxiety toward math and STEM subjects. Here, we investigate perceptions of math and STEM fields provided by cutting-edge language models, namely GPT-3, Chat-GPT, and GPT-4, by applying an approach from network science and cognitive psychology. Specifically, we use behavioral forma mentis networks (BFMNs) to understand how these LLMs frame math and STEM disciplines in relation to other concepts. We use data obtained by probing the three LLMs in a language generation task that has previously been applied to humans. Our findings indicate that LLMs have an overall negative perception of math and STEM fields, with math being perceived most negatively. We observe significant differences across the three LLMs. We observe that newer versions (i.e. GPT-4) produce richer, more complex perceptions as well as less negative perceptions compared to older versions and N=159 high-school students. These findings suggest that advances in the architecture of LLMs may lead to increasingly less biased models that could even perhaps someday aid in reducing harmful stereotypes in society rather than perpetuating them.\n        \u25b3 Less\n      ",
    "title": "Cognitive network science reveals bias in GPT-3, ChatGPT, and GPT-4 mirroring math anxiety in high-school students",
    "date": "22 May, 2023",
    "authors": [
      "Katherine Abramski",
      " Salvatore Citraro",
      " Luigi Lombardi",
      " Giulio Rossetti",
      " Massimo Stella"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2102.07389",
    "paper_id": "2102.07389",
    "abstract": "\n        Despite the success of neural networks, the issue of classification robustness remains, particularly highlighted by adversarial examples. In this paper, we address this challenge by focusing on the continuum of functions implemented in artificial neurons, ranging from pure AND gates to pure OR gates. Our hypothesis is that the presence of a sufficient number of OR-like neurons in a network can lead to classification brittleness and increased vulnerability to adversarial attacks. We define AND-like neurons and propose measures to increase their proportion in the network. These measures involve rescaling inputs to the [-1,1] interval and reducing the number of points in the steepest section of the sigmoidal activation function. A crucial component of our method is the comparison between a neuron's output distribution when fed with the actual dataset and a randomised version called the \"scrambled dataset.\" Experimental results on the MNIST dataset suggest that our approach holds promise as a direction for further exploration.\n        \u25b3 Less\n      ",
    "title": "And/or trade-off in artificial neurons: impact on adversarial robustness",
    "date": "22 May, 2023",
    "authors": [
      "Alessandro Fontana"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18321",
    "paper_id": "2305.18321",
    "abstract": "\n        Ising machines, which are hardware implementations of the Ising model of coupled spins, have been influential in the development of unsupervised learning algorithms at the origins of Artificial Intelligence (AI). However, their application to AI has been limited due to the complexities in matching supervised training methods with Ising machine physics, even though these methods are essential for achieving high accuracy. In this study, we demonstrate a novel approach to train Ising machines in a supervised way through the Equilibrium Propagation algorithm, achieving comparable results to software-based implementations. We employ the quantum annealing procedure of the D-Wave Ising machine to train a fully-connected neural network on the MNIST dataset. Furthermore, we demonstrate that the machine's connectivity supports convolution operations, enabling the training of a compact convolutional network with minimal spins per neuron. Our findings establish Ising machines as a promising trainable hardware platform for AI, with the potential to enhance machine learning applications.\n        \u25b3 Less\n      ",
    "title": "Training an Ising Machine with Equilibrium Propagation",
    "date": "22 May, 2023",
    "authors": [
      "J\u00e9r\u00e9mie Laydevant",
      " Danijela Markovic",
      " Julie Grollier"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.01576",
    "paper_id": "2211.01576",
    "abstract": "\n        We present a learning-enabled Task and Motion Planning (TAMP) algorithm for solving mobile manipulation problems in environments with many articulated and movable obstacles. Our idea is to bias the search procedure of a traditional TAMP planner with a learned plan feasibility predictor. The core of our algorithm is PIGINet, a novel Transformer-based learning method that takes in a task plan, the goal, and the initial state, and predicts the probability of finding motion trajectories associated with the task plan. We integrate PIGINet within a TAMP planner that generates a diverse set of high-level task plans, sorts them by their predicted likelihood of feasibility, and refines them in that order. We evaluate the runtime of our TAMP algorithm on seven families of kitchen rearrangement problems, comparing its performance to that of non-learning baselines. Our experiments show that PIGINet substantially improves planning efficiency, cutting down runtime by 80% on problems with small state spaces and 10%-50% on larger ones, after being trained on only 150-600 problems. Finally, it also achieves zero-shot generalization to problems with unseen object categories thanks to its visual encoding of objects. Project page https://piginet.github.io/.\n        \u25b3 Less\n      ",
    "title": "Sequence-Based Plan Feasibility Prediction for Efficient Task and Motion Planning",
    "date": "22 May, 2023",
    "authors": [
      "Zhutian Yang",
      " Caelan Reed Garrett",
      " Tom\u00e1s Lozano-P\u00e9rez",
      " Leslie Kaelbling",
      " Dieter Fox"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.15004",
    "paper_id": "2304.15004",
    "abstract": "\n        Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.\n        \u25b3 Less\n      ",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "date": "22 May, 2023",
    "authors": [
      "Rylan Schaeffer",
      " Brando Miranda",
      " Sanmi Koyejo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13347",
    "paper_id": "2305.13347",
    "abstract": "\n        Overlapping instruction subsets derived from human originated code have previously been shown to dramatically shrink the inductive programming search space, often by many orders of magnitude. Here we extend the instruction subset approach to consider direct instruction-instruction applications (or instruction digrams) as an additional search heuristic for inductive programming. In this study we analyse the frequency distribution of instruction digrams in a large sample of open source code. This indicates that the instruction digram distribution is highly skewed with over 93% of possible instruction digrams not represnted in the code sample. We demonstrate that instruction digrams can be used to constrain instruction selection during search, further reducing size of the the search space, in some cases by several orders of magnitude. This significantly increases the size of programs that can be generated using search based inductive programming techniques. We discuss the results and provide some suggestions for further work.\n        \u25b3 Less\n      ",
    "title": "Further Decimating the Inductive Programming Search Space with Instruction Digrams",
    "date": "22 May, 2023",
    "authors": [
      "Edward McDaid",
      " Sarah McDaid"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13190",
    "paper_id": "2305.13190",
    "abstract": "\n        This paper introduces a framework for assisting policy authors in refining and improving their policies. In particular, we focus on authorization and obligation policies that can be encoded in Gelfond and Lobo's AOPL language for policy specification. We propose a framework that detects the statements that make a policy inconsistent, underspecified, or ambiguous with respect to an action being executed in a given state. We also give attention to issues that arise at the intersection of authorization and obligation policies, for instance when the policy requires an unauthorized action to be executed. The framework is encoded in Answer Set Programming. Under consideration for acceptance in TPLP.\n        \u25b3 Less\n      ",
    "title": "An ASP Framework for the Refinement of Authorization and Obligation Policies",
    "date": "22 May, 2023",
    "authors": [
      "Daniela Inclezan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13191",
    "paper_id": "2305.13191",
    "abstract": "\n        Training a Named Entity Recognition (NER) model often involves fixing a taxonomy of entity types. However, requirements evolve and we might need the NER model to recognize additional entity types. A simple approach is to re-annotate entire dataset with both existing and additional entity types and then train the model on the re-annotated dataset. However, this is an extremely laborious task. To remedy this, we propose a novel approach called Partial Label Model (PLM) that uses only partially annotated datasets. We experiment with 6 diverse datasets and show that PLM consistently performs better than most other approaches (0.5 - 2.5 F1), including in novel settings for taxonomy expansion not considered in prior work. The gap between PLM and all other approaches is especially large in settings where there is limited data available for the additional entity types (as much as 11 F1), thus suggesting a more cost effective approaches to taxonomy expansion.\n        \u25b3 Less\n      ",
    "title": "Taxonomy Expansion for Named Entity Recognition",
    "date": "22 May, 2023",
    "authors": [
      "Karthikeyan K",
      " Yogarshi Vyas",
      " Jie Ma",
      " Giovanni Paolini",
      " Neha Anna John",
      " Shuai Wang",
      " Yassine Benajiba",
      " Vittorio Castelli",
      " Dan Roth",
      " Miguel Ballesteros"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.14749",
    "paper_id": "2304.14749",
    "abstract": "\n        Academic and policy proposals on algorithmic accountability often seek to understand algorithmic systems in their socio-technical context, recognising that they are produced by 'many hands'. Increasingly, however, algorithmic systems are also produced, deployed, and used within a supply chain comprising multiple actors tied together by flows of data between them. In such cases, it is the working together of an algorithmic supply chain of different actors who contribute to the production, deployment, use, and functionality that drives systems and produces particular outcomes. We argue that algorithmic accountability discussions must consider supply chains and the difficult implications they raise for the governance and accountability of algorithmic systems. In doing so, we explore algorithmic supply chains, locating them in their broader technical and political economic context and identifying some key features that should be understood in future work on algorithmic governance and accountability (particularly regarding general purpose AI services). To highlight ways forward and areas warranting attention, we further discuss some implications raised by supply chains: challenges for allocating accountability stemming from distributed responsibility for systems between actors, limited visibility due to the accountability horizon, service models of use and liability, and cross-border supply chains and regulatory arbitrage\n        \u25b3 Less\n      ",
    "title": "Understanding accountability in algorithmic supply chains",
    "date": "22 May, 2023",
    "authors": [
      "Jennifer Cobbe",
      " Michael Veale",
      " Jatinder Singh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13206",
    "paper_id": "2305.13206",
    "abstract": "\n        In combination with Reinforcement Learning, Monte-Carlo Tree Search has shown to outperform human grandmasters in games such as Chess, Shogi and Go with little to no prior domain knowledge. However, most classical use cases only feature up to two players. Scaling the search to an arbitrary number of players presents a computational challenge, especially if decisions have to be planned over a longer time horizon. In this work, we investigate techniques that transform general-sum multiplayer games into single-player and two-player games that consider other agents to act according to given opponent models. For our evaluation, we focus on the challenging Pommerman environment which involves partial observability, a long time horizon and sparse rewards. In combination with our search methods, we investigate the phenomena of opponent modeling using heuristics and self-play. Overall, we demonstrate the effectiveness of our multiplayer search variants both in a supervised learning and reinforcement learning setting.\n        \u25b3 Less\n      ",
    "title": "Know your Enemy: Investigating Monte-Carlo Tree Search with Opponent Models in Pommerman",
    "date": "22 May, 2023",
    "authors": [
      "Jannis Weil",
      " Johannes Czech",
      " Tobias Meuser",
      " Kristian Kersting"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.01753",
    "paper_id": "2306.01753",
    "abstract": "\n        Humans can infer the affordance of objects by extracting related contextual preconditions for each scenario. For example, upon seeing an image of a broken cup, we can infer that this precondition prevents the cup from being used for drinking. Reasoning with preconditions of commonsense is studied in NLP where the model explicitly gets the contextual precondition. However, it is unclear if SOTA visual language models (VLMs) can extract such preconditions and infer the affordance of objects with them. In this work, we introduce the task of preconditioned visual language inference and rationalization (PVLIR). We propose a learning resource based on three strategies to retrieve weak supervision signals for the task and develop a human-verified test set for evaluation. Our results reveal the shortcomings of SOTA VLM models in the task and draw a road map to address the challenges ahead in improving them.\n        \u25b3 Less\n      ",
    "title": "Preconditioned Visual Language Inference with Weak Supervision",
    "date": "22 May, 2023",
    "authors": [
      "Ehsan Qasemi",
      " Amani R. Maina-Kilaas",
      " Devadutta Dash",
      " Khalid Alsaggaf",
      " Muhao Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13246",
    "paper_id": "2305.13246",
    "abstract": "\n        Interactive Natural Language Processing (iNLP) has emerged as a novel paradigm within the field of NLP, aimed at addressing limitations in existing frameworks while aligning with the ultimate goals of artificial intelligence. This paradigm considers language models as agents capable of observing, acting, and receiving feedback iteratively from external entities. Specifically, language models in this context can: (1) interact with humans for better understanding and addressing user needs, personalizing responses, aligning with human values, and improving the overall user experience; (2) interact with knowledge bases for enriching language representations with factual knowledge, enhancing the contextual relevance of responses, and dynamically leveraging external information to generate more accurate and informed responses; (3) interact with models and tools for effectively decomposing and addressing complex tasks, leveraging specialized expertise for specific subtasks, and fostering the simulation of social behaviors; and (4) interact with environments for learning grounded representations of language, and effectively tackling embodied tasks such as reasoning, planning, and decision-making in response to environmental observations. This paper offers a comprehensive survey of iNLP, starting by proposing a unified definition and framework of the concept. We then provide a systematic classification of iNLP, dissecting its various components, including interactive objects, interaction interfaces, and interaction methods. We proceed to delve into the evaluation methodologies used in the field, explore its diverse applications, scrutinize its ethical and safety issues, and discuss prospective research directions. This survey serves as an entry point for researchers who are interested in this rapidly evolving area and offers a broad view of the current landscape and future trajectory of iNLP.\n        \u25b3 Less\n      ",
    "title": "Interactive Natural Language Processing",
    "date": "22 May, 2023",
    "authors": [
      "Zekun Wang",
      " Ge Zhang",
      " Kexin Yang",
      " Ning Shi",
      " Wangchunshu Zhou",
      " Shaochun Hao",
      " Guangzheng Xiong",
      " Yizhi Li",
      " Mong Yuan Sim",
      " Xiuying Chen",
      " Qingqing Zhu",
      " Zhenzhu Yang",
      " Adam Nik",
      " Qi Liu",
      " Chenghua Lin",
      " Shi Wang",
      " Ruibo Liu",
      " Wenhu Chen",
      " Ke Xu",
      " Dayiheng Liu",
      " Yike Guo",
      " Jie Fu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13258",
    "paper_id": "2305.13258",
    "abstract": "\n        NeSy4VRD is a multifaceted resource designed to support the development of neurosymbolic AI (NeSy) research. NeSy4VRD re-establishes public access to the images of the VRD dataset and couples them with an extensively revised, quality-improved version of the VRD visual relationship annotations. Crucially, NeSy4VRD provides a well-aligned, companion OWL ontology that describes the dataset domain.It comes with open source infrastructure that provides comprehensive support for extensibility of the annotations (which, in turn, facilitates extensibility of the ontology), and open source code for loading the annotations to/from a knowledge graph. We are contributing NeSy4VRD to the computer vision, NeSy and Semantic Web communities to help foster more NeSy research using OWL-based knowledge graphs.\n        \u25b3 Less\n      ",
    "title": "NeSy4VRD: A Multifaceted Resource for Neurosymbolic AI Research using Knowledge Graphs in Visual Relationship Detection",
    "date": "22 May, 2023",
    "authors": [
      "David Herron",
      " Ernesto Jim\u00e9nez-Ruiz",
      " Giacomo Tarroni",
      " Tillman Weyde"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13267",
    "paper_id": "2305.13267",
    "abstract": "\n        Pre-trained visual language models (VLM) have shown excellent performance in image caption tasks. However, it sometimes shows insufficient reasoning ability. In contrast, large language models (LLMs) emerge with powerful reasoning capabilities. Therefore, we propose a method called TReE, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios. TReE contains three stages: observation, thinking, and re-thinking. Observation stage indicates that VLM obtains the overall information of the relative image. Thinking stage combines the image information and task description as the prompt of the LLM, inference with the rationals. Re-Thinking stage learns from rationale and then inference the final result through VLM.\n        \u25b3 Less\n      ",
    "title": "Enhance Reasoning Ability of Visual-Language Models via Large Language Models",
    "date": "22 May, 2023",
    "authors": [
      "Yueting Yang",
      " Xintong Zhang",
      " Wenjuan Han"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14386",
    "paper_id": "2305.14386",
    "abstract": "\n        In this paper, we present a novel approach for distilling math word problem solving capabilities from large language models (LLMs) into smaller, more efficient student models. Our approach is designed to consider the student model's weaknesses and foster a tailored learning experience by generating targeted exercises aligned with educational science principles, such as knowledge tracing and personalized learning. Concretely, we let GPT-3 be a math tutor and run two steps iteratively: 1) assessing the student model's current learning status on a GPT-generated exercise book, and 2) improving the student model by training it with tailored exercise samples generated by GPT-3. Experimental results reveal that our approach outperforms LLMs (e.g., GPT-3 and PaLM) in accuracy across three distinct benchmarks while employing significantly fewer parameters. Furthermore, we provide a comprehensive analysis of the various components within our methodology to substantiate their efficacy.\n        \u25b3 Less\n      ",
    "title": "Let GPT be a Math Tutor: Teaching Math Word Problem Solvers with Customized Exercise Generation",
    "date": "22 May, 2023",
    "authors": [
      "Zhenwen Liang",
      " Wenhao Yu",
      " Tanmay Rajpurohit",
      " Peter Clark",
      " Xiangliang Zhang",
      " Ashwin Kaylan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.01471",
    "paper_id": "2302.01471",
    "abstract": "\n        The Metaverse is emerging as maturing technologies are empowering the different facets. Virtual Reality (VR) technologies serve as the backbone of the virtual universe within the Metaverse to offer a highly immersive user experience. As mobility is emphasized in the Metaverse context, VR devices reduce their weights at the sacrifice of local computation abilities. In this paper, for a system consisting of a Metaverse server and multiple VR users, we consider two cases of (i) the server generating frames and transmitting them to users, and (ii) users generating frames locally and thus consuming device energy. Moreover, in our multi-user VR scenario for the Metaverse, users have different characteristics and demands for Frames Per Second (FPS). Then the channel access arrangement (including the decisions on frame generation location), and transmission powers for the downlink communications from the server to the users are jointly optimized to improve the utilities of users. This joint optimization is addressed by deep reinforcement learning (DRL) with heterogeneous actions. Our proposed user-centric DRL algorithm is called User-centric Critic with Heterogenous Actors (UCHA). Extensive experiments demonstrate that our UCHA algorithm leads to remarkable results under various requirements and constraints.\n        \u25b3 Less\n      ",
    "title": "User-centric Heterogeneous-action Deep Reinforcement Learning for Virtual Reality in the Metaverse over Wireless Networks",
    "date": "22 May, 2023",
    "authors": [
      "Wenhan Yu",
      " Terence Jie Chua",
      " Jun Zhao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13283",
    "paper_id": "2305.13283",
    "abstract": "\n        In this work we consider the problem of fitting Random Utility Models (RUMs) to user choices. Given the winner distributions of the subsets of size kk of a universe, we obtain a polynomial-time algorithm that finds the RUM that best approximates the given distribution on average. Our algorithm is based on a linear program that we solve using the ellipsoid method. Given that its corresponding separation oracle problem is NP-hard, we devise an approximate separation oracle that can be viewed as a generalization of the weighted feedback arc set problem to hypergraphs. Our theoretical result can also be made practical: we obtain a heuristic that is effective and scales to real-world datasets.\n        \u25b3 Less\n      ",
    "title": "Approximating a RUM from Distributions on k-Slates",
    "date": "22 May, 2023",
    "authors": [
      "Flavio Chierichetti",
      " Mirko Giacchini",
      " Ravi Kumar",
      " Alessandro Panconesi",
      " Andrew Tomkins"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13284",
    "paper_id": "2305.13284",
    "abstract": "\n        In this paper, we address the problem of adapting models from a source domain to a target domain, a task that has become increasingly important due to the brittle generalization of deep neural networks. While several test-time adaptation techniques have emerged, they typically rely on synthetic toolbox data augmentations in cases of limited target data availability. We consider the challenging setting of single-shot adaptation and explore the design of augmentation strategies. We argue that augmentations utilized by existing methods are insufficient to handle large distribution shifts, and hence propose a new approach SiSTA, which first fine-tunes a generative model from the source domain using a single-shot target, and then employs novel sampling strategies for curating synthetic target data. Using experiments on a variety of benchmarks, distribution shifts and image corruptions, we find that SiSTA produces significantly improved generalization over existing baselines in face attribute detection and multi-class object recognition. Furthermore, SiSTA performs competitively to models obtained by training on larger target datasets. Our codes can be accessed at https://github.com/Rakshith-2905/SiSTA.\n        \u25b3 Less\n      ",
    "title": "Target-Aware Generative Augmentations for Single-Shot Adaptation",
    "date": "22 May, 2023",
    "authors": [
      "Kowshik Thopalli",
      " Rakshith Subramanyam",
      " Pavan Turaga",
      " Jayaraman J. Thiagarajan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18569",
    "paper_id": "2305.18569",
    "abstract": "\n        Understanding and addressing unfairness in LLMs are crucial for responsible AI deployment. However, there is a limited availability of quantitative analyses and in-depth studies regarding fairness evaluations in LLMs, especially when applying LLMs to high-stakes fields. This work aims to fill this gap by providing a systematic evaluation of the effectiveness and fairness of LLMs using ChatGPT as a study case. We focus on assessing ChatGPT's performance in high-takes fields including education, criminology, finance and healthcare. To make thorough evaluation, we consider both group fairness and individual fairness and we also observe the disparities in ChatGPT's outputs under a set of biased or unbiased prompts. This work contributes to a deeper understanding of LLMs' fairness performance, facilitates bias mitigation and fosters the development of responsible artificial intelligence systems.\n        \u25b3 Less\n      ",
    "title": "Fairness of ChatGPT",
    "date": "22 May, 2023",
    "authors": [
      "Yunqi Li",
      " Yongfeng Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13299",
    "paper_id": "2305.13299",
    "abstract": "\n        In-context learning (ICL) is an important paradigm for adapting large language models (LLMs) to new tasks, but the generalization behavior of ICL remains poorly understood. We investigate the inductive biases of ICL from the perspective of feature bias: which feature ICL is more likely to use given a set of underspecified demonstrations in which two features are equally predictive of the labels. First, we characterize the feature biases of GPT-3 models by constructing underspecified demonstrations from a range of NLP datasets and feature combinations. We find that LLMs exhibit clear feature biases - for example, demonstrating a strong bias to predict labels according to sentiment rather than shallow lexical features, like punctuation. Second, we evaluate the effect of different interventions that are designed to impose an inductive bias in favor of a particular feature, such as adding a natural language instruction or using semantically relevant label words. We find that, while many interventions can influence the learner to prefer a particular feature, it can be difficult to overcome strong prior biases. Overall, our results provide a broader picture of the types of features that ICL may be more likely to exploit and how to impose inductive biases that are better aligned with the intended task.\n        \u25b3 Less\n      ",
    "title": "Measuring Inductive Biases of In-Context Learning with Underspecified Demonstrations",
    "date": "22 May, 2023",
    "authors": [
      "Chenglei Si",
      " Dan Friedman",
      " Nitish Joshi",
      " Shi Feng",
      " Danqi Chen",
      " He He"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13396",
    "paper_id": "2305.13396",
    "abstract": "\n        Infants explore their complex physical and social environment in an organized way. To gain insight into what intrinsic motivations may help structure this exploration, we create a virtual infant agent and place it in a developmentally-inspired 3D environment with no external rewards. The environment has a virtual caregiver agent with the capability to interact contingently with the infant agent in ways that resemble play. We test intrinsic reward functions that are similar to motivations that have been proposed to drive exploration in humans: surprise, uncertainty, novelty, and learning progress. These generic reward functions lead the infant agent to explore its environment and discover the contingencies that are embedded into the caregiver agent. The reward functions that are proxies for novelty and uncertainty are the most successful in generating diverse experiences and activating the environment contingencies. We also find that learning a world model in the presence of an attentive caregiver helps the infant agent learn how to predict scenarios with challenging social and physical dynamics. Taken together, our findings provide insight into how curiosity-like intrinsic rewards and contingent social interaction lead to dynamic social behavior and the creation of a robust predictive world model.\n        \u25b3 Less\n      ",
    "title": "Developmental Curiosity and Social Interaction in Virtual Agents",
    "date": "22 May, 2023",
    "authors": [
      "Chris Doyle",
      " Sarah Shader",
      " Michelle Lau",
      " Megumi Sano",
      " Daniel L. K. Yamins",
      " Nick Haber"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16333",
    "paper_id": "2305.16333",
    "abstract": "\n        Aiming at reducing the reliance on expensive human annotations, data synthesis for Automatic Speech Recognition (ASR) has remained an active area of research. While prior work mainly focuses on synthetic speech generation for ASR data augmentation, its combination with text generation methods is considerably less explored. In this work, we explore text augmentation for ASR using large-scale pre-trained neural networks, and systematically compare those to traditional text augmentation methods. The generated synthetic texts are then converted to synthetic speech using a text-to-speech (TTS) system and added to the ASR training data. In experiments conducted on three datasets, we find that neural models achieve 9%-15% relative WER improvement and outperform traditional methods. We conclude that text augmentation, particularly through modern neural approaches, is a viable tool for improving the accuracy of ASR systems.\n        \u25b3 Less\n      ",
    "title": "Text Generation with Speech Synthesis for ASR Data Augmentation",
    "date": "22 May, 2023",
    "authors": [
      "Zhuangqun Huang",
      " Gil Keren",
      " Ziran Jiang",
      " Shashank Jain",
      " David Goss-Grubbs",
      " Nelson Cheng",
      " Farnaz Abtahi",
      " Duc Le",
      " David Zhang",
      " Antony D'Avirro",
      " Ethan Campbell-Taylor",
      " Jessie Salas",
      " Irina-Elena Veliche",
      " Xi Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13425",
    "paper_id": "2305.13425",
    "abstract": "\n        This paper presents EINCASM, a prototype system employing a novel framework for studying emergent intelligence in organisms resembling slime molds. EINCASM evolves neural cellular automata with NEAT to maximize cell growth constrained by nutrient and energy costs. These organisms capitalize physically simulated fluid to transport nutrients and chemical-like signals to orchestrate growth and adaptation to complex, changing environments. Our framework builds the foundation for studying how the presence of puzzles, physics, communication, competition and dynamic open-ended environments contribute to the emergence of intelligent behavior. We propose preliminary tests for intelligence in such organisms and suggest future work for more powerful systems employing EINCASM to better understand intelligence in distributed dynamical systems.\n        \u25b3 Less\n      ",
    "title": "EINCASM: Emergent Intelligence in Neural Cellular Automaton Slime Molds",
    "date": "22 May, 2023",
    "authors": [
      "Aidan Barbieux",
      " Rodrigo Canaan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.01527",
    "paper_id": "2211.01527",
    "abstract": "\n        We present an approach for autonomous sensor control for information gathering under partially observable, dynamic and sparsely sampled environments that maximizes information about entities present in that space. We describe our approach for the task of Radio-Frequency (RF) spectrum monitoring, where the goal is to search for and track unknown, dynamic signals in the environment. To this end, we extend the Deep Anticipatory Network (DAN) Reinforcement Learning (RL) framework by (1) improving exploration in sparse, non-stationary environments using a novel information gain reward, and (2) scaling up the control space and enabling the monitoring of complex, dynamic activity patterns using hybrid convolutional-recurrent neural layers. We also extend this problem to situations in which sampling from the intended RF spectrum/field is limited and propose a model-based version of the original RL algorithm that fine-tunes the controller via a model that is iteratively improved from the limited field sampling. Results in simulated RF environments of differing complexity show that our system outperforms the standard DAN architecture and is more flexible and robust than baseline expert-designed agents. We also show that it is adaptable to non-stationary emission environments.\n        \u25b3 Less\n      ",
    "title": "Sensor Control for Information Gain in Dynamic, Sparse and Partially Observed Environments",
    "date": "22 May, 2023",
    "authors": [
      "J. Brian Burns",
      " Aravind Sundaresan",
      " Pedro Sequeira",
      " Vidyasagar Sadhu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2012.06694",
    "paper_id": "2012.06694",
    "abstract": "\n        In the human brain, internal states are often correlated over time (due to local recurrence and other intrinsic circuit properties), punctuated by abrupt transitions. At first glance, temporal smoothness of internal states presents a problem for learning input-output mappings (e.g. category labels for images), because the internal representation of the input will contain a mixture of current input and prior inputs. However, when training with naturalistic data (e.g. movies) there is also temporal autocorrelation in the input. How does the temporal \"smoothness\" of internal states affect the efficiency of learning when the training data are also temporally smooth? How does it affect the kinds of representations that are learned? We found that, when trained with temporally smooth data, \"slow\" neural networks (equipped with linear recurrence and gating mechanisms) learned to categorize more efficiently than feedforward networks. Furthermore, networks with linear recurrence and multi-timescale gating could learn internal representations that \"un-mixed\" quickly-varying and slowly-varying data sources. Together, these findings demonstrate how a fundamental property of cortical dynamics (their temporal autocorrelation) can serve as an inductive bias, leading to more efficient category learning and to the representational separation of fast and slow sources in the environment.\n        \u25b3 Less\n      ",
    "title": "Consequences of Slow Neural Dynamics for Incremental Learning",
    "date": "22 May, 2023",
    "authors": [
      "Shima Rahimi Moghaddam",
      " Fanjun Bu",
      " Christopher J. Honey"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13467",
    "paper_id": "2305.13467",
    "abstract": "\n        Decentralized control schemes are increasingly favored in various domains that involve multi-agent systems due to the need for computational efficiency as well as general applicability to large-scale systems. However, in the absence of an explicit global coordinator, it is hard for distributed agents to determine how to efficiently interact with others. In this paper, we present a risk-aware decentralized control framework that provides guidance on how much relative responsibility share (a percentage) an individual agent should take to avoid collisions with others while moving efficiently without direct communications. We propose a novel Control Barrier Function (CBF)-inspired risk measurement to characterize the aggregate risk agents face from potential collisions under motion uncertainty. We use this measurement to allocate responsibility shares among agents dynamically and develop risk-aware decentralized safe controllers. In this way, we are able to leverage the flexibility of robots with lower risk to improve the motion flexibility for those with higher risk, thus achieving improved collective safety. We demonstrate the validity and efficiency of our proposed approach through two examples: ramp merging in autonomous driving and a multi-agent position-swapping game.\n        \u25b3 Less\n      ",
    "title": "Risk-aware Safe Control for Decentralized Multi-agent Systems via Dynamic Responsibility Allocation",
    "date": "22 May, 2023",
    "authors": [
      "Yiwei Lyu",
      " Wenhao Luo",
      " John M. Dolan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13500",
    "paper_id": "2305.13500",
    "abstract": "\n        Emotion understanding is an essential but highly challenging component of artificial general intelligence. The absence of extensively annotated datasets has significantly impeded advancements in this field. We present EmotionCLIP, the first pre-training paradigm to extract visual emotion representations from verbal and nonverbal communication using only uncurated data. Compared to numerical labels or descriptions used in previous methods, communication naturally contains emotion information. Furthermore, acquiring emotion representations from communication is more congruent with the human learning process. We guide EmotionCLIP to attend to nonverbal emotion cues through subject-aware context encoding and verbal emotion cues using sentiment-guided contrastive learning. Extensive experiments validate the effectiveness and transferability of EmotionCLIP. Using merely linear-probe evaluation protocol, EmotionCLIP outperforms the state-of-the-art supervised visual emotion recognition methods and rivals many multimodal approaches across various benchmarks. We anticipate that the advent of EmotionCLIP will address the prevailing issue of data scarcity in emotion understanding, thereby fostering progress in related domains. The code and pre-trained models are available at https://github.com/Xeaver/EmotionCLIP.\n        \u25b3 Less\n      ",
    "title": "Learning Emotion Representations from Verbal and Nonverbal Communication",
    "date": "22 May, 2023",
    "authors": [
      "Sitao Zhang",
      " Yimu Pan",
      " James Z. Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13509",
    "paper_id": "2305.13509",
    "abstract": "\n        In the last decade, Convolutional Neural Network (CNN) and transformer based object detectors have achieved high performance on a large variety of datasets. Though the majority of detection literature has developed this capability on datasets such as MS COCO, these detectors have still proven effective for remote sensing applications. Challenges in this particular domain, such as small numbers of annotated objects and low object density, hinder overall performance. In this work, we present a novel augmentation method, called collage pasting, for increasing the object density without a need for segmentation masks, thereby improving the detector performance. We demonstrate that collage pasting improves precision and recall beyond related methods, such as mosaic augmentation, and enables greater control of object density. However, we find that collage pasting is vulnerable to certain out-of-distribution shifts, such as image corruptions. To address this, we introduce two simple approaches for combining collage pasting with PixMix augmentation method, and refer to our combined techniques as ColMix. Through extensive experiments, we show that employing ColMix results in detectors with superior performance on aerial imagery datasets and robust to various corruptions.\n        \u25b3 Less\n      ",
    "title": "ColMix -- A Simple Data Augmentation Framework to Improve Object Detector Performance and Robustness in Aerial Images",
    "date": "22 May, 2023",
    "authors": [
      "Cuong Ly",
      " Grayson Jorgenson",
      " Dan Rosa de Jesus",
      " Henry Kvinge",
      " Adam Attarian",
      " Yijing Watkins"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13520",
    "paper_id": "2305.13520",
    "abstract": "\n        Data augmentation methods have played an important role in the recent advance of deep learning models, and have become an indispensable component of state-of-the-art models in semi-supervised, self-supervised, and supervised training for vision. Despite incurring no additional latency at test time, data augmentation often requires more epochs of training to be effective. For example, even the simple flips-and-crops augmentation requires training for more than 5 epochs to improve performance, whereas RandAugment requires more than 90 epochs. We propose a general framework called Tied-Augment, which improves the efficacy of data augmentation in a wide range of applications by adding a simple term to the loss that can control the similarity of representations under distortions. Tied-Augment can improve state-of-the-art methods from data augmentation (e.g. RandAugment, mixup), optimization (e.g. SAM), and semi-supervised learning (e.g. FixMatch). For example, Tied-RandAugment can outperform RandAugment by 2.0% on ImageNet. Notably, using Tied-Augment, data augmentation can be made to improve generalization even when training for a few epochs and when fine-tuning. We open source our code at https://github.com/ekurtulus/tied-augment/tree/main.\n        \u25b3 Less\n      ",
    "title": "Tied-Augment: Controlling Representation Similarity Improves Data Augmentation",
    "date": "22 May, 2023",
    "authors": [
      "Emirhan Kurtulus",
      " Zichao Li",
      " Yann Dauphin",
      " Ekin Dogus Cubuk"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18322",
    "paper_id": "2305.18322",
    "abstract": "\n        A number of datasets for Relation Extraction (RE) have been created to aide downstream tasks such as information retrieval, semantic search, question answering and textual entailment. However, these datasets fail to capture financial-domain specific challenges since most of these datasets are compiled using general knowledge sources such as Wikipedia, web-based text and news articles, hindering real-life progress and adoption within the financial world. To address this limitation, we propose REFinD, the first large-scale annotated dataset of relations, with \u223c\\sim29K instances and 22 relations amongst 8 types of entity pairs, generated entirely over financial documents. We also provide an empirical evaluation with various state-of-the-art models as benchmarks for the RE task and highlight the challenges posed by our dataset. We observed that various state-of-the-art deep learning models struggle with numeric inference, relational and directional ambiguity.\n        \u25b3 Less\n      ",
    "title": "REFinD: Relation Extraction Financial Dataset",
    "date": "22 May, 2023",
    "authors": [
      "Simerjot Kaur",
      " Charese Smiley",
      " Akshat Gupta",
      " Joy Sain",
      " Dongsheng Wang",
      " Suchetha Siddagangappa",
      " Toyin Aguda",
      " Sameena Shah"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13525",
    "paper_id": "2305.13525",
    "abstract": "\n        As state-of-the-art neural networks scale to billions of parameters, designing parallel algorithms that can train these networks efficiently on multi-GPU clusters has become critical. This paper presents Tensor3D, a novel three-dimensional (3D) approach to parallelize tensor computations, that strives to minimize the idle time incurred due to communication in parallel training of large multi-billion parameter models. First, we introduce an intelligent distribution of neural network parameters across GPUs that eliminates communication required for satisfying data dependencies of individual layers. Then, we propose a novel overdecomposition of the parallel training process, using which we achieve significant overlap of communication with computation, thereby reducing GPU idle time. Finally, we present a communication model, which helps users identify communication optimal decompositions of available hardware resources for a given neural network. For a 28B parameter CNN on 256 A100 GPUs, Tensor3D improves the training time by nearly 60% as compared to Megatron-LM.\n        \u25b3 Less\n      ",
    "title": "Communication-minimizing Asynchronous Tensor Parallelism",
    "date": "22 May, 2023",
    "authors": [
      "Siddharth Singh",
      " Zack Sating",
      " Abhinav Bhatele"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13530",
    "paper_id": "2305.13530",
    "abstract": "\n        This paper provides an overview of a text mining tool the StyloMetrix developed initially for the Polish language and further extended for English and recently for Ukrainian. The StyloMetrix is built upon various metrics crafted manually by computational linguists and researchers from literary studies to analyze grammatical, stylistic, and syntactic patterns. The idea of constructing the statistical evaluation of syntactic and grammar features is straightforward and familiar for the languages like English, Spanish, German, and others; it is yet to be developed for low-resource languages like Ukrainian. We describe the StyloMetrix pipeline and provide some experiments with this tool for the text classification task. We also describe our package's main limitations and the metrics' evaluation procedure.\n        \u25b3 Less\n      ",
    "title": "The Grammar and Syntax Based Corpus Analysis Tool For The Ukrainian Language",
    "date": "22 May, 2023",
    "authors": [
      "Daria Stetsenko",
      " Inez Okulska"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13541",
    "paper_id": "2305.13541",
    "abstract": "\n        Human activity recognition (HAR) is one of the core research themes in ubiquitous and wearable computing. With the shift to deep learning (DL) based analysis approaches, it has become possible to extract high-level features and perform classification in an end-to-end manner. Despite their promising overall capabilities, DL-based HAR may suffer from overfitting due to the notoriously small, often inadequate, amounts of labeled sample data that are available for typical HAR applications. In response to such challenges, we propose ConvBoost -- a novel, three-layer, structured model architecture and boosting framework for convolutional network based HAR. Our framework generates additional training data from three different perspectives for improved HAR, aiming to alleviate the shortness of labeled training data in the field. Specifically, with the introduction of three conceptual layers--Sampling Layer, Data Augmentation Layer, and Resilient Layer -- we develop three \"boosters\" -- R-Frame, Mix-up, and C-Drop -- to enrich the per-epoch training data by dense-sampling, synthesizing, and simulating, respectively. These new conceptual layers and boosters, that are universally applicable for any kind of convolutional network, have been designed based on the characteristics of the sensor data and the concept of frame-wise HAR. In our experimental evaluation on three standard benchmarks (Opportunity, PAMAP2, GOTOV) we demonstrate the effectiveness of our ConvBoost framework for HAR applications based on variants of convolutional networks: vanilla CNN, ConvLSTM, and Attention Models. We achieved substantial performance gains for all of them, which suggests that the proposed approach is generic and can serve as a practical solution for boosting the performance of existing ConvNet-based HAR models. This is an open-source project, and the code can be found at https://github.com/sshao2013/ConvBoost\n        \u25b3 Less\n      ",
    "title": "ConvBoost: Boosting ConvNets for Sensor-based Activity Recognition",
    "date": "22 May, 2023",
    "authors": [
      "Shuai Shao",
      " Yu Guan",
      " Bing Zhai",
      " Paolo Missier",
      " Thomas Ploetz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13546",
    "paper_id": "2305.13546",
    "abstract": "\n        The recent success of neural networks as implicit representation of data has driven growing interest in neural functionals: models that can process other neural networks as input by operating directly over their weight spaces. Nevertheless, constructing expressive and efficient neural functional architectures that can handle high-dimensional weight-space objects remains challenging. This paper uses the attention mechanism to define a novel set of permutation equivariant weight-space layers and composes them into deep equivariant models called neural functional Transformers (NFTs). NFTs respect weight-space permutation symmetries while incorporating the advantages of attention, which have exhibited remarkable success across multiple domains. In experiments processing the weights of feedforward MLPs and CNNs, we find that NFTs match or exceed the performance of prior weight-space methods. We also leverage NFTs to develop Inr2Array, a novel method for computing permutation invariant latent representations from the weights of implicit neural representations (INRs). Our proposed method improves INR classification accuracy by up to +17%+17\\% over existing methods. We provide an implementation of our layers at https://github.com/AllanYangZhou/nfn.\n        \u25b3 Less\n      ",
    "title": "Neural Functional Transformers",
    "date": "22 May, 2023",
    "authors": [
      "Allan Zhou",
      " Kaien Yang",
      " Yiding Jiang",
      " Kaylee Burns",
      " Winnie Xu",
      " Samuel Sokota",
      " J. Zico Kolter",
      " Chelsea Finn"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.10055",
    "paper_id": "2205.10055",
    "abstract": "\n        Classification is often the first problem described in introductory machine learning classes. Generalization guarantees of classification have historically been offered by Vapnik-Chervonenkis theory. Yet those guarantees are based on intractable algorithms, which has led to the theory of surrogate methods in classification. Guarantees offered by surrogate methods are based on calibration inequalities, which have been shown to be highly sub-optimal under some margin conditions, failing short to capture exponential convergence phenomena. Those \"super\" fast rates are becoming to be well understood for smooth surrogates, but the picture remains blurry for non-smooth losses such as the hinge loss, associated with the renowned support vector machines. In this paper, we present a simple mechanism to obtain fast convergence rates and we investigate its usage for SVM. In particular, we show that SVM can exhibit exponential convergence rates even without assuming the hard Tsybakov margin condition.\n        \u25b3 Less\n      ",
    "title": "A Case of Exponential Convergence Rates for SVM",
    "date": "22 May, 2023",
    "authors": [
      "Vivien Cabannes",
      " Stefano Vigogna"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18323",
    "paper_id": "2305.18323",
    "abstract": "\n        Augmented Language Models (ALMs) blend the reasoning capabilities of Large Language Models (LLMs) with tools that allow for knowledge retrieval and action execution. Existing ALM systems trigger LLM thought processes while pulling observations from these tools in an interleaved fashion. Specifically, an LLM reasons to call an external tool, gets halted to fetch the tool's response, and then decides the next action based on all preceding response tokens. Such a paradigm, though straightforward and easy to implement, often leads to huge computation complexity from redundant prompts and repeated execution. This study addresses such challenges for the first time, proposing a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption. Comprehensive evaluations across six public NLP benchmarks and a curated dataset reveal consistent performance enhancements with our proposed methodology. Notably, ReWOO achieves 5x token efficiency and 4% accuracy improvement on HotpotQA, a multi-step reasoning benchmark. Furthermore, ReWOO demonstrates robustness under tool-failure scenarios. Beyond prompt efficiency, decoupling parametric modules from non-parametric tool calls enables instruction fine-tuning to offload LLMs into smaller language models, thus substantially reducing model parameters. Our illustrative work offloads reasoning ability from 175B GPT3.5 into 7B LLaMA, demonstrating the significant potential for truly efficient and scalable ALM systems.\n        \u25b3 Less\n      ",
    "title": "ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models",
    "date": "22 May, 2023",
    "authors": [
      "Binfeng Xu",
      " Zhiyuan Peng",
      " Bowen Lei",
      " Subhabrata Mukherjee",
      " Yuchen Liu",
      " Dongkuan Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.01754",
    "paper_id": "2306.01754",
    "abstract": "\n        Software vulnerabilities bear enterprises significant costs. Despite extensive efforts in research and development of software vulnerability detection methods, uncaught vulnerabilities continue to put software owners and users at risk. Many current vulnerability detection methods require that code snippets can compile and build before attempting detection. This, unfortunately, introduces a long latency between the time a vulnerability is injected to the time it is removed, which can substantially increases the cost of fixing a vulnerability. We recognize that the current advances in machine learning can be used to detect vulnerable code patterns on syntactically incomplete code snippets as the developer is writing the code at EditTime. In this paper we present a practical system that leverages deep learning on a large-scale data set of vulnerable code patterns to learn complex manifestations of more than 250 vulnerability types and detect vulnerable code patterns at EditTime. We discuss zero-shot, few-shot, and fine-tuning approaches on state of the art pre-trained Large Language Models (LLMs). We show that in comparison with state of the art vulnerability detection models our approach improves the state of the art by 10%. We also evaluate our approach to detect vulnerability in auto-generated code by code LLMs. Evaluation on a benchmark of high-risk code scenarios shows a reduction of up to 90% vulnerability reduction.\n        \u25b3 Less\n      ",
    "title": "Transformer-based Vulnerability Detection in Code at EditTime: Zero-shot, Few-shot, or Fine-tuning?",
    "date": "22 May, 2023",
    "authors": [
      "Aaron Chan",
      " Anant Kharkar",
      " Roshanak Zilouchian Moghaddam",
      " Yevhen Mohylevskyy",
      " Alec Helyar",
      " Eslam Kamal",
      " Mohamed Elkamhawy",
      " Neel Sundaresan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13584",
    "paper_id": "2305.13584",
    "abstract": "\n        Compared to traditional neural networks with a single exit, a multi-exit network has multiple exits that allow for early output from intermediate layers of the model, thus bringing significant improvement in computational efficiency while maintaining similar recognition accuracy. When attempting to steal such valuable models using traditional model stealing attacks, we found that conventional methods can only steal the model's classification function while failing to capture its output strategy. This results in a significant decrease in computational efficiency for the stolen substitute model, thereby losing the advantages of multi-exit networks.In this paper, we propose the first model stealing attack to extract both the model function and output strategy. We employ bayesian changepoint detection to analyze the target model's output strategy and use performance loss and strategy loss to guide the training of the substitute model. Furthermore, we designed a novel output strategy search algorithm that can find the optimal output strategy to maximize the consistency between the victim model and the substitute model's outputs. Through experiments on multiple mainstream multi-exit networks and benchmark datasets, we thoroughly demonstrates the effectiveness of our method.\n        \u25b3 Less\n      ",
    "title": "Model Stealing Attack against Multi-Exit Networks",
    "date": "22 May, 2023",
    "authors": [
      "Li Pan",
      " Lv Peizhuo",
      " Chen Kai",
      " Cai Yuling",
      " Xiang Fan",
      " Zhang Shengzhi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.01015",
    "paper_id": "2301.01015",
    "abstract": "\n        In this paper we explore the task of modeling semi-structured object sequences; in particular, we focus our attention on the problem of developing a structure-aware input representation for such sequences. Examples of such data include user activity on websites, machine logs, and many others. This type of data is often represented as a sequence of sets of key-value pairs over time and can present modeling challenges due to an ever-increasing sequence length. We propose a two-part approach, which first considers each key independently and encodes a representation of its values over time; we then self-attend over these value-aware key representations to accomplish a downstream task. This allows us to operate on longer object sequences than existing methods. We introduce a novel shared-attention-head architecture between the two modules and present an innovative training schedule that interleaves the training of both modules with shared weights for some attention heads. Our experiments on multiple prediction tasks using real-world data demonstrate that our approach outperforms a unified network with hierarchical encoding, as well as other methods including a record-centric representation and a flattened representation of the sequence.\n        \u25b3 Less\n      ",
    "title": "Semi-Structured Object Sequence Encoders",
    "date": "22 May, 2023",
    "authors": [
      "Rudra Murthy V",
      " Riyaz Bhat",
      " Chulaka Gunasekara",
      " Siva Sankalp Patel",
      " Hui Wan",
      " Tejas Indulal Dhamecha",
      " Danish Contractor",
      " Marina Danilevsky"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13623",
    "paper_id": "2305.13623",
    "abstract": "\n        The exponential growth of social media platforms, such as Facebook and TikTok, has revolutionized communication and content publication in human society. Users on these platforms can publish multimedia content that delivers information via the combination of text, audio, images, and video. Meanwhile, the multimedia content release facility has been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisements, and pornography. To this end, content moderation software has been widely deployed on these platforms to detect and blocks toxic content. However, due to the complexity of content moderation models and the difficulty of understanding information across multiple modalities, existing content moderation software can fail to detect toxic content, which often leads to extremely negative impacts.\n  We introduce Semantic Fusion, a general, effective methodology for validating multimedia content moderation software. Our key idea is to fuse two or more existing single-modal inputs (e.g., a textual sentence and an image) into a new input that combines the semantics of its ancestors in a novel manner and has toxic nature by construction. This fused input is then used for validating multimedia content moderation software. We realized Semantic Fusion as DUO, a practical content moderation software testing tool. In our evaluation, we employ DUO to test five commercial content moderation software and two state-of-the-art models against three kinds of toxic content. The results show that DUO achieves up to 100% error finding rate (EFR) when testing moderation software. In addition, we leverage the test cases generated by DUO to retrain the two models we explored, which largely improves model robustness while maintaining the accuracy on the original test set.\n        \u25b3 Less\n      ",
    "title": "Validating Multimedia Content Moderation Software via Semantic Fusion",
    "date": "22 May, 2023",
    "authors": [
      "Wenxuan Wang",
      " Jingyuan Huang",
      " Chang Chen",
      " Jiazhen Gu",
      " Jianping Zhang",
      " Weibin Wu",
      " Pinjia He",
      " Michael Lyu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13634",
    "paper_id": "2305.13634",
    "abstract": "\n        The increasing maturity of big data applications has led to a proliferation of models targeting the same objectives within the same scenarios and datasets. However, selecting the most suitable model that considers model's features while taking specific requirements and constraints into account still poses a significant challenge. Existing methods have focused on worker-task assignments based on crowdsourcing, they neglect the scenario-dataset-model assignment problem. To address this challenge, a new problem named the Scenario-based Optimal Model Assignment (SOMA) problem is introduced and a novel framework entitled Scenario and Model Associative percepts (SMAP) is developed. SMAP is a heterogeneous information framework that can integrate various types of information to intelligently select a suitable dataset and allocate the optimal model for a specific scenario. To comprehensively evaluate models, a new score function that utilizes multi-head attention mechanisms is proposed. Moreover, a novel memory mechanism named the mnemonic center is developed to store the matched heterogeneous information and prevent duplicate matching. Six popular traffic scenarios are selected as study cases and extensive experiments are conducted on a dataset to verify the effectiveness and efficiency of SMAP and the score function.\n        \u25b3 Less\n      ",
    "title": "SMAP: A Novel Heterogeneous Information Framework for Scenario-based Optimal Model Assignment",
    "date": "22 May, 2023",
    "authors": [
      "Zekun Qiu",
      " Zhipu Xie",
      " Zehua Ji",
      " Yuhao Mao",
      " Ke Cheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18324",
    "paper_id": "2305.18324",
    "abstract": "\n        A common way to use large pre-trained language models for downstream tasks is to fine tune them using additional layers. This may not work well if downstream domain is a specialized domain whereas the large language model has been pre-trained on a generic corpus. In this paper, we discuss the use of regular expression patterns employed as features for domain knowledge during the process of fine tuning, in addition to domain specific text. Our experiments on real scenario production data show that this method of fine tuning improves the downstream text classification tasks as compared to fine tuning only on domain specific text. We also show that the use of attention network for fine tuning improves results compared to simple linear layers.\n        \u25b3 Less\n      ",
    "title": "Regex-augmented Domain Transfer Topic Classification based on a Pre-trained Language Model: An application in Financial Domain",
    "date": "22 May, 2023",
    "authors": [
      "Vanessa Liao",
      " Syed Shariyar Murtaza",
      " Yifan Nie",
      " Jimmy Lin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.12599",
    "paper_id": "2207.12599",
    "abstract": "\n        Graph neural networks (GNNs) have demonstrated a significant boost in prediction performance on graph data. At the same time, the predictions made by these models are often hard to interpret. In that regard, many efforts have been made to explain the prediction mechanisms of these models from perspectives such as GNNExplainer, XGNN and PGExplainer. Although such works present systematic frameworks to interpret GNNs, a holistic review for explainable GNNs is unavailable. In this survey, we present a comprehensive review of explainability techniques developed for GNNs. We focus on explainable graph neural networks and categorize them based on the use of explainable methods. We further provide the common performance metrics for GNNs explanations and point out several future research directions.\n        \u25b3 Less\n      ",
    "title": "A Survey of Explainable Graph Neural Networks: Taxonomy and Evaluation Metrics",
    "date": "22 May, 2023",
    "authors": [
      "Yiqiao Li",
      " Jianlong Zhou",
      " Sunny Verma",
      " Fang Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13648",
    "paper_id": "2305.13648",
    "abstract": "\n        Non-parametric, k-nearest-neighbor algorithms have recently made inroads to assist generative models such as language models and machine translation decoders. We explore whether such non-parametric models can improve machine translation models at the fine-tuning stage by incorporating statistics from the kNN predictions to inform the gradient updates for a baseline translation model. There are multiple methods which could be used to incorporate kNN statistics and we investigate gradient scaling by a gating mechanism, the kNN's ground truth probability, and reinforcement learning. For four standard in-domain machine translation datasets, compared with classic fine-tuning, we report consistent improvements of all of the three methods by as much as 1.45 BLEU and 1.28 BLEU for German-English and English-German translations respectively. Through qualitative analysis, we found particular improvements when it comes to translating grammatical relations or function words, which results in increased fluency of our model.\n        \u25b3 Less\n      ",
    "title": "Non-parametric, Nearest-neighbor-assisted Fine-tuning for Neural Machine Translation",
    "date": "22 May, 2023",
    "authors": [
      "Jiayi Wang",
      " Ke Wang",
      " Yuqi Zhang",
      " Yu Zhao",
      " Pontus Stenetorp"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13665",
    "paper_id": "2305.13665",
    "abstract": "\n        The use of deep neural networks in real-world applications require well-calibrated networks with confidence scores that accurately reflect the actual probability. However, it has been found that these networks often provide over-confident predictions, which leads to poor calibration. Recent efforts have sought to address this issue by focal loss to reduce over-confidence, but this approach can also lead to under-confident predictions. While different variants of focal loss have been explored, it is difficult to find a balance between over-confidence and under-confidence. In our work, we propose a new loss function by focusing on dual logits. Our method not only considers the ground truth logit, but also take into account the highest logit ranked after the ground truth logit. By maximizing the gap between these two logits, our proposed dual focal loss can achieve a better balance between over-confidence and under-confidence. We provide theoretical evidence to support our approach and demonstrate its effectiveness through evaluations on multiple models and datasets, where it achieves state-of-the-art performance. Code is available at https://github.com/Linwei94/DualFocalLoss\n        \u25b3 Less\n      ",
    "title": "Dual Focal Loss for Calibration",
    "date": "22 May, 2023",
    "authors": [
      "Linwei Tao",
      " Minjing Dong",
      " Chang Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.12901",
    "paper_id": "2301.12901",
    "abstract": "\n        Urban air mobility (UAM) has the potential to revolutionize transportation in metropolitan areas, providing a new mode of transportation that could alleviate congestion and improve accessibility. However, the integration of UAM into existing transportation systems is a complex task that requires a thorough understanding of its impact on traffic flow and capacity. In this paper, we conduct a survey to investigate the current state of research on UAM in metropolitan-scale traffic using simulation techniques. We identify key challenges and opportunities for the integration of UAM into urban transportation systems, including impacts on existing traffic patterns and congestion; safety analysis and risk assessment; potential economic and environmental benefits; and the development of shared infrastructure and routes for UAM and ground-based transportation. We also discuss the potential benefits of UAM, such as reduced travel times and improved accessibility for underserved areas. Our survey provides a comprehensive overview of the current state of research on UAM in metropolitan-scale traffic using simulation and highlights key areas for future research and development.\n        \u25b3 Less\n      ",
    "title": "Simulating the Integration of Urban Air Mobility into Existing Transportation Systems: A Survey",
    "date": "22 May, 2023",
    "authors": [
      "Xuan Jiang",
      " Yuhan Tang",
      " Zhiyi Tang",
      " Junzhe Cao",
      " Vishwanath Bulusu",
      " Xin Peng",
      " Cristian Poliziani",
      " Raja Sengupta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11876",
    "paper_id": "2305.11876",
    "abstract": "\n        The Internet revolution in 1990, followed by the data-driven and information revolution, has transformed the world as we know it. Nowadays, what seam to be 10 to 20 years ago, a science fiction idea (i.e., machines dominating the world) is seen as possible. This revolution also brought a need for new regulatory practices where user trust and artificial Intelligence (AI) discourse has a central role. This work aims to clarify some misconceptions about user trust in AI discourse and fight the tendency to design vulnerable interactions that lead to further breaches of trust, both real and perceived. Findings illustrate the lack of clarity in understanding user trust and its effects on computer science, especially in measuring user trust characteristics. It argues for clarifying those notions to avoid possible trust gaps and misinterpretations in AI adoption and appropriation.\n        \u25b3 Less\n      ",
    "title": "Challenges and Trends in User Trust Discourse in AI",
    "date": "22 May, 2023",
    "authors": [
      "Sonia Sousa",
      " Jose Cravino",
      " Paulo Martins"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13689",
    "paper_id": "2305.13689",
    "abstract": "\n        Although supervised learning has been highly successful in improving the state-of-the-art in the domain of image-based computer vision in the past, the margin of improvement has diminished significantly in recent years, indicating that a plateau is in sight. Meanwhile, the use of self-supervised learning (SSL) for the purpose of natural language processing (NLP) has seen tremendous successes during the past couple of years, with this new learning paradigm yielding powerful language models. Inspired by the excellent results obtained in the field of NLP, self-supervised methods that rely on clustering, contrastive learning, distillation, and information-maximization, which all fall under the banner of discriminative SSL, have experienced a swift uptake in the area of computer vision. Shortly afterwards, generative SSL frameworks that are mostly based on masked image modeling, complemented and surpassed the results obtained with discriminative SSL. Consequently, within a span of three years, over 100100 unique general-purpose frameworks for generative and discriminative SSL, with a focus on imaging, were proposed. In this survey, we review a plethora of research efforts conducted on image-oriented SSL, providing a historic view and paying attention to best practices as well as useful software packages. While doing so, we discuss pretext tasks for image-based SSL, as well as techniques that are commonly used in image-based SSL. Lastly, to aid researchers who aim at contributing to image-focused SSL, we outline a number of promising research directions.\n        \u25b3 Less\n      ",
    "title": "Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training",
    "date": "22 May, 2023",
    "authors": [
      "Utku Ozbulak",
      " Hyun Jung Lee",
      " Beril Boga",
      " Esla Timothy Anzaku",
      " Homin Park",
      " Arnout Van Messem",
      " Wesley De Neve",
      " Joris Vankerschaver"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13711",
    "paper_id": "2305.13711",
    "abstract": "\n        We propose LLM-Eval, a unified multi-dimensional automatic evaluation method for open-domain conversations with large language models (LLMs). Existing evaluation methods often rely on human annotations, ground-truth responses, or multiple LLM prompts, which can be expensive and time-consuming. To address these issues, we design a single prompt-based evaluation method that leverages a unified evaluation schema to cover multiple dimensions of conversation quality in a single model call. We extensively evaluate the performance of LLM-Eval on various benchmark datasets, demonstrating its effectiveness, efficiency, and adaptability compared to state-of-the-art evaluation methods. Our analysis also highlights the importance of choosing suitable LLMs and decoding strategies for accurate evaluation results. LLM-Eval offers a versatile and robust solution for evaluating open-domain conversation systems, streamlining the evaluation process and providing consistent performance across diverse scenarios.\n        \u25b3 Less\n      ",
    "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
    "date": "22 May, 2023",
    "authors": [
      "Yen-Ting Lin",
      " Yun-Nung Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13712",
    "paper_id": "2305.13712",
    "abstract": "\n        This paper investigates the capabilities of Large Language Models (LLMs) in the context of understanding their own knowledge and measuring their uncertainty. We argue this is an important feature for mitigating hallucinations. Specifically, we focus on addressing \\textit{known-unknown} questions, characterized by high uncertainty due to the absence of definitive answers. To facilitate our study, we collect a dataset with new Known-Unknown Questions (KUQ) and propose a novel categorization scheme to elucidate the sources of uncertainty. Subsequently, we assess the LLMs' ability to differentiate between known and unknown questions and classify them accordingly. Moreover, we evaluate the quality of their answers in an Open-Ended QA setting. To quantify the uncertainty expressed in the answers, we create a semantic evaluation method that measures the model's accuracy in expressing uncertainty between known vs unknown questions.\n        \u25b3 Less\n      ",
    "title": "Knowledge of Knowledge: Exploring Known-Unknowns Uncertainty with Large Language Models",
    "date": "22 May, 2023",
    "authors": [
      "Alfonso Amayuelas",
      " Liangming Pan",
      " Wenhu Chen",
      " William Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14394",
    "paper_id": "2305.14394",
    "abstract": "\n        In this study, we build a computational model of Prefrontal Cortex (PFC) using Spiking Neural Networks (SNN) to understand how neurons adapt and respond to tasks switched under short and longer duration of stimulus changes. We also explore behavioral deficits arising out of the PFC lesions by simulating lesioned states in our Spiking architecture model. Although there are some computational models of the PFC, SNN's have not been used to model them. In this study, we use SNN's having parameters close to biologically plausible values and train the model using unsupervised Spike Timing Dependent Plasticity (STDP) learning rule. Our model is based on connectionist architectures and exhibits neural phenomena like sustained activity which helps in generating short-term or working memory. We use these features to simulate lesions by deactivating synaptic pathways and record the weight adjustments of learned patterns and capture the accuracy of learning tasks in such conditions. All our experiments are trained and recorded using a real-world Fashion MNIST (FMNIST) dataset and through this work, we bridge the gap between bio-realistic models and those that perform well in pattern recognition tasks\n        \u25b3 Less\n      ",
    "title": "Unsupervised Spiking Neural Network Model of Prefrontal Cortex to study Task Switching with Synaptic deficiency",
    "date": "22 May, 2023",
    "authors": [
      "Ashwin Viswanathan Kannan",
      " Goutam Mylavarapu",
      " Johnson P Thomas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14395",
    "paper_id": "2305.14395",
    "abstract": "\n        Originally inspired by game-theory, path attribution framework stands out among the post-hoc model interpretation tools due to its axiomatic nature. However, recent developments show that this framework can still suffer from counter-intuitive results. Moreover, specifically for deep visual models, the existing path-based methods also fall short on conforming to the original intuitions that are the basis of the claimed axiomatic properties of this framework. We address these problems with a systematic investigation, and pinpoint the conditions in which the counter-intuitive results can be avoided for deep visual model interpretation with the path attribution strategy. We also devise a scheme to preclude the conditions in which visual model interpretation can invalidate the axiomatic properties of path attribution. These insights are combined into a method that enables reliable visual model interpretation. Our findings are establish empirically with multiple datasets, models and evaluation metrics. Extensive experiments show a consistent performance gain of our method over the baselines.\n        \u25b3 Less\n      ",
    "title": "Towards credible visual model interpretation with path attribution",
    "date": "22 May, 2023",
    "authors": [
      "Naveed Akhtar",
      " Muhammad A. A. K. Jalwana"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13729",
    "paper_id": "2305.13729",
    "abstract": "\n        Re-rankers, which order retrieved documents with respect to the relevance score on the given query, have gained attention for the information retrieval (IR) task. Rather than fine-tuning the pre-trained language model (PLM), the large-scale language model (LLM) is utilized as a zero-shot re-ranker with excellent results. While LLM is highly dependent on the prompts, the impact and the optimization of the prompts for the zero-shot re-ranker are not explored yet. Along with highlighting the impact of optimization on the zero-shot re-ranker, we propose a novel discrete prompt optimization method, Constrained Prompt generation (Co-Prompt), with the metric estimating the optimum for re-ranking. Co-Prompt guides the generated texts from PLM toward optimal prompts based on the metric without parameter update. The experimental results demonstrate that Co-Prompt leads to outstanding re-ranking performance against the baselines. Also, Co-Prompt generates more interpretable prompts for humans against other prompt optimization methods.\n        \u25b3 Less\n      ",
    "title": "Discrete Prompt Optimization via Constrained Generation for Zero-shot Re-ranker",
    "date": "22 May, 2023",
    "authors": [
      "Sukmin Cho",
      " Soyeong Jeong",
      " Jeongyeon Seo",
      " Jong C. Park"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13738",
    "paper_id": "2305.13738",
    "abstract": "\n        Artificial General Intelligence (AGI) requires comprehensive understanding and generation capabilities for a variety of tasks spanning different modalities and functionalities. Integrative AI is one important direction to approach AGI, through combining multiple models to tackle complex multimodal tasks. However, there is a lack of a flexible and composable platform to facilitate efficient and effective model composition and coordination. In this paper, we propose the i-Code Studio, a configurable and composable framework for Integrative AI. The i-Code Studio orchestrates multiple pre-trained models in a finetuning-free fashion to conduct complex multimodal tasks. Instead of simple model composition, the i-Code Studio provides an integrative, flexible, and composable setting for developers to quickly and easily compose cutting-edge services and technologies tailored to their specific requirements. The i-Code Studio achieves impressive results on a variety of zero-shot multimodal tasks, such as video-to-text retrieval, speech-to-speech translation, and visual question answering. We also demonstrate how to quickly build a multimodal agent based on the i-Code Studio that can communicate and personalize for users.\n        \u25b3 Less\n      ",
    "title": "i-Code Studio: A Configurable and Composable Framework for Integrative AI",
    "date": "22 May, 2023",
    "authors": [
      "Yuwei Fang",
      " Mahmoud Khademi",
      " Chenguang Zhu",
      " Ziyi Yang",
      " Reid Pryzant",
      " Yichong Xu",
      " Yao Qian",
      " Takuya Yoshioka",
      " Lu Yuan",
      " Michael Zeng",
      " Xuedong Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13741",
    "paper_id": "2305.13741",
    "abstract": "\n        Tasks that involve interaction with various targets are called multi-target tasks. When applying general reinforcement learning approaches for such tasks, certain targets that are difficult to access or interact with may be neglected throughout the course of training - a predicament we call Under-explored Target Problem (UTP). To address this problem, we propose L-SA (Learning by adaptive Sampling and Active querying) framework that includes adaptive sampling and active querying. In the L-SA framework, adaptive sampling dynamically samples targets with the highest increase of success rates at a high proportion, resulting in curricular learning from easy to hard targets. Active querying prompts the agent to interact more frequently with under-explored targets that need more experience or exploration. Our experimental results on visual navigation tasks show that the L-SA framework improves sample efficiency as well as success rates on various multi-target tasks with UTP. Also, it is experimentally demonstrated that the cyclic relationship between adaptive sampling and active querying effectively improves the sample richness of under-explored targets and alleviates UTP.\n        \u25b3 Less\n      ",
    "title": "L-SA: Learning Under-Explored Targets in Multi-Target Reinforcement Learning",
    "date": "22 May, 2023",
    "authors": [
      "Kibeom Kim",
      " Hyundo Lee",
      " Min Whoo Lee",
      " Moonheon Lee",
      " Minsu Lee",
      " Byoung-Tak Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.15185",
    "paper_id": "2210.15185",
    "abstract": "\n        Model-based reinforcement learning (MBRL) is recognized with the potential to be significantly more sample-efficient than model-free RL. How an accurate model can be developed automatically and efficiently from raw sensory inputs (such as images), especially for complex environments and tasks, is a challenging problem that hinders the broad application of MBRL in the real world. In this work, we propose a sensing-aware model-based reinforcement learning system called SAM-RL. Leveraging the differentiable physics-based simulation and rendering, SAM-RL automatically updates the model by comparing rendered images with real raw images and produces the policy efficiently. With the sensing-aware learning pipeline, SAM-RL allows a robot to select an informative viewpoint to monitor the task process. We apply our framework to real world experiments for accomplishing three manipulation tasks: robotic assembly, tool manipulation, and deformable object manipulation. We demonstrate the effectiveness of SAM-RL via extensive experiments. Videos are available on our project webpage at https://sites.google.com/view/rss-sam-rl.\n        \u25b3 Less\n      ",
    "title": "SAM-RL: Sensing-Aware Model-Based Reinforcement Learning via Differentiable Physics-Based Simulation and Rendering",
    "date": "22 May, 2023",
    "authors": [
      "Jun Lv",
      " Yunhai Feng",
      " Cheng Zhang",
      " Shuang Zhao",
      " Lin Shao",
      " Cewu Lu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12987",
    "paper_id": "2305.12987",
    "abstract": "\n        This paper details the process of developing the first native large generative language model for the Nordic languages, GPT-SW3. We cover all parts of the development process, from data collection and processing, training configuration and instruction finetuning, to evaluation and considerations for release strategies. We hope that this paper can serve as a guide and reference for other researchers that undertake the development of large generative models for smaller languages.\n        \u25b3 Less\n      ",
    "title": "GPT-SW3: An Autoregressive Language Model for the Nordic Languages",
    "date": "22 May, 2023",
    "authors": [
      "Ariel Ekgren",
      " Amaru Cuba Gyllensten",
      " Felix Stollenwerk",
      " Joey \u00d6hman",
      " Tim Isbister",
      " Evangelia Gogoulou",
      " Fredrik Carlsson",
      " Alice Heiman",
      " Judit Casademont",
      " Magnus Sahlgren"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.07285",
    "paper_id": "2306.07285",
    "abstract": "\n        Code pre-trained models (CodePTMs) have recently demonstrated a solid capacity to process various software intelligence tasks, e.g., code clone detection, code translation, and code summarization. The current mainstream method that deploys these models to downstream tasks is to fine-tune them on individual tasks, which is generally costly and needs sufficient data for large models. To tackle the issue, in this paper, we present TransCoder, a unified Transferable fine-tuning strategy for Code representation learning. Inspired by human inherent skills of knowledge generalization, TransCoder drives the model to learn better code-related meta-knowledge like human programmers. Specifically, we employ a tunable prefix encoder as the meta-learner to capture cross-task and cross-language transferable knowledge, respectively. Besides, tasks with minor training sample sizes and languages with small corpus can be remarkably benefited from our approach. Extensive experiments conducted on benchmark datasets clearly demonstrate that our method can lead to superior performance on various code-related tasks and encourage mutual reinforcement. We also show that TransCoder is applicable in low-resource scenarios.\n        \u25b3 Less\n      ",
    "title": "TransCoder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills",
    "date": "22 May, 2023",
    "authors": [
      "Qiushi Sun",
      " Nuo Chen",
      " Jianing Wang",
      " Xiang Li",
      " Ming Gao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04001",
    "paper_id": "2305.04001",
    "abstract": "\n        Recent advances in diffusion models have showcased promising results in the text-to-video (T2V) synthesis task. However, as these T2V models solely employ text as the guidance, they tend to struggle in modeling detailed temporal dynamics. In this paper, we introduce a novel T2V framework that additionally employ audio signals to control the temporal dynamics, empowering an off-the-shelf T2I diffusion to generate audio-aligned videos. We propose audio-based regional editing and signal smoothing to strike a good balance between the two contradicting desiderata of video synthesis, i.e., temporal flexibility and coherence. We empirically demonstrate the effectiveness of our method through experiments, and further present practical applications for contents creation.\n        \u25b3 Less\n      ",
    "title": "AADiff: Audio-Aligned Video Synthesis with Text-to-Image Diffusion",
    "date": "22 May, 2023",
    "authors": [
      "Seungwoo Lee",
      " Chaerin Kong",
      " Donghyeon Jeon",
      " Nojun Kwak"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13755",
    "paper_id": "2305.13755",
    "abstract": "\n        Discourse parsing, the task of analyzing the internal rhetorical structure of texts, is a challenging problem in natural language processing. Despite the recent advances in neural models, the lack of large-scale, high-quality corpora for training remains a major obstacle. Recent studies have attempted to overcome this limitation by using distant supervision, which utilizes results from other NLP tasks (e.g., sentiment polarity, attention matrix, and segmentation probability) to parse discourse trees. However, these methods do not take into account the differences between in-domain and out-of-domain tasks, resulting in lower performance and inability to leverage the high-quality in-domain data for further improvement. To address these issues, we propose a distant supervision framework that leverages the relations between topic structure and rhetorical structure. Specifically, we propose two distantly supervised methods, based on transfer learning and the teacher-student model, that narrow the gap between in-domain and out-of-domain tasks through label mapping and oracle annotation. Experimental results on the MCDTB and RST-DT datasets show that our methods achieve the best performance in both distant-supervised and supervised scenarios.\n        \u25b3 Less\n      ",
    "title": "Topic-driven Distant Supervision Framework for Macro-level Discourse Parsing",
    "date": "22 May, 2023",
    "authors": [
      "Feng Jiang",
      " Longwang He",
      " Peifeng Li",
      " Qiaoming Zhu",
      " Haizhou Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13765",
    "paper_id": "2305.13765",
    "abstract": "\n        Person identification is a problem that has received substantial attention, particularly in security domains. Gait recognition is one of the most convenient approaches enabling person identification at a distance without the need of high-quality images. There are several review studies addressing person identification such as the utilization of facial images, silhouette images, and wearable sensor. Despite skeleton-based person identification gaining popularity while overcoming the challenges of traditional approaches, existing survey studies lack the comprehensive review of skeleton-based approaches to gait identification. We present a detailed review of the human pose estimation and gait analysis that make the skeleton-based approaches possible. The study covers various types of related datasets, tools, methodologies, and evaluation metrics with associated challenges, limitations, and application domains. Detailed comparisons are presented for each of these aspects with recommendations for potential research and alternatives. A common trend throughout this paper is the positive impact that deep learning techniques are beginning to have on topics such as human pose estimation and gait identification. The survey outcomes might be useful for the related research community and other stakeholders in terms of performance analysis of existing methodologies, potential research gaps, application domains, and possible contributions in the future.\n        \u25b3 Less\n      ",
    "title": "Human Body Pose Estimation for Gait Identification: A Comprehensive Survey of Datasets and Models",
    "date": "22 May, 2023",
    "authors": [
      "Luke K. Topham",
      " Wasiq Khan",
      " Dhiya Al-Jumeily",
      " Abir Hussain"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13774",
    "paper_id": "2305.13774",
    "abstract": "\n        Audio deepfake detection is an emerging topic in the artificial intelligence community. The second Audio Deepfake Detection Challenge (ADD 2023) aims to spur researchers around the world to build new innovative technologies that can further accelerate and foster research on detecting and analyzing deepfake speech utterances. Different from previous challenges (e.g. ADD 2022), ADD 2023 focuses on surpassing the constraints of binary real/fake classification, and actually localizing the manipulated intervals in a partially fake speech as well as pinpointing the source responsible for generating any fake audio. Furthermore, ADD 2023 includes more rounds of evaluation for the fake audio game sub-challenge. The ADD 2023 challenge includes three subchallenges: audio fake game (FG), manipulation region location (RL) and deepfake algorithm recognition (AR). This paper describes the datasets, evaluation metrics, and protocols. Some findings are also reported in audio deepfake detection tasks.\n        \u25b3 Less\n      ",
    "title": "ADD 2023: the Second Audio Deepfake Detection Challenge",
    "date": "23 May, 2023",
    "authors": [
      "Jiangyan Yi",
      " Jianhua Tao",
      " Ruibo Fu",
      " Xinrui Yan",
      " Chenglong Wang",
      " Tao Wang",
      " Chu Yuan Zhang",
      " Xiaohui Zhang",
      " Yan Zhao",
      " Yong Ren",
      " Le Xu",
      " Junzuo Zhou",
      " Hao Gu",
      " Zhengqi Wen",
      " Shan Liang",
      " Zheng Lian",
      " Shuai Nie",
      " Haizhou Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13775",
    "paper_id": "2305.13775",
    "abstract": "\n        Many recent language models (LMs) of Transformers family exhibit so-called in-context learning (ICL) ability, manifested in the LMs' ability to modulate their function by a task described in a natural language input. Previous work curating these models assumes that ICL emerges from vast over-parametrization or the scale of multi-task training. However, a complementary branch of recent theoretical work attributes ICL emergence to specific properties of training data and creates functional in-context learners in small-scale, synthetic settings.\n  Inspired by recent findings on data properties driving the emergence of ICL, we propose a method to create LMs able to better utilize the in-context information, by constructing training scenarios where it is beneficial for the LM to capture the analogical reasoning concepts. We measure that data sampling of Concept-aware Training (CoAT) consistently improves models' reasoning ability. As a result, the in-context learners trained with CoAT on only two datasets of a single (QA) task perform comparably to larger models trained on 1600+ tasks.\n        \u25b3 Less\n      ",
    "title": "Concept-aware Training Improves In-context Learning Ability of Language Models",
    "date": "23 May, 2023",
    "authors": [
      "Michal \u0160tef\u00e1nik",
      " Marek Kadl\u010d\u00edk"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13776",
    "paper_id": "2305.13776",
    "abstract": "\n        Counterspeech has been demonstrated to be an efficacious approach for combating hate speech. While various conventional and controlled approaches have been studied in recent years to generate counterspeech, a counterspeech with a certain intent may not be sufficient in every scenario. Due to the complex and multifaceted nature of hate speech, utilizing multiple forms of counter-narratives with varying intents may be advantageous in different circumstances. In this paper, we explore intent-conditioned counterspeech generation. At first, we develop IntentCONAN, a diversified intent-specific counterspeech dataset with 6831 counterspeeches conditioned on five intents, i.e., informative, denouncing, question, positive, and humour. Subsequently, we propose QUARC, a two-stage framework for intent-conditioned counterspeech generation. QUARC leverages vector-quantized representations learned for each intent category along with PerFuMe, a novel fusion module to incorporate intent-specific information into the model. Our evaluation demonstrates that QUARC outperforms several baselines by an average of 10% across evaluation metrics. An extensive human evaluation supplements our hypothesis of better and more appropriate responses than comparative systems.\n        \u25b3 Less\n      ",
    "title": "Counterspeeches up my sleeve! Intent Distribution Learning and Persistent Fusion for Intent-Conditioned Counterspeech Generation",
    "date": "23 May, 2023",
    "authors": [
      "Rishabh Gupta",
      " Shaily Desai",
      " Manvi Goel",
      " Anil Bandhakavi",
      " Tanmoy Chakraborty",
      " Md. Shad Akhtar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13796",
    "paper_id": "2305.13796",
    "abstract": "\n        We propose SE-Bridge, a novel method for speech enhancement (SE). After recently applying the diffusion models to speech enhancement, we can achieve speech enhancement by solving a stochastic differential equation (SDE). Each SDE corresponds to a probabilistic flow ordinary differential equation (PF-ODE), and the trajectory of the PF-ODE solution consists of the speech states at different moments. Our approach is based on consistency model that ensure any speech states on the same PF-ODE trajectory, correspond to the same initial state. By integrating the Brownian Bridge process, the model is able to generate high-intelligibility speech samples without adversarial training. This is the first attempt that applies the consistency models to SE task, achieving state-of-the-art results in several metrics while saving 15 x the time required for sampling compared to the diffusion-based baseline. Our experiments on multiple datasets demonstrate the effectiveness of SE-Bridge in SE. Furthermore, we show through extensive experiments on downstream tasks, including Automatic Speech Recognition (ASR) and Speaker Verification (SV), that SE-Bridge can effectively support multiple downstream tasks.\n        \u25b3 Less\n      ",
    "title": "SE-Bridge: Speech Enhancement with Consistent Brownian Bridge",
    "date": "23 May, 2023",
    "authors": [
      "Zhibin Qiu",
      " Mengfan Fu",
      " Fuchun Sun",
      " Gulila Altenbek",
      " Hao Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13803",
    "paper_id": "2305.13803",
    "abstract": "\n        Existing feature distillation methods commonly adopt the One-to-one Representation Matching between any pre-selected teacher-student layer pair. In this paper, we present N-to-One Representation (NORM), a new two-stage knowledge distillation method, which relies on a simple Feature Transform (FT) module consisting of two linear layers. In view of preserving the intact information learnt by the teacher network, during training, our FT module is merely inserted after the last convolutional layer of the student network. The first linear layer projects the student representation to a feature space having N times feature channels than the teacher representation from the last convolutional layer, and the second linear layer contracts the expanded output back to the original feature space. By sequentially splitting the expanded student representation into N non-overlapping feature segments having the same number of feature channels as the teacher's, they can be readily forced to approximate the intact teacher representation simultaneously, formulating a novel many-to-one representation matching mechanism conditioned on a single teacher-student layer pair. After training, such an FT module will be naturally merged into the subsequent fully connected layer thanks to its linear property, introducing no extra parameters or architectural modifications to the student network at inference. Extensive experiments on different visual recognition benchmarks demonstrate the leading performance of our method. For instance, the ResNet18|MobileNet|ResNet50-1/4 model trained by NORM reaches 72.14%|74.26%|68.03% top-1 accuracy on the ImageNet dataset when using a pre-trained ResNet34|ResNet50|ResNet50 model as the teacher, achieving an absolute improvement of 2.01%|4.63%|3.03% against the individually trained counterpart. Code is available at https://github.com/OSVAI/NORM\n        \u25b3 Less\n      ",
    "title": "NORM: Knowledge Distillation via N-to-One Representation Matching",
    "date": "23 May, 2023",
    "authors": [
      "Xiaolong Liu",
      " Lujun Li",
      " Chao Li",
      " Anbang Yao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13821",
    "paper_id": "2305.13821",
    "abstract": "\n        Introduction: The COVID-19 pandemic highlighted the importance of making epidemiological data and scientific insights easily accessible and explorable for public health agencies, the general public, and researchers. State-of-the-art approaches for sharing data and insights included regularly updated reports and web dashboards. However, they face a trade-off between the simplicity and flexibility of data exploration. With the capabilities of recent large language models (LLMs) such as GPT-4, this trade-off can be overcome.\n  Results: We developed the chatbot \"GenSpectrum Chat\" (https://cov-spectrum.org/chat) which uses GPT-4 as the underlying large language model (LLM) to explore SARS-CoV-2 genomic sequencing data. Out of 500 inputs from real-world users, the chatbot provided a correct answer for 453 prompts; an incorrect answer for 13 prompts, and no answer although the question was within scope for 34 prompts. We also tested the chatbot with inputs from 10 different languages, and despite being provided solely with English instructions and examples, it successfully processed prompts in all tested languages.\n  Conclusion: LLMs enable new ways of interacting with information systems. In the field of public health, GenSpectrum Chat can facilitate the analysis of real-time pathogen genomic data. With our chatbot supporting interactive exploration in different languages, we envision quick and direct access to the latest evidence for policymakers around the world.\n        \u25b3 Less\n      ",
    "title": "GenSpectrum Chat: Data Exploration in Public Health Using Large Language Models",
    "date": "23 May, 2023",
    "authors": [
      "Chaoran Chen",
      " Tanja Stadler"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13824",
    "paper_id": "2305.13824",
    "abstract": "\n        As one of the core parts of flexible manufacturing systems, material handling involves storage and transportation of materials between workstations with automated vehicles. The improvement in material handling can impulse the overall efficiency of the manufacturing system. However, the occurrence of dynamic events during the optimisation of task arrangements poses a challenge that requires adaptability and effectiveness. In this paper, we aim at the scheduling of automated guided vehicles for dynamic material handling. Motivated by some real-world scenarios, unknown new tasks and unexpected vehicle breakdowns are regarded as dynamic events in our problem. We formulate the problem as a constrained Markov decision process which takes into account tardiness and available vehicles as cumulative and instantaneous constraints, respectively. An adaptive constrained reinforcement learning algorithm that combines Lagrangian relaxation and invalid action masking, named RCPOM, is proposed to address the problem with two hybrid constraints. Moreover, a gym-like dynamic material handling simulator, named DMH-GYM, is developed and equipped with diverse problem instances, which can be used as benchmarks for dynamic material handling. Experimental results on the problem instances demonstrate the outstanding performance of our proposed approach compared with eight state-of-the-art constrained and non-constrained reinforcement learning algorithms, and widely used dispatching rules for material handling.\n        \u25b3 Less\n      ",
    "title": "Constrained Reinforcement Learning for Dynamic Material Handling",
    "date": "23 May, 2023",
    "authors": [
      "Chengpeng Hu",
      " Ziming Wang",
      " Jialin Liu",
      " Junyi Wen",
      " Bifei Mao",
      " Xin Yao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2202.06658",
    "paper_id": "2202.06658",
    "abstract": "\n        Ensemble methods are commonly used to enhance the generalization performance of machine learning models. However, they present a challenge in deep learning systems due to the high computational overhead required to train an ensemble of deep neural networks (DNNs). Recent advancements such as fast geometric ensembling (FGE) and snapshot ensembles have addressed this issue by training model ensembles in the same time as a single model. Nonetheless, these techniques still require additional memory for test-time inference compared to single-model-based methods. In this paper, we propose a new method called parsimonious FGE (PFGE), which employs a lightweight ensemble of higher-performing DNNs generated through successive stochastic weight averaging procedures. Our experimental results on CIFAR-{10,100} and ImageNet datasets across various modern DNN architectures demonstrate that PFGE achieves 5x memory efficiency compared to previous methods, without compromising on generalization performance. For those interested, our code is available at https://github.com/ZJLAB-AMMI/PFGE.\n        \u25b3 Less\n      ",
    "title": "PFGE: Parsimonious Fast Geometric Ensembling of DNNs",
    "date": "23 May, 2023",
    "authors": [
      "Hao Guo",
      " Jiyong Jin",
      " Bin Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.12760",
    "paper_id": "2210.12760",
    "abstract": "\n        Uncertainty quantification is a central challenge in reliable and trustworthy machine learning. Naive measures such as last-layer scores are well-known to yield overconfident estimates in the context of overparametrized neural networks. Several methods, ranging from temperature scaling to different Bayesian treatments of neural networks, have been proposed to mitigate overconfidence, most often supported by the numerical observation that they yield better calibrated uncertainty measures. In this work, we provide a sharp comparison between popular uncertainty measures for binary classification in a mathematically tractable model for overparametrized neural networks: the random features model. We discuss a trade-off between classification accuracy and calibration, unveiling a double descent like behavior in the calibration curve of optimally regularized estimators as a function of overparametrization. This is in contrast with the empirical Bayes method, which we show to be well calibrated in our setting despite the higher generalization error and overparametrization.\n        \u25b3 Less\n      ",
    "title": "On double-descent in uncertainty quantification in overparametrized models",
    "date": "23 May, 2023",
    "authors": [
      "Lucas Clart\u00e9",
      " Bruno Loureiro",
      " Florent Krzakala",
      " Lenka Zdeborov\u00e1"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13235",
    "paper_id": "2305.13235",
    "abstract": "\n        Explaining the decisions of neural models is crucial for ensuring their trustworthiness at deployment time. Using Natural Language Explanations (NLEs) to justify a model's predictions has recently gained increasing interest. However, this approach usually demands large datasets of human-written NLEs for the ground-truth answers, which are expensive and potentially infeasible for some applications. For models to generate high-quality NLEs when only a few NLEs are available, the fine-tuning of Pre-trained Language Models (PLMs) in conjunction with prompt-based learning recently emerged. However, PLMs typically have billions of parameters, making fine-tuning expensive. We propose SparseFit, a sparse few-shot fine-tuning strategy that leverages discrete prompts to jointly generate predictions and NLEs. We experiment with SparseFit on the T5 model and four datasets and compare it against state-of-the-art parameter-efficient fine-tuning techniques. We perform automatic and human evaluations to assess the quality of the model-generated NLEs, finding that fine-tuning only 6.8% of the model parameters leads to competitive results for both the task performance and the quality of the NLEs.\n        \u25b3 Less\n      ",
    "title": "SPARSEFIT: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations",
    "date": "23 May, 2023",
    "authors": [
      "Jesus Solano",
      " Oana-Maria Camburu",
      " Pasquale Minervini"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13858",
    "paper_id": "2305.13858",
    "abstract": "\n        This dissertation presents a methodology for recording speed climbing training sessions with multiple cameras and annotating the videos with relevant data, including body position, hand and foot placement, and timing. The annotated data is then analyzed using deep learning techniques to create a standard dataset of speed climbing training videos. The results demonstrate the potential of the new dataset for improving speed climbing training and research, including identifying areas for improvement, creating personalized training plans, and analyzing the effects of different training methods.The findings will also be applied to the training process of the Jiangxi climbing team through further empirical research to test the findings and further explore the feasibility of this study.\n        \u25b3 Less\n      ",
    "title": "Producing a Standard Dataset of Speed Climbing Training Videos Using Deep Learning Techniques",
    "date": "23 May, 2023",
    "authors": [
      "Yufei Xie",
      " Shaoman Li",
      " Penghui Lin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16334",
    "paper_id": "2305.16334",
    "abstract": "\n        In most current research, large language models (LLMs) are able to perform reasoning tasks by generating chains of thought through the guidance of specific prompts. However, there still exists a significant discrepancy between their capability in solving complex reasoning problems and that of humans. At present, most approaches focus on chains of thought (COT) and tool use, without considering the adoption and application of human cognitive frameworks. It is well-known that when confronting complex reasoning challenges, humans typically employ various cognitive abilities, and necessitate interaction with all aspects of tools, knowledge, and the external environment information to accomplish intricate tasks. This paper introduces a novel intelligent framework, referred to as OlaGPT. OlaGPT carefully studied a cognitive architecture framework, and propose to simulate certain aspects of human cognition. The framework involves approximating different cognitive modules, including attention, memory, reasoning, learning, and corresponding scheduling and decision-making mechanisms. Inspired by the active learning mechanism of human beings, it proposes a learning unit to record previous mistakes and expert opinions, and dynamically refer to them to strengthen their ability to solve similar problems. The paper also outlines common effective reasoning frameworks for human problem-solving and designs Chain-of-Thought (COT) templates accordingly. A comprehensive decision-making mechanism is also proposed to maximize model accuracy. The efficacy of OlaGPT has been stringently evaluated on multiple reasoning datasets, and the experimental outcomes reveal that OlaGPT surpasses state-of-the-art benchmarks, demonstrating its superior performance. Our implementation of OlaGPT is available on GitHub: \\url{https://github.com/oladata-team/OlaGPT}.\n        \u25b3 Less\n      ",
    "title": "OlaGPT: Empowering LLMs With Human-like Problem-Solving Abilities",
    "date": "23 May, 2023",
    "authors": [
      "Yuanzhen Xie",
      " Tao Xie",
      " Mingxiong Lin",
      " WenTao Wei",
      " Chenglin Li",
      " Beibei Kong",
      " Lei Chen",
      " Chengxiang Zhuo",
      " Bo Hu",
      " Zang Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2206.01034",
    "paper_id": "2206.01034",
    "abstract": "\n        Most existing deep neural networks (DNNs) are easily disturbed by slight noise. However, there are few researches on physical attacks by deploying lighting equipment. The light-based physical attacks has excellent covertness, which brings great security risks to many vision-based applications (such as self-driving). Therefore, we propose a light-based physical attack, called adversarial laser spot (AdvLS), which optimizes the physical parameters of laser spots through genetic algorithm to perform physical attacks. It realizes robust and covert physical attack by using low-cost laser equipment. As far as we know, AdvLS is the first light-based physical attack that perform physical attacks in the daytime. A large number of experiments in the digital and physical environments show that AdvLS has excellent robustness and covertness. In addition, through in-depth analysis of the experimental data, we find that the adversarial perturbations generated by AdvLS have superior adversarial attack migration. The experimental results show that AdvLS impose serious interference to advanced DNNs, we call for the attention of the proposed AdvLS. The code of AdvLS is available at: https://github.com/ChengYinHu/AdvLS\n        \u25b3 Less\n      ",
    "title": "Adversarial Laser Spot: Robust and Covert Physical-World Attack to DNNs",
    "date": "23 May, 2023",
    "authors": [
      "Chengyin Hu",
      " Yilong Wang",
      " Kalibinuer Tiliwalidi",
      " Wen Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.14402",
    "paper_id": "2209.14402",
    "abstract": "\n        Graph Neural Networks (GNNs) are a popular class of machine learning models. Inspired by the learning to explain (L2X) paradigm, we propose L2XGNN, a framework for explainable GNNs which provides faithful explanations by design. L2XGNN learns a mechanism for selecting explanatory subgraphs (motifs) which are exclusively used in the GNNs message-passing operations. L2XGNN is able to select, for each input graph, a subgraph with specific properties such as being sparse and connected. Imposing such constraints on the motifs often leads to more interpretable and effective explanations. Experiments on several datasets suggest that L2XGNN achieves the same classification accuracy as baseline methods using the entire input graph while ensuring that only the provided explanations are used to make predictions. Moreover, we show that L2XGNN is able to identify motifs responsible for the graph's properties it is intended to predict.\n        \u25b3 Less\n      ",
    "title": "L2XGNN: Learning to Explain Graph Neural Networks",
    "date": "23 May, 2023",
    "authors": [
      "Giuseppe Serra",
      " Mathias Niepert"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13872",
    "paper_id": "2305.13872",
    "abstract": "\n        Deep generative models (DGMs) and their conditional counterparts provide a powerful ability for general-purpose generative modeling of data distributions. However, it remains challenging for existing methods to address advanced conditional generative problems without annotations, which can enable multiple applications like image-to-image translation and image editing. We present a unified Bayesian framework for such problems, which introduces an inference stage on latent variables within the learning process. In particular, we propose a variational Bayesian image translation network (VBITN) that enables multiple image translation and editing tasks. Comprehensive experiments show the effectiveness of our method on unsupervised image-to-image translation, and demonstrate the novel advanced capabilities for semantic editing and mixed domain translation.\n        \u25b3 Less\n      ",
    "title": "Variational Bayesian Framework for Advanced Image Generation with Domain-Related Variables",
    "date": "23 May, 2023",
    "authors": [
      "Yuxiao Li",
      " Santiago Mazuelas",
      " Yuan Shen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13875",
    "paper_id": "2305.13875",
    "abstract": "\n        Class imbalance and group (e.g., race, gender, and age) imbalance are acknowledged as two reasons in data that hinder the trade-off between fairness and utility of machine learning classifiers. Existing techniques have jointly addressed issues regarding class imbalance and group imbalance by proposing fair over-sampling techniques. Unlike the common oversampling techniques, which only address class imbalance, fair oversampling techniques significantly improve the abovementioned trade-off, as they can also address group imbalance. However, if the size of the original clusters is too small, these techniques may cause classifier overfitting. To address this problem, we herein develop a fair oversampling technique using data from heterogeneous clusters. The proposed technique generates synthetic data that have class-mix features or group-mix features to make classifiers robust to overfitting. Moreover, we develop an interpolation method that can enhance the validity of generated synthetic data by considering the original cluster distribution and data noise. Finally, we conduct experiments on five realistic datasets and three classifiers, and the experimental results demonstrate the effectiveness of the proposed technique in terms of fairness and utility.\n        \u25b3 Less\n      ",
    "title": "Fair Oversampling Technique using Heterogeneous Clusters",
    "date": "23 May, 2023",
    "authors": [
      "Ryosuke Sonoda"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13880",
    "paper_id": "2305.13880",
    "abstract": "\n        Learning-based methods for blind single image super resolution (SISR) conduct the restoration by a learned mapping between high-resolution (HR) images and their low-resolution (LR) counterparts degraded with arbitrary blur kernels. However, these methods mostly require an independent step to estimate the blur kernel, leading to error accumulation between steps. We propose an end-to-end learning framework for the blind SISR problem, which enables image restoration within a unified Bayesian framework with either full- or semi-supervision. The proposed method, namely SREMN, integrates learning techniques into the generalized expectation-maximization (GEM) algorithm and infers HR images from the maximum likelihood estimation (MLE). Extensive experiments show the superiority of the proposed method with comparison to existing work and novelty in semi-supervised learning.\n        \u25b3 Less\n      ",
    "title": "Generalized Expectation Maximization Framework for Blind Image Super Resolution",
    "date": "23 May, 2023",
    "authors": [
      "Yuxiao Li",
      " Zhiming Wang",
      " Yuan Shen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13884",
    "paper_id": "2305.13884",
    "abstract": "\n        With the increasing reliance on Open Source Software, users are exposed to third-party library vulnerabilities. Software Composition Analysis (SCA) tools have been created to alert users of such vulnerabilities. SCA requires the identification of vulnerability-fixing commits. Prior works have proposed methods that can automatically identify such vulnerability-fixing commits. However, identifying such commits is highly challenging, as only a very small minority of commits are vulnerability fixing. Moreover, code changes can be noisy and difficult to analyze. We observe that noise can occur at different levels of detail, making it challenging to detect vulnerability fixes accurately.\n  To address these challenges and boost the effectiveness of prior works, we propose MiDas (Multi-Granularity Detector for Vulnerability Fixes). Unique from prior works, Midas constructs different neural networks for each level of code change granularity, corresponding to commit-level, file-level, hunk-level, and line-level, following their natural organization. It then utilizes an ensemble model that combines all base models to generate the final prediction. This design allows MiDas to better handle the noisy and highly imbalanced nature of vulnerability-fixing commit data. Additionally, to reduce the human effort required to inspect code changes, we have designed an effort-aware adjustment for Midas's outputs based on commit length. The evaluation results demonstrate that MiDas outperforms the current state-of-the-art baseline in terms of AUC by 4.9% and 13.7% on Java and Python-based datasets, respectively. Furthermore, in terms of two effort-aware metrics, EffortCost@L and Popt@L, MiDas also outperforms the state-of-the-art baseline, achieving improvements of up to 28.2% and 15.9% on Java, and 60% and 51.4% on Python, respectively.\n        \u25b3 Less\n      ",
    "title": "Multi-Granularity Detector for Vulnerability Fixes",
    "date": "23 May, 2023",
    "authors": [
      "Truong Giang Nguyen",
      " Thanh Le-Cong",
      " Hong Jin Kang",
      " Ratnadira Widyasari",
      " Chengran Yang",
      " Zhipeng Zhao",
      " Bowen Xu",
      " Jiayuan Zhou",
      " Xin Xia",
      " Ahmed E. Hassan",
      " Xuan-Bach D. Le",
      " David Lo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18208",
    "paper_id": "2305.18208",
    "abstract": "\n        Localization systems based on ultra-wide band (UWB) measurements can have unsatisfactory performance in harsh environments due to the presence of non-line-of-sight (NLOS) errors. Learning-based methods for error mitigation have shown great performance improvement via directly exploiting the wideband waveform instead of handcrafted features. However, these methods require data samples fully labeled with actual measurement errors for training, which leads to time-consuming data collection. In this paper, we propose a semi-supervised learning method based on variational Bayes for UWB ranging error mitigation. Combining deep learning techniques and statistic tools, our method can efficiently accumulate knowledge from both labeled and unlabeled data samples. Extensive experiments illustrate the effectiveness of the proposed method under different supervision rates, and the superiority compared to other fully supervised methods even at a low supervision rate.\n        \u25b3 Less\n      ",
    "title": "A Semi-Supervised Learning Approach for Ranging Error Mitigation Based on UWB Waveform",
    "date": "23 May, 2023",
    "authors": [
      "Yuxiao Li",
      " Santiago Mazuelas",
      " Yuan Shen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13886",
    "paper_id": "2305.13886",
    "abstract": "\n        One of the major obstacles in designing an automatic target recognition (ATR) algorithm, is that there are often labeled images in one domain (i.e., infrared source domain) but no annotated images in the other target domains (i.e., visible, SAR, LIDAR). Therefore, automatically annotating these images is essential to build a robust classifier in the target domain based on the labeled images of the source domain. Transductive transfer learning is an effective way to adapt a network to a new target domain by utilizing a pretrained ATR network in the source domain. We propose an unpaired transductive transfer learning framework where a CycleGAN model and a well-trained ATR classifier in the source domain are used to construct an ATR classifier in the target domain without having any labeled data in the target domain. We employ a CycleGAN model to transfer the mid-wave infrared (MWIR) images to visible (VIS) domain images (or visible to MWIR domain). To train the transductive CycleGAN, we optimize a cost function consisting of the adversarial, identity, cycle-consistency, and categorical cross-entropy loss for both the source and target classifiers. In this paper, we perform a detailed experimental analysis on the challenging DSIAC ATR dataset. The dataset consists of ten classes of vehicles at different poses and distances ranging from 1-5 kilometers on both the MWIR and VIS domains. In our experiment, we assume that the images in the VIS domain are the unlabeled target dataset. We first detect and crop the vehicles from the raw images and then project them into a common distance of 2 kilometers. Our proposed transductive CycleGAN achieves 71.56% accuracy in classifying the visible domain vehicles in the DSIAC ATR dataset.\n        \u25b3 Less\n      ",
    "title": "Deep Transductive Transfer Learning for Automatic Target Recognition",
    "date": "23 May, 2023",
    "authors": [
      "Shoaib M. Sami",
      " Nasser M. Nasrabadi",
      " Raghuveer Rao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18206",
    "paper_id": "2305.18206",
    "abstract": "\n        Received waveforms contain rich information for both range information and environment semantics. However, its full potential is hard to exploit under multipath and non-line-of-sight conditions. This paper proposes a deep generative model (DGM) for simultaneous range error mitigation and environment identification. In particular, we present a Bayesian model for the generative process of the received waveform composed by latent variables for both range-related features and environment semantics. The simultaneous range error mitigation and environment identification is interpreted as an inference problem based on the DGM, and implemented in a unique end-to-end learning scheme. Comprehensive experiments on a general Ultra-wideband dataset demonstrate the superior performance on range error mitigation, scalability to different environments, and novel capability on simultaneous environment identification.\n        \u25b3 Less\n      ",
    "title": "Deep Generative Model for Simultaneous Range Error Mitigation and Environment Identification",
    "date": "23 May, 2023",
    "authors": [
      "Yuxiao Li",
      " Santiago Mazuelas",
      " Yuan Shen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13917",
    "paper_id": "2305.13917",
    "abstract": "\n        While large language models (LLMs) bring not only performance but also complexity, recent work has started to turn LLMs into data generators rather than task inferencers, where another affordable task model is trained for efficient deployment and inference. However, such an approach has primarily been applied to natural language tasks and has not yet been explored for symbolic language tasks with complex structured outputs (e.g., semantic parsing and code generation). In this paper, we propose SymGen which utilizes LLMs for generating various annotation-expensive symbolic language data. SymGen consists of an informative prompt to steer generation and an agreement-based verifier to improve data correctness. We conduct extensive experiments on six symbolic language tasks across various settings. Compared with the LLMs, we demonstrate the 1\\%-sized task model can achieve comparable or better performance, largely cutting inference and deployment costs. We also show that generated data with only a few human demonstrations can be as effective as over 10 times the amount of human-annotated data when training the task model, saving a considerable amount of annotation effort. SymGen sheds new light on data generation for complex tasks, and we release the code at \\href{https://github.com/HKUNLP/SymGen}{https://github.com/HKUNLP/SymGen}.\n        \u25b3 Less\n      ",
    "title": "Generating Data for Symbolic Language with Large Language Models",
    "date": "23 May, 2023",
    "authors": [
      "Jiacheng Ye",
      " Chengzu Li",
      " Lingpeng Kong",
      " Tao Yu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13926",
    "paper_id": "2305.13926",
    "abstract": "\n        Classification model selection is a process of identifying a suitable model class for a given classification task on a dataset. Traditionally, model selection is based on cross-validation, meta-learning, and user preferences, which are often time-consuming and resource-intensive. The performance of any machine learning classification task depends on the choice of the model class, the learning algorithm, and the dataset's characteristics. Our work proposes a novel method for automatic classification model selection from a set of candidate model classes by determining the empirical model-fitness for a dataset based only on its clustering indices. Clustering Indices measure the ability of a clustering algorithm to induce good quality neighborhoods with similar data characteristics. We propose a regression task for a given model class, where the clustering indices of a given dataset form the features and the dependent variable represents the expected classification performance. We compute the dataset clustering indices and directly predict the expected classification performance using the learned regressor for each candidate model class to recommend a suitable model class for dataset classification. We evaluate our model selection method through cross-validation with 60 publicly available binary class datasets and show that our top3 model recommendation is accurate for over 45 of 60 datasets. We also propose an end-to-end Automated ML system for data classification based on our model selection method. We evaluate our end-to-end system against popular commercial and noncommercial Automated ML systems using a different collection of 25 public domain binary class datasets. We show that the proposed system outperforms other methods with an excellent average rank of 1.68.\n        \u25b3 Less\n      ",
    "title": "Clustering Indices based Automatic Classification Model Selection",
    "date": "23 May, 2023",
    "authors": [
      "Sudarsun Santhiappan",
      " Nitin Shravan",
      " Balaraman Ravindran"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18327",
    "paper_id": "2305.18327",
    "abstract": "\n        The importance of ultrasonic nondestructive testing has been increasing in recent years, and there are high expectations for the potential of laser ultrasonic visualization testing, which combines laser ultrasonic testing with scattered wave visualization technology. Even if scattered waves are visualized, inspectors still need to carefully inspect the images. To automate this, this paper proposes a deep neural network for automatic defect detection and localization in LUVT images. To explore the structure of a neural network suitable to this task, we compared the LUVT image analysis problem with the generic object detection problem. Numerical experiments using real-world data from a SUS304 flat plate showed that the proposed method is more effective than the general object detection model in terms of prediction performance. We also show that the computational time required for prediction is faster than that of the general object detection model.\n        \u25b3 Less\n      ",
    "title": "A Study on Deep CNN Structures for Defect Detection From Laser Ultrasonic Visualization Testing Images",
    "date": "23 May, 2023",
    "authors": [
      "Miya Nakajima",
      " Takahiro Saitoh",
      " Tsuyoshi Kato"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13947",
    "paper_id": "2305.13947",
    "abstract": "\n        CANDECOMP/PARAFAC (CP) decomposition is the mostly used model to formulate the received tensor signal in a multi-domain massive multiple-input multiple-output (MIMO) system, as the receiver generally sums the components from different paths or users. To achieve accurate and low-latency channel estimation, good and fast CP decomposition algorithms are desired. The CP alternating least squares (CPALS) is the workhorse algorithm for calculating the CP decomposition. However, its performance depends on the initializations, and good starting values can lead to more efficient solutions. Existing initialization strategies are decoupled from the CPALS and are not necessarily favorable for solving the CP decomposition. To enhance the algorithm's speed and accuracy, this paper proposes a deep-learning-aided CPALS (DL-CPALS) method that uses a deep neural network (DNN) to generate favorable initializations. The proposed DL-CPALS integrates the DNN and CPALS to a model-based deep learning paradigm, where it trains the DNN to generate an initialization that facilitates fast and accurate CP decomposition. Moreover, benefiting from the CP low-rankness, the proposed method is trained using noisy data and does not require paired clean data. The proposed DL-CPALS is applied to millimeter wave MIMO orthogonal frequency division multiplexing (mmWave MIMO-OFDM) channel estimation. Experimental results demonstrate the significant improvements of the proposed method in terms of both speed and accuracy for CP decomposition and channel estimation.\n        \u25b3 Less\n      ",
    "title": "Deep-Learning-Aided Alternating Least Squares for Tensor CP Decomposition and Its Application to Massive MIMO Channel Estimation",
    "date": "23 May, 2023",
    "authors": [
      "Xiao Gong",
      " Wei Chen",
      " Bo Ai",
      " Geert Leus"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12380",
    "paper_id": "2305.12380",
    "abstract": "\n        Understanding the mechanisms underlying human attention is a fundamental challenge for both vision science and artificial intelligence. While numerous computational models of free-viewing have been proposed, less is known about the mechanisms underlying task-driven image exploration. To address this gap, we present CapMIT1003, a database of captions and click-contingent image explorations collected during captioning tasks. CapMIT1003 is based on the same stimuli from the well-known MIT1003 benchmark, for which eye-tracking data under free-viewing conditions is available, which offers a promising opportunity to concurrently study human attention under both tasks. We make this dataset publicly available to facilitate future research in this field. In addition, we introduce NevaClip, a novel zero-shot method for predicting visual scanpaths that combines contrastive language-image pretrained (CLIP) models with biologically-inspired neural visual attention (NeVA) algorithms. NevaClip simulates human scanpaths by aligning the representation of the foveated visual stimulus and the representation of the associated caption, employing gradient-driven visual exploration to generate scanpaths. Our experimental results demonstrate that NevaClip outperforms existing unsupervised computational models of human visual attention in terms of scanpath plausibility, for both captioning and free-viewing tasks. Furthermore, we show that conditioning NevaClip with incorrect or misleading captions leads to random behavior, highlighting the significant impact of caption guidance in the decision-making process. These findings contribute to a better understanding of mechanisms that guide human attention and pave the way for more sophisticated computational approaches to scanpath prediction that can integrate direct top-down guidance of downstream tasks.\n        \u25b3 Less\n      ",
    "title": "Contrastive Language-Image Pretrained Models are Zero-Shot Human Scanpath Predictors",
    "date": "23 May, 2023",
    "authors": [
      "Dario Zanca",
      " Andrea Zugarini",
      " Simon Dietz",
      " Thomas R. Altstidl",
      " Mark A. Turban Ndjeuha",
      " Leo Schwinn",
      " Bjoern Eskofier"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.05540",
    "paper_id": "2306.05540",
    "abstract": "\n        With the rapid progress of large language models (LLMs) and the huge amount of text they generated, it becomes more and more impractical to manually distinguish whether a text is machine-generated. Given the growing use of LLMs in social media and education, it prompts us to develop methods to detect machine-generated text, preventing malicious usage such as plagiarism, misinformation, and propaganda. Previous work has studied several zero-shot methods, which require no training data. These methods achieve good performance, but there is still a lot of room for improvement. In this paper, we introduce two novel zero-shot methods for detecting machine-generated text by leveraging the log rank information. One is called DetectLLM-LRR, which is fast and efficient, and the other is called DetectLLM-NPR, which is more accurate, but slower due to the need for perturbations. Our experiments on three datasets and seven language models show that our proposed methods improve over the state of the art by 3.9 and 1.75 AUROC points absolute. Moreover, DetectLLM-NPR needs fewer perturbations than previous work to achieve the same level of performance, which makes it more practical for real-world use. We also investigate the efficiency--performance trade-off based on users preference on these two measures and we provide intuition for using them in practice effectively. We release the data and the code of both methods in https://github.com/mbzuai-nlp/DetectLLM\n        \u25b3 Less\n      ",
    "title": "DetectLLM: Leveraging Log Rank Information for Zero-Shot Detection of Machine-Generated Text",
    "date": "23 May, 2023",
    "authors": [
      "Jinyan Su",
      " Terry Yue Zhuo",
      " Di Wang",
      " Preslav Nakov"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14403",
    "paper_id": "2305.14403",
    "abstract": "\n        Structured pruning can simplify network architecture and improve inference speed. Combined with the underlying hardware and inference engine in which the final model is deployed, better results can be obtained by using latency collaborative loss function to guide network pruning together. Existing pruning methods that optimize latency have demonstrated leading performance, however, they often overlook the hardware features and connection in the network. To address this problem, we propose a global importance score SP-LAMP(Structured Pruning Layer-Adaptive Magnitude-based Pruning) by deriving a global importance score LAMP from unstructured pruning to structured pruning. In SP-LAMP, each layer includes a filter with an SP-LAMP score of 1, and the remaining filters are grouped. We utilize a group knapsack solver to maximize the SP-LAMP score under latency constraints. In addition, we improve the strategy of collect the latency to make it more accurate. In particular, for ResNet50/ResNet18 on ImageNet and CIFAR10, SP-LAMP is 1.28x/8.45x faster with +1.7%/-1.57% top-1 accuracy changed, respectively. Experimental results in ResNet56 on CIFAR10 demonstrate that our algorithm achieves lower latency compared to alternative approaches while ensuring accuracy and FLOPs.\n        \u25b3 Less\n      ",
    "title": "Layer-adaptive Structured Pruning Guided by Latency",
    "date": "23 May, 2023",
    "authors": [
      "Siyuan Pan",
      " Linna Zhang",
      " Jie Zhang",
      " Xiaoshuang Li",
      " Liang Hou",
      " Xiaobing Tu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14404",
    "paper_id": "2305.14404",
    "abstract": "\n        Integrating the brain structural and functional connectivity features is of great significance in both exploring brain science and analyzing cognitive impairment clinically. However, it remains a challenge to effectively fuse structural and functional features in exploring the brain network. In this paper, a novel brain structure-function fusing-representation learning (BSFL) model is proposed to effectively learn fused representation from diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (fMRI) for mild cognitive impairment (MCI) analysis. Specifically, the decomposition-fusion framework is developed to first decompose the feature space into the union of the uniform and the unique spaces for each modality, and then adaptively fuse the decomposed features to learn MCI-related representation. Moreover, a knowledge-aware transformer module is designed to automatically capture local and global connectivity features throughout the brain. Also, a uniform-unique contrastive loss is further devised to make the decomposition more effective and enhance the complementarity of structural and functional features. The extensive experiments demonstrate that the proposed model achieves better performance than other competitive methods in predicting and analyzing MCI. More importantly, the proposed model could be a potential tool for reconstructing unified brain networks and predicting abnormal connections during the degenerative processes in MCI.\n        \u25b3 Less\n      ",
    "title": "Brain Structure-Function Fusing Representation Learning using Adversarial Decomposed-VAE for Analyzing MCI",
    "date": "23 May, 2023",
    "authors": [
      "Qiankun Zuo",
      " Baiying Lei",
      " Ning Zhong",
      " Yi Pan",
      " Shuqiang Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13962",
    "paper_id": "2305.13962",
    "abstract": "\n        Recently, talking face generation has drawn ever-increasing attention from the research community in computer vision due to its arduous challenges and widespread application scenarios, e.g. movie animation and virtual anchor. Although persevering efforts have been undertaken to enhance the fidelity and lip-sync quality of generated talking face videos, there is still large room for further improvements of synthesis quality and efficiency. Actually, these attempts somewhat ignore the explorations of fine-granularity feature extraction/integration and the consistency between probability distributions of landmarks, thereby recurring the issues of local details blurring and degraded fidelity. To mitigate these dilemmas, in this paper, a novel CLIP-based Attention and Probability Map Guided Network (CPNet) is delicately designed for inferring high-fidelity talking face videos. Specifically, considering the demands of fine-grained feature recalibration, a clip-based attention condenser is exploited to transfer knowledge with rich semantic priors from the prevailing CLIP model. Moreover, to guarantee the consistency in probability space and suppress the landmark ambiguity, we creatively propose the density map of facial landmark as auxiliary supervisory signal to guide the landmark distribution learning of generated frame. Extensive experiments on the widely-used benchmark dataset demonstrate the superiority of our CPNet against state of the arts in terms of image and lip-sync quality. In addition, a cohort of studies are also conducted to ablate the impacts of the individual pivotal components.\n        \u25b3 Less\n      ",
    "title": "CPNet: Exploiting CLIP-based Attention Condenser and Probability Map Guidance for High-fidelity Talking Face Generation",
    "date": "23 May, 2023",
    "authors": [
      "Jingning Xu",
      " Benlai Tang",
      " Mingjie Wang",
      " Minghao Li",
      " Meirong Ma"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13967",
    "paper_id": "2305.13967",
    "abstract": "\n        Automated Intelligent Cyberdefense Agents (AICAs) that are part Intrusion Detection Systems (IDS) and part Intrusion Response Systems (IRS) are being designed to protect against sophisticated and automated cyber-attacks. An AICA based on the ideas of Self-Adaptive Autonomic Computing Systems (SA-ACS) can be considered as a managing system that protects a managed system like a personal computer, web application, critical infrastructure, etc. An AICA, specifically the IRS components, can compute a wide range of potential responses to meet its security goals and objectives, such as taking actions to prevent the attack from completing, restoring the system to comply with the organizational security policy, containing or confining an attack, attack eradication, deploying forensics measures to enable future attack analysis, counterattack, and so on. To restrict its activities in order to minimize collateral/organizational damage, such an automated system must have set Rules of Engagement (RoE). Automated systems must determine which operations can be completely automated (and when), which actions require human operator confirmation, and which actions must never be undertaken. In this paper, to enable this control functionality over an IRS, we create Rules of EngaGement for Automated cybeR Defense (REGARD) system which holds a set of Rules of Engagement (RoE) to protect the managed system according to the instructions provided by the human operator. These rules help limit the action of the IRS on the managed system in compliance with the recommendations of the domain expert. We provide details of execution, management, operation, and conflict resolution for Rules of Engagement (RoE) to constrain the actions of an automated IRS. We also describe REGARD system implementation, security case studies for cyber defense, and RoE demonstrations.\n        \u25b3 Less\n      ",
    "title": "REGARD: Rules of EngaGement for Automated cybeR Defense to aid in Intrusion Response",
    "date": "23 May, 2023",
    "authors": [
      "Damodar Panigrahi",
      " William Anderson",
      " Joshua Whitman",
      " Sudip Mittal",
      " Benjamin A Blakely"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13987",
    "paper_id": "2305.13987",
    "abstract": "\n        Graph Transformer has recently received wide attention in the research community with its outstanding performance, yet its structural expressive power has not been well analyzed. Inspired by the connections between Weisfeiler-Lehman (WL) graph isomorphism test and graph neural network (GNN), we introduce \\textbf{SEG-WL test} (\\textbf{S}tructural \\textbf{E}ncoding enhanced \\textbf{G}lobal \\textbf{W}eisfeiler-\\textbf{L}ehman test), a generalized graph isomorphism test algorithm as a powerful theoretical tool for exploring the structural discriminative power of graph Transformers. We theoretically prove that the SEG-WL test is an expressivity upper bound on a wide range of graph Transformers, and the representational power of SEG-WL test can be approximated by a simple Transformer network arbitrarily under certain conditions. With the SEG-WL test, we show how graph Transformers' expressive power is determined by the design of structural encodings, and present conditions that make the expressivity of graph Transformers beyond WL test and GNNs. Moreover, motivated by the popular shortest path distance encoding, we follow the theory-oriented principles and develop a provably stronger structural encoding method, Shortest Path Induced Subgraph (\\textit{SPIS}) encoding. Our theoretical findings provide a novel and practical paradigm for investigating the expressive power of graph Transformers, and extensive synthetic and real-world experiments empirically verify the strengths of our proposed methods.\n        \u25b3 Less\n      ",
    "title": "On Structural Expressive Power of Graph Transformers",
    "date": "23 May, 2023",
    "authors": [
      "Wenhao Zhu",
      " Tianyu Wen",
      " Guojie Song",
      " Liang Wang",
      " Bo Zheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16335",
    "paper_id": "2305.16335",
    "abstract": "\n        Short text clustering is challenging since it takes imbalanced and noisy data as inputs. Existing approaches cannot solve this problem well, since (1) they are prone to obtain degenerate solutions especially on heavy imbalanced datasets, and (2) they are vulnerable to noises. To tackle the above issues, we propose a Robust Short Text Clustering (RSTC) model to improve robustness against imbalanced and noisy data. RSTC includes two modules, i.e., pseudo-label generation module and robust representation learning module. The former generates pseudo-labels to provide supervision for the later, which contributes to more robust representations and correctly separated clusters. To provide robustness against the imbalance in data, we propose self-adaptive optimal transport in the pseudo-label generation module. To improve robustness against the noise in data, we further introduce both class-wise and instance-wise contrastive learning in the robust representation learning module. Our empirical studies on eight short text clustering datasets demonstrate that RSTC significantly outperforms the state-of-the-art models. The code is available at: https://github.com/hmllmh/RSTC.\n        \u25b3 Less\n      ",
    "title": "Robust Representation Learning with Reliable Pseudo-labels Generation via Self-Adaptive Optimal Transport for Short Text Clustering",
    "date": "23 May, 2023",
    "authors": [
      "Xiaolin Zheng",
      " Mengling Hu",
      " Weiming Liu",
      " Chaochao Chen",
      " Xinting Liao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.06287",
    "paper_id": "2304.06287",
    "abstract": "\n        We present NeRFVS, a novel neural radiance fields (NeRF) based method to enable free navigation in a room. NeRF achieves impressive performance in rendering images for novel views similar to the input views while suffering for novel views that are significantly different from the training views. To address this issue, we utilize the holistic priors, including pseudo depth maps and view coverage information, from neural reconstruction to guide the learning of implicit neural representations of 3D indoor scenes. Concretely, an off-the-shelf neural reconstruction method is leveraged to generate a geometry scaffold. Then, two loss functions based on the holistic priors are proposed to improve the learning of NeRF: 1) A robust depth loss that can tolerate the error of the pseudo depth map to guide the geometry learning of NeRF; 2) A variance loss to regularize the variance of implicit neural representations to reduce the geometry and color ambiguity in the learning procedure. These two loss functions are modulated during NeRF optimization according to the view coverage information to reduce the negative influence brought by the view coverage imbalance. Extensive results demonstrate that our NeRFVS outperforms state-of-the-art view synthesis methods quantitatively and qualitatively on indoor scenes, achieving high-fidelity free navigation results.\n        \u25b3 Less\n      ",
    "title": "NeRFVS: Neural Radiance Fields for Free View Synthesis via Geometry Scaffolds",
    "date": "23 May, 2023",
    "authors": [
      "Chen Yang",
      " Peihao Li",
      " Zanwei Zhou",
      " Shanxin Yuan",
      " Bingbing Liu",
      " Xiaokang Yang",
      " Weichao Qiu",
      " Wei Shen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.12515",
    "paper_id": "2302.12515",
    "abstract": "\n        Learning communication strategies in cooperative multi-agent reinforcement learning (MARL) has recently attracted intensive attention. Early studies typically assumed a fully-connected communication topology among agents, which induces high communication costs and may not be feasible. Some recent works have developed adaptive communication strategies to reduce communication overhead, but these methods cannot effectively obtain valuable information from agents that are beyond the communication range. In this paper, we consider a realistic communication model where each agent has a limited communication range, and the communication topology dynamically changes. To facilitate effective agent communication, we propose a novel communication protocol called Adaptively Controlled Two-Hop Communication (AC2C). After an initial local communication round, AC2C employs an adaptive two-hop communication strategy to enable long-range information exchange among agents to boost performance, which is implemented by a communication controller. This controller determines whether each agent should ask for two-hop messages and thus helps to reduce the communication overhead during distributed execution. We evaluate AC2C on three cooperative multi-agent tasks, and the experimental results show that it outperforms relevant baselines with lower communication costs.\n        \u25b3 Less\n      ",
    "title": "AC2C: Adaptively Controlled Two-Hop Communication for Multi-Agent Reinforcement Learning",
    "date": "23 May, 2023",
    "authors": [
      "Xuefeng Wang",
      " Xinran Li",
      " Jiawei Shao",
      " Jun Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11831",
    "paper_id": "2305.11831",
    "abstract": "\n        This work presents a comprehensive analysis to regularize the Soft Actor-Critic (SAC) algorithm with automatic temperature adjustment. The the policy evaluation, the policy improvement and the temperature adjustment are reformulated, addressing certain modification and enhancing the clarity of the original theory in a more explicit manner.\n        \u25b3 Less\n      ",
    "title": "Regularization of Soft Actor-Critic Algorithms with Automatic Temperature Adjustment",
    "date": "23 May, 2023",
    "authors": [
      "Ben You"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18616",
    "paper_id": "2305.18616",
    "abstract": "\n        ChatGPT, launched in November 2022, has gained widespread attention from students and educators globally, with an online report by Hu (2023) stating it as the fastest-growing consumer application in history. While discussions on the use of ChatGPT in higher education are abundant, empirical studies on its impact on collaborative interdisciplinary learning are rare. To investigate its potential, we conducted a quasi-experimental study with 130 undergraduate students (STEM and non-STEM) learning digital literacy with or without ChatGPT over two weeks. Weekly surveys were conducted on collaborative interdisciplinary problem-solving, physical and cognitive engagement, and individual reflections on ChatGPT use. Analysis of survey responses showed significant main effects of topics on collaborative interdisciplinary problem-solving and physical and cognitive engagement, a marginal interaction effect between disciplinary backgrounds and ChatGPT conditions for cognitive engagement, and a significant interaction effect for physical engagement. Sentiment analysis of student reflections suggested no significant difference between STEM and non-STEM students' opinions towards ChatGPT. Qualitative analysis of reflections generated eight positive themes, including efficiency, addressing knowledge gaps, and generating human-like responses, and eight negative themes, including generic responses, lack of innovation, and counterproductive to self-discipline and thinking. Our findings suggest that ChatGPT use needs to be optimized by considering the topics being taught and the disciplinary backgrounds of students rather than applying it uniformly. These findings have implications for both pedagogical research and practices.\n        \u25b3 Less\n      ",
    "title": "Embrace Opportunities and Face Challenges: Using ChatGPT in Undergraduate Students' Collaborative Interdisciplinary Learning",
    "date": "23 May, 2023",
    "authors": [
      "Gaoxia Zhu",
      " Xiuyi Fan",
      " Chenyu Hou",
      " Tianlong Zhong",
      " Peter Seow",
      " Annabel Chen Shen-Hsing",
      " Preman Rajalingam",
      " Low Kin Yew",
      " Tan Lay Poh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.00354",
    "paper_id": "2304.00354",
    "abstract": "\n        Offline Meta Reinforcement Learning (OMRL) aims to learn transferable knowledge from offline datasets to enhance the learning process for new target tasks. Context-based Reinforcement Learning (RL) adopts a context encoder to expediently adapt the agent to new tasks by inferring the task representation, and then adjusting the policy based on this inferred representation. In this work, we focus on context-based OMRL, specifically on the challenge of learning task representation for OMRL. We conduct experiments that demonstrate that the context encoder trained on offline datasets might encounter distribution shift between the contexts used for training and testing. To overcome this problem, we present a hard-sampling-based strategy to train a robust task context encoder. Our experimental findings on diverse continuous control tasks reveal that utilizing our approach yields more robust task representations and better testing performance in terms of accumulated returns compared to baseline methods. Our code is available at https://github.com/ZJLAB-AMMI/HS-OMRL.\n        \u25b3 Less\n      ",
    "title": "On Context Distribution Shift in Task Representation Learning for Offline Meta RL",
    "date": "23 May, 2023",
    "authors": [
      "Chenyang Zhao",
      " Zihao Zhou",
      " Bin Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.06849",
    "paper_id": "2305.06849",
    "abstract": "\n        Long-form question answering (LFQA) aims at answering complex, open-ended questions with detailed, paragraph-length responses. The de facto paradigm of LFQA necessitates two procedures: information retrieval, which searches for relevant supporting facts, and information synthesis, which integrates these facts into a coherent answer. In this paper, we introduce WebCPM, the first Chinese LFQA dataset. One unique feature of WebCPM is that its information retrieval is based on interactive web search, which engages with a search engine in real time. Following WebGPT, we develop a web search interface. We recruit annotators to search for relevant information using our interface and then answer questions. Meanwhile, the web search behaviors of our annotators would be recorded. In total, we collect 5,500 high-quality question-answer pairs, together with 14,315 supporting facts and 121,330 web search actions. We fine-tune pre-trained language models to imitate human behaviors for web search and to generate answers based on the collected facts. Our LFQA pipeline, built on these fine-tuned models, generates answers that are no worse than human-written ones in 32.5% and 47.5% of the cases on our dataset and DuReader, respectively.\n        \u25b3 Less\n      ",
    "title": "WebCPM: Interactive Web Search for Chinese Long-form Question Answering",
    "date": "23 May, 2023",
    "authors": [
      "Yujia Qin",
      " Zihan Cai",
      " Dian Jin",
      " Lan Yan",
      " Shihao Liang",
      " Kunlun Zhu",
      " Yankai Lin",
      " Xu Han",
      " Ning Ding",
      " Huadong Wang",
      " Ruobing Xie",
      " Fanchao Qi",
      " Zhiyuan Liu",
      " Maosong Sun",
      " Jie Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14406",
    "paper_id": "2305.14406",
    "abstract": "\n        Demand forecasting in the online fashion industry is particularly amendable to global, data-driven forecasting models because of the industry's set of particular challenges. These include the volume of data, the irregularity, the high amount of turn-over in the catalog and the fixed inventory assumption. While standard deep learning forecasting approaches cater for many of these, the fixed inventory assumption requires a special treatment via controlling the relationship between price and demand closely. In this case study, we describe the data and our modelling approach for this forecasting problem in detail and present empirical results that highlight the effectiveness of our approach.\n        \u25b3 Less\n      ",
    "title": "Deep Learning based Forecasting: a case study from the online fashion industry",
    "date": "23 May, 2023",
    "authors": [
      "Manuel Kunz",
      " Stefan Birr",
      " Mones Raslan",
      " Lei Ma",
      " Zhen Li",
      " Adele Gouttes",
      " Mateusz Koren",
      " Tofigh Naghibi",
      " Johannes Stephan",
      " Mariia Bulycheva",
      " Matthias Grzeschik",
      " Armin Keki\u0107",
      " Michael Narodovitch",
      " Kashif Rasul",
      " Julian Sieber",
      " Tim Januschowski"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04532",
    "paper_id": "2305.04532",
    "abstract": "\n        Artificial intelligence is more ubiquitous in multiple domains. Smartphones, social media platforms, search engines, and autonomous vehicles are just a few examples of applications that utilize artificial intelligence technologies to enhance their performance. This study carries out a scoping review of the current state-of-the-art artificial intelligence technologies following the PRISMA framework. The goal was to find the most advanced technologies used in different domains of artificial intelligence technology research. Three recognized journals were used from artificial intelligence and machine learning domain: Journal of Artificial Intelligence Research, Journal of Machine Learning Research, and Machine Learning, and articles published in 2022 were observed. Certain qualifications were laid for the technological solutions: the technology must be tested against comparable solutions, commonly approved or otherwise well justified datasets must be used while applying, and results must show improvements against comparable solutions. One of the most important parts of the technology development appeared to be how to process and exploit the data gathered from multiple sources. The data can be highly unstructured and the technological solution should be able to utilize the data with minimum manual work from humans. The results of this review indicate that creating labeled datasets is very laborious, and solutions exploiting unsupervised or semi-supervised learning technologies are more and more researched. The learning algorithms should be able to be updated efficiently, and predictions should be interpretable. Using artificial intelligence technologies in real-world applications, safety and explainable predictions are mandatory to consider before mass adoption can occur.\n        \u25b3 Less\n      ",
    "title": "Latest Trends in Artificial Intelligence Technology: A Scoping Review",
    "date": "23 May, 2023",
    "authors": [
      "Teemu Niskanen",
      " Tuomo Sipola",
      " Olli V\u00e4\u00e4n\u00e4nen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14074",
    "paper_id": "2305.14074",
    "abstract": "\n        Inductive relation reasoning for knowledge graphs, aiming to infer missing links between brand-new entities, has drawn increasing attention. The models developed based on Graph Inductive Learning, called GraIL-based models, have shown promising potential for this task. However, the uni-directional message-passing mechanism hinders such models from exploiting hidden mutual relations between entities in directed graphs. Besides, the enclosing subgraph extraction in most GraIL-based models restricts the model from extracting enough discriminative information for reasoning. Consequently, the expressive ability of these models is limited. To address the problems, we propose a novel GraIL-based inductive relation reasoning model, termed MINES, by introducing a Message Intercommunication mechanism on the Neighbor-Enhanced Subgraph. Concretely, the message intercommunication mechanism is designed to capture the omitted hidden mutual information. It introduces bi-directed information interactions between connected entities by inserting an undirected/bi-directed GCN layer between uni-directed RGCN layers. Moreover, inspired by the success of involving more neighbors in other graph-based tasks, we extend the neighborhood area beyond the enclosing subgraph to enhance the information collection for inductive relation reasoning. Extensive experiments on twelve inductive benchmark datasets demonstrate that our MINES outperforms existing state-of-the-art models, and show the effectiveness of our intercommunication mechanism and reasoning on the neighbor-enhanced subgraph.\n        \u25b3 Less\n      ",
    "title": "Message Intercommunication for Inductive Relation Reasoning",
    "date": "23 May, 2023",
    "authors": [
      "Ke Liang",
      " Lingyuan Meng",
      " Sihang Zhou",
      " Siwei Wang",
      " Wenxuan Tu",
      " Yue Liu",
      " Meng Liu",
      " Xinwang Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14080",
    "paper_id": "2305.14080",
    "abstract": "\n        Latest developments in computer hardware, sensor technologies, and artificial intelligence can make virtual reality (VR) and virtual spaces an important part of human everyday life. Eye tracking offers not only a hands-free way of interaction but also the possibility of a deeper understanding of human visual attention and cognitive processes in VR. Despite these possibilities, eye-tracking data also reveal privacy-sensitive attributes of users when it is combined with the information about the presented stimulus. To address these possibilities and potential privacy issues, in this survey, we first cover major works in eye tracking, VR, and privacy areas between the years 2012 and 2022. While eye tracking in the VR part covers the complete pipeline of eye-tracking methodology from pupil detection and gaze estimation to offline use and analyses, as for privacy and security, we focus on eye-based authentication as well as computational methods to preserve the privacy of individuals and their eye-tracking data in VR. Later, taking all into consideration, we draw three main directions for the research community by mainly focusing on privacy challenges. In summary, this survey provides an extensive literature review of the utmost possibilities with eye tracking in VR and the privacy implications of those possibilities.\n        \u25b3 Less\n      ",
    "title": "Eye-tracked Virtual Reality: A Comprehensive Survey on Methods and Privacy Challenges",
    "date": "23 May, 2023",
    "authors": [
      "Efe Bozkir",
      " S\u00fcleyman \u00d6zdel",
      " Mengdi Wang",
      " Brendan David-John",
      " Hong Gao",
      " Kevin Butler",
      " Eakta Jain",
      " Enkelejda Kasneci"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.02781",
    "paper_id": "2212.02781",
    "abstract": "\n        To alleviate the practical constraints for deploying deep neural networks (DNNs) on edge devices, quantization is widely regarded as one promising technique. It reduces the resource requirements for computational power and storage space by quantizing the weights and/or activation tensors of a DNN into lower bit-width fixed-point numbers, resulting in quantized neural networks (QNNs). While it has been empirically shown to introduce minor accuracy loss, critical verified properties of a DNN might become invalid once quantized. Existing verification methods focus on either individual neural networks (DNNs or QNNs) or quantization error bound for partial quantization. In this work, we propose a quantization error bound verification method, named QEBVerif, where both weights and activation tensors are quantized. QEBVerif consists of two parts, i.e., a differential reachability analysis (DRA) and a mixed-integer linear programming (MILP) based verification method. DRA performs difference analysis between the DNN and its quantized counterpart layer-by-layer to compute a tight quantization error interval efficiently. If DRA fails to prove the error bound, then we encode the verification problem into an equivalent MILP problem which can be solved by off-the-shelf solvers. Thus, QEBVerif is sound, complete, and reasonably efficient. We implement QEBVerif and conduct extensive experiments, showing its effectiveness and efficiency.\n        \u25b3 Less\n      ",
    "title": "QEBVerif: Quantization Error Bound Verification of Neural Networks",
    "date": "23 May, 2023",
    "authors": [
      "Yedi Zhang",
      " Fu Song",
      " Jun Sun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.04572",
    "paper_id": "2305.04572",
    "abstract": "\n        Human language is firstly spoken and only secondarily written. Text, however, is a very convenient and efficient representation of language, and modern civilization has made it ubiquitous. Thus the field of NLP has overwhelmingly focused on processing written rather than spoken language. Work on spoken language, on the other hand, has been siloed off within the largely separate speech processing community which has been inordinately preoccupied with transcribing speech into text. Recent advances in deep learning have led to a fortuitous convergence in methods between speech processing and mainstream NLP. Arguably, the time is ripe for a unification of these two fields, and for starting to take spoken language seriously as the primary mode of human communication. Truly natural language processing could lead to better integration with the rest of language science and could lead to systems which are more data-efficient and more human-like, and which can communicate beyond the textual modality.\n        \u25b3 Less\n      ",
    "title": "Putting Natural in Natural Language Processing",
    "date": "23 May, 2023",
    "authors": [
      "Grzegorz Chrupa\u0142a"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.17141",
    "paper_id": "2305.17141",
    "abstract": "\n        In a multi-agent environment, In order to overcome and alleviate the non-stationarity of the multi-agent environment, the mainstream method is to adopt the framework of Centralized Training Decentralized Execution (CTDE). This thesis is based on the framework of CTDE, and studies the cooperative decision-making of multi-agent based on the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm for multi-agent proximal policy optimization. In order to alleviate the non-stationarity of the multi-agent environment, a multi-agent communication mechanism based on weight scheduling and attention module is introduced. Different agents can alleviate the non-stationarity caused by local observations through information exchange between agents, assisting in the collaborative decision-making of agents. The specific method is to introduce a communication module in the policy network part. The communication module is composed of a weight generator, a weight scheduler, a message encoder, a message pool and an attention module. Among them, the weight generator and weight scheduler will generate weights as the selection basis for communication, the message encoder is used to compress and encode communication information, the message pool is used to store communication messages, and the attention module realizes the interactive processing of the agent's own information and communication information. This thesis proposes a Multi-Agent Communication and Global Information Optimization Proximal Policy Optimization(MCGOPPO)algorithm, and conducted experiments in the SMAC and the MPE. The experimental results show that the improvement has achieved certain effects, which can better alleviate the non-stationarity of the multi-agent environment, and improve the collaborative decision-making ability among the agents.\n        \u25b3 Less\n      ",
    "title": "Research on Multi-Agent Communication and Collaborative Decision-Making Based on Deep Reinforcement Learning",
    "date": "23 May, 2023",
    "authors": [
      "Zeng Da"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14098",
    "paper_id": "2305.14098",
    "abstract": "\n        Explainability of AI models is an important topic that can have a significant impact in all domains and applications from autonomous driving to healthcare. The existing approaches to explainable AI (XAI) are mainly limited to simple machine learning algorithms, and the research regarding the explainability-accuracy tradeoff is still in its infancy especially when we are concerned about complex machine learning techniques like neural networks and deep learning (DL). In this work, we introduce a new approach for complex models based on the co-relation impact which enhances the explainability considerably while also ensuring the accuracy at a high level. We propose approaches for both scenarios of independent features and dependent features. In addition, we study the uncertainty associated with features and output. Furthermore, we provide an upper bound of the computation complexity of our proposed approach for the dependent features. The complexity bound depends on the order of logarithmic of the number of observations which provides a reliable result considering the higher dimension of dependent feature space with a smaller number of observations.\n        \u25b3 Less\n      ",
    "title": "Balancing Explainability-Accuracy of Complex Models",
    "date": "23 May, 2023",
    "authors": [
      "Poushali Sengupta",
      " Yan Zhang",
      " Sabita Maharjan",
      " Frank Eliassen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14103",
    "paper_id": "2305.14103",
    "abstract": "\n        Understanding the evolution of online news communities is essential for designing more effective news recommender systems. However, due to the lack of appropriate datasets and platforms, the existing literature is limited in understanding the impact of recommender systems on this evolutionary process and the underlying mechanisms, resulting in sub-optimal system designs that may affect long-term utilities. In this work, we propose SimuLine, a simulation platform to dissect the evolution of news recommendation ecosystems and present a detailed analysis of the evolutionary process and underlying mechanisms. SimuLine first constructs a latent space well reflecting the human behaviors, and then simulates the news recommendation ecosystem via agent-based modeling. Based on extensive simulation experiments and the comprehensive analysis framework consisting of quantitative metrics, visualization, and textual explanations, we analyze the characteristics of each evolutionary phase from the perspective of life-cycle theory, and propose a relationship graph illustrating the key factors and affecting mechanisms. Furthermore, we explore the impacts of recommender system designing strategies, including the utilization of cold-start news, breaking news, and promotion, on the evolutionary process, which shed new light on the design of recommender systems.\n        \u25b3 Less\n      ",
    "title": "Simulating News Recommendation Ecosystem for Fun and Profit",
    "date": "23 May, 2023",
    "authors": [
      "Guangping Zhang",
      " Dongsheng Li",
      " Hansu Gu",
      " Tun Lu",
      " Li Shang",
      " Ning Gu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14104",
    "paper_id": "2305.14104",
    "abstract": "\n        Machine learning (ML) systems in natural language processing (NLP) face significant challenges in generalizing to out-of-distribution (OOD) data, where the test distribution differs from the training data distribution. This poses important questions about the robustness of NLP models and their high accuracy, which may be artificially inflated due to their underlying sensitivity to systematic biases. Despite these challenges, there is a lack of comprehensive surveys on the generalization challenge from an OOD perspective in text classification. Therefore, this paper aims to fill this gap by presenting the first comprehensive review of recent progress, methods, and evaluations on this topic. We furth discuss the challenges involved and potential future research directions. By providing quick access to existing work, we hope this survey will encourage future research in this area.\n        \u25b3 Less\n      ",
    "title": "Out-of-Distribution Generalization in Text Classification: Past, Present, and Future",
    "date": "23 May, 2023",
    "authors": [
      "Linyi Yang",
      " Yaoxiao Song",
      " Xuan Ren",
      " Chenyang Lyu",
      " Yidong Wang",
      " Lingqiao Liu",
      " Jindong Wang",
      " Jennifer Foster",
      " Yue Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14106",
    "paper_id": "2305.14106",
    "abstract": "\n        Modern large language models (LLMs) have demonstrated impressive capabilities at sophisticated tasks, often through step-by-step reasoning similar to humans. This is made possible by their strong few and zero-shot abilities -- they can effectively learn from a handful of handcrafted, completed responses (\"in-context examples\"), or are prompted to reason spontaneously through specially designed triggers. Nonetheless, some limitations have been observed. First, performance in the few-shot setting is sensitive to the choice of examples, whose design requires significant human effort. Moreover, given the diverse downstream tasks of LLMs, it may be difficult or laborious to handcraft per-task labels. Second, while the zero-shot setting does not require handcrafting, its performance is limited due to the lack of guidance to the LLMs. To address these limitations, we propose Consistency-based Self-adaptive Prompting (COSP), a novel prompt design method for LLMs. Requiring neither handcrafted responses nor ground-truth labels, COSP selects and builds the set of examples from the LLM zero-shot outputs via carefully designed criteria that combine consistency, diversity and repetition. In the zero-shot setting for three different LLMs, we show that using only LLM predictions, COSP improves performance up to 15% compared to zero-shot baselines and matches or exceeds few-shot baselines for a range of reasoning tasks.\n        \u25b3 Less\n      ",
    "title": "Better Zero-Shot Reasoning with Self-Adaptive Prompting",
    "date": "23 May, 2023",
    "authors": [
      "Xingchen Wan",
      " Ruoxi Sun",
      " Hanjun Dai",
      " Sercan O. Arik",
      " Tomas Pfister"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14109",
    "paper_id": "2305.14109",
    "abstract": "\n        Deploying Deep Neural Networks (DNNs) on tiny devices is a common trend to process the increasing amount of sensor data being generated. Multi-objective optimization approaches can be used to compress DNNs by applying network pruning and weight quantization to minimize the memory footprint (RAM), the number of parameters (ROM) and the number of floating point operations (FLOPs) while maintaining the predictive accuracy. In this paper, we show that existing multi-objective Bayesian optimization (MOBOpt) approaches can fall short in finding optimal candidates on the Pareto front and propose a novel solver based on an ensemble of competing parametric policies trained using an Augmented Random Search Reinforcement Learning (RL) agent. Our methodology aims at finding feasible tradeoffs between a DNN's predictive accuracy, memory consumption on a given target system, and computational complexity. Our experiments show that we outperform existing MOBOpt approaches consistently on different data sets and architectures such as ResNet-18 and MobileNetV3.\n        \u25b3 Less\n      ",
    "title": "Augmented Random Search for Multi-Objective Bayesian Optimization of Neural Networks",
    "date": "23 May, 2023",
    "authors": [
      "Mark Deutel",
      " Georgios Kontes",
      " Christopher Mutschler",
      " J\u00fcrgen Teich"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.06009",
    "paper_id": "2211.06009",
    "abstract": "\n        Intelligent Mesh Generation (IMG) represents a novel and promising field of research, utilizing machine learning techniques to generate meshes. Despite its relative infancy, IMG has significantly broadened the adaptability and practicality of mesh generation techniques, delivering numerous breakthroughs and unveiling potential future pathways. However, a noticeable void exists in the contemporary literature concerning comprehensive surveys of IMG methods. This paper endeavors to fill this gap by providing a systematic and thorough survey of the current IMG landscape. With a focus on 113 preliminary IMG methods, we undertake a meticulous analysis from various angles, encompassing core algorithm techniques and their application scope, agent learning objectives, data types, targeted challenges, as well as advantages and limitations. We have curated and categorized the literature, proposing three unique taxonomies based on key techniques, output mesh unit elements, and relevant input data types. This paper also underscores several promising future research directions and challenges in IMG. To augment reader accessibility, a dedicated IMG project page is available at \\url{https://github.com/xzb030/IMG_Survey}.\n        \u25b3 Less\n      ",
    "title": "What's the Situation with Intelligent Mesh Generation: A Survey and Perspectives",
    "date": "23 May, 2023",
    "authors": [
      "Na Lei",
      " Zezeng Li",
      " Zebin Xu",
      " Ying Li",
      " Xianfeng Gu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14115",
    "paper_id": "2305.14115",
    "abstract": "\n        Data quality or data evaluation is sometimes a task as important as collecting a large volume of data when it comes to generating accurate artificial intelligence models. In fact, being able to evaluate the data can lead to a larger database that is better suited to a particular problem because we have the ability to filter out data obtained automatically of dubious quality. In this paper we present RLBoost, an algorithm that uses deep reinforcement learning strategies to evaluate a particular dataset and obtain a model capable of estimating the quality of any new data in order to improve the final predictive quality of a supervised learning model. This solution has the advantage that of being agnostic regarding the supervised model used and, through multi-attention strategies, takes into account the data in its context and not only individually. The results of the article show that this model obtains better and more stable results than other state-of-the-art algorithms such as LOO, DataShapley or DVRL.\n        \u25b3 Less\n      ",
    "title": "RLBoost: Boosting Supervised Models using Deep Reinforcement Learning",
    "date": "23 May, 2023",
    "authors": [
      "Eloy Anguiano Batanero",
      " \u00c1ngela Fern\u00e1ndez Pascual",
      " \u00c1lvaro Barbero Jim\u00e9nez"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14126",
    "paper_id": "2305.14126",
    "abstract": "\n        Embedding models have shown great power in knowledge graph completion (KGC) task. By learning structural constraints for each training triple, these methods implicitly memorize intrinsic relation rules to infer missing links. However, this paper points out that the multi-hop relation rules are hard to be reliably memorized due to the inherent deficiencies of such implicit memorization strategy, making embedding models underperform in predicting links between distant entity pairs. To alleviate this problem, we present Vertical Learning Paradigm (VLP), which extends embedding models by allowing to explicitly copy target information from related factual triples for more accurate prediction. Rather than solely relying on the implicit memory, VLP directly provides additional cues to improve the generalization ability of embedding models, especially making the distant link prediction significantly easier. Moreover, we also propose a novel relative distance based negative sampling technique (ReD) for more effective optimization. Experiments demonstrate the validity and generality of our proposals on two standard benchmarks. Our code is available at https://github.com/rui9812/VLP.\n        \u25b3 Less\n      ",
    "title": "To Copy Rather Than Memorize: A Vertical Learning Paradigm for Knowledge Graph Completion",
    "date": "23 May, 2023",
    "authors": [
      "Rui Li",
      " Xu Chen",
      " Chaozhuo Li",
      " Yanming Shen",
      " Jianan Zhao",
      " Yujing Wang",
      " Weihao Han",
      " Hao Sun",
      " Weiwei Deng",
      " Qi Zhang",
      " Xing Xie"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14128",
    "paper_id": "2305.14128",
    "abstract": "\n        In-context learning (ICL), teaching a large language model (LLM) to perform a task with few-shot demonstrations rather than adjusting the model parameters, has emerged as a strong paradigm for using LLMs. While early studies primarily used a fixed or random set of demonstrations for all test queries, recent research suggests that retrieving semantically similar demonstrations to the input from a pool of available demonstrations results in better performance. This work expands the applicability of retrieval-based ICL approaches by demonstrating that even simple word-overlap similarity measures such as BM25 outperform randomly selected demonstrations. Furthermore, we extend the success of retrieval-based ICL to instruction-finetuned LLMs as well as Chain-of-Thought (CoT) prompting. For instruction-finetuned LLMs, we find that although a model has already seen the training data at training time, retrieving demonstrations from the training data at test time yields better results compared to using no demonstrations or random demonstrations. Last but not least, we train a task-specific demonstration retriever that outperforms off-the-shelf retrievers.\n        \u25b3 Less\n      ",
    "title": "Dr.ICL: Demonstration-Retrieved In-context Learning",
    "date": "23 May, 2023",
    "authors": [
      "Man Luo",
      " Xin Xu",
      " Zhuyun Dai",
      " Panupong Pasupat",
      " Mehran Kazemi",
      " Chitta Baral",
      " Vaiva Imbrasaite",
      " Vincent Y Zhao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14142",
    "paper_id": "2305.14142",
    "abstract": "\n        Postoperative infection diagnosis is a common and serious complication that generally poses a high diagnostic challenge. This study focuses on PJI, a type of postoperative infection. X-ray examination is an imaging examination for suspected PJI patients that can evaluate joint prostheses and adjacent tissues, and detect the cause of pain. Laboratory examination data has high sensitivity and specificity and has significant potential in PJI diagnosis. In this study, we proposed a self-supervised masked autoencoder pre-training strategy and a multimodal fusion diagnostic network MED-NVC, which effectively implements the interaction between two modal features through the feature fusion network of CrossAttention. We tested our proposed method on our collected PJI dataset and evaluated its performance and feasibility through comparison and ablation experiments. The results showed that our method achieved an ACC of 94.71% and an AUC of 98.22%, which is better than the latest method and also reduces the number of parameters. Our proposed method has the potential to provide clinicians with a powerful tool for enhancing accuracy and efficiency.\n        \u25b3 Less\n      ",
    "title": "A multimodal method based on cross-attention and convolution for postoperative infection diagnosis",
    "date": "23 May, 2023",
    "authors": [
      "Xianjie Liu",
      " Hongwei Shi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14150",
    "paper_id": "2305.14150",
    "abstract": "\n        To fully evaluate the overall performance of different NLP models in a given domain, many evaluation benchmarks are proposed, such as GLUE, SuperGLUE and CLUE. The fi eld of natural language understanding has traditionally focused on benchmarks for various tasks in languages such as Chinese, English, and multilingua, however, there has been a lack of attention given to the area of classical Chinese, also known as \"wen yan wen\", which has a rich history spanning thousands of years and holds signifi cant cultural and academic value. For the prosperity of the NLP community, in this paper, we introduce the WYWEB evaluation benchmark, which consists of nine NLP tasks in classical Chinese, implementing sentence classifi cation, sequence labeling, reading comprehension, and machine translation. We evaluate the existing pre-trained language models, which are all struggling with this benchmark. We also introduce a number of supplementary datasets and additional tools to help facilitate further progress on classical Chinese NLU. The github repository is https://github.com/baudzhou/WYWEB.\n        \u25b3 Less\n      ",
    "title": "WYWEB: A NLP Evaluation Benchmark For Classical Chinese",
    "date": "23 May, 2023",
    "authors": [
      "Bo Zhou",
      " Qianglong Chen",
      " Tianyu Wang",
      " Xiaomi Zhong",
      " Yin Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14165",
    "paper_id": "2305.14165",
    "abstract": "\n        Deep neural networks (DNNs) have made remarkable strides in various computer vision tasks, including image classification, segmentation, and object detection. However, recent research has revealed a vulnerability in advanced DNNs when faced with deliberate manipulations of input data, known as adversarial attacks. Moreover, the accuracy of DNNs is heavily influenced by the distribution of the training dataset. Distortions or perturbations in the color space of input images can introduce out-of-distribution data, resulting in misclassification. In this work, we propose a brightness-variation dataset, which incorporates 24 distinct brightness levels for each image within a subset of ImageNet. This dataset enables us to simulate the effects of light and shadow on the images, so as is to investigate the impact of light and shadow on the performance of DNNs. In our study, we conduct experiments using several state-of-the-art DNN architectures on the aforementioned dataset. Through our analysis, we discover a noteworthy positive correlation between the brightness levels and the loss of accuracy in DNNs. Furthermore, we assess the effectiveness of recently proposed robust training techniques and strategies, including AugMix, Revisit, and Free Normalizer, using the ResNet50 architecture on our brightness-variation dataset. Our experimental results demonstrate that these techniques can enhance the robustness of DNNs against brightness variation, leading to improved performance when dealing with images exhibiting varying brightness levels.\n        \u25b3 Less\n      ",
    "title": "Impact of Light and Shadow on Robustness of Deep Neural Networks",
    "date": "23 May, 2023",
    "authors": [
      "Chengyin Hu",
      " Weiwen Shi",
      " Chao Li",
      " Jialiang Sun",
      " Donghua Wang",
      " Junqi Wu",
      " Guijian Tang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2206.12251",
    "paper_id": "2206.12251",
    "abstract": "\n        Although deep neural networks (DNNs) are known to be fragile, no one has studied the effects of zooming-in and zooming-out of images in the physical world on DNNs performance. In this paper, we demonstrate a novel physical adversarial attack technique called Adversarial Zoom Lens (AdvZL), which uses a zoom lens to zoom in and out of pictures of the physical world, fooling DNNs without changing the characteristics of the target object. The proposed method is so far the only adversarial attack technique that does not add physical adversarial perturbation attack DNNs. In a digital environment, we construct a data set based on AdvZL to verify the antagonism of equal-scale enlarged images to DNNs. In the physical environment, we manipulate the zoom lens to zoom in and out of the target object, and generate adversarial samples. The experimental results demonstrate the effectiveness of AdvZL in both digital and physical environments. We further analyze the antagonism of the proposed data set to the improved DNNs. On the other hand, we provide a guideline for defense against AdvZL by means of adversarial training. Finally, we look into the threat possibilities of the proposed approach to future autonomous driving and variant attack ideas similar to the proposed attack.\n        \u25b3 Less\n      ",
    "title": "Adversarial Zoom Lens: A Novel Physical-World Attack to DNNs",
    "date": "23 May, 2023",
    "authors": [
      "Chengyin Hu",
      " Weiwen Shi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14173",
    "paper_id": "2305.14173",
    "abstract": "\n        The ultimate goal for foundation models is realizing task-agnostic, i.e., supporting out-of-the-box usage without task-specific fine-tuning. Although breakthroughs have been made in natural language processing and image representation learning, it is still challenging for video models to reach it due to the increasing uncertainty of spatiotemporal signals. To ease training, existing works leverage image foundation models' prior knowledge and equip them with efficient temporal modules. Despite the satisfactory fine-tuning performance, we empirically find they fall short of out-of-the-box usage, given the even degraded performance in zero-shot/linear protocols compared to their baseline counterparts. In this work, we analyze the factor that leads to degradation from the perspective of language supervision distortion. We argue that tuning a text encoder end-to-end, as done in previous work, is suboptimal since it may overfit in terms of styles, thereby losing its original generalization ability to capture the semantics of various language registers. The overfitted text encoder, in turn, provides a harmful supervision signal, degrading the video representation. To tackle this issue, we propose a degradation-free pre-training strategy to retain the generalization ability of the text encoder via freezing shallow layers while enabling the task-related semantics capturing in tunable deep layers. As for the training objective, we adopted the transcript sorting task in TVTS incorporated with masking techniques to enable scalable training. As a result, we produce a series of models, dubbed TVTSv2, with up to one billion parameters. We achieve new state-of-the-arts on various video benchmarks with a frozen backbone, surpassing the recent ImageBind, InternVideo, etc. Code is available at https://github.com/TencentARC/TVTS.\n        \u25b3 Less\n      ",
    "title": "TVTSv2: Learning Out-of-the-box Spatiotemporal Visual Representations at Scale",
    "date": "23 May, 2023",
    "authors": [
      "Ziyun Zeng",
      " Yixiao Ge",
      " Zhan Tong",
      " Xihui Liu",
      " Shu-Tao Xia",
      " Ying Shan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.02783",
    "paper_id": "2305.02783",
    "abstract": "\n        The recent improvement in code generation capabilities due to the use of large language models has mainly benefited general purpose programming languages. Domain specific languages, such as the ones used for IT Automation, have received far less attention, despite involving many active developers and being an essential component of modern cloud platforms. This work focuses on the generation of Ansible-YAML, a widely used markup language for IT Automation. We present Ansible Wisdom, a natural-language to Ansible-YAML code generation tool, aimed at improving IT automation productivity. Ansible Wisdom is a transformer-based model, extended by training with a new dataset containing Ansible-YAML. We also develop two novel performance metrics for YAML and Ansible to capture the specific characteristics of this domain. Results show that Ansible Wisdom can accurately generate Ansible script from natural language prompts with performance comparable or better than existing state of the art code generation models. In few-shot settings we asses the impact of training with Ansible, YAML data and compare with different baselines including Codex-Davinci-002. We also show that after finetuning, our Ansible specific model (BLEU: 66.67) can outperform a much larger Codex-Davinci-002 (BLEU: 50.4) model, which was evaluated in few shot settings.\n        \u25b3 Less\n      ",
    "title": "Automated Code generation for Information Technology Tasks in YAML through Large Language Models",
    "date": "23 May, 2023",
    "authors": [
      "Saurabh Pujar",
      " Luca Buratti",
      " Xiaojie Guo",
      " Nicolas Dupuis",
      " Burn Lewis",
      " Sahil Suneja",
      " Atin Sood",
      " Ganesh Nalawade",
      " Matthew Jones",
      " Alessandro Morari",
      " Ruchir Puri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14201",
    "paper_id": "2305.14201",
    "abstract": "\n        We introduce Goat, a fine-tuned LLaMA model that significantly outperforms GPT-4 on a range of arithmetic tasks. Fine-tuned on a synthetically generated dataset, Goat achieves state-of-the-art performance on BIG-bench arithmetic sub-task. In particular, the zero-shot Goat-7B matches or even surpasses the accuracy achieved by the few-shot PaLM-540B. Surprisingly, Goat can achieve near-perfect accuracy on large-number addition and subtraction through supervised fine-tuning only, which is almost impossible with previous pretrained language models, such as Bloom, OPT, GPT-NeoX, etc. We attribute Goat's exceptional performance to LLaMA's consistent tokenization of numbers. To tackle more challenging tasks like large-number multiplication and division, we propose an approach that classifies tasks based on their learnability, and subsequently decomposes unlearnable tasks, such as multi-digit multiplication and division, into a series of learnable tasks by leveraging basic arithmetic principles. We thoroughly examine the performance of our model, offering a comprehensive evaluation of the effectiveness of our proposed decomposition steps. Additionally, Goat-7B can be easily trained using LoRA on a 24GB VRAM GPU, facilitating reproducibility for other researchers. We release our model, dataset, and the Python script for dataset generation.\n        \u25b3 Less\n      ",
    "title": "Goat: Fine-tuned LLaMA Outperforms GPT-4 on Arithmetic Tasks",
    "date": "23 May, 2023",
    "authors": [
      "Tiedong Liu",
      " Bryan Kian Hsiang Low"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14223",
    "paper_id": "2305.14223",
    "abstract": "\n        Game-based decision-making involves reasoning over both world dynamics and strategic interactions among the agents. Typically, empirical models capturing these respective aspects are learned and used separately. We investigate the potential gain from co-learning these elements: a world model for dynamics and an empirical game for strategic interactions. Empirical games drive world models toward a broader consideration of possible game dynamics induced by a diversity of strategy profiles. Conversely, world models guide empirical games to efficiently discover new strategies through planning. We demonstrate these benefits first independently, then in combination as realized by a new algorithm, Dyna-PSRO, that co-learns an empirical game and a world model. When compared to PSRO -- a baseline empirical-game building algorithm, Dyna-PSRO is found to compute lower regret solutions on partially observable general-sum games. In our experiments, Dyna-PSRO also requires substantially fewer experiences than PSRO, a key algorithmic advantage for settings where collecting player-game interaction data is a cost-limiting factor.\n        \u25b3 Less\n      ",
    "title": "Co-Learning Empirical Games and World Models",
    "date": "23 May, 2023",
    "authors": [
      "Max Olan Smith",
      " Michael P. Wellman"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14233",
    "paper_id": "2305.14233",
    "abstract": "\n        Fine-tuning on instruction data has been widely validated as an effective practice for implementing chat language models like ChatGPT. Scaling the diversity and quality of such data, although straightforward, stands a great chance of leading to improved performance. This paper aims to improve the upper bound of open-source models further. We first provide a systematically designed, diverse, informative, large-scale dataset of instructional conversations, UltraChat, which does not involve human queries. Our objective is to capture the breadth of interactions that a human might have with an AI assistant and employs a comprehensive framework to generate multi-turn conversation iteratively. UltraChat contains 1.5 million high-quality multi-turn dialogues and covers a wide range of topics and instructions. Our statistical analysis of UltraChat reveals its superiority in various key metrics, including scale, average length, diversity, coherence, etc., solidifying its position as a leading open-source dataset. Building upon UltraChat, we fine-tune a LLaMA model to create a powerful conversational model, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently outperforms other open-source models, including Vicuna, the previously recognized state-of-the-art open-source model. The dataset and the model will be publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.\n        \u25b3 Less\n      ",
    "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
    "date": "23 May, 2023",
    "authors": [
      "Ning Ding",
      " Yulin Chen",
      " Bokai Xu",
      " Yujia Qin",
      " Zhi Zheng",
      " Shengding Hu",
      " Zhiyuan Liu",
      " Maosong Sun",
      " Bowen Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14237",
    "paper_id": "2305.14237",
    "abstract": "\n        Explainable multi-hop question answering (QA) not only predicts answers but also identifies rationales, i. e. subsets of input sentences used to derive the answers. This problem has been extensively studied under the supervised setting, where both answer and rationale annotations are given. Because rationale annotations are expensive to collect and not always available, recent efforts have been devoted to developing methods that do not rely on supervision for rationales. However, such methods have limited capacities in modeling interactions between sentences, let alone reasoning across multiple documents. This work proposes a principled, probabilistic approach for training explainable multi-hop QA systems without rationale supervision. Our approach performs multi-hop reasoning by explicitly modeling rationales as sets, enabling the model to capture interactions between documents and sentences within a document. Experimental results show that our approach is more accurate at selecting rationales than the previous methods, while maintaining similar accuracy in predicting answers.\n        \u25b3 Less\n      ",
    "title": "HOP, UNION, GENERATE: Explainable Multi-hop Reasoning without Rationale Supervision",
    "date": "23 May, 2023",
    "authors": [
      "Wenting Zhao",
      " Justin T. Chiu",
      " Claire Cardie",
      " Alexander M. Rush"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14240",
    "paper_id": "2305.14240",
    "abstract": "\n        Machine Translation (MT) has been widely used for cross-lingual classification, either by translating the test set into English and running inference with a monolingual model (translate-test), or translating the training set into the target languages and finetuning a multilingual model (translate-train). However, most research in the area focuses on the multilingual models rather than the MT component. We show that, by using a stronger MT system and mitigating the mismatch between training on original text and running inference on machine translated text, translate-test can do substantially better than previously assumed. The optimal approach, however, is highly task dependent, as we identify various sources of cross-lingual transfer gap that affect different tasks and approaches differently. Our work calls into question the dominance of multilingual models for cross-lingual classification, and prompts to pay more attention to MT-based baselines.\n        \u25b3 Less\n      ",
    "title": "Revisiting Machine Translation for Cross-lingual Classification",
    "date": "23 May, 2023",
    "authors": [
      "Mikel Artetxe",
      " Vedanuj Goswami",
      " Shruti Bhosale",
      " Angela Fan",
      " Luke Zettlemoyer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14258",
    "paper_id": "2305.14258",
    "abstract": "\n        Since acquiring perfect supervision is usually difficult, real-world machine learning tasks often confront inaccurate, incomplete, or inexact supervision, collectively referred to as weak supervision. In this work, we present WSAUC, a unified framework for weakly supervised AUC optimization problems, which covers noisy label learning, positive-unlabeled learning, multi-instance learning, and semi-supervised learning scenarios. Within the WSAUC framework, we first frame the AUC optimization problems in various weakly supervised scenarios as a common formulation of minimizing the AUC risk on contaminated sets, and demonstrate that the empirical risk minimization problems are consistent with the true AUC. Then, we introduce a new type of partial AUC, specifically, the reversed partial AUC (rpAUC), which serves as a robust training objective for AUC maximization in the presence of contaminated labels. WSAUC offers a universal solution for AUC optimization in various weakly supervised scenarios by maximizing the empirical rpAUC. Theoretical and experimental results under multiple settings support the effectiveness of WSAUC on a range of weakly supervised AUC optimization tasks.\n        \u25b3 Less\n      ",
    "title": "Weakly Supervised AUC Optimization: A Unified Partial AUC Approach",
    "date": "23 May, 2023",
    "authors": [
      "Zheng Xie",
      " Yu Liu",
      " Hao-Yuan He",
      " Ming Li",
      " Zhi-Hua Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12296",
    "paper_id": "2305.12296",
    "abstract": "\n        Authoring high-quality digital materials is key to realism in 3D rendering. Previous generative models for materials have been trained exclusively on synthetic data; such data is limited in availability and has a visual gap to real materials. We circumvent this limitation by proposing PhotoMat: the first material generator trained exclusively on real photos of material samples captured using a cell phone camera with flash. Supervision on individual material maps is not available in this setting. Instead, we train a generator for a neural material representation that is rendered with a learned relighting module to create arbitrarily lit RGB images; these are compared against real photos using a discriminator. We then train a material maps estimator to decode material reflectance properties from the neural material representation. We train PhotoMat with a new dataset of 12,000 material photos captured with handheld phone cameras under flash lighting. We demonstrate that our generated materials have better visual quality than previous material generators trained on synthetic data. Moreover, we can fit analytical material models to closely match these generated neural materials, thus allowing for further editing and use in 3D rendering.\n        \u25b3 Less\n      ",
    "title": "PhotoMat: A Material Generator Learned from Single Flash Photos",
    "date": "23 May, 2023",
    "authors": [
      "Xilong Zhou",
      " Milo\u0161 Ha\u0161an",
      " Valentin Deschaintre",
      " Paul Guerrero",
      " Yannick Hold-Geoffroy",
      " Kalyan Sunkavalli",
      " Nima Khademi Kalantari"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14299",
    "paper_id": "2305.14299",
    "abstract": "\n        Learning high quality sentence embeddings from dialogues has drawn increasing attentions as it is essential to solve a variety of dialogue-oriented tasks with low annotation cost. However, directly annotating and gathering utterance relationships in conversations are difficult, while token-level annotations, \\eg, entities, slots and templates, are much easier to obtain. General sentence embedding methods are usually sentence-level self-supervised frameworks and cannot utilize token-level extra knowledge. In this paper, we introduce Template-aware Dialogue Sentence Embedding (TaDSE), a novel augmentation method that utilizes template information to effectively learn utterance representation via self-supervised contrastive learning framework. TaDSE augments each sentence with its corresponding template and then conducts pairwise contrastive learning over both sentence and template. We further enhance the effect with a synthetically augmented dataset that enhances utterance-template relation, in which entity detection (slot-filling) is a preliminary step. We evaluate TaDSE performance on five downstream benchmark datasets. The experiment results show that TaDSE achieves significant improvements over previous SOTA methods, along with a consistent Intent Classification task performance improvement margin. We further introduce a novel analytic instrument of Semantic Compression method, for which we discover a correlation with uniformity and alignment. Our code will be released soon.\n        \u25b3 Less\n      ",
    "title": "TaDSE: Template-aware Dialogue Sentence Embeddings",
    "date": "23 May, 2023",
    "authors": [
      "Minsik Oh",
      " Jiwei Li",
      " Guoyin Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14307",
    "paper_id": "2305.14307",
    "abstract": "\n        Debiasing methods that seek to mitigate the tendency of Language Models (LMs) to occasionally output toxic or inappropriate text have recently gained traction. In this paper, we propose a standardized protocol which distinguishes methods that yield not only desirable results, but are also consistent with their mechanisms and specifications. For example, we ask, given a debiasing method that is developed to reduce toxicity in LMs, if the definition of toxicity used by the debiasing method is reversed, would the debiasing results also be reversed? We used such considerations to devise three criteria for our new protocol: Specification Polarity, Specification Importance, and Domain Transferability. As a case study, we apply our protocol to a popular debiasing method, Self-Debiasing, and compare it to one we propose, called Instructive Debiasing, and demonstrate that consistency is as important an aspect to debiasing viability as is simply a desirable result. We show that our protocol provides essential insights into the generalizability and interpretability of debiasing methods that may otherwise go overlooked.\n        \u25b3 Less\n      ",
    "title": "Debiasing should be Good and Bad: Measuring the Consistency of Debiasing Techniques in Language Models",
    "date": "23 May, 2023",
    "authors": [
      "Robert Morabito",
      " Jad Kabbara",
      " Ali Emami"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14325",
    "paper_id": "2305.14325",
    "abstract": "\n        Large language models (LLMs) have demonstrated remarkable capabilities in language generation, understanding, and few-shot learning in recent years. An extensive body of work has explored how their performance may be further improved through the tools of prompting, ranging from verification, self-consistency, or intermediate scratchpads. In this paper, we present a complementary approach to improve language responses where multiple language model instances propose and debate their individual responses and reasoning processes over multiple rounds to arrive at a common final answer. Our findings indicate that this approach significantly enhances mathematical and strategic reasoning across a number of tasks. We also demonstrate that our approach improves the factual validity of generated content, reducing fallacious answers and hallucinations that contemporary models are prone to. Our approach may be directly applied to existing black-box models and uses identical procedure and prompts for all tasks we investigate. Overall, our findings suggest that such \"society of minds\" approach has the potential to significantly advance the capabilities of LLMs and pave the way for further breakthroughs in language generation and understanding.\n        \u25b3 Less\n      ",
    "title": "Improving Factuality and Reasoning in Language Models through Multiagent Debate",
    "date": "23 May, 2023",
    "authors": [
      "Yilun Du",
      " Shuang Li",
      " Antonio Torralba",
      " Joshua B. Tenenbaum",
      " Igor Mordatch"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14451",
    "paper_id": "2305.14451",
    "abstract": "\n        Structured kernel interpolation (SKI) accelerates Gaussian process (GP) inference by interpolating the kernel covariance function using a dense grid of inducing points, whose corresponding kernel matrix is highly structured and thus amenable to fast linear algebra. Unfortunately, SKI scales poorly in the dimension of the input points, since the dense grid size grows exponentially with the dimension. To mitigate this issue, we propose the use of sparse grids within the SKI framework. These grids enable accurate interpolation, but with a number of points growing more slowly with dimension. We contribute a novel nearly linear time matrix-vector multiplication algorithm for the sparse grid kernel matrix. Next, we describe how sparse grids can be combined with an efficient interpolation scheme based on simplices. With these changes, we demonstrate that SKI can be scaled to higher dimensions while maintaining accuracy.\n        \u25b3 Less\n      ",
    "title": "Kernel Interpolation with Sparse Grids",
    "date": "23 May, 2023",
    "authors": [
      "Mohit Yadav",
      " Daniel Sheldon",
      " Cameron Musco"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.09662",
    "paper_id": "2212.09662",
    "abstract": "\n        Visual language data such as plots, charts, and infographics are ubiquitous in the human world. However, state-of-the-art vision-language models do not perform well on these data. We propose MatCha (Math reasoning and Chart derendering pretraining) to enhance visual language models' capabilities in jointly modeling charts/plots and language data. Specifically, we propose several pretraining tasks that cover plot deconstruction and numerical reasoning which are the key capabilities in visual language modeling.\n  We perform the MatCha pretraining starting from Pix2Struct, a recently proposed image-to-text visual language model. On standard benchmarks such as PlotQA and ChartQA, the MatCha model outperforms state-of-the-art methods by as much as nearly 20%. We also examine how well MatCha pretraining transfers to domains such as screenshots, textbook diagrams, and document figures and observe overall improvement, verifying the usefulness of MatCha pretraining on broader visual language tasks.\n        \u25b3 Less\n      ",
    "title": "MatCha: Enhancing Visual Language Pretraining with Math Reasoning and Chart Derendering",
    "date": "23 May, 2023",
    "authors": [
      "Fangyu Liu",
      " Francesco Piccinno",
      " Syrine Krichene",
      " Chenxi Pang",
      " Kenton Lee",
      " Mandar Joshi",
      " Yasemin Altun",
      " Nigel Collier",
      " Julian Martin Eisenschlos"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.10505",
    "paper_id": "2212.10505",
    "abstract": "\n        Visual language such as charts and plots is ubiquitous in the human world. Comprehending plots and charts requires strong reasoning skills. Prior state-of-the-art (SOTA) models require at least tens of thousands of training examples and their reasoning capabilities are still much limited, especially on complex human-written queries. This paper presents the first one-shot solution to visual language reasoning. We decompose the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The key in this method is a modality conversion module, named as DePlot, which translates the image of a plot or chart to a linearized table. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs. To obtain DePlot, we standardize the plot-to-table task by establishing unified task formats and metrics, and train DePlot end-to-end on this task. DePlot can then be used off-the-shelf together with LLMs in a plug-and-play fashion. Compared with a SOTA model finetuned on more than >28k data points, DePlot+LLM with just one-shot prompting achieves a 24.0% improvement over finetuned SOTA on human-written queries from the task of chart QA.\n        \u25b3 Less\n      ",
    "title": "DePlot: One-shot visual language reasoning by plot-to-table translation",
    "date": "23 May, 2023",
    "authors": [
      "Fangyu Liu",
      " Julian Martin Eisenschlos",
      " Francesco Piccinno",
      " Syrine Krichene",
      " Chenxi Pang",
      " Kenton Lee",
      " Mandar Joshi",
      " Wenhu Chen",
      " Nigel Collier",
      " Yasemin Altun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.06801",
    "paper_id": "2212.06801",
    "abstract": "\n        Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.\n        \u25b3 Less\n      ",
    "title": "A fine-grained comparison of pragmatic language understanding in humans and language models",
    "date": "23 May, 2023",
    "authors": [
      "Jennifer Hu",
      " Sammy Floyd",
      " Olessia Jouravlev",
      " Evelina Fedorenko",
      " Edward Gibson"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2109.14099",
    "paper_id": "2109.14099",
    "abstract": "\n        The severe acute respiratory syndrome coronavirus type-2 (SARS-CoV-2) caused a global pandemic and immensely affected the global economy. Accurate, cost-effective, and quick tests have proven substantial in identifying infected people and mitigating the spread. Recently, multiple alternative platforms for testing coronavirus disease 2019 (COVID-19) have been published that show high agreement with current gold standard real-time polymerase chain reaction (RT-PCR) results. These new methods do away with nasopharyngeal (NP) swabs, eliminate the need for complicated reagents, and reduce the burden on RT-PCR test reagent supply. In the present work, we have designed an artificial intelligence-based (AI) testing method to provide confidence in the results. Current AI applications for COVID-19 studies often lack a biological foundation in the decision-making process, and our AI approach is one of the earliest to leverage explainable AI (X-AI) algorithms for COVID-19 diagnosis using mass spectrometry. Here, we have employed X-AI to explain the decision-making process on a local (per-sample) and global (all samples) basis underscored by biologically relevant features. We evaluated our technique with data extracted from human gargle samples and achieved a testing accuracy of 94.12%. Such techniques would strengthen the relationship between AI and clinical diagnostics by providing biomedical researchers and healthcare workers with trustworthy and, most importantly, explainable test results\n        \u25b3 Less\n      ",
    "title": "An Explainable-AI approach for Diagnosis of COVID-19 using MALDI-ToF Mass Spectrometry",
    "date": "23 May, 2023",
    "authors": [
      "Venkata Devesh Reddy Seethi",
      " Zane LaCasse",
      " Prajkta Chivte",
      " Joshua Bland",
      " Shrihari S. Kadkol",
      " Elizabeth R. Gaillard",
      " Pratool Bharti",
      " Hamed Alhoori"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.00347",
    "paper_id": "2209.00347",
    "abstract": "\n        A key challenge of continual reinforcement learning (CRL) in dynamic environments is to promptly adapt the RL agent's behavior as the environment changes over its lifetime, while minimizing the catastrophic forgetting of the learned information. To address this challenge, in this article, we propose DaCoRL, i.e., dynamics-adaptive continual RL. DaCoRL learns a context-conditioned policy using progressive contextualization, which incrementally clusters a stream of stationary tasks in the dynamic environment into a series of contexts and opts for an expandable multihead neural network to approximate the policy. Specifically, we define a set of tasks with similar dynamics as an environmental context and formalize context inference as a procedure of online Bayesian infinite Gaussian mixture clustering on environment features, resorting to online Bayesian inference to infer the posterior distribution over contexts. Under the assumption of a Chinese restaurant process prior, this technique can accurately classify the current task as a previously seen context or instantiate a new context as needed without relying on any external indicator to signal environmental changes in advance. Furthermore, we employ an expandable multihead neural network whose output layer is synchronously expanded with the newly instantiated context, and a knowledge distillation regularization term for retaining the performance on learned tasks. As a general framework that can be coupled with various deep RL algorithms, DaCoRL features consistent superiority over existing methods in terms of the stability, overall performance and generalization ability, as verified by extensive experiments on several robot navigation and MuJoCo locomotion tasks.\n        \u25b3 Less\n      ",
    "title": "Dynamics-Adaptive Continual Reinforcement Learning via Progressive Contextualization",
    "date": "23 May, 2023",
    "authors": [
      "Tiantian Zhang",
      " Zichuan Lin",
      " Yuxing Wang",
      " Deheng Ye",
      " Qiang Fu",
      " Wei Yang",
      " Xueqian Wang",
      " Bin Liang",
      " Bo Yuan",
      " Xiu Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.10974",
    "paper_id": "2303.10974",
    "abstract": "\n        Neural networks are deployed widely in natural language processing tasks on the industrial scale, and perhaps the most often they are used as compounds of automatic machine translation systems. In this work, we present a simple approach to fool state-of-the-art machine translation tools in the task of translation from Russian to English and vice versa. Using a novel black-box gradient-free tensor-based optimizer, we show that many online translation tools, such as Google, DeepL, and Yandex, may both produce wrong or offensive translations for nonsensical adversarial input queries and refuse to translate seemingly benign input phrases. This vulnerability may interfere with understanding a new language and simply worsen the user's experience while using machine translation systems, and, hence, additional improvements of these tools are required to establish better translation.\n        \u25b3 Less\n      ",
    "title": "Translate your gibberish: black-box adversarial attack on machine translation systems",
    "date": "23 May, 2023",
    "authors": [
      "Andrei Chertkov",
      " Olga Tsymboi",
      " Mikhail Pautov",
      " Ivan Oseledets"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.03460",
    "paper_id": "2302.03460",
    "abstract": "\n        Over the past decade explainable artificial intelligence has evolved from a predominantly technical discipline into a field that is deeply intertwined with social sciences. Insights such as human preference for contrastive -- more precisely, counterfactual -- explanations have played a major role in this transition, inspiring and guiding the research in computer science. Other observations, while equally important, have received much less attention. The desire of human explainees to communicate with artificial intelligence explainers through a dialogue-like interaction has been mostly neglected by the community. This poses many challenges for the effectiveness and widespread adoption of such technologies as delivering a single explanation optimised according to some predefined objectives may fail to engender understanding in its recipients and satisfy their unique needs given the diversity of human knowledge and intention. Using insights elaborated by Niklas Luhmann and, more recently, Elena Esposito we apply social systems theory to highlight challenges in explainable artificial intelligence and offer a path forward, striving to reinvigorate the technical research in this direction. This paper aims to demonstrate the potential of systems theoretical approaches to communication in understanding problems and limitations of explainable artificial intelligence.\n        \u25b3 Less\n      ",
    "title": "Mind the Gap! Bridging Explainable Artificial Intelligence and Human Understanding with Luhmann's Functional Theory of Communication",
    "date": "23 May, 2023",
    "authors": [
      "Bernard Keenan",
      " Kacper Sokol"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14497",
    "paper_id": "2305.14497",
    "abstract": "\n        Prompting methods such as Chain-of-Thought (CoT) have shed new light on enhancing the reasoning capabilities of large language models, and researchers have extensively explored the generation process of rationales and answers. However, they have overlooked the potential challenges posed by the poor quality of reasoning problems, which may influence the reasoning performance significantly. In this work, we propose Self-Polish (SP), a novel method that facilitates the model's problem-solving process by prompting them to progressively refine the given problems to be more comprehensible and solvable. Specifically, the method teaches models to eliminate irrelevant information, rearrange the logic structure and organize local conditions into new ones parallelly. SP is orthogonal to all other prompting methods, making it convenient to integrate with state-of-the-art techniques for further improvement. We conduct thorough experiments on five benchmarks to illustrate the effectiveness of the proposed method. For example, with Text-davinci-003, our method boosts the performance of standard few-shot prompting by 8.0%8.0\\% on GSM8K and 17.8%17.8\\% on MultiArith; it also improves the performance of CoT by 6.0%6.0\\% on GSM8K and 6.0%6.0\\% on MathQA, respectively. Furthermore, our method also showcases impressive performance on robustness evaluation.\n        \u25b3 Less\n      ",
    "title": "Self-Polish: Enhance Reasoning in Large Language Models via Problem Refinement",
    "date": "23 May, 2023",
    "authors": [
      "Zhiheng Xi",
      " Senjie Jin",
      " Yuhao Zhou",
      " Rui Zheng",
      " Songyang Gao",
      " Tao Gui",
      " Qi Zhang",
      " Xuanjing Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14502",
    "paper_id": "2305.14502",
    "abstract": "\n        Many recent developments in large language models focus on prompting them to perform specific tasks. One effective prompting method is in-context learning, where the model performs a (possibly new) generation/prediction task given one (or more) examples. Past work has shown that the choice of examples can make a large impact on task performance. However, finding good examples is not straightforward since the definition of a representative group of examples can vary greatly depending on the task. While there are many existing methods for selecting in-context examples, they generally score examples independently, ignoring the dependency between them and the order in which they are provided to the large language model. In this work, we propose Retrieval for In-Context Learning (RetICL), a learnable method for modeling and optimally selecting examples sequentially for in-context learning. We frame the problem of sequential example selection as a Markov decision process, design an example retriever model using an LSTM, and train it using proximal policy optimization (PPO). We validate RetICL on math problem solving datasets and show that it outperforms both heuristic and learnable baselines, and achieves state-of-the-art accuracy on the TabMWP dataset. We also use case studies to show that RetICL implicitly learns representations of math problem solving strategies.\n        \u25b3 Less\n      ",
    "title": "RetICL: Sequential Retrieval of In-Context Examples with Reinforcement Learning",
    "date": "23 May, 2023",
    "authors": [
      "Alexander Scarlatos",
      " Andrew Lan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14513",
    "paper_id": "2305.14513",
    "abstract": "\n        Windscreen optical quality is an important aspect of any advanced driver assistance system, and also for future autonomous driving, as today at least some cameras of the sensor suite are situated behind the windscreen. Automotive mass production processes require measurement systems that characterize the optical quality of the windscreens in a meaningful way, which for modern perception stacks implies meaningful for artificial intelligence (AI) algorithms. The measured optical quality needs to be linked to the performance of these algorithms, such that performance limits - and thus production tolerance limits - can be defined. In this article we demonstrate that the main metric established in the industry - refractive power - is fundamentally not capable of capturing relevant optical properties of windscreens. Further, as the industry is moving towards the modulation transfer function (MTF) as an alternative, we mathematically show that this metric cannot be used on windscreens alone, but that the windscreen forms a novel optical system together with the optics of the camera system. Hence, the required goal of a qualification system that is installed at the windscreen supplier and independently measures the optical quality cannot be achieved using MTF. We propose a novel concept to determine the optical quality of windscreens and to use simulation to link this optical quality to the performance of AI algorithms, which can hopefully lead to novel inspection systems.\n        \u25b3 Less\n      ",
    "title": "Windscreen Optical Quality for AI Algorithms: Refractive Power and MTF not Sufficient",
    "date": "23 May, 2023",
    "authors": [
      "Dominik Werner Wolf",
      " Markus Ulrich",
      " Alexander Braun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11389",
    "paper_id": "2305.11389",
    "abstract": "\n        Graph transformation that predicts graph transition from one mode to another is an important and common problem. Despite much progress in developing advanced graph transformation techniques in recent years, the fundamental assumption typically required in machine-learning models that the testing and training data preserve the same distribution does not always hold. As a result, domain generalization graph transformation that predicts graphs not available in the training data is under-explored, with multiple key challenges to be addressed including (1) the extreme space complexity when training on all input-output mode combinations, (2) difference of graph topologies between the input and the output modes, and (3) how to generalize the model to (unseen) target domains that are not in the training data. To fill the gap, we propose a multi-input, multi-output, hypernetwork-based graph neural network (MultiHyperGNN) that employs a encoder and a decoder to encode topologies of both input and output modes and semi-supervised link prediction to enhance the graph transformation task. Instead of training on all mode combinations, MultiHyperGNN preserves a constant space complexity with the encoder and the decoder produced by two novel hypernetworks. Comprehensive experiments show that MultiHyperGNN has a superior performance than competing models in both prediction and domain generalization tasks.\n        \u25b3 Less\n      ",
    "title": "Domain Generalization Deep Graph Transformation",
    "date": "23 May, 2023",
    "authors": [
      "Shiyu Wang",
      " Guangji Bai",
      " Qingyang Zhu",
      " Zhaohui Qin",
      " Liang Zhao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.11089",
    "paper_id": "2302.11089",
    "abstract": "\n        This review article is an attempt to survey all recent AI based techniques used to deal with major functions in This review paper presents a comprehensive overview of end-to-end deep learning frameworks used in the context of autonomous navigation, including obstacle detection, scene perception, path planning, and control. The paper aims to bridge the gap between autonomous navigation and deep learning by analyzing recent research studies and evaluating the implementation and testing of deep learning methods. It emphasizes the importance of navigation for mobile robots, autonomous vehicles, and unmanned aerial vehicles, while also acknowledging the challenges due to environmental complexity, uncertainty, obstacles, dynamic environments, and the need to plan paths for multiple agents. The review highlights the rapid growth of deep learning in engineering data science and its development of innovative navigation methods. It discusses recent interdisciplinary work related to this field and provides a brief perspective on the limitations, challenges, and potential areas of growth for deep learning methods in autonomous navigation. Finally, the paper summarizes the findings and practices at different stages, correlating existing and future methods, their applicability, scalability, and limitations. The review provides a valuable resource for researchers and practitioners working in the field of autonomous navigation and deep learning.\n        \u25b3 Less\n      ",
    "title": "Recent Advancements in Deep Learning Applications and Methods for Autonomous Navigation: A Comprehensive Review",
    "date": "23 May, 2023",
    "authors": [
      "Arman Asgharpoor Golroudbari",
      " Mohammad Hossein Sabour"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14537",
    "paper_id": "2305.14537",
    "abstract": "\n        On social networks, algorithmic personalization drives users into filter bubbles where they rarely see content that deviates from their interests. We present a model for content curation and personalization that avoids filter bubbles, along with algorithmic guarantees and nearly matching lower bounds. In our model, the platform interacts with nn users over TT timesteps, choosing content for each user from kk categories. The platform receives stochastic rewards as in a multi-arm bandit. To avoid filter bubbles, we draw on the intuition that if some users are shown some category of content, then all users should see at least a small amount of that content. We first analyze a naive formalization of this intuition and show it has unintended consequences: it leads to ``tyranny of the majority'' with the burden of diversification borne disproportionately by those with minority interests. This leads us to our model which distributes this burden more equitably. We require that the probability any user is shown a particular type of content is at least \u03b3\u03b3 times the average probability all users are shown that type of content. Full personalization corresponds to \u03b3=0\u03b3= 0 and complete homogenization corresponds to \u03b3=1\u03b3= 1; hence, \u03b3\u03b3 encodes a hard cap on the level of personalization. We also analyze additional formulations where the platform can exceed its cap but pays a penalty proportional to its constraint violation. We provide algorithmic guarantees for optimizing recommendations subject to these constraints. These include nearly matching upper and lower bounds for the entire range of \u03b3\u2208[0,1]\u03b3\\in [0,1] showing that the reward of a multi-agent variant of UCB is nearly optimal. Using real-world preference data, we empirically verify that under our model, users share the burden of diversification with only minor utility loss under our constraints.\n        \u25b3 Less\n      ",
    "title": "Disincentivizing Polarization in Social Networks",
    "date": "23 May, 2023",
    "authors": [
      "Christian Borgs",
      " Jennifer Chayes",
      " Christian Ikeokwu",
      " Ellen Vitercik"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14538",
    "paper_id": "2305.14538",
    "abstract": "\n        This paper presents a plug-and-play approach for translation with terminology constraints. Terminology constraints are an important aspect of many modern translation pipelines. In both specialized domains and newly emerging domains (such as the COVID-19 pandemic), accurate translation of technical terms is crucial. Recent approaches often train models to copy terminologies from the input into the output sentence by feeding the target terminology along with the input. But this requires expensive training whenever the underlying language model is changed or the system should specialize to a new domain. We propose Cascade Beam Search, a plug-and-play terminology-forcing approach that requires no training. Cascade Beam Search has two parts: 1) logit manipulation to increase the probability of target terminologies and 2) a cascading beam setup based on grid beam search, where beams are grouped by the number of terminologies they contain. We evaluate the performance of our approach by competing against the top submissions of the WMT21 terminology translation task. Our plug-and-play approach performs on par with the winning submissions without using a domain-specific language model and with no additional training.\n        \u25b3 Less\n      ",
    "title": "Cascaded Beam Search: Plug-and-Play Terminology-Forcing For Neural Machine Translation",
    "date": "23 May, 2023",
    "authors": [
      "Fr\u00e9d\u00e9ric Odermatt",
      " B\u00e9ni Egressy",
      " Roger Wattenhofer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.12257",
    "paper_id": "2210.12257",
    "abstract": "\n        Despite the success of automated machine learning (AutoML), which aims to find the best design, including the architecture of deep networks and hyper-parameters, conventional AutoML methods are computationally expensive and hardly provide insights into the relations of different model design choices. To tackle the challenges, we propose FALCON, an efficient sample-based method to search for the optimal model design. Our key insight is to model the design space of possible model designs as a design graph, where the nodes represent design choices, and the edges denote design similarities. FALCON features 1) a task-agnostic module, which performs message passing on the design graph via a Graph Neural Network (GNN), and 2) a task-specific module, which conducts label propagation of the known model performance information on the design graph. Both modules are combined to predict the design performances in the design space, navigating the search direction. We conduct extensive experiments on 27 node and graph classification tasks from various application domains, and an image classification task on the CIFAR-10 dataset. We empirically show that FALCON can efficiently obtain the well-performing designs for each task using only 30 explored nodes. Specifically, FALCON has a comparable time cost with the one-shot approaches while achieving an average improvement of 3.3% compared with the best baselines.\n        \u25b3 Less\n      ",
    "title": "Efficient Automatic Machine Learning via Design Graphs",
    "date": "23 May, 2023",
    "authors": [
      "Shirley Wu",
      " Jiaxuan You",
      " Jure Leskovec",
      " Rex Ying"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.16634",
    "paper_id": "2303.16634",
    "abstract": "\n        The quality of texts generated by natural language generation (NLG) systems is hard to measure automatically. Conventional reference-based metrics, such as BLEU and ROUGE, have been shown to have relatively low correlation with human judgments, especially for tasks that require creativity and diversity. Recent studies suggest using large language models (LLMs) as reference-free metrics for NLG evaluation, which have the benefit of being applicable to new tasks that lack human references. However, these LLM-based evaluators still have lower human correspondence than medium-size neural evaluators. In this work, we present G-Eval, a framework of using large language models with chain-of-thoughts (CoT) and a form-filling paradigm, to assess the quality of NLG outputs. We experiment with two generation tasks, text summarization and dialogue generation. We show that G-Eval with GPT-4 as the backbone model achieves a Spearman correlation of 0.514 with human on summarization task, outperforming all previous methods by a large margin. We also propose preliminary analysis on the behavior of LLM-based evaluators, and highlight the potential issue of LLM-based evaluators having a bias towards the LLM-generated texts. The code is at https://github.com/nlpyang/geval\n        \u25b3 Less\n      ",
    "title": "G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment",
    "date": "23 May, 2023",
    "authors": [
      "Yang Liu",
      " Dan Iter",
      " Yichong Xu",
      " Shuohang Wang",
      " Ruochen Xu",
      " Chenguang Zhu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14553",
    "paper_id": "2305.14553",
    "abstract": "\n        In July 2022, the Center for Security and Emerging Technology (CSET) at Georgetown University and the Program on Geopolitics, Technology, and Governance at the Stanford Cyber Policy Center convened a workshop of experts to examine the relationship between vulnerabilities in artificial intelligence systems and more traditional types of software vulnerabilities. Topics discussed included the extent to which AI vulnerabilities can be handled under standard cybersecurity processes, the barriers currently preventing the accurate sharing of information about AI vulnerabilities, legal issues associated with adversarial attacks on AI systems, and potential areas where government support could improve AI vulnerability management and mitigation.\n  This report is meant to accomplish two things. First, it provides a high-level discussion of AI vulnerabilities, including the ways in which they are disanalogous to other types of vulnerabilities, and the current state of affairs regarding information sharing and legal oversight of AI vulnerabilities. Second, it attempts to articulate broad recommendations as endorsed by the majority of participants at the workshop.\n        \u25b3 Less\n      ",
    "title": "Adversarial Machine Learning and Cybersecurity: Risks, Challenges, and Legal Implications",
    "date": "23 May, 2023",
    "authors": [
      "Micah Musser",
      " Andrew Lohn",
      " James X. Dempsey",
      " Jonathan Spring",
      " Ram Shankar Siva Kumar",
      " Brenda Leong",
      " Christina Liaghati",
      " Cindy Martinez",
      " Crystal D. Grant",
      " Daniel Rohrer",
      " Heather Frase",
      " Jonathan Elliott",
      " John Bansemer",
      " Mikel Rodriguez",
      " Mitt Regan",
      " Rumman Chowdhury",
      " Stefan Hermanek"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14555",
    "paper_id": "2305.14555",
    "abstract": "\n        Transformer models bring propelling advances in various NLP tasks, thus inducing lots of interpretability research on the learned representations of the models. However, we raise a fundamental question regarding the reliability of the representations. Specifically, we investigate whether transformers learn essentially isomorphic representation spaces, or those that are sensitive to the random seeds in their pretraining process. In this work, we formulate the Bijection Hypothesis, which suggests the use of bijective methods to align different models' representation spaces. We propose a model based on invertible neural networks, BERT-INN, to learn the bijection more effectively than other existing bijective methods such as the canonical correlation analysis (CCA). We show the advantage of BERT-INN both theoretically and through extensive experiments, and apply it to align the reproduced BERT embeddings to draw insights that are meaningful to the interpretability research. Our code is at https://github.com/twinkle0331/BERT-similarity.\n        \u25b3 Less\n      ",
    "title": "All Roads Lead to Rome? Exploring the Invariance of Transformers' Representations",
    "date": "23 May, 2023",
    "authors": [
      "Yuxin Ren",
      " Qipeng Guo",
      " Zhijing Jin",
      " Shauli Ravfogel",
      " Mrinmaya Sachan",
      " Bernhard Sch\u00f6lkopf",
      " Ryan Cotterell"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14556",
    "paper_id": "2305.14556",
    "abstract": "\n        Large pre-trained language models have exhibited unprecedented capabilities in producing high-quality text via prompting techniques. This fact introduces new possibilities for data collection and annotation, particularly in situations where such data is scarce, complex to gather, expensive, or even sensitive. In this paper, we explore the potential of these models to generate and annotate goal-oriented dialogues, and conduct an in-depth analysis to evaluate their quality. Our experiments employ ChatGPT, and encompass three categories of goal-oriented dialogues (task-oriented, collaborative, and explanatory), two generation modes (interactive and one-shot), and two languages (English and Italian). Based on extensive human-based evaluations, we demonstrate that the quality of generated dialogues and annotations is on par with those generated by humans.\n        \u25b3 Less\n      ",
    "title": "Unraveling ChatGPT: A Critical Analysis of AI-Generated Goal-Oriented Dialogues and Annotations",
    "date": "23 May, 2023",
    "authors": [
      "Tiziano Labruna",
      " Sofia Brenna",
      " Andrea Zaninello",
      " Bernardo Magnini"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14582",
    "paper_id": "2305.14582",
    "abstract": "\n        Deep learning models developed for time-series associated tasks have become more widely researched nowadays. However, due to the unintuitive nature of time-series data, the interpretability problem -- where we understand what is under the hood of these models -- becomes crucial. The advancement of similar studies in computer vision has given rise to many post-hoc methods, which can also shed light on how to explain time-series models. In this paper, we present a wide range of post-hoc interpretation methods for time-series models based on backpropagation, perturbation, and approximation. We also want to bring focus onto inherently interpretable models, a novel category of interpretation where human-understandable information is designed within the models. Furthermore, we introduce some common evaluation metrics used for the explanations, and propose several directions of future researches on the time-series interpretability problem. As a highlight, our work summarizes not only the well-established interpretation methods, but also a handful of fairly recent and under-developed techniques, which we hope to capture their essence and spark future endeavours to innovate and improvise.\n        \u25b3 Less\n      ",
    "title": "Interpretation of Time-Series Deep Models: A Survey",
    "date": "23 May, 2023",
    "authors": [
      "Ziqi Zhao",
      " Yucheng Shi",
      " Shushan Wu",
      " Fan Yang",
      " Wenzhan Song",
      " Ninghao Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14590",
    "paper_id": "2305.14590",
    "abstract": "\n        Current research in form understanding predominantly relies on large pre-trained language models, necessitating extensive data for pre-training. However, the importance of layout structure (i.e., the spatial relationship between the entity blocks in the visually rich document) to relation extraction has been overlooked. In this paper, we propose REgion-Aware Relation Extraction (RE2^2) that leverages region-level spatial structure among the entity blocks to improve their relation prediction. We design an edge-aware graph attention network to learn the interaction between entities while considering their spatial relationship defined by their region-level representations. We also introduce a constraint objective to regularize the model towards consistency with the inherent constraints of the relation extraction task. Extensive experiments across various datasets, languages and domains demonstrate the superiority of our proposed approach.\n        \u25b3 Less\n      ",
    "title": "RE\n2\n: Region-Aware Relation Extraction from Visually Rich Documents",
    "date": "23 May, 2023",
    "authors": [
      "Pritika Ramu",
      " Sijia Wang",
      " Lalla Mouatadid",
      " Joy Rimchala",
      " Lifu Huang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.11616",
    "paper_id": "2301.11616",
    "abstract": "\n        The rapid development of artificial intelligence (AI) has led to increasing concerns about the capability of AI systems to make decisions and behave responsibly. Responsible AI (RAI) refers to the development and use of AI systems that benefit humans, society, and the environment while minimising the risk of negative consequences. To ensure responsible AI, the risks associated with AI systems' development and use must be identified, assessed and mitigated. Various AI risk assessment frameworks have been released recently by governments, organisations, and companies. However, it can be challenging for AI stakeholders to have a clear picture of the available frameworks and determine the most suitable ones for a specific context. Additionally, there is a need to identify areas that require further research or development of new frameworks, as well as updating and maintaining existing ones. To fill the gap, we present a mapping study of 16 existing AI risk assessment frameworks from the industry, governments, and non-government organizations (NGOs). We identify key characteristics of each framework and analyse them in terms of RAI principles, stakeholders, system lifecycle stages, geographical locations, targeted domains, and assessment methods. Our study provides a comprehensive analysis of the current state of the frameworks and highlights areas of convergence and divergence among them. We also identify the deficiencies in existing frameworks and outlines the essential characteristics of a concrete and connected framework AI risk assessment (C2^2AIRA) framework. Our findings and insights can help relevant stakeholders choose suitable AI risk assessment frameworks and guide the design of future frameworks towards concreteness and connectedness.\n        \u25b3 Less\n      ",
    "title": "Towards Concrete and Connected AI Risk Assessment (C\n2\nAIRA): A Systematic Mapping Study",
    "date": "23 May, 2023",
    "authors": [
      "Boming Xia",
      " Qinghua Lu",
      " Harsha Perera",
      " Liming Zhu",
      " Zhenchang Xing",
      " Yue Liu",
      " Jon Whittle"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14597",
    "paper_id": "2305.14597",
    "abstract": "\n        While several previous studies have analyzed gender bias in research, we are still missing a comprehensive analysis of gender differences in the AI community, covering diverse topics and different development trends. Using the AI Scholar dataset of 78K researchers in the field of AI, we identify several gender differences: (1) Although female researchers tend to have fewer overall citations than males, this citation difference does not hold for all academic-age groups; (2) There exist large gender homophily in co-authorship on AI papers; (3) Female first-authored papers show distinct linguistic styles, such as longer text, more positive emotion words, and more catchy titles than male first-authored papers. Our analysis provides a window into the current demographic trends in our AI community, and encourages more gender equality and diversity in the future. Our code and data are at https://github.com/causalNLP/ai-scholar-gender.\n        \u25b3 Less\n      ",
    "title": "Voices of Her: Analyzing Gender Differences in the AI Publication World",
    "date": "23 May, 2023",
    "authors": [
      "Yiwen Ding",
      " Jiarui Liu",
      " Zhiheng Lyu",
      " Kun Zhang",
      " Bernhard Schoelkopf",
      " Zhijing Jin",
      " Rada Mihalcea"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14608",
    "paper_id": "2305.14608",
    "abstract": "\n        We study the problem of Inverse Reinforcement Learning (IRL) with an average-reward criterion. The goal is to recover an unknown policy and a reward function when the agent only has samples of states and actions from an experienced agent. Previous IRL methods assume that the expert is trained in a discounted environment, and the discount factor is known. This work alleviates this assumption by proposing an average-reward framework with efficient learning algorithms. We develop novel stochastic first-order methods to solve the IRL problem under the average-reward setting, which requires solving an Average-reward Markov Decision Process (AMDP) as a subproblem. To solve the subproblem, we develop a Stochastic Policy Mirror Descent (SPMD) method under general state and action spaces that needs O(1/\u03b5)\\mathcal{O}(1/\\varepsilon) steps of gradient computation. Equipped with SPMD, we propose the Inverse Policy Mirror Descent (IPMD) method for solving the IRL problem with a O(1/\u03b52)\\mathcal{O}(1/\\varepsilon^2) complexity. To the best of our knowledge, the aforementioned complexity results are new in IRL. Finally, we corroborate our analysis with numerical experiments using the MuJoCo benchmark and additional control tasks.\n        \u25b3 Less\n      ",
    "title": "Inverse Reinforcement Learning with the Average Reward Criterion",
    "date": "23 May, 2023",
    "authors": [
      "Feiyang Wu",
      " Jingyang Ke",
      " Anqi Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16338",
    "paper_id": "2305.16338",
    "abstract": "\n        Large language model (LLM)-based decision-making agents have shown the ability to generalize across multiple tasks. However, their performance relies on massive data and compute. We argue that this inefficiency stems from the forgetting phenomenon, in which a model memorizes its behaviors in parameters throughout training. As a result, training on a new task may deteriorate the model's performance on previous tasks. In contrast to LLMs' implicit memory mechanism, the human brain utilizes distributed memory storage, which helps manage and organize multiple skills efficiently, mitigating the forgetting phenomenon. Thus inspired, we propose an internal working memory module to store, blend, and retrieve information for different downstream tasks. Evaluation results show that the proposed method improves training efficiency and generalization in both Atari games and meta-world object manipulation tasks. Moreover, we demonstrate that memory fine-tuning further enhances the adaptability of the proposed architecture.\n        \u25b3 Less\n      ",
    "title": "Think Before You Act: Decision Transformers with Internal Working Memory",
    "date": "23 May, 2023",
    "authors": [
      "Jikun Kang",
      " Romain Laroche",
      " Xindi Yuan",
      " Adam Trischler",
      " Xue Liu",
      " Jie Fu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14618",
    "paper_id": "2305.14618",
    "abstract": "\n        Abductive reasoning aims to find plausible explanations for an event. This style of reasoning is critical for commonsense tasks where there are often multiple plausible explanations. Existing approaches for abductive reasoning in natural language processing (NLP) often rely on manually generated annotations for supervision; however, such annotations can be subjective and biased. Instead of using direct supervision, this work proposes an approach for abductive commonsense reasoning that exploits the fact that only a subset of explanations is correct for a given context. The method uses posterior regularization to enforce a mutual exclusion constraint, encouraging the model to learn the distinction between fluent explanations and plausible ones. We evaluate our approach on a diverse set of abductive reasoning datasets; experimental results show that our approach outperforms or is comparable to directly applying pretrained language models in a zero-shot manner and other knowledge-augmented zero-shot methods.\n        \u25b3 Less\n      ",
    "title": "Abductive Commonsense Reasoning Exploiting Mutually Exclusive Explanations",
    "date": "23 May, 2023",
    "authors": [
      "Wenting Zhao",
      " Justin T. Chiu",
      " Claire Cardie",
      " Alexander M. Rush"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.02503",
    "paper_id": "2302.02503",
    "abstract": "\n        Recent research on robustness has revealed significant performance gaps between neural image classifiers trained on datasets that are similar to the test set, and those that are from a naturally shifted distribution, such as sketches, paintings, and animations of the object categories observed during training. Prior work focuses on reducing this gap by designing engineered augmentations of training data or through unsupervised pretraining of a single large model on massive in-the-wild training datasets scraped from the Internet. However, the notion of a dataset is also undergoing a paradigm shift in recent years. With drastic improvements in the quality, ease-of-use, and access to modern generative models, generated data is pervading the web. In this light, we study the question: How do these generated datasets influence the natural robustness of image classifiers? We find that Imagenet classifiers trained on real data augmented with generated data achieve higher accuracy and effective robustness than standard training and popular augmentation strategies in the presence of natural distribution shifts. We analyze various factors influencing these results, including the choice of conditioning strategies and the amount of generated data. Additionally, we find that the standard ImageNet classifiers suffer a performance degradation of upto 20\\% on the generated data, indicating their fragility at accurately classifying the objects under novel variations. Lastly, we demonstrate that the image classifiers, which have been trained on real data augmented with generated data from the base generative model, exhibit greater resilience to natural distribution shifts compared to the classifiers trained on real data augmented with generated data from the finetuned generative model on the real data. The code, models, and datasets are available at https://github.com/Hritikbansal/generative-robustness.\n        \u25b3 Less\n      ",
    "title": "Leaving Reality to Imagination: Robust Classification via Generated Datasets",
    "date": "23 May, 2023",
    "authors": [
      "Hritik Bansal",
      " Aditya Grover"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2208.08661",
    "paper_id": "2208.08661",
    "abstract": "\n        Recent domain generalization (DG) approaches typically use the hypothesis learned on source domains for inference on the unseen target domain. However, such a hypothesis can be arbitrarily far from the optimal one for the target domain, induced by a gap termed ``adaptivity gap''. Without exploiting the domain information from the unseen test samples, adaptivity gap estimation and minimization are intractable, which hinders us to robustify a model to any unknown distribution. In this paper, we first establish a generalization bound that explicitly considers the adaptivity gap. Our bound motivates two strategies to reduce the gap: the first one is ensembling multiple classifiers to enrich the hypothesis space, then we propose effective gap estimation methods for guiding the selection of a better hypothesis for the target. The other method is minimizing the gap directly by adapting model parameters using online target samples. We thus propose \\textbf{Domain-specific Risk Minimization (DRM)}. During training, DRM models the distributions of different source domains separately; for inference, DRM performs online model steering using the source hypothesis for each arriving target sample. Extensive experiments demonstrate the effectiveness of the proposed DRM for domain generalization with the following advantages: 1) it significantly outperforms competitive baselines on different distributional shift settings; 2) it achieves either comparable or superior accuracies on all source domains compared to vanilla empirical risk minimization; 3) it remains simple and efficient during training, and 4) it is complementary to invariant learning approaches.\n        \u25b3 Less\n      ",
    "title": "Domain-Specific Risk Minimization for Out-of-Distribution Generalization",
    "date": "23 May, 2023",
    "authors": [
      "Yi-Fan Zhang",
      " Jindong Wang",
      " Jian Liang",
      " Zhang Zhang",
      " Baosheng Yu",
      " Liang Wang",
      " Dacheng Tao",
      " Xing Xie"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2202.01802",
    "paper_id": "2202.01802",
    "abstract": "\n        Adaptive mobile device-based health interventions often use machine learning models trained on non-mobile device data, such as social media text, due to the difficulty and high expense of collecting large text message (SMS) data. Therefore, understanding the differences and generalization of models between these platforms is crucial for proper deployment. We examined the psycho-linguistic differences between Facebook and text messages, and their impact on out-of-domain model performance, using a sample of 120 users who shared both. We found that users use Facebook for sharing experiences (e.g., leisure) and SMS for task-oriented and conversational purposes (e.g., plan confirmations), reflecting the differences in the affordances. To examine the downstream effects of these differences, we used pre-trained Facebook-based language models to estimate age, gender, depression, life satisfaction, and stress on both Facebook and SMS. We found no significant differences in correlations between the estimates and self-reports across 6 of 8 models. These results suggest using pre-trained Facebook language models to achieve better accuracy with just-in-time interventions.\n        \u25b3 Less\n      ",
    "title": "Different Affordances on Facebook and SMS Text Messaging Do Not Impede Generalization of Language-Based Predictive Models",
    "date": "23 May, 2023",
    "authors": [
      "Tingting Liu",
      " Salvatore Giorgi",
      " Xiangyu Tao",
      " Sharath Chandra Guntuku",
      " Douglas Bellew",
      " Brenda Curtis",
      " Lyle Ungar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14654",
    "paper_id": "2305.14654",
    "abstract": "\n        Animals have evolved various agile locomotion strategies, such as sprinting, leaping, and jumping. There is a growing interest in developing legged robots that move like their biological counterparts and show various agile skills to navigate complex environments quickly. Despite the interest, the field lacks systematic benchmarks to measure the performance of control policies and hardware in agility. We introduce the Barkour benchmark, an obstacle course to quantify agility for legged robots. Inspired by dog agility competitions, it consists of diverse obstacles and a time based scoring mechanism. This encourages researchers to develop controllers that not only move fast, but do so in a controllable and versatile way. To set strong baselines, we present two methods for tackling the benchmark. In the first approach, we train specialist locomotion skills using on-policy reinforcement learning methods and combine them with a high-level navigation controller. In the second approach, we distill the specialist skills into a Transformer-based generalist locomotion policy, named Locomotion-Transformer, that can handle various terrains and adjust the robot's gait based on the perceived environment and robot states. Using a custom-built quadruped robot, we demonstrate that our method can complete the course at half the speed of a dog. We hope that our work represents a step towards creating controllers that enable robots to reach animal-level agility.\n        \u25b3 Less\n      ",
    "title": "Barkour: Benchmarking Animal-level Agility with Quadruped Robots",
    "date": "23 May, 2023",
    "authors": [
      "Ken Caluwaerts",
      " Atil Iscen",
      " J. Chase Kew",
      " Wenhao Yu",
      " Tingnan Zhang",
      " Daniel Freeman",
      " Kuang-Huei Lee",
      " Lisa Lee",
      " Stefano Saliceti",
      " Vincent Zhuang",
      " Nathan Batchelor",
      " Steven Bohez",
      " Federico Casarini",
      " Jose Enrique Chen",
      " Omar Cortes",
      " Erwin Coumans",
      " Adil Dostmohamed",
      " Gabriel Dulac-Arnold",
      " Alejandro Escontrela",
      " Erik Frey",
      " Roland Hafner",
      " Deepali Jain",
      " Bauyrjan Jyenis",
      " Yuheng Kuang",
      " Edward Lee ",
      " et al. (19 additional authors not shown)"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14167",
    "paper_id": "2305.14167",
    "abstract": "\n        In recent years, the field of computer vision has seen significant advancements thanks to the development of large language models (LLMs). These models have enabled more effective and sophisticated interactions between humans and machines, paving the way for novel techniques that blur the lines between human and machine intelligence. In this paper, we introduce a new paradigm for object detection that we call reasoning-based object detection. Unlike conventional object detection methods that rely on specific object names, our approach enables users to interact with the system using natural language instructions, allowing for a higher level of interactivity. Our proposed method, called DetGPT, leverages state-of-the-art multi-modal models and open-vocabulary object detectors to perform reasoning within the context of the user's instructions and the visual scene. This enables DetGPT to automatically locate the object of interest based on the user's expressed desires, even if the object is not explicitly mentioned. For instance, if a user expresses a desire for a cold beverage, DetGPT can analyze the image, identify a fridge, and use its knowledge of typical fridge contents to locate the beverage. This flexibility makes our system applicable across a wide range of fields, from robotics and automation to autonomous driving. Overall, our proposed paradigm and DetGPT demonstrate the potential for more sophisticated and intuitive interactions between humans and machines. We hope that our proposed paradigm and approach will provide inspiration to the community and open the door to more interative and versatile object detection systems. Our project page is launched at detgpt.github.io.\n        \u25b3 Less\n      ",
    "title": "DetGPT: Detect What You Need via Reasoning",
    "date": "23 May, 2023",
    "authors": [
      "Renjie Pi",
      " Jiahui Gao",
      " Shizhe Diao",
      " Rui Pan",
      " Hanze Dong",
      " Jipeng Zhang",
      " Lewei Yao",
      " Jianhua Han",
      " Hang Xu",
      " Lingpeng Kong",
      " Tong Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14656",
    "paper_id": "2305.14656",
    "abstract": "\n        In nature, the behaviors of many complex systems can be described by parsimonious math equations. Automatically distilling these equations from limited data is cast as a symbolic regression process which hitherto remains a grand challenge. Keen efforts in recent years have been placed on tackling this issue and demonstrated success in symbolic regression. However, there still exist bottlenecks that current methods struggle to break when the discrete search space tends toward infinity and especially when the underlying math formula is intricate. To this end, we propose a novel Reinforcement Symbolic Regression Machine (RSRM) that masters the capability of uncovering complex math equations from only scarce data. The RSRM model is composed of three key modules: (1) a Monte Carlo tree search (MCTS) agent that explores optimal math expression trees consisting of pre-defined math operators and variables, (2) a Double Q-learning block that helps reduce the feasible search space of MCTS via properly understanding the distribution of reward, and (3) a modulated sub-tree discovery block that heuristically learns and defines new math operators to improve representation ability of math expression trees. Biding of these modules yields the state-of-the-art performance of RSRM in symbolic regression as demonstrated by multiple sets of benchmark examples. The RSRM model shows clear superiority over several representative baseline models.\n        \u25b3 Less\n      ",
    "title": "RSRM: Reinforcement Symbolic Regression Machine",
    "date": "23 May, 2023",
    "authors": [
      "Yilong Xu",
      " Yang Liu",
      " Hao Sun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14688",
    "paper_id": "2305.14688",
    "abstract": "\n        The answering quality of an aligned large language model (LLM) can be drastically improved if treated with proper crafting of prompts. In this paper, we propose ExpertPrompting to elicit the potential of LLMs to answer as distinguished experts. We first utilize In-Context Learning to automatically synthesize detailed and customized descriptions of the expert identity for each specific instruction, and then ask LLMs to provide answer conditioned on such agent background. Based on this augmented prompting strategy, we produce a new set of instruction-following data using GPT-3.5, and train a competitive open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation to show that 1) the expert data is of significantly higher quality than vanilla answers, and 2) ExpertLLaMA outperforms existing open-source opponents and achieves 96\\% of the original ChatGPT's capability. All data and the ExpertLLaMA model will be made publicly available at \\url{https://github.com/OFA-Sys/ExpertLLaMA}.\n        \u25b3 Less\n      ",
    "title": "ExpertPrompting: Instructing Large Language Models to be Distinguished Experts",
    "date": "23 May, 2023",
    "authors": [
      "Benfeng Xu",
      " An Yang",
      " Junyang Lin",
      " Quan Wang",
      " Chang Zhou",
      " Yongdong Zhang",
      " Zhendong Mao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2206.02336",
    "paper_id": "2206.02336",
    "abstract": "\n        Few-shot learning is a challenging task that requires language models to generalize from limited examples. Large language models like GPT-3 and PaLM have made impressive progress in this area, but they still face difficulties in reasoning tasks such as GSM8K, a benchmark for arithmetic problems. To improve their reasoning skills, previous work has proposed to guide the language model with prompts that elicit a series of reasoning steps before giving the final answer, achieving a significant improvement on GSM8K from 17.9% to 58.1% in problem-solving rate. In this paper, we present DIVERSE (Diverse Verifier on Reasoning Step), a novel approach that further enhances the reasoning capability of language models. DIVERSE has three main components: first, it generates diverse prompts to explore different reasoning paths for the same question; second, it uses a verifier to filter out incorrect answers based on a weighted voting scheme; and third, it verifies each reasoning step individually instead of the whole chain. We evaluate DIVERSE on the latest language model code-davinci-002 and show that it achieves new state-of-the-art results on six of eight reasoning benchmarks (e.g., GSM8K 74.4% to 83.2%).\n        \u25b3 Less\n      ",
    "title": "Making Large Language Models Better Reasoners with Step-Aware Verifier",
    "date": "23 May, 2023",
    "authors": [
      "Yifei Li",
      " Zeqi Lin",
      " Shizhuo Zhang",
      " Qiang Fu",
      " Bei Chen",
      " Jian-Guang Lou",
      " Weizhu Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14701",
    "paper_id": "2305.14701",
    "abstract": "\n        Humans can learn languages from remarkably little experience. Developing computational models that explain this ability has been a major challenge in cognitive science. Bayesian models that build in strong inductive biases - factors that guide generalization - have been successful at explaining how humans might generalize from few examples in controlled settings but are usually too restrictive to be tractably applied to more naturalistic data. By contrast, neural networks have flexible representations that allow them to learn well from naturalistic data but require many more examples than humans receive. We show that learning from limited naturalistic data is possible with an approach that combines the strong inductive biases of a Bayesian model with the flexible representations of a neural network. This approach works by distilling a Bayesian model's biases into a neural network. Like a Bayesian model, the resulting system can learn formal linguistic patterns from a small number of examples. Like a neural network, it can also learn aspects of English syntax from a corpus of natural language - and it outperforms a standard neural network at acquiring the linguistic phenomena of recursion and priming. Bridging the divide between Bayesian models and neural networks makes it possible to handle a broader range of learning scenarios than either approach can handle on its own.\n        \u25b3 Less\n      ",
    "title": "Modeling rapid language learning by distilling Bayesian priors into artificial neural networks",
    "date": "23 May, 2023",
    "authors": [
      "R. Thomas McCoy",
      " Thomas L. Griffiths"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11176",
    "paper_id": "2305.11176",
    "abstract": "\n        Foundation models have made significant strides in various applications, including text-to-image generation, panoptic segmentation, and natural language processing. This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks. Specifically, Instruct2Act employs the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks. In the perception section, pre-defined APIs are used to access multiple foundation models where the Segment Anything Model (SAM) accurately locates candidate objects, and CLIP classifies them. In this way, the framework leverages the expertise of foundation models and robotic abilities to convert complex high-level instructions into precise policy codes. Our approach is adjustable and flexible in accommodating various instruction modalities and input types and catering to specific task demands. We validated the practicality and efficiency of our approach by assessing it on robotic tasks in different scenarios within tabletop manipulation domains. Furthermore, our zero-shot method outperformed many state-of-the-art learning-based policies in several tasks. The code for our proposed approach is available at https://github.com/OpenGVLab/Instruct2Act, serving as a robust benchmark for high-level robotic instruction tasks with assorted modality inputs.\n        \u25b3 Less\n      ",
    "title": "Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model",
    "date": "23 May, 2023",
    "authors": [
      "Siyuan Huang",
      " Zhengkai Jiang",
      " Hao Dong",
      " Yu Qiao",
      " Peng Gao",
      " Hongsheng Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14710",
    "paper_id": "2305.14710",
    "abstract": "\n        Instruction-tuned models are trained on crowdsourcing datasets with task instructions to achieve superior performance. However, in this work we raise security concerns about this training paradigm. Our studies demonstrate that an attacker can inject backdoors by issuing very few malicious instructions among thousands of gathered data and control model behavior through data poisoning, without even the need of modifying data instances or labels themselves. Through such instruction attacks, the attacker can achieve over 90% attack success rate across four commonly used NLP datasets, and cause persistent backdoors that are easily transferred to 15 diverse datasets zero-shot. In this way, the attacker can directly apply poisoned instructions designed for one dataset on many other datasets. Moreover, the poisoned model cannot be cured by continual learning. Lastly, instruction attacks show resistance to existing inference-time defense. These findings highlight the need for more robust defenses against data poisoning attacks in instructiontuning models and underscore the importance of ensuring data quality in instruction crowdsourcing.\n        \u25b3 Less\n      ",
    "title": "Instructions as Backdoors: Backdoor Vulnerabilities of Instruction Tuning for Large Language Models",
    "date": "23 May, 2023",
    "authors": [
      "Jiashu Xu",
      " Mingyu Derek Ma",
      " Fei Wang",
      " Chaowei Xiao",
      " Muhao Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.14225",
    "paper_id": "2302.14225",
    "abstract": "\n        Masked Language Modeling (MLM) is widely used to pretrain language models. The standard random masking strategy in MLM causes the pre-trained language models (PLMs) to be biased toward high-frequency tokens. Representation learning of rare tokens is poor and PLMs have limited performance on downstream tasks. To alleviate this frequency bias issue, we propose two simple and effective Weighted Sampling strategies for masking tokens based on the token frequency and training loss. We apply these two strategies to BERT and obtain Weighted-Sampled BERT (WSBERT). Experiments on the Semantic Textual Similarity benchmark (STS) show that WSBERT significantly improves sentence embeddings over BERT. Combining WSBERT with calibration methods and prompt learning further improves sentence embeddings. We also investigate fine-tuning WSBERT on the GLUE benchmark and show that Weighted Sampling also improves the transfer learning capability of the backbone PLM. We further analyze and provide insights into how WSBERT improves token embeddings.\n        \u25b3 Less\n      ",
    "title": "Weighted Sampling for Masked Language Modeling",
    "date": "23 May, 2023",
    "authors": [
      "Linhan Zhang",
      " Qian Chen",
      " Wen Wang",
      " Chong Deng",
      " Xin Cao",
      " Kongzhang Hao",
      " Yuxin Jiang",
      " Wei Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14717",
    "paper_id": "2305.14717",
    "abstract": "\n        Definition modeling is an important task in advanced natural language applications such as understanding and conversation. Since its introduction, it focus on generating one definition for a target word or phrase in a given context, which we refer to as Single Definition Modeling (SDM). However, this approach does not adequately model the correlations and patterns among different contexts and definitions of words. In addition, the creation of a training dataset for SDM requires significant human expertise and effort. In this paper, we carefully design a new task called Multiple Definition Modeling (MDM) that pool together all contexts and definition of target words. We demonstrate the ease of creating a model as well as multiple training sets automatically. % In the experiments, we demonstrate and analyze the benefits of MDM, including improving SDM's performance by using MDM as the pretraining task and its comparable performance in the zero-shot setting.\n        \u25b3 Less\n      ",
    "title": "Exploiting Correlations Between Contexts and Definitions with Multiple Definition Modeling",
    "date": "23 May, 2023",
    "authors": [
      "Linhan Zhang",
      " Qian Chen",
      " Wen Wang",
      " Yuxin Jiang",
      " Bing Li",
      " Wei Wang",
      " Xin Cao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14731",
    "paper_id": "2305.14731",
    "abstract": "\n        Depth cameras have found applications in diverse fields, such as computer vision, artificial intelligence, and video gaming. However, the high latency and low frame rate of existing commodity depth cameras impose limitations on their applications. We propose a fast and accurate depth map reconstruction technique to reduce latency and increase the frame rate in depth cameras. Our approach uses only a commodity depth camera and color camera in a hybrid camera setup; our prototype is implemented using a Kinect Azure depth camera at 30 fps and a high-speed RGB iPhone 11 Pro camera captured at 240 fps. The proposed network, AutoDepthNet, is an encoder-decoder model that captures frames from the high-speed RGB camera and combines them with previous depth frames to reconstruct a stream of high frame rate depth maps. On GPU, with a 480 x 270 output resolution, our system achieves an inference time of 8 ms, enabling real-time use at up to 200 fps with parallel processing. AutoDepthNet can estimate depth values with an average RMS error of 0.076, a 44.5% improvement compared to an optical flow-based comparison method. Our method can also improve depth map quality by estimating depth values for missing and invalidated pixels. The proposed method can be easily applied to existing depth cameras and facilitates the use of depth cameras in applications that require high-speed depth estimation. We also showcase the effectiveness of the framework in upsampling different sparse datasets e.g. video object segmentation. As a demonstration of our method, we integrated our framework into existing body tracking systems and demonstrated the robustness of the proposed method in such applications.\n        \u25b3 Less\n      ",
    "title": "AutoDepthNet: High Frame Rate Depth Map Reconstruction using Commodity Depth and RGB Cameras",
    "date": "23 May, 2023",
    "authors": [
      "Peyman Gholami",
      " Robert Xiao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.09247",
    "paper_id": "2306.09247",
    "abstract": "\n        Privacy policies are long, complex documents that end-users seldom read. Privacy labels aim to ameliorate these issues by providing succinct summaries of salient data practices. In December 2020, Apple began requiring that app developers submit privacy labels describing their apps' data practices. Yet, research suggests that app developers often struggle to do so. In this paper, we automatically identify possible discrepancies between mobile app privacy policies and their privacy labels. Such discrepancies could be indicators of potential privacy compliance issues.\n  We introduce the Automated Privacy Label Analysis System (ATLAS). ATLAS includes three components: a pipeline to systematically retrieve iOS App Store listings and privacy policies; an ensemble-based classifier capable of predicting privacy labels from the text of privacy policies with 91.3% accuracy using state-of-the-art NLP techniques; and a discrepancy analysis mechanism that enables a large-scale privacy analysis of the iOS App Store.\n  Our system has enabled us to analyze 354,725 iOS apps. We find several interesting trends. For example, only 40.3% of apps in the App Store provide easily accessible privacy policies, and only 29.6% of apps provide both accessible privacy policies and privacy labels. Among apps that provide both, 88.0% have at least one possible discrepancy between the text of their privacy policy and their privacy label, which could be indicative of a potential compliance issue. We find that, on average, apps have 5.32 such potential compliance issues.\n  We hope that ATLAS will help app developers, researchers, regulators, and mobile app stores alike. For example, app developers could use our classifier to check for discrepancies between their privacy policies and privacy labels, and regulators could use our system to help review apps at scale for potential compliance issues.\n        \u25b3 Less\n      ",
    "title": "ATLAS: Automatically Detecting Discrepancies Between Privacy Policies and Privacy Labels",
    "date": "23 May, 2023",
    "authors": [
      "Akshath Jain",
      " David Rodriguez",
      " Jose M. del Alamo",
      " Norman Sadeh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14751",
    "paper_id": "2305.14751",
    "abstract": "\n        In the constant updates of the product dialogue systems, we need to retrain the natural language understanding (NLU) model as new data from the real users would be merged into the existent data accumulated in the last updates. Within the newly added data, new intents would emerge and might have semantic entanglement with the existing intents, e.g. new intents that are semantically too specific or generic are actually subset or superset of some existing intents in the semantic space, thus impairing the robustness of the NLU model. As the first attempt to solve this problem, we setup a new benchmark consisting of 4 Dialogue Version Control dataSets (DialogVCS). We formulate the intent detection with imperfect data in the system update as a multi-label classification task with positive but unlabeled intents, which asks the models to recognize all the proper intents, including the ones with semantic entanglement, in the inference. We also propose comprehensive baseline models and conduct in-depth analyses for the benchmark, showing that the semantically entangled intents can be effectively recognized with an automatic workflow.\n        \u25b3 Less\n      ",
    "title": "DialogVCS: Robust Natural Language Understanding in Dialogue System Upgrade",
    "date": "23 May, 2023",
    "authors": [
      "Zefan Cai",
      " Xin Zheng",
      " Tianyu Liu",
      " Xu Wang",
      " Haoran Meng",
      " Jiaqi Han",
      " Gang Yuan",
      " Binghuai Lin",
      " Baobao Chang",
      " Yunbo Cao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14752",
    "paper_id": "2305.14752",
    "abstract": "\n        In this paper we present a novel solution that combines the capabilities of Large Language Models (LLMs) with Formal Verification strategies to verify and automatically repair software vulnerabilities. Initially, we employ Bounded Model Checking (BMC) to locate the software vulnerability and derive a counterexample. The counterexample provides evidence that the system behaves incorrectly or contains a vulnerability. The counterexample that has been detected, along with the source code, are provided to the LLM engine. Our approach involves establishing a specialized prompt language for conducting code debugging and generation to understand the vulnerability's root cause and repair the code. Finally, we use BMC to verify the corrected version of the code generated by the LLM. As a proof of concept, we create ESBMC-AI based on the Efficient SMT-based Context-Bounded Model Checker (ESBMC) and a pre-trained Transformer model, specifically gpt-3.5-turbo, to detect and fix errors in C programs. Our experimentation involved generating a dataset comprising 1000 C code samples, each consisting of 20 to 50 lines of code. Notably, our proposed method achieved an impressive success rate of up to 80% in repairing vulnerable code encompassing buffer overflow and pointer dereference failures. We assert that this automated approach can effectively incorporate into the software development lifecycle's continuous integration and deployment (CI/CD) process.\n        \u25b3 Less\n      ",
    "title": "A New Era in Software Security: Towards Self-Healing Software via Large Language Models and Formal Verification",
    "date": "23 May, 2023",
    "authors": [
      "Yiannis Charalambous",
      " Norbert Tihanyi",
      " Ridhi Jain",
      " Youcheng Sun",
      " Mohamed Amine Ferrag",
      " Lucas C. Cordeiro"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14775",
    "paper_id": "2305.14775",
    "abstract": "\n        While pre-trained language models (PLMs) have shown evidence of acquiring vast amounts of knowledge, it remains unclear how much of this parametric knowledge is actually usable in performing downstream tasks. We propose a systematic framework to measure parametric knowledge utilization in PLMs. Our framework first extracts knowledge from a PLM's parameters and subsequently constructs a downstream task around this extracted knowledge. Performance on this task thus depends exclusively on utilizing the model's possessed knowledge, avoiding confounding factors like insufficient signal. As an instantiation, we study factual knowledge of PLMs and measure utilization across 125M to 13B parameter PLMs. We observe that: (1) PLMs exhibit two gaps - in acquired vs. utilized knowledge, (2) they show limited robustness in utilizing knowledge under distribution shifts, and (3) larger models close the acquired knowledge gap but the utilized knowledge gap remains. Overall, our study provides insights into PLMs' capabilities beyond their acquired knowledge.\n        \u25b3 Less\n      ",
    "title": "Measuring the Knowledge Acquisition-Utilization Gap in Pretrained Language Models",
    "date": "23 May, 2023",
    "authors": [
      "Amirhossein Kazemnejad",
      " Mehdi Rezagholizadeh",
      " Prasanna Parthasarathi",
      " Sarath Chandar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14784",
    "paper_id": "2305.14784",
    "abstract": "\n        Anthropomorphization is the tendency to attribute human-like traits to non-human entities. It is prevalent in many social contexts -- children anthropomorphize toys, adults do so with brands, and it is a literary device. It is also a versatile tool in science, with behavioral psychology and evolutionary biology meticulously documenting its consequences. With widespread adoption of AI systems, and the push from stakeholders to make it human-like through alignment techniques, human voice, and pictorial avatars, the tendency for users to anthropomorphize it increases significantly. We take a dyadic approach to understanding this phenomenon with large language models (LLMs) by studying (1) the objective legal implications, as analyzed through the lens of the recent blueprint of AI bill of rights and the (2) subtle psychological aspects customization and anthropomorphization. We find that anthropomorphized LLMs customized for different user bases violate multiple provisions in the legislative blueprint. In addition, we point out that anthropomorphization of LLMs affects the influence they can have on their users, thus having the potential to fundamentally change the nature of human-AI interaction, with potential for manipulation and negative influence. With LLMs being hyper-personalized for vulnerable groups like children and patients among others, our work is a timely and important contribution. We propose a conservative strategy for the cautious use of anthropomorphization to improve trustworthiness of AI systems.\n        \u25b3 Less\n      ",
    "title": "Anthropomorphization of AI: Opportunities and Risks",
    "date": "23 May, 2023",
    "authors": [
      "Ameet Deshpande",
      " Tanmay Rajpurohit",
      " Karthik Narasimhan",
      " Ashwin Kalyan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14785",
    "paper_id": "2305.14785",
    "abstract": "\n        This paper sheds light on the limitations of ChatGPT's understanding capabilities, focusing on simple inference tasks that are typically easy for humans but appear to be challenging for the model. Specifically, we target (i) grammatically-specified entailments, (ii) premises with evidential adverbs of uncertainty, and (iii) monotonicity entailments. We present expert-designed evaluation sets for these inference types and conduct experiments in a zero-shot setup. Our results show that the model struggles with these types of inferences, exhibiting moderate to low accuracy. Moreover, while ChatGPT demonstrates knowledge of the underlying linguistic concepts when prompted directly, it often fails to incorporate this knowledge to make correct inferences. Even more strikingly, further experiments show that embedding the premise under presupposition triggers or non-factive verbs causes the model to predict entailment more frequently {regardless} of the correct semantic label. Overall these results suggest that, despite GPT's celebrated language understanding capacity, ChatGPT has blindspots with respect to certain types of entailment, and that certain entailment-cancelling features act as ``blinds'' overshadowing the semantics of the embedded premise. Our analyses emphasize the need for further research into the linguistic comprehension and reasoning capabilities of LLMs, in order to improve their reliability, and establish their trustworthiness for real-world applications.\n        \u25b3 Less\n      ",
    "title": "ChatGPT and Simple Linguistic Inferences: Blind Spots and Blinds",
    "date": "23 May, 2023",
    "authors": [
      "Victoria Basmov",
      " Yoav Goldberg",
      " Reut Tsarfaty"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14790",
    "paper_id": "2305.14790",
    "abstract": "\n        Topic segmentation and outline generation strive to divide a document into coherent topic sections and generate corresponding subheadings. Such a process unveils the discourse topic structure of a document that benefits quickly grasping and understanding the overall context of the document from a higher level. However, research and applications in this field have been restrained due to the lack of proper paragraph-level topic representations and large-scale, high-quality corpora in Chinese compared to the success achieved in English. Addressing these issues, we introduce a hierarchical paragraph-level topic structure representation with title, subheading, and paragraph that comprehensively models the document discourse topic structure. In addition, we ensure a more holistic representation of topic distribution within the document by using sentences instead of keywords to represent sub-topics. Following this representation, we construct the largest Chinese Paragraph-level Topic Structure corpus (CPTS), four times larger than the previously largest one. We also employ a two-stage man-machine collaborative annotation method to ensure the high quality of the corpus both in form and semantics. Finally, we validate the computability of CPTS on two fundamental tasks (topic segmentation and outline generation) by several strong baselines, and its efficacy has been preliminarily confirmed on the downstream task: discourse parsing. The representation, corpus, and benchmark we established will provide a solid foundation for future studies.\n        \u25b3 Less\n      ",
    "title": "Advancing Topic Segmentation and Outline Generation in Chinese Texts: The Paragraph-level Topic Representation, Corpus, and Benchmark",
    "date": "23 May, 2023",
    "authors": [
      "Feng Jiang",
      " Weihao Liu",
      " Xiaomin Chu",
      " Peifeng Li",
      " Qiaoming Zhu",
      " Haizhou Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13941",
    "paper_id": "2305.13941",
    "abstract": "\n        Sign language is a visual language that enhances communication between people and is frequently used as the primary form of communication by people with hearing loss. Even so, not many people with hearing loss use sign language, and they frequently experience social isolation. Therefore, it is necessary to create human-computer interface systems that can offer hearing-impaired people a social platform. Most commercial sign language translation systems now on the market are sensor-based, pricey, and challenging to use. Although vision-based systems are desperately needed, they must first overcome several challenges. Earlier continuous sign language recognition techniques used hidden Markov models, which have a limited ability to include temporal information. To get over these restrictions, several machine learning approaches are being applied to transform hand and sign language motions into spoken or written language. In this study, we compare various deep learning techniques for recognising sign language. Our survey aims to provide a comprehensive overview of the most recent approaches and challenges in this field.\n        \u25b3 Less\n      ",
    "title": "A Comparative Analysis of Techniques and Algorithms for Recognising Sign Language",
    "date": "23 May, 2023",
    "authors": [
      "Rupesh Kumar",
      " Ayush Sinha",
      " Ashutosh Bajpai",
      " S. K Singh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11421",
    "paper_id": "2305.11421",
    "abstract": "\n        In this paper, we investigate the challenge of spatio-temporal video prediction, which involves generating future videos based on historical data streams. Existing approaches typically utilize external information such as semantic maps to enhance video prediction, which often neglect the inherent physical knowledge embedded within videos. Furthermore, their high computational demands could impede their applications for high-resolution videos. To address these constraints, we introduce a novel approach called Physics-assisted Spatio-temporal Network (PastNet) for generating high-quality video predictions. The core of our PastNet lies in incorporating a spectral convolution operator in the Fourier domain, which efficiently introduces inductive biases from the underlying physical laws. Additionally, we employ a memory bank with the estimated intrinsic dimensionality to discretize local features during the processing of complex spatio-temporal signals, thereby reducing computational costs and facilitating efficient high-resolution video prediction. Extensive experiments on various widely-used datasets demonstrate the effectiveness and efficiency of the proposed PastNet compared with state-of-the-art methods, particularly in high-resolution scenarios. Our code is available at https://github.com/easylearningscores/PastNet.\n        \u25b3 Less\n      ",
    "title": "PastNet: Introducing Physical Inductive Biases for Spatio-temporal Video Prediction",
    "date": "23 May, 2023",
    "authors": [
      "Hao Wu",
      " Wei Xiong",
      " Fan Xu",
      " Xiao Luo",
      " Chong Chen",
      " Xian-Sheng Hua",
      " Haixin Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18330",
    "paper_id": "2305.18330",
    "abstract": "\n        Automatic evaluation of hashtag recommendation models is a fundamental task in many online social network systems. In the traditional evaluation method, the recommended hashtags from an algorithm are firstly compared with the ground truth hashtags for exact correspondences. The number of exact matches is then used to calculate the hit rate, hit ratio, precision, recall, or F1-score. This way of evaluating hashtag similarities is inadequate as it ignores the semantic correlation between the recommended and ground truth hashtags. To tackle this problem, we propose a novel semantic evaluation framework for hashtag recommendation, called #REval. This framework includes an internal module referred to as BERTag, which automatically learns the hashtag embeddings. We investigate on how the #REval framework performs under different word embedding methods and different numbers of synonyms and hashtags in the recommendation using our proposed #REval-hit-ratio measure. Our experiments of the proposed framework on three large datasets show that #REval gave more meaningful hashtag synonyms for hashtag recommendation evaluation. Our analysis also highlights the sensitivity of the framework to the word embedding technique, with #REval based on BERTag more superior over #REval based on FastText and Word2Vec.\n        \u25b3 Less\n      ",
    "title": "#REVAL: a semantic evaluation framework for hashtag recommendation",
    "date": "23 May, 2023",
    "authors": [
      "Areej Alsini",
      " Du Q. Huynh",
      " Amitava Datta"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.14061",
    "paper_id": "2303.14061",
    "abstract": "\n        This paper presents a novel approach to Multi-Agent Reinforcement Learning (MARL) that combines cooperative task decomposition with the learning of reward machines (RMs) encoding the structure of the sub-tasks. The proposed method helps deal with the non-Markovian nature of the rewards in partially observable environments and improves the interpretability of the learnt policies required to complete the cooperative task. The RMs associated with each sub-task are learnt in a decentralised manner and then used to guide the behaviour of each agent. By doing so, the complexity of a cooperative multi-agent problem is reduced, allowing for more effective learning. The results suggest that our approach is a promising direction for future research in MARL, especially in complex environments with large state spaces and multiple agents.\n        \u25b3 Less\n      ",
    "title": "Learning Reward Machines in Cooperative Multi-Agent Tasks",
    "date": "23 May, 2023",
    "authors": [
      "Leo Ardon",
      " Daniel Furelos-Blanco",
      " Alessandra Russo"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14826",
    "paper_id": "2305.14826",
    "abstract": "\n        Efficient traffic management is crucial for maintaining urban mobility, especially in densely populated areas where congestion, accidents, and delays can lead to frustrating and expensive commutes. However, existing prediction methods face challenges in terms of optimizing a single objective and understanding the complex composition of the transportation system. Moreover, they lack the ability to understand the macroscopic system and cannot efficiently utilize big data. In this paper, we propose a novel approach, Transportation Foundation Model (TFM), which integrates the principles of traffic simulation into traffic prediction. TFM uses graph structures and dynamic graph generation algorithms to capture the participatory behavior and interaction of transportation system actors. This data-driven and model-free simulation method addresses the challenges faced by traditional systems in terms of structural complexity and model accuracy and provides a foundation for solving complex transportation problems with real data. The proposed approach shows promising results in accurately predicting traffic outcomes in an urban transportation setting.\n        \u25b3 Less\n      ",
    "title": "Building Transportation Foundation Model via Generative Graph Transformer",
    "date": "23 May, 2023",
    "authors": [
      "Xuhong Wang",
      " Ding Wang",
      " Liang Chen",
      " Yilun Lin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14842",
    "paper_id": "2305.14842",
    "abstract": "\n        Sentiment analysis (SA) is the automated process of detecting and understanding the emotions conveyed through written text. Over the past decade, SA has gained significant popularity in the field of Natural Language Processing (NLP). With the widespread use of social media and online platforms, SA has become crucial for companies to gather customer feedback and shape their marketing strategies. Additionally, researchers rely on SA to analyze public sentiment on various topics. In this particular research study, a comprehensive survey was conducted to explore the latest trends and techniques in SA. The survey encompassed a wide range of methods, including lexicon-based, graph-based, network-based, machine learning, deep learning, ensemble-based, rule-based, and hybrid techniques. The paper also addresses the challenges and opportunities in SA, such as dealing with sarcasm and irony, analyzing multi-lingual data, and addressing ethical concerns. To provide a practical case study, Twitter was chosen as one of the largest online social media platforms. Furthermore, the researchers shed light on the diverse application areas of SA, including social media, healthcare, marketing, finance, and politics. The paper also presents a comparative and comprehensive analysis of existing trends and techniques, datasets, and evaluation metrics. The ultimate goal is to offer researchers and practitioners a systematic review of SA techniques, identify existing gaps, and suggest possible improvements. This study aims to enhance the efficiency and accuracy of SA processes, leading to smoother and error-free outcomes.\n        \u25b3 Less\n      ",
    "title": "Exploring Sentiment Analysis Techniques in Natural Language Processing: A Comprehensive Review",
    "date": "23 May, 2023",
    "authors": [
      "Karthick Prasad Gunasekaran"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14852",
    "paper_id": "2305.14852",
    "abstract": "\n        Given the ever-increasing size of modern neural networks, the significance of sparse architectures has surged due to their accelerated inference speeds and minimal memory demands. When it comes to global pruning techniques, Iterative Magnitude Pruning (IMP) still stands as a state-of-the-art algorithm despite its simple nature, particularly in extremely sparse regimes. In light of the recent finding that the two successive matching IMP solutions are linearly connected without a loss barrier, we propose Sparse Weight Averaging with Multiple Particles (SWAMP), a straightforward modification of IMP that achieves performance comparable to an ensemble of two IMP solutions. For every iteration, we concurrently train multiple sparse models, referred to as particles, using different batch orders yet the same matching ticket, and then weight average such models to produce a single mask. We demonstrate that our method consistently outperforms existing baselines across different sparsities through extensive experiments on various data and neural network structures.\n        \u25b3 Less\n      ",
    "title": "SWAMP: Sparse Weight Averaging with Multiple Particles for Iterative Magnitude Pruning",
    "date": "24 May, 2023",
    "authors": [
      "Moonseok Choi",
      " Hyungi Lee",
      " Giung Nam",
      " Juho Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16341",
    "paper_id": "2305.16341",
    "abstract": "\n        In this paper, we investigate the effectiveness of integrating a hierarchical taxonomy of labels as prior knowledge into the learning algorithm of a flat classifier. We introduce two methods to integrate the hierarchical taxonomy as an explicit regularizer into the loss function of learning algorithms. By reasoning on a hierarchical taxonomy, a neural network alleviates its output distributions over the classes, allowing conditioning on upper concepts for a minority class. We limit ourselves to the flat classification task and provide our experimental results on two industrial in-house datasets and two public benchmarks, RCV1 and Amazon product reviews. Our obtained results show the significant effect of a taxonomy in increasing the performance of a learner in semisupervised multi-class classification and the considerable results obtained in a fully supervised fashion.\n        \u25b3 Less\n      ",
    "title": "TaxoKnow: Taxonomy as Prior Knowledge in the Loss Function of Multi-class Classification",
    "date": "24 May, 2023",
    "authors": [
      "Mohsen Pourvali",
      " Yao Meng",
      " Chen Sheng",
      " Yangzhou Du"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14882",
    "paper_id": "2305.14882",
    "abstract": "\n        Model interpretability has long been a hard problem for the AI community especially in the multimodal setting, where vision and language need to be aligned and reasoned at the same time. In this paper, we specifically focus on the problem of Visual Question Answering (VQA). While previous researches try to probe into the network structures of black-box multimodal models, we propose to tackle the problem from a different angle -- to treat interpretability as an explicit additional goal.\n  Given an image and question, we argue that an interpretable VQA model should be able to tell what conclusions it can get from which part of the image, and show how each statement help to arrive at an answer. We introduce InterVQA: Interpretable-by-design VQA, where we design an explicit intermediate dynamic reasoning structure for VQA problems and enforce symbolic reasoning that only use the structure for final answer prediction to take place. InterVQA produces high-quality explicit intermediate reasoning steps, while maintaining similar to the state-of-the-art (sota) end-task performance.\n        \u25b3 Less\n      ",
    "title": "Interpretable by Design Visual Question Answering",
    "date": "24 May, 2023",
    "authors": [
      "Xingyu Fu",
      " Ben Zhou",
      " Sihao Chen",
      " Mark Yatskar",
      " Dan Roth"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14904",
    "paper_id": "2305.14904",
    "abstract": "\n        News articles are driven by the informational sources journalists use in reporting. Modeling when, how and why sources get used together in stories can help us better understand the information we consume and even help journalists with the task of producing it. In this work, we take steps toward this goal by constructing the largest and widest-ranging annotated dataset, to date, of informational sources used in news writing. We show that our dataset can be used to train high-performing models for information detection and source attribution. We further introduce a novel task, source prediction, to study the compositionality of sources in news articles. We show good performance on this task, which we argue is an important proof for narrative science exploring the internal structure of news articles and aiding in planning-based language generation, and an important step towards a source-recommendation system to aid journalists.\n        \u25b3 Less\n      ",
    "title": "Identifying Informational Sources in News Articles",
    "date": "24 May, 2023",
    "authors": [
      "Alexander Spangher",
      " Nanyun Peng",
      " Jonathan May",
      " Emilio Ferrara"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14917",
    "paper_id": "2305.14917",
    "abstract": "\n        This paper addresses structural ambiguity in Dutch relative clauses. By investigating the task of disambiguation by grounding, we study how the presence of a prior sentence can resolve relative clause ambiguities. We apply this method to two parsing architectures in an attempt to demystify the parsing and language model components of two present-day neural parsers. Results show that a neurosymbolic parser, based on proof nets, is more open to data bias correction than an approach based on universal dependencies, although both setups suffer from a comparable initial data bias.\n        \u25b3 Less\n      ",
    "title": "Structural Ambiguity and its Disambiguation in Language Model Based Parsers: the Case of Dutch Clause Relativization",
    "date": "24 May, 2023",
    "authors": [
      "Gijs Wijnholds",
      " Michael Moortgat"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14948",
    "paper_id": "2305.14948",
    "abstract": "\n        Music Representing Corpus Virtual (MRCV) is an open source software suite designed to explore the capabilities of Artificial Intelligence (AI) and Machine Learning (ML) in Music Generation, Sound Design, and Virtual Instrument Creation (MGSDIC). The software is accessible to users of varying levels of experience, with an emphasis on providing an explorative approach to MGSDIC. The main aim of MRCV is to facilitate creativity, allowing users to customize input datasets for training the neural networks, and offering a range of options for each neural network (thoroughly documented in the Github Wiki). The software suite is designed to be accessible to musicians, audio professionals, sound designers, and composers, regardless of their prior experience in AI or ML. The documentation is prepared in such a way as to abstract technical details, thereby making it easy to understand. The software is open source, meaning users can contribute to its development, and the community can collectively benefit from the insights and experience of other users.\n        \u25b3 Less\n      ",
    "title": "Music Representing Corpus Virtual: An Open Sourced Library for Explorative Music Generation, Sound Design, and Instrument Creation with Artificial Intelligence and Machine Learning",
    "date": "24 May, 2023",
    "authors": [
      "Christopher Johann Clarke"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.14502",
    "paper_id": "2304.14502",
    "abstract": "\n        The analysis of human movements has been extensively studied due to its wide variety of practical applications, such as human-robot interaction, human learning applications, or clinical diagnosis. Nevertheless, the state-of-the-art still faces scientific challenges when modeling human movements. To begin, new models must account for the stochasticity of human movement and the physical structure of the human body in order to accurately predict the evolution of full-body motion descriptors over time. Second, while utilizing deep learning algorithms, their explainability in terms of body posture predictions needs to be improved as they lack comprehensible representations of human movement. This paper addresses these challenges by introducing three novel methods for creating explainable representations of human movement. In this study, human body movement is formulated as a state-space model adhering to the structure of the Gesture Operational Model (GOM), whose parameters are estimated through the application of deep learning and statistical algorithms. The trained models are used for the full-body dexterity analysis of expert professionals, in which dynamic associations between body joints are identified, and for generating artificially professional movements.\n        \u25b3 Less\n      ",
    "title": "Deep state-space modeling for explainable representation, analysis, and generation of professional human poses",
    "date": "24 May, 2023",
    "authors": [
      "Brenda Elizabeth Olivas-Padilla",
      " Alina Glushkova",
      " Sotiris Manitsaris"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.02223",
    "paper_id": "2304.02223",
    "abstract": "\n        Most entropy measures depend on the spread of the probability distribution over the sample space X\\mathcal{X}, and the maximum entropy achievable scales proportionately with the sample space cardinality |X||\\mathcal{X}|. For a finite |X||\\mathcal{X}|, this yields robust entropy measures which satisfy many important properties, such as invariance to bijections, while the same is not true for continuous spaces (where |X|=\u221e|\\mathcal{X}|=\\infty). Furthermore, since R\\mathbb{R} and Rd\\mathbb{R}^d (d\u2208Z+d\\in \\mathbb{Z}^+) have the same cardinality (from Cantor's correspondence argument), cardinality-dependent entropy measures cannot encode the data dimensionality. In this work, we question the role of cardinality and distribution spread in defining entropy measures for continuous spaces, which can undergo multiple rounds of transformations and distortions, e.g., in neural networks. We find that the average value of the local intrinsic dimension of a distribution, denoted as ID-Entropy, can serve as a robust entropy measure for continuous spaces, while capturing the data dimensionality. We find that ID-Entropy satisfies many desirable properties and can be extended to conditional entropy, joint entropy and mutual-information variants. ID-Entropy also yields new information bottleneck principles and also links to causality. In the context of deep learning, for feedforward architectures, we show, theoretically and empirically, that the ID-Entropy of a hidden layer directly controls the generalization gap for both classifiers and auto-encoders, when the target function is Lipschitz continuous. Our work primarily shows that, for continuous spaces, taking a structural rather than a statistical approach yields entropy measures which preserve intrinsic data dimensionality, while being relevant for studying various architectures.\n        \u25b3 Less\n      ",
    "title": "Local Intrinsic Dimensional Entropy",
    "date": "24 May, 2023",
    "authors": [
      "Rohan Ghosh",
      " Mehul Motani"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14970",
    "paper_id": "2305.14970",
    "abstract": "\n        Event temporal reasoning aims at identifying the temporal relations between two or more events. However, knowledge conflicts arise when there is a mismatch between the actual temporal relations of events in the context and the prior knowledge or biases learned by the model. We first systematically define distinct kinds of bias in event temporal reasoning, which include event relation prior bias, tense bias, narrative bias, and dependency bias, as indicators to study knowledge conflicts. To mitigate such event-related knowledge conflict, we introduce a Counterfactual Data Augmentation based method that can be applied to both Pre-trained Language Models (PLMs) and Large Language Models (LLMs) either as additional training data or demonstrations for In-Context Learning. Experiments suggest the importance of mitigating knowledge conflicts in event temporal reasoning tasks for reducing hallucination and highlight the potential of counterfactual data augmentation for improving model performance.\n        \u25b3 Less\n      ",
    "title": "Getting Sick After Seeing a Doctor? Diagnosing and Mitigating Knowledge Conflicts in Event Temporal Reasoning",
    "date": "24 May, 2023",
    "authors": [
      "Tianqing Fang",
      " Zhaowei Wang",
      " Wenxuan Zhou",
      " Hongming Zhang",
      " Yangqiu Song",
      " Muhao Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16343",
    "paper_id": "2305.16343",
    "abstract": "\n        Automatic Term Recognition is used to extract domain-specific terms that belong to a given domain. In order to be accurate, these corpus and language-dependent methods require large volumes of textual data that need to be processed to extract candidate terms that are afterward scored according to a given metric. To improve text preprocessing and candidate terms extraction and scoring, we propose a distributed Spark-based architecture to automatically extract domain-specific terms. The main contributions are as follows: (1) propose a novel distributed automatic domain-specific multi-word term recognition architecture built on top of the Spark ecosystem; (2) perform an in-depth analysis of our architecture in terms of accuracy and scalability; (3) design an easy-to-integrate Python implementation that enables the use of Big Data processing in fields such as Computational Linguistics and Natural Language Processing. We prove empirically the feasibility of our architecture by performing experiments on two real-world datasets.\n        \u25b3 Less\n      ",
    "title": "A Distributed Automatic Domain-Specific Multi-Word Term Recognition Architecture using Spark Ecosystem",
    "date": "24 May, 2023",
    "authors": [
      "Ciprian-Octavian Truic\u0103",
      " Neculai-Ovidiu Istrate",
      " Elena-Simona Apostol"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14981",
    "paper_id": "2305.14981",
    "abstract": "\n        Improving factual consistency of abstractive summarization has been a widely studied topic. However, most of the prior works on training factuality-aware models have ignored the negative effect it has on summary quality. We propose EFACTSUM (i.e., Effective Factual Summarization), a candidate summary generation and ranking technique to improve summary factuality without sacrificing summary quality. We show that using a contrastive learning framework with our refined candidate summaries leads to significant gains on both factuality and similarity-based metrics. Specifically, we propose a ranking strategy in which we effectively combine two metrics, thereby preventing any conflict during training. Models trained using our approach show up to 6 points of absolute improvement over the base model with respect to FactCC on XSUM and 11 points on CNN/DM, without negatively affecting either similarity-based metrics or absractiveness.\n        \u25b3 Less\n      ",
    "title": "Improving Factuality of Abstractive Summarization without Sacrificing Summary Quality",
    "date": "24 May, 2023",
    "authors": [
      "Tanay Dixit",
      " Fei Wang",
      " Muhao Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.04657",
    "paper_id": "2306.04657",
    "abstract": "\n        In empathetic conversations, individuals express their empathy towards others. Previous work has mainly focused on generating empathetic responses by utilizing the speaker's emotion. Besides, external commonsense knowledge has been applied to enhance the system's understandings of the speaker's situation. However, given an event, commonsense knowledge base contains various relations, potentially leading to confusion for the dialogue system. Consequently, inconsistencies arise among the emotion, generated response and speaker's contextual information. To this end, we propose a novel approach for empathetic response generation, which incorporates an adaptive module for commonsense knowledge selection to ensure consistency between the generated empathetic responses and the speaker's situation. This selected knowledge is used to refine the commonsense cognition and empathy expression for generated responses. Experimental results show that our approach significantly outperforms baseline models in both automatic and human evaluations, exhibiting the generation of more coherent and empathetic responses. Moreover, case studies highlight the interpretability of knowledge selection in the responses and the effectiveness of adaptive module in our model. Code: https://github.com/Hanscal/DCKS.\n        \u25b3 Less\n      ",
    "title": "Improving Empathetic Dialogue Generation by Dynamically Infusing Commonsense Knowledge",
    "date": "24 May, 2023",
    "authors": [
      "Hua Cai",
      " Xuli Shen",
      " Qing Xu",
      " Weilin Shen",
      " Xiaomei Wang",
      " Weifeng Ge",
      " Xiaoqing Zheng",
      " Xiangyang Xue"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2210.14389",
    "paper_id": "2210.14389",
    "abstract": "\n        Research on Korean grammatical error correction (GEC) is limited, compared to other major languages such as English. We attribute this problematic circumstance to the lack of a carefully designed evaluation benchmark for Korean GEC. In this work, we collect three datasets from different sources (Kor-Lang8, Kor-Native, and Kor-Learner) that covers a wide range of Korean grammatical errors. Considering the nature of Korean grammar, We then define 14 error types for Korean and provide KAGAS (Korean Automatic Grammatical error Annotation System), which can automatically annotate error types from parallel corpora. We use KAGAS on our datasets to make an evaluation benchmark for Korean, and present baseline models trained from our datasets. We show that the model trained with our datasets significantly outperforms the currently used statistical Korean GEC system (Hanspell) on a wider range of error types, demonstrating the diversity and usefulness of the datasets. The implementations and datasets are open-sourced.\n        \u25b3 Less\n      ",
    "title": "Towards standardizing Korean Grammatical Error Correction: Datasets and Annotation",
    "date": "24 May, 2023",
    "authors": [
      "Soyoung Yoon",
      " Sungjoon Park",
      " Gyuwan Kim",
      " Junhee Cho",
      " Kihyo Park",
      " Gyutae Kim",
      " Minjoon Seo",
      " Alice Oh"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15008",
    "paper_id": "2305.15008",
    "abstract": "\n        LLM-powered chatbots are becoming widely adopted in applications such as healthcare, personal assistants, industry hiring decisions, etc. In many of these cases, chatbots are fed sensitive, personal information in their prompts, as samples for in-context learning, retrieved records from a database, or as part of the conversation. The information provided in the prompt could directly appear in the output, which might have privacy ramifications if there is sensitive information there. As such, in this paper, we aim to understand the input copying and regurgitation capabilities of these models during inference and how they can be directly instructed to limit this copying by complying with regulations such as HIPAA and GDPR, based on their internal knowledge of them. More specifically, we find that when ChatGPT is prompted to summarize cover letters of a 100 candidates, it would retain personally identifiable information (PII) verbatim in 57.4% of cases, and we find this retention to be non-uniform between different subgroups of people, based on attributes such as gender identity. We then probe ChatGPT's perception of privacy-related policies and privatization mechanisms by directly instructing it to provide compliant outputs and observe a significant omission of PII from output.\n        \u25b3 Less\n      ",
    "title": "Are Chatbots Ready for Privacy-Sensitive Applications? An Investigation into Input Regurgitation and Prompt-Induced Sanitization",
    "date": "24 May, 2023",
    "authors": [
      "Aman Priyanshu",
      " Supriti Vijay",
      " Ayush Kumar",
      " Rakshit Naidu",
      " Fatemehsadat Mireshghallah"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.13680",
    "paper_id": "2304.13680",
    "abstract": "\n        The new regulatory framework proposal on Artificial Intelligence (AI) published by the European Commission establishes a new risk-based legal approach. The proposal highlights the need to develop adequate risk assessments for the different uses of AI. This risk assessment should address, among others, the detection and mitigation of bias in AI. In this work we analyze statistical approaches to measure biases in automatic decision-making systems. We focus our experiments in face recognition technologies. We propose a novel way to measure the biases in machine learning models using a statistical approach based on the N-Sigma method. N-Sigma is a popular statistical approach used to validate hypotheses in general science such as physics and social areas and its application to machine learning is yet unexplored. In this work we study how to apply this methodology to develop new risk assessment frameworks based on bias analysis and we discuss the main advantages and drawbacks with respect to other popular statistical tests.\n        \u25b3 Less\n      ",
    "title": "Measuring Bias in AI Models: An Statistical Approach Introducing N-Sigma",
    "date": "24 May, 2023",
    "authors": [
      "Daniel DeAlcala",
      " Ignacio Serna",
      " Aythami Morales",
      " Julian Fierrez",
      " Javier Ortega-Garcia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15024",
    "paper_id": "2305.15024",
    "abstract": "\n        In the era of sustainable smart agriculture, a massive amount of agricultural news text is being posted on the Internet, in which massive agricultural knowledge has been accumulated. In this context, it is urgent to explore effective text classification techniques for users to access the required agricultural knowledge with high efficiency. Mainstream deep learning approaches employing fine-tuning strategies on pre-trained language models (PLMs), have demonstrated remarkable performance gains over the past few years. Nonetheless, these methods still face many drawbacks that are complex to solve, including: 1. Limited agricultural training data due to the expensive-cost and labour-intensive annotation; 2. Poor domain transferability, especially of cross-linguistic ability; 3. Complex and expensive large models deployment.Inspired by the extraordinary success brought by the recent ChatGPT (e.g. GPT-3.5, GPT-4), in this work, we systematically investigate and explore the capability and utilization of ChatGPT applying to the agricultural informatization field. ....(shown in article).... Code has been released on Github https://github.com/albert-jin/agricultural_textual_classification_ChatGPT.\n        \u25b3 Less\n      ",
    "title": "ChatAgri: Exploring Potentials of ChatGPT on Cross-linguistic Agricultural Text Classification",
    "date": "24 May, 2023",
    "authors": [
      "Biao Zhao",
      " Weiqiang Jin",
      " Javier Del Ser",
      " Guang Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15032",
    "paper_id": "2305.15032",
    "abstract": "\n        Recently, various intermediate layer distillation (ILD) objectives have been shown to improve compression of BERT models via Knowledge Distillation (KD). However, a comprehensive evaluation of the objectives in both task-specific and task-agnostic settings is lacking. To the best of our knowledge, this is the first work comprehensively evaluating distillation objectives in both settings. We show that attention transfer gives the best performance overall. We also study the impact of layer choice when initializing the student from the teacher layers, finding a significant impact on the performance in task-specific distillation. For vanilla KD and hidden states transfer, initialisation with lower layers of the teacher gives a considerable improvement over higher layers, especially on the task of QNLI (up to an absolute percentage change of 17.8 in accuracy). Attention transfer behaves consistently under different initialisation settings. We release our code as an efficient transformer-based model distillation framework for further studies.\n        \u25b3 Less\n      ",
    "title": "How to Distill your BERT: An Empirical Study on the Impact of Weight Initialisation and Distillation Objectives",
    "date": "24 May, 2023",
    "authors": [
      "Xinpeng Wang",
      " Leonie Weissweiler",
      " Hinrich Sch\u00fctze",
      " Barbara Plank"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2105.10719",
    "paper_id": "2105.10719",
    "abstract": "\n        Masking some input variables of a deep neural network (DNN) and computing output changes on the masked input sample represent a typical way to compute attributions of input variables in the sample. People usually mask an input variable using its baseline value. However, there is no theory to examine whether baseline value faithfully represents the absence of an input variable, \\emph{i.e.,} removing all signals from the input variable. Fortunately, recent studies show that the inference score of a DNN can be strictly disentangled into a set of causal patterns (or concepts) encoded by the DNN. Therefore, we propose to use causal patterns to examine the faithfulness of baseline values. More crucially, it is proven that causal patterns can be explained as the elementary rationale of the Shapley value. Furthermore, we propose a method to learn optimal baseline values, and experimental results have demonstrated its effectiveness.\n        \u25b3 Less\n      ",
    "title": "Can We Faithfully Represent Masked States to Compute Shapley Values on a DNN?",
    "date": "24 May, 2023",
    "authors": [
      "Jie Ren",
      " Zhanpeng Zhou",
      " Qirui Chen",
      " Quanshi Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.02941",
    "paper_id": "2302.02941",
    "abstract": "\n        Message Passing Neural Networks (MPNNs) are instances of Graph Neural Networks that leverage the graph to send messages over the edges. This inductive bias leads to a phenomenon known as over-squashing, where a node feature is insensitive to information contained at distant nodes. Despite recent methods introduced to mitigate this issue, an understanding of the causes for over-squashing and of possible solutions are lacking. In this theoretical work, we prove that: (i) Neural network width can mitigate over-squashing, but at the cost of making the whole network more sensitive; (ii) Conversely, depth cannot help mitigate over-squashing: increasing the number of layers leads to over-squashing being dominated by vanishing gradients; (iii) The graph topology plays the greatest role, since over-squashing occurs between nodes at high commute (access) time. Our analysis provides a unified framework to study different recent methods introduced to cope with over-squashing and serves as a justification for a class of methods that fall under graph rewiring.\n        \u25b3 Less\n      ",
    "title": "On Over-Squashing in Message Passing Neural Networks: The Impact of Width, Depth, and Topology",
    "date": "24 May, 2023",
    "authors": [
      "Francesco Di Giovanni",
      " Lorenzo Giusti",
      " Federico Barbero",
      " Giulia Luise",
      " Pietro Lio'",
      " Michael Bronstein"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15055",
    "paper_id": "2305.15055",
    "abstract": "\n        Many existing works on voice conversion (VC) tasks use automatic speech recognition (ASR) models for ensuring linguistic consistency between source and converted samples. However, for the low-data resource domains, training a high-quality ASR remains to be a challenging task. In this work, we propose a novel iterative way of improving both the ASR and VC models. We first train an ASR model which is used to ensure content preservation while training a VC model. In the next iteration, the VC model is used as a data augmentation method to further fine-tune the ASR model and generalize it to diverse speakers. By iteratively leveraging the improved ASR model to train VC model and vice-versa, we experimentally show improvement in both the models. Our proposed framework outperforms the ASR and one-shot VC baseline models on English singing and Hindi speech domains in subjective and objective evaluations in low-data resource settings.\n        \u25b3 Less\n      ",
    "title": "Iteratively Improving Speech Recognition and Voice Conversion",
    "date": "24 May, 2023",
    "authors": [
      "Mayank Kumar Singh",
      " Naoya Takahashi",
      " Onoe Naoyuki"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11169",
    "paper_id": "2305.11169",
    "abstract": "\n        We present evidence that language models can learn meaning despite being trained only to perform next token prediction on text, specifically a corpus of programs. Each program is preceded by a specification in the form of (textual) input-output examples. Working with programs enables us to precisely define concepts relevant to meaning in language (e.g., correctness and semantics), making program synthesis well-suited as an intermediate testbed for characterizing the presence (or absence) of meaning in language models.\n  We first train a Transformer model on the corpus of programs, then probe the trained model's hidden states as it completes a program given a specification. Despite providing no inductive bias toward learning the semantics of the language, we find that a linear probe is able to extract abstractions of both current and future program states from the model states. Moreover, there is a strong, statistically significant correlation between the accuracy of the probe and the model's ability to generate a program that implements the specification. To evaluate whether the semantics are represented in the model states rather than learned by the probe, we design a novel experimental procedure that intervenes on the semantics of the language while preserving the lexicon and syntax. We also demonstrate that the model learns to generate correct programs that are, on average, shorter than those in the training set, which is evidence that language model outputs may differ from the training distribution in semantically meaningful ways. In summary, this paper does not propose any new techniques for training language models, but develops an experimental framework for and provides insights into the acquisition and representation of (formal) meaning in language models.\n        \u25b3 Less\n      ",
    "title": "Evidence of Meaning in Language Models Trained on Programs",
    "date": "24 May, 2023",
    "authors": [
      "Charles Jin",
      " Martin Rinard"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15075",
    "paper_id": "2305.15075",
    "abstract": "\n        In this paper, we present HuatuoGPT, a large language model (LLM) for medical consultation. The core recipe of HuatuoGPT is to leverage both \\textit{distilled data from ChatGPT} and \\textit{real-world data from doctors} in the supervised fine-tuned stage. The responses of ChatGPT are usually detailed, well-presented and informative while it cannot perform like a doctor in many aspects, e.g. for integrative diagnosis. We argue that real-world data from doctors would be complementary to distilled data in the sense the former could tame a distilled language model to perform like doctors. To better leverage the strengths of both data, we train a reward model to align the language model with the merits that both data bring, following an RLAIF (reinforced learning from AI feedback) fashion. To evaluate and benchmark the models, we propose a comprehensive evaluation scheme (including automatic and manual metrics). Experimental results demonstrate that HuatuoGPT achieves state-of-the-art results in performing medical consultation among open-source LLMs in GPT-4 evaluation, human evaluation, and medical benchmark datasets. It is worth noting that by using additional real-world data and RLAIF, the distilled language model (i.e., HuatuoGPT) outperforms its teacher model ChatGPT in most cases. Our code, data, and models are publicly available at \\url{https://github.com/FreedomIntelligence/HuatuoGPT}. The online demo is available at \\url{https://www.HuatuoGPT.cn/}.\n        \u25b3 Less\n      ",
    "title": "HuatuoGPT, towards Taming Language Model to Be a Doctor",
    "date": "24 May, 2023",
    "authors": [
      "Hongbo Zhang",
      " Junying Chen",
      " Feng Jiang",
      " Fei Yu",
      " Zhihong Chen",
      " Jianquan Li",
      " Guiming Chen",
      " Xiangbo Wu",
      " Zhiyi Zhang",
      " Qingying Xiao",
      " Xiang Wan",
      " Benyou Wang",
      " Haizhou Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15097",
    "paper_id": "2305.15097",
    "abstract": "\n        Construction progress monitoring (CPM) is essential for effective project management, ensuring on-time and on-budget delivery. Traditional CPM methods often rely on manual inspection and reporting, which are time-consuming and prone to errors. This paper proposes a novel approach for automated CPM using state-of-the-art object detection algorithms. The proposed method leverages e.g. YOLOv8's real-time capabilities and high accuracy to identify and track construction elements within site images and videos. A dataset was created, consisting of various building elements and annotated with relevant objects for training and validation. The performance of the proposed approach was evaluated using standard metrics, such as precision, recall, and F1-score, demonstrating significant improvement over existing methods. The integration of Computer Vision into CPM provides stakeholders with reliable, efficient, and cost-effective means to monitor project progress, facilitating timely decision-making and ultimately contributing to the successful completion of construction projects.\n        \u25b3 Less\n      ",
    "title": "Computer Vision for Construction Progress Monitoring: A Real-Time Object Detection Approach",
    "date": "24 May, 2023",
    "authors": [
      "Jiesheng Yang",
      " Andreas Wilde",
      " Karsten Menzel",
      " Md Zubair Sheikh",
      " Boris Kuznetsov"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.08252",
    "paper_id": "2305.08252",
    "abstract": "\n        We present a comprehensive evaluation of Parameter-Efficient Fine-Tuning (PEFT) techniques for diverse medical image analysis tasks. PEFT is increasingly exploited as a valuable approach for knowledge transfer from pre-trained models in natural language processing, vision, speech, and cross-modal tasks, such as vision-language and text-to-image generation. However, its application in medical image analysis remains relatively unexplored. As foundation models are increasingly exploited in the medical domain, it is crucial to investigate and comparatively assess various strategies for knowledge transfer that can bolster a range of downstream tasks. Our study, the first of its kind (to the best of our knowledge), evaluates 16 distinct PEFT methodologies proposed for convolutional and transformer-based networks, focusing on image classification and text-to-image generation tasks across six medical datasets ranging in size, modality, and complexity. Through a battery of more than 600 controlled experiments, we demonstrate performance gains of up to 22% under certain scenarios and demonstrate the efficacy of PEFT for medical text-to-image generation. Further, we reveal the instances where PEFT methods particularly dominate over conventional fine-tuning approaches by studying their relationship with downstream data volume.\n        \u25b3 Less\n      ",
    "title": "Parameter-Efficient Fine-Tuning for Medical Image Analysis: The Missed Opportunity",
    "date": "24 May, 2023",
    "authors": [
      "Raman Dutt",
      " Linus Ericsson",
      " Pedro Sanchez",
      " Sotirios A. Tsaftaris",
      " Timothy Hospedales"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.02919",
    "paper_id": "2205.02919",
    "abstract": "\n        Although moral responsibility is not circumscribed by causality, they are both closely intermixed. Furthermore, rationally understanding the evolution of the physical world is inherently linked with the idea of causality. Thus, the decision-making applications based on automated planning inevitably have to deal with causality, especially if they consider imputability aspects or integrate references to ethical norms. The many debates around causation in the last decades have shown how complex this notion is and thus, how difficult is its integration with planning. As a result, much of the work in computational ethics relegates causality to the background, despite the considerations stated above. This paper's contribution is to provide a complete and sound translation into logic programming from an actual causation definition suitable for action languages, this definition is a formalisation of Wright's NESS test. The obtained logic program allows to deal with complex causal relations. In addition to enabling agents to reason about causality, this contribution specifically enables the computational ethics domain to handle situations that were previously out of reach. In a context where ethical considerations in decision-making are increasingly important, advances in computational ethics can greatly benefit the entire AI community.\n        \u25b3 Less\n      ",
    "title": "Action Languages Based Actual Causality for Computational Ethics: a Sound and Complete Implementation in ASP",
    "date": "24 May, 2023",
    "authors": [
      "Camilo Sarmiento",
      " Gauvain Bourgne",
      " Katsumi Inoue",
      " Daniele Cavalli",
      " Jean-Gabriel Ganascia"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15109",
    "paper_id": "2305.15109",
    "abstract": "\n        We provide a learning-based technique for guessing a winning strategy in a parity game originating from an LTL synthesis problem. A cheaply obtained guess can be useful in several applications. Not only can the guessed strategy be applied as best-effort in cases where the game's huge size prohibits rigorous approaches, but it can also increase the scalability of rigorous LTL synthesis in several ways. Firstly, checking whether a guessed strategy is winning is easier than constructing one. Secondly, even if the guess is wrong in some places, it can be fixed by strategy iteration faster than constructing one from scratch. Thirdly, the guess can be used in on-the-fly approaches to prioritize exploration in the most fruitful directions.\n  In contrast to previous works, we (i)~reflect the highly structured logical information in game's states, the so-called semantic labelling, coming from the recent LTL-to-automata translations, and (ii)~learn to reflect it properly by learning from previously solved games, bringing the solving process closer to human-like reasoning.\n        \u25b3 Less\n      ",
    "title": "Guessing Winning Policies in LTL Synthesis by Semantic Learning",
    "date": "24 May, 2023",
    "authors": [
      "Jan Kretinsky",
      " Tobias Meggendorfer",
      " Maximilian Prokop",
      " Sabine Rieder"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.04285",
    "paper_id": "2212.04285",
    "abstract": "\n        High-quality healthcare in the US can be cost-prohibitive for certain socioeconomic groups. In this paper, we examined data from the US Census and the CDC to determine the degree to which specific socioeconomic factors correlate with both specific and general health metrics. We employed visual analysis to find broad trends and predictive modeling to identify more complex relationships between variables. Our results indicate that certain socioeconomic factors, like income and educational attainment, are highly correlated with aggregate measures of health.\n        \u25b3 Less\n      ",
    "title": "The Impact of Socioeconomic Factors on Health Disparities",
    "date": "24 May, 2023",
    "authors": [
      "Krish Khanna",
      " Jeffrey Lu",
      " Jay Warrier"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2102.12551",
    "paper_id": "2102.12551",
    "abstract": "\n        Scientists form hypotheses and experimentally test them. If a hypothesis fails (is refuted), scientists try to explain the failure to eliminate other hypotheses. The more precise the failure analysis the more hypotheses can be eliminated. Thus inspired, we introduce failure explanation techniques for inductive logic programming. Given a hypothesis represented as a logic program, we test it on examples. If a hypothesis fails, we explain the failure in terms of failing sub-programs. In case a positive example fails, we identify failing sub-programs at the granularity of literals. We introduce a failure explanation algorithm based on analysing branches of SLD-trees. We integrate a meta-interpreter based implementation of this algorithm with the test-stage of the Popper ILP system. We show that fine-grained failure analysis allows for learning fine-grained constraints on the hypothesis space. Our experimental results show that explaining failures can drastically reduce hypothesis space exploration and learning times.\n        \u25b3 Less\n      ",
    "title": "Learning logic programs by explaining their failures",
    "date": "24 May, 2023",
    "authors": [
      "Rolf Morel",
      " Andrew Cropper"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15130",
    "paper_id": "2305.15130",
    "abstract": "\n        Natural language understanding (NLU) studies often exaggerate or underestimate the capabilities of systems, thereby limiting the reproducibility of their findings. These erroneous evaluations can be attributed to the difficulty of defining and testing NLU adequately. In this position paper, we reconsider this challenge by identifying two types of researcher degrees of freedom. We revisit Turing's original interpretation of the Turing test and indicate that an NLU test does not provide an operational definition; it merely provides inductive evidence that the test subject understands the language sufficiently well to meet stakeholder objectives. In other words, stakeholders are free to arbitrarily define NLU through their objectives. To use the test results as inductive evidence, stakeholders must carefully assess if the interpretation of test scores is valid or not. However, designing and using NLU tests involve other degrees of freedom, such as specifying target skills and defining evaluation metrics. As a result, achieving consensus among stakeholders becomes difficult. To resolve this issue, we propose a validity argument, which is a framework comprising a series of validation criteria across test components. By demonstrating that current practices in NLU studies can be associated with those criteria and organizing them into a comprehensive checklist, we prove that the validity argument can serve as a coherent guideline for designing credible test sets and facilitating scientific communication.\n        \u25b3 Less\n      ",
    "title": "On Degrees of Freedom in Defining and Testing Natural Language Understanding",
    "date": "24 May, 2023",
    "authors": [
      "Saku Sugawara",
      " Shun Tsugita"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15138",
    "paper_id": "2305.15138",
    "abstract": "\n        Millions of users are active on social media. To allow users to better showcase themselves and network with others, we explore the auto-generation of social media self-introduction, a short sentence outlining a user's personal interests. While most prior work profiles users with tags (e.g., ages), we investigate sentence-level self-introductions to provide a more natural and engaging way for users to know each other. Here we exploit a user's tweeting history to generate their self-introduction. The task is non-trivial because the history content may be lengthy, noisy, and exhibit various personal interests. To address this challenge, we propose a novel unified topic-guided encoder-decoder (UTGED) framework; it models latent topics to reflect salient user interest, whose topic mixture then guides encoding a user's history and topic words control decoding their self-introduction. For experiments, we collect a large-scale Twitter dataset, and extensive results show the superiority of our UTGED to the advanced encoder-decoder models without topic modeling.\n        \u25b3 Less\n      ",
    "title": "Topic-Guided Self-Introduction Generation for Social Media Users",
    "date": "24 May, 2023",
    "authors": [
      "Chunpu Xu",
      " Jing Li",
      " Piji Li",
      " Min Yang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2310.11470",
    "paper_id": "2310.11470",
    "abstract": "\n        In this chapter, we present the main classic machine learning methods. A large part of the chapter is devoted to supervised learning techniques for classification and regression, including nearest-neighbor methods, linear and logistic regressions, support vector machines and tree-based algorithms. We also describe the problem of overfitting as well as strategies to overcome it. We finally provide a brief overview of unsupervised learning methods, namely for clustering and dimensionality reduction.\n        \u25b3 Less\n      ",
    "title": "Classic machine learning methods",
    "date": "24 May, 2023",
    "authors": [
      "Johann Faouzi",
      " Olivier Colliot"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15149",
    "paper_id": "2305.15149",
    "abstract": "\n        Cauliflower is a hand-harvested crop that must fulfill high-quality standards in sales making the timing of harvest important. However, accurately determining harvest-readiness can be challenging due to the cauliflower head being covered by its canopy. While deep learning enables automated harvest-readiness estimation, errors can occur due to field-variability and limited training data. In this paper, we analyze the reliability of a harvest-readiness classifier with interpretable machine learning. By identifying clusters of saliency maps, we derive reliability scores for each classification result using knowledge about the domain and the image properties. For unseen data, the reliability can be used to (i) inform farmers to improve their decision-making and (ii) increase the model prediction accuracy. Using RGB images of single cauliflower plants at different developmental stages from the GrowliFlower dataset, we investigate various saliency mapping approaches and find that they result in different quality of reliability scores. With the most suitable interpretation tool, we adjust the classification result and achieve a 15.72% improvement of the overall accuracy to 88.14% and a 15.44% improvement of the average class accuracy to 88.52% for the GrowliFlower dataset.\n        \u25b3 Less\n      ",
    "title": "Reliability Scores from Saliency Map Clusters for Improved Image-based Harvest-Readiness Prediction in Cauliflower",
    "date": "24 May, 2023",
    "authors": [
      "Jana Kierdorf",
      " Ribana Roscher"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.14138",
    "paper_id": "2207.14138",
    "abstract": "\n        Ad hoc teamwork (AHT) is the challenge of designing a robust learner agent that effectively collaborates with unknown teammates without prior coordination mechanisms. Early approaches address the AHT challenge by training the learner with a diverse set of handcrafted teammate policies, usually designed based on an expert's domain knowledge about the policies the learner may encounter. However, implementing teammate policies for training based on domain knowledge is not always feasible. In such cases, recent approaches attempted to improve the robustness of the learner by training it with teammate policies generated by optimising information-theoretic diversity metrics. The problem with optimising existing information-theoretic diversity metrics for teammate policy generation is the emergence of superficially different teammates. When used for AHT training, superficially different teammate behaviours may not improve a learner's robustness during collaboration with unknown teammates. In this paper, we present an automated teammate policy generation method optimising the Best-Response Diversity (BRDiv) metric, which measures diversity based on the compatibility of teammate policies in terms of returns. We evaluate our approach in environments with multiple valid coordination strategies, comparing against methods optimising information-theoretic diversity metrics and an ablation not optimising any diversity metric. Our experiments indicate that optimising BRDiv yields a diverse set of training teammate policies that improve the learner's performance relative to previous teammate generation approaches when collaborating with near-optimal previously unseen teammate policies.\n        \u25b3 Less\n      ",
    "title": "Generating Teammates for Training Robust Ad Hoc Teamwork Agents via Best-Response Diversity",
    "date": "24 May, 2023",
    "authors": [
      "Arrasy Rahman",
      " Elliot Fosong",
      " Ignacio Carlucho",
      " Stefano V. Albrecht"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15165",
    "paper_id": "2305.15165",
    "abstract": "\n        Personalized privacy becomes critical in deep learning for Trustworthy AI. While Differentially Private Stochastic Gradient Descent (DP-SGD) is widely used in deep learning methods supporting privacy, it provides the same level of privacy to all individuals, which may lead to overprotection and low utility. In practice, different users may require different privacy levels, and the model can be improved by using more information about the users with lower privacy requirements. There are also recent works on differential privacy of individuals when using DP-SGD, but they are mostly about individual privacy accounting and do not focus on satisfying different privacy levels. We thus extend DP-SGD to support a recent privacy notion called (\u03a6\u03a6,\u0394\u0394)-Personalized Differential Privacy ((\u03a6\u03a6,\u0394\u0394)-PDP), which extends an existing PDP concept called \u03a6\u03a6-PDP. Our algorithm uses a multi-round personalized sampling mechanism and embeds it within the DP-SGD iterations. Experiments on real datasets show that our algorithm outperforms DP-SGD and simple combinations of DP-SGD with existing PDP mechanisms in terms of model performance and efficiency due to its embedded sampling mechanism.\n        \u25b3 Less\n      ",
    "title": "Personalized DP-SGD using Sampling Mechanisms",
    "date": "24 May, 2023",
    "authors": [
      "Geon Heo",
      " Junseok Seo",
      " Steven Euijong Whang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2304.04227",
    "paper_id": "2304.04227",
    "abstract": "\n        Video captioning aims to convey dynamic scenes from videos using natural language, facilitating the understanding of spatiotemporal information within our environment. Although there have been recent advances, generating detailed and enriched video descriptions continues to be a substantial challenge. In this work, we introduce Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions. Our method employs a ChatGPT model as a controller, specifically designed to select frames for posing video content-driven questions. Subsequently, a robust algorithm is utilized to answer these visual queries. This question-answer framework effectively uncovers intricate video details and shows promise as a method for enhancing video content. Following multiple conversational rounds, ChatGPT can summarize enriched video content based on previous conversations. We qualitatively demonstrate that our Video ChatCaptioner can generate captions containing more visual details about the videos. The code is publicly available at https://github.com/Vision-CAIR/ChatCaptioner\n        \u25b3 Less\n      ",
    "title": "Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions",
    "date": "24 May, 2023",
    "authors": [
      "Jun Chen",
      " Deyao Zhu",
      " Kilichbek Haydarov",
      " Xiang Li",
      " Mohamed Elhoseiny"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15186",
    "paper_id": "2305.15186",
    "abstract": "\n        Automatic literature review generation is one of the most challenging tasks in natural language processing. Although large language models have tackled literature review generation, the absence of large-scale datasets has been a stumbling block to the progress. We release SciReviewGen, consisting of over 10,000 literature reviews and 690,000 papers cited in the reviews. Based on the dataset, we evaluate recent transformer-based summarization models on the literature review generation task, including Fusion-in-Decoder extended for literature review generation. Human evaluation results show that some machine-generated summaries are comparable to human-written reviews, while revealing the challenges of automatic literature review generation such as hallucinations and a lack of detailed information. Our dataset and code are available at https://github.com/tetsu9923/SciReviewGen.\n        \u25b3 Less\n      ",
    "title": "SciReviewGen: A Large-scale Dataset for Automatic Literature Review Generation",
    "date": "24 May, 2023",
    "authors": [
      "Tetsu Kasanishi",
      " Masaru Isonuma",
      " Junichiro Mori",
      " Ichiro Sakata"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15203",
    "paper_id": "2305.15203",
    "abstract": "\n        Despite their impressive performance in classification, neural networks are known to be vulnerable to adversarial attacks. These attacks are small perturbations of the input data designed to fool the model. Naturally, a question arises regarding the potential connection between the architecture, settings, or properties of the model and the nature of the attack. In this work, we aim to shed light on this problem by focusing on the implicit bias of the neural network, which refers to its inherent inclination to favor specific patterns or outcomes. Specifically, we investigate one aspect of the implicit bias, which involves the essential Fourier frequencies required for accurate image classification. We conduct tests to assess the statistical relationship between these frequencies and those necessary for a successful attack. To delve into this relationship, we propose a new method that can uncover non-linear correlations between sets of coordinates, which, in our case, are the aforementioned frequencies. By exploiting the entanglement between intrinsic dimension and correlation, we provide empirical evidence that the network bias in Fourier space and the target frequencies of adversarial attacks are closely tied.\n        \u25b3 Less\n      ",
    "title": "Relating Implicit Bias and Adversarial Attacks through Intrinsic Dimension",
    "date": "24 May, 2023",
    "authors": [
      "Lorenzo Basile",
      " Nikos Karantzas",
      " Alberto D'Onofrio",
      " Luca Bortolussi",
      " Alex Rodriguez",
      " Fabio Anselmi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16346",
    "paper_id": "2305.16346",
    "abstract": "\n        The rising prevalence of type 2 diabetes mellitus (T2DM) necessitates the development of predictive models for T2DM risk assessment. Artificial intelligence (AI) models are being extensively used for this purpose, but a comprehensive review of their advancements and challenges is lacking. This scoping review analyzes existing literature on AI-based models for T2DM risk prediction. Forty studies were included, mainly published in the past four years. Traditional machine learning models were more prevalent than deep learning models. Electronic health records were the most commonly used data source. Unimodal AI models relying on EHR data were prominent, while only a few utilized multimodal models. Both unimodal and multimodal models showed promising performance, with the latter outperforming the former. Internal validation was common, while external validation was limited. Interpretability methods were reported in half of the studies. Few studies reported novel biomarkers, and open-source code availability was limited. This review provides insights into the current state and limitations of AI-based T2DM risk prediction models and highlights challenges for their development and clinical implementation.\n        \u25b3 Less\n      ",
    "title": "Artificial Intelligence-Based Methods for Precision Medicine: Diabetes Risk Prediction",
    "date": "24 May, 2023",
    "authors": [
      "Farida Mohsen",
      " Hamada R. H. Al-Absi",
      " Noha A. Yousri",
      " Nady El Hajj",
      " Zubair Shah"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16347",
    "paper_id": "2305.16347",
    "abstract": "\n        Synthesis of digital artifacts conditioned on user prompts has become an important paradigm facilitating an explosion of use cases with generative AI. However, such models often fail to connect the generated outputs and desired target concepts/preferences implied by the prompts. Current research addressing this limitation has largely focused on enhancing the prompts before output generation or improving the model's performance up front. In contrast, this paper conceptualizes prompt evolution, imparting evolutionary selection pressure and variation during the generative process to produce multiple outputs that satisfy the target concepts/preferences better. We propose a multi-objective instantiation of this broader idea that uses a multi-label image classifier-guided approach. The predicted labels from the classifiers serve as multiple objectives to optimize, with the aim of producing diversified images that meet user preferences. A novelty of our evolutionary algorithm is that the pre-trained generative model gives us implicit mutation operations, leveraging the model's stochastic generative capability to automate the creation of Pareto-optimized images more faithful to user preferences.\n        \u25b3 Less\n      ",
    "title": "Prompt Evolution for Generative AI: A Classifier-Guided Approach",
    "date": "24 May, 2023",
    "authors": [
      "Melvin Wong",
      " Yew-Soon Ong",
      " Abhishek Gupta",
      " Kavitesh K. Bali",
      " Caishun Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15220",
    "paper_id": "2305.15220",
    "abstract": "\n        Empowerment -- a domain independent, information-theoretic metric -- has previously been shown to assist in the evolutionary search for neural cellular automata (NCA) capable of homeostasis when employed as a fitness function. In our previous study, we successfully extended empowerment, defined as maximum time-lagged mutual information between agents' actions and future sensations, to a distributed sensorimotor system embodied as an NCA. However, the time-delay between actions and their corresponding sensations was arbitrarily chosen. Here, we expand upon previous work by exploring how the time scale at which empowerment operates impacts its efficacy as an auxiliary objective to accelerate the discovery of homeostatic NCAs. We show that shorter time delays result in marked improvements over empowerment with longer delays, when compared to evolutionary selection only for homeostasis. Moreover, we evaluate stability and adaptability of evolved NCAs, both hallmarks of living systems that are of interest to replicate in artificial ones. We find that short-term empowered NCA are more stable and are capable of generalizing better to unseen homeostatic challenges. Taken together, these findings motivate the use of empowerment during the evolution of other artifacts, and suggest how it should be incorporated to accelerate evolution of desired behaviors for them. Source code for the experiments in this paper can be found at: https://github.com/caitlingrasso/empowered-nca-II.\n        \u25b3 Less\n      ",
    "title": "Selection for short-term empowerment accelerates the evolution of homeostatic neural cellular automata",
    "date": "24 May, 2023",
    "authors": [
      "Caitlin Grasso",
      " Josh Bongard"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15222",
    "paper_id": "2305.15222",
    "abstract": "\n        Hospital discharge documentation is among the most essential, yet time-consuming documents written by medical practitioners. The objective of this study was to automatically generate hospital discharge summaries using neural network summarization models. We studied various data preparation and neural network training techniques that generate discharge summaries. Using nursing notes and discharge summaries from the MIMIC-III dataset, we studied the viability of the automatic generation of various sections of a discharge summary using four state-of-the-art neural network summarization models (BART, T5, Longformer and FLAN-T5). Our experiments indicated that training environments including nursing notes as the source, and discrete sections of the discharge summary as the target output (e.g. \"History of Present Illness\") improve language model efficiency and text quality. According to our findings, the fine-tuned BART model improved its ROUGE F1 score by 43.6% against its standard off-the-shelf version. We also found that fine-tuning the baseline BART model with other setups caused different degrees of improvement (up to 80% relative improvement). We also observed that a fine-tuned T5 generally achieves higher ROUGE F1 scores than other fine-tuned models and a fine-tuned FLAN-T5 achieves the highest ROUGE score overall, i.e., 45.6. For majority of the fine-tuned language models, summarizing discharge summary report sections separately outperformed the summarization the entire report quantitatively. On the other hand, fine-tuning language models that were previously instruction fine-tuned showed better performance in summarizing entire reports. This study concludes that a focused dataset designed for the automatic generation of discharge summaries by a language model can produce coherent Discharge Summary sections.\n        \u25b3 Less\n      ",
    "title": "Neural Summarization of Electronic Health Records",
    "date": "24 May, 2023",
    "authors": [
      "Koyena Pal",
      " Seyed Ali Bahrainian",
      " Laura Mercurio",
      " Carsten Eickhoff"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15233",
    "paper_id": "2305.15233",
    "abstract": "\n        Existing cross-lingual transfer (CLT) prompting methods are only concerned with monolingual demonstration examples in the source language. In this paper, we propose In-CLT, a novel cross-lingual transfer prompting method that leverages both source and target languages to construct the demonstration examples. We conduct comprehensive evaluations on multilingual benchmarks, focusing on question answering tasks. Experiment results show that In-CLT prompt not only improves multilingual models' cross-lingual transferability, but also demonstrates remarkable unseen language generalization ability. In-CLT prompting, in particular, improves model performance by 10 to 20\\% points on average when compared to prior cross-lingual transfer approaches. We also observe the surprising performance gain on the other multilingual benchmarks, especially in reasoning tasks. Furthermore, we investigate the relationship between lexical similarity and pre-training corpora in terms of the cross-lingual transfer gap.\n        \u25b3 Less\n      ",
    "title": "Boosting Cross-lingual Transferability in Multilingual Models via In-Context Learning",
    "date": "24 May, 2023",
    "authors": [
      "Sunkyoung Kim",
      " Dayeon Ki",
      " Yireun Kim",
      " Jinsik Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.12899",
    "paper_id": "2302.12899",
    "abstract": "\n        This paper presents a method for optimizing wireless networks by adjusting cell parameters that affect both the performance of the cell being optimized and the surrounding cells. The method uses multiple reinforcement learning agents that share a common policy and take into account information from neighboring cells to determine the state and reward. In order to avoid impairing network performance during the initial stages of learning, agents are pre-trained in an earlier phase of offline learning. During this phase, an initial policy is obtained using feedback from a static network simulator and considering a wide variety of scenarios. Finally, agents can intelligently tune the cell parameters of a test network by suggesting small incremental changes, slowly guiding the network toward an optimal configuration. The agents propose optimal changes using the experience gained with the simulator in the pre-training phase, but they can also continue to learn from current network readings after each change. The results show how the proposed approach significantly improves the performance gains already provided by expert system-based methods when applied to remote antenna tilt optimization. The significant gains of this approach have truly been observed when compared with a similar method in which the state and reward do not incorporate information from neighboring cells.\n        \u25b3 Less\n      ",
    "title": "Multi-Agent Reinforcement Learning with Common Policy for Antenna Tilt Optimization",
    "date": "24 May, 2023",
    "authors": [
      "Adriano Mendo",
      " Jose Outes-Carnero",
      " Yak Ng-Molina",
      " Juan Ramiro-Moreno"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15253",
    "paper_id": "2305.15253",
    "abstract": "\n        Domain generalization aims to solve the challenge of Out-of-Distribution (OOD) generalization by leveraging common knowledge learned from multiple training domains to generalize to unseen test domains. To accurately evaluate the OOD generalization ability, it is necessary to ensure that test data information is unavailable. However, the current domain generalization protocol may still have potential test data information leakage. This paper examines the potential risks of test data information leakage in two aspects of the current protocol: pretraining on ImageNet and oracle model selection. We propose that training from scratch and using multiple test domains would result in a more precise evaluation of OOD generalization ability. We also rerun the algorithms with the modified protocol and introduce a new leaderboard to encourage future research in domain generalization with a fairer comparison.\n        \u25b3 Less\n      ",
    "title": "Rethinking the Evaluation Protocol of Domain Generalization",
    "date": "24 May, 2023",
    "authors": [
      "Han Yu",
      " Xingxuan Zhang",
      " Renzhe Xu",
      " Jiashuo Liu",
      " Yue He",
      " Peng Cui"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15256",
    "paper_id": "2305.15256",
    "abstract": "\n        Discounting is an important dimension in multi-agent systems as long as we want to reason about strategies and time. It is a key aspect in economics as it captures the intuition that the far-away future is not as important as the near future. Traditional verification techniques allow to check whether there is a winning strategy for a group of agents but they do not take into account the fact that satisfying a goal sooner is different from satisfying it after a long wait. In this paper, we augment Strategy Logic with future discounting over a set of discounted functions D, denoted SLdisc[D]. We consider \"until\" operators with discounting functions: the satisfaction value of a specification in SLdisc[D] is a value in [0, 1], where the longer it takes to fulfill requirements, the smaller the satisfaction value is. We motivate our approach with classical examples from Game Theory and study the complexity of model-checking SLdisc[D]-formulas.\n        \u25b3 Less\n      ",
    "title": "Discounting in Strategy Logic",
    "date": "24 May, 2023",
    "authors": [
      "Munyque Mittelmann",
      " Aniello Murano",
      " Laurent Perrussel"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15268",
    "paper_id": "2305.15268",
    "abstract": "\n        Events serve as fundamental units of occurrence within various contexts. The processing of event semantics in textual information forms the basis of numerous natural language processing (NLP) applications. Recent studies have begun leveraging large language models (LLMs) to address event semantic processing. However, the extent that LLMs can effectively tackle these challenges remains uncertain. Furthermore, the lack of a comprehensive evaluation framework for event semantic processing poses a significant challenge in evaluating these capabilities. In this paper, we propose an overarching framework for event semantic processing, encompassing understanding, reasoning, and prediction, along with their fine-grained aspects. To comprehensively evaluate the event semantic processing abilities of models, we introduce a novel benchmark called EVEVAL. We collect 8 datasets that cover all aspects of event semantic processing. Extensive experiments are conducted on EVEVAL, leading to several noteworthy findings based on the obtained results.\n        \u25b3 Less\n      ",
    "title": "EvEval: A Comprehensive Evaluation of Event Semantics for Large Language Models",
    "date": "24 May, 2023",
    "authors": [
      "Zhengwei Tao",
      " Zhi Jin",
      " Xiaoying Bai",
      " Haiyan Zhao",
      " Yanlin Feng",
      " Jia Li",
      " Wenpeng Hu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2202.07255",
    "paper_id": "2202.07255",
    "abstract": "\n        Prompting shows promising results in few-shot scenarios. However, its strength for multilingual/cross-lingual problems has not been fully exploited. Zhao and Sch\u00fctze (2021) made initial explorations in this direction by presenting that cross-lingual prompting outperforms cross-lingual finetuning. In this paper, we conduct an empirical exploration on the effect of each component in cross-lingual prompting and derive language-agnostic Universal Prompting, which helps alleviate the discrepancies between source-language training and target-language inference. Based on this, we propose DPA, a dual prompt augmentation framework, aiming at relieving the data scarcity issue in few-shot cross-lingual prompting. Notably, for XNLI, our method achieves 46.54% with only 16 English training examples per class, significantly better than 34.99% of finetuning. Our code is available at https://github.com/DAMO-NLP-SG/DPA.\n        \u25b3 Less\n      ",
    "title": "Enhancing Cross-lingual Prompting with Dual Prompt Augmentation",
    "date": "24 May, 2023",
    "authors": [
      "Meng Zhou",
      " Xin Li",
      " Yue Jiang",
      " Lidong Bing"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.01427",
    "paper_id": "2211.01427",
    "abstract": "\n        Recent works have demonstrated that natural language can be used to generate and edit 3D shapes. However, these methods generate shapes with limited fidelity and diversity. We introduce CLIP-Sculptor, a method to address these constraints by producing high-fidelity and diverse 3D shapes without the need for (text, shape) pairs during training. CLIP-Sculptor achieves this in a multi-resolution approach that first generates in a low-dimensional latent space and then upscales to a higher resolution for improved shape fidelity. For improved shape diversity, we use a discrete latent space which is modeled using a transformer conditioned on CLIP's image-text embedding space. We also present a novel variant of classifier-free guidance, which improves the accuracy-diversity trade-off. Finally, we perform extensive experiments demonstrating that CLIP-Sculptor outperforms state-of-the-art baselines. The code is available at https://ivl.cs.brown.edu/#/projects/clip-sculptor.\n        \u25b3 Less\n      ",
    "title": "CLIP-Sculptor: Zero-Shot Generation of High-Fidelity and Diverse Shapes from Natural Language",
    "date": "24 May, 2023",
    "authors": [
      "Aditya Sanghi",
      " Rao Fu",
      " Vivian Liu",
      " Karl Willis",
      " Hooman Shayani",
      " Amir Hosein Khasahmadi",
      " Srinath Sridhar",
      " Daniel Ritchie"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2212.02908",
    "paper_id": "2212.02908",
    "abstract": "\n        Autonomous cars are indispensable when humans go further down the hands-free route. Although existing literature highlights that the acceptance of the autonomous car will increase if it drives in a human-like manner, sparse research offers the naturalistic experience from a passenger's seat perspective to examine the humanness of current autonomous cars. The present study tested whether the AI driver could create a human-like ride experience for passengers based on 69 participants' feedback in a real-road scenario. We designed a ride experience-based version of the non-verbal Turing test for automated driving. Participants rode in autonomous cars (driven by either human or AI drivers) as a passenger and judged whether the driver was human or AI. The AI driver failed to pass our test because passengers detected the AI driver above chance. In contrast, when the human driver drove the car, the passengers' judgement was around chance. We further investigated how human passengers ascribe humanness in our test. Based on Lewin's field theory, we advanced a computational model combining signal detection theory with pre-trained language models to predict passengers' humanness rating behaviour. We employed affective transition between pre-study baseline emotions and corresponding post-stage emotions as the signal strength of our model. Results showed that the passengers' ascription of humanness would increase with the greater affective transition. Our study suggested an important role of affective transition in passengers' ascription of humanness, which might become a future direction for autonomous driving.\n        \u25b3 Less\n      ",
    "title": "Towards human-compatible autonomous car: A study of non-verbal Turing test in automated driving with affective transition modelling",
    "date": "24 May, 2023",
    "authors": [
      "Zhaoning Li",
      " Qiaoli Jiang",
      " Zhengming Wu",
      " Anqi Liu",
      " Haiyan Wu",
      " Miner Huang",
      " Kai Huang",
      " Yixuan Ku"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15318",
    "paper_id": "2305.15318",
    "abstract": "\n        A ProbLog program is a logic program with facts that only hold with a specified probability. In this contribution we extend this ProbLog language by the ability to answer \"What if\" queries. Intuitively, a ProbLog program defines a distribution by solving a system of equations in terms of mutually independent predefined Boolean random variables. In the theory of causality, Judea Pearl proposes a counterfactual reasoning for such systems of equations. Based on Pearl's calculus, we provide a procedure for processing these counterfactual queries on ProbLog programs, together with a proof of correctness and a full implementation. Using the latter, we provide insights into the influence of different parameters on the scalability of inference. Finally, we also show that our approach is consistent with CP-logic, i.e. with the causal semantics for logic programs with annotated with disjunctions.\n        \u25b3 Less\n      ",
    "title": "\"What if?\" in Probabilistic Logic Programming",
    "date": "24 May, 2023",
    "authors": [
      "Rafael Kiesel",
      " Kilian R\u00fcckschlo\u00df",
      " Felix Weitk\u00e4mper"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15334",
    "paper_id": "2305.15334",
    "abstract": "\n        Large Language Models (LLMs) have seen an impressive wave of advances recently, with models now excelling in a variety of tasks, such as mathematical reasoning and program synthesis. However, their potential to effectively use tools via API calls remains unfulfilled. This is a challenging task even for today's state-of-the-art LLMs such as GPT-4, largely due to their inability to generate accurate input arguments and their tendency to hallucinate the wrong usage of an API call. We release Gorilla, a finetuned LLaMA-based model that surpasses the performance of GPT-4 on writing API calls. When combined with a document retriever, Gorilla demonstrates a strong capability to adapt to test-time document changes, enabling flexible user updates or version changes. It also substantially mitigates the issue of hallucination, commonly encountered when prompting LLMs directly. To evaluate the model's ability, we introduce APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and TensorHub APIs. The successful integration of the retrieval system with Gorilla demonstrates the potential for LLMs to use tools more accurately, keep up with frequently updated documentation, and consequently increase the reliability and applicability of their outputs. Gorilla's code, model, data, and demo are available at https://gorilla.cs.berkeley.edu\n        \u25b3 Less\n      ",
    "title": "Gorilla: Large Language Model Connected with Massive APIs",
    "date": "24 May, 2023",
    "authors": [
      "Shishir G. Patil",
      " Tianjun Zhang",
      " Xin Wang",
      " Joseph E. Gonzalez"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15338",
    "paper_id": "2305.15338",
    "abstract": "\n        In executable task-oriented semantic parsing, the system aims to translate users' utterances in natural language to machine-interpretable programs (API calls) that can be executed according to pre-defined API specifications. With the popularity of Large Language Models (LLMs), in-context learning offers a strong baseline for such scenarios, especially in data-limited regimes. However, LLMs are known to hallucinate and therefore pose a formidable challenge in constraining generated content. Thus, it remains uncertain if LLMs can effectively perform task-oriented utterance-to-API generation where respecting API's structural and task-specific constraints is crucial.\n  In this work, we seek to measure, analyze and mitigate such constraints violations. First, we identify the categories of various constraints in obtaining API-semantics from task-oriented utterances, and define fine-grained metrics that complement traditional ones. Second, we leverage these metrics to conduct a detailed error analysis of constraints violations seen in state-of-the-art LLMs, which motivates us to investigate two mitigation strategies: Semantic-Retrieval of Demonstrations (SRD) and API-aware Constrained Decoding (API-CD). Our experiments show that these strategies are effective at reducing constraints violations and improving the quality of the generated API calls, but require careful consideration given their implementation complexity and latency.\n        \u25b3 Less\n      ",
    "title": "Measuring and Mitigating Constraint Violations of In-Context Learning for Utterance-to-API Semantic Parsing",
    "date": "24 May, 2023",
    "authors": [
      "Shufan Wang",
      " Sebastien Jean",
      " Sailik Sengupta",
      " James Gung",
      " Nikolaos Pappas",
      " Yi Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15340",
    "paper_id": "2305.15340",
    "abstract": "\n        Agent-based modelling (ABMing) is a powerful and intuitive approach to modelling complex systems; however, the intractability of ABMs' likelihood functions and the non-differentiability of the mathematical operations comprising these models present a challenge to their use in the real world. These difficulties have in turn generated research on approximate Bayesian inference methods for ABMs and on constructing differentiable approximations to arbitrary ABMs, but little work has been directed towards designing approximate Bayesian inference techniques for the specific case of differentiable ABMs. In this work, we aim to address this gap and discuss how generalised variational inference procedures may be employed to provide misspecification-robust Bayesian parameter inferences for differentiable ABMs. We demonstrate with experiments on a differentiable ABM of the COVID-19 pandemic that our approach can result in accurate inferences, and discuss avenues for future work.\n        \u25b3 Less\n      ",
    "title": "Bayesian calibration of differentiable agent-based models",
    "date": "24 May, 2023",
    "authors": [
      "Arnau Quera-Bofarull",
      " Ayush Chopra",
      " Anisoara Calinescu",
      " Michael Wooldridge",
      " Joel Dyer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15348",
    "paper_id": "2305.15348",
    "abstract": "\n        Fine-tuning large-scale Transformers has led to the explosion of many AI applications across Natural Language Processing and Computer Vision tasks. However, fine-tuning all pre-trained model parameters becomes impractical as the model size and number of tasks increase. Parameter-efficient transfer learning (PETL) methods aim to address these challenges. While effective in reducing the number of trainable parameters, PETL methods still require significant energy and computational resources to fine-tune. In this paper, we introduce \\textbf{RE}current \\textbf{AD}aption (READ) -- a lightweight and memory-efficient fine-tuning method -- to overcome the limitations of the current PETL approaches. Specifically, READ inserts a small RNN network alongside the backbone model so that the model does not have to back-propagate through the large backbone network. Through comprehensive empirical evaluation of the GLUE benchmark, we demonstrate READ can achieve a 56\\%56\\% reduction in the training memory consumption and an 84\\%84\\% reduction in the GPU energy usage while retraining high model quality compared to full-tuning. Additionally, the model size of READ does not grow with the backbone model size, making it a highly scalable solution for fine-tuning large Transformers.\n        \u25b3 Less\n      ",
    "title": "READ: Recurrent Adaptation of Large Transformers",
    "date": "24 May, 2023",
    "authors": [
      "Sid Wang",
      " John Nguyen",
      " Ke Li",
      " Carole-Jean Wu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.10621",
    "paper_id": "2205.10621",
    "abstract": "\n        Few-shot relational learning for static knowledge graphs (KGs) has drawn greater interest in recent years, while few-shot learning for temporal knowledge graphs (TKGs) has hardly been studied. Compared to KGs, TKGs contain rich temporal information, thus requiring temporal reasoning techniques for modeling. This poses a greater challenge in learning few-shot relations in the temporal context. In this paper, we follow the previous work that focuses on few-shot relational learning on static KGs and extend two fundamental TKG reasoning tasks, i.e., interpolated and extrapolated link prediction, to the one-shot setting. We propose four new large-scale benchmark datasets and develop a TKG reasoning model for learning one-shot relations in TKGs. Experimental results show that our model can achieve superior performance on all datasets in both TKG link prediction tasks.\n        \u25b3 Less\n      ",
    "title": "Learning Meta Representations of One-shot Relations for Temporal Knowledge Graph Link Prediction",
    "date": "24 May, 2023",
    "authors": [
      "Zifeng Ding",
      " Bailan He",
      " Yunpu Ma",
      " Zhen Han",
      " Volker Tresp"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2310.06998",
    "paper_id": "2310.06998",
    "abstract": "\n        Developments in artificial intelligence (AI) are likely to affect social engineering and change cyber defense operations. The broad and sweeping nature of AI impact means that many aspects of social engineering could be automated, potentially giving adversaries an advantage. In this review, we assess the ways phishing and spear-phishing might be affected by machine learning techniques. By performing a systematic review of demonstrated ML-enabled phishing campaigns, we take a broad survey the space for current developments. We develop a detailed approach for evaluation by creating a risk framework for analyzing and contextualizing these developments. The object of this review is to answer the research questions: (1) Are there high-risk ML-enabled phishing use cases? (2) Is there a meaningful difference between traditional targeted phishing campaigns and ML-enabled phishing campaigns? Practitioners may use this review to inform standards, future research directions, and cyber defense strategies.\n        \u25b3 Less\n      ",
    "title": "A Systematic Review of Machine Learning Enabled Phishing",
    "date": "24 May, 2023",
    "authors": [
      "Krystal A. Jackson"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12820",
    "paper_id": "2305.12820",
    "abstract": "\n        Recent advances in tabular question answering (QA) with large language models are constrained in their coverage and only answer questions over a single table. However, real-world queries are complex in nature, often over multiple tables in a relational database or web page. Single table questions do not involve common table operations such as set operations, Cartesian products (joins), or nested queries. Furthermore, multi-table operations often result in a tabular output, which necessitates table generation capabilities of tabular QA models. To fill this gap, we propose a new task of answering questions over multiple tables. Our model, MultiTabQA, not only answers questions over multiple tables, but also generalizes to generate tabular answers. To enable effective training, we build a pre-training dataset comprising of 132,645 SQL queries and tabular answers. Further, we evaluate the generated tables by introducing table-specific metrics of varying strictness assessing various levels of granularity of the table structure. MultiTabQA outperforms state-of-the-art single table QA models adapted to a multi-table QA setting by finetuning on three datasets: Spider, Atis and GeoQuery.\n        \u25b3 Less\n      ",
    "title": "MultiTabQA: Generating Tabular Answers for Multi-Table Question Answering",
    "date": "24 May, 2023",
    "authors": [
      "Vaishali Pal",
      " Andrew Yates",
      " Evangelos Kanoulas",
      " Maarten de Rijke"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15367",
    "paper_id": "2305.15367",
    "abstract": "\n        Image translation has wide applications, such as style transfer and modality conversion, usually aiming to generate images having both high degrees of realism and faithfulness. These problems remain difficult, especially when it is important to preserve semantic structures. Traditional image-level similarity metrics are of limited use, since the semantics of an image are high-level, and not strongly governed by pixel-wise faithfulness to an original image. Towards filling this gap, we introduce SAMScore, a generic semantic structural similarity metric for evaluating the faithfulness of image translation models. SAMScore is based on the recent high-performance Segment Anything Model (SAM), which can perform semantic similarity comparisons with standout accuracy. We applied SAMScore on 19 image translation tasks, and found that it is able to outperform all other competitive metrics on all of the tasks. We envision that SAMScore will prove to be a valuable tool that will help to drive the vibrant field of image translation, by allowing for more precise evaluations of new and evolving translation models. The code is available at https://github.com/Kent0n-Li/SAMScore.\n        \u25b3 Less\n      ",
    "title": "SAMScore: A Semantic Structural Similarity Metric for Image Translation Evaluation",
    "date": "24 May, 2023",
    "authors": [
      "Yunxiang Li",
      " Meixu Chen",
      " Wenxuan Yang",
      " Kai Wang",
      " Jun Ma",
      " Alan C. Bovik",
      " You Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15454",
    "paper_id": "2305.15454",
    "abstract": "\n        The adoption of data science brings vast benefits to Small and Medium-sized Enterprises (SMEs) including business productivity, economic growth, innovation and jobs creation. Data Science can support SMEs to optimise production processes, anticipate customers' needs, predict machinery failures and deliver efficient smart services. Businesses can also harness the power of Artificial Intelligence (AI) and Big Data and the smart use of digital technologies to enhance productivity and performance, paving the way for innovation. However, integrating data science decisions into an SME requires both skills and IT investments. In most cases, such expenses are beyond the means of SMEs due to limited resources and restricted access to financing. This paper presents trends and challenges towards an effective data-driven decision making for organisations based on a case study of 85 SMEs, mostly from the West Midlands region of England. The work is supported as part of a 3 years ERDF (European Regional Development Funded project) in the areas of big data management, analytics and business intelligence. We present two case studies that demonstrates the potential of Digitisation, AI and Machine Learning and use these as examples to unveil challenges and showcase the wealth of current available opportunities for SMEs.\n        \u25b3 Less\n      ",
    "title": "Trends and Challenges Towards an Effective Data-Driven Decision Making in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs",
    "date": "24 May, 2023",
    "authors": [
      "Abdel-Rahman Tawil",
      " Muhidin Mohamed",
      " Xavier Schmoor",
      " Konstantinos Vlachos",
      " Diana Haidar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15374",
    "paper_id": "2305.15374",
    "abstract": "\n        A plethora of approaches have been proposed for joint entity-relation (ER) extraction. Most of these methods largely depend on a large amount of manually annotated training data. However, manual data annotation is time consuming, labor intensive, and error prone. Human beings learn using both data (through induction) and knowledge (through deduction). Answer Set Programming (ASP) has been a widely utilized approach for knowledge representation and reasoning that is elaboration tolerant and adept at reasoning with incomplete information. This paper proposes a new approach, ASP-enhanced Entity-Relation extraction (ASPER), to jointly recognize entities and relations by learning from both data and domain knowledge. In particular, ASPER takes advantage of the factual knowledge (represented as facts in ASP) and derived knowledge (represented as rules in ASP) in the learning process of neural network models. We have conducted experiments on two real datasets and compare our method with three baselines. The results show that our ASPER model consistently outperforms the baselines.\n        \u25b3 Less\n      ",
    "title": "ASPER: Answer Set Programming Enhanced Neural Network Models for Joint Entity-Relation Extraction",
    "date": "24 May, 2023",
    "authors": [
      "Trung Hoang Le",
      " Huiping Cao",
      " Tran Cao Son"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15382",
    "paper_id": "2305.15382",
    "abstract": "\n        Higher-order logic HOL offers a very simple syntax and semantics for representing and reasoning about typed data structures. But its type system lacks advanced features where types may depend on terms. Dependent type theory offers such a rich type system, but has rather substantial conceptual differences to HOL, as well as comparatively poor proof automation support. We introduce a dependently-typed extension DHOL of HOL that retains the style and conceptual framework of HOL. Moreover, we build a translation from DHOL to HOL and implement it as a preprocessor to a HOL theorem prover, thereby obtaining a theorem prover for DHOL.\n        \u25b3 Less\n      ",
    "title": "Theorem Proving in Dependently-Typed Higher-Order Logic -- Extended Preprint",
    "date": "24 May, 2023",
    "authors": [
      "Colin Rothgang",
      " Florian Rabe",
      " Christoph Benzm\u00fcller"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.01421",
    "paper_id": "2302.01421",
    "abstract": "\n        We propose an algorithm to solve a class of Stackelberg games (possibly with multiple followers) in a follower agnostic manner. Particularly, unlike other contemporary works, our algorithm does not require the use of an oracle estimator for the gradient of the leader's objective or knowledge about the follower's utility function or strategy space. Instead, we design two-loop algorithm where the leader updates its strategies using specially constructed gradient estimator obtained by probing followers with specially designed strategies. Upon receiving the followers engage in an adaptation rule such that the joint strategy of followers converges near equilibrium which is the only information observed by leader to construct the aforementioned gradient estimator. We provide non-asymptotic convergence rates to stationary points of the leader's objective in the absence of convexity of the closed-loop function and further show asymptotic convergence to a local minima of the leader's objective.\n        \u25b3 Less\n      ",
    "title": "Follower Agnostic Methods for Stackelberg Games",
    "date": "24 May, 2023",
    "authors": [
      "Chinmay Maheshwari",
      " S. Shankar Sasty",
      " Lillian Ratliff",
      " Eric Mazumdar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15385",
    "paper_id": "2305.15385",
    "abstract": "\n        The great behavioral heterogeneity observed between individuals with the same psychiatric disorder and even within one individual over time complicates both clinical practice and biomedical research. However, modern technologies are an exciting opportunity to improve behavioral characterization. Existing psychiatry methods that are qualitative or unscalable, such as patient surveys or clinical interviews, can now be collected at a greater capacity and analyzed to produce new quantitative measures. Furthermore, recent capabilities for continuous collection of passive sensor streams, such as phone GPS or smartwatch accelerometer, open avenues of novel questioning that were previously entirely unrealistic. Their temporally dense nature enables a cohesive study of real-time neural and behavioral signals.\n  To develop comprehensive neurobiological models of psychiatric disease, it will be critical to first develop strong methods for behavioral quantification. There is huge potential in what can theoretically be captured by current technologies, but this in itself presents a large computational challenge -- one that will necessitate new data processing tools, new machine learning techniques, and ultimately a shift in how interdisciplinary work is conducted. In my thesis, I detail research projects that take different perspectives on digital psychiatry, subsequently tying ideas together with a concluding discussion on the future of the field. I also provide software infrastructure where relevant, with extensive documentation.\n  Major contributions include scientific arguments and proof of concept results for daily free-form audio journals as an underappreciated psychiatry research datatype, as well as novel stability theorems and pilot empirical success for a proposed multi-area recurrent neural network architecture.\n        \u25b3 Less\n      ",
    "title": "Behavior quantification as the missing link between fields: Tools for digital psychiatry and their role in the future of neurobiology",
    "date": "24 May, 2023",
    "authors": [
      "Michaela Ennis"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15387",
    "paper_id": "2305.15387",
    "abstract": "\n        The integration of multi-document pre-training objectives into language models has resulted in remarkable improvements in multi-document downstream tasks. In this work, we propose extending this idea by pre-training a generic multi-document model from a novel cross-document question answering pre-training objective. To that end, given a set (or cluster) of topically-related documents, we systematically generate semantically-oriented questions from a salient sentence in one document and challenge the model, during pre-training, to answer these questions while \"peeking\" into other topically-related documents. In a similar manner, the model is also challenged to recover the sentence from which the question was generated, again while leveraging cross-document information. This novel multi-document QA formulation directs the model to better recover cross-text informational relations, and introduces a natural augmentation that artificially increases the pre-training data. Further, unlike prior multi-document models that focus on either classification or summarization tasks, our pre-training objective formulation enables the model to perform tasks that involve both short text generation (e.g., QA) and long text generation (e.g., summarization). Following this scheme, we pre-train our model -- termed QAmden -- and evaluate its performance across several multi-document tasks, including multi-document QA, summarization, and query-focused summarization, yielding improvements of up to 7%, and significantly outperforms zero-shot GPT-3.5 and GPT-4.\n        \u25b3 Less\n      ",
    "title": "Peek Across: Improving Multi-Document Modeling via Cross-Document Question-Answering",
    "date": "24 May, 2023",
    "authors": [
      "Avi Caciularu",
      " Matthew E. Peters",
      " Jacob Goldberger",
      " Ido Dagan",
      " Arman Cohan"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.02007",
    "paper_id": "2205.02007",
    "abstract": "\n        We stand at the foot of a significant inflection in the trajectory of scientific discovery. As society continues on its fast-paced digital transformation, so does humankind's collective scientific knowledge and discourse. We now read and write papers in digitized form, and a great deal of the formal and informal processes of science are captured digitally -- including papers, preprints and books, code and datasets, conference presentations, and interactions in social networks and collaboration and communication platforms. The transition has led to the creation and growth of a tremendous amount of information -- much of which is available for public access -- opening exciting opportunities for computational models and systems that analyze and harness it. In parallel, exponential growth in data processing power has fueled remarkable advances in artificial intelligence, including large neural language models capable of learning powerful representations from unstructured text. Dramatic changes in scientific communication -- such as the advent of the first scientific journal in the 17th century -- have historically catalyzed revolutions in scientific thought. The confluence of societal and computational trends suggests that computer science is poised to ignite a revolution in the scientific process itself.\n        \u25b3 Less\n      ",
    "title": "A Computational Inflection for Scientific Discovery",
    "date": "24 May, 2023",
    "authors": [
      "Tom Hope",
      " Doug Downey",
      " Oren Etzioni",
      " Daniel S. Weld",
      " Eric Horvitz"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15498",
    "paper_id": "2305.15498",
    "abstract": "\n        Large language models (LLMs) have shown impressive capabilities in natural language understanding and generation. Their potential for deeper user understanding and improved personalized user experience on recommendation platforms is, however, largely untapped. This paper aims to address this gap. Recommender systems today capture users' interests through encoding their historical activities on the platforms. The generated user representations are hard to examine or interpret. On the other hand, if we were to ask people about interests they pursue in their life, they might talk about their hobbies, like I just started learning the ukulele, or their relaxation routines, e.g., I like to watch Saturday Night Live, or I want to plant a vertical garden. We argue, and demonstrate through extensive experiments, that LLMs as foundation models can reason through user activities, and describe their interests in nuanced and interesting ways, similar to how a human would.\n  We define interest journeys as the persistent and overarching user interests, in other words, the non-transient ones. These are the interests that we believe will benefit most from the nuanced and personalized descriptions. We introduce a framework in which we first perform personalized extraction of interest journeys, and then summarize the extracted journeys via LLMs, using techniques like few-shot prompting, prompt-tuning and fine-tuning. Together, our results in prompting LLMs to name extracted user journeys in a large-scale industrial platform demonstrate great potential of these models in providing deeper, more interpretable, and controllable user understanding. We believe LLM powered user understanding can be a stepping stone to entirely new user experiences on recommendation platforms that are journey-aware, assistive, and enabling frictionless conversation down the line.\n        \u25b3 Less\n      ",
    "title": "Large Language Models for User Interest Journeys",
    "date": "24 May, 2023",
    "authors": [
      "Konstantina Christakopoulou",
      " Alberto Lalama",
      " Cj Adams",
      " Iris Qu",
      " Yifat Amir",
      " Samer Chucri",
      " Pierce Vollucci",
      " Fabio Soldo",
      " Dina Bseiso",
      " Sarah Scodel",
      " Lucas Dixon",
      " Ed H. Chi",
      " Minmin Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15507",
    "paper_id": "2305.15507",
    "abstract": "\n        Large Language Models (LLMs) have successfully been applied to code generation tasks, raising the question of how well these models understand programming. Typical programming languages have invariances and equivariances in their semantics that human programmers intuitively understand and exploit, such as the (near) invariance to the renaming of identifiers. We show that LLMs not only fail to properly generate correct Python code when default function names are swapped, but some of them even become more confident in their incorrect predictions as the model size increases, an instance of the recently discovered phenomenon of Inverse Scaling, which runs contrary to the commonly observed trend of increasing prediction quality with increasing model size. Our findings indicate that, despite their astonishing typical-case performance, LLMs still lack a deep, abstract understanding of the content they manipulate, making them unsuitable for tasks that statistically deviate from their training data, and that mere scaling is not enough to achieve such capability.\n        \u25b3 Less\n      ",
    "title": "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python",
    "date": "24 May, 2023",
    "authors": [
      "Antonio Valerio Miceli-Barone",
      " Fazl Barez",
      " Ioannis Konstas",
      " Shay B. Cohen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16349",
    "paper_id": "2305.16349",
    "abstract": "\n        Token embeddings, a mapping from discrete lexical symbols to continuous vectors, are at the heart of any language model (LM). However, lexical symbol meanings can also be determined and even redefined by their structural role in a long context. In this paper, we ask: is it possible for a language model to be performant without \\emph{any} fixed token embeddings? Such a language model would have to rely entirely on the co-occurence and repetition of tokens in the context rather than the \\textit{a priori} identity of any token. To answer this, we study \\textit{lexinvariant}language models that are invariant to lexical symbols and therefore do not need fixed token embeddings in practice. First, we prove that we can construct a lexinvariant LM to converge to the true language model at a uniform rate that is polynomial in terms of the context length, with a constant factor that is sublinear in the vocabulary size. Second, to build a lexinvariant LM, we simply encode tokens using random Gaussian vectors, such that each token maps to the same representation within each sequence but different representations across sequences. Empirically, we demonstrate that it can indeed attain perplexity comparable to that of a standard language model, given a sufficiently long context. We further explore two properties of the lexinvariant language models: First, given text generated from a substitution cipher of English, it implicitly implements Bayesian in-context deciphering and infers the mapping to the underlying real tokens with high accuracy. Second, it has on average 4X better accuracy over synthetic in-context reasoning tasks. Finally, we discuss regularizing standard language models towards lexinvariance and potential practical applications.\n        \u25b3 Less\n      ",
    "title": "Lexinvariant Language Models",
    "date": "24 May, 2023",
    "authors": [
      "Qian Huang",
      " Eric Zelikman",
      " Sarah Li Chen",
      " Yuhuai Wu",
      " Gregory Valiant",
      " Percy Liang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15535",
    "paper_id": "2305.15535",
    "abstract": "\n        Humans have developed the capability to teach relevant aspects of new or adapted tasks to a social peer with very few task demonstrations by making use of scaffolding strategies that leverage prior knowledge and importantly prior joint experience to yield a joint understanding and a joint execution of the required steps to solve the task. This process has been discovered and analyzed in parent-infant interaction and constitutes a ``co-construction'' as it allows both, the teacher and the learner, to jointly contribute to the task. We propose to focus research in robot interactive learning on this co-construction process to enable robots to learn from non-expert users in everyday situations. In the following, we will review current proposals for interactive task learning and discuss their main contributions with respect to the entailing interaction. We then discuss our notion of co-construction and summarize research insights from adult-child and human-robot interactions to elucidate its nature in more detail. From this overview we finally derive research desiderata that entail the dimensions architecture, representation, interaction and explainability.\n        \u25b3 Less\n      ",
    "title": "From Interactive to Co-Constructive Task Learning",
    "date": "24 May, 2023",
    "authors": [
      "Anna-Lisa Vollmer",
      " Daniel Leidner",
      " Michael Beetz",
      " Britta Wrede"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15541",
    "paper_id": "2305.15541",
    "abstract": "\n        Translating natural language sentences to first-order logic (NL-FOL translation) is a longstanding challenge in the NLP and formal logic literature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned for NL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable of directly translating natural language into FOL rules, which outperforms GPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5, and can achieve similar performance as GPT-4 with a fraction of the cost. This correction ability was achieved by a novel supervised fine-tuning (SFT) + reinforcement learning with human feedback (RLHF) framework, which initially trains on synthetically perturbed NL-FOL pairs to encourage chain-of-thought reasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifier as the reward model.\n  To train LogicLLaMA, we present MALLS (large language M\\textbf{M}odel generA\\textbf{A}ted NL\\textbf{L}-FOL\\textbf{L} pairS\\textbf{S}), a dataset of 34K high-quality and diverse sentence-level NL-FOL pairs collected from GPT-4. The dataset was created by implementing a pipeline that prompts GPT-4 for pairs, and dynamically adjusts the prompts to ensure the collection of pairs with rich and diverse contexts at different levels of complexity, and verifies the validity of the generated FOL rules. Codes, weights, and data are available at https://github.com/gblackout/LogicLLaMA\\href{https://github.com/gblackout/LogicLLaMA}{\\small \\text{https://github.com/gblackout/LogicLLaMA}}.\n        \u25b3 Less\n      ",
    "title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation",
    "date": "24 May, 2023",
    "authors": [
      "Yuan Yang",
      " Siheng Xiong",
      " Ali Payani",
      " Ehsan Shareghi",
      " Faramarz Fekri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13938",
    "paper_id": "2305.13938",
    "abstract": "\n        Concerns regarding unfairness and discrimination in the context of artificial intelligence (AI) systems have recently received increased attention from both legal and computer science scholars. Yet, the degree of overlap between notions of algorithmic bias and fairness on the one hand, and legal notions of discrimination and equality on the other, is often unclear, leading to misunderstandings between computer science and law. What types of bias and unfairness does the law address when it prohibits discrimination? What role can fairness metrics play in establishing legal compliance? In this paper, we aim to illustrate to what extent European Union (EU) non-discrimination law coincides with notions of algorithmic fairness proposed in computer science literature and where they differ. The contributions of this paper are as follows. First, we analyse seminal examples of algorithmic unfairness through the lens of EU non-discrimination law, drawing parallels with EU case law. Second, we set out the normative underpinnings of fairness metrics and technical interventions and compare these to the legal reasoning of the Court of Justice of the EU. Specifically, we show how normative assumptions often remain implicit in both disciplinary approaches and explain the ensuing limitations of current AI practice and non-discrimination law. We conclude with implications for AI practitioners and regulators.\n        \u25b3 Less\n      ",
    "title": "Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree",
    "date": "24 May, 2023",
    "authors": [
      "Hilde Weerts",
      " Rapha\u00eble Xenidis",
      " Fabien Tarissan",
      " Henrik Palmer Olsen",
      " Mykola Pechenizkiy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2206.00128",
    "paper_id": "2206.00128",
    "abstract": "\n        Tree ensembles are powerful models that achieve excellent predictive performances, but can grow to unwieldy sizes. These ensembles are often post-processed (pruned) to reduce memory footprint and improve interpretability. We present ForestPrune, a novel optimization framework to post-process tree ensembles by pruning depth layers from individual trees. Since the number of nodes in a decision tree increases exponentially with tree depth, pruning deep trees drastically compactifies ensembles. We develop a specialized optimization algorithm to efficiently obtain high-quality solutions to problems under ForestPrune. Our algorithm typically reaches good solutions in seconds for medium-size datasets and ensembles, with 10000s of rows and 100s of trees, resulting in significant speedups over existing approaches. Our experiments demonstrate that ForestPrune produces parsimonious models that outperform models extracted by existing post-processing algorithms.\n        \u25b3 Less\n      ",
    "title": "ForestPrune: Compact Depth-Controlled Tree Ensembles",
    "date": "24 May, 2023",
    "authors": [
      "Brian Liu",
      " Rahul Mazumder"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2204.03140",
    "paper_id": "2204.03140",
    "abstract": "\n        Autonomous exploration has many important applications. However, classic information gain-based or frontier-based exploration only relies on the robot current state to determine the immediate exploration goal, which lacks the capability of predicting the value of future states and thus leads to inefficient exploration decisions. This paper presents a method to learn how \"good\" states are, measured by the state value function, to provide a guidance for robot exploration in real-world challenging environments. We formulate our work as an off-policy evaluation (OPE) problem for robot exploration (OPERE). It consists of offline Monte-Carlo training on real-world data and performs Temporal Difference (TD) online adaptation to optimize the trained value estimator. We also design an intrinsic reward function based on sensor information coverage to enable the robot to gain more information with sparse extrinsic rewards. Results show that our method enables the robot to predict the value of future states so as to better guide robot exploration. The proposed algorithm achieves better prediction and exploration performance compared with the state-of-the-arts. To the best of our knowledge, this work for the first time demonstrates value function prediction on real-world dataset for robot exploration in challenging subterranean and urban environments. More details and demo videos can be found at https://jeffreyyh.github.io/opere/.\n        \u25b3 Less\n      ",
    "title": "Off-Policy Evaluation with Online Adaptation for Robot Exploration in Challenging Environments",
    "date": "24 May, 2023",
    "authors": [
      "Yafei Hu",
      " Junyi Geng",
      " Chen Wang",
      " John Keller",
      " Sebastian Scherer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2207.06983",
    "paper_id": "2207.06983",
    "abstract": "\n        Existing approaches for generating multitrack music with transformer models have been limited in terms of the number of instruments, the length of the music segments and slow inference. This is partly due to the memory requirements of the lengthy input sequences necessitated by existing representations. In this work, we propose a new multitrack music representation that allows a diverse set of instruments while keeping a short sequence length. Our proposed Multitrack Music Transformer (MMT) achieves comparable performance with state-of-the-art systems, landing in between two recently proposed models in a subjective listening test, while achieving substantial speedups and memory reductions over both, making the method attractive for real time improvisation or near real time creative applications. Further, we propose a new measure for analyzing musical self-attention and show that the trained model attends more to notes that form a consonant interval with the current note and to notes that are 4N beats away from the current step.\n        \u25b3 Less\n      ",
    "title": "Multitrack Music Transformer",
    "date": "24 May, 2023",
    "authors": [
      "Hao-Wen Dong",
      " Ke Chen",
      " Shlomo Dubnov",
      " Julian McAuley",
      " Taylor Berg-Kirkpatrick"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.01157",
    "paper_id": "2305.01157",
    "abstract": "\n        Reasoning over knowledge graphs (KGs) is a challenging task that requires a deep understanding of the complex relationships between entities and the underlying logic of their relations. Current approaches rely on learning geometries to embed entities in vector space for logical query operations, but they suffer from subpar performance on complex queries and dataset-specific representations. In this paper, we propose a novel decoupled approach, Language-guided Abstract Reasoning over Knowledge graphs (LARK), that formulates complex KG reasoning as a combination of contextual KG search and logical query reasoning, to leverage the strengths of graph extraction algorithms and large language models (LLM), respectively. Our experiments demonstrate that the proposed approach outperforms state-of-the-art KG reasoning methods on standard benchmark datasets across several logical query constructs, with significant performance gain for queries of higher complexity. Furthermore, we show that the performance of our approach improves proportionally to the increase in size of the underlying LLM, enabling the integration of the latest advancements in LLMs for logical reasoning over KGs. Our work presents a new direction for addressing the challenges of complex KG reasoning and paves the way for future research in this area.\n        \u25b3 Less\n      ",
    "title": "Complex Logical Reasoning over Knowledge Graphs using Large Language Models",
    "date": "24 May, 2023",
    "authors": [
      "Nurendra Choudhary",
      " Chandan K. Reddy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16351",
    "paper_id": "2305.16351",
    "abstract": "\n        Federated learning provides a promising privacy-preserving way for utilizing large-scale private edge data from massive Internet-of-Things (IoT) devices. While existing research extensively studied optimizing the learning process, computing efficiency, and communication overhead, one important and often overlooked aspect is that participants contribute predictive knowledge from their data, impacting the quality of the federated models learned. While FedAvg treats each client equally and assigns weight solely based on the number of samples, the diversity of samples on each client could greatly affect the local update performance and the final aggregated model. In this paper, we propose a novel approach to address this issue by introducing a Weighted Averaging (WeiAvg) framework that emphasizes updates from high-diversity clients and diminishes the influence of those from low-diversity clients. Specifically, we introduced a projection-based approximation method to estimate the diversity of client data, instead of the computation of an entropy. We use the approximation because the locally computed entropy may not be transmitted due to excess privacy risk. Extensive experimental results show that WeiAvg converges faster and achieves higher accuracy than the original FedAvg algorithm and FedProx.\n        \u25b3 Less\n      ",
    "title": "WeiAvg: Federated Learning Model Aggregation Promoting Data Diversity",
    "date": "24 May, 2023",
    "authors": [
      "Fan Dong",
      " Ali Abbasi",
      " Steve Drew",
      " Henry Leung",
      " Xin Wang",
      " Jiayu Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15587",
    "paper_id": "2305.15587",
    "abstract": "\n        Natural Language Processing (NLP) models based on Machine Learning (ML) are susceptible to adversarial attacks -- malicious algorithms that imperceptibly modify input text to force models into making incorrect predictions. However, evaluations of these attacks ignore the property of imperceptibility or study it under limited settings. This entails that adversarial perturbations would not pass any human quality gate and do not represent real threats to human-checked NLP systems. To bypass this limitation and enable proper assessment (and later, improvement) of NLP model robustness, we have surveyed 378 human participants about the perceptibility of text adversarial examples produced by state-of-the-art methods. Our results underline that existing text attacks are impractical in real-world scenarios where humans are involved. This contrasts with previous smaller-scale human studies, which reported overly optimistic conclusions regarding attack success. Through our work, we hope to position human perceptibility as a first-class success criterion for text attacks, and provide guidance for research to build effective attack algorithms and, in turn, design appropriate defence mechanisms.\n        \u25b3 Less\n      ",
    "title": "How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks",
    "date": "24 May, 2023",
    "authors": [
      "Salijona Dyrmishi",
      " Salah Ghamizi",
      " Maxime Cordy"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.12219",
    "paper_id": "2305.12219",
    "abstract": "\n        Despite substantial advancements, Natural Language Processing (NLP) models often require post-training adjustments to enforce business rules, rectify undesired behavior, and align with user values. These adjustments involve operationalizing \"concepts\"--dictating desired model responses to certain inputs. However, it's difficult for a single entity to enumerate and define all possible concepts, indicating a need for a multi-user, collaborative model alignment framework. Moreover, the exhaustive delineation of a concept is challenging, and an improper approach can create shortcuts or interfere with original data or other concepts.\n  To address these challenges, we introduce CoDev, a framework that enables multi-user interaction with the model, thereby mitigating individual limitations. CoDev aids users in operationalizing their concepts using Large Language Models, and relying on the principle that NLP models exhibit simpler behaviors in local regions. Our main insight is learning a \\emph{local} model for each concept, and a \\emph{global} model to integrate the original data with all concepts. We then steer a large language model to generate instances within concept boundaries where local and global disagree. Our experiments show CoDev is effective at helping multiple users operationalize concepts and avoid interference for a variety of scenarios, tasks, and models.\n        \u25b3 Less\n      ",
    "title": "Collaborative Development of NLP models",
    "date": "24 May, 2023",
    "authors": [
      "Fereshte Khani",
      " Marco Tulio Ribeiro"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15597",
    "paper_id": "2305.15597",
    "abstract": "\n        The mission of open knowledge graph (KG) completion is to draw new findings from known facts. Existing works that augment KG completion require either (1) factual triples to enlarge the graph reasoning space or (2) manually designed prompts to extract knowledge from a pre-trained language model (PLM), exhibiting limited performance and requiring expensive efforts from experts. To this end, we propose TAGREAL that automatically generates quality query prompts and retrieves support information from large text corpora to probe knowledge from PLM for KG completion. The results show that TAGREAL achieves state-of-the-art performance on two benchmark datasets. We find that TAGREAL has superb performance even with limited training data, outperforming existing embedding-based, graph-based, and PLM-based methods.\n        \u25b3 Less\n      ",
    "title": "Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models",
    "date": "24 May, 2023",
    "authors": [
      "Pengcheng Jiang",
      " Shivam Agarwal",
      " Bowen Jin",
      " Xuan Wang",
      " Jimeng Sun",
      " Jiawei Han"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.03025",
    "paper_id": "2302.03025",
    "abstract": "\n        Universality is a key hypothesis in mechanistic interpretability -- that different models learn similar features and circuits when trained on similar tasks. In this work, we study the universality hypothesis by examining how small neural networks learn to implement group composition. We present a novel algorithm by which neural networks may implement composition for any finite group via mathematical representation theory. We then show that networks consistently learn this algorithm by reverse engineering model logits and weights, and confirm our understanding using ablations. By studying networks of differing architectures trained on various groups, we find mixed evidence for universality: using our algorithm, we can completely characterize the family of circuits and features that networks learn on this task, but for a given network the precise circuits learned -- as well as the order they develop -- are arbitrary.\n        \u25b3 Less\n      ",
    "title": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations",
    "date": "24 May, 2023",
    "authors": [
      "Bilal Chughtai",
      " Lawrence Chan",
      " Neel Nanda"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15602",
    "paper_id": "2305.15602",
    "abstract": "\n        Reinforcement learning (RL) is an area of significant research interest, and safe RL in particular is attracting attention due to its ability to handle safety-driven constraints that are crucial for real-world applications. This work proposes a novel approach to RL training, called control invariant set (CIS) enhanced RL, which leverages the advantages of utilizing the explicit form of CIS to improve stability guarantees and sampling efficiency. Furthermore, the robustness of the proposed approach is investigated in the presence of uncertainty. The approach consists of two learning stages: offline and online. In the offline stage, CIS is incorporated into the reward design, initial state sampling, and state reset procedures. This incorporation of CIS facilitates improved sampling efficiency during the offline training process. In the online stage, RL is retrained whenever the predicted next step state is outside of the CIS, which serves as a stability criterion, by introducing a Safety Supervisor to examine the safety of the action and make necessary corrections. The stability analysis is conducted for both cases, with and without uncertainty. To evaluate the proposed approach, we apply it to a simulated chemical reactor. The results show a significant improvement in sampling efficiency during offline training and closed-loop stability guarantee in the online implementation, with and without uncertainty.\n        \u25b3 Less\n      ",
    "title": "Control invariant set enhanced safe reinforcement learning: improved sampling efficiency, guaranteed stability and robustness",
    "date": "24 May, 2023",
    "authors": [
      "Song Bo",
      " Bernard T. Agyeman",
      " Xunyuan Yin",
      " Jinfeng Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15608",
    "paper_id": "2305.15608",
    "abstract": "\n        Semantic segmentation is a critical task in computer vision that aims to identify and classify individual pixels in an image, with numerous applications for example autonomous driving and medical image analysis. However, semantic segmentation can be super challenging particularly due to the need for large amounts of annotated data. Annotating images is a time-consuming and costly process, often requiring expert knowledge and significant effort. In this paper, we propose a novel approach for semantic segmentation by eliminating the need of ground-truth segmentation maps. Instead, our approach requires only the rough information of individual semantic class proportions, shortened as semantic proportions. It greatly simplifies the data annotation process and thus will significantly reduce the annotation time and cost, making it more feasible for large-scale applications. Moreover, it opens up new possibilities for semantic segmentation tasks where obtaining the full ground-truth segmentation maps may not be feasible or practical. Extensive experimental results demonstrate that our approach can achieve comparable and sometimes even better performance against the benchmark method that relies on the ground-truth segmentation maps. Utilising semantic proportions suggested in this work offers a promising direction for future research in the field of semantic segmentation.\n        \u25b3 Less\n      ",
    "title": "Semantic Segmentation by Semantic Proportions",
    "date": "24 May, 2023",
    "authors": [
      "Halil Ibrahim Aysel",
      " Xiaohao Cai",
      " Adam Pr\u00fcgel-Bennett"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.07759",
    "paper_id": "2305.07759",
    "abstract": "\n        Language models (LMs) are powerful tools for natural language processing, but they often struggle to produce coherent and fluent text when they are small. Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) can rarely generate coherent and consistent English text beyond a few words even after extensive training. This raises the question of whether the emergence of the ability to produce coherent English text only occurs at larger scales (with hundreds of millions of parameters or more) and complex architectures (with many layers of global attention).\n  In this work, we introduce TinyStories, a synthetic dataset of short stories that only contain words that a typical 3 to 4-year-olds usually understand, generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to train and evaluate LMs that are much smaller than the state-of-the-art models (below 10 million total parameters), or have much simpler architectures (with only one transformer block), yet still produce fluent and consistent stories with several paragraphs that are diverse and have almost perfect grammar, and demonstrate reasoning capabilities.\n  We also introduce a new paradigm for the evaluation of language models: We suggest a framework which uses GPT-4 to grade the content generated by these models as if those were stories written by students and graded by a (human) teacher. This new paradigm overcomes the flaws of standard benchmarks which often requires the model's output to be very structures, and moreover provides a multidimensional score for the model, providing scores for different capabilities such as grammar, creativity and consistency.\n  We hope that TinyStories can facilitate the development, analysis and research of LMs, especially for low-resource or specialized domains, and shed light on the emergence of language capabilities in LMs.\n        \u25b3 Less\n      ",
    "title": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?",
    "date": "24 May, 2023",
    "authors": [
      "Ronen Eldan",
      " Yuanzhi Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2203.07648",
    "paper_id": "2203.07648",
    "abstract": "\n        Recent progress in representation and contrastive learning in NLP has not widely considered the class of \\textit{sociopragmatic meaning} (i.e., meaning in interaction within different language communities). To bridge this gap, we propose a novel framework for learning task-agnostic representations transferable to a wide range of sociopragmatic tasks (e.g., emotion, hate speech, humor, sarcasm). Our framework outperforms other contrastive learning frameworks for both in-domain and out-of-domain data, across both the general and few-shot settings. For example, compared to two popular pre-trained language models, our method obtains an improvement of 11.6611.66 average F1F_1 on 1616 datasets when fine-tuned on only 2020 training samples per dataset.Our code is available at: https://github.com/UBC-NLP/infodcl\n        \u25b3 Less\n      ",
    "title": "Contrastive Learning of Sociopragmatic Meaning in Social Media",
    "date": "24 May, 2023",
    "authors": [
      "Chiyu Zhang",
      " Muhammad Abdul-Mageed",
      " Ganesh Jawahar"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.09091",
    "paper_id": "2305.09091",
    "abstract": "\n        Attempts to import dual-system descriptions of System-1 and System-2 into AI have been hindered by a lack of clarity over their distinction. We address this and other issues by situating System-1 and System-2 within the Common Model of Cognition. Results show that what are thought to be distinctive characteristics of System-1 and 2 instead form a spectrum of cognitive properties. The Common Model provides a comprehensive vision of the computational units involved in System-1 and System-2, their underlying mechanisms, and the implications for learning, metacognition, and emotion.\n        \u25b3 Less\n      ",
    "title": "AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition",
    "date": "24 May, 2023",
    "authors": [
      "Brendan Conway-Smith",
      " Robert L. West"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15629",
    "paper_id": "2305.15629",
    "abstract": "\n        Problem definition: Access to accurate predictions of patients' outcomes can enhance medical staff's decision-making, which ultimately benefits all stakeholders in the hospitals. A large hospital network in the US has been collaborating with academics and consultants to predict short-term and long-term outcomes for all inpatients across their seven hospitals. Methodology/results: We develop machine learning models that predict the probabilities of next 24-hr/48-hr discharge and intensive care unit transfers, end-of-stay mortality and discharge dispositions. All models achieve high out-of-sample AUC (75.7%-92.5%) and are well calibrated. In addition, combining 48-hr discharge predictions with doctors' predictions simultaneously enables more patient discharges (10%-28.7%) and fewer 7-day/30-day readmissions (pp-value <0.001<0.001). We implement an automated pipeline that extracts data and updates predictions every morning, as well as user-friendly software and a color-coded alert system to communicate these patient-level predictions (alongside explanations) to clinical teams. Managerial implications: Since we have been gradually deploying the tool, and training medical staff, over 200 doctors, nurses, and case managers across seven hospitals use it in their daily patient review process. We observe a significant reduction in the average length of stay (0.67 days per patient) following its adoption and anticipate substantial financial benefits (between $55 and $72 million annually) for the healthcare system.\n        \u25b3 Less\n      ",
    "title": "Patient Outcome Predictions Improve Operations at a Large Hospital Network",
    "date": "24 May, 2023",
    "authors": [
      "Liangyuan Na",
      " Kimberly Villalobos Carballo",
      " Jean Pauphilet",
      " Ali Haddad-Sisakht",
      " Daniel Kombert",
      " Melissa Boisjoli-Langlois",
      " Andrew Castiglione",
      " Maram Khalifa",
      " Pooja Hebbal",
      " Barry Stein",
      " Dimitris Bertsimas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2209.12016",
    "paper_id": "2209.12016",
    "abstract": "\n        Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-scale empirical study, which we use to validate our design choices and analyze our models. We also show robust performance on the Real-Word RL benchmark, hinting at resiliency to environment perturbations during adaptation. Project website: https://masteringurlb.github.io/\n        \u25b3 Less\n      ",
    "title": "Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels",
    "date": "24 May, 2023",
    "authors": [
      "Sai Rajeswar",
      " Pietro Mazzaglia",
      " Tim Verbelen",
      " Alexandre Pich\u00e9",
      " Bart Dhoedt",
      " Aaron Courville",
      " Alexandre Lacoste"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15644",
    "paper_id": "2305.15644",
    "abstract": "\n        To ensure the out-of-distribution (OOD) generalization performance, traditional domain generalization (DG) methods resort to training on data from multiple sources with different underlying distributions. And the success of those DG methods largely depends on the fact that there are diverse training distributions. However, it usually needs great efforts to obtain enough heterogeneous data due to the high expenses, privacy issues or the scarcity of data. Thus an interesting yet seldom investigated problem arises: how to improve the OOD generalization performance when the perceived heterogeneity is limited. In this paper, we instantiate a new framework called few-domain generalization (FDG), which aims to learn a generalizable model from very few domains of novel tasks with the knowledge acquired from previous learning experiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling (MATS) procedure to differentiate base tasks according to their semantic and domain-shift similarity to the novel task. Empirically, we show that the newly introduced FDG framework can substantially improve the OOD generalization performance on the novel task and further combining MATS with episodic training could outperform several state-of-the-art DG baselines on widely used benchmarks like PACS and DomainNet.\n        \u25b3 Less\n      ",
    "title": "Meta Adaptive Task Sampling for Few-Domain Generalization",
    "date": "24 May, 2023",
    "authors": [
      "Zheyan Shen",
      " Han Yu",
      " Peng Cui",
      " Jiashuo Liu",
      " Xingxuan Zhang",
      " Linjun Zhou",
      " Furui Liu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15652",
    "paper_id": "2305.15652",
    "abstract": "\n        Although existing image anomaly detection methods yield impressive results, they are mostly an offline learning paradigm that requires excessive data pre-collection, limiting their adaptability in industrial scenarios with online streaming data. Online learning-based image anomaly detection methods are more compatible with industrial online streaming data but are rarely noticed. For the first time, this paper presents a fully online learning image anomaly detection method, namely LeMO, learning memory for online image anomaly detection. LeMO leverages learnable memory initialized with orthogonal random noise, eliminating the need for excessive data in memory initialization and circumventing the inefficiencies of offline data collection. Moreover, a contrastive learning-based loss function for anomaly detection is designed to enable online joint optimization of memory and image target-oriented features. The presented method is simple and highly effective. Extensive experiments demonstrate the superior performance of LeMO in the online setting. Additionally, in the offline setting, LeMO is also competitive with the current state-of-the-art methods and achieves excellent performance in few-shot scenarios.\n        \u25b3 Less\n      ",
    "title": "Towards Total Online Unsupervised Anomaly Detection and Localization in Industrial Vision",
    "date": "24 May, 2023",
    "authors": [
      "Han Gao",
      " Huiyuan Luo",
      " Fei Shen",
      " Zhengtao Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.18617",
    "paper_id": "2305.18617",
    "abstract": "\n        Generative AI tools such as ChatGPT have recently gained significant attention in higher education. This study aims to understand how universities establish policies regarding the use of AI tools and explore the factors that influence their decisions. Our study examines ChatGPT policies implemented at universities around the world, including their existence, content, and issuance dates. Specifically, we analyzed the top 500 universities according to the 2022 QS World University Rankings. Our findings indicate that there is significant variation in university policies. Less than one-third of the universities included in the study had implemented ChatGPT policies. Of the universities with ChatGPT policies, approximately 67 percent embraced ChatGPT in teaching and learning, more than twice the number of universities that banned it. The majority of the universities that ban the use of ChatGPT in assessments allow individual instructors to deviate from this restrictive policy. Our empirical analysis identifies several factors that are significantly and positively correlated with a university's likelihood of having a ChatGPT policy, including the university's academic reputation score, being in an English-speaking country, and the general public attitudes toward ChatGPT. In addition, we found that a university's likelihood of having a ban policy is positively associated with faculty student ratio, citations, and the English-speaking country dummy, while negatively associated with the number of peer universities within the same country that have banned ChatGPT. We discuss the challenges faced by universities based our empirical findings.\n        \u25b3 Less\n      ",
    "title": "Waiting, Banning, and Embracing: An Empirical Analysis of Adapting Policies for Generative AI in Higher Education",
    "date": "24 May, 2023",
    "authors": [
      "Ping Xiao",
      " Yuanyuan Chen",
      " Weining Bao"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2211.05985",
    "paper_id": "2211.05985",
    "abstract": "\n        The spread of misinformation is a prominent problem in today's society, and many researchers in academia and industry are trying to combat it. Due to the vast amount of misinformation that is created every day, it is unrealistic to leave this task to human fact-checkers. Data scientists and researchers have been working on automated misinformation detection for years, and it is still a challenging problem today. The goal of our research is to add a new level to automated misinformation detection; classifying segments of text with persuasive writing techniques in order to produce interpretable reasoning for why an article can be marked as misinformation. To accomplish this, we present a novel annotation scheme containing many common persuasive writing tactics, along with a dataset with human annotations accordingly. For this task, we make use of a RoBERTa model for text classification, due to its high performance in NLP. We develop several language model-based baselines and present the results of our persuasive strategy label predictions as well as the improvements these intermediate labels make in detecting misinformation and producing interpretable results.\n        \u25b3 Less\n      ",
    "title": "Using Persuasive Writing Strategies to Explain and Detect Health Misinformation",
    "date": "24 May, 2023",
    "authors": [
      "Danial Kamali",
      " Joseph Romain",
      " Huiyi Liu",
      " Wei Peng",
      " Jingbo Meng",
      " Parisa Kordjamshidi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15669",
    "paper_id": "2305.15669",
    "abstract": "\n        Offline-to-online reinforcement learning (RL), by combining the benefits of offline pretraining and online finetuning, promises enhanced sample efficiency and policy performance. However, existing methods, effective as they are, suffer from suboptimal performance, limited adaptability, and unsatisfactory computational efficiency. We propose a novel framework, PROTO, which overcomes the aforementioned limitations by augmenting the standard RL objective with an iteratively evolving regularization term. Performing a trust-region-style update, PROTO yields stable initial finetuning and optimal final performance by gradually evolving the regularization term to relax the constraint strength. By adjusting only a few lines of code, PROTO can bridge any offline policy pretraining and standard off-policy RL finetuning to form a powerful offline-to-online RL pathway, birthing great adaptability to diverse methods. Simple yet elegant, PROTO imposes minimal additional computation and enables highly efficient online finetuning. Extensive experiments demonstrate that PROTO achieves superior performance over SOTA baselines, offering an adaptable and efficient offline-to-online RL framework.\n        \u25b3 Less\n      ",
    "title": "PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning",
    "date": "24 May, 2023",
    "authors": [
      "Jianxiong Li",
      " Xiao Hu",
      " Haoran Xu",
      " Jingjing Liu",
      " Xianyuan Zhan",
      " Ya-Qin Zhang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16353",
    "paper_id": "2305.16353",
    "abstract": "\n        Audio Deepfake Detection (ADD) aims to detect the fake audio generated by text-to-speech (TTS), voice conversion (VC) and replay, etc., which is an emerging topic. Traditionally we take the mono signal as input and focus on robust feature extraction and effective classifier design. However, the dual-channel stereo information in the audio signal also includes important cues for deepfake, which has not been studied in the prior work. In this paper, we propose a novel ADD model, termed as M2S-ADD, that attempts to discover audio authenticity cues during the mono-to-stereo conversion process. We first projects the mono to a stereo signal using a pretrained stereo synthesizer, then employs a dual-branch neural architecture to process the left and right channel signals, respectively. In this way, we effectively reveal the artifacts in the fake audio, thus improve the ADD performance. The experiments on the ASVspoof2019 database show that M2S-ADD outperforms all baselines that input mono. We release the source code at \\url{https://github.com/AI-S2-Lab/M2S-ADD}.\n        \u25b3 Less\n      ",
    "title": "Betray Oneself: A Novel Audio DeepFake Detection Model via Mono-to-Stereo Conversion",
    "date": "24 May, 2023",
    "authors": [
      "Rui Liu",
      " Jinhua Zhang",
      " Guanglai Gao",
      " Haizhou Li"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15678",
    "paper_id": "2305.15678",
    "abstract": "\n        Recent advancements in high-quality, large-scale English resources have pushed the frontier of English Automatic Text Simplification (ATS) research. However, less work has been done on multilingual text simplification due to the lack of a diverse evaluation benchmark that covers complex-simple sentence pairs in many languages. This paper introduces the MultiSim benchmark, a collection of 27 resources in 12 distinct languages containing over 1.7 million complex-simple sentence pairs. This benchmark will encourage research in developing more effective multilingual text simplification models and evaluation metrics. Our experiments using MultiSim with pre-trained multilingual language models reveal exciting performance improvements from multilingual training in non-English settings. We observe strong performance from Russian in zero-shot cross-lingual transfer to low-resource languages. We further show that few-shot prompting with BLOOM-176b achieves comparable quality to reference simplifications outperforming fine-tuned models in most languages. We validate these findings through human evaluation.\n        \u25b3 Less\n      ",
    "title": "Revisiting non-English Text Simplification: A Unified Multilingual Benchmark",
    "date": "24 May, 2023",
    "authors": [
      "Michael J. Ryan",
      " Tarek Naous",
      " Wei Xu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15692",
    "paper_id": "2305.15692",
    "abstract": "\n        Currently, video behavior recognition is one of the most foundational tasks of computer vision. The 2D neural networks of deep learning are built for recognizing pixel-level information such as images with RGB, RGB-D, or optical flow formats, with the current increasingly wide usage of surveillance video and more tasks related to human action recognition. There are increasing tasks requiring temporal information for frames dependency analysis. The researchers have widely studied video-based recognition rather than image-based(pixel-based) only to extract more informative elements from geometry tasks. Our current related research addresses multiple novel proposed research works and compares their advantages and disadvantages between the derived deep learning frameworks rather than machine learning frameworks. The comparison happened between existing frameworks and datasets, which are video format data only. Due to the specific properties of human actions and the increasingly wide usage of deep neural networks, we collected all research works within the last three years between 2020 to 2022. In our article, the performance of deep neural networks surpassed most of the techniques in the feature learning and extraction tasks, especially video action recognition.\n        \u25b3 Less\n      ",
    "title": "Deep Neural Networks in Video Human Action Recognition: A Review",
    "date": "24 May, 2023",
    "authors": [
      "Zihan Wang",
      " Yang Yang",
      " Zhi Liu",
      " Yifan Zheng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15695",
    "paper_id": "2305.15695",
    "abstract": "\n        With strong capabilities of reasoning and a generic understanding of the world, Large Language Models (LLMs) have shown great potential in building versatile embodied decision making agents capable of performing diverse tasks. However, when deployed to unfamiliar environments, we show that LLM agents face challenges in efficiently gathering necessary information, leading to suboptimal performance. On the other hand, in unfamiliar scenarios, human individuals often seek additional information from their peers before taking action, leveraging external knowledge to avoid unnecessary trial and error. Building upon this intuition, we propose \\textit{Asking Before Action} (ABA), a method that empowers the agent to proactively query external sources for pertinent information using natural language during their interactions in the environment. In this way, the agent is able to enhance its efficiency and performance by mitigating wasteful steps and circumventing the difficulties associated with exploration in unfamiliar environments. We empirically evaluate our method on an embodied decision making benchmark, ALFWorld, and demonstrate that despite modest modifications in prompts, our method exceeds baseline LLM agents by more than 4040%. Further experiments on two variants of ALFWorld illustrate that by imitation learning, ABA effectively retains and reuses queried and known information in subsequent tasks, mitigating the need for repetitive inquiries. Both qualitative and quantitative results exhibit remarkable performance on tasks that previous methods struggle to solve.\n        \u25b3 Less\n      ",
    "title": "Asking Before Action: Gather Information in Embodied Decision Making with Language Models",
    "date": "24 May, 2023",
    "authors": [
      "Xiaoyu Chen",
      " Shenao Zhang",
      " Pushi Zhang",
      " Li Zhao",
      " Jianyu Chen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15697",
    "paper_id": "2305.15697",
    "abstract": "\n        Recently, inference privacy has attracted increasing attention. The inference privacy concern arises most notably in the widely deployed edge-cloud video analytics systems, where the cloud needs the videos captured from the edge. The video data can contain sensitive information and subject to attack when they are transmitted to the cloud for inference. Many privacy protection schemes have been proposed. Yet, the performance of a scheme needs to be determined by experiments or inferred by analyzing the specific case. In this paper, we propose a new metric, \\textit{privacy protectability}, to characterize to what degree a video stream can be protected given a certain video analytics task. Such a metric has strong operational meaning. For example, low protectability means that it may be necessary to set up an overall secure environment. We can also evaluate a privacy protection scheme, e.g., assume it obfuscates the video data, what level of protection this scheme has achieved after obfuscation. Our definition of privacy protectability is rooted in information theory and we develop efficient algorithms to estimate the metric. We use experiments on real data to validate that our metric is consistent with empirical measurements on how well a video stream can be protected for a video analytics task.\n        \u25b3 Less\n      ",
    "title": "Privacy Protectability: An Information-theoretical Approach",
    "date": "24 May, 2023",
    "authors": [
      "Siping Shi",
      " Bihai Zhang",
      " Dan Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.08057",
    "paper_id": "2306.08057",
    "abstract": "\n        Learning symbolic expressions directly from experiment data is a vital step in AI-driven scientific discovery. Nevertheless, state-of-the-art approaches are limited to learning simple expressions. Regressing expressions involving many independent variables still remain out of reach. Motivated by the control variable experiments widely utilized in science, we propose Control Variable Genetic Programming (CVGP) for symbolic regression over many independent variables. CVGP expedites symbolic expression discovery via customized experiment design, rather than learning from a fixed dataset collected a priori. CVGP starts by fitting simple expressions involving a small set of independent variables using genetic programming, under controlled experiments where other variables are held as constants. It then extends expressions learned in previous generations by adding new independent variables, using new control variable experiments in which these variables are allowed to vary. Theoretically, we show CVGP as an incremental building approach can yield an exponential reduction in the search space when learning a class of expressions. Experimentally, CVGP outperforms several baselines in learning symbolic expressions involving multiple independent variables.\n        \u25b3 Less\n      ",
    "title": "Symbolic Regression via Control Variable Genetic Programming",
    "date": "24 May, 2023",
    "authors": [
      "Nan Jiang",
      " Yexiang Xue"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2302.05441",
    "paper_id": "2302.05441",
    "abstract": "\n        Transfer learning with a small amount of target data is an effective and common approach to adapting a pre-trained model to distribution shifts. In some situations, target data labels may be expensive to obtain, so we may only have access to a limited number of target data points. To make the most of a very small target dataset, we propose a lightweight, sample-efficient approach that learns a diverse set of features and adapts to a target distribution by interpolating these features. Our approach, Project and Probe (Pro2^2), first learns a linear projection that maps a pre-trained embedding onto orthogonal directions while being predictive of labels in the source dataset. The goal of this step is to learn a variety of predictive features, so that at least some of them remain useful after distribution shift. Pro2^2 then learns a linear classifier on top of these projected features using a small target dataset. Theoretically, we find that Pro2^2 results in more sample-efficient generalization by inducing a favorable bias-variance tradeoff. Our experiments on four datasets, with multiple distribution shift settings for each, show that Pro2^2 improves performance by 5-15% when given limited target data compared to prior methods such as standard linear probing.\n        \u25b3 Less\n      ",
    "title": "Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features",
    "date": "24 May, 2023",
    "authors": [
      "Annie S. Chen",
      " Yoonho Lee",
      " Amrith Setlur",
      " Sergey Levine",
      " Chelsea Finn"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15719",
    "paper_id": "2305.15719",
    "abstract": "\n        Recent progress in music generation has been remarkably advanced by the state-of-the-art MusicLM, which comprises a hierarchy of three LMs, respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet, sampling with the MusicLM requires processing through these LMs one by one to obtain the fine-grained acoustic tokens, making it computationally expensive and prohibitive for a real-time generation. Efficient music generation with a quality on par with MusicLM remains a significant challenge. In this paper, we present MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusion model that generates music audios of state-of-the-art quality meanwhile reducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling 10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM for semantic modeling, and applies a novel dual-path diffusion (DPD) model and an audio VAE-GAN to efficiently decode the conditioning semantic tokens into waveform. DPD is proposed to simultaneously model the coarse and fine acoustics by incorporating the semantic information into segments of latents effectively via cross-attention at each denoising step. Our experimental results suggest the superiority of MeLoDy, not only in its practical advantages on sampling speed and infinitely continuable generation, but also in its state-of-the-art musicality, audio quality, and text correlation.\n  Our samples are available at https://Efficient-MeLoDy.github.io/.\n        \u25b3 Less\n      ",
    "title": "Efficient Neural Music Generation",
    "date": "24 May, 2023",
    "authors": [
      "Max W. Y. Lam",
      " Qiao Tian",
      " Tang Li",
      " Zongyu Yin",
      " Siyuan Feng",
      " Ming Tu",
      " Yuliang Ji",
      " Rui Xia",
      " Mingbo Ma",
      " Xuchen Song",
      " Jitong Chen",
      " Yuping Wang",
      " Yuxuan Wang"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11566",
    "paper_id": "2305.11566",
    "abstract": "\n        We present a lightweight system for stereo matching through embedded GPUs. It breaks the trade-off between accuracy and processing speed in stereo matching, enabling our embedded system to further improve the matching accuracy while ensuring real-time processing. The main idea of our method is to construct a tiny neural network based on variational auto-encoder (VAE) to upsample and refinement a small size of coarse disparity map, which is first generated by a traditional matching method. The proposed hybrid structure cannot only bring the advantage of traditional methods in terms of computational complexity, but also ensure the matching accuracy under the impact of neural network. Extensive experiments on the KITTI 2015 benchmark demonstrate that our tiny system exhibits high robustness in improving the accuracy of the coarse disparity maps generated by different algorithms, while also running in real-time on embedded GPUs.\n        \u25b3 Less\n      ",
    "title": "StereoVAE: A lightweight stereo matching system through embedded GPUs",
    "date": "25 May, 2023",
    "authors": [
      "Qiong Chang",
      " Xiang Li",
      " Xin Xu",
      " Xin Liu",
      " Yun Li",
      " Miyazaki Jun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15734",
    "paper_id": "2305.15734",
    "abstract": "\n        Several recent studies have elucidated why knowledge distillation (KD) improves model performance. However, few have researched the other advantages of KD in addition to its improving model performance. In this study, we have attempted to show that KD enhances the interpretability as well as the accuracy of models. We measured the number of concept detectors identified in network dissection for a quantitative comparison of model interpretability. We attributed the improvement in interpretability to the class-similarity information transferred from the teacher to student models. First, we confirmed the transfer of class-similarity information from the teacher to student model via logit distillation. Then, we analyzed how class-similarity information affects model interpretability in terms of its presence or absence and degree of similarity information. We conducted various quantitative and qualitative experiments and examined the results on different datasets, different KD methods, and according to different measures of interpretability. Our research showed that KD models by large models could be used more reliably in various fields.\n        \u25b3 Less\n      ",
    "title": "On the Impact of Knowledge Distillation for Model Interpretability",
    "date": "25 May, 2023",
    "authors": [
      "Hyeongrok Han",
      " Siwon Kim",
      " Hyun-Soo Choi",
      " Sungroh Yoon"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15735",
    "paper_id": "2305.15735",
    "abstract": "\n        Most MPC (Model Predictive Control) algorithms used in industries and studied in the control academia use a two-term QP (quadratic programming), where the first term is the weighted norm of the output errors, and the second term is that of the input increments. In this work, a DMC (Dynamic Matrix Control) algorithm that uses three-term QP is studied, where the third term is the weighted norm of the output increments. In the analysis, a relationship between the three-term DMC and the two-term DMC is established; based on that, the closed-loop response curves are derived. Based on the analysis, two controller tuning procedures are developed for the three-term DMC, one for closed-loop step response and one for disturbance reduction. Finally, it will be proven that the three-term DMC can achieve a higher performance and robustness than the two-term DMC can. Simulation studies are used to demonstrate the findings and the tuning methods.\n        \u25b3 Less\n      ",
    "title": "Analysis and tuning of a three-term DMC",
    "date": "25 May, 2023",
    "authors": [
      "Yun Zhu",
      " Kangkang Zhang",
      " Yuncai Zhu",
      " Jinming Zhou"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15740",
    "paper_id": "2305.15740",
    "abstract": "\n        When virtual agents interact with humans, gestures are crucial to delivering their intentions with speech. Previous multimodal co-speech gesture generation models required encoded features of all modalities to generate gestures. If some input modalities are removed or contain noise, the model may not generate the gestures properly. To acquire robust and generalized encodings, we propose a novel framework with a multimodal pre-trained encoder for co-speech gesture generation. In the proposed method, the multi-head-attention-based encoder is trained with self-supervised learning to contain the information on each modality. Moreover, we collect full-body gestures that consist of 3D joint rotations to improve visualization and apply gestures to the extensible body model. Through the series of experiments and human evaluation, the proposed method renders realistic co-speech gestures not only when all input modalities are given but also when the input modalities are missing or noisy.\n        \u25b3 Less\n      ",
    "title": "MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation",
    "date": "25 May, 2023",
    "authors": [
      "Gwantae Kim",
      " Seonghyeok Noh",
      " Insung Ham",
      " Hanseok Ko"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15743",
    "paper_id": "2305.15743",
    "abstract": "\n        Traffic simulation is a crucial tool for transportation decision-making and policy development. However, achieving realistic simulations in the face of the high dimensionality and heterogeneity of traffic environments is a longstanding challenge. In this paper, we present TransWordNG, a traffic simulator that uses Data-driven algorithms and Graph Computing techniques to learn traffic dynamics from real data. The functionality and structure of TransWorldNG are introduced, which utilize a foundation model for transportation management and control. The results demonstrate that TransWorldNG can generate more realistic traffic patterns compared to traditional simulators. Additionally, TransWorldNG exhibits better scalability, as it shows linear growth in computation time as the scenario scale increases. To the best of our knowledge, this is the first traffic simulator that can automatically learn traffic patterns from real-world data and efficiently generate accurate and realistic traffic environments.\n        \u25b3 Less\n      ",
    "title": "TransWorldNG: Traffic Simulation via Foundation Model",
    "date": "25 May, 2023",
    "authors": [
      "Ding Wang",
      " Xuhong Wang",
      " Liang Chen",
      " Shengyue Yao",
      " Ming Jing",
      " Honghai Li",
      " Li Li",
      " Shiqiang Bao",
      " Fei-Yue Wang",
      " Yilun Lin"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2205.08099",
    "paper_id": "2205.08099",
    "abstract": "\n        State-of-the-art deep learning models have a parameter count that reaches into the billions. Training, storing and transferring such models is energy and time consuming, thus costly. A big part of these costs is caused by training the network. Model compression lowers storage and transfer costs, and can further make training more efficient by decreasing the number of computations in the forward and/or backward pass. Thus, compressing networks also at training time while maintaining a high performance is an important research topic. This work is a survey on methods which reduce the number of trained weights in deep learning models throughout the training. Most of the introduced methods set network parameters to zero which is called pruning. The presented pruning approaches are categorized into pruning at initialization, lottery tickets and dynamic sparse training. Moreover, we discuss methods that freeze parts of a network at its random initialization. By freezing weights, the number of trainable parameters is shrunken which reduces gradient computations and the dimensionality of the model's optimization space. In this survey we first propose dimensionality reduced training as an underlying mathematical model that covers pruning and freezing during training. Afterwards, we present and discuss different dimensionality reduced training methods.\n        \u25b3 Less\n      ",
    "title": "Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey",
    "date": "25 May, 2023",
    "authors": [
      "Paul Wimmer",
      " Jens Mehnert",
      " Alexandru Paul Condurache"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.11442",
    "paper_id": "2305.11442",
    "abstract": "\n        Existing solutions to zero-shot text classification either conduct prompting with pre-trained language models, which is sensitive to the choices of templates, or rely on large-scale annotated data of relevant tasks for meta-tuning. In this work, we propose a new paradigm based on self-supervised learning to solve zero-shot text classification tasks by tuning the language models with unlabeled data, called self-supervised tuning. By exploring the inherent structure of free texts, we propose a new learning objective called first sentence prediction to bridge the gap between unlabeled data and text classification tasks. After tuning the model to learn to predict the first sentence in a paragraph based on the rest, the model is able to conduct zero-shot inference on unseen tasks such as topic classification and sentiment analysis. Experimental results show that our model outperforms the state-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis reveals that our model is less sensitive to the prompt design. Our code and pre-trained models are publicly available at https://github.com/DAMO-NLP-SG/SSTuning .\n        \u25b3 Less\n      ",
    "title": "Zero-Shot Text Classification via Self-Supervised Tuning",
    "date": "25 May, 2023",
    "authors": [
      "Chaoqun Liu",
      " Wenxuan Zhang",
      " Guizhen Chen",
      " Xiaobao Wu",
      " Anh Tuan Luu",
      " Chip Hong Chang",
      " Lidong Bing"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15757",
    "paper_id": "2305.15757",
    "abstract": "\n        Recent years have seen increasing concerns about the unsafe response generation of large-scale dialogue systems, where agents will learn offensive or biased behaviors from the real-world corpus. Some methods are proposed to address the above issue by detecting and replacing unsafe training examples in a pipeline style. Though effective, they suffer from a high annotation cost and adapt poorly to unseen scenarios as well as adversarial attacks. Besides, the neglect of providing safe responses (e.g. simply replacing with templates) will cause the information-missing problem of dialogues. To address these issues, we propose an unsupervised pseudo-label sampling method, TEMP, that can automatically assign potential safe responses. Specifically, our TEMP method groups responses into several clusters and samples multiple labels with an adaptively sharpened sampling strategy, inspired by the observation that unsafe samples in the clusters are usually few and distribute in the tail. Extensive experiments in chitchat and task-oriented dialogues show that our TEMP outperforms state-of-the-art models with weak supervision signals and obtains comparable results under unsupervised learning settings.\n        \u25b3 Less\n      ",
    "title": "Healing Unsafe Dialogue Responses with Weak Supervision Signals",
    "date": "25 May, 2023",
    "authors": [
      "Zi Liang",
      " Pinghui Wang",
      " Ruofei Zhang",
      " Shuo Zhang",
      " Xiaofan Ye Yi Huang",
      " Junlan Feng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15765",
    "paper_id": "2305.15765",
    "abstract": "\n        This paper addresses the problem of 3D referring expression comprehension (REC) in autonomous driving scenario, which aims to ground a natural language to the targeted region in LiDAR point clouds. Previous approaches for REC usually focus on the 2D or 3D-indoor domain, which is not suitable for accurately predicting the location of the queried 3D region in an autonomous driving scene. In addition, the upper-bound limitation and the heavy computation cost motivate us to explore a better solution. In this work, we propose a new multi-modal visual grounding task, termed LiDAR Grounding. Then we devise a Multi-modal Single Shot Grounding (MSSG) approach with an effective token fusion strategy. It jointly learns the LiDAR-based object detector with the language features and predicts the targeted region directly from the detector without any post-processing. Moreover, the image feature can be flexibly integrated into our approach to provide rich texture and color information. The cross-modal learning enforces the detector to concentrate on important regions in the point cloud by considering the informative language expressions, thus leading to much better accuracy and efficiency. Extensive experiments on the Talk2Car dataset demonstrate the effectiveness of the proposed methods. Our work offers a deeper insight into the LiDAR-based grounding task and we expect it presents a promising direction for the autonomous driving community.\n        \u25b3 Less\n      ",
    "title": "Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving",
    "date": "25 May, 2023",
    "authors": [
      "Wenhao Cheng",
      " Junbo Yin",
      " Wei Li",
      " Ruigang Yang",
      " Jianbing Shen"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15770",
    "paper_id": "2305.15770",
    "abstract": "\n        Time series prediction is a prevalent issue across various disciplines, such as meteorology, traffic surveillance, investment, and energy production and consumption. Many statistical and machine-learning strategies have been developed to tackle this problem. However, these approaches either lack explainability or exhibit less satisfactory performance when the prediction horizon increases. To this end, we propose a novel plan for the designing of networks' architecture based on transformations, possessing the potential to achieve an enhanced receptive field in learning which brings benefits to fuse features across scales. In this context, we introduce four different transformation mechanisms as bases to construct the learning model including Fourier Transform (FT), Singular Value Decomposition (SVD), matrix multiplication and Conv block. Hence, we develop four learning models based on the above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD. Note that the FT and SVD blocks are capable of learning global information, while the Conv blocks focus on learning local information. The matrix block is sparsely designed to learn both global and local information simultaneously. The above Transformation Learning Networks (TLNets) have been extensively tested and compared with multiple baseline models based on several real-world datasets and showed clear potential in long-range time-series forecasting.\n        \u25b3 Less\n      ",
    "title": "TLNets: Transformation Learning Networks for long-range time-series prediction",
    "date": "25 May, 2023",
    "authors": [
      "Wei Wang",
      " Yang Liu",
      " Hao Sun"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15782",
    "paper_id": "2305.15782",
    "abstract": "\n        We define an extension of predicate logic, called Binding Logic, where variables can be bound in terms and in propositions. We introduce a notion of model for this logic and prove a soundness and completeness theorem for it. This theorem is obtained by encoding this logic back into predicate logic and using the classical soundness and completeness theorem there.\n        \u25b3 Less\n      ",
    "title": "Binding Logic: proofs and models",
    "date": "25 May, 2023",
    "authors": [
      "Gilles Dowek",
      " Th\u00e9r\u00e8se Hardin",
      " Claude Kirchner"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.08056",
    "paper_id": "2306.08056",
    "abstract": "\n        Distributed trust is a nebulous concept that has evolved from different perspectives in recent years. While one can attribute its current prominence to blockchain and cryptocurrency, the distributed trust concept has been cultivating progress in federated learning, trustworthy and responsible AI in an ecosystem setting, data sharing, privacy issues across organizational boundaries, and zero trust cybersecurity. This paper will survey the concept of distributed trust in multiple disciplines. It will take a system/software architecture point of view to look at trust redistribution/shift and the associated tradeoffs in systems and applications enabled by distributed trust technologies.\n        \u25b3 Less\n      ",
    "title": "Distributed Trust Through the Lens of Software Architecture",
    "date": "25 May, 2023",
    "authors": [
      "Sin Kit Lo",
      " Yue Liu",
      " Guangsheng Yu",
      " Qinghua Lu",
      " Xiwei Xu",
      " Liming Zhu"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2301.05860",
    "paper_id": "2301.05860",
    "abstract": "\n        Graphs have a superior ability to represent relational data, like chemical compounds, proteins, and social networks. Hence, graph-level learning, which takes a set of graphs as input, has been applied to many tasks including comparison, regression, classification, and more. Traditional approaches to learning a set of graphs heavily rely on hand-crafted features, such as substructures. But while these methods benefit from good interpretability, they often suffer from computational bottlenecks as they cannot skirt the graph isomorphism problem. Conversely, deep learning has helped graph-level learning adapt to the growing scale of graphs by extracting features automatically and encoding graphs into low-dimensional representations. As a result, these deep graph learning methods have been responsible for many successes. Yet, there is no comprehensive survey that reviews graph-level learning starting with traditional learning and moving through to the deep learning approaches. This article fills this gap and frames the representative algorithms into a systematic taxonomy covering traditional learning, graph-level deep neural networks, graph-level graph neural networks, and graph pooling. To ensure a thoroughly comprehensive survey, the evolutions, interactions, and communications between methods from four different branches of development are also examined. This is followed by a brief review of the benchmark data sets, evaluation metrics, and common downstream applications. The survey concludes with a broad overview of 12 current and future directions in this booming field.\n        \u25b3 Less\n      ",
    "title": "State of the Art and Potentialities of Graph-level Learning",
    "date": "25 May, 2023",
    "authors": [
      "Zhenyu Yang",
      " Ge Zhang",
      " Jia Wu",
      " Jian Yang",
      " Quan Z. Sheng",
      " Shan Xue",
      " Chuan Zhou",
      " Charu Aggarwal",
      " Hao Peng",
      " Wenbin Hu",
      " Edwin Hancock",
      " Pietro Li\u00f2"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15788",
    "paper_id": "2305.15788",
    "abstract": "\n        The paper speculates about how ChatGPT-like systems can support the field of automated service composition and identifies new research areas to explore in order to take advantage of such tools in the field of service-oriented composition.\n        \u25b3 Less\n      ",
    "title": "Service Composition in the ChatGPT Era",
    "date": "25 May, 2023",
    "authors": [
      "Marco Aiello",
      " Ilche Georgievski"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.13869",
    "paper_id": "2305.13869",
    "abstract": "\n        The superconducting linear accelerator is a highly flexiable facility for modern scientific discoveries, necessitating weekly reconfiguration and tuning. Accordingly, minimizing setup time proves essential in affording users with ample experimental time. We propose a trend-based soft actor-critic(TBSAC) beam control method with strong robustness, allowing the agents to be trained in a simulated environment and applied to the real accelerator directly with zero-shot. To validate the effectiveness of our method, two different typical beam control tasks were performed on China Accelerator Facility for Superheavy Elements (CAFe II) and a light particle injector(LPI) respectively. The orbit correction tasks were performed in three cryomodules in CAFe II seperately, the time required for tuning has been reduced to one-tenth of that needed by human experts, and the RMS values of the corrected orbit were all less than 1mm. The other transmission efficiency optimization task was conducted in the LPI, our agent successfully optimized the transmission efficiency of radio-frequency quadrupole(RFQ) to over 85%85\\% within 2 minutes. The outcomes of these two experiments offer substantiation that our proposed TBSAC approach can efficiently and effectively accomplish beam commissioning tasks while upholding the same standard as skilled human experts. As such, our method exhibits potential for future applications in other accelerator commissioning fields.\n        \u25b3 Less\n      ",
    "title": "Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator",
    "date": "25 May, 2023",
    "authors": [
      "Xiaolong Chen",
      " Xin Qi",
      " Chunguang Su",
      " Yuan He",
      " Zhijun Wang",
      " Kunxiang Sun",
      " Chao Jin",
      " Weilong Chen",
      " Shuhui Liu",
      " Xiaoying Zhao",
      " Duanyang Jia",
      " Man Yi"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15793",
    "paper_id": "2305.15793",
    "abstract": "\n        In recent years, numerous screening methods have been published for ultrahigh-dimensional data that contain hundreds of thousands of features; however, most of these features cannot handle data with thousands of classes. Prediction models built to authenticate users based on multichannel biometric data result in this type of problem. In this study, we present a novel method known as random forest-based multiround screening (RFMS) that can be effectively applied under such circumstances. The proposed algorithm divides the feature space into small subsets and executes a series of partial model builds. These partial models are used to implement tournament-based sorting and the selection of features based on their importance. To benchmark RFMS, a synthetic biometric feature space generator known as BiometricBlender is employed. Based on the results, the RFMS is on par with industry-standard feature screening methods while simultaneously possessing many advantages over these methods.\n        \u25b3 Less\n      ",
    "title": "Feature space reduction method for ultrahigh-dimensional, multiclass data: Random forest-based multiround screening (RFMS)",
    "date": "25 May, 2023",
    "authors": [
      "Gergely Hancz\u00e1r",
      " Marcell Stippinger",
      " D\u00e1vid Han\u00e1k",
      " Marcell T. Kurbucz",
      " Oliv\u00e9r M. T\u00f6rteli",
      " \u00c1gnes Chripk\u00f3",
      " Zolt\u00e1n Somogyv\u00e1ri"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15801",
    "paper_id": "2305.15801",
    "abstract": "\n        A successful tactic that is followed by the scientific community for advancing AI is to treat games as problems, which has been proven to lead to various breakthroughs. We adapt this strategy in order to study Rocket League, a widely popular but rather under-explored 3D multiplayer video game with a distinct physics engine and complex dynamics that pose a significant challenge in developing efficient and high-performance game-playing agents. In this paper, we present Lucy-SKG, a Reinforcement Learning-based model that learned how to play Rocket League in a sample-efficient manner, outperforming by a notable margin the two highest-ranking bots in this game, namely Necto (2022 bot champion) and its successor Nexto, thus becoming a state-of-the-art agent. Our contributions include: a) the development of a reward analysis and visualization library, b) novel parameterizable reward shape functions that capture the utility of complex reward types via our proposed Kinesthetic Reward Combination (KRC) technique, and c) design of auxiliary neural architectures for training on reward prediction and state representation tasks in an on-policy fashion for enhanced efficiency in learning speed and performance. By performing thorough ablation studies for each component of Lucy-SKG, we showed their independent effectiveness in overall performance. In doing so, we demonstrate the prospects and challenges of using sample-efficient Reinforcement Learning techniques for controlling complex dynamical systems under competitive team-based multiplayer conditions.\n        \u25b3 Less\n      ",
    "title": "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep Reinforcement Learning",
    "date": "25 May, 2023",
    "authors": [
      "Vasileios Moschopoulos",
      " Pantelis Kyriakidis",
      " Aristotelis Lazaridis",
      " Ioannis Vlahavas"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15809",
    "paper_id": "2305.15809",
    "abstract": "\n        Large language models (LLMs) providing generative AI have become popular to support software engineers in creating, summarizing, optimizing, and documenting source code. It is still unknown how LLMs can support control engineers using typical control programming languages in programming tasks. Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source code generation but did not yet tackle control logic programming. The contribution of this paper is an exploratory study, for which we created 100 LLM prompts in 10 representative categories to analyze control logic generation for of PLCs and DCS from natural language. We tested the prompts by generating answers with ChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3 Structured Text code in many cases and demonstrated useful reasoning skills that could boost control engineer productivity. Our prompt collection is the basis for a more formal LLM benchmark to test and compare such models for control logic generation.\n        \u25b3 Less\n      ",
    "title": "ChatGPT for PLC/DCS Control Logic Generation",
    "date": "25 May, 2023",
    "authors": [
      "Heiko Koziolek",
      " Sten Gruener",
      " Virendra Ashiwal"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2303.08035",
    "paper_id": "2303.08035",
    "abstract": "\n        Deep Learning (DL) systems have proliferated in many applications, requiring specialized hardware accelerators and chips. In the nano-era, devices have become increasingly more susceptible to permanent and transient faults. Therefore, we need an efficient methodology for analyzing the resilience of advanced DL systems against such faults, and understand how the faults in neural accelerator chips manifest as errors at the DL application level, where faults can lead to undetectable and unrecoverable errors. Using fault injection, we can perform resilience investigations of the DL system by modifying neuron weights and outputs at the software-level, as if the hardware had been affected by a transient fault. Existing fault models reduce the search space, allowing faster analysis, but requiring a-priori knowledge on the model, and not allowing further analysis of the filtered-out search space. Therefore, we propose ISimDL, a novel methodology that employs neuron sensitivity to generate importance sampling-based fault-scenarios. Without any a-priori knowledge of the model-under-test, ISimDL provides an equivalent reduction of the search space as existing works, while allowing long simulations to cover all the possible faults, improving on existing model requirements. Our experiments show that the importance sampling provides up to 15x higher precision in selecting critical faults than the random uniform sampling, reaching such precision in less than 100 faults. Additionally, we showcase another practical use-case for importance sampling for reliable DNN design, namely Fault Aware Training (FAT). By using ISimDL to select the faults leading to errors, we can insert the faults during the DNN training process to harden the DNN against such faults. Using importance sampling in FAT reduces the overhead required for finding faults that lead to a predetermined drop in accuracy by more than 12x.\n        \u25b3 Less\n      ",
    "title": "ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning",
    "date": "25 May, 2023",
    "authors": [
      "Alessio Colucci",
      " Andreas Steininger",
      " Muhammad Shafique"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15816",
    "paper_id": "2305.15816",
    "abstract": "\n        Diffusion-based generative models have exhibited powerful generative performance in recent years. However, as many attributes exist in the data distribution and owing to several limitations of sharing the model parameters across all levels of the generation process, it remains challenging to control specific styles for each attribute. To address the above problem, this paper presents decoupled denoising diffusion models (DDDMs) with disentangled representations, which can control the style for each attribute in generative models. We apply DDDMs to voice conversion (VC) tasks to address the challenges of disentangling and controlling each speech attribute (e.g., linguistic information, intonation, and timbre). First, we use a self-supervised representation to disentangle the speech representation. Subsequently, the DDDMs are applied to resynthesize the speech from the disentangled representations for denoising with respect to each attribute. Moreover, we also propose the prior mixup for robust voice style transfer, which uses the converted representation of the mixed style as a prior distribution for the diffusion models. The experimental results reveal that our method outperforms publicly available VC models. Furthermore, we show that our method provides robust generative performance regardless of the model size. Audio samples are available https://hayeong0.github.io/DDDM-VC-demo/.\n        \u25b3 Less\n      ",
    "title": "DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled Representation and Prior Mixup for Verified Robust Voice Conversion",
    "date": "25 May, 2023",
    "authors": [
      "Ha-Yeong Choi",
      " Sang-Hoon Lee",
      " Seong-Whan Lee"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.16361",
    "paper_id": "2305.16361",
    "abstract": "\n        EXplainable Artificial Intelligence (XAI) aims to help users to grasp the reasoning behind the predictions of an Artificial Intelligence (AI) system. Many XAI approaches have emerged in recent years. Consequently, a subfield related to the evaluation of XAI methods has gained considerable attention, with the aim to determine which methods provide the best explanation using various approaches and criteria. However, the literature lacks a comparison of the evaluation metrics themselves, that one can use to evaluate XAI methods. This work aims to fill this gap by comparing 14 different metrics when applied to nine state-of-the-art XAI methods and three dummy methods (e.g., random saliency maps) used as references. Experimental results show which of these metrics produces highly correlated results, indicating potential redundancy. We also demonstrate the significant impact of varying the baseline hyperparameter on the evaluation metric values. Finally, we use dummy methods to assess the reliability of metrics in terms of ranking, pointing out their limitations.\n        \u25b3 Less\n      ",
    "title": "An Experimental Investigation into the Evaluation of Explainability Methods",
    "date": "25 May, 2023",
    "authors": [
      "S\u00e9drick Stassin",
      " Alexandre Englebert",
      " G\u00e9raldin Nanfack",
      " Julien Albert",
      " Nassim Versbraegen",
      " Gilles Peiffer",
      " Miriam Doh",
      " Nicolas Riche",
      " Beno\u00eet Frenay",
      " Christophe De Vleeschouwer"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.15853",
    "paper_id": "2305.15853",
    "abstract": "\n        Several explanation methods such as Integrated Gradients (IG) can be characterised as path-based methods, as they rely on a straight line between the data and an uninformative baseline. However, when applied to language models, these methods produce a path for each word of a sentence simultaneously, which could lead to creating sentences from interpolated words either having no clear meaning, or having a significantly different meaning compared to the original sentence. In order to keep the meaning of these sentences as close as possible to the original one, we propose Sequential Integrated Gradients (SIG), which computes the importance of each word in a sentence by keeping fixed every other words, only creating interpolations between the baseline and the word of interest. Moreover, inspired by the training procedure of several language models, we also propose to replace the baseline token \"pad\" with the trained token \"mask\". While being a simple improvement over the original IG method, we show on various models and datasets that SIG proves to be a very effective method for explaining language models.\n        \u25b3 Less\n      ",
    "title": "Sequential Integrated Gradients: a simple but effective method for explaining language models",
    "date": "25 May, 2023",
    "authors": [
      "Joseph Enguehard"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2202.04350",
    "paper_id": "2202.04350",
    "abstract": "\n        Large pre-trained language models based on transformer architecture have drastically changed the natural language processing (NLP) landscape. However, deploying those models for on-device applications in constrained devices such as smart watches is completely impractical due to their size and inference cost. As an alternative to transformer-based architectures, recent work on efficient NLP has shown that weight-efficient models can attain competitive performance for simple tasks, such as slot filling and intent classification, with model sizes in the order of the megabyte. This work introduces the pNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLP that achieves high weight-efficiency thanks to a novel projection layer. We evaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingual semantic parsing datasets, MTOP and multiATIS. Our quantized model achieves 99.4% and 97.8% the performance of mBERT on MTOP and multi-ATIS, while using 170x fewer parameters. Our model consistently beats the state-of-the-art of tiny models (pQRNN), which is twice as large, by a margin up to 7.8% on MTOP.\n        \u25b3 Less\n      ",
    "title": "pNLP-Mixer: an Efficient all-MLP Architecture for Language",
    "date": "25 May, 2023",
    "authors": [
      "Francesco Fusco",
      " Damian Pascual",
      " Peter Staar",
      " Diego Antognini"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2305.14635",
    "paper_id": "2305.14635",
    "abstract": "\n        End-to-end speech translation (ST) is the task of translating speech signals in the source language into text in the target language. As a cross-modal task, end-to-end ST is difficult to train with limited data. Existing methods often try to transfer knowledge from machine translation (MT), but their performances are restricted by the modality gap between speech and text. In this paper, we propose Cross-modal Mixup via Optimal Transport CMOT to overcome the modality gap. We find the alignment between speech and text sequences via optimal transport and then mix up the sequences from different modalities at a token level using the alignment. Experiments on the MuST-C ST benchmark demonstrate that CMOT achieves an average BLEU of 30.0 in 8 translation directions, outperforming previous methods. Further analysis shows CMOT can adaptively find the alignment between modalities, which helps alleviate the modality gap between speech and text. Code is publicly available at https://github.com/ictnlp/CMOT.\n        \u25b3 Less\n      ",
    "title": "CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation",
    "date": "25 May, 2023",
    "authors": [
      "Yan Zhou",
      " Qingkai Fang",
      " Yang Feng"
    ]
  },
  {
    "url": "https://arxiv.org/abs/2306.05381",
    "paper_id": "2306.05381",
    "abstract": "\n        Car-following is a control process in which a following vehicle (FV) adjusts its acceleration to keep a safe distance from the lead vehicle (LV). Recently, there has been a booming of data-driven models that enable more accurate modeling of car-following through real-world driving datasets. Although there are several public datasets available, their formats are not always consistent, making it challenging to determine the state-of-the-art models and how well a new model performs compared to existing ones. In contrast, research fields such as image recognition and object detection have benchmark datasets like ImageNet, Microsoft COCO, and KITTI. To address this gap and promote the development of microscopic traffic flow modeling, we establish a public benchmark dataset for car-following behavior modeling. The benchmark consists of more than 80K car-following events extracted from five public driving datasets using the same criteria. These events cover diverse situations including different road types, various weather conditions, and mixed traffic flows with autonomous vehicles. Moreover, to give an overview of current progress in car-following modeling, we implemented and tested representative baseline models with the benchmark. Results show that the deep deterministic policy gradient (DDPG) based model performs competitively with a lower MSE for spacing compared to traditional intelligent driver model (IDM) and Gazis-Herman-Rothery (GHR) models, and a smaller collision rate compared to fully connected neural network (NN) and long short-term memory (LSTM) models in most datasets. The established benchmark will provide researchers with consistent data formats and metrics for cross-comparing different car-following models, promoting the development of more accurate models. We open-source our dataset and implementation code in https://github.com/HKUST-DRIVE-AI-LAB/FollowNet.\n        \u25b3 Less\n      ",
    "title": "FollowNet: A Comprehensive Benchmark for Car-Following Behavior Modeling",
    "date": "25 May, 2023",
    "authors": [
      "Xianda Chen",
      " Meixin Zhu",
      " Kehua Chen",
      " Pengqin Wang",
      " Hongliang Lu",
      " Hui Zhong",
      " Xu Han",
      " Yinhai Wang"
    ]
  }
]